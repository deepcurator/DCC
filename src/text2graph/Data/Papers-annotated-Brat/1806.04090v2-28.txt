we show that atomo on the svd of each layer's gradient, can lead to less variance, and faster training, for the same communication budget as that of qsgd or terngrad.