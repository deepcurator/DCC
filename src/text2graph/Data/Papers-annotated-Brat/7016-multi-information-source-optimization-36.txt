outside of both the miso and multi-fidelity settings, klein et al [17] considered hyperparameter optimization of machine learning algorithms over a large dataset d. supposing access to subsets of d of arbitrary sizes, they show how to exploit regularity of performance across dataset sizes to significantly speed up the optimization process for support vector machines and neural networks.