in this work, we propose terngrad that uses ternary gradients to accelerate distributed deep learning in data parallelism.