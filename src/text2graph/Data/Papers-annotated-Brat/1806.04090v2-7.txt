we show that methods such as qsgd and terngrad are special cases of atomo and show that sparsifiying gradients in their singular value decomposition (svd), rather than the coordinate-wise one, can lead to significantly faster distributed training.