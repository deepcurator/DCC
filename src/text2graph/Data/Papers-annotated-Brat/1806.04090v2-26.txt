then, we argue that for some neural network applications, viewing the gradient as a concatenation of matrices (each corresponding to a layer), and applying atomic sparsification to their svd is meaningful and well-motivated by the fact that these matrices are "nearly" low-rank, e.g., see fig.