first, we identify a basic underlying limitation in using gradient descent-based methods in conjunction with smooth (infinitely differentiable) kernels typically used in machine learning, showing that only very smooth functions can be approximated after polynomially many steps of gradient descent.