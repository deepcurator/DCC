one reason why neural networks are so large is that they bias towards linear behavior: if the activation function is largely linear, so will the hidden layer be.