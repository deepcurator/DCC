abstract distributed model training suffers from communication overheads due to frequent gradient updates transmitted between compute nodes.