by exploring an expressive posterior over the parameters of the generator, the bayesian gan avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including svhn, celeba, and cifar-10, outperforming dcgan, wasserstein gans, and dcgan ensembles.