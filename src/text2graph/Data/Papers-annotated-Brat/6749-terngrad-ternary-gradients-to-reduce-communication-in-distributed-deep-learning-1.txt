abstract high network communication cost for synchronizing gradients and parameters is the well-known bottleneck of distributed training.