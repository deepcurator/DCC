keskar et al (2017) focused on a long observed phenomenon (lecun et al, 1998a ) -that when a large batch size is used while training dnns, the trained models appear to generalize less well.