despite the np-hardness of training general neural loss functions (blum & rivest, 1989) , simple gradient methods often find global minimizers (parameter configurations with zero or near-zero training loss), even when data and labels are randomized before training (zhang et al, 2017) .