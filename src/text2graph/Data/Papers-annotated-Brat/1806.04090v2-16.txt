to reduce the cost of of communication during distributed model training, a series of recent studies propose communicating low-precision or sparsified versions of the computed gradients during model updates.