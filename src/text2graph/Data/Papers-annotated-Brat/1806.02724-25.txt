to summarize our contributions: we propose a novel approach to vision-and-language navigation incorporating a visually grounded speaker-follower model, and introduce a panoramic representation to efficiently represent high-level actions.