â€¢ there is no inherent "generalization gap": large-batch training can generalize as well as small batch training by adapting the number of iterations.