even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles.