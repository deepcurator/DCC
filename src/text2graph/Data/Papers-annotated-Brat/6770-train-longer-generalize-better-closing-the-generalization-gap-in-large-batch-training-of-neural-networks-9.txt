we further investigate different techniques to train models in the large-batch regime and present a novel algorithm named "ghost batch normalization" which enables significant decrease in the generalization gap without increasing the number of updates.