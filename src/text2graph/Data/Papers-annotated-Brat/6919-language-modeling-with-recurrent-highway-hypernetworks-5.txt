finally, we argue for rhns as a drop-in replacement for lstms (analogous to lstms for vanilla rnns) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.