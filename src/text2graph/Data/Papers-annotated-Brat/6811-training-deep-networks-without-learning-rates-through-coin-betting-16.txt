we leverage over recent advancements in the stochastic optimization literature to design a backprop-agation procedure that does not have a learning rate at all, yet it is as simple as the vanilla sgd.