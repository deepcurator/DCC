with respect to the optimization process, stochastic gradient descent (sgd) has proved itself to be a key component of the deep learning success, but its effectiveness strictly depends on the choice of the initial learning rate and learning rate schedule.