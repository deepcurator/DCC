naturally, this has inspired a lot of research and has given rise to new and currently very popular optimization methods such as adam [9] , adagrad [5] , or rmsprop [24] , which serve as competitive alternatives to classical stochastic gradient descent (sgd).