1	"we then use visualizations to explore sharpness/flatness of minimizers found by different methods, as well as the effect of <e1>network</e1> architecture choices (use of skip connections, number of filters, <e2>network</e2> depth) on the loss landscape."
sameAs(e1, e2)
Comment:

2	"it is well known <e1>that</e1> certain network architecture designs (e.g., skip connections) produce loss functions <e2>that</e2> train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better."
sameAs(e1, e2)
Comment:

3	"it is well known <e1>that</e1> certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers <e2>that</e2> generalize better."
sameAs(e1, e2)
Comment:

4	"it is well known that certain network architecture designs (e.g., skip connections) produce loss functions <e1>that</e1> train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers <e2>that</e2> generalize better."
sameAs(e1, e2)
Comment:

5	"despite the np-hardness of <e1>training</e1> general neural loss functions (blum & rivest, 1989) , simple gradient methods often find global minimizers (parameter configurations with zero or near-zero <e2>training</e2> loss), even when data and labels are randomized before training (zhang et al, 2017) ."
sameAs(e1, e2)
Comment:

6	"despite the np-hardness of <e1>training</e1> general neural loss functions (blum & rivest, 1989) , simple gradient methods often find global minimizers (parameter configurations with zero or near-zero training loss), even when data and labels are randomized before <e2>training</e2> (zhang et al, 2017) ."
sameAs(e1, e2)
Comment:

7	"despite the np-hardness of training general neural loss functions (blum & rivest, 1989) , simple gradient methods often find global minimizers (parameter configurations with zero or near-zero <e1>training</e1> loss), even when data and labels are randomized before <e2>training</e2> (zhang et al, 2017) ."
sameAs(e1, e2)
Comment:

8	"for <e1>data</e1> that is inherently of less dimension than the ambient space, such as surfaces in 3d space, or line sketches in 2d, it can be more effective if the <e2>data</e2> is represented as point cloud in the ambient space, rather than a dense grid of the entire space."
sameAs(e1, e2)
Comment:

9	"suppose the unordered set of the c dimensional input <e1>features</e1> are the same f = { f a , f b , f c , f d } in all the cases in figure 1 , and we have one convolution kernel k = [k α , k β , k γ , k δ ] t in shape 4 × c. in (i), by following canonical order given by the regular grid structure, the <e2>features</e2> in the local 2 × 2 patch can be casted into (ii, iii, and iv) ."
sameAs(e1, e2)
Comment:

10	"to address these problems, we propose to learn a x-transformation from the <e1>input</e1> points, and then use it to simultaneously weight the <e2>input</e2> features associated with the points and permute them into latent potentially canonical order, before the element-wise product and sum operations are applied."
sameAs(e1, e2)
Comment:

11	"we allow using a different gradient <e1>method</e1> for each of the two <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

12	"the performance is validated on multiple architectures including dense nets, cnns, resnets, and the recurrent differential neural computer on classical datasets <e1>mnist</e1>, fashion <e2>mnist</e2>, cifar10 and others."
sameAs(e1, e2)
Comment:

13	"naturally, this has inspired a lot of research and has given rise to new and currently very popular optimization methods such as adam [9] , adagrad [5] , or rmsprop [24] , which serve as competitive alternatives to classical <e1>stochastic gradient descent</e1> (<e2>sgd</e2>)."
sameAs(e1, e2)
Comment:

14	"while fully-supervised deep learning <e1>approaches</e1> [15, 21] can learn to bridge modalities, generative <e2>approaches</e2> promise to capture the joint distribution across modalities and flexibly support missing data."
sameAs(e1, e2)
Comment:

15	"while fully-supervised deep learning approaches [15, 21] can learn to bridge <e1>modalities</e1>, generative approaches promise to capture the joint distribution across <e2>modalities</e2> and flexibly support missing data."
sameAs(e1, e2)
Comment:

16	"the vae [11] jointly trains a generative model, from latent variables to <e1>observations</e1>, with an inference network from <e2>observations</e2> to latents."
sameAs(e1, e2)
Comment:

17	"moving to multiple <e1>modalities</e1> and missing data, we would naively need an inference network for each combination of <e2>modalities</e2>."
sameAs(e1, e2)
Comment:

18	"we train on <e1>mnist</e1> [13] , binarized <e2>mnist</e2> [12] , multimnist [5, 19] , fashionmnist [27] , and celeba [14] ."
sameAs(e1, e2)
Comment:

19	"notably, our <e1>model</e1> shares parameters to efficiently learn under any combination of missing <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

20	"deep neural networks are capable of modeling complex <e1>data</e1> distributions and they scale well with large training <e2>data</e2>."
sameAs(e1, e2)
Comment:

21	"introduction generative models based on <e1>deep learning</e1> have been successfully applied to many domains such as image synthesis (van den oord et al, 2016; karras et al, 2017) , <e2>audio synthesis</e2> engel et al, 2017; arik et al, 2017a) , and language modeling (jozefowicz et al, 2016; merity et al, 2017) ."
Used-for(e1, e2)
Comment:

22	"abstract solomonoff's general theory of inference (solomonoff, 1964)  and the minimum description length principle (grünwald, 2007; rissanen, 2007) formalize occam's razor, and hold <e1>that</e1> a good model of data is a model <e2>that</e2> is good at losslessly compressing the data, including the cost of describing the model itself."
sameAs(e1, e2)
Comment:

23	"abstract solomonoff's general theory of inference (solomonoff, 1964)  and the minimum description length principle (grünwald, 2007; rissanen, 2007) formalize occam's razor, and hold that a good <e1>model</e1> of data is a <e2>model</e2> that is good at losslessly compressing the data, including the cost of describing the model itself."
sameAs(e1, e2)
Comment:

24	"abstract solomonoff's general theory of inference (solomonoff, 1964)  and the minimum description length principle (grünwald, 2007; rissanen, 2007) formalize occam's razor, and hold that a good <e1>model</e1> of data is a model that is good at losslessly compressing the data, including the cost of describing the <e2>model</e2> itself."
sameAs(e1, e2)
Comment:

25	"abstract solomonoff's general theory of inference (solomonoff, 1964)  and the minimum description length principle (grünwald, 2007; rissanen, 2007) formalize occam's razor, and hold that a good model of <e1>data</e1> is a model that is good at losslessly compressing the <e2>data</e2>, including the cost of describing the model itself."
sameAs(e1, e2)
Comment:

26	"abstract solomonoff's general theory of inference (solomonoff, 1964)  and the minimum description length principle (grünwald, 2007; rissanen, 2007) formalize occam's razor, and hold that a good model of data is a <e1>model</e1> that is good at losslessly compressing the data, including the cost of describing the <e2>model</e2> itself."
sameAs(e1, e2)
Comment:

27	"to quantify the complexity of these <e1>models</e1> in light of their generalization ability, several metrics beyond parameter-counting have been measured, such as the number of degrees of freedom of <e2>models</e2> (gao and jojic, 2016) , or their intrinsic dimension (li et al, 2018) ."
sameAs(e1, e2)
Comment:

28	"in information theory and minimum description length (mdl), learning a good <e1>model</e1> of the data is recast as using the <e2>model</e2> to losslessly transmit the data in as few bits as possible."
sameAs(e1, e2)
Comment:

29	"in information theory and minimum description length (mdl), learning a good model of the <e1>data</e1> is recast as using the model to losslessly transmit the <e2>data</e2> in as few bits as possible."
sameAs(e1, e2)
Comment:

30	"the overall codelength can be understood as a combination of quality-of-fit of the <e1>model</e1> (compressed data length), together with the cost of encoding (transmitting) the <e2>model</e2> itself."
sameAs(e1, e2)
Comment:

31	"standard sample complexity <e1>bounds</e1> (vc-dimension, pac-bayes...) are related to the compressed length of the data in a model, and any compression scheme leads to generalization <e2>bounds</e2> (blum and langford, 2003) ."
sameAs(e1, e2)
Comment:

32	"in unsupervised learning, <e1>autoencoders</e1> and especially variational <e2>autoencoders</e2> (kingma and welling, 2013) are compression methods of the data (ollivier, 2014) ."
sameAs(e1, e2)
Comment:

33	"in supervised learning, the <e1>information</e1> bottleneck method studies 32nd conference on neural <e2>information</e2> processing systems (nips 2018), montréal, canada."
sameAs(e1, e2)
Comment:

34	"how the hidden representations in a neural network compress the <e1>inputs</e1> while preserving the mutual information between <e2>inputs</e2> and outputs (tishby and zaslavsky, 2015; shwartz-ziv and tishby, 2017; achille and soatto, 2017) ."
sameAs(e1, e2)
Comment:

35	"1) immediately reveals <e1>that</e1> these models do not compress fake labels (and indeed, theoretically, they cannot, see appendix a), <e2>that</e2> no information is present in the model parameters, and that no learning has occurred."
sameAs(e1, e2)
Comment:

36	"1) immediately reveals <e1>that</e1> these models do not compress fake labels (and indeed, theoretically, they cannot, see appendix a), that no information is present in the model parameters, and <e2>that</e2> no learning has occurred."
sameAs(e1, e2)
Comment:

37	"1) immediately reveals that these models do not compress fake labels (and indeed, theoretically, they cannot, see appendix a), <e1>that</e1> no information is present in the model parameters, and <e2>that</e2> no learning has occurred."
sameAs(e1, e2)
Comment:

38	"unexpectedly, we found that these variational methods provide surprisingly poor compression <e1>bounds</e1>, despite being explicitly built to minimize such <e2>bounds</e2>."
sameAs(e1, e2)
Comment:

39	"first, <e1>we</e1> briefly introduce and discuss representative methods for conducting adversarial attacks and defenses in section 2. then <e2>we</e2> elaborate on the motivation and basic ideas of our method in section 3. section 4 provides implementation details of our method and experimentally compares it with the state-of-the-arts, and finally, section 5 draws the conclusions."
sameAs(e1, e2)
Comment:

40	"first, we briefly introduce and discuss representative methods for conducting adversarial attacks and defenses in section 2. then we elaborate on the motivation and basic ideas of <e1>our method</e1> in section 3. section 4 provides implementation details of <e2>our method</e2> and experimentally compares it with the state-of-the-arts, and finally, section 5 draws the conclusions."
sameAs(e1, e2)
Comment:

41	"it is important to develop <e1>models</e1> that are robust to adversarial perturbations for a variety of reasons: • so that machine learning can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so that machine learning is more useful for modelbased optimization, • to gain a better understanding of how to provide performance guarantees for <e2>models</e2> under distribution shift, • to gain a better understanding of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

42	"it is important to develop models <e1>that</e1> are robust to adversarial perturbations for a variety of reasons: • so <e2>that</e2> machine learning can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so that machine learning is more useful for modelbased optimization, • to gain a better understanding of how to provide performance guarantees for models under distribution shift, • to gain a better understanding of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

43	"it is important to develop models <e1>that</e1> are robust to adversarial perturbations for a variety of reasons: • so that machine learning can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so <e2>that</e2> machine learning is more useful for modelbased optimization, • to gain a better understanding of how to provide performance guarantees for models under distribution shift, • to gain a better understanding of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

44	"it is important to develop models that are robust to adversarial perturbations for a variety of reasons: • so <e1>that</e1> machine learning can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so <e2>that</e2> machine learning is more useful for modelbased optimization, • to gain a better understanding of how to provide performance guarantees for models under distribution shift, • to gain a better understanding of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

45	"it is important to develop models that are robust to adversarial perturbations for a variety of reasons: • so that <e1>machine learning</e1> can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so that <e2>machine learning</e2> is more useful for modelbased optimization, • to gain a better understanding of how to provide performance guarantees for models under distribution shift, • to gain a better understanding of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

46	"it is important to develop models that are robust to adversarial perturbations for a variety of reasons: • so that machine learning can be used in situations where an attacker may attempt to interfere with the operation of the deployed system, • so that machine learning is more useful for modelbased optimization, • to gain a better <e1>understanding</e1> of how to provide performance guarantees for models under distribution shift, • to gain a better <e2>understanding</e2> of how to enforce smoothness assumptions, etc."
sameAs(e1, e2)
Comment:

47	"• we show <e1>that</e1> clean logit pairing is a method with minimal computational cost <e2>that</e2> defends against pgd black box attacks almost as well as adversarial training for two datasets."
sameAs(e1, e2)
Comment:

48	"• we show <e1>that</e1> adversarial logit pairing is a method <e2>that</e2> leads to higher accuracy when subjected to white box and black box attacks."
sameAs(e1, e2)
Comment:

49	"next, <e1>we</e1> introduce enhanced defenses using a technique <e2>we</e2> call logit pairing, a method that encourages logits for pairs of examples to be similar."
sameAs(e1, e2)
Comment:

50	"when applied to clean <e1>examples</e1> and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean <e2>examples</e2> only is competitive with adversarial training in terms of accuracy on two datasets."
sameAs(e1, e2)
Comment:

51	"when applied to clean examples and their <e1>adversarial</e1> counterparts, logit pairing improves accuracy on adversarial examples over vanilla <e2>adversarial</e2> training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of accuracy on two datasets."
sameAs(e1, e2)
Comment:

52	"when applied to clean examples and their <e1>adversarial</e1> counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with <e2>adversarial</e2> training in terms of accuracy on two datasets."
sameAs(e1, e2)
Comment:

53	"when applied to clean examples and their adversarial counterparts, logit pairing improves <e1>accuracy</e1> on adversarial examples over vanilla adversarial training; we also find that logit pairing on clean examples only is competitive with adversarial training in terms of <e2>accuracy</e2> on two datasets."
sameAs(e1, e2)
Comment:

54	"when applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla <e1>adversarial</e1> training; we also find that logit pairing on clean examples only is competitive with <e2>adversarial</e2> training in terms of accuracy on two datasets."
sameAs(e1, e2)
Comment:

55	"when applied to clean examples and their adversarial counterparts, logit pairing improves accuracy on adversarial examples over vanilla adversarial <e1>training</e1>; we also find that logit pairing on clean examples only is competitive with adversarial <e2>training</e2> in terms of accuracy on two datasets."
sameAs(e1, e2)
Comment:

56	"in <e1>this</e1> paper, we argue that the current de facto way of evaluating ssl techniques does not address <e2>this</e2> question in a satisfying way."
sameAs(e1, e2)
Comment:

57	"in this paper, we argue that the current de facto <e1>way</e1> of evaluating ssl techniques does not address this question in a satisfying <e2>way</e2>."
sameAs(e1, e2)
Comment:

58	"• realistically small validation sets would preclude reliable comparison of different <e1>methods</e1>, <e2>models</e2>, and hyperparamarxiv:1804.09170v2 [cs."
Compare(e1, e2)
Comment:

59	"however, we argue <e1>that</e1> these benchmarks fail to address many issues <e2>that</e2> these algorithms would face in real-world applications."
sameAs(e1, e2)
Comment:

60	"however, we argue that <e1>these</e1> benchmarks fail to address many issues that <e2>these</e2> algorithms would face in real-world applications."
sameAs(e1, e2)
Comment:

61	"in section 3, <e1>we</e1> give an overview of modern ssl approaches for deep architectures, with a specific emphasis on those that <e2>we</e2> include in our study."
sameAs(e1, e2)
Comment:

62	"we find <e1>that</e1> the performance of simple baselines which do not use unlabeled data is often underreported, <e2>that</e2> ssl methods differ in sensitivity to the amount of labeled and unlabeled data, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples."
sameAs(e1, e2)
Comment:

63	"we find <e1>that</e1> the performance of simple baselines which do not use unlabeled data is often underreported, that ssl methods differ in sensitivity to the amount of labeled and unlabeled data, and <e2>that</e2> performance can degrade substantially when the unlabeled dataset contains out-of-class examples."
sameAs(e1, e2)
Comment:

64	"we find that the performance of simple baselines which do not use <e1>unlabeled data</e1> is often underreported, that ssl methods differ in sensitivity to the amount of labeled and <e2>unlabeled data</e2>, and that performance can degrade substantially when the unlabeled dataset contains out-of-class examples."
sameAs(e1, e2)
Comment:

65	"we find that the performance of simple baselines which do not use unlabeled data is often underreported, <e1>that</e1> ssl methods differ in sensitivity to the amount of labeled and unlabeled data, and <e2>that</e2> performance can degrade substantially when the unlabeled dataset contains out-of-class examples."
sameAs(e1, e2)
Comment:

66	"however, <e1>these</e1> successes come at a distinct cost; namely, creating <e2>these</e2> large datasets typically requires a great deal of human effort (to manually label examples), pain or risk (for medical datasets involving invasive tests) or financial expense (to hire labelers or build the infrastructure needed for domain-specific data collection)."
sameAs(e1, e2)
Comment:

67	"these powerful multivariate binary distributions can represent any distribution defined on a set of binary random variables [12] , and have seen application in <e1>unsupervised learning</e1> [13] , <e2>supervised learning</e2> [14, 15] , reinforcement learning [16] , dimensionality reduction [17] , and collaborative filtering [18] ."
Conjunction(e1, e2)
Comment:

68	"recently, boltzmann machines have been used as priors for variational <e1>autoencoders</e1> (vaes) in the discrete variational <e2>autoencoder</e2> (dvae) [19] and its successor dvae++ [20] ."
sameAs(e1, e2)
Comment:

69	"however, both these models assume a particular variational <e1>bound</e1> and tighter bounds such as the importance weighted (iw) <e2>bound</e2> [21] cannot be used for training."
sameAs(e1, e2)
Comment:

70	"we remove this constraint by introducing two <e1>continuous</e1> relaxations that convert a boltzmann machine to a distribution over <e2>continuous</e2> random variables."
sameAs(e1, e2)
Comment:

71	"this paper makes <e1>two</e1> contributions: i) we introduce <e2>two</e2> continuous relaxations of boltzmann machines and use these relaxations to train a discrete vae with a boltzmann prior using the iw bound."
sameAs(e1, e2)
Comment:

72	"this paper makes two contributions: i) we introduce two continuous <e1>relaxations</e1> of boltzmann machines and use these <e2>relaxations</e2> to train a discrete vae with a boltzmann prior using the iw bound."
sameAs(e1, e2)
Comment:

73	"ii) we generalize the overlapping transformations of [20] to any pair of distributions with computable probability density <e1>function</e1> (pdf) and cumulative density <e2>function</e2> (cdf)."
sameAs(e1, e2)
Comment:

74	"using these more general overlapping <e1>transformations</e1>, we propose new smoothing <e2>transformations</e2> using mixtures of gaussian and power-function [24] distributions."
sameAs(e1, e2)
Comment:

75	"experiments on the <e1>mnist</e1> and omniglot <e2>datasets</e2> show that these relaxations outperform previous discrete vaes with boltzmann priors."
isA(e1, e2)
Comment:

76	"these advances have been made using continuous <e1>latent variable models</e1> in spite of the computational efficiency and greater interpretability offered by discrete <e2>latent variables</e2>."
isA(e1, e2)
Comment:

77	"further, <e1>models</e1> such as clustering, semi-supervised learning, and variational memory addressing [11] all require discrete variables, which makes the training of discrete <e2>models</e2> an important challenge."
sameAs(e1, e2)
Comment:

78	"further, models such as clustering, semi-supervised learning, and variational memory addressing [11] all require <e1>discrete</e1> variables, which makes the training of <e2>discrete</e2> models an important challenge."
sameAs(e1, e2)
Comment:

79	"abstract this paper addresses the mode collapse for <e1>generative adversarial networks</e1> (<e2>gans</e2>)."
sameAs(e1, e2)
Comment:

80	"ii) we consider the geometric interpretation of modes, and argue that the modes of a <e1>data</e1> distribution should be viewed under a specific distance metric of <e2>data</e2> items -different metrics may lead to different distributions of modes, and a proper metric can result in interpretable modes."
sameAs(e1, e2)
Comment:

81	"ii) we consider the geometric interpretation of modes, and argue that the modes of a data distribution should be viewed under a specific distance <e1>metric</e1> of data items -different <e2>metrics</e2> may lead to different distributions of modes, and a proper metric can result in interpretable modes."
sameAs(e1, e2)
Comment:

82	"ii) we consider the geometric interpretation of modes, and argue that the modes of a data distribution should be viewed under a specific distance <e1>metric</e1> of data items -different metrics may lead to different distributions of modes, and a proper <e2>metric</e2> can result in interpretable modes."
sameAs(e1, e2)
Comment:

83	"ii) we consider the geometric interpretation of modes, and argue that the modes of a data distribution should be viewed under a specific distance metric of data items -different <e1>metrics</e1> may lead to different distributions of modes, and a proper <e2>metric</e2> can result in interpretable modes."
sameAs(e1, e2)
Comment:

84	"latent space eases the optimization of <e1>gans</e1>, and unlike existing <e2>gans</e2> that assume a user-specified dimensionality of the latent space, our method automatically decides the dimensionality of the latent space from the provided dataset."
sameAs(e1, e2)
Comment:

85	"to exploit the constructed gaussian mixture for addressing mode collapse, we propose a simple extension to the gan objective that encourages the pairwise 2 <e1>distance</e1> of latent-space random vectors to match the <e2>distance</e2> of the generated data samples in the metric space."
sameAs(e1, e2)
Comment:

86	"through a series of (nontrivial) theoretical analyses, we show that if bourgan is fully optimized, the logarithmic <e1>pairwise distance</e1> distribution of its generated samples closely match the logarithmic <e2>pairwise distance</e2> distribution of the real data items."
sameAs(e1, e2)
Comment:

87	"in particular, <e1>our method</e1> is robust to handle data distributions with multiple separated modes -challenging situations where all existing gans that we have experimented with produce unwanted samples (ones that are not in any modes), whereas <e2>our method</e2> is able to generate samples spreading over all modes while avoiding unwanted samples."
sameAs(e1, e2)
Comment:

88	"in particular, our method is robust to handle data distributions with multiple separated modes -challenging situations where all existing gans <e1>that</e1> we have experimented with produce unwanted samples (ones <e2>that</e2> are not in any modes), whereas our method is able to generate samples spreading over all modes while avoiding unwanted samples."
sameAs(e1, e2)
Comment:

89	"introduction in unsupervised learning, <e1>generative adversarial networks</e1> (<e2>gans</e2>) [1] is by far one of the most widely used methods for training deep generative models."
sameAs(e1, e2)
Comment:

90	"for this reason, visual attention based <e1>models</e1> have succeeded in multimodal learning <e2>tasks</e2>, identifying selective regions in a spatial map of an image defined by the model."
Used-for(e1, e2)
Comment:

91	"our <e1>model</e1> achieves a new state-of-the-art maintaining simplicity of <e2>model</e2> structure."
sameAs(e1, e2)
Comment:

92	"we quantitatively and qualitatively evaluate our <e1>model</e1> on visual question answering (vqa 2.0) and flickr30k entities datasets, showing that ban significantly outperforms previous <e2>methods</e2> and achieves new state-of-the-arts on both datasets."
Compare(e1, e2)
Comment:

93	"since <e1>vision</e1> and natural language are the major modalities of human interaction, understanding and reasoning of <e2>vision</e2> and natural language information become a key challenge."
sameAs(e1, e2)
Comment:

94	"since vision and <e1>natural language</e1> are the major modalities of human interaction, understanding and reasoning of vision and <e2>natural language</e2> information become a key challenge."
sameAs(e1, e2)
Comment:

95	"empirically, we find that adaptive parameterization can learn non-linear patterns where a non-adaptive <e1>baseline</e1> fails, or outperform the <e2>baseline</e2> using 30-50% fewer parameters."
sameAs(e1, e2)
Comment:

96	"introduction while a two-layer feed-forward neural network is sufficient to approximate any <e1>function</e1> (cybenko, 1989; hornik, 1991) , in practice much deeper networks are necessary to learn a good approximation to a complex <e2>function</e2>."
sameAs(e1, e2)
Comment:

97	"thus, classic tools such as multinomial logistic regression (mlr), feed forward (ffnn) or <e1>recurrent neural networks</e1> (<e2>rnn</e2>) did not have a correspondence in these geometries."
sameAs(e1, e2)
Comment:

98	"in <e1>this</e1> paper we address <e2>this</e2> question for one of the simplest, yet useful, non-euclidean domains: spaces of constant negative curvature, i.e."
sameAs(e1, e2)
Comment:

99	"the main contribution of this paper is to bridge the gap between hyperbolic and euclidean geometry in the context of neural networks and deep learning by generalizing in a principled manner both the basic operations as well as multinomial logistic regression (mlr), feed-forward (ffnn), simple and gated (gru) <e1>recurrent neural networks</e1> (<e2>rnn</e2>) to the poincaré model of the hyperbolic geometry."
sameAs(e1, e2)
Comment:

100	"here, we bridge this gap in a principled manner by combining the formalism of möbius gyrovector <e1>spaces</e1> with the riemannian geometry of the poincaré model of hyperbolic <e2>spaces</e2>."
sameAs(e1, e2)
Comment:

101	"despite such success, having redundant and highly correlated neurons (e.g., weights of kernels/filters in convolutional neural networks (cnns)) caused by over-parametrization presents an issue [35, 39] , which motivated a series of influential works in <e1>network</e1> compression [9, 1] and parameter-efficient <e2>network</e2> architectures [15, 17, 60] ."
sameAs(e1, e2)
Comment:

102	"these works either compress the <e1>network</e1> by pruning redundant neurons or directly modify the <e2>network</e2> architecture, aiming to achieve comparable performance while using fewer parameters."
sameAs(e1, e2)
Comment:

103	"in particular, there is a recent trend of studies <e1>that</e1> feature the significance of angular learning at both loss and convolution levels [27, 26, 28, 25] , based on the observation <e2>that</e2> the angles in deep embeddings learned by cnns tend to encode semantic difference."
sameAs(e1, e2)
Comment:

104	"given the above motivation, we draw inspiration from a well-known physics <e1>problem</e1>, called thomson <e2>problem</e2> [46, 41] ."
sameAs(e1, e2)
Comment:

105	"different from mhe in hidden layers, <e1>classifier</e1> neurons should be distributed in the full space for the best <e2>classification performance</e2> [27, 26] ."
Compare(e1, e2)
Comment:

106	"to this end, we draw inspiration from a well-known <e1>problem</e1> in physics -thomson <e2>problem</e2>, where one seeks to find a state that distributes n electrons on a unit sphere as even as possible with minimum potential energy."
sameAs(e1, e2)
Comment:

107	"however, <e1>we</e1> further consider minimizing hyperspherical energy defined with respect to angular distance, which <e2>we</e2> will refer to as angular-mhe (a-mhe) in the following paper."
sameAs(e1, e2)
Comment:

108	"in light of this intuition, we reduce the redundancy <e1>regularization</e1> problem to generic energy minimization, and propose a minimum hyperspherical energy (mhe) objective as generic <e2>regularization</e2> for neural networks."
sameAs(e1, e2)
Comment:

109	"in this paper, <e1>we</e1> are concerned with generalizing to populations different from the training distribution, in settings where <e2>we</e2> have no access to any data from the unknown target distributions."
sameAs(e1, e2)
Comment:

110	"although such algorithms can successfully learn models that perform well on known <e1>target</e1> distributions, the assumption of a priori fixed <e2>target</e2> distributions can be restrictive in practical scenarios."
sameAs(e1, e2)
Comment:

111	"for example, consider a semantic segmentation algorithm used by a <e1>robot</e1>: every task, <e2>robot</e2>, environment and camera configuration will result in a different target distribution, and these diverse scenarios can be identified only after the model is trained and deployed, making it difficult to collect samples from them."
sameAs(e1, e2)
Comment:

112	"for example, consider a semantic segmentation algorithm used by a robot: every <e1>task</e1>, robot, environment and camera configuration will result in a different target distribution, and these diverse scenarios can be identified only after the <e2>model</e2> is trained and deployed, making it difficult to collect samples from them."
Evaluate-for(e1, e2)
Comment:

113	"inspired by recent developments in distributionally robust optimization and adversarial <e1>training</e1> [35, 21, 13] , we consider the following worst-case problem around the (<e2>training</e2>) source distribution p 0 minimize θ∈θ sup p :d(p,p0)≤ρ e p [ (θ; (x, y ))]."
sameAs(e1, e2)
Comment:

114	"the <e1>solution</e1> to worst-case <e2>problem</e2> (1) guarantees good performance against data distributions that are distance ρ away from the source domain p 0 ."
Used-for(e1, e2)
Comment:

115	"to allow data distributions <e1>that</e1> have different support to <e2>that</e2> of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so that target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts that preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

116	"to allow data distributions <e1>that</e1> have different support to that of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so <e2>that</e2> target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts that preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

117	"to allow data distributions <e1>that</e1> have different support to that of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so that target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts <e2>that</e2> preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

118	"to allow data distributions that have different support to <e1>that</e1> of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so <e2>that</e2> target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts that preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

119	"to allow data distributions that have different support to <e1>that</e1> of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so that target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts <e2>that</e2> preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

120	"to allow data distributions that have different support to that of the source p 0 , we use wasserstein <e1>distances</e1> as our metric d. our <e2>distance</e2> will be defined on the semantic space 3 , so that target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts that preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

121	"to allow data distributions that have different support to that of the source p 0 , we use wasserstein distances as our metric d. our distance will be defined on the semantic space 3 , so <e1>that</e1> target populations p satisfying d(p, p 0 ) ≤ ρ will be realistic covariate shifts <e2>that</e2> preserve the same semantic representation of the source (e.g., adding color to a greyscale image)."
sameAs(e1, e2)
Comment:

122	"in this regard, <e1>we</e1> expect the solution to the worst-case problem (1)-the model that <e2>we</e2> wish to learn-to have favorable performance across covariate shifts in the semantic space."
sameAs(e1, e2)
Comment:

123	"in this regard, we expect the <e1>solution</e1> to the worst-case <e2>problem</e2> (1)-the model that we wish to learn-to have favorable performance across covariate shifts in the semantic space."
Used-for(e1, e2)
Comment:

124	"each iteration of <e1>our method</e1> uses small values of ρ, and we provide a number of theoretical interpretations of <e2>our method</e2>."
sameAs(e1, e2)
Comment:

125	"first, <e1>we</e1> show that our iterative algorithm is an adaptive data augmentation method where <e2>we</e2> add adversarially perturbed samples-at the current model-to the dataset (section 3)."
sameAs(e1, e2)
Comment:

126	"first, we show that our iterative algorithm is an adaptive data augmentation <e1>method</e1> where we add adversarially perturbed samples-at the current <e2>model</e2>-to the dataset (section 3)."
Used-for(e1, e2)
Comment:

127	"further, <e1>we</e1> show that for softmax losses, each iteration of our method can be thought of as a data-dependent regularization scheme where <e2>we</e2> regularize towards the parameter vector corresponding to the true label, instead of regularizing towards zero like classical regularizers such as ridge or lasso."
sameAs(e1, e2)
Comment:

128	"we propose to <e1>learn</e1> an ensemble of models that correspond to different distances ρ. in other words, our iterative method generates a collection of datasets, each corresponding to a different inter-dataset distance level ρ, and we <e2>learn</e2> a model for each of them."
sameAs(e1, e2)
Comment:

129	"we propose to learn an ensemble of models that correspond to different <e1>distances</e1> ρ. in other words, our iterative method generates a collection of datasets, each corresponding to a different inter-dataset <e2>distance</e2> level ρ, and we learn a model for each of them."
sameAs(e1, e2)
Comment:

130	"we propose to learn an ensemble of models that correspond to different distances ρ. in other words, our iterative <e1>method</e1> generates a collection of datasets, each corresponding to a different inter-dataset distance level ρ, and we learn a <e2>model</e2> for each of them."
Used-for(e1, e2)
Comment:

131	"at test time, we use a heuristic <e1>method</e1> to choose an appropriate <e2>model</e2> from the ensemble."
Used-for(e1, e2)
Comment:

132	"only using training data from the source domain, we propose an iterative procedure <e1>that</e1> augments the dataset with examples from a fictitious target domain <e2>that</e2> is "hard" under the current model."
sameAs(e1, e2)
Comment:

133	"in both settings, we observe <e1>that</e1> our method allows to learn models <e2>that</e2> improve performance across a priori unknown target distributions that have varying distance from the original source domain."
sameAs(e1, e2)
Comment:

134	"in both settings, we observe <e1>that</e1> our method allows to learn models that improve performance across a priori unknown target distributions <e2>that</e2> have varying distance from the original source domain."
sameAs(e1, e2)
Comment:

135	"in both settings, we observe that our method allows to learn models <e1>that</e1> improve performance across a priori unknown target distributions <e2>that</e2> have varying distance from the original source domain."
sameAs(e1, e2)
Comment:

136	"for softmax losses, we show <e1>that</e1> our method is a data-dependent regularization scheme <e2>that</e2> behaves differently from classical regularizers (e.g., ridge or lasso) that regularize towards zero."
sameAs(e1, e2)
Comment:

137	"for softmax losses, we show <e1>that</e1> our method is a data-dependent regularization scheme that behaves differently from classical regularizers (e.g., ridge or lasso) <e2>that</e2> regularize towards zero."
sameAs(e1, e2)
Comment:

138	"for softmax losses, we show that our method is a data-dependent regularization scheme <e1>that</e1> behaves differently from classical regularizers (e.g., ridge or lasso) <e2>that</e2> regularize towards zero."
sameAs(e1, e2)
Comment:

139	"on digit recognition and semantic segmentation tasks, we empirically observe <e1>that</e1> our method learns models <e2>that</e2> improve performance across a priori unknown data distributions."
sameAs(e1, e2)
Comment:

140	"while performance evaluated on the validation dataset-usually from the same population as the <e1>training</e1> dataset-is a standard metric on which many systems are optimized, it has been observed that performance on populations different from that of the <e2>training</e2> data can be much worse [16, 3, 1, 33, 38] ."
sameAs(e1, e2)
Comment:

141	"while performance evaluated on the validation dataset-usually from the same population as the training dataset-is a standard metric on which many systems are optimized, it has been observed <e1>that</e1> performance on populations different from <e2>that</e2> of the training data can be much worse [16, 3, 1, 33, 38] ."
sameAs(e1, e2)
Comment:

142	"given the abundance of training data, this would also seem a natural problem for <e1>unsupervised learning methods</e1> and indeed many papers apply <e2>unsupervised learning</e2> to small patches of images [41, 4, 31] ."
isA(e1, e2)
Comment:

143	"recent advances in deep learning, have also enabled unsupervised learning of full sized images using various models: variational auto encoders [21, 17] , pixelcnn [32, 39, 23, 38] , normalizing <e1>flow</e1> [9, 8] and <e2>flow</e2> gan [14] ."
sameAs(e1, e2)
Comment:

144	"perhaps the most dramatic success in modeling full images has been achieved by <e1>generative adversarial networks</e1> (<e2>gans</e2>) [13] , which can learn to generate remarkably realistic samples at high resolution [34, 25] , (fig."
sameAs(e1, e2)
Comment:

145	"a recurring criticism of <e1>gans</e1>, at the same time, is that while they are excellent at generating pretty pictures, they often fail to model the entire data distribution, a phenomenon usually referred to as mode collapse: "because of the mode collapse problem, applications of <e2>gans</e2> are often limited to problems where it is acceptable for the model to produce a small number of distinct outputs" [12] ."
sameAs(e1, e2)
Comment:

146	"a recurring criticism of gans, at the same time, is that while <e1>they</e1> are excellent at generating pretty pictures, <e2>they</e2> often fail to model the entire data distribution, a phenomenon usually referred to as mode collapse: "because of the mode collapse problem, applications of gans are often limited to problems where it is acceptable for the model to produce a small number of distinct outputs" [12] ."
sameAs(e1, e2)
Comment:

147	"a recurring criticism of gans, at the same time, is that while they are excellent at generating pretty pictures, they often fail to <e1>model</e1> the entire data distribution, a phenomenon usually referred to as mode collapse: "because of the mode collapse problem, applications of gans are often limited to problems where it is acceptable for the <e2>model</e2> to produce a small number of distinct outputs" [12] ."
sameAs(e1, e2)
Comment:

148	"in recent years, gans have gained much attention as a possible <e1>solution</e1> to the <e2>problem</e2>, and in particular have shown the ability to generate remarkably realistic high resolution sampled images."
Used-for(e1, e2)
Comment:

149	"two evaluation methods that are widely accepted [27, 1] are <e1>inception</e1> score (is) [34] and fréchet <e2>inception</e2> distance (fid) [16] ."
sameAs(e1, e2)
Comment:

150	"there are two significant drawbacks to this approach: the deep representation is insensitive to <e1>image</e1> properties and artifacts that the underlying <e2>classification</e2> network is trained to be invariant to [27, 18] and when the evaluated domain (e.g."
Used-for(e1, e2)
Comment:

151	"at the same time, many authors have pointed out <e1>that</e1> gans may fail to model the full distribution ("mode collapse") and <e2>that</e2> using the learned models for anything other than generating samples may be very difficult."
sameAs(e1, e2)
Comment:

152	"overall, the current situation is <e1>that</e1> while many authors criticize gans for "mode collapse" and decry the lack of an objective evaluation measure, the focus of much of the current research is on improved learning procedures for gans <e2>that</e2> will enable generating high quality images of increasing resolution, and papers often include sentences of the type "we feel the quality of the generated images is at least comparable to the best published results so far.""
sameAs(e1, e2)
Comment:

153	"overall, the current situation is that while many authors criticize <e1>gans</e1> for "mode collapse" and decry the lack of an objective evaluation measure, the focus of much of the current research is on improved learning procedures for <e2>gans</e2> that will enable generating high quality images of increasing resolution, and papers often include sentences of the type "we feel the quality of the generated images is at least comparable to the best published results so far.""
sameAs(e1, e2)
Comment:

154	"overall, the current situation is that while many authors criticize gans for "mode collapse" and decry the lack of an objective evaluation measure, the focus of much of the current research is on improved learning procedures for gans that will enable generating high <e1>quality</e1> images of increasing resolution, and papers often include sentences of the type "we feel the <e2>quality</e2> of the generated images is at least comparable to the best published results so far.""
sameAs(e1, e2)
Comment:

155	"overall, the current situation is that while many authors criticize gans for "mode collapse" and decry the lack of an objective evaluation measure, the focus of much of the current research is on improved learning procedures for gans that will enable generating high quality <e1>images</e1> of increasing resolution, and papers often include sentences of the type "we feel the quality of the generated <e2>images</e2> is at least comparable to the best published results so far.""
sameAs(e1, e2)
Comment:

156	"in <e1>this</e1> paper, we try to address <e2>this</e2> question more directly by comparing gans to perhaps the simplest statistical model, the gaussian mixture model."
sameAs(e1, e2)
Comment:

157	"unlike previous automatic <e1>methods</e1> for evaluating <e2>models</e2>, our method does not rely on an additional neural network nor does it require approximating intractable computations."
Compare(e1, e2)
Comment:

158	"while gmms have previously been shown to be successful in modeling small patches of <e1>images</e1>, we show how to train them on full sized <e2>images</e2> despite the high dimensionality."
sameAs(e1, e2)
Comment:

159	"our results show that gmms can generate realistic samples (although less sharp than those of <e1>gans</e1>) but also capture the full distribution which <e2>gans</e2> fail to do."
sameAs(e1, e2)
Comment:

160	"unlike previous automatic <e1>methods</e1> for evaluating <e2>models</e2>, our method does not rely on an additional neural network nor does it require approximating intractable computations."
Compare(e1, e2)
Comment:

161	"while gmms have previously been shown to be successful in modeling small patches of <e1>images</e1>, we show how to train them on full sized <e2>images</e2> despite the high dimensionality."
sameAs(e1, e2)
Comment:

162	"our results show that gmms can generate realistic samples (although less sharp than those of <e1>gans</e1>) but also capture the full distribution, which <e2>gans</e2> fail to do."
sameAs(e1, e2)
Comment:

163	"some of the proposed measures, such as <e1>inception</e1> score (is) [19] and fréchet <e2>inception</e2> distance (fid) [9] , have shown promising results in practice."
sameAs(e1, e2)
Comment:

164	"however, all of the metrics commonly applied to evaluating generative models share a crucial weakness: since <e1>they</e1> yield a one-dimensional score, <e2>they</e2> are unable to distinguish between different failure cases."
sameAs(e1, e2)
Comment:

165	"for example, the generative models shown in figure 1 obtain similar fids but exhibit different sample characteristics: the <e1>model</e1> on the left trained on mnist produces realistic samples, for example, the <e2>model</e2> on the left produces reasonably looking faces on celeba, but too many dark images."
sameAs(e1, e2)
Comment:

166	"by the proposed metric (middle), the <e1>models</e1> on the left achieve higher precision and lower recall than the <e2>models</e2> on the right, which suffices to successfully distinguishing between the failure cases."
sameAs(e1, e2)
Comment:

167	"by the proposed metric (middle), the models on the left achieve higher <e1>precision</e1> and lower <e2>recall</e2> than the models on the right, which suffices to successfully distinguishing between the failure cases."
Conjunction(e1, e2)
Comment:

168	"motivated by this shortcoming, we present a novel approach which disentangles the divergence between distributions into two components: <e1>precision</e1> and <e2>recall</e2>."
Conjunction(e1, e2)
Comment:

169	"given a reference distribution p and a learned distribution q, <e1>precision</e1> intuitively measures the quality of the samples in q, while <e2>recall</e2> measures the proportion of p that is covered by q. furthermore, we propose an elegant algorithm which can compute these quantities based on samples from p and q. in particular, using this approach we are able to quantify the degree of mode dropping and mode inventing based on samples from the true and the learned distribution."
Conjunction(e1, e2)
Comment:

170	"given a reference distribution p and a learned distribution q, <e1>precision</e1> intuitively measures the quality of the samples in q, while recall measures the proportion of p that is covered by q. furthermore, we propose an elegant algorithm which can compute these quantities based on samples from p and q. in particular, using this <e2>approach</e2> we are able to quantify the degree of mode dropping and mode inventing based on samples from the true and the learned distribution."
Evaluate-for(e1, e2)
Comment:

171	"given a reference distribution p and a learned distribution q, precision intuitively <e1>measures</e1> the quality of the samples in q, while recall <e2>measures</e2> the proportion of p that is covered by q. furthermore, we propose an elegant algorithm which can compute these quantities based on samples from p and q. in particular, using this approach we are able to quantify the degree of mode dropping and mode inventing based on samples from the true and the learned distribution."
sameAs(e1, e2)
Comment:

172	"given a reference distribution p and a learned distribution q, precision intuitively measures the quality of the samples in q, while <e1>recall</e1> measures the proportion of p that is covered by q. furthermore, we propose an elegant algorithm which can compute these quantities based on samples from p and q. in particular, using this <e2>approach</e2> we are able to quantify the degree of mode dropping and mode inventing based on samples from the true and the learned distribution."
Evaluate-for(e1, e2)
Comment:

173	"given a reference distribution p and a learned distribution q, precision intuitively measures the quality of the samples in q, while recall measures the proportion of p that is covered by q. furthermore, <e1>we</e1> propose an elegant algorithm which can compute these quantities based on samples from p and q. in particular, using this approach <e2>we</e2> are able to quantify the degree of mode dropping and mode inventing based on samples from the true and the learned distribution."
sameAs(e1, e2)
Comment:

174	"our contributions: (1) <e1>we</e1> introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) <e2>we</e2> propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

175	"our contributions: (1) <e1>we</e1> introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) <e2>we</e2> relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

176	"our contributions: (1) <e1>we</e1> introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) <e2>we</e2> demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

177	"our contributions: (1) <e1>we</e1> introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) <e2>we</e2> compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

178	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and <e2>recall</e2> for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
Conjunction(e1, e2)
Comment:

179	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed <e2>approach</e2> -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
Evaluate-for(e1, e2)
Comment:

180	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high <e2>precision</e2>, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

181	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low <e2>recall</e2>), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
Conjunction(e1, e2)
Comment:

182	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low <e2>precision</e2>, high recall)."
sameAs(e1, e2)
Comment:

183	"our contributions: (1) we introduce a novel definition of <e1>precision</e1> and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high <e2>recall</e2>)."
Conjunction(e1, e2)
Comment:

184	"our contributions: (1) we introduce a novel definition of precision and <e1>recall</e1> for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed <e2>approach</e2> -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
Evaluate-for(e1, e2)
Comment:

185	"our contributions: (1) we introduce a novel definition of precision and <e1>recall</e1> for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low <e2>recall</e2>), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

186	"our contributions: (1) we introduce a novel definition of precision and <e1>recall</e1> for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high <e2>recall</e2>)."
sameAs(e1, e2)
Comment:

187	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove <e1>that</e1> the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate <e2>that</e2> in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

188	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove <e1>that</e1> the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric <e2>that</e2> experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

189	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove <e1>that</e1> the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore <e2>that</e2> gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

190	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) <e1>we</e1> propose an efficient algorithm to compute these quantities, (3) <e2>we</e2> relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

191	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) <e1>we</e1> propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) <e2>we</e2> demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

192	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) <e1>we</e1> propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) <e2>we</e2> compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

193	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute <e1>these</e1> quantities, (3) we relate <e2>these</e2> notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

194	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) <e1>we</e1> relate these notions to total variation, is and fid, (4) <e2>we</e2> demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

195	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) <e1>we</e1> relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) <e2>we</e2> compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

196	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) <e1>we</e1> demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) <e2>we</e2> compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

197	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate <e1>that</e1> in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric <e2>that</e2> experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

198	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate <e1>that</e1> in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore <e2>that</e2> gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

199	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric <e1>that</e1> experimentally confirms the folklore <e2>that</e2> gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

200	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" <e1>images</e1>, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" <e2>images</e2>, but cover the whole distribution (low precision, high recall)."
sameAs(e1, e2)
Comment:

201	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high <e1>precision</e1>, low <e2>recall</e2>), while vaes produce "blurry" images, but cover the whole distribution (low precision, high recall)."
Conjunction(e1, e2)
Comment:

202	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high <e1>precision</e1>, low recall), while vaes produce "blurry" images, but cover the whole distribution (low <e2>precision</e2>, high recall)."
sameAs(e1, e2)
Comment:

203	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high <e1>precision</e1>, low recall), while vaes produce "blurry" images, but cover the whole distribution (low precision, high <e2>recall</e2>)."
Conjunction(e1, e2)
Comment:

204	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low <e1>recall</e1>), while vaes produce "blurry" images, but cover the whole distribution (low precision, high <e2>recall</e2>)."
sameAs(e1, e2)
Comment:

205	"our contributions: (1) we introduce a novel definition of precision and recall for distributions and prove that the notion is theoretically sound and has desirable properties, (2) we propose an efficient algorithm to compute these quantities, (3) we relate these notions to total variation, is and fid, (4) we demonstrate that in practice one can quantify the degree of mode dropping and mode inventing on real world data sets (image and text data), and (5) we compare several types of generative models based on the proposed approach -to our knowledge, this is the first metric that experimentally confirms the folklore that gans often produce "sharper" images, but can suffer from mode collapse (high precision, low recall), while vaes produce "blurry" images, but cover the whole distribution (low <e1>precision</e1>, high <e2>recall</e2>)."
Conjunction(e1, e2)
Comment:

206	"we propose a novel definition of <e1>precision</e1> and <e2>recall</e2> for distributions which disentangles the divergence into two separate dimensions."
Conjunction(e1, e2)
Comment:

207	"we then apply the rmc to a suite of <e1>tasks</e1> that may profit from more explicit memory-memory interactions, and hence, a potentially increased capacity for relational reasoning across time: partially observed reinforcement learning <e2>tasks</e2>, program evaluation, and language modeling on the wikitext-103, project gutenberg, and gigaword datasets."
sameAs(e1, e2)
Comment:

208	"it is unclear, however, whether <e1>they</e1> also have an ability to perform complex relational reasoning with the information <e2>they</e2> remember."
sameAs(e1, e2)
Comment:

209	"here, we first confirm our intuitions <e1>that</e1> standard memory architectures may struggle at tasks <e2>that</e2> heavily involve an understanding of the ways in which entities are connected -i.e., tasks involving relational reasoning."
sameAs(e1, e2)
Comment:

210	"here, we first confirm our intuitions that standard memory architectures may struggle at <e1>tasks</e1> that heavily involve an understanding of the ways in which entities are connected -i.e., <e2>tasks</e2> involving relational reasoning."
sameAs(e1, e2)
Comment:

211	"we then improve upon these deficits by using a new <e1>memory</e1> module -a relational <e2>memory</e2> core (rmc) -which employs multi-head dot product attention to allow memories to interact."
sameAs(e1, e2)
Comment:

212	"in neural network research many successful <e1>approaches</e1> to modeling sequential data also use memory <e2>systems</e2>, such as lstms [3] and memory-augmented neural networks generally [4] [5] [6] [7] ."
Evaluate-for(e1, e2)
Comment:

213	"in neural network research many successful approaches to modeling sequential data also use <e1>memory</e1> systems, such as lstms [3] and <e2>memory</e2>-augmented neural networks generally [4] [5] [6] [7] ."
sameAs(e1, e2)
Comment:

214	"bolstered by augmented memory capacities, bounded computational costs over <e1>time</e1>, and an ability to deal with vanishing gradients, these networks learn to correlate events across <e2>time</e2> to be proficient at storing and retrieving information."
sameAs(e1, e2)
Comment:

215	"unsupervised attention-guided <e1>image</e1>-to-<e2>image</e2> translation"
sameAs(e1, e2)
Comment:

216	"abstract current unsupervised <e1>image</e1>-to-<e2>image</e2> translation techniques struggle to focus their attention on individual objects without altering the background or the way multiple objects interact within a scene."
sameAs(e1, e2)
Comment:

217	"abstract current unsupervised image-to-image translation techniques struggle to focus their attention on individual <e1>objects</e1> without altering the background or the way multiple <e2>objects</e2> interact within a scene."
sameAs(e1, e2)
Comment:

218	"we demonstrate qualitatively and quantitatively that our <e1>approach</e1> attends to relevant regions in the image without requiring supervision, which creates more realistic mappings when compared to those of recent <e2>approaches</e2>."
Compare(e1, e2)
Comment:

219	"we demonstrate qualitatively and quantitatively that our approach attends to relevant regions in the image without requiring <e1>supervision</e1>, which creates more realistic mappings when compared to those of recent <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

220	"figure 1: by explicitly modeling attention, our algorithm is able to better alter the object of interest in unsupervised <e1>image</e1>-to-<e2>image</e2> translation tasks, without changing the background at the same time."
sameAs(e1, e2)
Comment:

221	"figure 1 : by explicitly modeling attention, our algorithm is able to better alter the object of interest in unsupervised <e1>image</e1>-to-<e2>image</e2> translation tasks, without changing the background at the same time."
sameAs(e1, e2)
Comment:

222	"in this paper we treat the vision-and-language navigation task as a trajectory search problem, where the agent needs to find (based on the instruction) the best trajectory in the environment to navigate from the start <e1>location</e1> to the goal <e2>location</e2>."
sameAs(e1, e2)
Comment:

223	"our model involves an instruction interpretation (follower) <e1>module</e1>, mapping instructions to action sequences; and an instruction generation (speaker) <e2>module</e2>, mapping action sequences to instructions (figure 1 ), both implemented with standard sequence-tosequence architectures."
sameAs(e1, e2)
Comment:

224	"our model involves an instruction interpretation (follower) module, mapping instructions to <e1>action</e1> sequences; and an instruction generation (speaker) module, mapping <e2>action</e2> sequences to instructions (figure 1 ), both implemented with standard sequence-tosequence architectures."
sameAs(e1, e2)
Comment:

225	"our approach consists of an instruction follower <e1>model</e1> (left) and a speaker <e2>model</e2> (right)."
sameAs(e1, e2)
Comment:

226	"though explicit probabilistic reasoning combining speaker and follower agents is a staple of the literature on computational pragmatics [14] , application of <e1>these</e1> models has largely been limited to extremely simple decision-making <e2>tasks</e2> like single forced choices."
Used-for(e1, e2)
Comment:

227	"though explicit probabilistic reasoning combining speaker and follower agents is a staple of the literature on computational pragmatics [14] , application of these <e1>models</e1> has largely been limited to extremely simple decision-making <e2>tasks</e2> like single forced choices."
Used-for(e1, e2)
Comment:

228	"we incorporate the speaker both at <e1>training time</e1> and at test <e2>time</e2>, where it works together with the learned instruction follower model to solve the navigation task (see figure 2 for an overview of our approach)."
Used-for(e1, e2)
Comment:

229	"this procedure, using the external speaker <e1>model</e1>, improves upon planning using only the follower <e2>model</e2>."
sameAs(e1, e2)
Comment:

230	"we evaluate this speaker-follower <e1>model</e1> on the roomto-room (r2r) dataset [1] , and show that each component in our <e2>model</e2> improves performance at the instruction following task."
sameAs(e1, e2)
Comment:

231	"in machine learning settings, <e1>this</e1> is doubly challenging: it is difficult to collect enough annotated data to enable learning of <e2>this</e2> reasoning process from scratch, and also difficult to implement the reasoning process using generic sequence models."
sameAs(e1, e2)
Comment:

232	"experiments show that all three components of this <e1>approach</e1>-speaker-driven data augmentation, pragmatic reasoning and panoramic action space-dramatically improve the performance of a baseline instruction follower, more than doubling the success rate over the best existing <e2>approach</e2> on a standard benchmark."
sameAs(e1, e2)
Comment:

233	"the agent must follow this instruction to navigate from its starting <e1>location</e1> to a goal <e2>location</e2>, as shown in figure 1 (left)."
sameAs(e1, e2)
Comment:

234	"as the number of parameters in state-of-the-art models <e1>scales</e1> to hundreds of millions [23, 24] , the size of gradients <e2>scales</e2> proportionally."
sameAs(e1, e2)
Comment:

235	"in this work, we show <e1>that</e1> stochastic gradient sparsification and quantization are facets of a general approach <e2>that</e2> sparsifies a gradient on any of its possible atomic decompositions, e.g., its entry-wise or singular value decomposition, its fourier representation, etc."
sameAs(e1, e2)
Comment:

236	"in this work, we show that stochastic <e1>gradient</e1> sparsification and quantization are facets of a general approach that sparsifies a <e2>gradient</e2> on any of its possible atomic decompositions, e.g., its entry-wise or singular value decomposition, its fourier representation, etc."
sameAs(e1, e2)
Comment:

237	"atomo sets up and optimally solves a meta-optimization <e1>that</e1> minimizes the variance of the sparsified gradient, subject to the constraints <e2>that</e2> it is sparse on the atomic basis, and also is an unbiased estimator of the input."
sameAs(e1, e2)
Comment:

238	"then, we argue <e1>that</e1> for some neural network applications, viewing the gradient as a concatenation of matrices (each corresponding to a layer), and applying atomic sparsification to their svd is meaningful and well-motivated by the fact <e2>that</e2> these matrices are "nearly" low-rank, e.g., see fig."
sameAs(e1, e2)
Comment:

239	"then, we argue that for some neural network applications, viewing the gradient as a concatenation of <e1>matrices</e1> (each corresponding to a layer), and applying atomic sparsification to their svd is meaningful and well-motivated by the fact that these <e2>matrices</e2> are "nearly" low-rank, e.g., see fig."
sameAs(e1, e2)
Comment:

240	"we show <e1>that</e1> atomo on the svd of each layer's gradient, can lead to less variance, and faster training, for the same communication budget as <e2>that</e2> of qsgd or terngrad."
sameAs(e1, e2)
Comment:

241	"we present extensive experiments showing that using atomo with svd sparsification, can lead to up to 2× faster <e1>training time</e1> (including the <e2>time</e2> to compute the svd) compared to qsgd, on vgg and resnet-18, and svhn and cifar-10."
Used-for(e1, e2)
Comment:

242	"we argue <e1>that</e1> these are facets of a general sparsification method <e2>that</e2> can operate on any possible atomic decomposition."
sameAs(e1, e2)
Comment:

243	"these works both note, as we do, <e1>that</e1> variance (or equivalently the mean squared error) controls important quantities such as convergence, and they seek to find a low-communication vector averaging scheme <e2>that</e2> minimizes it."
sameAs(e1, e2)
Comment:

244	"first, [50] consider the <e1>problem</e1> of minimizing the sparsity of a gradient for a fixed variance, while we consider the reverse <e2>problem</e2>, that is, minimizing the variance subject to a sparsity budget."
sameAs(e1, e2)
Comment:

245	"first, [50] consider the problem of minimizing the <e1>sparsity</e1> of a gradient for a fixed variance, while we consider the reverse problem, that is, minimizing the variance subject to a <e2>sparsity</e2> budget."
sameAs(e1, e2)
Comment:

246	"first, [50] consider the problem of minimizing the sparsity of a gradient for a fixed <e1>variance</e1>, while we consider the reverse problem, that is, minimizing the <e2>variance</e2> subject to a sparsity budget."
sameAs(e1, e2)
Comment:

247	"the second more important difference is that while [50] focuses on entry-wise sparsification, <e1>we</e1> consider a general problem where <e2>we</e2> sparsify according to any atomic decomposition."
sameAs(e1, e2)
Comment:

248	"finally, low-<e1>rank</e1> factorizations and sketches of the gradients when viewed as matrices were proposed in [53, 44, 25, 52, 29] ; arguably most of these methods (with the exception of [29] ) aimed to address the high flops required during inference by using low-<e2>rank</e2> models."
sameAs(e1, e2)
Comment:

249	"finally, low-rank factorizations and sketches of the gradients when viewed as matrices were proposed in [53, 44, 25, 52, 29] ; arguably most of these <e1>methods</e1> (with the exception of [29] ) aimed to address the high flops required during inference by using low-rank <e2>models</e2>."
Compare(e1, e2)
Comment:

250	"we show <e1>that</e1> methods such as qsgd and terngrad are special cases of atomo and show <e2>that</e2> sparsifiying gradients in their singular value decomposition (svd), rather than the coordinate-wise one, can lead to significantly faster distributed training."
sameAs(e1, e2)
Comment:

251	"introduction distributed computing <e1>systems</e1> have become vital to the success of modern machine learning <e2>systems</e2>."
sameAs(e1, e2)
Comment:

252	"work in parallel and distributed optimization has shown that these systems can obtain massive speed up gains in both <e1>convex</e1> and non-<e2>convex</e2> settings [9, 18, 19, 26, 34, 41] ."
sameAs(e1, e2)
Comment:

253	"variational neural machine translation (vnmt)  attempts to achieve this by augmenting a <e1>baseline</e1> model with a latent variable intended to represent the underlying semantics of the source sentence, achieving higher bleu scores than the <e2>baseline</e2>."
sameAs(e1, e2)
Comment:

254	"unlike the majority of neural machine translation models (which model the conditional distribution of the <e1>target</e1> sentence given the source sentence), gnmt models the joint distribution of the <e2>target</e2> sentence and the source sentence."
sameAs(e1, e2)
Comment:

255	"when there are missing <e1>words</e1> in the source sentence, gnmt is able to use its learned representation to infer what those <e2>words</e2> may be and produce good translations accordingly."
sameAs(e1, e2)
Comment:

256	"we also show that by setting the source and target languages to the same value, monolingual <e1>data</e1> can be leveraged to further reduce the impact of overfitting in the limited paired <e2>data</e2> context, and to provide significant improvements for translating between previously unseen language pairs."
sameAs(e1, e2)
Comment:

257	"an <e1>image</e1> and a caption) can be used to learn representations with more understanding about their environment compared to when only a single modality (an <e2>image</e2> or a caption alone) is available."
sameAs(e1, e2)
Comment:

258	"such representations can then be included as components in larger <e1>models</e1> which may be responsible for several <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

259	"however, it can often be expensive to acquire multi-modal <e1>data</e1> even when large amounts of unsupervised <e2>data</e2> may be available."
sameAs(e1, e2)
Comment:

260	"abstract given a single image x from <e1>domain</e1> a and a set of images from <e2>domain</e2> b, our task is to generate the analogous of x in b. we argue that this task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task."
sameAs(e1, e2)
Comment:

261	"abstract given a single image x from <e1>domain</e1> a and a set of images from domain b, our task is to generate the analogous of x in b. we argue that this task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised <e2>domain</e2> translation methods fail on this task."
sameAs(e1, e2)
Comment:

262	"abstract given a single image x from domain a and a set of images from <e1>domain</e1> b, our task is to generate the analogous of x in b. we argue that this task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised <e2>domain</e2> translation methods fail on this task."
sameAs(e1, e2)
Comment:

263	"abstract given a single image x from domain a and a set of images from domain b, our <e1>task</e1> is to generate the analogous of x in b. we argue that this <e2>task</e2> could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task."
sameAs(e1, e2)
Comment:

264	"abstract given a single image x from domain a and a set of images from domain b, our <e1>task</e1> is to generate the analogous of x in b. we argue that this task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this <e2>task</e2>."
sameAs(e1, e2)
Comment:

265	"abstract given a single image x from domain a and a set of images from domain b, our task is to generate the analogous of x in b. we argue <e1>that</e1> this task could be a key ai capability <e2>that</e2> underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task."
sameAs(e1, e2)
Comment:

266	"abstract given a single image x from domain a and a set of images from domain b, our task is to generate the analogous of x in b. we argue <e1>that</e1> this task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence <e2>that</e2> the existing unsupervised domain translation methods fail on this task."
sameAs(e1, e2)
Comment:

267	"abstract given a single image x from domain a and a set of images from domain b, our task is to generate the analogous of x in b. we argue that <e1>this</e1> task could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on <e2>this</e2> task."
sameAs(e1, e2)
Comment:

268	"abstract given a single image x from domain a and a set of images from domain b, our task is to generate the analogous of x in b. we argue that this <e1>task</e1> could be a key ai capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this <e2>task</e2>."
sameAs(e1, e2)
Comment:

269	"abstract given a single image x from domain a and a set of images from domain b, our task is to generate the analogous of x in b. we argue that this task could be a key ai capability <e1>that</e1> underlines the ability of cognitive agents to act in the world and present empirical evidence <e2>that</e2> the existing unsupervised domain translation methods fail on this task."
sameAs(e1, e2)
Comment:

270	"computationally, this generation step requires solving the <e1>task</e1> we term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a <e2>model</e2> of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
Evaluate-for(e1, e2)
Comment:

271	"computationally, this generation step requires solving the task <e1>we</e1> term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as <e2>we</e2> know, with the one-shot case."
sameAs(e1, e2)
Comment:

272	"computationally, this generation step requires solving the task we term <e1>one</e1>-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the <e2>one</e2>-shot case."
sameAs(e1, e2)
Comment:

273	"computationally, this generation step requires solving the task we term one-shot unsupervised cross <e1>domain</e1> translation: given a single sample x from an unknown <e2>domain</e2> a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

274	"computationally, this generation step requires solving the task we term one-shot unsupervised cross <e1>domain</e1> translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of <e2>domain</e2> b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

275	"computationally, this generation step requires solving the task we term one-shot unsupervised cross <e1>domain</e1> translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised <e2>domain</e2> translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

276	"computationally, this generation step requires solving the task we term one-shot unsupervised cross <e1>domain</e1> translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from <e2>domain</e2> a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

277	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown <e1>domain</e1> a and many samples or, almost equivalently, a model of <e2>domain</e2> b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

278	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown <e1>domain</e1> a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised <e2>domain</e2> translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

279	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown <e1>domain</e1> a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from <e2>domain</e2> a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

280	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of <e1>domain</e1> b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised <e2>domain</e2> translation, where many samples from domain a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

281	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of <e1>domain</e1> b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised domain translation, where many samples from <e2>domain</e2> a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

282	"computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain a and many samples or, almost equivalently, a model of domain b, generate a sample y ∈ b that is analogous to x. while there has been a great deal of research dedicated to unsupervised <e1>domain</e1> translation, where many samples from <e2>domain</e2> a are provided, the literature does not deal, as far as we know, with the one-shot case."
sameAs(e1, e2)
Comment:

283	"to be clear, since parts of the literature may refer to <e1>these</e1> type of <e2>tasks</e2> as zero-shot learning, we are not given any training images in a except for the image to be mapped x. consider, for example, the task posed in [2] of mapping zebras to horses."
Used-for(e1, e2)
Comment:

284	"however, it seems entirely possible to map a single zebra <e1>image</e1> to the analogous horse <e2>image</e2> even without seeing any other zebra image."
sameAs(e1, e2)
Comment:

285	"however, it seems entirely possible to map a single zebra <e1>image</e1> to the analogous horse image even without seeing any other zebra <e2>image</e2>."
sameAs(e1, e2)
Comment:

286	"however, it seems entirely possible to map a single zebra image to the analogous horse <e1>image</e1> even without seeing any other zebra <e2>image</e2>."
sameAs(e1, e2)
Comment:

287	"the method we present, called ost, uses the <e1>two</e1> domains asymmetrically and employs <e2>two</e2> steps."
sameAs(e1, e2)
Comment:

288	"first, a variational autoencoder is constructed for <e1>domain</e1> b. this allows us to encode samples from <e2>domain</e2> b effectively as well as generate new samples based on random latent space vectors."
sameAs(e1, e2)
Comment:

289	"in the second phase, the variational <e1>autoencoder</e1> is cloned to create two copies that share the top layers of the encoders and the bottom layers of the decoders, one for the samples in b and one for the sample x in a. the <e2>autoencoders</e2> are trained with reconstruction losses as well as with a single-sample one-way circularity-loss."
sameAs(e1, e2)
Comment:

290	"in the second phase, the variational autoencoder is cloned to create two copies that share the top <e1>layers</e1> of the encoders and the bottom <e2>layers</e2> of the decoders, one for the samples in b and one for the sample x in a. the autoencoders are trained with reconstruction losses as well as with a single-sample one-way circularity-loss."
sameAs(e1, e2)
Comment:

291	"in the second phase, the variational autoencoder is cloned to create two copies that share the top layers of the encoders and the bottom layers of the decoders, <e1>one</e1> for the samples in b and <e2>one</e2> for the sample x in a. the autoencoders are trained with reconstruction losses as well as with a single-sample one-way circularity-loss."
sameAs(e1, e2)
Comment:

292	"in the second phase, the variational autoencoder is cloned to create two copies that share the top layers of the encoders and the bottom layers of the decoders, <e1>one</e1> for the samples in b and one for the sample x in a. the autoencoders are trained with reconstruction losses as well as with a single-sample <e2>one</e2>-way circularity-loss."
sameAs(e1, e2)
Comment:

293	"in the second phase, the variational autoencoder is cloned to create two copies that share the top layers of the encoders and the bottom layers of the decoders, one for the samples in b and <e1>one</e1> for the sample x in a. the autoencoders are trained with reconstruction losses as well as with a single-sample <e2>one</e2>-way circularity-loss."
sameAs(e1, e2)
Comment:

294	"the gradient from sample x updates only the unshared <e1>layers</e1> and not the shared <e2>layers</e2>."
sameAs(e1, e2)
Comment:

295	"this way, the autoencoder of b is adjusted by x through the loss incurred on unshared <e1>layers</e1> for domain b by the circularity loss, and through subsequent adaptation of the shared <e2>layers</e2> to fit the samples of b. this allows the shared layers to gradually adapt to the new sample x, but prevents overfitting on this single sample."
sameAs(e1, e2)
Comment:

296	"this way, the autoencoder of b is adjusted by x through the loss incurred on unshared <e1>layers</e1> for domain b by the circularity loss, and through subsequent adaptation of the shared layers to fit the samples of b. this allows the shared <e2>layers</e2> to gradually adapt to the new sample x, but prevents overfitting on this single sample."
sameAs(e1, e2)
Comment:

297	"this way, the autoencoder of b is adjusted by x through the loss incurred on unshared layers for domain b by the circularity loss, and through subsequent adaptation of the shared <e1>layers</e1> to fit the samples of b. this allows the shared <e2>layers</e2> to gradually adapt to the new sample x, but prevents overfitting on this single sample."
sameAs(e1, e2)
Comment:

298	"this way, the autoencoder of b is adjusted by x through the loss incurred on unshared layers for domain b by the circularity loss, and through subsequent adaptation of the shared layers to fit the samples of b. <e1>this</e1> allows the shared layers to gradually adapt to the new sample x, but prevents overfitting on <e2>this</e2> single sample."
sameAs(e1, e2)
Comment:

299	"on most datasets the <e1>method</e1> also presents a comparable accuracy with a single training example to the accuracy obtained by the other <e2>methods</e2> for the entire set of domain a images."
Compare(e1, e2)
Comment:

300	"on most datasets the method also presents a comparable <e1>accuracy</e1> with a single training example to the <e2>accuracy</e2> obtained by the other methods for the entire set of domain a images."
sameAs(e1, e2)
Comment:

301	"this success sheds new light on the potential mechanisms that underlie unsupervised <e1>domain</e1> translation, since in the one-shot case, constraints on the inter-sample correlations in <e2>domain</e2> a do not apply."
sameAs(e1, e2)
Comment:

302	"then, given the new sample x, we create a variational autoencoder for domain a by adapting the <e1>layers</e1> that are close to the image in order to directly fit x, and only indirectly adapt the other <e2>layers</e2>."
sameAs(e1, e2)
Comment:

303	"our experiments indicate that the new <e1>method</e1> does as well, when trained on one sample x, as the existing domain transfer <e2>methods</e2>, when these enjoy a multitude of training samples from domain a. our code is made publicly available at https://github.com/sagiebenaim/oneshottranslation."
Compare(e1, e2)
Comment:

304	"our experiments indicate that the new method does as well, when trained on one sample x, as the existing <e1>domain</e1> transfer methods, when these enjoy a multitude of training samples from <e2>domain</e2> a. our code is made publicly available at https://github.com/sagiebenaim/oneshottranslation."
sameAs(e1, e2)
Comment:

305	"whenever a new sample is observed, the agent generates, using the <e1>internal model</e1>, a virtual sample that is analogous to the observed one, and compares the observed and blended objects in order to update the <e2>internal model</e2>."
sameAs(e1, e2)
Comment:

306	"the wasserstein metric used in wgans is based on a notion of <e1>distance</e1> between individual images, which induces a notion of <e2>distance</e2> between probability distributions of images."
sameAs(e1, e2)
Comment:

307	"the wasserstein metric used in wgans is based on a notion of distance between individual <e1>images</e1>, which induces a notion of distance between probability distributions of <e2>images</e2>."
sameAs(e1, e2)
Comment:

308	"the wasserstein <e1>distance</e1> is inherently based on a notion of <e2>distance</e2> between images which in all implementations of wasserstein gans (wgan) so far has been the 2 distance."
sameAs(e1, e2)
Comment:

309	"the wasserstein <e1>distance</e1> is inherently based on a notion of distance between images which in all implementations of wasserstein gans (wgan) so far has been the 2 <e2>distance</e2>."
sameAs(e1, e2)
Comment:

310	"the wasserstein distance is inherently based on a notion of <e1>distance</e1> between images which in all implementations of wasserstein gans (wgan) so far has been the 2 <e2>distance</e2>."
sameAs(e1, e2)
Comment:

311	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) are one of the most popular generative models [6] ."
sameAs(e1, e2)
Comment:

312	"in particular, [2, 7] introduces a critic built around the wasserstein distance between the distribution of true <e1>images</e1> and generated <e2>images</e2>."
sameAs(e1, e2)
Comment:

313	"however, <e1>these</e1> <e2>tasks</e2> are also ideal for machine learning, since they can be represented as classic regression and classification problems."
Used-for(e1, e2)
Comment:

314	"software is a form of human communication; <e1>software</e1> corpora have similar statistical properties to natural language corpora; and these properties can be exploited to build better <e2>software</e2> engineering tools."
sameAs(e1, e2)
Comment:

315	"software is a form of human communication; software corpora have similar statistical <e1>properties</e1> to natural language corpora; and these <e2>properties</e2> can be exploited to build better software engineering tools."
sameAs(e1, e2)
Comment:

316	"our work makes the following contributions: • <e1>we</e1> formulate a robust distributional hypothesis for code, from which <e2>we</e2> draw a novel distributed representation of code statements based on contextual flow and llvm ir."
sameAs(e1, e2)
Comment:

317	"• using one simple lstm <e1>architecture</e1> and fixed pre-trained embeddings, we match or surpass the best-performing approaches in each task, including specialized dnn <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

318	"we provide a novel definition of contextual <e1>flow</e1> for this ir, leveraging both the underlying data-and control-<e2>flow</e2> of the program."
sameAs(e1, e2)
Comment:

319	"nevertheless, at run-time, all that is required to make a 3d <e1>convolution</e1> equivariant using our method, is to parameterize the <e2>convolution</e2> kernel as a linear combination of pre-computed steerable basis kernels."
sameAs(e1, e2)
Comment:

320	"the architectures presented here fall within the framework of steerable g-cnns [8, 10, 41, 46] , which represent their input as <e1>fields</e1> over a homogeneous space (r 3 in this case), and use steerable the most closely related works achieving full se(3) equivariance are the tensor <e2>field</e2> network (tfn) [41] and the n-body networks (nbns) [27] ."
sameAs(e1, e2)
Comment:

321	"in [34, 39] , a spherical <e1>tensor</e1> algebra was utilized to expand signals in terms of spherical <e2>tensor</e2> fields."
sameAs(e1, e2)
Comment:

322	"it has long been understood <e1>that</e1> the equations <e2>that</e2> define a model or natural law should respect the symmetries of the system under study, and that knowledge of symmetries provides a powerful constraint on the space of admissible models."
sameAs(e1, e2)
Comment:

323	"it has long been understood <e1>that</e1> the equations that define a model or natural law should respect the symmetries of the system under study, and <e2>that</e2> knowledge of symmetries provides a powerful constraint on the space of admissible models."
sameAs(e1, e2)
Comment:

324	"it has long been understood that the equations <e1>that</e1> define a model or natural law should respect the symmetries of the system under study, and <e2>that</e2> knowledge of symmetries provides a powerful constraint on the space of admissible models."
sameAs(e1, e2)
Comment:

325	"a standard solution first detects a sparse set of category-specific keypoints, and then uses such points within a geometric reasoning <e1>framework</e1> (e.g., a pnp <e2>algorithm</e2> [24] ) to recover the 3d pose or camera angle."
Used-for(e1, e2)
Comment:

326	"we formulate 3d <e1>pose estimation</e1> as one such task, and our key technical contributions include (1) a novel differentiable <e2>pose estimation</e2> objective and (2) a multi-view consistency loss function."
sameAs(e1, e2)
Comment:

327	"the <e1>pose</e1> objective seeks optimal keypoints for recovering the relative <e2>pose</e2> between two views of an object."
sameAs(e1, e2)
Comment:

328	"a complete answer to this question will require further investigation, but we show preliminary evidence that swapping convolution for coordconv can improve <e1>models</e1> on a diverse set of <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

329	"although straightforward <e1>cnns</e1> excel at many tasks, in many other cases progress has been accelerated through the development of specialized layers that complement the abilities of <e2>cnns</e2>."
sameAs(e1, e2)
Comment:

330	"detection models like faster r-<e1>cnn</e1> [27] make use of layers to compute coordinate transforms and focus attention, spatial transformer networks [13] make use of differentiable cameras to transform data from the output of one <e2>cnn</e2> into a form more amenable to processing with another,  and some generative models like draw [8] iteratively perceive, focus, and refine a canvas rather than using a single pass through a cnn to generate an image."
sameAs(e1, e2)
Comment:

331	"detection models like faster r-<e1>cnn</e1> [27] make use of layers to compute coordinate transforms and focus attention, spatial transformer networks [13] make use of differentiable cameras to transform data from the output of one cnn into a form more amenable to processing with another,  and some generative models like draw [8] iteratively perceive, focus, and refine a canvas rather than using a single pass through a <e2>cnn</e2> to generate an image."
sameAs(e1, e2)
Comment:

332	"detection models like faster r-cnn [27] make use of layers to compute coordinate transforms and focus attention, spatial transformer networks [13] make use of differentiable cameras to transform data from the output of one <e1>cnn</e1> into a form more amenable to processing with another,  and some generative models like draw [8] iteratively perceive, focus, and refine a canvas rather than using a single pass through a <e2>cnn</e2> to generate an image."
sameAs(e1, e2)
Comment:

333	"in this work, we expose and analyze a generic inability of cnns to transform spatial representations between two different types: from a dense cartesian <e1>representation</e1> to a sparse, pixel-based <e2>representation</e2> or in the opposite direction."
sameAs(e1, e2)
Comment:

334	"in <e1>this</e1> paper we show a striking counterexample to <e2>this</e2> intuition via the seemingly trivial coordinate transform problem, which simply requires learning a mapping between coordinates in (x, y) cartesian space and coordinates in one-hot pixel space."
sameAs(e1, e2)
Comment:

335	"4. the supervised rendering task adds complexity to the above by requiring a network to paint a full <e1>image</e1> from the not-so-clevr dataset given the (x, y) coordinates of the center of a square in the <e2>image</e2>."
sameAs(e1, e2)
Comment:

336	"the <e1>task</e1> is still fully supervised, but as before, the <e2>task</e2> is difficult to learn for convolution and trivial for coordconv (section 4.3)."
sameAs(e1, e2)
Comment:

337	"on two-object sort-of-clevr [29] images, <e1>generative adversarial networks</e1> (<e2>gans</e2>) and variational autoencoders (vaes) using coordconv exhibit less mode collapse, perhaps because ease of learning coordinate transforms translates to ease of using latents to span a 2d cartesian space."
sameAs(e1, e2)
Comment:

338	"coordconv solves the coordinate transform problem with perfect generalization and 150 <e1>times</e1> faster with 10-100 <e2>times</e2> fewer parameters than convolution."
sameAs(e1, e2)
Comment:

339	"the categorical <e1>method</e1> is fit with amortized variational inference using a learned inference network to improve the training bound and policy gradient variance reduction <e2>baseline</e2> using soft attention."
Compare(e1, e2)
Comment:

340	"experiments describe how to implement this <e1>approach</e1> for two major attention-based <e2>models</e2>: neural machine translation and visual question answering ( figure 1 gives an overview of our approach for machine translation)."
Used-for(e1, e2)
Comment:

341	"experiments describe how to implement this <e1>approach</e1> for two major attention-based models: neural machine translation and visual question answering ( figure 1 gives an overview of our <e2>approach</e2> for machine translation)."
sameAs(e1, e2)
Comment:

342	"related work latent <e1>alignment</e1> has long been a core problem in nlp, starting with the seminal ibm models [9] , hmm-based <e2>alignment</e2> models [65] , and a fast log-linear reparameterization of the ibm 2 model [18] ."
sameAs(e1, e2)
Comment:

343	"neural soft attention <e1>models</e1> were originally introduced as an alternative approach for neural machine translation [6] , and have subsequently been successful on a wide range of <e2>tasks</e2> (see [13] for a review of applications)."
Used-for(e1, e2)
Comment:

344	"our <e1>method</e1> uses uses different estimators and targets the single sample approach for efficiency, allowing the <e2>method</e2> to be employed for nmt and vqa applications."
sameAs(e1, e2)
Comment:

345	"our <e1>method</e1> uses uses different estimators and targets the single sample approach for efficiency, allowing the method to be employed for nmt and vqa <e2>applications</e2>."
Used-for(e1, e2)
Comment:

346	"our method uses uses different estimators and targets the single sample <e1>approach</e1> for efficiency, allowing the <e2>method</e2> to be employed for nmt and vqa applications."
Compare(e1, e2)
Comment:

347	"our method uses uses different estimators and targets the single sample approach for efficiency, allowing the <e1>method</e1> to be employed for nmt and vqa <e2>applications</e2>."
Used-for(e1, e2)
Comment:

348	"of particular interest are <e1>those</e1> that augment an rnn with latent variables (typically gaussian) at each time step [15, 20, 57, 21, 34] and <e2>those</e2> that incorporate latent variables into sequence-to-sequence models [73, 7, 60] ."
sameAs(e1, e2)
Comment:

349	"of particular interest are those <e1>that</e1> augment an rnn with latent variables (typically gaussian) at each time step [15, 20, 57, 21, 34] and those <e2>that</e2> incorporate latent variables into sequence-to-sequence models [73, 7, 60] ."
sameAs(e1, e2)
Comment:

350	"of particular interest are those that augment an rnn with <e1>latent variables</e1> (typically gaussian) at each time step [15, 20, 57, 21, 34] and those that incorporate <e2>latent variables</e2> into sequence-to-sequence models [73, 7, 60] ."
sameAs(e1, e2)
Comment:

351	"introduction attention networks [6] have quickly become the foundation for state-of-the-art models in <e1>natural language understanding</e1>, question answering, <e2>speech recognition</e2>, image captioning, and more [13, 70, 14, 12, 55, 69, 61, 54] ."
isA(e1, e2)
Comment:

352	"we find <e1>that</e1> our proposed model can learn functions over representations <e2>that</e2> capture the underlying numerical nature of the data and generalize to numbers that are several orders of magnitude larger than those observed during training."
sameAs(e1, e2)
Comment:

353	"we find <e1>that</e1> our proposed model can learn functions over representations that capture the underlying numerical nature of the data and generalize to numbers <e2>that</e2> are several orders of magnitude larger than those observed during training."
sameAs(e1, e2)
Comment:

354	"we find that our proposed <e1>model</e1> can learn functions over representations that capture the underlying numerical nature of the data and generalize to numbers that are several orders of magnitude larger than <e2>those</e2> observed during training."
Compare(e1, e2)
Comment:

355	"we find that our proposed model can learn functions over representations <e1>that</e1> capture the underlying numerical nature of the data and generalize to numbers <e2>that</e2> are several orders of magnitude larger than those observed during training."
sameAs(e1, e2)
Comment:

356	"we call this module a neural arithmetic <e1>logic</e1> unit (nalu), by analogy to the arithmetic <e2>logic</e2> unit in traditional processors."
sameAs(e1, e2)
Comment:

357	"experiments show that nalu-enhanced neural networks can learn to track time, perform arithmetic over <e1>images</e1> of numbers, translate numerical language into real-valued scalars, execute computer code, and count objects in <e2>images</e2>."
sameAs(e1, e2)
Comment:

358	"abstract we introduce a <e1>method</e1> which enables a recurrent dynamics <e2>model</e2> to be temporally abstract."
Used-for(e1, e2)
Comment:

359	"our starting point is the hypothesis <e1>that</e1> at each instant of the evolution, there is an ideal temporal step length associated with those state transitions which are easiest to predict: intervals which are too long correspond to complicated mechanisms <e2>that</e2> could be simplified by breaking them down into a successive application of simpler mechanisms."
sameAs(e1, e2)
Comment:

360	"our starting point is the hypothesis that at each instant of the evolution, there is an ideal temporal step length associated with those state transitions which are easiest to predict: intervals which are too long correspond to complicated <e1>mechanisms</e1> that could be simplified by breaking them down into a successive application of simpler <e2>mechanisms</e2>."
sameAs(e1, e2)
Comment:

361	"for example, when some frames are missing or extremely noisy, a frame-by-frame prediction <e1>method</e1> would be forced to <e2>model</e2> the noise, especially if it is not independent of the state."
Used-for(e1, e2)
Comment:

362	"flexibly adjusting the <e1>time</e1> resolution of predictions also results in more computationally efficiency, as fewer steps need to be predicted where they are not necessary -a key requirement for real-<e2>time</e2> applications."
sameAs(e1, e2)
Comment:

363	"on the other hand, predicting <e1>that</e1> the ball will hit the horizontal platform on the bottom is easy because it only requires knowing <e2>that</e2> when the ball falls somewhere into the funnel, it will come out at the bottom end, irrespective of how long it bounces around."
sameAs(e1, e2)
Comment:

364	"if <e1>we</e1> are only interested in predicting where the ball will ultimately land, <e2>we</e2> can skip the difficult parts, provided that they are inconsequential."
sameAs(e1, e2)
Comment:

365	"for example, model-based reinforcement learning (daw, 2012; arulkumaran et al, 2017) decomposes the <e1>task</e1> into the two components of learning a <e2>model</e2> and then using the learned model for planning ahead."
Evaluate-for(e1, e2)
Comment:

366	"for example, model-based reinforcement learning (daw, 2012; arulkumaran et al, 2017) decomposes the <e1>task</e1> into the two components of learning a model and then using the learned <e2>model</e2> for planning ahead."
Evaluate-for(e1, e2)
Comment:

367	"for example, model-based reinforcement learning (daw, 2012; arulkumaran et al, 2017) decomposes the task into the two components of learning a <e1>model</e1> and then using the learned <e2>model</e2> for planning ahead."
sameAs(e1, e2)
Comment:

368	"despite significant recent advances, even relatively simple <e1>tasks</e1> like pushing objects is still a challenging robotic task and foresight for robot planning is still limited to relatively short horizon <e2>tasks</e2> ."
sameAs(e1, e2)
Comment:

369	"many dynamical systems have the property that <e1>long-term</e1> <e2>predictions</e2> of future states are easiest to learn if they are obtained by a sequence of incremental predictions."
Feature-of(e1, e2)
Comment:

370	"many dynamical systems have the property that <e1>long-term</e1> predictions of future states are easiest to learn if they are obtained by a sequence of incremental <e2>predictions</e2>."
Feature-of(e1, e2)
Comment:

371	"many dynamical systems have the property that long-term <e1>predictions</e1> of future states are easiest to learn if they are obtained by a sequence of incremental <e2>predictions</e2>."
sameAs(e1, e2)
Comment:

372	"abstract we study the problem of <e1>video</e1>-to-<e2>video</e2> synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

373	"abstract we study the problem of <e1>video</e1>-to-video synthesis, whose goal is to learn a mapping function from an input source <e2>video</e2> (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

374	"abstract we study the problem of <e1>video</e1>-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic <e2>video</e2> that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

375	"abstract we study the problem of <e1>video</e1>-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source <e2>video</e2>."
sameAs(e1, e2)
Comment:

376	"abstract we study the problem of video-to-<e1>video</e1> synthesis, whose goal is to learn a mapping function from an input source <e2>video</e2> (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

377	"abstract we study the problem of video-to-<e1>video</e1> synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic <e2>video</e2> that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

378	"abstract we study the problem of video-to-<e1>video</e1> synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source <e2>video</e2>."
sameAs(e1, e2)
Comment:

379	"abstract we study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source <e1>video</e1> (e.g., a sequence of semantic segmentation masks) to an output photorealistic <e2>video</e2> that precisely depicts the content of the source video."
sameAs(e1, e2)
Comment:

380	"abstract we study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source <e1>video</e1> (e.g., a sequence of semantic segmentation masks) to an output photorealistic video that precisely depicts the content of the source <e2>video</e2>."
sameAs(e1, e2)
Comment:

381	"abstract we study the problem of video-to-video synthesis, whose goal is to learn a mapping function from an input source video (e.g., a sequence of semantic segmentation masks) to an output photorealistic <e1>video</e1> that precisely depicts the content of the source <e2>video</e2>."
sameAs(e1, e2)
Comment:

382	"the <e1>video</e1> synthesis problem exists in various forms, including future <e2>video</e2> prediction [15, 18, 42, 45, 50, 65, 68, 71, 77] and unconditional video synthesis [59, 67, 69] ."
sameAs(e1, e2)
Comment:

383	"the <e1>video</e1> synthesis problem exists in various forms, including future video prediction [15, 18, 42, 45, 50, 65, 68, 71, 77] and unconditional <e2>video</e2> synthesis [59, 67, 69] ."
sameAs(e1, e2)
Comment:

384	"the video synthesis problem exists in various forms, including future <e1>video</e1> prediction [15, 18, 42, 45, 50, 65, 68, 71, 77] and unconditional <e2>video</e2> synthesis [59, 67, 69] ."
sameAs(e1, e2)
Comment:

385	"in this paper, we study a new form: <e1>video</e1>-to-<e2>video</e2> synthesis."
sameAs(e1, e2)
Comment:

386	"at the core, we aim to learn a mapping function that can convert an input <e1>video</e1> to an output <e2>video</e2>."
sameAs(e1, e2)
Comment:

387	"to the best of our knowledge, a general-purpose <e1>solution</e1> to video-to-video synthesis has not yet been explored by prior work, although its image counterpart, the image-to-image translation <e2>problem</e2>, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83] ."
Used-for(e1, e2)
Comment:

388	"to the best of our knowledge, a general-purpose solution to <e1>video</e1>-to-<e2>video</e2> synthesis has not yet been explored by prior work, although its image counterpart, the image-to-image translation problem, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83] ."
sameAs(e1, e2)
Comment:

389	"to the best of our knowledge, a general-purpose solution to video-to-video synthesis has not yet been explored by prior work, although its <e1>image</e1> counterpart, the <e2>image</e2>-to-image translation problem, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83] ."
sameAs(e1, e2)
Comment:

390	"to the best of our knowledge, a general-purpose solution to video-to-video synthesis has not yet been explored by prior work, although its <e1>image</e1> counterpart, the image-to-<e2>image</e2> translation problem, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83] ."
sameAs(e1, e2)
Comment:

391	"to the best of our knowledge, a general-purpose solution to video-to-video synthesis has not yet been explored by prior work, although its image counterpart, the <e1>image</e1>-to-<e2>image</e2> translation problem, is a popular research topic [6, 31, 33, 43, 44, 63, 66, 73, 82, 83] ."
sameAs(e1, e2)
Comment:

392	"we cast the <e1>video</e1>-to-<e2>video</e2> synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized videos given input videos resembles that of real videos."
sameAs(e1, e2)
Comment:

393	"we cast the video-to-video synthesis <e1>problem</e1> as a distribution matching <e2>problem</e2>, where the goal is to train a model such that the conditional distribution of the synthesized videos given input videos resembles that of real videos."
sameAs(e1, e2)
Comment:

394	"we cast the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such <e1>that</e1> the conditional distribution of the synthesized videos given input videos resembles <e2>that</e2> of real videos."
sameAs(e1, e2)
Comment:

395	"we cast the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized <e1>videos</e1> given input <e2>videos</e2> resembles that of real videos."
sameAs(e1, e2)
Comment:

396	"we cast the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized <e1>videos</e1> given input videos resembles that of real <e2>videos</e2>."
sameAs(e1, e2)
Comment:

397	"we cast the video-to-video synthesis problem as a distribution matching problem, where the goal is to train a model such that the conditional distribution of the synthesized videos given input <e1>videos</e1> resembles that of real <e2>videos</e2>."
sameAs(e1, e2)
Comment:

398	"while its <e1>image</e1> counterpart, the <e2>image</e2>-to-image translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature."
sameAs(e1, e2)
Comment:

399	"while its <e1>image</e1> counterpart, the image-to-<e2>image</e2> translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature."
sameAs(e1, e2)
Comment:

400	"while its image counterpart, the <e1>image</e1>-to-<e2>image</e2> translation problem, is a popular topic, the video-to-video synthesis problem is less explored in the literature."
sameAs(e1, e2)
Comment:

401	"while its image counterpart, the image-to-image translation <e1>problem</e1>, is a popular topic, the video-to-video synthesis <e2>problem</e2> is less explored in the literature."
sameAs(e1, e2)
Comment:

402	"while its image counterpart, the image-to-image translation problem, is a popular topic, the <e1>video</e1>-to-<e2>video</e2> synthesis problem is less explored in the literature."
sameAs(e1, e2)
Comment:

403	"to this end, we <e1>learn</e1> a conditional generative adversarial model [20] given paired input and output videos, with carefully-designed generators and discriminators, and a new spatio-temporal learning objective, our method can <e2>learn</e2> to synthesize high-resolution, photorealistic, temporally coherent videos."
sameAs(e1, e2)
Comment:

404	"to this end, we learn a conditional generative adversarial model [20] given paired input and output <e1>videos</e1>, with carefully-designed generators and discriminators, and a new spatio-temporal learning objective, our method can learn to synthesize high-resolution, photorealistic, temporally coherent <e2>videos</e2>."
sameAs(e1, e2)
Comment:

405	"in addition, our method works for other input video formats such as <e1>face</e1> sketches and body poses, enabling many applications from <e2>face</e2> swapping to human motion transfer."
sameAs(e1, e2)
Comment:

406	"in this paper, we propose a <e1>video</e1>-to-<e2>video</e2> synthesis approach under the generative adversarial learning framework."
sameAs(e1, e2)
Comment:

407	"first, using three well known <e1>dnns</e1>  googlenet)   we find the human visual system to be more robust to nearly all of the tested image manipulations, and we observe progressively diverging classification error-patterns between humans and <e2>dnns</e2> when the signal gets weaker."
sameAs(e1, e2)
Comment:

408	"first, using three well known dnns  googlenet)   <e1>we</e1> find the human visual system to be more robust to nearly all of the tested image manipulations, and <e2>we</e2> observe progressively diverging classification error-patterns between humans and dnns when the signal gets weaker."
sameAs(e1, e2)
Comment:

409	"first, using three well known dnns  googlenet)   we find the human visual system to be more robust to nearly all of the tested <e1>image</e1> manipulations, and we observe progressively diverging <e2>classification</e2> error-patterns between humans and dnns when the signal gets weaker."
Used-for(e1, e2)
Comment:

410	"secondly, we show that dnns trained directly on distorted images consistently surpass human performance on the exact distortion types <e1>they</e1> were trained on, yet <e2>they</e2> display extremely poor generalisation abilities when tested on other distortion types."
sameAs(e1, e2)
Comment:

411	"for example, training on salt-and-pepper <e1>noise</e1> does not imply robustness on uniform white <e2>noise</e2> and vice versa."
sameAs(e1, e2)
Comment:

412	"our new dataset consisting of 83k carefully measured human psychophysical trials provide a useful reference for lifelong <e1>robustness</e1> against image degradations set by the human visual <e2>system</e2>."
Evaluate-for(e1, e2)
Comment:

413	"while recent work [18] have demonstrated promising ability in the above <e1>task</e1>, designs of exisitng models typically require high computational costs when more than two <e2>data</e2> domains or multiple feature attributes are of interest."
Conjunction(e1, e2)
Comment:

414	"to perform joint <e1>feature</e1> disentanglement and translation across multiple data domains, we propose a compact yet effective model of unified <e2>feature</e2> disentanglement network (ufdn), which is composed of a pair of unified encoder and generator as shown in figure 1 ."
sameAs(e1, e2)
Comment:

415	"from this figure, it can be seen that our encoder takes <e1>data</e1> instances from multiple domains as inputs, and a domain-invariant latent feature space is derived via adversarial training, followed by a generator/decoder which recovers or translates <e2>data</e2> across domains."
sameAs(e1, e2)
Comment:

416	"from this figure, it can be seen that our encoder takes data instances from multiple <e1>domains</e1> as inputs, and a domain-invariant latent feature space is derived via adversarial training, followed by a generator/decoder which recovers or translates data across <e2>domains</e2>."
sameAs(e1, e2)
Comment:

417	"our model is able to disentangle the underlying factors which represent <e1>domain</e1>-specific information (e.g., <e2>domain</e2> code, attribute of interest, etc.)."
sameAs(e1, e2)
Comment:

418	"in addition to very promising results in multi-<e1>domain</e1> image-to-image translation, we further confirm that our ufdn is able to perform continuous image translation using the interpolated <e2>domain</e2> code in the resulting latent space."
sameAs(e1, e2)
Comment:

419	"in addition to very promising results in multi-domain <e1>image</e1>-to-<e2>image</e2> translation, we further confirm that our ufdn is able to perform continuous image translation using the interpolated domain code in the resulting latent space."
sameAs(e1, e2)
Comment:

420	"in addition to very promising results in multi-domain <e1>image</e1>-to-image translation, we further confirm that our ufdn is able to perform continuous <e2>image</e2> translation using the interpolated domain code in the resulting latent space."
sameAs(e1, e2)
Comment:

421	"in addition to very promising results in multi-domain image-to-<e1>image</e1> translation, we further confirm that our ufdn is able to perform continuous <e2>image</e2> translation using the interpolated domain code in the resulting latent space."
sameAs(e1, e2)
Comment:

422	"the contributions of this paper are highlighted as follows: • we propose a unified <e1>feature</e1> disentanglement network (ufdn), which learns deep disentangled <e2>feature</e2> representation for multi-domain image translation and manipulation."
sameAs(e1, e2)
Comment:

423	"• our ufdn views both data domains and <e1>image</e1> attributes of interest as latent factors to be disentangled, wich realizes multi-domain <e2>image</e2> translation in a single unified framework."
sameAs(e1, e2)
Comment:

424	"in particular, learning deep representation with the ability to exploit relationship between <e1>data</e1> across different <e2>data</e2> domains has attracted the attention from the researchers."
sameAs(e1, e2)
Comment:

425	"recent developments of deep learning technologies have shown progress in the tasks of <e1>cross-domain</e1> visual classification [6, 26, 27] and <e2>cross-domain</e2> image translation [10, 24, 30, 11, 28, 17, 16, 4] ."
sameAs(e1, e2)
Comment:

426	"this work falls under neural guided <e1>search</e1>, where the machine learning model guides a symbolic <e2>search</e2> technique."
sameAs(e1, e2)
Comment:

427	"we take the integration with a symbolic <e1>system</e1> further: we use the internal representation of the symbolic <e2>system</e2> as input to the neural model."
sameAs(e1, e2)
Comment:

428	"the symbolic <e1>system</e1> we use is a constraint logic programming <e2>system</e2> called minikanren 1 [8] , chosen for its ability to encode synthesis problems that are difficult to encode in other systems."
sameAs(e1, e2)
Comment:

429	"we present a <e1>method</e1> for solving programming by example (pbe) problems by using a neural <e2>model</e2> to guide the search of a constraint logic programming system called minikanren."
Used-for(e1, e2)
Comment:

430	"we present a <e1>method</e1> for solving programming by example (pbe) problems by using a neural model to guide the <e2>search</e2> of a constraint logic programming system called minikanren."
Used-for(e1, e2)
Comment:

431	"our <e1>model</e1> uses these internal constraints to score candidate <e2>programs</e2> and guide minikanren's search."
Part-of(e1, e2)
Comment:

432	"third, constraint lengths are relatively stable even as we synthesize more complex <e1>programs</e1>, thus our approach should be able to generalize to <e2>programs</e2> larger than those seen in training."
sameAs(e1, e2)
Comment:

433	"we explore two approaches to scoring constraints: using a <e1>recurrent neural network</e1> (<e2>rnn</e2>) and graph neural network (gnn) [10] ."
sameAs(e1, e2)
Comment:

434	"we explore two approaches to scoring constraints: using a <e1>recurrent neural network</e1> (rnn) and graph <e2>neural network</e2> (gnn) [10] ."
isA(e1, e2)
Comment:

435	"we explore two approaches to scoring constraints: using a recurrent neural network (<e1>rnn</e1>) and graph <e2>neural network</e2> (gnn) [10] ."
isA(e1, e2)
Comment:

436	"we test our approach to solve pbe <e1>problems</e1> in a subset of lisp, and show improved performance on generated <e2>problems</e2> held out from training."
sameAs(e1, e2)
Comment:

437	"to gauge generalizability, we test our <e1>approaches</e1> on three families of synthesis problems, comparing against state-of-the-art <e2>systems</e2> λ 2 [11] , escher [12] , and myth [13] ."
Evaluate-for(e1, e2)
Comment:

438	"we show that our neural-guided approach using constraints can synthesize <e1>problems</e1> faster in many cases, and has the potential to generalize to larger <e2>problems</e2>."
sameAs(e1, e2)
Comment:

439	"we explore <e1>recurrent neural network</e1> and graph <e2>neural network models</e2>."
isA(e1, e2)
Comment:

440	"programming by example (pbe) is one way to formulate <e1>program</e1> synthesis problems, where example input/output pairs are used to specify a target <e2>program</e2>."
sameAs(e1, e2)
Comment:

441	"in a sense, <e1>supervised learning</e1> can be considered program synthesis, but <e2>supervised learning</e2> via successful models like deep neural networks famously lacks interpretability."
sameAs(e1, e2)
Comment:

442	"most recent works on <e1>subspace</e1> clustering [49, 6, 10, 23, 46, 26, 16, 52] focus on clustering linear <e2>subspaces</e2>."
sameAs(e1, e2)
Comment:

443	"however, in practice, the <e1>data</e1> do not necessarily conform to linear subspace <e2>models</e2>."
Used-for(e1, e2)
Comment:

444	"this layer encodes the "self-expressiveness" property [38, 9] of <e1>data</e1> drawn from a union of subspaces, that is, the fact that each <e2>data</e2> sample can be represented as a linear combination of other samples in the same subspace."
sameAs(e1, e2)
Comment:

445	"this layer encodes the "self-expressiveness" property [38, 9] of data drawn from a union of <e1>subspaces</e1>, that is, the fact that each data sample can be represented as a linear combination of other samples in the same <e2>subspace</e2>."
sameAs(e1, e2)
Comment:

446	"this layer encodes the "self-expressiveness" property [38, 9] of data drawn from a union of subspaces, <e1>that</e1> is, the fact <e2>that</e2> each data sample can be represented as a linear combination of other samples in the same subspace."
sameAs(e1, e2)
Comment:

447	"introduction in this paper, we tackle the problem of <e1>subspace</e1> clustering [42] -a sub-field of unsupervised learning -which aims to cluster data points drawn from a union of low-dimensional <e2>subspaces</e2> in an unsupervised manner."
sameAs(e1, e2)
Comment:

448	"abstract learning with <e1>recurrent neural networks</e1> (<e2>rnns</e2>) on long sequences is a notoriously difficult task."
sameAs(e1, e2)
Comment:

449	"however, long sequence learning with <e1>rnns</e1> remains a challenging problem for the following reasons: first, memorizing extremely long-term dependencies while maintaining mid-and short-term memory is difficult; second, training <e2>rnns</e2> using back-propagationthrough-time is impeded by vanishing and exploding gradients; and lastly, both forward-and back-propagation are performed in a sequential manner, which makes the training time-consuming."
sameAs(e1, e2)
Comment:

450	"recent attempts have focused on multi-timescale designs, including clockwork <e1>rnns</e1> [12] , phased lstm [17] , hierarchical multi-scale <e2>rnns</e2> [5] , etc."
sameAs(e1, e2)
Comment:

451	"the problem of vanishing and exploding <e1>gradients</e1> is mitigated by lstm and gru memory gates; other partial solutions include <e2>gradient</e2> clipping [18] , orthogonal and unitary weight optimization [2, 14, 24] , and skip connections across multiple timestamps [8, 30] ."
sameAs(e1, e2)
Comment:

452	"however, the length of dependencies captured by a dilated cnn is limited by its kernel size, whereas an <e1>rnn</e1>'s autoregressive <e2>modeling</e2> can, in theory, capture potentially infinitely  long dependencies with a small number of parameters."
Used-for(e1, e2)
Comment:

453	"2) we stack multiple dilated recurrent <e1>layers</e1> with hierarchical dilations to construct a dilatedrnn, which learns temporal dependencies of different scales at different <e2>layers</e2>."
sameAs(e1, e2)
Comment:

454	"many studies [6, 14] have shown that vanilla rnn cells perform poorly in <e1>these</e1> learning <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

455	"introduction <e1>recurrent neural networks</e1> (<e2>rnns</e2>) have been shown to have remarkable performance on many sequential learning problems."
sameAs(e1, e2)
Comment:

456	"exponential family <e1>embeddings</e1> (efe) is a probabilistic perspective on <e2>embeddings</e2> that encompasses many existing methods and opens the door to bringing expressive probabilistic modeling (bishop, 2006; murphy, 2012) to the problem of learning distributed representations."
sameAs(e1, e2)
Comment:

457	"we develop structured exponential family <e1>embeddings</e1> (s-efe), an extension of efe for studying how <e2>embeddings</e2> can vary across groups of related data."
sameAs(e1, e2)
Comment:

458	"while the naïve <e1>approach</e1> of fitting an individual embedding model for each group would typically suffer from lack of data-especially in groups for which fewer observations are available-we develop two <e2>methods</e2> that can share information across groups."
Compare(e1, e2)
Comment:

459	"while the naïve approach of fitting an individual embedding <e1>model</e1> for each group would typically suffer from lack of data-especially in groups for which fewer observations are available-we develop two <e2>methods</e2> that can share information across groups."
Compare(e1, e2)
Comment:

460	"here we develop structured exponential family <e1>embeddings</e1> (s-efe), a method for discovering <e2>embeddings</e2> that vary across related groups of data."
sameAs(e1, e2)
Comment:

461	"we can see that how intelligence is used varies by field: in computer science the most similar <e1>words</e1> include artificial and ai; in finance, similar <e2>words</e2> include abilities and consciousness."
sameAs(e1, e2)
Comment:

462	"(<e1>we</e1> use the language of text for concreteness; as <e2>we</e2> mentioned, efe extend to other types of data.)"
sameAs(e1, e2)
Comment:

463	"the second thread is multilingual <e1>embeddings</e1> (klementiev et al, 2012; mikolov et al, 2013b; ammar et al, 2016; zou et al, 2013) ; our approach is different in that most words appear in all groups and we are interested in the variations of the <e2>embeddings</e2> across those groups."
sameAs(e1, e2)
Comment:

464	"we present two techniques to share statistical strength among the embedding vectors, <e1>one</e1> based on hierarchical modeling and <e2>one</e2> based on amortization."
sameAs(e1, e2)
Comment:

465	"[47, 44, 46] ) have emerged as the dominant paradigm across a variety of setting and datasets -from text-only <e1>dialog</e1> [44, 40, 23, 3 ] to more recently, visual <e2>dialog</e2> [7, 9, 8, 33, 45] , where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the dialog history."
sameAs(e1, e2)
Comment:

466	"[47, 44, 46] ) have emerged as the dominant paradigm across a variety of setting and datasets -from text-only <e1>dialog</e1> [44, 40, 23, 3 ] to more recently, visual dialog [7, 9, 8, 33, 45] , where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the <e2>dialog</e2> history."
sameAs(e1, e2)
Comment:

467	"[47, 44, 46] ) have emerged as the dominant paradigm across a variety of setting and datasets -from text-only dialog [44, 40, 23, 3 ] to more recently, visual <e1>dialog</e1> [7, 9, 8, 33, 45] , where an agent must answer a sequence of questions grounded in an image, requiring it to reason about both visual content and the <e2>dialog</e2> history."
sameAs(e1, e2)
Comment:

468	"across a variety of domains, a recurring problem with mle trained neural dialog models is that they tend to produce 'safe' generic responses, such as 'not sure' or 'i don't know' in text-only <e1>dialog</e1> [23] , and 'i can't see' or 'i can't tell' in visual <e2>dialog</e2> [7, 8] ."
sameAs(e1, e2)
Comment:

469	"one reason for this emergent behavior is that the space of possible next utterances in a <e1>dialog</e1> is highly multi-modal (there are many possible paths a <e2>dialog</e2> may take in the future)."
sameAs(e1, e2)
Comment:

470	"it is clear <e1>that</e1> novel training paradigms are needed; <e2>that</e2> is the focus of this paper."
sameAs(e1, e2)
Comment:

471	"one promising alternative to mle <e1>training</e1> proposed by recent work [36, 27] is sequence-level <e2>training</e2> of neural sequence models, specifically, using reinforcement learning to optimize taskspecific sequence metrics such as bleu [34] , rouge [24] , cider [48] ."
sameAs(e1, e2)
Comment:

472	"in this paper, inspired by the success of adversarial training [16] , we propose to train a generative visual <e1>dialog model</e1> (g) to produce sequences that score highly under a discriminative visual <e2>dialog model</e2> (d) ."
sameAs(e1, e2)
Comment:

473	"note that while our proposed approach is inspired by adversarial training, there are a number of subtle but crucial differences over <e1>generative adversarial networks</e1> (<e2>gans</e2>)."
sameAs(e1, e2)
Comment:

474	"unlike traditional gans, one novelty in our setup is that our discriminator receives a list of <e1>candidate</e1> responses and explicitly learns to reason about similarities and differences across <e2>candidates</e2>."
sameAs(e1, e2)
Comment:

475	"in this process, d <e1>learns</e1> a task-dependent perceptual similarity [12, 19, 15] and <e2>learns</e2> to recognize multiple correct responses in the feature space."
sameAs(e1, e2)
Comment:

476	"', besides the ground-truth answer 'no, i do not', d can also assign high scores to other options <e1>that</e1> are valid responses to the question, including the one generated by g: 'not <e2>that</e2> i can see'."
sameAs(e1, e2)
Comment:

477	"in <e1>that</e1> sense, our proposed approach may be viewed as an instance of 'knowledge transfer' [17, 5] from d to g. we employ a metric-learning loss function and a self-attention answer encoding mechanism for d <e2>that</e2> makes it particularly conducive to this knowledge transfer by encouraging perceptually meaningful similarities to emerge."
sameAs(e1, e2)
Comment:

478	"in that sense, our proposed approach may be viewed as an instance of '<e1>knowledge</e1> transfer' [17, 5] from d to g. we employ a metric-learning loss function and a self-attention answer encoding mechanism for d that makes it particularly conducive to this <e2>knowledge</e2> transfer by encouraging perceptually meaningful similarities to emerge."
sameAs(e1, e2)
Comment:

479	"in that sense, our proposed approach may be viewed as an instance of 'knowledge <e1>transfer</e1>' [17, 5] from d to g. we employ a metric-learning loss function and a self-attention answer encoding mechanism for d that makes it particularly conducive to this knowledge <e2>transfer</e2> by encouraging perceptually meaningful similarities to emerge."
sameAs(e1, e2)
Comment:

480	"our primary technical contribution is an end-to-end trainable generative visual dialog model, where the generator receives <e1>gradients</e1> from the discriminator loss of the sequence sampled from g. note that this is challenging because the output of g is a sequence of discrete symbols, which naïvely is not amenable to <e2>gradient</e2>-based training."
sameAs(e1, e2)
Comment:

481	"we propose to leverage the recently proposed gumbel-softmax (gs) approximation to the discrete distribution [18, 30] -specifically, a <e1>recurrent neural network</e1> (<e2>rnn</e2>) augmented with a sequence of gs samplers, which when coupled with the straight-through gradient estimator [2, 18] enables end-to-end differentiability."
sameAs(e1, e2)
Comment:

482	"specifically, our discriminator-trained g outperforms the mle-trained g by 1.7% on <e1>recall</e1>@5 on the visdial dataset, essentially improving over state-of-the-art [7] by 2.43% <e2>recall</e2>@5 and 2.67% recall@10. moreover, our generative model produces more diverse and informative responses (see table 3 )."
sameAs(e1, e2)
Comment:

483	"specifically, our discriminator-trained g outperforms the mle-trained g by 1.7% on <e1>recall</e1>@5 on the visdial dataset, essentially improving over state-of-the-art [7] by 2.43% recall@5 and 2.67% <e2>recall</e2>@10. moreover, our generative model produces more diverse and informative responses (see table 3 )."
sameAs(e1, e2)
Comment:

484	"specifically, our discriminator-trained g outperforms the mle-trained g by 1.7% on recall@5 on the visdial dataset, essentially improving over state-of-the-art [7] by 2.43% <e1>recall</e1>@5 and 2.67% <e2>recall</e2>@10. moreover, our generative model produces more diverse and informative responses (see table 3 )."
sameAs(e1, e2)
Comment:

485	"as a side contribution specific to this application, <e1>we</e1> introduce a novel encoder for neural visual dialog models, which maintains two separate memory banks -one for visual memory (where do <e2>we</e2> look in the image?)"
sameAs(e1, e2)
Comment:

486	"as a side contribution specific to this application, we introduce a novel encoder for neural visual dialog models, which maintains two separate <e1>memory</e1> banks -one for visual <e2>memory</e2> (where do we look in the image?)"
sameAs(e1, e2)
Comment:

487	"our work aims to achieve the best of both worlds -the practical usefulness of g and the strong performance of d -via knowledge transfer from d to g. our primary contribution is an <e1>end-to-end</e1> trainable generative visual dialog model, where g receives gradients from d as a perceptual (not adversarial) loss of the sequence sampled from g. we leverage the recently proposed gumbel-softmax (gs) approximation to the discrete distribution -specifically, a rnn augmented with a sequence of gs samplers, coupled with the straight-through gradient estimator to enable <e2>end-to-end</e2> differentiability."
sameAs(e1, e2)
Comment:

488	"our work aims to achieve the best of both worlds -the practical usefulness of g and the strong performance of d -via knowledge transfer from d to g. our primary contribution is an end-to-end trainable generative visual dialog model, where g receives <e1>gradients</e1> from d as a perceptual (not adversarial) loss of the sequence sampled from g. we leverage the recently proposed gumbel-softmax (gs) approximation to the discrete distribution -specifically, a rnn augmented with a sequence of gs samplers, coupled with the straight-through <e2>gradient</e2> estimator to enable end-to-end differentiability."
sameAs(e1, e2)
Comment:

489	"introduction <e1>one</e1> fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog agents -specifically, agents that can perceive or understand their environment (through vision, audio, or <e2>other</e2> sensors), and communicate their understanding with humans or other agents in natural language."
Conjunction(e1, e2)
Comment:

490	"introduction <e1>one</e1> fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog agents -specifically, agents that can perceive or understand their environment (through vision, audio, or other sensors), and communicate their understanding with humans or <e2>other</e2> agents in natural language."
Conjunction(e1, e2)
Comment:

491	"introduction one fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog <e1>agents</e1> -specifically, <e2>agents</e2> that can perceive or understand their environment (through vision, audio, or other sensors), and communicate their understanding with humans or other agents in natural language."
sameAs(e1, e2)
Comment:

492	"introduction one fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog <e1>agents</e1> -specifically, agents that can perceive or understand their environment (through vision, audio, or other sensors), and communicate their understanding with humans or other <e2>agents</e2> in natural language."
sameAs(e1, e2)
Comment:

493	"introduction one fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog agents -specifically, <e1>agents</e1> that can perceive or understand their environment (through vision, audio, or other sensors), and communicate their understanding with humans or other <e2>agents</e2> in natural language."
sameAs(e1, e2)
Comment:

494	"introduction one fundamental goal of artificial intelligence (ai) is the development of perceptually-grounded dialog agents -specifically, agents that can perceive or understand their environment (through vision, audio, or <e1>other</e1> sensors), and communicate their understanding with humans or <e2>other</e2> agents in natural language."
sameAs(e1, e2)
Comment:

495	"the question <e1>we</e1> try to address in this work is can <e2>we</e2> learn a multi-view stereo system?"
sameAs(e1, e2)
Comment:

496	"we design our <e1>system</e1> inspired by classical approaches while learning each component from data embedded in an end to end <e2>system</e2>."
sameAs(e1, e2)
Comment:

497	"classical multi-view stereopsis is traditionally able to handle both <e1>objects</e1> and scenes -we only showcase our system for the case of <e2>objects</e2> with scenes left for future work."
sameAs(e1, e2)
Comment:

498	"classical multi-view stereopsis is traditionally able to handle both objects and <e1>scenes</e1> -we only showcase our system for the case of objects with <e2>scenes</e2> left for future work."
sameAs(e1, e2)
Comment:

499	"we thoroughly evaluate our <e1>approach</e1> on the shapenet dataset and demonstrate the benefits over classical <e2>approaches</e2> and recent learning based methods."
Compare(e1, e2)
Comment:

500	"we thoroughly evaluate our <e1>approach</e1> on the shapenet dataset and demonstrate the benefits over classical approaches and recent learning based <e2>methods</e2>."
Compare(e1, e2)
Comment:

501	"abstract universal style <e1>transfer</e1> aims to <e2>transfer</e2> arbitrary visual styles to content images."
sameAs(e1, e2)
Comment:

502	"the seminal work by gatys et al [8, 9] show that the correlation between features, i.e., gram <e1>matrix</e1> or covariance <e2>matrix</e2> (shown to be as effective as gram matrix in [20] ), extracted by a trained deep neural network has remarkable ability of capturing visual styles."
sameAs(e1, e2)
Comment:

503	"the seminal work by gatys et al [8, 9] show that the correlation between features, i.e., gram <e1>matrix</e1> or covariance matrix (shown to be as effective as gram <e2>matrix</e2> in [20] ), extracted by a trained deep neural network has remarkable ability of capturing visual styles."
sameAs(e1, e2)
Comment:

504	"the seminal work by gatys et al [8, 9] show that the correlation between features, i.e., gram matrix or covariance <e1>matrix</e1> (shown to be as effective as gram <e2>matrix</e2> in [20] ), extracted by a trained deep neural network has remarkable ability of capturing visual styles."
sameAs(e1, e2)
Comment:

505	"despite the recent rapid progress, these existing works often trade off between generalization, quality and efficiency, which means that optimization-based methods can handle arbitrary styles with pleasing <e1>visual quality</e1> but at the expense of high computational costs, while feed-forward approaches can be executed efficiently but are limited to a fixed number of styles or compromised <e2>visual quality</e2>."
sameAs(e1, e2)
Comment:

506	"in each intermediate layer, our main goal is to transform the extracted content <e1>features</e1> such that they exhibit the same statistical characteristics as the style <e2>features</e2> of the same layer and we found that the classic signal whitening and coloring transforms (wcts) on those features are able to achieve this goal in an almost effortless manner."
sameAs(e1, e2)
Comment:

507	"in each intermediate layer, our main goal is to transform the extracted content <e1>features</e1> such that they exhibit the same statistical characteristics as the style features of the same layer and we found that the classic signal whitening and coloring transforms (wcts) on those <e2>features</e2> are able to achieve this goal in an almost effortless manner."
sameAs(e1, e2)
Comment:

508	"in each intermediate layer, our main goal is to transform the extracted content features such <e1>that</e1> they exhibit the same statistical characteristics as the style features of the same layer and we found <e2>that</e2> the classic signal whitening and coloring transforms (wcts) on those features are able to achieve this goal in an almost effortless manner."
sameAs(e1, e2)
Comment:

509	"in each intermediate layer, our main goal is to transform the extracted content features such that they exhibit the same statistical characteristics as the style <e1>features</e1> of the same layer and we found that the classic signal whitening and coloring transforms (wcts) on those <e2>features</e2> are able to achieve this goal in an almost effortless manner."
sameAs(e1, e2)
Comment:

510	"in each intermediate layer, our main goal is to transform the extracted content features such that they exhibit the same statistical characteristics as the style features of the same layer and we found that the classic signal whitening and coloring transforms (wcts) on <e1>those</e1> <e2>features</e2> are able to achieve this goal in an almost effortless manner."
Compare(e1, e2)
Comment:

511	"in this work, we first employ the vgg-19 network [26] as the feature extractor (encoder), and train a symmetric decoder to invert the vgg-19 features to the original <e1>image</e1>, which is essentially the <e2>image</e2> reconstruction task (figure 1(a) )."
sameAs(e1, e2)
Comment:

512	"to perform style transfer, we apply wct to one layer of content <e1>features</e1> such that its covariance matrix matches that of style <e2>features</e2>, as shown in figure 1 (b)."
sameAs(e1, e2)
Comment:

513	"to perform style transfer, we apply wct to one layer of content features such <e1>that</e1> its covariance matrix matches <e2>that</e2> of style features, as shown in figure 1 (b)."
sameAs(e1, e2)
Comment:

514	"in addition to this single-level stylization, <e1>we</e1> further develop a multi-level stylization pipeline, as depicted in figure 1 (c), where <e2>we</e2> apply wct sequentially to multiple feature layers."
sameAs(e1, e2)
Comment:

515	"we also introduce a control parameter <e1>that</e1> defines the degree of style transfer so <e2>that</e2> the users can choose the balance between stylization and content preservation."
sameAs(e1, e2)
Comment:

516	"note <e1>that</e1> this learning-free scheme is fundamentally different from existing feed-forward networks <e2>that</e2> require learning with pre-defined styles and fine-tuning for new styles."
sameAs(e1, e2)
Comment:

517	"the main contributions of this work are summarized as follows: • we propose to use <e1>feature</e1> transforms, i.e., whitening and coloring, to directly match content <e2>feature</e2> statistics to those of a style image in the deep feature space."
sameAs(e1, e2)
Comment:

518	"the whitening and coloring transforms reflect a direct matching of feature covariance of the content <e1>image</e1> to a given style <e2>image</e2>, which shares similar spirits with the optimization of gram matrix based cost in neural style transfer."
sameAs(e1, e2)
Comment:

519	"given a pair of examples, i.e., the content and style <e1>image</e1>, it aims to synthesize an <e2>image</e2> that preserves some notion of the content but carries characteristics of the style."
sameAs(e1, e2)
Comment:

520	"in the <e1>gan</e1> game, there is an unknown distribution p which we want to approximate using a parameterised distribution q. q is learned by a generator by finding a saddle point of a function which we summarize for now as f -<e2>gan</e2>(p, q), where f is a convex function (see eq."
sameAs(e1, e2)
Comment:

521	"in the gan game, there is an unknown distribution p which <e1>we</e1> want to approximate using a parameterised distribution q. q is learned by a generator by finding a saddle point of a function which <e2>we</e2> summarize for now as f -gan(p, q), where f is a convex function (see eq."
sameAs(e1, e2)
Comment:

522	"in the gan game, there is an unknown distribution p which we want to approximate using a parameterised distribution q. q is learned by a generator by finding a saddle point of a <e1>function</e1> which we summarize for now as f -gan(p, q), where f is a convex <e2>function</e2> (see eq."
sameAs(e1, e2)
Comment:

523	"a part of the generator's training involves as a subroutine a supervised adversary -hence, the saddle point formulation -called discriminator, which tries to guess whether randomly generated observations come from p or q. ideally, at the end of <e1>this</e1> supervised game, we want q to be close to p, and a good measure of <e2>this</e2> is the f -divergence i f (p q), also known as ali-silvey distance [1, 12] ."
sameAs(e1, e2)
Comment:

524	"we show that current deep <e1>architectures</e1> are able to factorize a very large number of such densities using an especially compact design, hence displaying the power of deep <e2>architectures</e2> and their concinnity in the f -gan game."
sameAs(e1, e2)
Comment:

525	"the key to our results is a variational generalization of an old theorem that relates the kl <e1>divergence</e1> between regular exponential families and <e2>divergences</e2> between their natural parameters."
sameAs(e1, e2)
Comment:

526	"we complete this picture with additional <e1>results</e1> and experimental insights on how these <e2>results</e2> may be used to ground further improvements of gan architectures, via (i) a principled design of the activation functions in the generator and (ii) an explicit integration of proper composite losses' link function in the discriminator."
sameAs(e1, e2)
Comment:

527	"toward multimodal <e1>image</e1>-to-<e2>image</e2> translation"
sameAs(e1, e2)
Comment:

528	"abstract many <e1>image</e1>-to-<e2>image</e2> translation problems are ambiguous, as a single input image may correspond to multiple possible outputs."
sameAs(e1, e2)
Comment:

529	"abstract many <e1>image</e1>-to-image translation problems are ambiguous, as a single input <e2>image</e2> may correspond to multiple possible outputs."
sameAs(e1, e2)
Comment:

530	"abstract many image-to-<e1>image</e1> translation problems are ambiguous, as a single input <e2>image</e2> may correspond to multiple possible outputs."
sameAs(e1, e2)
Comment:

531	"for example, networks have been used to inpaint missing image regions [20, 34, 47] , add color to grayscale <e1>images</e1> [19, 20, 27, 50] , and generate photorealistic <e2>images</e2> from sketches [20, 40] ."
sameAs(e1, e2)
Comment:

532	"we start with the pix2pix framework [20] , which has previously been shown to produce highquality results for various <e1>image</e1>-to-<e2>image</e2> translation tasks."
sameAs(e1, e2)
Comment:

533	"the method trains a generator network, conditioned on the input <e1>image</e1>, with two losses: (1) a regression loss to produce similar output to the known paired ground truth <e2>image</e2> and (2) a learned discriminator loss to encourage realism."
sameAs(e1, e2)
Comment:

534	"we not multimodal <e1>image</e1>-to-<e2>image</e2> translation using our proposed method: given an input image from one domain (night image of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

535	"we not multimodal <e1>image</e1>-to-image translation using our proposed method: given an input <e2>image</e2> from one domain (night image of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

536	"we not multimodal <e1>image</e1>-to-image translation using our proposed method: given an input image from one domain (night <e2>image</e2> of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

537	"we not multimodal image-to-<e1>image</e1> translation using our proposed method: given an input <e2>image</e2> from one domain (night image of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

538	"we not multimodal image-to-<e1>image</e1> translation using our proposed method: given an input image from one domain (night <e2>image</e2> of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

539	"we not multimodal image-to-image translation using our proposed <e1>method</e1>: given an input image from one domain (night image of a scene), we aim to <e2>model</e2> a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
Used-for(e1, e2)
Comment:

540	"we not multimodal image-to-image translation using our proposed method: given an input <e1>image</e1> from one domain (night <e2>image</e2> of a scene), we aim to model a distribution of potential outputs in the target domain (corresponding day images), producing both realistic and diverse results."
sameAs(e1, e2)
Comment:

541	"for example, when generating a day <e1>image</e1> from a night <e2>image</e2>, the latent vector may encode information about the sky color, lighting effects on the ground, and cloud patterns."
sameAs(e1, e2)
Comment:

542	"in <e1>this</e1> work, we instantiate <e2>this</e2> idea by exploring several objective functions, inspired by literature in unconditional generative modeling: • cvae-gan (conditional variational autoencoder gan): one approach is first encoding the ground truth image into the latent space, giving the generator a noisy "peek" into the desired output."
sameAs(e1, e2)
Comment:

543	"in this work, we instantiate this idea by exploring several objective functions, inspired by literature in unconditional generative modeling: • cvae-<e1>gan</e1> (conditional variational autoencoder <e2>gan</e2>): one approach is first encoding the ground truth image into the latent space, giving the generator a noisy "peek" into the desired output."
sameAs(e1, e2)
Comment:

544	"using this, along with the input <e1>image</e1>, the generator should be able to reconstruct the specific output <e2>image</e2>."
sameAs(e1, e2)
Comment:

545	"• clr-<e1>gan</e1> (conditional latent regressor <e2>gan</e2>): another approach is to first provide a randomly drawn latent vector to the generator."
sameAs(e1, e2)
Comment:

546	"this <e1>method</e1> could be seen as a conditional formulation of the "latent regressor" <e2>model</e2> [8, 10] and also related to infogan [4] ."
Used-for(e1, e2)
Comment:

547	"we explore several variants of this <e1>approach</e1> by employing different training objectives, network architectures, and <e2>methods</e2> of injecting the latent code."
Compare(e1, e2)
Comment:

548	"abstract in unsupervised domain mapping, the learner is given two unmatched <e1>datasets</e1> a and b. the goal is to learn a mapping g ab that translates a sample in a to the analog sample in b. recent <e2>approaches</e2> have shown that when learning simultaneously both g ab and the inverse mapping g ba , convincing mappings are obtained."
Used-for(e1, e2)
Comment:

549	"abstract in unsupervised domain mapping, the learner is given two unmatched datasets a and b. the goal is to learn a mapping g ab <e1>that</e1> translates a sample in a to the analog sample in b. recent approaches have shown <e2>that</e2> when learning simultaneously both g ab and the inverse mapping g ba , convincing mappings are obtained."
sameAs(e1, e2)
Comment:

550	"on the <e1>one</e1> extreme, fully supervised methods employ pairs of matched samples, <e2>one</e2> in each domain, in order to learn the mapping [9] ."
sameAs(e1, e2)
Comment:

551	"if the <e1>two</e1> domains are highly related, it was demonstrated that just by sharing weights between the networks working on the <e2>two</e2> domains, and without any further supervision, one can map samples between the two domains [21, 13] ."
sameAs(e1, e2)
Comment:

552	"if the <e1>two</e1> domains are highly related, it was demonstrated that just by sharing weights between the networks working on the two domains, and without any further supervision, one can map samples between the <e2>two</e2> domains [21, 13] ."
sameAs(e1, e2)
Comment:

553	"if the two <e1>domains</e1> are highly related, it was demonstrated that just by sharing weights between the networks working on the two <e2>domains</e2>, and without any further supervision, one can map samples between the two domains [21, 13] ."
sameAs(e1, e2)
Comment:

554	"if the two <e1>domains</e1> are highly related, it was demonstrated that just by sharing weights between the networks working on the two domains, and without any further supervision, one can map samples between the two <e2>domains</e2> [21, 13] ."
sameAs(e1, e2)
Comment:

555	"if the two domains are highly related, it was demonstrated that just by sharing weights between the networks working on the <e1>two</e1> domains, and without any further supervision, one can map samples between the <e2>two</e2> domains [21, 13] ."
sameAs(e1, e2)
Comment:

556	"if the two domains are highly related, it was demonstrated that just by sharing weights between the networks working on the two <e1>domains</e1>, and without any further supervision, one can map samples between the two <e2>domains</e2> [21, 13] ."
sameAs(e1, e2)
Comment:

557	"this is done by requiring circularity, i.e., that mapping a sample from <e1>one</e1> domain to the <e2>other</e2> and then back, produces the original sample."
Conjunction(e1, e2)
Comment:

558	"while there is no real practical reason not to do so, since training batches contain multiple samples, we demonstrate that similar constraints can even be applied per <e1>image</e1> by computing the distance between, e.g., the top part of the <e2>image</e2> and the bottom part."
sameAs(e1, e2)
Comment:

559	"out of the many tasks made possible by gans, the task of mapping an <e1>image</e1> in a source domain to the analog <e2>image</e2> in a target domain is of a particular interest."
sameAs(e1, e2)
Comment:

560	"abstract in this paper we introduce a natural <e1>image</e1> prior that directly represents a gaussiansmoothed version of the natural <e2>image</e2> distribution."
sameAs(e1, e2)
Comment:

561	"here, we propose an <e1>image</e1> prior that is directly based on an estimate of the natural <e2>image</e2> probability distribution."
sameAs(e1, e2)
Comment:

562	"although <e1>this</e1> seems like the most intuitive and straightforward idea to formulate a prior, only few previous techniques have taken <e2>this</e2> route [20] ."
sameAs(e1, e2)
Comment:

563	"most previous deep learning <e1>priors</e1> are derived in the context of specific algorithms to solve the restoration problem, but it is not clear how these <e2>priors</e2> relate to the probability distribution of natural images."
sameAs(e1, e2)
Comment:

564	"most previous deep learning priors are derived in the context of specific <e1>algorithms</e1> to solve the restoration <e2>problem</e2>, but it is not clear how these priors relate to the probability distribution of natural images."
Used-for(e1, e2)
Comment:

565	"note that <e1>we</e1> cannot hope to use the true image probability distribution itself as our prior, since <e2>we</e2> only have a finite set of samples from this distribution."
sameAs(e1, e2)
Comment:

566	"we approximate the estimator with a <e1>bound</e1>, and show that the gradient of the <e2>bound</e2> includes the gradient of the logarithm of our prior, that is, the gaussian smoothed density."
sameAs(e1, e2)
Comment:

567	"we approximate the estimator with a bound, and show <e1>that</e1> the gradient of the bound includes the gradient of the logarithm of our prior, <e2>that</e2> is, the gaussian smoothed density."
sameAs(e1, e2)
Comment:

568	"we approximate the estimator with a bound, and show that the <e1>gradient</e1> of the bound includes the <e2>gradient</e2> of the logarithm of our prior, that is, the gaussian smoothed density."
sameAs(e1, e2)
Comment:

569	"in addition, the gradient of the logarithm of the smoothed density is proportional to the <e1>mean-shift</e1> vector [8] , and it has recently been shown that denoising autoencoders (daes) learn such a <e2>mean-shift</e2> vector field for a given set of data samples [1, 4] ."
sameAs(e1, e2)
Comment:

570	"we include our prior in a formulation of <e1>image restoration</e1> as a bayes estimator that also allows us to solve noise-blind <e2>image restoration</e2> problems."
sameAs(e1, e2)
Comment:

571	"hence we call our <e1>prior</e1> a deep mean-shift <e2>prior</e2>, and our framework is an example of bayesian inference using deep learning."
sameAs(e1, e2)
Comment:

572	"in the last decades, several natural <e1>image</e1> priors have been proposed, including total variation [29] , gradient sparsity priors [12] , models based on <e2>image</e2> patches [5] , and gaussian mixtures of local filters [25] , just to name a few of the most successful ideas."
sameAs(e1, e2)
Comment:

573	"in the last decades, several natural image <e1>priors</e1> have been proposed, including total variation [29] , gradient sparsity <e2>priors</e2> [12] , models based on image patches [5] , and gaussian mixtures of local filters [25] , just to name a few of the most successful ideas."
sameAs(e1, e2)
Comment:

574	"beyond <e1>that</e1>, it is beneficial <e2>that</e2> the model incorporates the invariance of the molecular energy with respect to rotation, translation and atom indexing."
sameAs(e1, e2)
Comment:

575	"this work provides the following key contributions: • we propose continuous-filter convolutional (cfconv) <e1>layers</e1> as a means to move beyond grid-bound data such as <e2>images</e2> or audio towards modeling objects with arbitrary positions such as astronomical observations or atoms in molecules and materials."
Part-of(e1, e2)
Comment:

576	"an inductive approach to generating <e1>node</e1> embeddings also facilitates generalization across <e2>graphs</e2> with the same form of features: for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model."
Part-of(e1, e2)
Comment:

577	"an inductive approach to generating <e1>node</e1> embeddings also facilitates generalization across graphs with the same form of features: for example, one could train an embedding generator on protein-protein interaction <e2>graphs</e2> derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model."
Part-of(e1, e2)
Comment:

578	"an inductive approach to generating <e1>node</e1> embeddings also facilitates generalization across graphs with the same form of features: for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce <e2>node</e2> embeddings for data collected on new organisms using the trained model."
sameAs(e1, e2)
Comment:

579	"an inductive approach to generating node <e1>embeddings</e1> also facilitates generalization across graphs with the same form of features: for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node <e2>embeddings</e2> for data collected on new organisms using the trained model."
sameAs(e1, e2)
Comment:

580	"an inductive approach to generating node embeddings also facilitates generalization across <e1>graphs</e1> with the same form of features: for example, one could train an embedding generator on protein-protein interaction <e2>graphs</e2> derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained model."
sameAs(e1, e2)
Comment:

581	"an inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of <e1>features</e1>: for example, one could train an embedding generator on protein-protein interaction graphs derived from a <e2>model</e2> organism, and then easily produce node embeddings for data collected on new organisms using the trained model."
Used-for(e1, e2)
Comment:

582	"an inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of <e1>features</e1>: for example, one could train an embedding generator on protein-protein interaction graphs derived from a model organism, and then easily produce node embeddings for data collected on new organisms using the trained <e2>model</e2>."
Used-for(e1, e2)
Comment:

583	"an inductive approach to generating node embeddings also facilitates generalization across graphs with the same form of features: for example, one could train an embedding generator on protein-protein interaction graphs derived from a <e1>model</e1> organism, and then easily produce node embeddings for data collected on new organisms using the trained <e2>model</e2>."
sameAs(e1, e2)
Comment:

584	"the inductive <e1>node</e1> embedding problem is especially difficult, compared to the transductive setting, because generalizing to unseen nodes requires "aligning" newly observed subgraphs to the <e2>node</e2> embeddings that the algorithm has already optimized on."
sameAs(e1, e2)
Comment:

585	"an inductive framework must learn to recognize structural properties of a <e1>node</e1>'s neighborhood that reveal both the <e2>node</e2>'s local role in the graph, as well as its global position."
sameAs(e1, e2)
Comment:

586	"an inductive framework must learn to recognize structural properties of a <e1>node</e1>'s neighborhood that reveal both the node's local role in the <e2>graph</e2>, as well as its global position."
Part-of(e1, e2)
Comment:

587	"an inductive framework must learn to recognize structural properties of a node's neighborhood that reveal both the <e1>node</e1>'s local role in the <e2>graph</e2>, as well as its global position."
Part-of(e1, e2)
Comment:

588	"the majority of these approaches directly optimize the embeddings for each <e1>node</e1> using matrix-factorization-based objectives, and do not naturally generalize to unseen data, since they make predictions on nodes in a single, fixed <e2>graph</e2> [5, 11, 23, 28, 35, 36, 37, 39] ."
Part-of(e1, e2)
Comment:

589	"so far, <e1>graph</e1> convolutional networks (gcns) have only been applied in the transductive setting with fixed <e2>graphs</e2> [17, 18] ."
sameAs(e1, e2)
Comment:

590	"in this work we both extend gcns to the task of inductive unsupervised learning and propose a <e1>framework</e1> that generalizes the gcn <e2>approach</e2> to use trainable aggregation functions (beyond simple convolutions)."
Used-for(e1, e2)
Comment:

591	"however, most existing <e1>approaches</e1> require that all nodes in the graph are present during training of the embeddings; these previous <e2>approaches</e2> are inherently transductive and do not naturally generalize to unseen nodes."
sameAs(e1, e2)
Comment:

592	"however, most existing approaches require that all <e1>nodes</e1> in the graph are present during training of the embeddings; these previous approaches are inherently transductive and do not naturally generalize to unseen <e2>nodes</e2>."
sameAs(e1, e2)
Comment:

593	"unlike embedding approaches <e1>that</e1> are based on matrix factorization, we leverage node features (e.g., text attributes, node profile information, node degrees) in order to learn an embedding function <e2>that</e2> generalizes to unseen nodes."
sameAs(e1, e2)
Comment:

594	"unlike embedding approaches that are based on matrix factorization, we leverage <e1>node</e1> features (e.g., text attributes, <e2>node</e2> profile information, node degrees) in order to learn an embedding function that generalizes to unseen nodes."
sameAs(e1, e2)
Comment:

595	"unlike embedding approaches that are based on matrix factorization, we leverage <e1>node</e1> features (e.g., text attributes, node profile information, <e2>node</e2> degrees) in order to learn an embedding function that generalizes to unseen nodes."
sameAs(e1, e2)
Comment:

596	"unlike embedding approaches that are based on matrix factorization, we leverage node features (e.g., text attributes, <e1>node</e1> profile information, <e2>node</e2> degrees) in order to learn an embedding function that generalizes to unseen nodes."
sameAs(e1, e2)
Comment:

597	"by incorporating <e1>node</e1> features in the learning algorithm, we simultaneously learn the topological structure of each <e2>node</e2>'s neighborhood as well as the distribution of node features in the neighborhood."
sameAs(e1, e2)
Comment:

598	"by incorporating <e1>node</e1> features in the learning algorithm, we simultaneously learn the topological structure of each node's neighborhood as well as the distribution of <e2>node</e2> features in the neighborhood."
sameAs(e1, e2)
Comment:

599	"by incorporating node <e1>features</e1> in the learning algorithm, we simultaneously learn the topological structure of each node's neighborhood as well as the distribution of node <e2>features</e2> in the neighborhood."
sameAs(e1, e2)
Comment:

600	"by incorporating node features in the learning algorithm, we simultaneously learn the topological structure of each <e1>node</e1>'s neighborhood as well as the distribution of <e2>node</e2> features in the neighborhood."
sameAs(e1, e2)
Comment:

601	"by incorporating node features in the learning algorithm, we simultaneously learn the topological structure of each node's <e1>neighborhood</e1> as well as the distribution of node features in the <e2>neighborhood</e2>."
sameAs(e1, e2)
Comment:

602	"while we focus on feature-rich <e1>graphs</e1> (e.g., citation data with text attributes, biological data with functional/molecular markers), our approach can also make use of structural features that are present in all <e2>graphs</e2> (e.g., node degrees)."
sameAs(e1, e2)
Comment:

603	"while we focus on feature-rich graphs (e.g., citation <e1>data</e1> with text attributes, biological <e2>data</e2> with functional/molecular markers), our approach can also make use of structural features that are present in all graphs (e.g., node degrees)."
sameAs(e1, e2)
Comment:

604	"instead of training a distinct embedding vector for each <e1>node</e1>, we train a set of aggregator functions that learn to aggregate feature information from a <e2>node</e2>'s local neighborhood (figure 1 )."
sameAs(e1, e2)
Comment:

605	"here we present graphsage, a general inductive framework that leverages <e1>node</e1> feature information (e.g., text attributes) to efficiently generate <e2>node</e2> embeddings for previously unseen data."
sameAs(e1, e2)
Comment:

606	"we use two evolving document graphs based on citation <e1>data</e1> and reddit post <e2>data</e2> (predicting paper and post categories, respectively), and a multigraph generalization experiment based on a dataset of protein-protein interactions (predicting protein functions)."
sameAs(e1, e2)
Comment:

607	"we use two evolving document graphs based on citation data and reddit post data (<e1>predicting</e1> paper and post categories, respectively), and a multigraph generalization experiment based on a dataset of protein-protein interactions (<e2>predicting</e2> protein functions)."
sameAs(e1, e2)
Comment:

608	"using these benchmarks, we show that our <e1>approach</e1> is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised <e2>approach</e2> improves classification f1-scores by an average of 51% compared to using node features alone and graphsage consistently outperforms a strong, transductive baseline [28] , despite this baseline taking ∼100× longer to run on unseen nodes."
sameAs(e1, e2)
Comment:

609	"using these benchmarks, we show that our approach is able to effectively generate representations for unseen <e1>nodes</e1> and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification f1-scores by an average of 51% compared to using node features alone and graphsage consistently outperforms a strong, transductive baseline [28] , despite this baseline taking ∼100× longer to run on unseen <e2>nodes</e2>."
sameAs(e1, e2)
Comment:

610	"using these benchmarks, we show that our approach is able to effectively generate representations for unseen nodes and outperform relevant baselines by a significant margin: across domains, our supervised approach improves classification f1-scores by an average of 51% compared to using node features alone and graphsage consistently outperforms a strong, transductive <e1>baseline</e1> [28] , despite this <e2>baseline</e2> taking ∼100× longer to run on unseen nodes."
sameAs(e1, e2)
Comment:

611	"lastly, we probe the expressive capability of our approach and show, through theoretical analysis, <e1>that</e1> graphsage is capable of learning structural information about a node's role in a graph, despite the fact <e2>that</e2> it is inherently based on features (section 5)."
sameAs(e1, e2)
Comment:

612	"lastly, we probe the expressive capability of our approach and show, through theoretical analysis, that graphsage is capable of learning structural information about a <e1>node</e1>'s role in a <e2>graph</e2>, despite the fact that it is inherently based on features (section 5)."
Part-of(e1, e2)
Comment:

613	"instead of training individual <e1>embeddings</e1> for each node, we learn a function that generates <e2>embeddings</e2> by sampling and aggregating features from a node's local neighborhood."
sameAs(e1, e2)
Comment:

614	"instead of training individual embeddings for each <e1>node</e1>, we learn a function that generates embeddings by sampling and aggregating features from a <e2>node</e2>'s local neighborhood."
sameAs(e1, e2)
Comment:

615	"our <e1>algorithm</e1> outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and reddit post data, and we show that our <e2>algorithm</e2> generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
sameAs(e1, e2)
Comment:

616	"our algorithm outperforms strong baselines on three inductive <e1>node</e1>-classification benchmarks: we classify the category of unseen nodes in evolving information <e2>graphs</e2> based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
Part-of(e1, e2)
Comment:

617	"our algorithm outperforms strong baselines on three inductive <e1>node</e1>-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen <e2>graphs</e2> using a multi-graph dataset of protein-protein interactions."
Part-of(e1, e2)
Comment:

618	"our algorithm outperforms strong baselines on three inductive <e1>node</e1>-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-<e2>graph</e2> dataset of protein-protein interactions."
Part-of(e1, e2)
Comment:

619	"our algorithm outperforms strong baselines on three inductive node-classification benchmarks: <e1>we</e1> classify the category of unseen nodes in evolving information graphs based on citation and reddit post data, and <e2>we</e2> show that our algorithm generalizes to completely unseen graphs using a multi-graph dataset of protein-protein interactions."
sameAs(e1, e2)
Comment:

620	"our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information <e1>graphs</e1> based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen <e2>graphs</e2> using a multi-graph dataset of protein-protein interactions."
sameAs(e1, e2)
Comment:

621	"our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information <e1>graphs</e1> based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen graphs using a multi-<e2>graph</e2> dataset of protein-protein interactions."
sameAs(e1, e2)
Comment:

622	"our algorithm outperforms strong baselines on three inductive node-classification benchmarks: we classify the category of unseen nodes in evolving information graphs based on citation and reddit post data, and we show that our algorithm generalizes to completely unseen <e1>graphs</e1> using a multi-<e2>graph</e2> dataset of protein-protein interactions."
sameAs(e1, e2)
Comment:

623	"introduction low-dimensional vector embeddings of nodes in large <e1>graphs</e1> 1 have proved extremely useful as feature inputs for a wide variety of prediction and <e2>graph</e2> analysis tasks [5, 11, 28, 35, 36] ."
sameAs(e1, e2)
Comment:

624	"the basic idea behind <e1>node</e1> embedding approaches is to use dimensionality reduction techniques to distill the high-dimensional information about a <e2>node</e2>'s neighborhood into a dense vector embedding."
sameAs(e1, e2)
Comment:

625	"these <e1>node</e1> embeddings can then be fed to downstream machine learning systems and aid in tasks such as <e2>node</e2> classification, clustering, and link prediction [11, 28, 35] ."
sameAs(e1, e2)
Comment:

626	"however, previous works have focused on embedding <e1>nodes</e1> from a single fixed graph, and many real-world applications require embeddings to be quickly generated for unseen <e2>nodes</e2>, or entirely new (sub)graphs."
sameAs(e1, e2)
Comment:

627	"however, previous works have focused on embedding nodes from a single fixed <e1>graph</e1>, and many real-world applications require embeddings to be quickly generated for unseen nodes, or entirely new (sub)<e2>graphs</e2>."
sameAs(e1, e2)
Comment:

628	"rather than minimizing the classification cost at the zero-dimensional <e1>data</e1> points of the input space, the regularized model minimizes the cost on a manifold around each <e2>data</e2> point, thus pushing decision boundaries away from the labeled data points (figure 1b )."
sameAs(e1, e2)
Comment:

629	"rather than minimizing the classification cost at the zero-dimensional <e1>data</e1> points of the input space, the regularized model minimizes the cost on a manifold around each data point, thus pushing decision boundaries away from the labeled <e2>data</e2> points (figure 1b )."
sameAs(e1, e2)
Comment:

630	"rather than minimizing the classification cost at the zero-dimensional data points of the input space, the regularized model minimizes the cost on a manifold around each <e1>data</e1> point, thus pushing decision boundaries away from the labeled <e2>data</e2> points (figure 1b )."
sameAs(e1, e2)
Comment:

631	"it maintains an exponential moving average of label <e1>predictions</e1> on each training example, and penalizes <e2>predictions</e2> that are inconsistent with this target."
sameAs(e1, e2)
Comment:

632	"another approach is to choose the teacher <e1>model</e1> carefully instead of barely replicating the student <e2>model</e2>."
sameAs(e1, e2)
Comment:

633	"our goal, then, is to form a better teacher <e1>model</e1> from the student <e2>model</e2> without additional training."
sameAs(e1, e2)
Comment:

634	"laine & aila [13] named the <e1>method</e1> the ⇧ <e2>model</e2>; we will use this name for it and their version of it as the basis of our experiments."
Used-for(e1, e2)
Comment:

635	"at each training step, all the ema <e1>predictions</e1> of the examples in that minibatch are updated based on the new <e2>predictions</e2>."
sameAs(e1, e2)
Comment:

636	"consequently, the ema prediction of each example is formed by an ensemble of the <e1>model</e1>'s current version and <e2>those</e2> earlier versions that evaluated the same example."
Compare(e1, e2)
Comment:

637	"to overcome this problem, we propose mean teacher, a <e1>method</e1> that averages <e2>model</e2> weights instead of label predictions."
Used-for(e1, e2)
Comment:

638	"this ensembling improves the quality of the <e1>predictions</e1>, and using them as the teacher <e2>predictions</e2> improves results."
sameAs(e1, e2)
Comment:

639	"(one could evaluate all the targets periodically more than once <e1>per</e1> epoch, but keeping the evaluation span constant would require o(n 2 ) evaluations <e2>per</e2> epoch where n is the number of training examples.)"
sameAs(e1, e2)
Comment:

640	"the training of large-scale models with huge amounts of <e1>data</e1> are often carried on distributed systems [ [7] [8] [9] , where <e2>data</e2> parallelism is adopted to exploit the compute capability empowered by multiple workers [10] ."
sameAs(e1, e2)
Comment:

641	"in realizing the <e1>data</e1> parallelism of sgd, model copies in computing workers are trained in parallel by applying different subsets of <e2>data</e2>."
sameAs(e1, e2)
Comment:

642	"a centralized parameter server performs <e1>gradient</e1> synchronization by collecting all <e2>gradients</e2> and averaging them to update parameters."
sameAs(e1, e2)
Comment:

643	"for instance, aji and heafield [27] proposed to heuristically sparsify dense <e1>gradients</e1> by dropping off small values in order to reduce <e2>gradient</e2> communication."
sameAs(e1, e2)
Comment:

644	"our work belongs to the category of gradient quantization, which is an orthogonal <e1>approach</e1> to sparsity <e2>methods</e2>."
Compare(e1, e2)
Comment:

645	"we propose terngrad that quantizes <e1>gradients</e1> to ternary levels {−1, 0, 1} to reduce the overhead of <e2>gradient</e2> synchronization."
sameAs(e1, e2)
Comment:

646	"comparing with previous works, our major contributions include: (1) <e1>we</e1> use ternary values for gradients to reduce communication; (2) <e2>we</e2> mathematically prove the convergence of terngrad in general by proposing a statistical bound on gradients; (3) we propose layer-wise ternarizing and gradient clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

647	"comparing with previous works, our major contributions include: (1) <e1>we</e1> use ternary values for gradients to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical bound on gradients; (3) <e2>we</e2> propose layer-wise ternarizing and gradient clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

648	"comparing with previous works, our major contributions include: (1) we use ternary values for <e1>gradients</e1> to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical bound on <e2>gradients</e2>; (3) we propose layer-wise ternarizing and gradient clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

649	"comparing with previous works, our major contributions include: (1) we use ternary values for <e1>gradients</e1> to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical bound on gradients; (3) we propose layer-wise ternarizing and <e2>gradient</e2> clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

650	"comparing with previous works, our major contributions include: (1) we use ternary values for gradients to reduce communication; (2) <e1>we</e1> mathematically prove the convergence of terngrad in general by proposing a statistical bound on gradients; (3) <e2>we</e2> propose layer-wise ternarizing and gradient clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

651	"comparing with previous works, our major contributions include: (1) we use ternary values for gradients to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical <e1>bound</e1> on gradients; (3) we propose layer-wise ternarizing and gradient clipping to move this <e2>bound</e2> closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

652	"comparing with previous works, our major contributions include: (1) we use ternary values for gradients to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical <e1>bound</e1> on gradients; (3) we propose layer-wise ternarizing and gradient clipping to move this bound closer toward the <e2>bound</e2> of standard sgd."
sameAs(e1, e2)
Comment:

653	"comparing with previous works, our major contributions include: (1) we use ternary values for gradients to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical bound on <e1>gradients</e1>; (3) we propose layer-wise ternarizing and <e2>gradient</e2> clipping to move this bound closer toward the bound of standard sgd."
sameAs(e1, e2)
Comment:

654	"comparing with previous works, our major contributions include: (1) we use ternary values for gradients to reduce communication; (2) we mathematically prove the convergence of terngrad in general by proposing a statistical bound on gradients; (3) we propose layer-wise ternarizing and gradient clipping to move this <e1>bound</e1> closer toward the <e2>bound</e2> of standard sgd."
sameAs(e1, e2)
Comment:

655	"our experiments show that applying terngrad on alexnet doesn't incur any <e1>accuracy</e1> loss and can even improve <e2>accuracy</e2>."
sameAs(e1, e2)
Comment:

656	"we prove <e1>that</e1> neuralfdr has strong false discovery rate (fdr) guarantees, and show <e2>that</e2> it makes substantially more discoveries in synthetic and real datasets."
sameAs(e1, e2)
Comment:

657	"we have powerful procedures to systematically reject hypotheses while controlling the false discovery rate (fdr) note <e1>that</e1> here the convention is <e2>that</e2> a "discovery" corresponds to a "rejected" null hypothesis."
sameAs(e1, e2)
Comment:

658	"popular procedures for multiple <e1>hypotheses</e1> testing correspond to having one constant threshold for all the <e2>hypotheses</e2> (bh [3] ), or a constant for each group of hypotheses (group bh [13] , ihw [14, 15] )."
sameAs(e1, e2)
Comment:

659	"popular procedures for multiple <e1>hypotheses</e1> testing correspond to having one constant threshold for all the hypotheses (bh [3] ), or a constant for each group of <e2>hypotheses</e2> (group bh [13] , ihw [14, 15] )."
sameAs(e1, e2)
Comment:

660	"popular procedures for multiple hypotheses testing correspond to having one constant threshold for all the <e1>hypotheses</e1> (bh [3] ), or a constant for each group of <e2>hypotheses</e2> (group bh [13] , ihw [14, 15] )."
sameAs(e1, e2)
Comment:

661	"we provide extensive simulation on synthetic and <e1>real datasets</e1> to demonstrate that our <e2>algorithm</e2> makes more discoveries while controlling fdr compared to state-of-the-art methods."
Evaluate-for(e1, e2)
Comment:

662	"in contrast, the currently widely-used algorithms either ignore the hypothesis <e1>features</e1> (bh [3] , storey's bh [21] ) or are designed for simple discrete <e2>features</e2> (group bh [13] , ihw [15] )."
sameAs(e1, e2)
Comment:

663	"we show <e1>that</e1> neuralfdr controls false discovery with high probability for independent hypotheses and asymptotically under weak dependence [13, 21] , and we demonstrate on both synthetic and real datasets <e2>that</e2> it controls fdr while making substantially more discoveries."
sameAs(e1, e2)
Comment:

664	"the focus there is the <e1>framework</e1> rather than the end-to-end <e2>algorithm</e2> as compared to nueralfdr."
Used-for(e1, e2)
Comment:

665	"for the empirical experiment, sabha <e1>estimates</e1> the null proportion using non-parametric methods while adapt <e2>estimates</e2> the distribution of the p-value and the features with a two-group gamma glm mixture model and spline regression."
sameAs(e1, e2)
Comment:

666	"for the empirical experiment, sabha estimates the null proportion using non-parametric methods while adapt estimates the distribution of the p-value and the <e1>features</e1> with a two-group gamma glm mixture <e2>model</e2> and spline regression."
Used-for(e1, e2)
Comment:

667	"however popular empirically-validated testing approaches, such as benjamini-hochberg's procedure (bh) and independent hypothesis weighting (ihw), either ignore these <e1>features</e1> or assume that the <e2>features</e2> are categorical or uni-variate."
sameAs(e1, e2)
Comment:

668	"we parametrize the discovery threshold as a neural network, which enables flexible handling of multi-dimensional <e1>discrete</e1> and <e2>continuous</e2> features as well as efficient end-to-end optimization."
Conjunction(e1, e2)
Comment:

669	"this fundamental idea of <e1>task</e1> relatedness has motivated a variety of methods, including multi-<e2>task</e2> feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-task relationship learning that models inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

670	"this fundamental idea of <e1>task</e1> relatedness has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-<e2>task</e2> relationship learning that models inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

671	"this fundamental idea of <e1>task</e1> relatedness has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-task relationship learning that models inherent <e2>task</e2> relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

672	"this fundamental idea of task relatedness has motivated a variety of <e1>methods</e1>, including multi-task feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-task relationship learning that <e2>models</e2> inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
Compare(e1, e2)
Comment:

673	"this fundamental idea of task relatedness has motivated a variety of methods, including multi-<e1>task</e1> feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-<e2>task</e2> relationship learning that models inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

674	"this fundamental idea of task relatedness has motivated a variety of methods, including multi-<e1>task</e1> feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-task relationship learning that models inherent <e2>task</e2> relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

675	"this fundamental idea of task relatedness has motivated a variety of methods, including multi-task <e1>feature</e1> learning that learns a shared <e2>feature</e2> representation [1, 2, 6, 5, 23] , and multi-task relationship learning that models inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

676	"this fundamental idea of task relatedness has motivated a variety of methods, including multi-task feature learning <e1>that</e1> learns a shared feature representation [1, 2, 6, 5, 23] , and multi-task relationship learning <e2>that</e2> models inherent task relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

677	"this fundamental idea of task relatedness has motivated a variety of methods, including multi-task feature learning that learns a shared feature representation [1, 2, 6, 5, 23] , and multi-<e1>task</e1> relationship learning that models inherent <e2>task</e2> relationship [10, 14, 29, 31, 15, 17, 8] ."
sameAs(e1, e2)
Comment:

678	"learning inherent <e1>task</e1> relatedness is a hard problem, since the training <e2>data</e2> of different tasks may be sampled from different distributions and fitted by different models."
Conjunction(e1, e2)
Comment:

679	"learning inherent task relatedness is a hard problem, since the training <e1>data</e1> of different tasks may be sampled from different distributions and fitted by different <e2>models</e2>."
Used-for(e1, e2)
Comment:

680	"unfortunately, if cross-<e1>task</e1> knowledge transfer is impossible, then we will overfit each <e2>task</e2> due to limited amount of labeled data."
sameAs(e1, e2)
Comment:

681	"unfortunately, if cross-<e1>task</e1> knowledge transfer is impossible, then we will overfit each task due to limited amount of labeled <e2>data</e2>."
Conjunction(e1, e2)
Comment:

682	"unfortunately, if cross-task knowledge transfer is impossible, then we will overfit each <e1>task</e1> due to limited amount of labeled <e2>data</e2>."
Conjunction(e1, e2)
Comment:

683	"recent research also reveals that deep features eventually transition from general to specific along the network, and <e1>feature</e1> transferability drops significantly in higher layers with increasing task dissimilarity [28] , hence the sharing of all <e2>feature</e2> layers may be risky to negativetransfer."
sameAs(e1, e2)
Comment:

684	"recent research also reveals that deep features eventually transition from general to specific along the network, and feature transferability drops significantly in higher <e1>layers</e1> with increasing task dissimilarity [28] , hence the sharing of all feature <e2>layers</e2> may be risky to negativetransfer."
sameAs(e1, e2)
Comment:

685	"since deep features eventually transition from general to specific along deep networks, a fundamental problem of multi-task learning is how to exploit the <e1>task</e1> relatedness underlying parameter tensors and improve feature transferability in the multiple <e2>task</e2>-specific layers."
sameAs(e1, e2)
Comment:

686	"therefore, it remains an open problem how to exploit the <e1>task</e1> relationship across different deep networks while improving the feature transferability in <e2>task</e2>-specific layers of the deep networks."
sameAs(e1, e2)
Comment:

687	"therefore, it remains an open problem how to exploit the task relationship across different <e1>deep networks</e1> while improving the feature transferability in task-specific layers of the <e2>deep networks</e2>."
sameAs(e1, e2)
Comment:

688	"this paper presents multilinear relationship network (mrn) for multi-task learning, which discovers the <e1>task</e1> relationships based on multiple <e2>task</e2>-specific layers of deep convolutional neural networks."
sameAs(e1, e2)
Comment:

689	"since the <e1>parameters</e1> of deep networks are natively tensors, the tensor normal distribution [21] is explored for multi-task learning, which is imposed as the prior distribution over network <e2>parameters</e2> of all task-specific layers to learn find-grained multilinear relationships of tasks, classes and features."
sameAs(e1, e2)
Comment:

690	"by jointly learning transferable features and multilinear relationships, mrn is able to circumvent the dilemma of negative-<e1>transfer</e1> in feature layers and under-<e2>transfer</e2> in classifier layer."
sameAs(e1, e2)
Comment:

691	"this paper presents multilinear relationship networks (mrn) that discover the <e1>task</e1> relationships based on novel tensor normal priors over parameter tensors of multiple <e2>task</e2>-specific layers in deep convolutional networks."
sameAs(e1, e2)
Comment:

692	"by jointly learning transferable <e1>features</e1> and multilinear relationships of tasks and <e2>features</e2>, mrn is able to alleviate the dilemma of negativetransfer in the feature layers and under-transfer in the classifier layer."
sameAs(e1, e2)
Comment:

693	"multi-<e1>task</e1> learning is based on the idea that the performance of one <e2>task</e2> can be improved using related tasks as inductive bias [4] ."
sameAs(e1, e2)
Comment:

694	"knowing the <e1>task</e1> relationship should enable the transfer of shared knowledge from relevant tasks such that only <e2>task</e2>-specific features need to be learned."
sameAs(e1, e2)
Comment:

695	"introduction for quite a few years, deep neural networks (dnns) have persistently enabled significant improvements in many application domains, such as object recognition from images (he et al, 2016) ; <e1>speech recognition</e1> (amodei et al, 2015) ; <e2>natural language processing</e2> (luong et al, 2015) and computer games control using reinforcement learning (silver et al, 2016; mnih et al, 2015) ."
Conjunction(e1, e2)
Comment:

696	"it has been argued <e1>that</e1> saddle-points can be avoided (ge et al, 2015) and <e2>that</e2> "bad" local minima in the training error vanish exponentially (dauphin et al, 2014; choromanska et al, 2015; ."
sameAs(e1, e2)
Comment:

697	"in <e1>this</e1> study we suggest a first attempt to tackle <e2>this</e2> issue."
sameAs(e1, e2)
Comment:

698	"first, • <e1>we</e1> propose that the initial learning phase can be described using a high-dimensional "random walk on a random potential" process, with an "ultra-slow" logarithmic increase in the distance of the weights from their initialization, as <e2>we</e2> observe empirically."
sameAs(e1, e2)
Comment:

699	"• there is no inherent "generalization gap": large-batch <e1>training</e1> can generalize as well as small batch <e2>training</e2> by adapting the number of iterations."
sameAs(e1, e2)
Comment:

700	"abstract in this paper, we analyze the numerics of common algorithms for training <e1>generative adversarial networks</e1> (<e2>gans</e2>)."
sameAs(e1, e2)
Comment:

701	"utilizing <e1>these</e1> insights, we design a new algorithm that overcomes some of <e2>these</e2> problems."
sameAs(e1, e2)
Comment:

702	"experimentally, we show <e1>that</e1> our algorithm leads to stable training on many gan architectures, including some <e2>that</e2> are known to be hard to train."
sameAs(e1, e2)
Comment:

703	"using <e1>these</e1> findings, we design a new algorithm that overcomes some of <e2>these</e2> limitations and has better convergence properties."
sameAs(e1, e2)
Comment:

704	"experimentally, we demonstrate its superiority on training common <e1>gan</e1> architectures and show convergence on <e2>gan</e2> architectures that are known to be notoriously hard to train."
sameAs(e1, e2)
Comment:

705	"experimentally, we demonstrate its superiority on training common gan <e1>architectures</e1> and show convergence on gan <e2>architectures</e2> that are known to be notoriously hard to train."
sameAs(e1, e2)
Comment:

706	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) [10] have been very successful in learning probability distributions."
sameAs(e1, e2)
Comment:

707	"since their first appearance, gans have been successfully applied to a variety of tasks, including <e1>image</e1>-to-<e2>image</e2> translation [12] , image super-resolution [13] , image in-painting [27] domain adaptation [26] , probabilistic inference [14, 9, 8] and many more."
sameAs(e1, e2)
Comment:

708	"abstract deep generative models based on <e1>generative adversarial networks</e1> (<e2>gans</e2>) have demonstrated impressive sample quality but in order to work they require a careful choice of architecture, parameter initialization, and selection of hyper-parameters."
sameAs(e1, e2)
Comment:

709	"we demonstrate the effectiveness of this regularizer accross several architectures trained on common benchmark image <e1>generation</e1> <e2>tasks</e2>."
isA(e1, e2)
Comment:

710	"with respect to the optimization process, <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) has proved itself to be a key component of the deep learning success, but its effectiveness strictly depends on the choice of the initial learning rate and learning rate schedule."
sameAs(e1, e2)
Comment:

711	"in section 5, we present a novel strategy to bet on a coin that extends previous ones in a data-dependent way, proving optimal convergence rate in the <e1>convex</e1> and quasi-<e2>convex</e2> setting (defined in section 3)."
sameAs(e1, e2)
Comment:

712	"contrary to previous methods, <e1>we</e1> do not adapt the learning rates nor <e2>we</e2> make use of the assumed curvature of the objective function."
sameAs(e1, e2)
Comment:

713	"theoretical convergence is proven for <e1>convex</e1> and quasi-<e2>convex</e2> functions and empirical evidence shows the advantage of our algorithm over popular stochastic gradient algorithms."
sameAs(e1, e2)
Comment:

714	"abstract generative moment matching <e1>network</e1> (gmmn) is a deep generative model that differs from generative adversarial <e2>network</e2> (gan) by replacing the discriminator in gan with a two-sample test based on kernel maximum mean discrepancy (mmd)."
sameAs(e1, e2)
Comment:

715	"abstract generative moment matching network (gmmn) is a deep generative model that differs from generative adversarial network (<e1>gan</e1>) by replacing the discriminator in <e2>gan</e2> with a two-sample test based on kernel maximum mean discrepancy (mmd)."
sameAs(e1, e2)
Comment:

716	"rather than estimating the density of p x , generative adversarial <e1>network</e1> (gan) [5] starts from a base distribution p z over z, such as gaussian distribution, then trains a transformation <e2>network</e2> g ✓ such that p ✓ ⇡ p x , where p ✓ is the underlying distribution of g ✓ (z) and z ⇠ p z ."
sameAs(e1, e2)
Comment:

717	"instead of training an auxiliary <e1>network</e1> f for measuring the distance between p x and p ✓ , generative moment matching <e2>network</e2> (gmmn) [9, 10] uses kernel maximum mean discrepancy (mmd) [11] , which is the centerpiece of nonparametric two-sample test, to determine the distribution distances."
sameAs(e1, e2)
Comment:

718	"instead of training an auxiliary network f for measuring the <e1>distance</e1> between p x and p ✓ , generative moment matching network (gmmn) [9, 10] uses kernel maximum mean discrepancy (mmd) [11] , which is the centerpiece of nonparametric two-sample test, to determine the distribution <e2>distances</e2>."
sameAs(e1, e2)
Comment:

719	"computationally, it also requires larger batch size than gan needs for training, which is considered to be less efficient [9, 10, 14, 8] in this work, we try to improve gmmn and consider using mmd with adversarially learned <e1>kernels</e1> instead of fixed gaussian <e2>kernels</e2> to have better hypothesis testing power."
sameAs(e1, e2)
Comment:

720	"second, we prove a new <e1>distance</e1> measure via kernel learning, which is a sensitive loss function to the <e2>distance</e2> between p x and p ✓ (weak ⇤ topology)."
sameAs(e1, e2)
Comment:

721	"finally, <e1>we</e1> also study the connection to existing works in section 4. interestingly, <e2>we</e2> show wasserstein gan [8] is the special case of the proposed mmd gan under certain conditions."
sameAs(e1, e2)
Comment:

722	"finally, we also study the connection to existing works in section 4. interestingly, we show wasserstein <e1>gan</e1> [8] is the special case of the proposed mmd <e2>gan</e2> under certain conditions."
sameAs(e1, e2)
Comment:

723	"the new approach combines the key ideas in both gmmn and <e1>gan</e1>, hence we name it mmd <e2>gan</e2>."
sameAs(e1, e2)
Comment:

724	"in our evaluation on multiple benchmark datasets, including mnist, cifar-10, celeba and lsun, the performance of mmd <e1>gan</e1> significantly outperforms gmmn, and is competitive with other representative <e2>gan</e2> works."
sameAs(e1, e2)
Comment:

725	"we propose a model for this problem <e1>that</e1> aims to extract as much information as possible from each training batch, a capability <e2>that</e2> is of increased importance when the available data for learning each class is scarce."
sameAs(e1, e2)
Comment:

726	"we choose to work within the <e1>framework</e1> of structured prediction and we optimize mean average precision (map) using a standard structural svm (ssvm) [3] , as well as a direct loss minimization (dlm) [4] <e2>approach</e2>."
Used-for(e1, e2)
Comment:

727	"we propose an information retrieval-inspired approach for <e1>this</e1> problem that is motivated by the increased importance of maximally leveraging all the available information in <e2>this</e2> low-data regime."
sameAs(e1, e2)
Comment:

728	"we also introduce a new form of a few-shot learning <e1>task</e1>, 'few-shot retrieval', where given a 'query' image and a pool of candidates all coming from previously-unseen classes, the <e2>task</e2> is to 'retrieve' all relevant (identically labelled) candidates for the query."
sameAs(e1, e2)
Comment:

729	"we also introduce a new form of a few-shot learning task, 'few-shot retrieval', where given a 'query' image and a pool of <e1>candidates</e1> all coming from previously-unseen classes, the task is to 'retrieve' all relevant (identically labelled) <e2>candidates</e2> for the query."
sameAs(e1, e2)
Comment:

730	"we define a <e1>training</e1> objective that aims to extract as much information as possible from each <e2>training</e2> batch by effectively optimizing over all relative orderings of the batch points simultaneously."
sameAs(e1, e2)
Comment:

731	"in particular, <e1>we</e1> view each batch point as a 'query' that ranks the remaining ones based on its predicted relevance to them and <e2>we</e2> define a model within the framework of structured prediction to optimize mean average precision over these rankings."
sameAs(e1, e2)
Comment:

732	"more concretely, k-shot n-way classification is the <e1>task</e1> of classifying a <e2>data</e2> point into one of n classes, when only k examples of each class are available to inform this decision."
Conjunction(e1, e2)
Comment:

733	"in order to <e1>learn</e1> parameters and structure simultaneously in a differentiable framework, we design a neural controller system with an attention mechanism and memory to <e2>learn</e2> to sequentially compose the primitive differentiable operations used by tensorlog."
sameAs(e1, e2)
Comment:

734	"at each stage of the computation, the controller uses attention to "softly" choose a subset of tensorlog's <e1>operations</e1>, and then performs the <e2>operations</e2> with contents selected from the memory."
sameAs(e1, e2)
Comment:

735	"neural lp also performs well on standard benchmark <e1>datasets</e1> for statistical relational learning, including <e2>datasets</e2> about biomedicine and kinship relationships [12] ."
sameAs(e1, e2)
Comment:

736	"since good performance on many of these datasets can be obtained using short <e1>rules</e1>, we also evaluate neural lp on a synthetic task which requires longer <e2>rules</e2>."
sameAs(e1, e2)
Comment:

737	"second, we experimentally evaluate neural lp on several types of knowledge base reasoning tasks, illustrating that this new <e1>approach</e1> to inductive logic programming outperforms <e2>prior work</e2>."
Compare(e1, e2)
Comment:

738	"in this paper, we explore an alternative <e1>approach</e1>: a completely differentiable system for learning <e2>models</e2> defined by sets of first-order rules."
Used-for(e1, e2)
Comment:

739	"these rates can be significantly higher for other planar and volumetric <e1>imaging</e1> techniques, e.g., light-sheet [1] or scape <e2>imaging</e2> [4] , where the data rates can exceed 1tb per hour."
sameAs(e1, e2)
Comment:

740	"while effective for small or medium <e1>datasets</e1>, direct factorization can be impractical, since a typical experiment can quickly produce <e2>datasets</e2> larger than the available ram."
sameAs(e1, e2)
Comment:

741	"several strategies have been proposed to enhance scalability, including parallel <e1>processing</e1> [9] , spatiotemporal decimation [10] , dimensionality reduction [23] , and out-of-core <e2>processing</e2> [13] ."
sameAs(e1, e2)
Comment:

742	"monitoring the <e1>activity</e1> of a roi, which usually corresponds to a soma, typically entails averaging the fluorescence over the corresponding roi, resulting in a signal that is only a proxy for the actual neural <e2>activity</e2> and which can be sensitive to motion artifacts and drifts, as well as spatially overlapping sources, background/neuropil contamination, and noise."
sameAs(e1, e2)
Comment:

743	"our framework is highly scalable with minimal <e1>memory</e1> requirements, as it processes the data in a streaming fashion one frame at a time, while keeping in <e2>memory</e2> a set of low dimensional sufficient statistics and a small minibatch of the last data frames."
sameAs(e1, e2)
Comment:

744	"our framework is highly scalable with minimal memory requirements, as it processes the <e1>data</e1> in a streaming fashion one frame at a time, while keeping in memory a set of low dimensional sufficient statistics and a small minibatch of the last <e2>data</e2> frames."
sameAs(e1, e2)
Comment:

745	"our <e1>algorithm</e1> integrates and extends the online nmf <e2>algorithm</e2> of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution algorithm of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

746	"our <e1>algorithm</e1> integrates and extends the online nmf algorithm of [19] , the cnmf source extraction <e2>algorithm</e2> of [26] , and the near-online deconvolution algorithm of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

747	"our <e1>algorithm</e1> integrates and extends the online nmf algorithm of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution <e2>algorithm</e2> of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

748	"our <e1>algorithm</e1> integrates and extends the online nmf algorithm of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution algorithm of [11] , to provide a <e2>framework</e2> capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
Part-of(e1, e2)
Comment:

749	"our algorithm integrates and extends the online nmf <e1>algorithm</e1> of [19] , the cnmf source extraction <e2>algorithm</e2> of [26] , and the near-online deconvolution algorithm of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

750	"our algorithm integrates and extends the online nmf <e1>algorithm</e1> of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution <e2>algorithm</e2> of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

751	"our algorithm integrates and extends the online nmf <e1>algorithm</e1> of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution algorithm of [11] , to provide a <e2>framework</e2> capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
Part-of(e1, e2)
Comment:

752	"our algorithm integrates and extends the online nmf algorithm of [19] , the cnmf source extraction <e1>algorithm</e1> of [26] , and the near-online deconvolution <e2>algorithm</e2> of [11] , to provide a framework capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
sameAs(e1, e2)
Comment:

753	"our algorithm integrates and extends the online nmf algorithm of [19] , the cnmf source extraction <e1>algorithm</e1> of [26] , and the near-online deconvolution algorithm of [11] , to provide a <e2>framework</e2> capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
Part-of(e1, e2)
Comment:

754	"our algorithm integrates and extends the online nmf algorithm of [19] , the cnmf source extraction algorithm of [26] , and the near-online deconvolution <e1>algorithm</e1> of [11] , to provide a <e2>framework</e2> capable of real time identification and processing of hundreds of neurons in a typical 2p experiment (512×512 pixel wide fov imaged at 30hz), enabling novel designs of closed-loop experiments."
Part-of(e1, e2)
Comment:

755	"we apply onacid to two large-scale (50 and 65 minute long) mouse in vivo 2p datasets; our <e1>algorithm</e1> can find and track hundreds of neurons faster than real-time, and outperforms the cnmf <e2>algorithm</e2> of [26] benchmarked on multiple manual annotations using a precision-recall framework."
sameAs(e1, e2)
Comment:

756	"we apply onacid to two large-scale (50 and 65 minute long) mouse in vivo 2p datasets; our <e1>algorithm</e1> can find and track hundreds of neurons faster than real-time, and outperforms the cnmf algorithm of [26] benchmarked on multiple manual annotations using a precision-recall <e2>framework</e2>."
Part-of(e1, e2)
Comment:

757	"we apply onacid to two large-scale (50 and 65 minute long) mouse in vivo 2p datasets; our algorithm can find and track hundreds of neurons faster than real-time, and outperforms the cnmf <e1>algorithm</e1> of [26] benchmarked on multiple manual annotations using a precision-recall <e2>framework</e2>."
Part-of(e1, e2)
Comment:

758	"we apply onacid to two large-scale (50 and 65 minute long) mouse in vivo 2p datasets; our algorithm can find and track hundreds of neurons faster than real-time, and outperforms the cnmf algorithm of [26] benchmarked on multiple manual annotations using a <e1>precision</e1>-<e2>recall</e2> framework."
Conjunction(e1, e2)
Comment:

759	"while deriving such <e1>processing</e1> algorithms is an active area of research, most existing methods require the <e2>processing</e2> of large amounts of data at a time, rendering them vulnerable to the volume of the recorded data, and preventing real-time experimental interrogation."
sameAs(e1, e2)
Comment:

760	"while deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of <e1>data</e1> at a time, rendering them vulnerable to the volume of the recorded <e2>data</e2>, and preventing real-time experimental interrogation."
sameAs(e1, e2)
Comment:

761	"while deriving such processing algorithms is an active area of research, most existing methods require the processing of large amounts of data at a <e1>time</e1>, rendering them vulnerable to the volume of the recorded data, and preventing real-<e2>time</e2> experimental interrogation."
sameAs(e1, e2)
Comment:

762	"we apply our algorithm on two large scale experimental <e1>datasets</e1>, benchmark its performance on manually annotated data, and show that it outperforms a popular offline <e2>approach</e2>."
Evaluate-for(e1, e2)
Comment:

763	"to infer the neural population <e1>activity</e1> from the raw imaging data, an analysis pipeline is employed which typically involves solving the following problems (all of which are still areas of active research): i) correcting for motion artifacts during the imaging experiment, ii) identifying/extracting the sources (neurons and axonal or dendritic processes) in the imaged field of view (fov), and iii) denoising and deconvolving the neural <e2>activity</e2> from the dynamics of the expressed calcium indicator."
sameAs(e1, e2)
Comment:

764	"to infer the neural population activity from the raw <e1>imaging</e1> data, an analysis pipeline is employed which typically involves solving the following problems (all of which are still areas of active research): i) correcting for motion artifacts during the <e2>imaging</e2> experiment, ii) identifying/extracting the sources (neurons and axonal or dendritic processes) in the imaged field of view (fov), and iii) denoising and deconvolving the neural activity from the dynamics of the expressed calcium indicator."
sameAs(e1, e2)
Comment:

765	"the fine spatiotemporal resolution of calcium imaging comes at a <e1>data</e1> rate cost; a typical two-photon (2p) experiment on a 512×512 pixel large fov imaged at 30hz, generates ∼50gb of <e2>data</e2> (in 16-bit integer format) per hour."
sameAs(e1, e2)
Comment:

766	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) are powerful models for learning complex distributions."
sameAs(e1, e2)
Comment:

767	"the <e1>distance</e1> between these distributions was the object of study in [2] , and highlighted the impact of the <e2>distance</e2> choice on the stability of the optimization."
sameAs(e1, e2)
Comment:

768	"the original gan formulation optimizes the jensen-shannon <e1>divergence</e1>, while later work generalized this to optimize f-<e2>divergences</e2> [3], kl [4], the least squares objective [5] ."
sameAs(e1, e2)
Comment:

769	"we propose a theoretically sound and time efficient data dependent constraint on the critic of wasserstein <e1>gan</e1>, that allows a stable training of <e2>gan</e2> and does not compromise the capacity of the critic."
sameAs(e1, e2)
Comment:

770	"in this paper we introduce fisher <e1>gan</e1> which fits within the integral probability metrics (ipm) framework for training <e2>gans</e2>."
sameAs(e1, e2)
Comment:

771	"we show in this paper <e1>that</e1> fisher gan allows for stable and time efficient training <e2>that</e2> does not compromise the capacity of the critic, and does not need data independent constraints such as weight clipping."
sameAs(e1, e2)
Comment:

772	"we validate our claims on both <e1>image</e1> sample generation and semi-supervised <e2>classification</e2> using fisher gan."
Used-for(e1, e2)
Comment:

773	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) [1] have recently become a prominent method to learn high-dimensional probability distributions."
sameAs(e1, e2)
Comment:

774	"introduction <e1>generative adversarial networks</e1> (gans) [1] have recently become a prominent <e2>method</e2> to learn high-dimensional probability distributions."
Part-of(e1, e2)
Comment:

775	"introduction generative adversarial networks (<e1>gans</e1>) [1] have recently become a prominent <e2>method</e2> to learn high-dimensional probability distributions."
Part-of(e1, e2)
Comment:

776	"finally, we show <e1>that</e1> representational dissimilarity matrices (rdms), a tool for comparing population codes between neural systems, can separate these higherperforming networks with data of a type <e2>that</e2> could plausibly be collected in a neurophysiological or imaging experiment."
sameAs(e1, e2)
Comment:

777	"vision <e1>systems</e1> process intensities from retinal photoreceptor arrays, auditory <e2>systems</e2> interpret the amplitudes and frequencies of hair-cell displacements, and somatosensory systems integrate data from direct physical interactions."
sameAs(e1, e2)
Comment:

778	"vision <e1>systems</e1> process intensities from retinal photoreceptor arrays, auditory systems interpret the amplitudes and frequencies of hair-cell displacements, and somatosensory <e2>systems</e2> integrate data from direct physical interactions."
sameAs(e1, e2)
Comment:

779	"vision systems process intensities from retinal photoreceptor arrays, auditory <e1>systems</e1> interpret the amplitudes and frequencies of hair-cell displacements, and somatosensory <e2>systems</e2> integrate data from direct physical interactions."
sameAs(e1, e2)
Comment:

780	"similar to <e1>hierarchical</e1> processing in the visual system (e.g., from v1 to v2, v4 and it [11, 12] ), processing in the somatosensory system is also known to be <e2>hierarchical</e2> [27, 17, 18] ."
sameAs(e1, e2)
Comment:

781	"similar to hierarchical <e1>processing</e1> in the visual system (e.g., from v1 to v2, v4 and it [11, 12] ), <e2>processing</e2> in the somatosensory system is also known to be hierarchical [27, 17, 18] ."
sameAs(e1, e2)
Comment:

782	"similar to hierarchical processing in the visual <e1>system</e1> (e.g., from v1 to v2, v4 and it [11, 12] ), processing in the somatosensory <e2>system</e2> is also known to be hierarchical [27, 17, 18] ."
sameAs(e1, e2)
Comment:

783	"for example, in the whisker trigeminal system, <e1>information</e1> from the whiskers is relayed from primary sensory neurons in the trigeminal ganglion to multiple trigeminal nuclei; these nuclei are the origin of several parallel pathways conveying <e2>information</e2> to the thalamus [36, 24] and then to primary and secondary somatosensory cortex (s1 and s2) [4] ."
sameAs(e1, e2)
Comment:

784	"however, although the rodent somatosensory <e1>system</e1> has been the subject of extensive experimental efforts [2, 26, 20, 32] , there have been comparatively few attempts at computational modeling of <e2>this</e2> important sensory system."
Used-for(e1, e2)
Comment:

785	"however, although the rodent somatosensory <e1>system</e1> has been the subject of extensive experimental efforts [2, 26, 20, 32] , there have been comparatively few attempts at computational modeling of this important sensory <e2>system</e2>."
sameAs(e1, e2)
Comment:

786	"the underlying idea of this approach is thus to use goal-driven modeling (fig 1) , in which the dnn parameters -both <e1>discrete</e1> and <e2>continuous</e2> -are optimized for performance on a challenging ethologically-relevant task [35] ."
Conjunction(e1, e2)
Comment:

787	"insofar as shape recognition is a strong constraint on network parameters, optimized neural networks resulting from such a <e1>task</e1> may be an effective <e2>model</e2> of real trigeminal-system neural response patterns."
Evaluate-for(e1, e2)
Comment:

788	"thus, a biophysically-realistic embodied <e1>model</e1> of the whisker array is a critical first component of any <e2>model</e2> of the vibrissal system."
sameAs(e1, e2)
Comment:

789	"we find <e1>that</e1> most networks perform poorly on the challenging shape recognition task, but <e2>that</e2> specific architectures from several families can achieve reasonable performance levels."
sameAs(e1, e2)
Comment:

790	"we evaluate <e1>our method</e1> on two datasets derived from the uspto [13] , and compare <e2>our methods</e2> to the current top performing system [3] ."
sameAs(e1, e2)
Comment:

791	"our <e1>method</e1> achieves 83.9% and 77.9% accuracy on two datasets, outperforming the <e2>baseline</e2> approach by 10%, while running 140 times faster."
Compare(e1, e2)
Comment:

792	"our method achieves 83.9% and 77.9% <e1>accuracy</e1> on two datasets, outperforming the baseline <e2>approach</e2> by 10%, while running 140 times faster."
Evaluate-for(e1, e2)
Comment:

793	"our method achieves 83.9% and 77.9% accuracy on two <e1>datasets</e1>, outperforming the baseline <e2>approach</e2> by 10%, while running 140 times faster."
Evaluate-for(e1, e2)
Comment:

794	"our <e1>framework</e1> outperforms the top-performing template-based <e2>approach</e2> with a 10% margin, while running orders of magnitude faster."
Used-for(e1, e2)
Comment:

795	"then, we use rebar to train generative sigmoid belief networks (sbns) on the <e1>mnist</e1> and omniglot <e2>datasets</e2> and to train conditional generative models on mnist."
isA(e1, e2)
Comment:

796	"then, we use rebar to train generative sigmoid belief networks (sbns) on the <e1>mnist</e1> and omniglot datasets and to train conditional generative models on <e2>mnist</e2>."
sameAs(e1, e2)
Comment:

797	"although we focus on binary <e1>variables</e1> for simplicity, this work is equally applicable to categorical <e2>variables</e2> (appendix c)."
sameAs(e1, e2)
Comment:

798	"then, we introduce a modification to the continuous <e1>relaxation</e1> and show that the tightness of the <e2>relaxation</e2> can be adapted online, removing it as a hyperparameter."
sameAs(e1, e2)
Comment:

799	"we call this the rebar <e1>gradient</e1> estimator, because it combines reinforce <e2>gradients</e2> with gradients of the concrete relaxation."
sameAs(e1, e2)
Comment:

800	"we call this the rebar <e1>gradient</e1> estimator, because it combines reinforce gradients with <e2>gradients</e2> of the concrete relaxation."
sameAs(e1, e2)
Comment:

801	"we call this the rebar gradient estimator, because it combines reinforce <e1>gradients</e1> with <e2>gradients</e2> of the concrete relaxation."
sameAs(e1, e2)
Comment:

802	"ideally, a game <e1>environment</e1> for fundamental rl research is: • extensive: the <e2>environment</e2> should capture many diverse aspects of the real world, such as rich dynamics, partial information, delayed/long-term rewards, concurrent actions with different granularity, etc."
sameAs(e1, e2)
Comment:

803	"• lightweight: a platform should be fast and capable of generating samples hundreds or thousands of <e1>times</e1> faster than real-<e2>time</e2> with minimal computational resources (e.g., a single machine)."
sameAs(e1, e2)
Comment:

804	"for instance, the interplay between rl methods and environments during training is often limited to providing simplistic interfaces (e.g., <e1>one</e1> interface for <e2>one</e2> game) in scripting languages like python."
sameAs(e1, e2)
Comment:

805	"the full access to the game <e1>data</e1> also allows for supervised training with small-scale internal <e2>data</e2>."
sameAs(e1, e2)
Comment:

806	"these include <e1>one</e1>-to-<e2>one</e2>, many-to-one and oneto-many mappings."
sameAs(e1, e2)
Comment:

807	"these include <e1>one</e1>-to-one, many-to-<e2>one</e2> and oneto-many mappings."
sameAs(e1, e2)
Comment:

808	"these include one-to-<e1>one</e1>, many-to-<e2>one</e2> and oneto-many mappings."
sameAs(e1, e2)
Comment:

809	"in contrast, existing environments (e.g., openai gym [6] and universe [33]) wrap <e1>one</e1> game in <e2>one</e2> python interface, which makes it cumbersome to change topologies."
sameAs(e1, e2)
Comment:

810	"in addition, our platform is flexible in terms of <e1>environment</e1>-agent communication topologies, choices of rl methods, changes in game parameters, and can host existing c/c++-based game <e2>environments</e2> like ale [4] ."
sameAs(e1, e2)
Comment:

811	"this is equivalent to minimizing the kullback-leibler (<e1>kl</e1>) divergence between data and model distributions: d <e2>kl</e2> (p data p model )."
sameAs(e1, e2)
Comment:

812	"this is equivalent to minimizing the kullback-leibler (kl) divergence between <e1>data</e1> and model distributions: d kl (p <e2>data</e2> p model )."
sameAs(e1, e2)
Comment:

813	"this is equivalent to minimizing the kullback-leibler (kl) divergence between data and <e1>model</e1> distributions: d kl (p data p <e2>model</e2> )."
sameAs(e1, e2)
Comment:

814	"it has been observed <e1>that</e1> this minimization tends to result in p model <e2>that</e2> covers multiple modes of p data , but may produce completely unseen and potentially undesirable samples [29] ."
sameAs(e1, e2)
Comment:

815	"by contrast, another approach is to swap the arguments and instead, minimize: d <e1>kl</e1> (p model p data ), which is usually referred to as the reverse <e2>kl</e2> divergence [23, 11, 15, 29] ."
sameAs(e1, e2)
Comment:

816	"different from most generative models <e1>that</e1> maximize data likelihood or its lower bound, gan takes a radical approach <e2>that</e2> simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

817	"different from most generative models <e1>that</e1> maximize data likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g <e2>that</e2> generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

818	"different from most generative models <e1>that</e1> maximize data likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d <e2>that</e2> acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

819	"different from most generative models that maximize <e1>data</e1> likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g that generates <e2>data</e2> by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

820	"different from most generative models that maximize data likelihood or its lower bound, <e1>gan</e1> takes a radical approach that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this <e2>method</e2> can be categorized into the family of deep generative models or generative neural models [9] ."
Part-of(e1, e2)
Comment:

821	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical <e1>approach</e1> that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a <e2>classifier</e2> to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
Used-for(e1, e2)
Comment:

822	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical <e1>approach</e1> that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this <e2>method</e2> can be categorized into the family of deep generative models or generative neural models [9] ."
Compare(e1, e2)
Comment:

823	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical approach <e1>that</e1> simulates a game between two players: a generator g <e2>that</e2> generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

824	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical approach <e1>that</e1> simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d <e2>that</e2> acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

825	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g <e1>that</e1> generates data by mapping samples from a noise space to the input space; and a discriminator d <e2>that</e2> acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models [9] ."
sameAs(e1, e2)
Comment:

826	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a <e1>classifier</e1> to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via neural networks, thus this <e2>method</e2> can be categorized into the family of deep generative models or generative neural models [9] ."
Part-of(e1, e2)
Comment:

827	"different from most generative models that maximize data likelihood or its lower bound, gan takes a radical approach that simulates a game between two players: a generator g that generates data by mapping samples from a noise space to the input space; and a discriminator d that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator g. both g and d are parameterized via <e1>neural networks</e1>, thus this <e2>method</e2> can be categorized into the family of deep generative models or generative neural models [9] ."
Used-for(e1, e2)
Comment:

828	"the behavior of jsd minimization has been empirically proven to be more similar to reverse <e1>kl</e1> than to <e2>kl</e2> divergence [29, 15] ."
sameAs(e1, e2)
Comment:

829	"in essence, it combines the kullback-leibler (<e1>kl</e1>) and reverse <e2>kl</e2> divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes."
sameAs(e1, e2)
Comment:

830	"in essence, it combines the kullback-leibler (kl) and reverse kl <e1>divergences</e1> into a unified objective function, thus it exploits the complementary statistical properties from these <e2>divergences</e2> to effectively diversify the estimated density in capturing multi-modes."
sameAs(e1, e2)
Comment:

831	"our approach combines the <e1>kl</e1> and reverse <e2>kl</e2> divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes."
sameAs(e1, e2)
Comment:

832	"our approach combines the kl and reverse kl <e1>divergences</e1> into a unified objective function, thus it exploits the complementary statistical properties from these <e2>divergences</e2> to effectively diversify the estimated density in capturing multi-modes."
sameAs(e1, e2)
Comment:

833	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 <e1>that</e1> rewards high scores for data sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p data , and a generator g <e2>that</e2> generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

834	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for <e1>data</e1> sampled from p <e2>data</e2> rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p data , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

835	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for <e1>data</e1> sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring <e2>data</e2> from p g rather p data , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

836	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for <e1>data</e1> sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p <e2>data</e2> , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

837	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for <e1>data</e1> sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p data , and a generator g that generates <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

838	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p <e1>data</e1> rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring <e2>data</e2> from p g rather p data , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

839	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p <e1>data</e1> rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p <e2>data</e2> , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

840	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p <e1>data</e1> rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p data , and a generator g that generates <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

841	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring <e1>data</e1> from p g rather p <e2>data</e2> , and a generator g that generates data to fool both two discriminators."
sameAs(e1, e2)
Comment:

842	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring <e1>data</e1> from p g rather p data , and a generator g that generates <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

843	"we materialize our idea using gan's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator d 1 that rewards high scores for data sampled from p data rather than generated from the generator distribution p g whilst another discriminator d 2 , conversely, favoring data from p g rather p <e1>data</e1> , and a generator g that generates <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

844	"we provide theoretical analysis showing that, given g, d 1 and d 2 with enough capacity, i.e., in the nonparametric limit, at the optimal points, the training criterion indeed results in the minimal distance between data and model distribution with respect to both their <e1>kl</e1> and reverse <e2>kl</e2> divergences."
sameAs(e1, e2)
Comment:

845	"this helps the model place fair distribution of probability mass across the modes of the <e1>data</e1> generating distribution, thus allowing one to recover the <e2>data</e2> distribution and generate diverse samples using the generator in a single shot."
sameAs(e1, e2)
Comment:

846	"we term our method dual discriminator generative adversarial nets (d2gan) which, unlike gan, has <e1>two</e1> discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both <e2>two</e2> discriminators."
sameAs(e1, e2)
Comment:

847	"we term our method dual discriminator generative adversarial nets (d2gan) which, unlike gan, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from <e1>data</e1> distribution whilst another discriminator, conversely, favoring <e2>data</e2> from the generator, and the generator produces data to fool both two discriminators."
sameAs(e1, e2)
Comment:

848	"we term our method dual discriminator generative adversarial nets (d2gan) which, unlike gan, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from <e1>data</e1> distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

849	"we term our method dual discriminator generative adversarial nets (d2gan) which, unlike gan, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring <e1>data</e1> from the generator, and the generator produces <e2>data</e2> to fool both two discriminators."
sameAs(e1, e2)
Comment:

850	"in short, our main contributions are: (i) a novel generative adversarial model <e1>that</e1> encourages the diversity of samples produced by the generator; (ii) a theoretical analysis to prove <e2>that</e2> our objective is optimized towards minimizing both kl and reverse kl divergence and has a global optimum where p g = p data ; and (iii) a comprehensive evaluation on the effectiveness of our proposed method using a wide range of quantitative criteria on large-scale datasets."
sameAs(e1, e2)
Comment:

851	"in short, our main contributions are: (i) a novel generative adversarial model that encourages the diversity of samples produced by the generator; (ii) a theoretical analysis to prove that our objective is optimized towards minimizing both <e1>kl</e1> and reverse <e2>kl</e2> divergence and has a global optimum where p g = p data ; and (iii) a comprehensive evaluation on the effectiveness of our proposed method using a wide range of quantitative criteria on large-scale datasets."
sameAs(e1, e2)
Comment:

852	"we develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of d2gan reduces to minimizing both <e1>kl</e1> and reverse <e2>kl</e2> divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem."
sameAs(e1, e2)
Comment:

853	"we develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of d2gan reduces to minimizing both kl and reverse kl divergences between <e1>data</e1> distribution and the distribution induced from the <e2>data</e2> generated by the generator, hence effectively avoiding the mode collapsing problem."
sameAs(e1, e2)
Comment:

854	"their common approach is to address the density estimation problem where one aims to learn a <e1>model</e1> distribution p <e2>model</e2> that approximates the true, but unknown, data distribution p data ."
sameAs(e1, e2)
Comment:

855	"their common approach is to address the density estimation problem where one aims to learn a model distribution p model that approximates the true, but unknown, <e1>data</e1> distribution p <e2>data</e2> ."
sameAs(e1, e2)
Comment:

856	"abstract we establish a new connection between value and <e1>policy</e1> based reinforcement learning (rl) based on a relationship between softmax temporal value consistency and <e2>policy</e2> optimality under entropy regularization."
sameAs(e1, e2)
Comment:

857	"ideally, one would like to combine the unbiasedness and stability of on-<e1>policy</e1> training with the data efficiency of off-<e2>policy</e2> approaches."
sameAs(e1, e2)
Comment:

858	"this desire has motivated substantial recent work on off-<e1>policy</e1> actor-critic methods, where the data efficiency of <e2>policy</e2> gradient is improved by training an offpolicy critic [19, 21, 10] ."
sameAs(e1, e2)
Comment:

859	"although such methods have demonstrated improvements over on-<e1>policy</e1> actor-critic approaches, they have not resolved the theoretical difficulty associated with off-<e2>policy</e2> learning under function approximation."
sameAs(e1, e2)
Comment:

860	"in this paper, we exploit a relationship between <e1>policy</e1> optimization under entropy regularization and softmax value consistency to obtain a new form of stable off-<e2>policy</e2> learning."
sameAs(e1, e2)
Comment:

861	"even though <e1>entropy</e1> regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under <e2>entropy</e2> regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

862	"even though entropy regularized <e1>policy</e1> optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal <e2>policy</e2> probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

863	"even though entropy regularized <e1>policy</e1> optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-<e2>policy</e2> actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

864	"even though entropy regularized policy <e1>optimization</e1> is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel <e2>optimization</e2> objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

865	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one <e1>that</e1> has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study <e2>that</e2> are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

866	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one <e1>that</e1> has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency <e2>that</e2> relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

867	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one <e1>that</e1> has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective <e2>that</e2> allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

868	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one <e1>that</e1> has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe <e2>that</e2> under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

869	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one <e1>that</e1> has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model <e2>that</e2> coherently fulfills both roles."
sameAs(e1, e2)
Comment:

870	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -<e1>we</e1> contribute new observations to this study that are essential for the methods <e2>we</e2> propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

871	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -<e1>we</e1> contribute new observations to this study that are essential for the methods we propose: first, <e2>we</e2> identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

872	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -<e1>we</e1> contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, <e2>we</e2> use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

873	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -<e1>we</e1> contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, <e2>we</e2> observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

874	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to <e1>this</e1> study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use <e2>this</e2> result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

875	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to <e1>this</e1> study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under <e2>this</e2> objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

876	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study <e1>that</e1> are essential for the methods we propose: first, we identify a strong form of path consistency <e2>that</e2> relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

877	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study <e1>that</e1> are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective <e2>that</e2> allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

878	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study <e1>that</e1> are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe <e2>that</e2> under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

879	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study <e1>that</e1> are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model <e2>that</e2> coherently fulfills both roles."
sameAs(e1, e2)
Comment:

880	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods <e1>we</e1> propose: first, <e2>we</e2> identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

881	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods <e1>we</e1> propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, <e2>we</e2> use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

882	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods <e1>we</e1> propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, <e2>we</e2> observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

883	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, <e1>we</e1> identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, <e2>we</e2> use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

884	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, <e1>we</e1> identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, <e2>we</e2> observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

885	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency <e1>that</e1> relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective <e2>that</e2> allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

886	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency <e1>that</e1> relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe <e2>that</e2> under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

887	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency <e1>that</e1> relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model <e2>that</e2> coherently fulfills both roles."
sameAs(e1, e2)
Comment:

888	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal <e1>policy</e1> probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-<e2>policy</e2> actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

889	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent <e1>state</e1> values for any <e2>action</e2> sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
Conjunction(e1, e2)
Comment:

890	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, <e1>we</e1> use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, <e2>we</e2> observe that under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

891	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use <e1>this</e1> result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe that under <e2>this</e2> objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

892	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective <e1>that</e1> allows for a stable form of off-policy actor-critic learning; finally, we observe <e2>that</e2> under this objective the actor and critic can be unified in a single model that coherently fulfills both roles."
sameAs(e1, e2)
Comment:

893	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective <e1>that</e1> allows for a stable form of off-policy actor-critic learning; finally, we observe that under this objective the actor and critic can be unified in a single model <e2>that</e2> coherently fulfills both roles."
sameAs(e1, e2)
Comment:

894	"even though entropy regularized policy optimization is a well studied topic in rl [46, 39, 40, 47, 5, 4, 6, 7] -in fact, one that has been attracting renewed interest from concurrent work [25, 11] -we contribute new observations to this study that are essential for the methods we propose: first, we identify a strong form of path consistency that relates optimal policy probabilities under entropy regularization to softmax consistent state values for any action sequence; second, we use this result to formulate a novel optimization objective that allows for a stable form of off-policy actor-critic learning; finally, we observe <e1>that</e1> under this objective the actor and critic can be unified in a single model <e2>that</e2> coherently fulfills both roles."
sameAs(e1, e2)
Comment:

895	"specifically, we show that softmax consistent <e1>action</e1> values correspond to optimal entropy regularized policy probabilities along any <e2>action</e2> sequence, regardless of provenance."
sameAs(e1, e2)
Comment:

896	"from this observation, we develop a new rl <e1>algorithm</e1>, path consistency learning (pcl), that minimizes a notion of soft consistency error along multi-step action sequences extracted from both on-and off-<e2>policy</e2> traces."
Used-for(e1, e2)
Comment:

897	"such "off-<e1>policy</e1>" methods are able to exploit data from other sources, such as experts, making them inherently more sample efficient than on-<e2>policy</e2> methods [10] ."
sameAs(e1, e2)
Comment:

898	"such "off-policy" <e1>methods</e1> are able to exploit data from other sources, such as experts, making them inherently more sample efficient than on-policy <e2>methods</e2> [10] ."
sameAs(e1, e2)
Comment:

899	"while deep architectures have seen enormous success in recent years, it is an interesting research question to investigate what kind of non-local generalisation <e1>structures</e1> can be encoded in shallow <e2>structures</e2> like kernels, while preserving the elegant properties of gps."
sameAs(e1, e2)
Comment:

900	"we stress <e1>that</e1> our approach is different in <e2>that</e2> the process itself is convolved, which does not require the introduction of additional parameters."
sameAs(e1, e2)
Comment:

901	"in this work, we adopt the variational framework for <e1>approximation</e1> in gp models, because it can simultaneously give a computational speed-up to o n m 2 (with m n ) through sparse <e2>approximations</e2> [2] and approximate posteriors due to non-gaussian likelihoods [18] ."
sameAs(e1, e2)
Comment:

902	"the variational choice is both elegant and practical: it can be shown <e1>that</e1> the variational objective minimises the kl divergence across the entire latent process [4, 19] , which guarantees <e2>that</e2> the exact model will be approximated given enough resources."
sameAs(e1, e2)
Comment:

903	"other methods, such as ep/fitc [14, 20, 21, 22] , can be seen as approximate models <e1>that</e1> do not share this property, leading to behaviour <e2>that</e2> would not be expected from the model that is to be approximated [23] ."
sameAs(e1, e2)
Comment:

904	"other methods, such as ep/fitc [14, 20, 21, 22] , can be seen as approximate models <e1>that</e1> do not share this property, leading to behaviour that would not be expected from the model <e2>that</e2> is to be approximated [23] ."
sameAs(e1, e2)
Comment:

905	"other methods, such as ep/fitc [14, 20, 21, 22] , can be seen as approximate models that do not share this property, leading to behaviour <e1>that</e1> would not be expected from the model <e2>that</e2> is to be approximated [23] ."
sameAs(e1, e2)
Comment:

906	"however, orthogonal to being able to algorithmically handle large quantities of <e1>data</e1> is the question of how to build gp <e2>models</e2> that generalise well."
Used-for(e1, e2)
Comment:

907	"they have been used to pre-process inputs in two-layer <e1>models</e1>: calibrators-then-linear <e2>models</e2> [4] , calibrators-then-lattice models [1] , and calibrators-then-ensemble-of-lattices model [3] ."
sameAs(e1, e2)
Comment:

908	"they have been used to pre-process inputs in two-layer <e1>models</e1>: calibrators-then-linear models [4] , calibrators-then-lattice <e2>models</e2> [1] , and calibrators-then-ensemble-of-lattices model [3] ."
sameAs(e1, e2)
Comment:

909	"they have been used to pre-process inputs in two-layer models: calibrators-then-linear <e1>models</e1> [4] , calibrators-then-lattice <e2>models</e2> [1] , and calibrators-then-ensemble-of-lattices model [3] ."
sameAs(e1, e2)
Comment:

910	"they have been used to pre-process inputs in two-layer models: calibrators-then-linear models [4] , calibrators-then-<e1>lattice</e1> models [1] , and calibrators-then-ensemble-of-<e2>lattices</e2> model [3] ."
sameAs(e1, e2)
Comment:

911	"introduction <e1>we</e1> propose building models with multiple layers of lattices, which <e2>we</e2> refer to as deep lattice networks (dlns)."
sameAs(e1, e2)
Comment:

912	"introduction we propose building models with multiple layers of <e1>lattices</e1>, which we refer to as deep <e2>lattice</e2> networks (dlns)."
sameAs(e1, e2)
Comment:

913	"while <e1>we</e1> hypothesize that dlns may generally be useful, <e2>we</e2> focus on the challenge of learning flexible partially-monotonic functions, that is, models that are guaranteed to be monotonic with respect to a user-specified subset of the inputs."
sameAs(e1, e2)
Comment:

914	"while we hypothesize <e1>that</e1> dlns may generally be useful, we focus on the challenge of learning flexible partially-monotonic functions, <e2>that</e2> is, models that are guaranteed to be monotonic with respect to a user-specified subset of the inputs."
sameAs(e1, e2)
Comment:

915	"while we hypothesize <e1>that</e1> dlns may generally be useful, we focus on the challenge of learning flexible partially-monotonic functions, that is, models <e2>that</e2> are guaranteed to be monotonic with respect to a user-specified subset of the inputs."
sameAs(e1, e2)
Comment:

916	"while we hypothesize that dlns may generally be useful, we focus on the challenge of learning flexible partially-monotonic functions, <e1>that</e1> is, models <e2>that</e2> are guaranteed to be monotonic with respect to a user-specified subset of the inputs."
sameAs(e1, e2)
Comment:

917	"for example, if <e1>one</e1> is predicting whether to give someone else a loan, we expect and would like to constrain the prediction to be monotonically increasing with respect to the applicant's income, if all <e2>other</e2> features are unchanged."
Conjunction(e1, e2)
Comment:

918	"despite this fact, recent models often process the whole image, which exposes them to noise and increases the associated <e1>computational cost</e1> or they use heuristic <e2>methods</e2> to decrease the size of search regions."
Evaluate-for(e1, e2)
Comment:

919	"attention mechanisms have recently been explored in <e1>machine learning</e1> in a wide variety of contexts [27, 14] , often providing new capabilities to <e2>machine learning algorithms</e2> [11, 12, 7] ."
sameAs(e1, e2)
Comment:

920	"the receptive <e1>field</e1> of a neuron and by increasing sensitivity at a given location in the visual <e2>field</e2> (spatial attention)."
sameAs(e1, e2)
Comment:

921	"it can also amplify activity in different parts of the cortex, which are specialised in processing different types of <e1>features</e1>, leading to response enhancement with respect to those <e2>features</e2> (appearance attention)."
sameAs(e1, e2)
Comment:

922	"it can also amplify activity in different parts of the cortex, which are specialised in processing different types of features, leading to response enhancement with respect to <e1>those</e1> <e2>features</e2> (appearance attention)."
Compare(e1, e2)
Comment:

923	"inspired by how the human visual cortex employs spatial attention and separate "where" and "what" processing pathways to actively suppress irrelevant visual <e1>features</e1>, this work develops a hierarchical attentive recurrent <e2>model</e2> for single object tracking in videos."
Used-for(e1, e2)
Comment:

924	"the proposed <e1>approach</e1> exploits this property to create a feedback loop that steers the three <e2>layers</e2> of visual attention mechanisms in our hierarchical attentive recurrent tracking (hart) framework, see figure 1 ."
Used-for(e1, e2)
Comment:

925	"this follows from our interest in estimating the distribution over object <e1>locations</e1> in a sequence of images, given the initial <e2>location</e2> from whence our tracking commenced."
sameAs(e1, e2)
Comment:

926	"finally, our attentionbased <e1>tracker</e1> is demonstrated using real-world sequences in challenging scenarios where previous recurrent attentive <e2>trackers</e2> have failed."
Compare(e1, e2)
Comment:

927	"section 5 presents experiments on kth and <e1>kitti</e1> <e2>datasets</e2> with comparison to related attention-based trackers."
isA(e1, e2)
Comment:

928	"evaluation of the proposed model is performed on two datasets: pedestrian <e1>tracking</e1> on the kth activity recognition dataset and the more difficult kitti object <e2>tracking</e2> dataset."
sameAs(e1, e2)
Comment:

929	"introduction in computer vision, designing an <e1>algorithm</e1> for model-free tracking of anonymous objects is challenging, since no target-specific information can be gathered a priori and yet the <e2>algorithm</e2> has to handle target appearance changes, varying lighting conditions and occlusion."
sameAs(e1, e2)
Comment:

930	"introduction in computer vision, designing an algorithm for model-free tracking of anonymous objects is challenging, since no <e1>target</e1>-specific information can be gathered a priori and yet the algorithm has to handle <e2>target</e2> appearance changes, varying lighting conditions and occlusion."
sameAs(e1, e2)
Comment:

931	"abstract data augmentation is a ubiquitous technique for increasing the size of labeled training sets by leveraging <e1>task</e1>-specific <e2>data</e2> transformations that preserve class labels."
Conjunction(e1, e2)
Comment:

932	"two example sequences of incremental image tfs applied to cifar-10 <e1>images</e1> (left); a conditional word-swap tf using an externally trained language model and specifically targeting nouns (nn) between entity mentions (e1,e2) for a relation extraction task (middle); and an unsupervised segementation-based translation tf applied to mass-containing mammography <e2>images</e2> (right)."
sameAs(e1, e2)
Comment:

933	"in contrast, our aim is to directly and flexibly leverage <e1>domain experts</e1>' <e2>knowledge</e2> of invariances as a valuable form of weak supervision in real-world settings where labeled training data is limited."
Used-for(e1, e2)
Comment:

934	"for example, tfs could rotate an <e1>image</e1> by a small degree, swap a word in a sentence, or translate a segmented structure in an <e2>image</e2> (fig."
sameAs(e1, e2)
Comment:

935	"we then design a generative adversarial objective [9] which allows us to train the sequence model to produce transformed <e1>data</e1> points which are still within the <e2>data</e2> distribution of interest, using unlabeled data."
sameAs(e1, e2)
Comment:

936	"the learned <e1>model</e1> can then be used to perform data augmentation on labeled training data for any end discriminative <e2>model</e2>."
sameAs(e1, e2)
Comment:

937	"on a real-world mammography <e1>image</e1> task, we achieve a 3.4 accuracy point boost above randomly composed augmentation by learning to appropriately combine standard <e2>image</e2> tfs with domainspecific tfs derived in collaboration with radiology experts."
sameAs(e1, e2)
Comment:

938	"we propose a <e1>method</e1> for automating this process by learning a generative sequence <e2>model</e2> over user-specified transformation functions using a generative adversarial approach."
Used-for(e1, e2)
Comment:

939	"and on a 10%-subsample of the cifar-10 dataset, we achieve a 4.0 <e1>accuracy</e1> point gain over a standard heuristic augmentation <e2>approach</e2> and are competitive with comparable semi-supervised approaches."
Evaluate-for(e1, e2)
Comment:

940	"and on a 10%-subsample of the cifar-10 dataset, we achieve a 4.0 accuracy point gain over a standard heuristic augmentation <e1>approach</e1> and are competitive with comparable semi-supervised <e2>approaches</e2>."
Compare(e1, e2)
Comment:

941	"in our experiments, we show the efficacy of our <e1>approach</e1> on both image and text datasets, achieving improvements of 4.0 accuracy points on cifar-10, 1.4 f1 points on the ace relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation <e2>approaches</e2>."
Compare(e1, e2)
Comment:

942	"in our experiments, we show the efficacy of our approach on both image and text <e1>datasets</e1>, achieving improvements of 4.0 accuracy points on cifar-10, 1.4 f1 points on the ace relation extraction task, and 3.4 accuracy points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

943	"in our experiments, we show the efficacy of our approach on both image and text datasets, achieving improvements of 4.0 <e1>accuracy</e1> points on cifar-10, 1.4 f1 points on the ace relation extraction task, and 3.4 <e2>accuracy</e2> points when using domain-specific transformation operations on a medical imaging dataset as compared to standard heuristic augmentation approaches."
sameAs(e1, e2)
Comment:

944	"the technique of artificially expanding labeled training sets by transforming <e1>data</e1> points in ways which preserve class labels -known as data augmentation -has quickly become a critical and effective tool for combatting this labeled <e2>data</e2> scarcity problem."
sameAs(e1, e2)
Comment:

945	"we later demonstrate <e1>that</e1> gradient issues persist in lstms (see results) and <e2>that</e2> the grid-like architecture of stacked lstms is suboptimal."
sameAs(e1, e2)
Comment:

946	"we later demonstrate that gradient issues persist in <e1>lstms</e1> (see results) and that the grid-like architecture of stacked <e2>lstms</e2> is suboptimal."
sameAs(e1, e2)
Comment:

947	"this consideration is highly desirable, given the successes of recent convolutional <e1>architectures</e1> on language modeling tasks [14] [15], which were previously dominated by recurrent <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

948	"we present <e1>experimental results</e1> for rhns complementary to the original work's theoretical results and theoretical results for hypernetworks complementary to the original work's <e2>experimental results</e2>."
sameAs(e1, e2)
Comment:

949	"we present experimental results for rhns complementary to the original work's theoretical <e1>results</e1> and theoretical <e2>results</e2> for hypernetworks complementary to the original work's experimental results."
sameAs(e1, e2)
Comment:

950	"the original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved-<e1>we</e1> consider these in depth and frame several feasible solutions that <e2>we</e2> believe will yield further gains in the future."
sameAs(e1, e2)
Comment:

951	"finally, we argue for rhns as a drop-in replacement for <e1>lstms</e1> (analogous to <e2>lstms</e2> for vanilla rnns) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures."
sameAs(e1, e2)
Comment:

952	"though many consider lstms [2] the de-facto <e1>solution</e1> to vanishing gradients, in practice, the <e2>problem</e2> is far from solved (see discussion)."
Used-for(e1, e2)
Comment:

953	"several <e1>lstm</e1> variants have been developed, most notably grus [3] , which are simpler than <e2>lstm</e2> cells but benefit from only marginally better gradient flow."
sameAs(e1, e2)
Comment:

954	"greff et al and britz et al conducted exhaustive (for all practical purposes) <e1>architecture</e1> searches over simple lstm variants and demonstrated that none achieved significant improvement [4] [5]-in particular, the latter work discovered that lstms consistently outperform comparable grus on machine translation, and no proposed cell <e2>architecture</e2> to date has been proven significantly better than the lstm."
sameAs(e1, e2)
Comment:

955	"greff et al and britz et al conducted exhaustive (for all practical purposes) architecture searches over simple <e1>lstm</e1> variants and demonstrated that none achieved significant improvement [4] [5]-in particular, the latter work discovered that lstms consistently outperform comparable grus on machine translation, and no proposed cell architecture to date has been proven significantly better than the <e2>lstm</e2>."
sameAs(e1, e2)
Comment:

956	"greff et al and britz et al conducted exhaustive (for all practical purposes) architecture searches over simple lstm variants and demonstrated <e1>that</e1> none achieved significant improvement [4] [5]-in particular, the latter work discovered <e2>that</e2> lstms consistently outperform comparable grus on machine translation, and no proposed cell architecture to date has been proven significantly better than the lstm."
sameAs(e1, e2)
Comment:

957	"methods in this line of thought are <e1>network</e1> pruning, where unnecessary connections are being removed [38, 24, 21] , or student-teacher learning where a large <e2>network</e2> is used to train a significantly smaller network [5, 26] ."
sameAs(e1, e2)
Comment:

958	"methods in this line of thought are <e1>network</e1> pruning, where unnecessary connections are being removed [38, 24, 21] , or student-teacher learning where a large network is used to train a significantly smaller <e2>network</e2> [5, 26] ."
sameAs(e1, e2)
Comment:

959	"methods in this line of thought are network pruning, where unnecessary connections are being removed [38, 24, 21] , or student-teacher learning where a large <e1>network</e1> is used to train a significantly smaller <e2>network</e2> [5, 26] ."
sameAs(e1, e2)
Comment:

960	"from a bayesian perspective network <e1>pruning</e1> and reducing bit precision for the weights is aligned with achieving high accuracy, because bayesian methods search for the optimal model structure (which leads to <e2>pruning</e2> with sparsity inducing priors), and reward uncertain posteriors over parameters through the bits back argument [27] (which leads to removing insignificant bits)."
sameAs(e1, e2)
Comment:

961	"in <e1>this</e1> work, we argue that the most principled and effective way to attack <e2>this</e2> problem is by adopting a bayesian point of view, where through sparsity inducing priors we prune large parts of the network."
sameAs(e1, e2)
Comment:

962	"in this work, <e1>we</e1> argue that the most principled and effective way to attack this problem is by adopting a bayesian point of view, where through sparsity inducing priors <e2>we</e2> prune large parts of the network."
sameAs(e1, e2)
Comment:

963	"by employing sparsity inducing priors for hidden units (and not individual <e1>weights</e1>) we can prune neurons including all their ingoing and outgoing <e2>weights</e2>."
sameAs(e1, e2)
Comment:

964	"as an additional bayesian bonus we can use the variational <e1>posterior</e1> uncertainty to assess which bits are significant and remove the ones which fluctuate too much under approximate <e2>posterior</e2> sampling."
sameAs(e1, e2)
Comment:

965	"we introduce two novelties in this paper: 1) <e1>we</e1> use hierarchical priors to prune nodes instead of individual weights, and 2) <e2>we</e2> use the posterior uncertainties to determine the optimal fixed point precision to encode the weights."
sameAs(e1, e2)
Comment:

966	"we introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual <e1>weights</e1>, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the <e2>weights</e2>."
sameAs(e1, e2)
Comment:

967	"the focus of this paper is to provide a comprehensive <e1>framework</e1> for deploying the gaussian process probabilistic modelling <e2>approach</e2> to streaming data."
Used-for(e1, e2)
Comment:

968	"a feasible alternative would train on just the most recent k <e1>training</e1> data points, but this completely ignores potentially large amounts of informative <e2>training</e2> data and it does not provide a method for incorporating the old model into the new one which would save computation (except perhaps through initialisation of the hyperparameters)."
sameAs(e1, e2)
Comment:

969	"a feasible alternative would train on just the most recent k training <e1>data</e1> points, but this completely ignores potentially large amounts of informative training <e2>data</e2> and it does not provide a method for incorporating the old model into the new one which would save computation (except perhaps through initialisation of the hyperparameters)."
sameAs(e1, e2)
Comment:

970	"a feasible alternative would train on just the most recent k training data points, but this completely ignores potentially large amounts of informative training data and it does not provide a <e1>method</e1> for incorporating the old <e2>model</e2> into the new one which would save computation (except perhaps through initialisation of the hyperparameters)."
Used-for(e1, e2)
Comment:

971	"what is needed is a <e1>method</e1> for performing learning and sparse approximation that incrementally updates the previously fit <e2>model</e2> using the new data."
Used-for(e1, e2)
Comment:

972	"such an approach would utilise all the previous training <e1>data</e1> (as they will have been incorporated into the previously fit model) and leverage as much of the previous computation as possible at each stage (since the algorithm only requires access to the <e2>data</e2> at the current time point)."
sameAs(e1, e2)
Comment:

973	"the framework subsumes csató and opper's two seminal <e1>approaches</e1> to online regression [8, 9] that were based upon the variational free energy (vfe) and expectation propagation (ep) <e2>approaches</e2> to approximate inference respectively."
sameAs(e1, e2)
Comment:

974	"introduction probabilistic models employing gaussian processes have become a standard <e1>approach</e1> to solving many machine learning tasks, thanks largely to the modelling flexibility, robustness to overfitting, and well-calibrated uncertainty estimates afforded by the <e2>approach</e2> [1] ."
sameAs(e1, e2)
Comment:

975	"one of the pillars of the modern gaussian process probabilistic modelling <e1>approach</e1> is a set of sparse approximation schemes that allow the prohibitive computational cost of gp <e2>methods</e2>, typically o(n 3 ) for training and o(n 2 ) for prediction where n is the number of training points, to be substantially reduced whilst still retaining accuracy."
Compare(e1, e2)
Comment:

976	"one of the pillars of the modern gaussian process probabilistic modelling approach is a set of sparse approximation schemes that allow the prohibitive <e1>computational cost</e1> of gp <e2>methods</e2>, typically o(n 3 ) for training and o(n 2 ) for prediction where n is the number of training points, to be substantially reduced whilst still retaining accuracy."
Evaluate-for(e1, e2)
Comment:

977	"one of the pillars of the modern gaussian process probabilistic modelling approach is a set of sparse approximation schemes that allow the prohibitive computational cost of gp methods, typically o(n 3 ) for <e1>training</e1> and o(n 2 ) for prediction where n is the number of <e2>training</e2> points, to be substantially reduced whilst still retaining accuracy."
sameAs(e1, e2)
Comment:

978	"arguably the most important and influential <e1>approximations</e1> of this sort are pseudo-point <e2>approximation</e2> schemes that employ a set of m n pseudo-points to summarise the observational data thereby reducing computational costs to o(n m 2 ) and o(m 2 ) for training and prediction, respectively [2, 3] ."
sameAs(e1, e2)
Comment:

979	"stochastic optimisation methods that employ mini-batches of training <e1>data</e1> can be used to further reduce computational costs [4, 5, 6, 7] , allowing gps to be scaled to datasets comprising millions of <e2>data</e2> points."
sameAs(e1, e2)
Comment:

980	"our hypothesis is that these concentric spherical <e1>convolutions</e1> should outperform standard 3d <e2>convolutions</e2> in cases where data is naturally parameterized in terms of a radial component."
sameAs(e1, e2)
Comment:

981	"we will consider structural <e1>environments</e1> in a molecule as being defined from the viewpoint of a single amino acid or nucleotide: how does such an entity experience its <e2>environment</e2> in terms of the mass and charge of surrounding atoms?"
sameAs(e1, e2)
Comment:

982	"we show <e1>that</e1> a standard convolutional neural network architectures can be used to learn various features of molecular structure, and <e2>that</e2> our spherical convolutions indeed outperform standard 3d convolutions for this purpose."
sameAs(e1, e2)
Comment:

983	"we show that a standard convolutional neural network architectures can be used to learn various features of molecular structure, and that our spherical <e1>convolutions</e1> indeed outperform standard 3d <e2>convolutions</e2> for this purpose."
sameAs(e1, e2)
Comment:

984	"unfortunately, this convenience does not trivially extend to <e1>data</e1> in non-euclidean spaces, such as spherical <e2>data</e2>."
sameAs(e1, e2)
Comment:

985	"in this paper, we introduce two strategies for conducting convolutions on the <e1>sphere</e1>, using either a spherical-polar grid or a grid based on the cubed-<e2>sphere</e2> representation."
sameAs(e1, e2)
Comment:

986	"we show <e1>that</e1> the models are capable of learning non-trivial functions in these molecular environments, and <e2>that</e2> our spherical convolutions generally outperform standard 3d convolutions in this setting."
sameAs(e1, e2)
Comment:

987	"we show that the models are capable of learning non-trivial functions in these molecular environments, and that our spherical <e1>convolutions</e1> generally outperform standard 3d <e2>convolutions</e2> in this setting."
sameAs(e1, e2)
Comment:

988	"in particular, despite the lack of any <e1>domain</e1> specific feature-engineering, we demonstrate performance comparable to state-of-the-art methods in the field, which build on decades of <e2>domain</e2>-specific knowledge."
sameAs(e1, e2)
Comment:

989	"for early visual processing stages like the retina or primary visual cortex, several nonlinear <e1>methods</e1> have been proposed, including energy <e2>models</e2> [7, 8] , spike-triggered covariance methods [9, 10] , linear-nonlinear (ln-ln) cascades [11, 12] , convolutional subunit models [13, 14] and glms based on handcrafted nonlinear feature spaces [15] ."
Compare(e1, e2)
Comment:

990	"for early visual processing stages like the retina or primary visual cortex, several nonlinear <e1>methods</e1> have been proposed, including energy models [7, 8] , spike-triggered covariance <e2>methods</e2> [9, 10] , linear-nonlinear (ln-ln) cascades [11, 12] , convolutional subunit models [13, 14] and glms based on handcrafted nonlinear feature spaces [15] ."
sameAs(e1, e2)
Comment:

991	"for early visual processing stages like the retina or primary visual cortex, several nonlinear <e1>methods</e1> have been proposed, including energy models [7, 8] , spike-triggered covariance methods [9, 10] , linear-nonlinear (ln-ln) cascades [11, 12] , convolutional subunit <e2>models</e2> [13, 14] and glms based on handcrafted nonlinear feature spaces [15] ."
Compare(e1, e2)
Comment:

992	"for early visual processing stages like the retina or primary visual cortex, several nonlinear methods have been proposed, including energy <e1>models</e1> [7, 8] , spike-triggered covariance methods [9, 10] , linear-nonlinear (ln-ln) cascades [11, 12] , convolutional subunit <e2>models</e2> [13, 14] and glms based on handcrafted nonlinear feature spaces [15] ."
sameAs(e1, e2)
Comment:

993	"for early visual processing stages like the retina or primary visual cortex, several nonlinear methods have been proposed, including energy models [7, 8] , spike-triggered covariance <e1>methods</e1> [9, 10] , linear-nonlinear (ln-ln) cascades [11, 12] , convolutional subunit <e2>models</e2> [13, 14] and glms based on handcrafted nonlinear feature spaces [15] ."
Compare(e1, e2)
Comment:

994	"traditional methods for neural <e1>system</e1> identification do not capitalize on <e2>this</e2> separation of "what" and "where"."
Used-for(e1, e2)
Comment:

995	"one of the most obvious similarities between different neurons, however, is that the visual <e1>system</e1> simultaneously extracts similar <e2>features</e2> at many different locations."
Used-for(e1, e2)
Comment:

996	"thus, <e1>we</e1> should be able to learn much more complex nonlinear functions by combining data from many neurons and learning a common feature space from which <e2>we</e2> can linearly predict the activity of each neuron."
sameAs(e1, e2)
Comment:

997	"learning deep convolutional feature spaces <e1>that</e1> are shared among many neurons provides an exciting path forward, but the architectural design needs to account for data limitations: while new experimental techniques enable recordings from thousands of neurons, experimental time is limited so <e2>that</e2> one can sample only a small fraction of each neuron's response space."
sameAs(e1, e2)
Comment:

998	"here, we show <e1>that</e1> a major bottleneck for fitting convolutional neural networks (cnns) to neural data is the estimation of the individual receptive field locations -a problem <e2>that</e2> has been scratched only at the surface thus far."
sameAs(e1, e2)
Comment:

999	"abstract this paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space <e1>that</e1> constitutes its frames, but in a latent space <e2>that</e2> describes the non-linear dynamics of the objects in its world."
sameAs(e1, e2)
Comment:

1000	"the temporal <e1>dynamics</e1> in the learned a t -manifold are modelled with a linear gaussian state space model that is adapted to handle complex <e2>dynamics</e2> (despite the linear relations among its states z t )."
sameAs(e1, e2)
Comment:

1001	"the temporal dynamics in the learned a t -manifold are modelled with a linear gaussian <e1>state</e1> space model that is adapted to handle complex dynamics (despite the linear relations among its <e2>states</e2> z t )."
sameAs(e1, e2)
Comment:

1002	"the separation between recognition and dynamics <e1>model</e1> allows for missing data imputation to be done via a combination of the latent states z t of the <e2>model</e2> and its encodings a t only, without having to forward-sample high-dimensional images x t in an autoregressive way."
sameAs(e1, e2)
Comment:

1003	"the <e1>model</e1> is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing <e2>methods</e2> in generative and missing data imputation tasks."
Compare(e1, e2)
Comment:

1004	"to disentangle two latent representations, an object's, and <e1>that</e1> of its dynamics, this paper introduces kalman variational auto-encoders (kvaes), a model <e2>that</e2> separates an intuition of dynamics from an object recognition network (section 3)."
sameAs(e1, e2)
Comment:

1005	"to disentangle two latent representations, an object's, and that of its <e1>dynamics</e1>, this paper introduces kalman variational auto-encoders (kvaes), a model that separates an intuition of <e2>dynamics</e2> from an object recognition network (section 3)."
sameAs(e1, e2)
Comment:

1006	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) can implicitly learn rich distributions over images, audio, and data which are hard to model with an explicit likelihood."
sameAs(e1, e2)
Comment:

1007	"the inferred <e1>data</e1> distribution can then be used for accurate and highly <e2>data</e2>-efficient semi-supervised learning."
sameAs(e1, e2)
Comment:

1008	"by exploring an expressive posterior over the parameters of the generator, the bayesian <e1>gan</e1> avoids mode-collapse, produces interpretable and diverse candidate samples, and provides state-of-the-art quantitative results for semi-supervised learning on benchmarks including svhn, celeba, and cifar-10, outperforming dcgan, wasserstein <e2>gans</e2>, and dcgan ensembles."
sameAs(e1, e2)
Comment:

1009	"two major <e1>approaches</e1> to recommender <e2>systems</e2> are collaborative [5] and content [32] filtering techniques."
Evaluate-for(e1, e2)
Comment:

1010	"mathematically, a recommendation method can be posed as a <e1>matrix</e1> completion problem [9] , where columns and rows represent users and items, respectively, and <e2>matrix</e2> values represent scores determining whether a user would like an item or not."
sameAs(e1, e2)
Comment:

1011	"recent works have showed a boost of performance of these techniques when introducing the pairwise relationships between users/items in the form of <e1>graphs</e1>, and imposing smoothness priors on these <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

1012	"these approaches can be generally related to the field of signal processing on <e1>graphs</e1> [37] , extending classical harmonic analysis methods to non-euclidean domains (<e2>graphs</e2>)."
sameAs(e1, e2)
Comment:

1013	"our <e1>matrix</e1> completion architecture combines a novel multi-graph convolutional neural network that can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score <e2>matrix</e2>."
sameAs(e1, e2)
Comment:

1014	"our matrix completion architecture combines a novel multi-<e1>graph</e1> convolutional neural network that can learn meaningful statistical <e2>graph</e2>-structured patterns from users and items, and a recurrent neural network that applies a learnable diffusion on the score matrix."
sameAs(e1, e2)
Comment:

1015	"our matrix completion architecture combines a novel multi-graph convolutional neural network <e1>that</e1> can learn meaningful statistical graph-structured patterns from users and items, and a recurrent neural network <e2>that</e2> applies a learnable diffusion on the score matrix."
sameAs(e1, e2)
Comment:

1016	"introduction recommender <e1>systems</e1> have become a central part of modern intelligent <e2>systems</e2>."
sameAs(e1, e2)
Comment:

1017	"these networks are typically trained using variants of <e1>stochastic gradient descent</e1> (<e2>sgd</e2>), allowing training on large data with modern gpu hardware."
sameAs(e1, e2)
Comment:

1018	"first, we identify a basic underlying limitation in using <e1>gradient descent</e1>-based methods in conjunction with smooth (infinitely differentiable) kernels typically used in machine learning, showing that only very smooth functions can be approximated after polynomially many steps of <e2>gradient descent</e2>."
sameAs(e1, e2)
Comment:

1019	"while many approximate second-order methods are available, they rely on low-rank <e1>approximations</e1> and, as we discuss below, lead to over-regularization (<e2>approximation</e2> bias)."
sameAs(e1, e2)
Comment:

1020	"while eigenpro uses approximate second-order information, it is only employed to modify first-order <e1>gradient descent</e1>, leading to the same mathematical solution as <e2>gradient descent</e2> (without introducing a bias)."
sameAs(e1, e2)
Comment:

1021	"2 gradient descent for shallow <e1>methods</e1> shallow <e2>methods</e2>."
sameAs(e1, e2)
Comment:

1022	"least square linear regression aims to recover the parameter vector α * <e1>that</e1> minimize the empirical loss such <e2>that</e2> α * = arg min α∈h l(α) where l(α) def = 1 n n i=1 ( α, x i h − y i ) 2 ."
sameAs(e1, e2)
Comment:

1023	"since ∇l(α) | α=α * = 0, minimizing l(α) is equivalent to solving the linear <e1>system</e1> hα − b = 0 (1) with b = x t y. when d = dim(h) < ∞, the time complexity of solving the linear <e2>system</e2> in eq."
sameAs(e1, e2)
Comment:

1024	"among them, generative adversarial <e1>imitation learning</e1> (gail, [12] ) is a model-free <e2>imitation learning</e2> method that is highly effective and scales to relatively high dimensional environments."
sameAs(e1, e2)
Comment:

1025	"our approach is an extension of gail, where the objective is augmented with a mutual information term between the latent variables and the observed <e1>state</e1>-<e2>action</e2> pairs."
Conjunction(e1, e2)
Comment:

1026	"while showing stronger representation power over many conventional hand-crafted features, cnns often require a large amount of <e1>training</e1> data and face certain <e2>training</e2> difficulties such as overfitting, vanishing/exploding gradient, covariate shift, etc."
sameAs(e1, e2)
Comment:

1027	"however, [20] pointed out that residual <e1>networks</e1> (resnets) are essentially an exponential ensembles of shallow <e2>networks</e2> where they avoid the vanishing/exploding gradient problem but do not provide direct solutions."
sameAs(e1, e2)
Comment:

1028	"we argue this motivation from several aspects: <e1>training</e1> stability, <e2>training</e2> efficiency, and generalization power."
sameAs(e1, e2)
Comment:

1029	"however, the rest of the <e1>network</e1> remains a conventional convolution <e2>network</e2>."
sameAs(e1, e2)
Comment:

1030	"one simply needs to replace the convolutional <e1>operators</e1> and the loss functions with the proposed sphereconv <e2>operators</e2> and hyperspherical loss functions."
sameAs(e1, e2)
Comment:

1031	"one simply needs to replace the convolutional operators and the <e1>loss functions</e1> with the proposed sphereconv operators and hyperspherical <e2>loss functions</e2>."
sameAs(e1, e2)
Comment:

1032	"abstract dynamic neural network toolkits such as pytorch, dynet, and chainer offer more flexibility for implementing models <e1>that</e1> cope with data of varying dimensions and structure, relative to toolkits <e2>that</e2> operate on statically declared computations (e.g., tensorflow, cntk, and theano)."
sameAs(e1, e2)
Comment:

1033	"in some cases this is easy: when <e1>inputs</e1> and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector <e2>inputs</e2>), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1034	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized <e1>tensors</e1> (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on <e2>tensors</e2> (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1035	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the <e1>mnist</e1> and cifar <e2>datasets</e2>, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
isA(e1, e2)
Comment:

1036	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn <e2>rnn</e2> rnn rnn rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1037	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn rnn rnn <e2>rnn</e2> rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1038	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn rnn rnn rnn rnn <e2>rnn</e2> rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1039	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn rnn rnn rnn rnn rnn rnn <e2>rnn</e2> l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1040	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn rnn rnn rnn rnn rnn rnn rnn l <e2>rnn</e2> rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1041	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library <e1>rnn</e1> rnn rnn rnn rnn rnn rnn rnn rnn l rnn rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1042	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn <e1>rnn</e1> rnn <e2>rnn</e2> rnn rnn rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1043	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn <e1>rnn</e1> rnn rnn rnn <e2>rnn</e2> rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1044	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn <e1>rnn</e1> rnn rnn rnn rnn rnn <e2>rnn</e2> l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1045	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn <e1>rnn</e1> rnn rnn rnn rnn rnn rnn l <e2>rnn</e2> rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1046	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn <e1>rnn</e1> rnn rnn rnn rnn rnn rnn l rnn rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1047	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn <e1>rnn</e1> rnn <e2>rnn</e2> rnn rnn l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1048	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn <e1>rnn</e1> rnn rnn rnn <e2>rnn</e2> l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1049	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn <e1>rnn</e1> rnn rnn rnn rnn l <e2>rnn</e2> rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1050	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn <e1>rnn</e1> rnn rnn rnn rnn l rnn rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1051	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn <e1>rnn</e1> rnn <e2>rnn</e2> l rnn rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1052	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn <e1>rnn</e1> rnn rnn l <e2>rnn</e2> rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1053	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn <e1>rnn</e1> rnn rnn l rnn rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1054	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn <e1>rnn</e1> l <e2>rnn</e2> rnn rnn rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1055	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn <e1>rnn</e1> l rnn rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1056	"in some cases this is easy: when inputs and outputs are naturally represented as fixed sized tensors (e.g., images of a fixed size such those in the mnist and cifar datasets, or regression problems on fixed sized vector inputs), and the computations required to process each instance are instance-invariant and expressible as standard operations on tensors (e.g., a series of matrix multiplications, convolutions, and elementwise nonlinearities), a suitably flexible tensor library rnn rnn rnn rnn rnn rnn rnn rnn rnn l <e1>rnn</e1> rnn <e2>rnn</e2> rnn l l (1) l (2) l ( y (2) y (3) x ( two computation graphs for computing the loss on a minibatch of three training instances consisting of a sequence of input vectors paired with a fixed sized output vector."
sameAs(e1, e2)
Comment:

1057	"the same <e1>computation</e1> is executed by the right-hand ("batched") <e2>computation</e2> graph: it aggregates the inputs in order to make better use of modern processors."
sameAs(e1, e2)
Comment:

1058	"experiments show <e1>that</e1> our algorithm compares favorably to manually batched code, <e2>that</e2> significant speed improvements are possible on architectures with no straightforward manual batching design, and that we obtain better performance than tensorflow fold [19] , an alternative framework built to simulate dynamic graph definition and automatic batching on top of tensorflow ( §4)."
sameAs(e1, e2)
Comment:

1059	"experiments show <e1>that</e1> our algorithm compares favorably to manually batched code, that significant speed improvements are possible on architectures with no straightforward manual batching design, and <e2>that</e2> we obtain better performance than tensorflow fold [19] , an alternative framework built to simulate dynamic graph definition and automatic batching on top of tensorflow ( §4)."
sameAs(e1, e2)
Comment:

1060	"experiments show that our <e1>algorithm</e1> compares favorably to manually batched code, that significant speed improvements are possible on architectures with no straightforward manual batching design, and that we obtain better performance than tensorflow fold [19] , an alternative <e2>framework</e2> built to simulate dynamic graph definition and automatic batching on top of tensorflow ( §4)."
Part-of(e1, e2)
Comment:

1061	"experiments show that our algorithm compares favorably to manually batched code, <e1>that</e1> significant speed improvements are possible on architectures with no straightforward manual batching design, and <e2>that</e2> we obtain better performance than tensorflow fold [19] , an alternative framework built to simulate dynamic graph definition and automatic batching on top of tensorflow ( §4)."
sameAs(e1, e2)
Comment:

1062	"experiments show that our algorithm compares favorably to manually batched code, that significant speed improvements are possible on architectures with no straightforward manual batching design, and that we obtain better performance than <e1>tensorflow</e1> fold [19] , an alternative framework built to simulate dynamic graph definition and automatic batching on top of <e2>tensorflow</e2> ( §4)."
sameAs(e1, e2)
Comment:

1063	"developers simply write minibatch <e1>computations</e1> as aggregations of single instance <e2>computations</e2>, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations."
sameAs(e1, e2)
Comment:

1064	"on a variety of tasks, we obtain throughput similar to <e1>that</e1> obtained with manual batches, as well as comparable speedups over singleinstance learning on architectures <e2>that</e2> are impractical to batch manually."
sameAs(e1, e2)
Comment:

1065	"introduction modern cpus and gpus evaluate batches of arithmetic <e1>operations</e1> significantly faster than the sequential evaluation of the same <e2>operations</e2>."
sameAs(e1, e2)
Comment:

1066	"for example, performing elementwise operations takes nearly the same amount of time on the gpu whether operating on tens or on thousands of elements, and multiplying a few hundred different vectors by the same <e1>matrix</e1> is significantly slower than executing a single (equivalent) <e2>matrix</e2>-matrix product using an optimized gemm implementation on either a gpu or a cpu."
sameAs(e1, e2)
Comment:

1067	"we use our streaming algorithm for <e1>neural network</e1> <e2>interpretability</e2> on inception v3 [szegedy et al, 2016] ."
Used-for(e1, e2)
Comment:

1068	"for that purpose, we define a new set <e1>function</e1> maximization problem similar to lime [ribeiro et al, 2016] and apply our framework to approximately maximize this <e2>function</e2>."
sameAs(e1, e2)
Comment:

1069	"our <e1>algorithm</e1> obtains similar explanations of inception v3 predictions 10 times faster than the state-of-the-art lime <e2>framework</e2> of ribeiro et al [2016] ."
Part-of(e1, e2)
Comment:

1070	"notably, this model utilizes sampled mini-batches called episodes during training, where each episode is designed to mimic the few-shot <e1>task</e1> by subsampling classes as well as <e2>data</e2> points."
Conjunction(e1, e2)
Comment:

1071	"their <e1>approach</e1> involves training an lstm [11] to produce the updates to a <e2>classifier</e2>, given an episode, such that it will generalize well to a test-set."
Used-for(e1, e2)
Comment:

1072	"here, rather than training a single <e1>model</e1> over multiple episodes, the lstm meta-learner learns to train a custom <e2>model</e2> for each episode."
sameAs(e1, e2)
Comment:

1073	"introduction few-shot classification [22, 18, 15] is a task in which a classifier must be adapted to accommodate new <e1>classes</e1> not seen in training, given only a few examples of each of these <e2>classes</e2>."
sameAs(e1, e2)
Comment:

1074	"abstract <e1>we</e1> consider bayesian methods for multi-information source optimization (miso), in which <e2>we</e2> seek to optimize an expensive-to-evaluate black-box objective function while also accessing cheaper but biased and noisy approximations ("information sources")."
sameAs(e1, e2)
Comment:

1075	"they proposed a gp <e1>model</e1> to jointly <e2>model</e2> such "auxiliary tasks" and the primary task, building on previous work on gp regression for multiple tasks in [3, 10, 35] ."
sameAs(e1, e2)
Comment:

1076	"they proposed a gp model to jointly model such "auxiliary <e1>tasks</e1>" and the primary task, building on previous work on gp regression for multiple <e2>tasks</e2> in [3, 10, 35] ."
sameAs(e1, e2)
Comment:

1077	"we demonstrate in experiments <e1>that</e1> our approach improves over the method of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias <e2>that</e2> varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive entropy search which maximally reduces the maximizer's entropy in one step and hence only indirectly reduces regret."
sameAs(e1, e2)
Comment:

1078	"we demonstrate in experiments that our <e1>approach</e1> improves over the <e2>method</e2> of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive entropy search which maximally reduces the maximizer's entropy in one step and hence only indirectly reduces regret."
Compare(e1, e2)
Comment:

1079	"we demonstrate in experiments that our approach improves over the <e1>method</e1> of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to <e2>model</e2> bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive entropy search which maximally reduces the maximizer's entropy in one step and hence only indirectly reduces regret."
Used-for(e1, e2)
Comment:

1080	"we demonstrate in experiments that our approach improves over the <e1>method</e1> of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive entropy <e2>search</e2> which maximally reduces the maximizer's entropy in one step and hence only indirectly reduces regret."
Used-for(e1, e2)
Comment:

1081	"we demonstrate in experiments that our approach improves over the method of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in <e1>one</e1> step, unlike predictive entropy search which maximally reduces the maximizer's entropy in <e2>one</e2> step and hence only indirectly reduces regret."
sameAs(e1, e2)
Comment:

1082	"we demonstrate in experiments that our approach improves over the method of swersky et al [34] , and we believe this improvement results from two factors: first, our statistical model is more flexible in its ability to model bias that varies across the domain; second, our acquisition function directly and maximally reduces simple regret in one step, unlike predictive <e1>entropy</e1> search which maximally reduces the maximizer's <e2>entropy</e2> in one step and hence only indirectly reduces regret."
sameAs(e1, e2)
Comment:

1083	"we demonstrate in experiments that our <e1>approach</e1> improves over the <e2>method</e2> of lam et al [18] , and we believe this improvement results from two factors: first, their statistical approach assumes an independent prior on each is, despite their being linked through modeling a common objective; and second their acquisition function selects the point to sample and the is to query separately via a heuristic rather than jointly using an optimality analysis."
Compare(e1, e2)
Comment:

1084	"poloczek et al [25] demonstrated that a variant of the <e1>algorithm</e1> proposed in this article can reduce the optimization costs significantly by warm-starting bayesian optimization, as does the <e2>algorithm</e2> of swersky et al [34] ."
sameAs(e1, e2)
Comment:

1085	"poloczek et al [25] demonstrated that a variant of the algorithm proposed in this article can reduce the <e1>optimization</e1> costs significantly by warm-starting bayesian <e2>optimization</e2>, as does the algorithm of swersky et al [34] ."
sameAs(e1, e2)
Comment:

1086	"outside of both the miso and multi-fidelity settings, klein et al [17] considered hyperparameter <e1>optimization</e1> of machine learning algorithms over a large dataset d. supposing access to subsets of d of arbitrary sizes, they show how to exploit regularity of performance across dataset sizes to significantly speed up the <e2>optimization</e2> process for support vector machines and neural networks."
sameAs(e1, e2)
Comment:

1087	"outside of both the miso and multi-fidelity settings, klein et al [17] considered hyperparameter optimization of machine learning algorithms over a large dataset d. supposing access to subsets of d of arbitrary <e1>sizes</e1>, they show how to exploit regularity of performance across dataset <e2>sizes</e2> to significantly speed up the optimization process for support vector machines and neural networks."
sameAs(e1, e2)
Comment:

1088	"the class of gp covariance <e1>kernels</e1> we propose are a subset of the class of linear models of coregionalization <e2>kernels</e2> [10, 2] , with a restricted form derived from a generative model particular to miso."
sameAs(e1, e2)
Comment:

1089	"introduction <e1>we</e1> consider bayesian multi-information source optimization (miso), in which <e2>we</e2> optimize an expensive-to-evaluate black-box objective function while optionally accessing cheaper biased noisy approximations, often referred to as "information sources (is)"."
sameAs(e1, e2)
Comment:

1090	"during the forward propagation, skip connection enables a very top layer to access information from a distant bottom layer; while for the backward propagation, it facilitates <e1>gradient</e1> back-propagation to the bottom layer without diminishing magnitude, which effectively alleviates the <e2>gradient</e2> vanishing problem and eases the optimization."
sameAs(e1, e2)
Comment:

1091	"densenet uses a densely connected path to concatenate the input <e1>features</e1> with the output <e2>features</e2>, enabling each micro-block to receive raw information from all previous micro-blocks."
sameAs(e1, e2)
Comment:

1092	"similar with residual <e1>network</e1> family, densenet can be categorized to the densely connected <e2>network</e2> family."
sameAs(e1, e2)
Comment:

1093	"similar with residual network <e1>family</e1>, densenet can be categorized to the densely connected network <e2>family</e2>."
sameAs(e1, e2)
Comment:

1094	"in particular, we first provide a new understanding of the densely connected <e1>networks</e1> from the lens of a higher order recurrent neural network (hornn) [19] , and explore the relations between densely connected <e2>networks</e2> and residual networks."
sameAs(e1, e2)
Comment:

1095	"in particular, we first provide a new understanding of the densely connected <e1>networks</e1> from the lens of a higher order recurrent neural network (hornn) [19] , and explore the relations between densely connected networks and residual <e2>networks</e2>."
sameAs(e1, e2)
Comment:

1096	"in particular, we first provide a new understanding of the densely connected networks from the lens of a higher order recurrent neural network (hornn) [19] , and explore the relations between densely connected <e1>networks</e1> and residual <e2>networks</e2>."
sameAs(e1, e2)
Comment:

1097	"more specifically, we bridge the densely connected <e1>networks</e1> with the hornns, showing that the densely connected <e2>networks</e2> are hornns when the weights are shared across steps."
sameAs(e1, e2)
Comment:

1098	"inspired by [12] which demonstrates the relations between the residual <e1>networks</e1> and rnns, we prove that the residual <e2>networks</e2> are densely connected networks when connections are shared across layers."
sameAs(e1, e2)
Comment:

1099	"inspired by [12] which demonstrates the relations between the residual <e1>networks</e1> and rnns, we prove that the residual networks are densely connected <e2>networks</e2> when connections are shared across layers."
sameAs(e1, e2)
Comment:

1100	"inspired by [12] which demonstrates the relations between the residual networks and rnns, we prove that the residual <e1>networks</e1> are densely connected <e2>networks</e2> when connections are shared across layers."
sameAs(e1, e2)
Comment:

1101	"with this unified view on the state-of-the-art deep architecture, we find that the deep residual <e1>networks</e1> implicitly reuse the features through the residual path, while densely connected <e2>networks</e2> keep exploring new features through the densely connected path."
sameAs(e1, e2)
Comment:

1102	"with this unified view on the state-of-the-art deep architecture, we find that the deep residual networks implicitly reuse the <e1>features</e1> through the residual path, while densely connected networks keep exploring new <e2>features</e2> through the densely connected path."
sameAs(e1, e2)
Comment:

1103	"by revealing the equivalence of the state-of-the-art residual <e1>network</e1> (resnet) and densely convolutional <e2>network</e2> (densenet) within the hornn framework, we find that resnet enables feature re-usage while densenet enables new features exploration which are both important for learning good representations."
sameAs(e1, e2)
Comment:

1104	"by revealing the equivalence of the state-of-the-art residual network (<e1>resnet</e1>) and densely convolutional network (densenet) within the hornn framework, we find that <e2>resnet</e2> enables feature re-usage while densenet enables new features exploration which are both important for learning good representations."
sameAs(e1, e2)
Comment:

1105	"to enjoy the benefits from both path topologies, our proposed dual path network shares common <e1>features</e1> while maintaining the flexibility to explore new <e2>features</e2> through dual path architectures."
sameAs(e1, e2)
Comment:

1106	"in particular, on the imagnet-1k dataset, a shallow dpn surpasses the best resnext-101(64 × 4d) with 26% smaller <e1>model</e1> size, 25% less computational cost and 8% lower memory consumption, and a deeper dpn (dpn-131) further pushes the state-of-the-art single <e2>model</e2> performance with about 2 times faster training speed."
sameAs(e1, e2)
Comment:

1107	"experiments on the places365 large-scale scene dataset, <e1>pascal voc</e1> detection dataset, and <e2>pascal voc</e2> segmentation dataset also demonstrate its consistently better performance than densenet, resnet and the latest resnext model over various applications."
sameAs(e1, e2)
Comment:

1108	"there are compelling reasons to use gps, even when little is known about the <e1>data</e1>: a gp grows in complexity to suit the <e2>data</e2>; a gp is robust to overfitting while providing reasonable error bars on predictions; a gp can model a rich class of functions with few hyperparameters."
sameAs(e1, e2)
Comment:

1109	"to some extent <e1>kernels</e1> can be learned from data, but inference over a large and richly parameterized space of <e2>kernels</e2> is expensive, and approximate methods may be at risk of overfitting."
sameAs(e1, e2)
Comment:

1110	"optimization of the marginal likelihood with respect to <e1>hyperparameters</e1> approximates bayesian inference only if the number of <e2>hyperparameters</e2> is small (mackay, 1999) ."
sameAs(e1, e2)
Comment:

1111	"kernels can be combined through sums and products (duvenaud et al, 2013) to create more expressive compositional <e1>kernels</e1>, but this approach is limited to simple base <e2>kernels</e2>, and their optimization is expensive."
sameAs(e1, e2)
Comment:

1112	"dgps are richer <e1>models</e1> than standard gps, just as deep networks are richer than generalized linear <e2>models</e2>."
sameAs(e1, e2)
Comment:

1113	"in common with many state-of-the-art gp approximation schemes <e1>we</e1> start from a sparse inducing point variational framework (matthews et al, 2016) to achieve computational tractability within each layer, but <e2>we</e2> do not force independence between the layers."
sameAs(e1, e2)
Comment:

1114	"since <e1>we</e1> preserve the non-linearity of the full model in our variational posterior <e2>we</e2> lose analytic tractability."
sameAs(e1, e2)
Comment:

1115	"we are primarily interested in large <e1>data</e1> applications, so we further subsample the <e2>data</e2> in minibatches."
sameAs(e1, e2)
Comment:

1116	"the qmdp-net combines the strengths of <e1>model-free</e1> learning and <e2>model-based</e2> planning."
Conjunction(e1, e2)
Comment:

1117	"qmdp-net combines the strengths of <e1>model-free</e1> learning and <e2>model-based</e2> planning."
Conjunction(e1, e2)
Comment:

1118	"a qmdp-net is a recurrent <e1>policy</e1> network, but it represents a <e2>policy</e2> by connecting a pomdp model with an algorithm that solves the model, thus embedding the solution structure of planning in a network 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. )"
sameAs(e1, e2)
Comment:

1119	"a qmdp-net is a recurrent policy <e1>network</e1>, but it represents a policy by connecting a pomdp model with an algorithm that solves the model, thus embedding the solution structure of planning in a <e2>network</e2> 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. )"
sameAs(e1, e2)
Comment:

1120	"a qmdp-net is a recurrent policy network, but it represents a policy by connecting a pomdp <e1>model</e1> with an algorithm that solves the <e2>model</e2>, thus embedding the solution structure of planning in a network 31st conference on neural information processing systems (nips 2017), long beach, ca, usa. )"
sameAs(e1, e2)
Comment:

1121	"a policy trained on expert demonstrations in a set of randomly generated <e1>environments</e1> generalizes to a new <e2>environment</e2>."
sameAs(e1, e2)
Comment:

1122	"it is a recurrent <e1>policy</e1> network, but it represents a <e2>policy</e2> for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a network learning architecture."
sameAs(e1, e2)
Comment:

1123	"it is a recurrent policy <e1>network</e1>, but it represents a policy for a parameterized set of tasks by connecting a model with a planning algorithm that solves the model, thus embedding the solution structure of planning in a <e2>network</e2> learning architecture."
sameAs(e1, e2)
Comment:

1124	"it is a recurrent policy network, but it represents a policy for a parameterized set of tasks by connecting a <e1>model</e1> with a planning algorithm that solves the <e2>model</e2>, thus embedding the solution structure of planning in a network learning architecture."
sameAs(e1, e2)
Comment:

1125	"the trained policy generalizes to new <e1>environments</e1> and also "transfers" to more complex <e2>environments</e2> ( fig."
sameAs(e1, e2)
Comment:

1126	"interestingly, while qmdp-net encodes the qmdp <e1>algorithm</e1>, it sometimes outperformed the qmdp <e2>algorithm</e2> in our experiments, as a result of end-to-end learning."
sameAs(e1, e2)
Comment:

1127	"we train a qmdpnet on different <e1>tasks</e1> so that it can generalize to new ones in the parameterized task set and "transfer" to other similar <e2>tasks</e2> beyond the set."
sameAs(e1, e2)
Comment:

1128	"interestingly, while qmdp-net encodes the qmdp <e1>algorithm</e1>, it sometimes outperforms the qmdp <e2>algorithm</e2> in the experiments, as a result of end-to-end learning."
sameAs(e1, e2)
Comment:

1129	"in a partially observable world, the agent cannot determine the state exactly based on the current observation; to plan optimal <e1>actions</e1>, it must integrate information over the past history of <e2>actions</e2> and observations."
sameAs(e1, e2)
Comment:

1130	"abstract understanding why a model makes a certain <e1>prediction</e1> can be as crucial as the <e2>prediction</e2>'s accuracy in many applications."
sameAs(e1, e2)
Comment:

1131	"in some applications, simple <e1>models</e1> (e.g., linear <e2>models</e2>) are often preferred for their ease of interpretation, even if they may be less accurate than complex ones."
sameAs(e1, e2)
Comment:

1132	"however, the growing availability of big <e1>data</e1> has increased the benefits of using complex <e2>models</e2>, so bringing to the forefront the trade-off between accuracy and interpretability of a model's output."
Used-for(e1, e2)
Comment:

1133	"but an understanding of how these <e1>methods</e1> relate and when one <e2>method</e2> is preferable to another is still lacking."
Compare(e1, e2)
Comment:

1134	"1 our <e1>approach</e1> leads to three potentially surprising results that bring clarity to the growing space of <e2>methods</e2>: 1. we introduce the perspective of viewing any explanation of a model's prediction as a model itself, which we term the explanation model."
Compare(e1, e2)
Comment:

1135	"1 our approach leads to three potentially surprising <e1>results</e1> that bring clarity to the growing space of <e2>methods</e2>: 1. we introduce the perspective of viewing any explanation of a model's prediction as a model itself, which we term the explanation model."
Compare(e1, e2)
Comment:

1136	"1 our approach leads to three potentially surprising results that bring clarity to the growing space of methods: 1. <e1>we</e1> introduce the perspective of viewing any explanation of a model's prediction as a model itself, which <e2>we</e2> term the explanation model."
sameAs(e1, e2)
Comment:

1137	"1 our approach leads to three potentially surprising results that bring clarity to the growing space of methods: 1. we introduce the perspective of viewing any explanation of a <e1>model</e1>'s prediction as a <e2>model</e2> itself, which we term the explanation model."
sameAs(e1, e2)
Comment:

1138	"1 our approach leads to three potentially surprising results that bring clarity to the growing space of methods: 1. we introduce the perspective of viewing any explanation of a <e1>model</e1>'s prediction as a model itself, which we term the explanation <e2>model</e2>."
sameAs(e1, e2)
Comment:

1139	"1 our approach leads to three potentially surprising results that bring clarity to the growing space of methods: 1. we introduce the perspective of viewing any explanation of a model's prediction as a <e1>model</e1> itself, which we term the explanation <e2>model</e2>."
sameAs(e1, e2)
Comment:

1140	"this lets us define the class of additive feature attribution <e1>methods</e1> (section 2), which unifies six current <e2>methods</e2>."
sameAs(e1, e2)
Comment:

1141	"2. we then show <e1>that</e1> game theory results guaranteeing a unique solution apply to the entire class of additive feature attribution methods (section 3) and propose shap values as a unified measure of feature importance <e2>that</e2> various methods approximate (section 4)."
sameAs(e1, e2)
Comment:

1142	"2. we then show that game theory <e1>results</e1> guaranteeing a unique solution apply to the entire class of additive feature attribution <e2>methods</e2> (section 3) and propose shap values as a unified measure of feature importance that various methods approximate (section 4)."
Compare(e1, e2)
Comment:

1143	"2. we then show that game theory <e1>results</e1> guaranteeing a unique solution apply to the entire class of additive feature attribution methods (section 3) and propose shap values as a unified measure of feature importance that various <e2>methods</e2> approximate (section 4)."
Compare(e1, e2)
Comment:

1144	"2. we then show that game theory results guaranteeing a unique solution apply to the entire class of additive <e1>feature</e1> attribution methods (section 3) and propose shap values as a unified measure of <e2>feature</e2> importance that various methods approximate (section 4)."
sameAs(e1, e2)
Comment:

1145	"2. we then show that game theory results guaranteeing a unique solution apply to the entire class of additive feature attribution <e1>methods</e1> (section 3) and propose shap values as a unified measure of feature importance that various <e2>methods</e2> approximate (section 4)."
sameAs(e1, e2)
Comment:

1146	"3. we propose new shap value estimation <e1>methods</e1> and demonstrate that they are better aligned with human intuition as measured by user studies and more effectually discriminate among model output classes than several existing <e2>methods</e2> (section 5)."
sameAs(e1, e2)
Comment:

1147	"3. we propose new shap value estimation methods and demonstrate that they are better aligned with human intuition as measured by user studies and more effectually discriminate among <e1>model</e1> output classes than several existing <e2>methods</e2> (section 5)."
Compare(e1, e2)
Comment:

1148	"however, the highest <e1>accuracy</e1> for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or <e2>deep learning models</e2>, creating a tension between accuracy and interpretability."
Part-of(e1, e2)
Comment:

1149	"however, the highest <e1>accuracy</e1> for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between <e2>accuracy</e2> and interpretability."
sameAs(e1, e2)
Comment:

1150	"in response, various <e1>methods</e1> have recently been proposed to help users interpret the predictions of complex <e2>models</e2>, but it is often unclear how these methods are related and when one method is preferable over another."
Compare(e1, e2)
Comment:

1151	"in response, various <e1>methods</e1> have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these <e2>methods</e2> are related and when one method is preferable over another."
sameAs(e1, e2)
Comment:

1152	"in response, various <e1>methods</e1> have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one <e2>method</e2> is preferable over another."
Compare(e1, e2)
Comment:

1153	"in response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these <e1>methods</e1> are related and when one <e2>method</e2> is preferable over another."
Compare(e1, e2)
Comment:

1154	"the new class unifies six existing <e1>methods</e1>, notable because several recent <e2>methods</e2> in the class lack the proposed desirable properties."
sameAs(e1, e2)
Comment:

1155	"here, we focus on exponential family <e1>embeddings</e1> (efe) (rudolph et al, 2016) , a method that encompasses many existing methods for <e2>embeddings</e2> and opens the door to bringing expressive probabilistic modeling (bishop, 2006; murphy, 2012) to the problem of learning distributed representations."
sameAs(e1, e2)
Comment:

1156	"here, we focus on exponential family embeddings (efe) (rudolph et al, 2016) , a <e1>method</e1> that encompasses many existing <e2>methods</e2> for embeddings and opens the door to bringing expressive probabilistic modeling (bishop, 2006; murphy, 2012) to the problem of learning distributed representations."
Compare(e1, e2)
Comment:

1157	"with this in mind, we build a generalization of the efe model (rudolph et al, 2016) <e1>that</e1> relaxes the assumption <e2>that</e2> the target depends on all elements in the context."
sameAs(e1, e2)
Comment:

1158	"one difficulty here is that the varied <e1>sizes</e1> of the contexts require varied input and output <e2>sizes</e2> for the shared structure."
sameAs(e1, e2)
Comment:

1159	"first, we develop a <e1>model</e1> that allows conditioning on a subset of the elements in the context in an efe <e2>model</e2>."
sameAs(e1, e2)
Comment:

1160	"tian et al [24] use n <e1>matching</e1> pairs in batch for generating n 2 − n negative samples and require that the distance to the ground truth <e2>matchings</e2> is minimum in each row and column."
sameAs(e1, e2)
Comment:

1161	"no other constraint on the <e1>distance</e1> or <e2>distance</e2> ratio is enforced."
sameAs(e1, e2)
Comment:

1162	"we show <e1>that</e1> the proposed loss, <e2>that</e2> maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures."
sameAs(e1, e2)
Comment:

1163	"simonyan and zisserman [20] proposed a simple filter plus <e1>pooling</e1> scheme learned with convex optimization to replace the hand-crafted filters and <e2>poolings</e2> in sift."
sameAs(e1, e2)
Comment:

1164	"in practice, the size of deep neural networks has been being tremendously increased, from lenet-5 with less than 1m <e1>parameters</e1> [2] to vgg-16 with 133m <e2>parameters</e2> [3] ."
sameAs(e1, e2)
Comment:

1165	"however, finding an optimal <e1>pruning</e1> solution is np-hard because the search space for <e2>pruning</e2> is exponential in terms of parameter size."
sameAs(e1, e2)
Comment:

1166	"though previous work along this research line has shown some promising <e1>results</e1>, most existing <e2>methods</e2> either fail to significantly compress a well-trained deep network or require a heavy retraining process for the pruned deep network to re-boost its prediction performance."
Compare(e1, e2)
Comment:

1167	"though previous work along this research line has shown some promising results, most existing methods either fail to significantly compress a well-trained deep <e1>network</e1> or require a heavy retraining process for the pruned deep <e2>network</e2> to re-boost its prediction performance."
sameAs(e1, e2)
Comment:

1168	"in this paper, we propose a new layer-wise <e1>pruning</e1> method for deep neural networks, aiming to achieve the following three goals: 1) for each layer, parameters can be highly compressed after <e2>pruning</e2>, while the reconstructed error is small."
sameAs(e1, e2)
Comment:

1169	"the <e1>parameters</e1> of each individual layer only, i.e., the hessian matrix is only over <e2>parameters</e2> for a specific layer, the computation becomes tractable."
sameAs(e1, e2)
Comment:

1170	"with such a layer-wise <e1>pruning</e1> framework using second-order derivatives for trimming parameters for each layer, we empirically show that after significantly <e2>pruning</e2> parameters, there is only a little drop of prediction performance compared with that before pruning."
sameAs(e1, e2)
Comment:

1171	"with such a layer-wise <e1>pruning</e1> framework using second-order derivatives for trimming parameters for each layer, we empirically show that after significantly pruning parameters, there is only a little drop of prediction performance compared with that before <e2>pruning</e2>."
sameAs(e1, e2)
Comment:

1172	"with such a layer-wise pruning framework using second-order derivatives for trimming <e1>parameters</e1> for each layer, we empirically show that after significantly pruning <e2>parameters</e2>, there is only a little drop of prediction performance compared with that before pruning."
sameAs(e1, e2)
Comment:

1173	"with such a layer-wise pruning framework using second-order derivatives for trimming parameters for each layer, we empirically show <e1>that</e1> after significantly pruning parameters, there is only a little drop of prediction performance compared with <e2>that</e2> before pruning."
sameAs(e1, e2)
Comment:

1174	"with such a layer-wise pruning framework using second-order derivatives for trimming parameters for each layer, we empirically show that after significantly <e1>pruning</e1> parameters, there is only a little drop of prediction performance compared with that before <e2>pruning</e2>."
sameAs(e1, e2)
Comment:

1175	"1) we propose a new layer-wise <e1>pruning</e1> method for deep neural networks, which is able to significantly trim networks and preserve the prediction performance of networks after <e2>pruning</e2> with a theoretical guarantee."
sameAs(e1, e2)
Comment:

1176	"1) we propose a new layer-wise pruning method for deep neural networks, which is able to significantly trim <e1>networks</e1> and preserve the prediction performance of <e2>networks</e2> after pruning with a theoretical guarantee."
sameAs(e1, e2)
Comment:

1177	"in our proposed method, <e1>parameters</e1> of each individual layer are pruned independently based on second order derivatives of a layer-wise error function with respect to the corresponding <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

1178	"recommender <e1>systems</e1> have proven to be effective tools for this <e2>task</e2>, receiving increasingly more attention."
Used-for(e1, e2)
Comment:

1179	"one common <e1>approach</e1> to building accurate recommender <e2>models</e2> is collaborative filtering (cf)."
Used-for(e1, e2)
Comment:

1180	"however, while there has been some recent progress in applying <e1>deep learning</e1> to cf [7, 22, 6, 23] , little investigation has been done on using <e2>deep learning</e2> to address the cold start problem."
sameAs(e1, e2)
Comment:

1181	"in <e1>this</e1> work we propose a model to address <e2>this</e2> gap."
sameAs(e1, e2)
Comment:

1182	"hence, instead of adding additional objective terms to <e1>model</e1> content, we modify the learning procedure to explicitly condition the <e2>model</e2> for the missing input."
sameAs(e1, e2)
Comment:

1183	"the key idea behind our approach is that by applying dropout [18] to <e1>input</e1> mini-batches, we can train dnns to generalize to missing <e2>input</e2>."
sameAs(e1, e2)
Comment:

1184	"by selecting an appropriate amount of dropout we show <e1>that</e1> it is possible to learn a dnn-based latent model <e2>that</e2> performs comparably to state-of-the-art on warm start while significantly outperforming it on cold start."
sameAs(e1, e2)
Comment:

1185	"unlike existing approaches <e1>that</e1> incorporate additional content-based objective terms, we instead focus on the optimization and show <e2>that</e2> neural network models can be explicitly trained for cold start through dropout."
sameAs(e1, e2)
Comment:

1186	"our <e1>model</e1> can be applied on top of any existing latent <e2>model</e2> effectively providing cold start capabilities, and full power of deep architectures."
sameAs(e1, e2)
Comment:

1187	"at lower levels neurons have smaller receptive <e1>fields</e1> whereas at higher levels they have larger receptive <e2>fields</e2>."
sameAs(e1, e2)
Comment:

1188	"we introduce a <e1>hierarchical</e1> neural network, named as pointnet++, to process a set of points sampled in a metric space in a <e2>hierarchical</e2> fashion."
sameAs(e1, e2)
Comment:

1189	"similar to cnns, we extract <e1>local features</e1> capturing fine geometric structures from small neighborhoods; such <e2>local features</e2> are further grouped into larger units and processed to produce higher level features."
sameAs(e1, e2)
Comment:

1190	"deciding the appropriate <e1>scale</e1> of local neighborhood balls, however, is a more challenging yet intriguing problem, due to the entanglement of feature <e2>scale</e2> and non-uniformity of input point set."
sameAs(e1, e2)
Comment:

1191	"assisted with random <e1>input</e1> dropout during training, the network learns to adaptively weight patterns detected at different scales and combine multi-scale features according to the <e2>input</e2> data."
sameAs(e1, e2)
Comment:

1192	"assisted with random input dropout during training, the network learns to adaptively weight patterns detected at different <e1>scales</e1> and combine multi-<e2>scale</e2> features according to the input data."
sameAs(e1, e2)
Comment:

1193	"abstract existing <e1>markov chain monte carlo</e1> (<e2>mcmc</e2>) methods are either based on generalpurpose and domain-agnostic schemes, which can lead to slow convergence, or problem-specific proposals hand-crafted by an expert."
sameAs(e1, e2)
Comment:

1194	"notable examples include black-box variational inference and variational autoencoders [1] [2] [3] , which enabled variational methods to benefit from the expressive power of deep neural networks, and adversarial <e1>training</e1> [4, 5] , which allowed the <e2>training</e2> of new families of implicit generative models with efficient ancestral sampling."
sameAs(e1, e2)
Comment:

1195	"we then propose adversarial <e1>training</e1> as an effective, likelihoodfree method for <e2>training</e2> a markov chain to match a target distribution."
sameAs(e1, e2)
Comment:

1196	"we then use the <e1>approach</e1> to train a markov chain to sample efficiently from a model prescribed by an analytic expression (e.g., a bayesian posterior distribution), the classic use case for mcmc <e2>techniques</e2>."
Compare(e1, e2)
Comment:

1197	"empirical <e1>results</e1> demonstrate that a-nice-mc combines the strong guarantees of mcmc with the expressiveness of deep neural networks, and is able to significantly outperform competing <e2>methods</e2> such as hamiltonian monte carlo."
Compare(e1, e2)
Comment:

1198	"the latter approximates a complex distribution using a small number of typical states, obtained by sampling ancestrally from a proposal distribution or iteratively using a suitable markov chain (<e1>markov chain monte carlo</e1>, or <e2>mcmc</e2>)."
sameAs(e1, e2)
Comment:

1199	"while this property could stabilize the parameter growth by reducing the <e1>gradients</e1> for larger weights, it could also have an adverse effect in terms of optimization since there can be an infinite number of networks, with the same forward pass but different scaling, which may converge to different local optima owing to different <e2>gradients</e2>."
sameAs(e1, e2)
Comment:

1200	"mapping scale invariant weight vectors to two well-known matrix manifolds yields the same <e1>metric</e1> tensor, leading to a natural choice of the manifold and <e2>metric</e2>."
sameAs(e1, e2)
Comment:

1201	"next, we present two optimization algorithmscorresponding to the <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) with momentum and adam [9] algorithms."
sameAs(e1, e2)
Comment:

1202	"experiments show that the resulting <e1>algorithm</e1> consistently outperforms the original bn <e2>algorithm</e2> on various types of network architectures and datasets."
sameAs(e1, e2)
Comment:

1203	"it speeds up <e1>training</e1> deep neural networks by normalizing the distribution of the input to each neuron in the network by the mean and standard deviation of the input computed over a mini-batch of <e2>training</e2> data, potentially reducing internal covariate shift [1] , the change in the distributions of internal nodes of a deep network during the training."
sameAs(e1, e2)
Comment:

1204	"it speeds up <e1>training</e1> deep neural networks by normalizing the distribution of the input to each neuron in the network by the mean and standard deviation of the input computed over a mini-batch of training data, potentially reducing internal covariate shift [1] , the change in the distributions of internal nodes of a deep network during the <e2>training</e2>."
sameAs(e1, e2)
Comment:

1205	"it speeds up training deep neural networks by normalizing the distribution of the <e1>input</e1> to each neuron in the network by the mean and standard deviation of the <e2>input</e2> computed over a mini-batch of training data, potentially reducing internal covariate shift [1] , the change in the distributions of internal nodes of a deep network during the training."
sameAs(e1, e2)
Comment:

1206	"it speeds up training deep neural networks by normalizing the distribution of the input to each neuron in the <e1>network</e1> by the mean and standard deviation of the input computed over a mini-batch of training data, potentially reducing internal covariate shift [1] , the change in the distributions of internal nodes of a deep <e2>network</e2> during the training."
sameAs(e1, e2)
Comment:

1207	"it speeds up training deep neural networks by normalizing the distribution of the input to each neuron in the network by the mean and standard deviation of the input computed over a mini-batch of <e1>training</e1> data, potentially reducing internal covariate shift [1] , the change in the distributions of internal nodes of a deep network during the <e2>training</e2>."
sameAs(e1, e2)
Comment:

1208	"the authors of <e1>bn</e1> demonstrated that applying <e2>bn</e2> to a layer makes its forward pass invariant to linear scaling of its weight parameters [1] ."
sameAs(e1, e2)
Comment:

1209	"abstract a triangle generative adversarial network (∆-gan) is developed for semisupervised cross-domain joint distribution matching, where the training data consists of samples from each <e1>domain</e1>, and supervision of <e2>domain</e2> correspondence is provided by only a few paired samples."
sameAs(e1, e2)
Comment:

1210	"this is an important <e1>task</e1>, since mapping <e2>data</e2> samples from one domain to another has a wide range of applications."
Conjunction(e1, e2)
Comment:

1211	"for instance, <e1>matching</e1> the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation [4] , while image-to-image translation [5] is another challenging problem that requires <e2>matching</e2> the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1212	"for instance, matching the joint distribution of <e1>image</e1>-text pairs allows simultaneous <e2>image</e2> captioning and textconditional image generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1213	"for instance, matching the joint distribution of <e1>image</e1>-text pairs allows simultaneous image captioning and textconditional <e2>image</e2> generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1214	"for instance, matching the joint distribution of <e1>image</e1>-text pairs allows simultaneous image captioning and textconditional image generation [4] , while <e2>image</e2>-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1215	"for instance, matching the joint distribution of <e1>image</e1>-text pairs allows simultaneous image captioning and textconditional image generation [4] , while image-to-<e2>image</e2> translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1216	"for instance, matching the joint distribution of <e1>image</e1>-text pairs allows simultaneous image captioning and textconditional image generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of <e2>image</e2>-image pairs."
sameAs(e1, e2)
Comment:

1217	"for instance, matching the joint distribution of image-text pairs allows simultaneous <e1>image</e1> captioning and textconditional <e2>image</e2> generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1218	"for instance, matching the joint distribution of image-text pairs allows simultaneous <e1>image</e1> captioning and textconditional image generation [4] , while <e2>image</e2>-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1219	"for instance, matching the joint distribution of image-text pairs allows simultaneous <e1>image</e1> captioning and textconditional image generation [4] , while image-to-<e2>image</e2> translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1220	"for instance, matching the joint distribution of image-text pairs allows simultaneous <e1>image</e1> captioning and textconditional image generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of <e2>image</e2>-image pairs."
sameAs(e1, e2)
Comment:

1221	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional <e1>image</e1> generation [4] , while <e2>image</e2>-to-image translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1222	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional <e1>image</e1> generation [4] , while image-to-<e2>image</e2> translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1223	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional <e1>image</e1> generation [4] , while image-to-image translation [5] is another challenging problem that requires matching the joint distribution of <e2>image</e2>-image pairs."
sameAs(e1, e2)
Comment:

1224	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation [4] , while <e1>image</e1>-to-<e2>image</e2> translation [5] is another challenging problem that requires matching the joint distribution of image-image pairs."
sameAs(e1, e2)
Comment:

1225	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation [4] , while <e1>image</e1>-to-image translation [5] is another challenging problem that requires matching the joint distribution of <e2>image</e2>-image pairs."
sameAs(e1, e2)
Comment:

1226	"for instance, matching the joint distribution of image-text pairs allows simultaneous image captioning and textconditional image generation [4] , while image-to-<e1>image</e1> translation [5] is another challenging problem that requires matching the joint distribution of <e2>image</e2>-image pairs."
sameAs(e1, e2)
Comment:

1227	"if paired <e1>data</e1> are available, a simple approach to achieve this is to train a conditional gan model [4, 6] , from which a joint distribution is readily manifested and can be matched to the empirical joint distribution provided by the paired <e2>data</e2>."
sameAs(e1, e2)
Comment:

1228	"∆-gan consists of four neural networks, <e1>two</e1> generators and <e2>two</e2> discriminators."
sameAs(e1, e2)
Comment:

1229	"this motivates the proposed triangle generative adversarial network (∆-<e1>gan</e1>), a <e2>gan</e2> framework that allows semi-supervised joint distribution matching, where the supervision of domain correspondence is provided by a few paired samples."
sameAs(e1, e2)
Comment:

1230	"∆-gan consists of <e1>two</e1> generators and <e2>two</e2> discriminators."
sameAs(e1, e2)
Comment:

1231	"∆-<e1>gan</e1> bears close resemblance to triple <e2>gan</e2> [12] , a recently proposed method that can also be utilized for semi-supervised joint distribution mapping."
sameAs(e1, e2)
Comment:

1232	"∆-<e1>gan</e1> bears close resemblance to triple gan [12] , a recently proposed <e2>method</e2> that can also be utilized for semi-supervised joint distribution mapping."
Part-of(e1, e2)
Comment:

1233	"∆-gan bears close resemblance to triple <e1>gan</e1> [12] , a recently proposed <e2>method</e2> that can also be utilized for semi-supervised joint distribution mapping."
Part-of(e1, e2)
Comment:

1234	"first, ∆-<e1>gan</e1> uses two discriminators in total, which implicitly defines a ternary discriminative function, instead of a binary discriminator as used in triple <e2>gan</e2>."
sameAs(e1, e2)
Comment:

1235	"second, ∆-<e1>gan</e1> can be considered as a combination of conditional gan and ali, while triple <e2>gan</e2> consists of two conditional gans."
sameAs(e1, e2)
Comment:

1236	"second, ∆-gan can be considered as a combination of <e1>conditional gan</e1> and ali, while triple gan consists of two <e2>conditional gans</e2>."
sameAs(e1, e2)
Comment:

1237	"third, the distributions characterized by the two generators in both ∆-<e1>gan</e1> and triple <e2>gan</e2> concentrate to the data distribution in theory."
sameAs(e1, e2)
Comment:

1238	"the generators are designed to learn the <e1>two</e1>-way conditional distributions between the <e2>two</e2> domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and two kinds of fake data pairs."
sameAs(e1, e2)
Comment:

1239	"the generators are designed to learn the <e1>two</e1>-way conditional distributions between the two domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and <e2>two</e2> kinds of fake data pairs."
sameAs(e1, e2)
Comment:

1240	"the generators are designed to learn the two-way conditional distributions between the <e1>two</e1> domains, while the discriminators implicitly define a ternary discriminative function, which is trained to distinguish real data pairs and <e2>two</e2> kinds of fake data pairs."
sameAs(e1, e2)
Comment:

1241	"however, when the discriminator is optimal, the objective of ∆-<e1>gan</e1> becomes the jensen-shannon divergence (jsd) among three distributions, which is symmetric; the objective of triple <e2>gan</e2> consists of a jsd term plus a kullback-leibler (kl) divergence term."
sameAs(e1, e2)
Comment:

1242	"however, when the discriminator is optimal, the objective of ∆-gan becomes the jensen-shannon <e1>divergence</e1> (jsd) among three distributions, which is symmetric; the objective of triple gan consists of a jsd term plus a kullback-leibler (kl) <e2>divergence</e2> term."
sameAs(e1, e2)
Comment:

1243	"on the other hand, ∆-<e1>gan</e1> is a fully adversarial approach that does not require that the conditional densities can be computed; ∆-<e2>gan</e2> only require that the conditional densities can be sampled from in a way that allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1244	"on the other hand, ∆-gan is a fully adversarial approach <e1>that</e1> does not require <e2>that</e2> the conditional densities can be computed; ∆-gan only require that the conditional densities can be sampled from in a way that allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1245	"on the other hand, ∆-gan is a fully adversarial approach <e1>that</e1> does not require that the conditional densities can be computed; ∆-gan only require <e2>that</e2> the conditional densities can be sampled from in a way that allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1246	"on the other hand, ∆-gan is a fully adversarial approach <e1>that</e1> does not require that the conditional densities can be computed; ∆-gan only require that the conditional densities can be sampled from in a way <e2>that</e2> allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1247	"on the other hand, ∆-gan is a fully adversarial approach that does not require <e1>that</e1> the conditional densities can be computed; ∆-gan only require <e2>that</e2> the conditional densities can be sampled from in a way that allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1248	"on the other hand, ∆-gan is a fully adversarial approach that does not require <e1>that</e1> the conditional densities can be computed; ∆-gan only require that the conditional densities can be sampled from in a way <e2>that</e2> allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1249	"on the other hand, ∆-gan is a fully adversarial approach that does not require that the conditional densities can be computed; ∆-gan only require <e1>that</e1> the conditional densities can be sampled from in a way <e2>that</e2> allows gradient backpropagation."
sameAs(e1, e2)
Comment:

1250	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, <e2>image</e2>-image and image-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1251	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, image-image and <e2>image</e2>-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1252	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, image-image and image-attribute pairs, and use them for semi-supervised <e2>classification</e2>, image-to-image translation and attribute-based image editing, respectively."
Used-for(e1, e2)
Comment:

1253	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, image-image and image-attribute pairs, and use them for semi-supervised classification, <e2>image</e2>-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1254	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, image-image and image-attribute pairs, and use them for semi-supervised classification, image-to-<e2>image</e2> translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1255	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: <e1>image</e1>-label, image-image and image-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based <e2>image</e2> editing, respectively."
sameAs(e1, e2)
Comment:

1256	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, <e1>image</e1>-image and <e2>image</e2>-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1257	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, <e1>image</e1>-image and image-attribute pairs, and use them for semi-supervised <e2>classification</e2>, image-to-image translation and attribute-based image editing, respectively."
Used-for(e1, e2)
Comment:

1258	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, <e1>image</e1>-image and image-attribute pairs, and use them for semi-supervised classification, <e2>image</e2>-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1259	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, <e1>image</e1>-image and image-attribute pairs, and use them for semi-supervised classification, image-to-<e2>image</e2> translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1260	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, <e1>image</e1>-image and image-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based <e2>image</e2> editing, respectively."
sameAs(e1, e2)
Comment:

1261	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and <e1>image</e1>-attribute pairs, and use them for semi-supervised <e2>classification</e2>, image-to-image translation and attribute-based image editing, respectively."
Used-for(e1, e2)
Comment:

1262	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and <e1>image</e1>-attribute pairs, and use them for semi-supervised classification, <e2>image</e2>-to-image translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1263	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and <e1>image</e1>-attribute pairs, and use them for semi-supervised classification, image-to-<e2>image</e2> translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1264	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and <e1>image</e1>-attribute pairs, and use them for semi-supervised classification, image-to-image translation and attribute-based <e2>image</e2> editing, respectively."
sameAs(e1, e2)
Comment:

1265	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and image-attribute pairs, and use them for semi-supervised classification, <e1>image</e1>-to-<e2>image</e2> translation and attribute-based image editing, respectively."
sameAs(e1, e2)
Comment:

1266	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and image-attribute pairs, and use them for semi-supervised classification, <e1>image</e1>-to-image translation and attribute-based <e2>image</e2> editing, respectively."
sameAs(e1, e2)
Comment:

1267	"in experiments, in order to demonstrate the versatility of the proposed model, we consider three domain pairs: image-label, image-image and image-attribute pairs, and use them for semi-supervised classification, image-to-<e1>image</e1> translation and attribute-based <e2>image</e2> editing, respectively."
sameAs(e1, e2)
Comment:

1268	"in experiments, three different kinds of domain pairs are considered, <e1>image</e1>-label, <e2>image</e2>-image and image-attribute pairs."
sameAs(e1, e2)
Comment:

1269	"in experiments, three different kinds of domain pairs are considered, <e1>image</e1>-label, image-image and <e2>image</e2>-attribute pairs."
sameAs(e1, e2)
Comment:

1270	"in experiments, three different kinds of domain pairs are considered, image-label, <e1>image</e1>-image and <e2>image</e2>-attribute pairs."
sameAs(e1, e2)
Comment:

1271	"experiments on semi-supervised image classification, <e1>image</e1>-to-<e2>image</e2> translation and attribute-based image generation demonstrate the superiority of the proposed approach."
sameAs(e1, e2)
Comment:

1272	"experiments on semi-supervised image classification, <e1>image</e1>-to-image translation and attribute-based <e2>image</e2> generation demonstrate the superiority of the proposed approach."
sameAs(e1, e2)
Comment:

1273	"experiments on semi-supervised image classification, image-to-<e1>image</e1> translation and attribute-based <e2>image</e2> generation demonstrate the superiority of the proposed approach."
sameAs(e1, e2)
Comment:

1274	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) [1] have emerged as a powerful framework for learning generative models of arbitrarily complex data distributions."
sameAs(e1, e2)
Comment:

1275	"despite the impressive results, these neural networks are still trained using simple variants of <e1>stochastic gradient descent</e1> (<e2>sgd</e2>)."
sameAs(e1, e2)
Comment:

1276	"natural policy <e1>gradient</e1> [10] uses the technique of natural gradient descent [1] to perform <e2>gradient</e2> updates."
sameAs(e1, e2)
Comment:

1277	"natural gradient methods follow the steepest descent direction <e1>that</e1> uses the fisher metric as the underlying metric, a metric <e2>that</e2> is based not on the choice of coordinates but rather on the manifold (i.e., the surface)."
sameAs(e1, e2)
Comment:

1278	"natural gradient methods follow the steepest descent direction that uses the fisher <e1>metric</e1> as the underlying <e2>metric</e2>, a metric that is based not on the choice of coordinates but rather on the manifold (i.e., the surface)."
sameAs(e1, e2)
Comment:

1279	"natural gradient methods follow the steepest descent direction that uses the fisher <e1>metric</e1> as the underlying metric, a <e2>metric</e2> that is based not on the choice of coordinates but rather on the manifold (i.e., the surface)."
sameAs(e1, e2)
Comment:

1280	"natural gradient methods follow the steepest descent direction that uses the fisher metric as the underlying <e1>metric</e1>, a <e2>metric</e2> that is based not on the choice of coordinates but rather on the manifold (i.e., the surface)."
sameAs(e1, e2)
Comment:

1281	"we extend the framework of natural policy gradient and propose to optimize both the actor and the critic using kronecker-factored approximate curvature (k-fac) with trust <e1>region</e1>; hence we call our method actor critic using kronecker-factored trust <e2>region</e2> (acktr)."
sameAs(e1, e2)
Comment:

1282	"in this paper, we introduce the actor-critic using kronecker-factored trust <e1>region</e1> (acktr; pronounced "actor") method, a scalable trust-<e2>region</e2> optimization algorithm for actor-critic methods."
sameAs(e1, e2)
Comment:

1283	"in this paper, we introduce the actor-critic using kronecker-factored trust region (acktr; pronounced "actor") <e1>method</e1>, a scalable trust-region optimization algorithm for actor-critic <e2>methods</e2>."
Compare(e1, e2)
Comment:

1284	"the proposed <e1>algorithm</e1> uses a kronecker-factored approximation to natural <e2>policy</e2> gradient that allows the covariance matrix of the gradient to be inverted efficiently."
Used-for(e1, e2)
Comment:

1285	"the proposed algorithm uses a kronecker-factored approximation to natural policy <e1>gradient</e1> that allows the covariance matrix of the <e2>gradient</e2> to be inverted efficiently."
sameAs(e1, e2)
Comment:

1286	"empirically, we show that acktr substantially improves both sample efficiency and the final performance of the agent in the atari environments [4] and the mujoco [26] <e1>tasks</e1> compared to the state-of-the-art on-policy actor-critic <e2>method</e2> a2c [17] and the famous trust region optimizer trpo [21] ."
Evaluate-for(e1, e2)
Comment:

1287	"to the best of our knowledge, this is the first scalable trust region natural gradient <e1>method</e1> for actor-critic <e2>methods</e2>."
Compare(e1, e2)
Comment:

1288	"it is also the <e1>method</e1> that learns non-trivial <e2>tasks</e2> in continuous control as well as discrete control policies directly from raw pixel inputs."
Used-for(e1, e2)
Comment:

1289	"we tested our approach across <e1>discrete</e1> domains in atari games as well as <e2>continuous</e2> domains in the mujoco environment."
Conjunction(e1, e2)
Comment:

1290	"we tested our approach across discrete <e1>domains</e1> in atari games as well as continuous <e2>domains</e2> in the mujoco environment."
sameAs(e1, e2)
Comment:

1291	"with the proposed <e1>methods</e1>, we are able to achieve higher rewards and a 2-to 3-fold improvement in sample efficiency on average, compared to previous state-of-the-art on-policy actor-critic <e2>methods</e2>."
sameAs(e1, e2)
Comment:

1292	"a value function plays an important role in rl, because it predicts the expected return, conditioned on a <e1>state</e1> or <e2>state</e2>-action pair."
sameAs(e1, e2)
Comment:

1293	"a value function plays an important role in rl, because it predicts the expected return, conditioned on a <e1>state</e1> or state-<e2>action</e2> pair."
Conjunction(e1, e2)
Comment:

1294	"a value function plays an important role in rl, because it predicts the expected return, conditioned on a state or <e1>state</e1>-<e2>action</e2> pair."
Conjunction(e1, e2)
Comment:

1295	"by modelling the current estimate of the optimal value <e1>function</e1> with a deep neural network, dqn carries out a strong generalisation on the value <e2>function</e2>, and hence on the policy."
sameAs(e1, e2)
Comment:

1296	"specifically, we propose to replace the optimal value <e1>function</e1> as target for training with an alternative value <e2>function</e2> that is easier to learn, but still yields a reasonable-but generally not optimal-policy, when acting greedily with respect to it."
sameAs(e1, e2)
Comment:

1297	"intrinsic motivation (stout et al, 2005; schmidhuber, 2010) uses this observation to improve learning in sparse-<e1>reward</e1> domains, by adding a domain-specific intrinsic <e2>reward</e2> signal to the reward coming from the environment."
sameAs(e1, e2)
Comment:

1298	"intrinsic motivation (stout et al, 2005; schmidhuber, 2010) uses this observation to improve learning in sparse-<e1>reward</e1> domains, by adding a domain-specific intrinsic reward signal to the <e2>reward</e2> coming from the environment."
sameAs(e1, e2)
Comment:

1299	"intrinsic motivation (stout et al, 2005; schmidhuber, 2010) uses this observation to improve learning in sparse-reward domains, by adding a domain-specific intrinsic <e1>reward</e1> signal to the <e2>reward</e2> coming from the environment."
sameAs(e1, e2)
Comment:

1300	"our main strategy for constructing an easy-to-learn value function is to decompose the <e1>reward function</e1> of the environment into n different <e2>reward functions</e2>."
sameAs(e1, e2)
Comment:

1301	"each agent gives its <e1>action</e1>-values of the current state to an aggregator, which combines them into a single value for each <e2>action</e2>."
sameAs(e1, e2)
Comment:

1302	"each agent gives its action-values of the current <e1>state</e1> to an aggregator, which combines them into a single value for each <e2>action</e2>."
Conjunction(e1, e2)
Comment:

1303	"while this approach works well in many <e1>domains</e1>, in <e2>domains</e2> where the optimal value function cannot easily be reduced to a low-dimensional representation, learning can be very slow and unstable."
sameAs(e1, e2)
Comment:

1304	"hra takes as input a decomposed <e1>reward function</e1> and learns a separate value <e2>function</e2> for each component reward function."
isA(e1, e2)
Comment:

1305	"hra takes as input a decomposed <e1>reward function</e1> and learns a separate value function for each component <e2>reward function</e2>."
sameAs(e1, e2)
Comment:

1306	"abstract <e1>generative adversarial networks</e1> (gan) are an effective <e2>method</e2> for training generative models of complex data such as natural images."
Part-of(e1, e2)
Comment:

1307	"abstract generative adversarial networks (<e1>gan</e1>) are an effective <e2>method</e2> for training generative models of complex data such as natural images."
Part-of(e1, e2)
Comment:

1308	"for each picture in the corpus, <e1>we</e1> can thus use the classifier's confidence to compute a weight which <e2>we</e2> use for that picture in the next iteration, to be performed on the re-weighted dataset."
sameAs(e1, e2)
Comment:

1309	"1 <e1>algorithm</e1> 1 adagan, a meta-<e2>algorithm</e2> to construct a "strong" mixture of t individual generative models (f.ex."
sameAs(e1, e2)
Comment:

1310	"train vanilla <e1>gan</e1> g1 = <e2>gan</e2>(sn , w1) with a uniform weight w1 = (1/n, ."
sameAs(e1, e2)
Comment:

1311	", t do #choose the overall weight of the next mixture <e1>component</e1> βt = choosemixtureweight(t) #update the weight of each training example wt = updatetrainingweights(gt−1, sn , βt) #train t-th "weak" <e2>component</e2> generator g before discussing how to build the mixture, let us consider the question of building a single generative model."
sameAs(e1, e2)
Comment:

1312	"one popular <e1>approach</e1> are generative adversarial networks (gan) [2] , where the generator is trained adversarially against a <e2>classifier</e2>, which tries to differentiate the true from the generated data."
Used-for(e1, e2)
Comment:

1313	"while the original gan algorithm often produces realistically looking <e1>data</e1>, several issues were reported in the literature, among which the missing modes problem, where the generator converges to only one or a few modes of the <e2>data</e2> distribution, thus not providing enough variability in the generated data."
sameAs(e1, e2)
Comment:

1314	"while the original gan algorithm often produces realistically looking <e1>data</e1>, several issues were reported in the literature, among which the missing modes problem, where the generator converges to only one or a few modes of the data distribution, thus not providing enough variability in the generated <e2>data</e2>."
sameAs(e1, e2)
Comment:

1315	"while the original gan algorithm often produces realistically looking data, several issues were reported in the literature, among which the missing modes problem, where the generator converges to only one or a few modes of the <e1>data</e1> distribution, thus not providing enough variability in the generated <e2>data</e2>."
sameAs(e1, e2)
Comment:

1316	"we call it adagan, for adaptive <e1>gan</e1>, but we could actually use any other generator: a gaussian mixture model, a vae [1] , a wgan [3] , or even an unrolled [4] or mode-regularized <e2>gan</e2> [5] , which were both already specifically developed to tackle the missing mode problem."
sameAs(e1, e2)
Comment:

1317	"a major downside of these approaches is that the resulting mixture is a product of components and sampling from such a <e1>model</e1> is nontrivial (at least when applied to gans where the <e2>model</e2> density is not expressed analytically) and requires techniques such as annealed importance sampling [9] for the normalization."
sameAs(e1, e2)
Comment:

1318	"we prove analytically that such an incremental procedure leads to <e1>convergence</e1> to the true distribution in a finite number of steps if each step is optimal, and <e2>convergence</e2> at an exponential rate otherwise."
sameAs(e1, e2)
Comment:

1319	"they also proved that under certain conditions, finite <e1>mixtures</e1> can approximate arbitrary <e2>mixtures</e2> at a rate 1/k where k is the number of components in the mixture when the weight of each newly added component is 1/k."
sameAs(e1, e2)
Comment:

1320	"they also proved that under certain conditions, finite mixtures can approximate arbitrary mixtures at a rate 1/k where k is the number of <e1>components</e1> in the mixture when the weight of each newly added <e2>component</e2> is 1/k."
isA(e1, e2)
Comment:

1321	"these <e1>results</e1> are specific to the kullback divergence but are consistent with our more general <e2>results</e2>."
sameAs(e1, e2)
Comment:

1322	"in section 2.4 <e1>we</e1> show that if optimization at each step is perfect, the process converges to the true data distribution at exponential rate (or even in a finite number of steps, for which <e2>we</e2> provide a necessary and sufficient condition)."
sameAs(e1, e2)
Comment:

1323	"finally, <e1>we</e1> report initial empirical results in section 4, where <e2>we</e2> compare adagan with several benchmarks, including original gan and uniform mixture of multiple independently trained gans."
sameAs(e1, e2)
Comment:

1324	"finally, we report initial empirical results in section 4, where we compare adagan with several benchmarks, including original <e1>gan</e1> and uniform mixture of multiple independently trained <e2>gans</e2>."
sameAs(e1, e2)
Comment:

1325	"introduction imagine we have a large corpus, containing unlabeled pictures of animals, and our <e1>task</e1> is to build a generative probabilistic model of the <e2>data</e2>."
Conjunction(e1, e2)
Comment:

1326	"as mentioned above, it is common to approach these problems in a two-step fashion: first to fit a predictive <e1>model</e1> to observed data by minimizing some criterion such as negative log-likelihood, and then to use this <e2>model</e2> to compute or approximate the necessary expected costs in the stochastic programming setting."
sameAs(e1, e2)
Comment:

1327	"while this procedure can work well in many instances, it ignores the fact <e1>that</e1> the true cost of the system (the optimization objective evaluated on actual instantiations in the real world) may benefit from a model <e2>that</e2> actually attains worse overall likelihood, but makes more accurate predictions over certain manifolds of the underlying space."
sameAs(e1, e2)
Comment:

1328	"we propose to train a probabilistic model not (solely) for predictive <e1>accuracy</e1>, but so that-when it is later used within the loop of a stochastic programming procedure-it produces <e2>solutions</e2> that minimize the ultimate task-based loss."
Evaluate-for(e1, e2)
Comment:

1329	"we propose to train a probabilistic model not (solely) for predictive accuracy, but so <e1>that</e1>-when it is later used within the loop of a stochastic programming procedure-it produces solutions <e2>that</e2> minimize the ultimate task-based loss."
sameAs(e1, e2)
Comment:

1330	"this formulation may seem somewhat counterintuitive, given that a "perfect" predictive <e1>model</e1> would of course also be the optimal <e2>model</e2> to use within a stochastic programming framework."
sameAs(e1, e2)
Comment:

1331	"however, the reality <e1>that</e1> all models do make errors illustrates <e2>that</e2> we should indeed look to a final task-based objective to determine the proper error tradeoffs within a machine learning setting."
sameAs(e1, e2)
Comment:

1332	"this paper proposes one way to evaluate <e1>task</e1>-based tradeoffs in a fully automated fashion, by computing derivatives through the solution to the stochastic programming problem in a manner that can improve the underlying <e2>model</e2>."
Evaluate-for(e1, e2)
Comment:

1333	"this paper proposes one way to evaluate task-based tradeoffs in a fully automated fashion, by computing derivatives through the <e1>solution</e1> to the stochastic programming <e2>problem</e2> in a manner that can improve the underlying model."
Used-for(e1, e2)
Comment:

1334	"however, the <e1>criteria</e1> by which we train these algorithms often differ from the ultimate <e2>criteria</e2> on which we evaluate them."
sameAs(e1, e2)
Comment:

1335	"however, the criteria by which <e1>we</e1> train these algorithms often differ from the ultimate criteria on which <e2>we</e2> evaluate them."
sameAs(e1, e2)
Comment:

1336	"we then describe our <e1>approach</e1> within the formal context of stochastic programming, and give a generic <e2>method</e2> for propagating task loss through these problems in a manner that can update the models."
Compare(e1, e2)
Comment:

1337	"we then describe our <e1>approach</e1> within the formal context of stochastic programming, and give a generic method for propagating task loss through these problems in a manner that can update the <e2>models</e2>."
Used-for(e1, e2)
Comment:

1338	"we report on three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling <e1>task</e1>, and a real-world energy storage arbitrage <e2>task</e2>."
sameAs(e1, e2)
Comment:

1339	"we show that the proposed <e1>approach</e1> outperforms traditional modeling and purely black-box policy optimization <e2>approaches</e2>."
Compare(e1, e2)
Comment:

1340	"we present three experimental evaluations of the proposed approach: a classical inventory stock problem, a real-world electrical grid scheduling <e1>task</e1>, and a real-world energy storage arbitrage <e2>task</e2>."
sameAs(e1, e2)
Comment:

1341	"we show that the proposed <e1>approach</e1> can outperform both traditional modeling and purely black-box policy optimization <e2>approaches</e2> in these applications."
Compare(e1, e2)
Comment:

1342	"introduction while prediction <e1>algorithms</e1> commonly operate within some larger process, the criteria by which we train these <e2>algorithms</e2> often differ from the ultimate criteria on which we evaluate them: the performance of the full "closed-loop" system on the ultimate task at hand."
sameAs(e1, e2)
Comment:

1343	"introduction while prediction algorithms commonly operate within some larger process, the <e1>criteria</e1> by which we train these algorithms often differ from the ultimate <e2>criteria</e2> on which we evaluate them: the performance of the full "closed-loop" system on the ultimate task at hand."
sameAs(e1, e2)
Comment:

1344	"introduction while prediction algorithms commonly operate within some larger process, the criteria by which <e1>we</e1> train these algorithms often differ from the ultimate criteria on which <e2>we</e2> evaluate them: the performance of the full "closed-loop" system on the ultimate task at hand."
sameAs(e1, e2)
Comment:

1345	"for instance, instead of merely classifying images in a standalone setting, one may want to use <e1>these</e1> classifications within planning and control <e2>tasks</e2> such as autonomous driving."
Used-for(e1, e2)
Comment:

1346	"while a typical image classification algorithm might optimize accuracy or log likelihood, in a driving task we may ultimately care more about the difference between classifying a pedestrian as a <e1>tree</e1> vs. classifying a garbage can as a <e2>tree</e2>."
sameAs(e1, e2)
Comment:

1347	"similarly, when <e1>we</e1> use a probabilistic prediction algorithm to generate forecasts of upcoming electricity demand, <e2>we</e2> then want to use these forecasts to minimize the costs of a scheduling procedure that allocates generation for a power grid."
sameAs(e1, e2)
Comment:

1348	"abstract despite the growing prominence of <e1>generative adversarial networks</e1> (<e2>gans</e2>), optimization in gans is still a poorly understood topic."
sameAs(e1, e2)
Comment:

1349	"abstract despite the growing prominence of <e1>generative adversarial networks</e1> (gans), optimization in <e2>gans</e2> is still a poorly understood topic."
sameAs(e1, e2)
Comment:

1350	"abstract despite the growing prominence of generative adversarial networks (<e1>gans</e1>), optimization in <e2>gans</e2> is still a poorly understood topic."
sameAs(e1, e2)
Comment:

1351	"in this paper, we consider the "gradient descent" formulation of gan <e1>optimization</e1>, the setting where both the generator and the discriminator are updated simultaneously via simple (stochastic) gradient updates; that is, there are no inner and outer <e2>optimization</e2> loops, and neither the generator nor the discriminator are assumed to be optimized to convergence."
sameAs(e1, e2)
Comment:

1352	"despite the fact <e1>that</e1>, as we show, this does not correspond to a convex-concave optimization problem (even for simple linear generator and discriminator representations), we show <e2>that</e2>: under suitable conditions on the representational powers of the discriminator and the generator, the resulting gan dynamical system is locally exponentially stable."
sameAs(e1, e2)
Comment:

1353	"despite the fact that, as <e1>we</e1> show, this does not correspond to a convex-concave optimization problem (even for simple linear generator and discriminator representations), <e2>we</e2> show that: under suitable conditions on the representational powers of the discriminator and the generator, the resulting gan dynamical system is locally exponentially stable."
sameAs(e1, e2)
Comment:

1354	"the additional penalty is highly related to (but also notably different from) recent proposals for practical <e1>gan</e1> optimization, such as the unrolled <e2>gan</e2> [metz et al, 2017] and the improved wasserstein gan training [gulrajani et al, 2017] ."
sameAs(e1, e2)
Comment:

1355	"the additional penalty is highly related to (but also notably different from) recent proposals for practical <e1>gan</e1> optimization, such as the unrolled gan [metz et al, 2017] and the improved wasserstein <e2>gan</e2> training [gulrajani et al, 2017] ."
sameAs(e1, e2)
Comment:

1356	"the additional penalty is highly related to (but also notably different from) recent proposals for practical gan optimization, such as the unrolled <e1>gan</e1> [metz et al, 2017] and the improved wasserstein <e2>gan</e2> training [gulrajani et al, 2017] ."
sameAs(e1, e2)
Comment:

1357	"in this paper, <e1>we</e1> analyze the "gradient descent" form of gan optimization, i.e., the natural setting where <e2>we</e2> simultaneously take small gradient steps in both generator and discriminator parameters."
sameAs(e1, e2)
Comment:

1358	"in contrast, in this paper <e1>we</e1> delve into an equally important question of whether the updates are stable even when the generator is in fact very close to the true distribution (and <e2>we</e2> answer in the affirmative)."
sameAs(e1, e2)
Comment:

1359	"we show that even though <e1>gan</e1> optimization does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this optimization procedure are still locally asymptotically stable for the traditional <e2>gan</e2> formulation."
sameAs(e1, e2)
Comment:

1360	"we show that even though gan <e1>optimization</e1> does not correspond to a convex-concave game (even for simple parameterizations), under proper conditions, equilibrium points of this <e2>optimization</e2> procedure are still locally asymptotically stable for the traditional gan formulation."
sameAs(e1, e2)
Comment:

1361	"specifically, our regularization term (motivated by stability analysis) captures a degree of "foresight" of the generator in the optimization <e1>procedure</e1>, similar to the unrolled gans <e2>procedure</e2> [metz et al, 2017] ."
sameAs(e1, e2)
Comment:

1362	"the first of these two presents a stabilizing regularizer that is based on a <e1>gradient</e1> norm, where the <e2>gradient</e2> is calculated with respect to the datapoints."
sameAs(e1, e2)
Comment:

1363	"our <e1>approach</e1> has some strong similarities with that of the second work noted above; however, the authors there do not establish or disprove stability, and instead note the presence of zero eigenvalues (which we will treat in some depth) as a motivation for their alternative optimization <e2>method</e2>."
Compare(e1, e2)
Comment:

1364	"our approach has some strong <e1>similarities</e1> with that of the second work noted above; however, the authors there do not establish or disprove stability, and instead note the presence of zero eigenvalues (which we will treat in some depth) as a motivation for their alternative optimization <e2>method</e2>."
Used-for(e1, e2)
Comment:

1365	"the technical tools we use to analyze the <e1>gan</e1> optimization dynamics in this paper come from the fields of stochastic approximation algorithms and the analysis of nonlinear differential equations -notably the "ode <e2>method</e2>" for analyzing convergence properties of dynamical systems [borkar and meyn, 2000, kushner and yin, 2003] ."
Part-of(e1, e2)
Comment:

1366	"though such analysis is typically used to show global asymptotic convergence of the stochastic <e1>approximation</e1> algorithm to an equilibrium point (assuming the related ode also is globally asymptotically stable), it can also be used to analyze the local asymptotic stability properties of the stochastic <e2>approximation</e2> algorithm around equilibrium points."
sameAs(e1, e2)
Comment:

1367	"though such analysis is typically used to show global asymptotic convergence of the stochastic approximation <e1>algorithm</e1> to an equilibrium point (assuming the related ode also is globally asymptotically stable), it can also be used to analyze the local asymptotic stability properties of the stochastic approximation <e2>algorithm</e2> around equilibrium points."
sameAs(e1, e2)
Comment:

1368	"2 <e1>this</e1> is the technique we follow throughout <e2>this</e2> entire work, though for brevity we will focus entirely on the analysis of the continuous time ordinary differential equation, and appeal to these standard results to imply similar properties regarding the discrete updates."
sameAs(e1, e2)
Comment:

1369	"2 this is the technique <e1>we</e1> follow throughout this entire work, though for brevity <e2>we</e2> will focus entirely on the analysis of the continuous time ordinary differential equation, and appeal to these standard results to imply similar properties regarding the discrete updates."
sameAs(e1, e2)
Comment:

1370	"motivated by this stability analysis, we propose an additional regularization term for gradient descent <e1>gan</e1> updates, which is able to guarantee local stability for both the wgan and the traditional <e2>gan</e2>, and also shows practical promise in speeding up convergence and addressing mode collapse."
sameAs(e1, e2)
Comment:

1371	"introduction since their introduction a few years ago, <e1>generative adversarial networks</e1> (<e2>gans</e2>) [goodfellow et al, 2014] have gained prominence as one of the most widely used methods for training deep generative models."
sameAs(e1, e2)
Comment:

1372	"thus, an important contribution of <e1>this</e1> paper is a proof of <e2>this</e2> seemingly simple fact: under some conditions, the jacobian of the dynamical system given by the gan update is a hurwitz matrix at an equilibrium (or, if there are zero-eigenvalues, if they correspond to a subspace of equilibria, the system is still asymptotically stable)."
sameAs(e1, e2)
Comment:

1373	"thus, an important contribution of this paper is a proof of this seemingly simple fact: under some conditions, the jacobian of the dynamical <e1>system</e1> given by the gan update is a hurwitz matrix at an equilibrium (or, if there are zero-eigenvalues, if they correspond to a subspace of equilibria, the <e2>system</e2> is still asymptotically stable)."
sameAs(e1, e2)
Comment:

1374	"while this is a trivial property to show for <e1>convex</e1>-concave games, the fact that the gan is not <e2>convex</e2>-concave leads to a substantially more challenging analysis."
sameAs(e1, e2)
Comment:

1375	"at the core of the gan methodology is the idea of jointly training two networks: a generator network, meant to produce samples from some distribution (that ideally will mimic examples from the <e1>data</e1> distribution), and a discriminator network, which attempts to differentiate between samples from the <e2>data</e2> distribution and the ones produced by the generator."
sameAs(e1, e2)
Comment:

1376	"crowding is a well known effect in human vision [9, 10] , in which <e1>objects</e1> (targets) that can be recognized in isolation can no longer be recognized in the presence of nearby <e2>objects</e2> (flankers), even though there is no occlusion."
sameAs(e1, e2)
Comment:

1377	"experimental data suggests that crowding depends on the <e1>distance</e1> of the target and the flankers [11] , eccentricity (the <e2>distance</e2> of the target to the fixation point), as well as the similarity between the target and the flankers [12, 13] or the configuration of the flankers around the target object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1378	"experimental data suggests that crowding depends on the distance of the <e1>target</e1> and the flankers [11] , eccentricity (the distance of the <e2>target</e2> to the fixation point), as well as the similarity between the target and the flankers [12, 13] or the configuration of the flankers around the target object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1379	"experimental data suggests that crowding depends on the distance of the <e1>target</e1> and the flankers [11] , eccentricity (the distance of the target to the fixation point), as well as the similarity between the <e2>target</e2> and the flankers [12, 13] or the configuration of the flankers around the target object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1380	"experimental data suggests that crowding depends on the distance of the <e1>target</e1> and the flankers [11] , eccentricity (the distance of the target to the fixation point), as well as the similarity between the target and the flankers [12, 13] or the configuration of the flankers around the <e2>target</e2> object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1381	"experimental data suggests that crowding depends on the distance of the target and the flankers [11] , eccentricity (the distance of the <e1>target</e1> to the fixation point), as well as the similarity between the <e2>target</e2> and the flankers [12, 13] or the configuration of the flankers around the target object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1382	"experimental data suggests that crowding depends on the distance of the target and the flankers [11] , eccentricity (the distance of the <e1>target</e1> to the fixation point), as well as the similarity between the target and the flankers [12, 13] or the configuration of the flankers around the <e2>target</e2> object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1383	"experimental data suggests that crowding depends on the distance of the target and the flankers [11] , eccentricity (the distance of the target to the fixation point), as well as the similarity between the <e1>target</e1> and the flankers [12, 13] or the configuration of the flankers around the <e2>target</e2> object [11, 14, 15] ."
sameAs(e1, e2)
Comment:

1384	"examples of the generated images using <e1>mnist</e1> [20] , notmnist [21] , and omniglot [22] <e2>datasets</e2> are depicted in fig 1, in which even mnist digits are the target objects."
isA(e1, e2)
Comment:

1385	"examples of the generated images using <e1>mnist</e1> [20] , notmnist [21] , and omniglot [22] datasets are depicted in fig 1, in which even <e2>mnist</e2> digits are the target objects."
sameAs(e1, e2)
Comment:

1386	"our experiments reveal the dependence of crowding on image factors, such as flanker configuration, <e1>target</e1>-flanker similarity, and <e2>target</e2> eccentricity."
sameAs(e1, e2)
Comment:

1387	"we analyze both deep convolutional neural networks (dcnns) as well as an extension of dcnns <e1>that</e1> are multi-scale and <e2>that</e2> change the receptive field size of the convolution filters with their position in the image."
sameAs(e1, e2)
Comment:

1388	"in addition, we show that <e1>training</e1> the models with cluttered images does not make models robust to clutter and flankers configurations not seen in <e2>training</e2>."
sameAs(e1, e2)
Comment:

1389	"in addition, we show that training the <e1>models</e1> with cluttered images does not make <e2>models</e2> robust to clutter and flankers configurations not seen in training."
sameAs(e1, e2)
Comment:

1390	"our results reveal that the eccentricity-dependent model, trained on <e1>target</e1> objects in isolation, can recognize such <e2>targets</e2> in the presence of flankers, if the targets are near the center of the image, whereas dcnns cannot."
sameAs(e1, e2)
Comment:

1391	"our results reveal that the eccentricity-dependent model, trained on <e1>target</e1> objects in isolation, can recognize such targets in the presence of flankers, if the <e2>targets</e2> are near the center of the image, whereas dcnns cannot."
sameAs(e1, e2)
Comment:

1392	"our results reveal that the eccentricity-dependent model, trained on target objects in isolation, can recognize such <e1>targets</e1> in the presence of flankers, if the <e2>targets</e2> are near the center of the image, whereas dcnns cannot."
sameAs(e1, e2)
Comment:

1393	"also, for all tested <e1>networks</e1>, when trained on targets in isolation, we find that recognition accuracy of the <e2>networks</e2> decreases the closer the flankers are to the target and the more flankers there are."
sameAs(e1, e2)
Comment:

1394	"also, for all tested networks, when trained on <e1>targets</e1> in isolation, we find that recognition accuracy of the networks decreases the closer the flankers are to the <e2>target</e2> and the more flankers there are."
sameAs(e1, e2)
Comment:

1395	"we find <e1>that</e1> visual similarity between the target and flankers also plays a role and <e2>that</e2> pooling in early layers of the network leads to more crowding."
sameAs(e1, e2)
Comment:

1396	"additionally, we show that incorporating flankers into the images of the <e1>training</e1> set for learning the dnns does not lead to robustness against configurations not seen at <e2>training</e2>."
sameAs(e1, e2)
Comment:

1397	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) are powerful generative models, but suffer from training instability."
sameAs(e1, e2)
Comment:

1398	"wgan requires <e1>that</e1> the discriminator (called the critic in <e2>that</e2> work) must lie within the space of 1-lipschitz functions, which the authors enforce through weight clipping."
sameAs(e1, e2)
Comment:

1399	"the recently proposed wasserstein <e1>gan</e1> (wgan) makes progress toward stable training of <e2>gans</e2>, but sometimes can still generate only poor samples or fail to converge."
sameAs(e1, e2)
Comment:

1400	"† introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) [9] are a powerful class of generative models that cast generative modeling as a game between two networks: a generator network produces synthetic data given some noise source and a discriminator network discriminates between the generator's output and true data."
sameAs(e1, e2)
Comment:

1401	"for the purposes of solving a single <e1>task</e1>, the primary characteristic required is suitability for that <e2>task</e2>."
sameAs(e1, e2)
Comment:

1402	"a representation that has some factorisable <e1>structure</e1>, and consistent <e2>semantics</e2> associated to different parts, is more likely to generalise to a new task."
Conjunction(e1, e2)
Comment:

1403	"probabilistic generative models provide a general framework for learning representations: a model is specified by a joint probability distribution both over the <e1>data</e1> and over latent random variables, and a representation can be found by considering the posterior on latent variables given specific <e2>data</e2>."
sameAs(e1, e2)
Comment:

1404	"the learned representation -that is, inferred values of <e1>latent variables</e1> -depends then not just on the data, but also on the generative model in its choice of <e2>latent variables</e2> and the relationships between the latent variables and the data."
sameAs(e1, e2)
Comment:

1405	"the learned representation -that is, inferred values of <e1>latent variables</e1> -depends then not just on the data, but also on the generative model in its choice of latent variables and the relationships between the <e2>latent variables</e2> and the data."
sameAs(e1, e2)
Comment:

1406	"the learned representation -that is, inferred values of latent variables -depends then not just on the <e1>data</e1>, but also on the generative model in its choice of latent variables and the relationships between the latent variables and the <e2>data</e2>."
sameAs(e1, e2)
Comment:

1407	"the learned representation -that is, inferred values of latent variables -depends then not just on the data, but also on the generative model in its choice of <e1>latent variables</e1> and the relationships between the <e2>latent variables</e2> and the data."
sameAs(e1, e2)
Comment:

1408	"in an explicitly constructed graphical model, the <e1>structure</e1> and form of the joint distribution ensures that latent variables will have particular <e2>semantics</e2>, yielding a disentangled representation."
Conjunction(e1, e2)
Comment:

1409	"although <e1>they</e1> address learning representations which then enable them to better reconstruct data, the representations themselves do not always exhibit consistent meaning along axes of variation: <e2>they</e2> produce entangled representations."
sameAs(e1, e2)
Comment:

1410	"although they address learning <e1>representations</e1> which then enable them to better reconstruct data, the <e2>representations</e2> themselves do not always exhibit consistent meaning along axes of variation: they produce entangled representations."
sameAs(e1, e2)
Comment:

1411	"although they address learning <e1>representations</e1> which then enable them to better reconstruct data, the representations themselves do not always exhibit consistent meaning along axes of variation: they produce entangled <e2>representations</e2>."
sameAs(e1, e2)
Comment:

1412	"although they address learning representations which then enable them to better reconstruct data, the <e1>representations</e1> themselves do not always exhibit consistent meaning along axes of variation: they produce entangled <e2>representations</e2>."
sameAs(e1, e2)
Comment:

1413	"while such approaches have considerable merit, particularly when faced with the absence of any side information about <e1>data</e1>, there are often situations when aspects of variation in <e2>data</e2> can be, or are desired to be characterised."
sameAs(e1, e2)
Comment:

1414	"here we propose a more general class of partiallyspecified <e1>graphical models</e1>: probabilistic <e2>graphical models</e2> in which the modeller only needs specify the exact relationship for some subset of the random variables in the model."
sameAs(e1, e2)
Comment:

1415	"a subclass of partially-specified models <e1>that</e1> is particularly common is <e2>that</e2> where we can obtain supervision data for some subset of the variables."
sameAs(e1, e2)
Comment:

1416	"for example, when considering images of people's <e1>faces</e1>, we might wish to capture the person's identity in one context, and the lighting conditions on the <e2>faces</e2> in another, facial features in another, or combinations of these in yet other contexts."
sameAs(e1, e2)
Comment:

1417	"for example, when considering images of people's faces, we might wish to capture the person's identity in <e1>one</e1> context, and the lighting conditions on the faces in another, facial features in another, or combinations of these in yet <e2>other</e2> contexts."
Conjunction(e1, e2)
Comment:

1418	"in this paper we introduce a recipe for learning and inference in partially-specified models, a flexible framework that learns disentangled representations of <e1>data</e1> by using graphical model structures to encode constraints to interpret the <e2>data</e2>."
sameAs(e1, e2)
Comment:

1419	"we present this <e1>framework</e1> in the context of variational autoencoders (vaes), developing a generalised formulation of semi-supervised learning with dgms that enables our <e2>framework</e2> to automatically employ the correct factorisation of the objective for any given choice of model and set of latents taken to be observed."
sameAs(e1, e2)
Comment:

1420	"we introduce a variational objective which is applicable to a more general class of models, allowing us to consider graphical-<e1>model</e1> structures with arbitrary dependencies between latents, continuous-domain latents, and <e2>those</e2> with dynamically changing dependencies."
Compare(e1, e2)
Comment:

1421	"this allows us to train partially-specified models that make relatively strong assumptions about a subset of interpretable <e1>variables</e1> and rely on the flexibility of neural networks to learn representations for the remaining <e2>variables</e2>."
sameAs(e1, e2)
Comment:

1422	"aligning the positions to steps in computation time, they generate a sequence of hidden <e1>states</e1> h t , as a function of the previous hidden <e2>state</e2> h t−1 and the input for position t. this inherently sequential nature precludes parallelization within training examples, which becomes critical at longer sequence lengths, as memory constraints limit batching across examples."
sameAs(e1, e2)
Comment:

1423	"attention mechanisms have become an integral part of compelling sequence <e1>modeling</e1> and transduction models in various tasks, allowing <e2>modeling</e2> of dependencies without regard to their distance in the input or output sequences [2, 16] ."
sameAs(e1, e2)
Comment:

1424	"attention mechanisms have become an integral part of compelling sequence modeling and transduction <e1>models</e1> in various <e2>tasks</e2>, allowing modeling of dependencies without regard to their distance in the input or output sequences [2, 16] ."
Used-for(e1, e2)
Comment:

1425	"our model achieves 28.4 <e1>bleu</e1> on the wmt 2014 englishto-german translation task, improving over the existing best results, including ensembles, by over 2 <e2>bleu</e2>."
sameAs(e1, e2)
Comment:

1426	"on the wmt 2014 english-to-french translation <e1>task</e1>, our <e2>model</e2> establishes a new single-model state-of-the-art bleu score of 41.0 after training for 3.5 days on eight gpus, a small fraction of the training costs of the best models from the literature."
Evaluate-for(e1, e2)
Comment:

1427	"on the wmt 2014 english-to-french translation <e1>task</e1>, our model establishes a new single-<e2>model</e2> state-of-the-art bleu score of 41.0 after training for 3.5 days on eight gpus, a small fraction of the training costs of the best models from the literature."
Evaluate-for(e1, e2)
Comment:

1428	"on the wmt 2014 english-to-french translation task, our <e1>model</e1> establishes a new single-<e2>model</e2> state-of-the-art bleu score of 41.0 after training for 3.5 days on eight gpus, a small fraction of the training costs of the best models from the literature."
sameAs(e1, e2)
Comment:

1429	"on the wmt 2014 english-to-french translation task, our model establishes a new single-model state-of-the-art bleu score of 41.0 after <e1>training</e1> for 3.5 days on eight gpus, a small fraction of the <e2>training</e2> costs of the best models from the literature."
sameAs(e1, e2)
Comment:

1430	"introduction <e1>recurrent neural networks</e1>, long short-term memory [12] and gated recurrent [7] <e2>neural networks</e2> in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as language modeling and machine translation [29, 2, 5] ."
isA(e1, e2)
Comment:

1431	"introduction <e1>recurrent neural networks</e1>, long short-term memory [12] and gated recurrent [7] neural networks in particular, have been firmly established as state of the art approaches in sequence <e2>modeling</e2> and transduction problems such as language modeling and machine translation [29, 2, 5] ."
Used-for(e1, e2)
Comment:

1432	"introduction recurrent neural networks, long short-term memory [12] and gated recurrent [7] <e1>neural networks</e1> in particular, have been firmly established as state of the art approaches in sequence modeling and transduction problems such as <e2>language modeling</e2> and machine translation [29, 2, 5] ."
Used-for(e1, e2)
Comment:

1433	"abstract this paper proposes a novel deep reinforcement learning (rl) architecture, called value prediction network (vpn), which integrates <e1>model-free</e1> and <e2>model-based</e2> rl methods into a single neural network."
Conjunction(e1, e2)
Comment:

1434	"the vpn combines model-based rl (i.e., learning the dynamics of an abstract <e1>state</e1> space sufficient for computing future rewards and values) and model-free rl (i.e., mapping the learned abstract <e2>states</e2> to rewards and values) in a unified framework."
sameAs(e1, e2)
Comment:

1435	"the vpn combines model-based rl (i.e., learning the dynamics of an abstract state space sufficient for computing future <e1>rewards</e1> and values) and model-free rl (i.e., mapping the learned abstract states to <e2>rewards</e2> and values) in a unified framework."
sameAs(e1, e2)
Comment:

1436	"in order to train a vpn, we propose a combination of temporal-difference <e1>search</e1> [28] (td <e2>search</e2>) and n-step q-learning [20] ."
sameAs(e1, e2)
Comment:

1437	"furthermore, we show <e1>that</e1> our vpn outperforms dqn on several atari games [2] even with short-lookahead planning, which suggests <e2>that</e2> our approach can be potentially useful for learning better abstract-state representations and reducing sample-complexity."
sameAs(e1, e2)
Comment:

1438	"our experimental results show that vpn has several advantages over both <e1>model-free</e1> and <e2>model-based</e2> baselines in a stochastic environment where careful planning is required but building an accurate observation-prediction model is difficult."
Conjunction(e1, e2)
Comment:

1439	"introduction <e1>model-based</e1> reinforcement learning (rl) <e2>approaches</e2> attempt to learn a model that predicts future observations conditioned on actions and can thus be used to simulate the real environment and do multi-step lookaheads for planning."
isA(e1, e2)
Comment:

1440	"in fact, raw <e1>observations</e1> may contain information unnecessary for planning, such as dynamically changing backgrounds in visual <e2>observations</e2> that are irrelevant to their value/utility."
sameAs(e1, e2)
Comment:

1441	"henceforth <e1>we</e1> will denote observed potential confounders 2 by x, and unobserved confounders by z. in most real-world observational studies <e2>we</e2> cannot hope to measure all possible confounders."
sameAs(e1, e2)
Comment:

1442	"for example, <e1>we</e1> cannot measure the socio-economic status of patients directly, but <e2>we</e2> might be able to get a proxy for it by knowing their zip code and job type."
sameAs(e1, e2)
Comment:

1443	"in particular, it is assumed <e1>that</e1> the hidden confounder is either categorical with known number of categories, or <e2>that</e2> the model is linear-gaussian."
sameAs(e1, e2)
Comment:

1444	"perhaps ses as confounder is comprised of two dimensions, the economic <e1>one</e1> (related to wealth and income) and the social <e2>one</e2> (related to education and cultural capital)."
sameAs(e1, e2)
Comment:

1445	"these include <e1>methods</e1> with provable guarantees, typically based on the <e2>method</e2>-of-moments (e.g."
Compare(e1, e2)
Comment:

1446	"however, <e1>they</e1> have the significant advantage that <e2>they</e2> make substantially weaker assumptions about the data generating process and the structure of the hidden confounders."
sameAs(e1, e2)
Comment:

1447	"finally, <e1>we</e1> note that our method does not currently deal with the related problem of selection bias, and <e2>we</e2> leave this to future work."
sameAs(e1, e2)
Comment:

1448	"the general idea is <e1>that</e1> in many cases one should first attempt to infer the joint distribution p(x, z) between the proxy and the hidden confounders, and then use <e2>that</e2> knowledge to adjust for the hidden confounders [55, 41, 32, 37, 12] ."
sameAs(e1, e2)
Comment:

1449	"for the example in figure 1 , cai and kuroki [9] , greenland and lash [18] , pearl [41] show that if z and x are categorical, with x having at least as many categories as z, and with the <e1>matrix</e1> p(x, z) being full-rank, one could identify the causal effect of t on y using a simple <e2>matrix</e2> inversion formula, an approach called "effect restoration"."
sameAs(e1, e2)
Comment:

1450	"abstract semi-supervised learning methods based on <e1>generative adversarial networks</e1> (<e2>gans</e2>) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time."
sameAs(e1, e2)
Comment:

1451	"feature matching (fm) <e1>gans</e1> [16] apply <e2>gans</e2> to semi-supervised learning on kclass classification."
sameAs(e1, e2)
Comment:

1452	"for example, [16] observed that mini-batch discrimination generates better images than <e1>feature matching</e1>, but <e2>feature matching</e2> obtains a much better semi-supervised learning performance."
sameAs(e1, e2)
Comment:

1453	"empirically, our <e1>approach</e1> substantially improves over vanilla feature matching gans, and obtains new state-of-the-art results on mnist, svhn, and cifar-10 when all <e2>methods</e2> are compared under the same discriminator architecture."
Compare(e1, e2)
Comment:

1454	"empirically, our approach substantially improves over vanilla feature matching gans, and obtains new state-of-the-art <e1>results</e1> on mnist, svhn, and cifar-10 when all <e2>methods</e2> are compared under the same discriminator architecture."
Compare(e1, e2)
Comment:

1455	"our <e1>results</e1> on mnist and svhn also represent state-of-the-art amongst all single-model <e2>results</e2>."
sameAs(e1, e2)
Comment:

1456	"traditional <e1>graph</e1>-based methods [2, 26] were extended to deep neural networks [22, 23, 8] , which involves applying convolutional neural networks [10] and feature learning techniques to <e2>graphs</e2> so that the underlying manifold structure can be exploited."
sameAs(e1, e2)
Comment:

1457	"recently, <e1>generative adversarial networks</e1> (<e2>gans</e2>) [6] were demonstrated to be able to generate visually realistic images."
sameAs(e1, e2)
Comment:

1458	"developing computational <e1>models</e1> for language-vision <e2>tasks</e2> is challenging, especially because of the open question underlying all these tasks: how to fuse/integrate visual and textual representations?"
Used-for(e1, e2)
Comment:

1459	"developing computational <e1>models</e1> for language-vision tasks is challenging, especially because of the open question underlying all these <e2>tasks</e2>: how to fuse/integrate visual and textual representations?"
Used-for(e1, e2)
Comment:

1460	"developing computational models for <e1>language</e1>-<e2>vision</e2> tasks is challenging, especially because of the open question underlying all these tasks: how to fuse/integrate visual and textual representations?"
Conjunction(e1, e2)
Comment:

1461	"developing computational models for language-vision <e1>tasks</e1> is challenging, especially because of the open question underlying all these <e2>tasks</e2>: how to fuse/integrate visual and textual representations?"
sameAs(e1, e2)
Comment:

1462	"developing computational models for language-vision tasks is challenging, especially because of the open question underlying all <e1>these</e1> <e2>tasks</e2>: how to fuse/integrate visual and textual representations?"
Used-for(e1, e2)
Comment:

1463	"to what extent should <e1>we</e1> process visual and linguistic input separately, and at which stage should <e2>we</e2> fuse them?"
sameAs(e1, e2)
Comment:

1464	"in this paper, we restrict our attention to the domain of visual question answering which is a natural testbed for fusing <e1>language</e1> and <e2>vision</e2>."
Conjunction(e1, e2)
Comment:

1465	"the activations from a resnet network [12] ), and obtain a language embedding using a <e1>recurrent neural network</e1> (<e2>rnn</e2>) over word-embeddings."
sameAs(e1, e2)
Comment:

1466	"this view dominates the current literature in computational <e1>models</e1> for language-vision <e2>tasks</e2>, where visual and linguistic inputs are mostly processed independently before being fused into a single representation."
Used-for(e1, e2)
Comment:

1467	"this view dominates the current literature in computational models for <e1>language</e1>-<e2>vision</e2> tasks, where visual and linguistic inputs are mostly processed independently before being fused into a single representation."
Conjunction(e1, e2)
Comment:

1468	"first, using a pretrained convnet as feature extractor prevents <e1>overfitting</e1>; despite a large training set of a few hundred thousand samples, backpropagating the error of the downstream task into the weights of all layers often leads to <e2>overfitting</e2>."
sameAs(e1, e2)
Comment:

1469	"in parallel, the neuroscience community has been exploring to what extent the processing of <e1>language</e1> and <e2>vision</e2> is coupled [8] ."
Conjunction(e1, e2)
Comment:

1470	"the language cue that people hear ahead of an <e1>image</e1> activates visual predictions and speed up the <e2>image</e2> recognition process."
sameAs(e1, e2)
Comment:

1471	"in <e1>this</e1> paper, we deviate from <e2>this</e2> classic pipeline and propose to modulate the entire visual processing by a linguistic input."
sameAs(e1, e2)
Comment:

1472	"[6] , but stress <e1>that</e1> our approach is a general fusing mechanism <e2>that</e2> can be applied to other multi-modal tasks."
sameAs(e1, e2)
Comment:

1473	"to summarize, our contributions are three fold: • <e1>we</e1> propose conditional batch normalization to modulate the entire visual processing by language from the early processing stages, • <e2>we</e2> condition the batch normalization parameters of a pretrained resnet on linguistic input, leading to a new network architecture: modern, • we demonstrate improvements on state-of-the-art models for two vqa tasks and show the contribution of this modulation on the early stages."
sameAs(e1, e2)
Comment:

1474	"to summarize, our contributions are three fold: • <e1>we</e1> propose conditional batch normalization to modulate the entire visual processing by language from the early processing stages, • we condition the batch normalization parameters of a pretrained resnet on linguistic input, leading to a new network architecture: modern, • <e2>we</e2> demonstrate improvements on state-of-the-art models for two vqa tasks and show the contribution of this modulation on the early stages."
sameAs(e1, e2)
Comment:

1475	"to summarize, our contributions are three fold: • we propose conditional batch normalization to modulate the entire visual <e1>processing</e1> by language from the early <e2>processing</e2> stages, • we condition the batch normalization parameters of a pretrained resnet on linguistic input, leading to a new network architecture: modern, • we demonstrate improvements on state-of-the-art models for two vqa tasks and show the contribution of this modulation on the early stages."
sameAs(e1, e2)
Comment:

1476	"to summarize, our contributions are three fold: • we propose conditional batch normalization to modulate the entire visual processing by language from the early processing stages, • <e1>we</e1> condition the batch normalization parameters of a pretrained resnet on linguistic input, leading to a new network architecture: modern, • <e2>we</e2> demonstrate improvements on state-of-the-art models for two vqa tasks and show the contribution of this modulation on the early stages."
sameAs(e1, e2)
Comment:

1477	"to summarize, our contributions are three fold: • we propose conditional batch normalization to modulate the entire visual processing by language from the early processing stages, • we condition the batch normalization parameters of a pretrained resnet on linguistic input, leading to a new network architecture: modern, • we demonstrate improvements on state-of-the-art <e1>models</e1> for two vqa <e2>tasks</e2> and show the contribution of this modulation on the early stages."
Used-for(e1, e2)
Comment:

1478	"we apply cbn to a pre-trained residual network (<e1>resnet</e1>), leading to the modulated <e2>resnet</e2> (modern) architecture, and show that this significantly improves strong baselines on two visual question answering tasks."
sameAs(e1, e2)
Comment:

1479	"introduction human beings combine the processing of <e1>language</e1> and <e2>vision</e2> with apparent ease."
Conjunction(e1, e2)
Comment:

1480	"for example, <e1>we</e1> can use natural language to describe perceived objects and <e2>we</e2> are able to imagine a visual scene from a given textual description."
sameAs(e1, e2)
Comment:

1481	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) excel at creating realistic images with complex models for which maximum likelihood is infeasible."
sameAs(e1, e2)
Comment:

1482	"gan learning is a game between the generator, which constructs <e1>synthetic data</e1> from random variables, and the discriminator, which separates <e2>synthetic data</e2> from real world data."
sameAs(e1, e2)
Comment:

1483	"the generator's goal is to construct <e1>data</e1> in such a way that the discriminator cannot tell them apart from real world <e2>data</e2>."
sameAs(e1, e2)
Comment:

1484	"e <e1>convergence</e1> to a neighborhood is the best we can hereas by using diminishing step sizes, <e2>convergence</e2> bability one to the optimal points is made possible."
sameAs(e1, e2)
Comment:

1485	"instead, we have shown <e1>that</e1> provided <e2>that</e2> the biased asymptotically uniformly bounded, the iterates return ntraction region" infinitely often."
sameAs(e1, e2)
Comment:

1486	"5 <e1>that</e1> the smaller the upper-bound is, the smaller the tion region" a η becomes, indicating <e2>that</e2> the iterates "closer" to the optimal points."
sameAs(e1, e2)
Comment:

1487	"for the evaluation of the performance of gans at image generation, we introduce the 'fréchet <e1>inception</e1> distance" (fid) which captures the similarity of generated images to real ones better than the <e2>inception</e2> score."
sameAs(e1, e2)
Comment:

1488	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) [16] have achieved outstanding results in generating realistic images [42, 31, 25, 1, 4] and producing text [21] ."
sameAs(e1, e2)
Comment:

1489	"so far, <e1>these</e1> methods have mostly focused on the single object case whereas, for real world <e2>tasks</e2> such as reasoning and physical interaction, it is often necessary to identify and manipulate multiple entities and their relationships."
Used-for(e1, e2)
Comment:

1490	"one <e1>solution</e1> to this <e2>problem</e2> involves learning a separate representation for each object."
Used-for(e1, e2)
Comment:

1491	"in <e1>this</e1> work, we tackle <e2>this</e2> problem of learning how to group and efficiently represent individual entities, in an unsupervised manner, based solely on the statistical structure of the data."
sameAs(e1, e2)
Comment:

1492	"a first step towards solving <e1>these</e1> <e2>tasks</e2> is the automated discovery of distributed symbol-like representations."
Used-for(e1, e2)
Comment:

1493	"based on the expectation maximization framework <e1>we</e1> then derive a differentiable clustering method, which <e2>we</e2> call neural expectation maximization (n-em)."
sameAs(e1, e2)
Comment:

1494	"in <e1>this</e1> paper, we explicitly formalize <e2>this</e2> problem as inference in a spatial mixture model where each component is parametrized by a neural network."
sameAs(e1, e2)
Comment:

1495	"these spatial re-arrangements result in certain dna <e1>regions</e1> becoming accessible or restricted and therefore affecting expressions of genes in the neighborhood <e2>region</e2>."
sameAs(e1, e2)
Comment:

1496	"remc recently released 2,804 genome-wide <e1>datasets</e1>, among which 166 <e2>datasets</e2> are gene expression reads (rna-seq datasets) and the rest are signal reads of various chromatin marks across 100 different "normal" human cells/tissues [18] ."
sameAs(e1, e2)
Comment:

1497	"remc recently released 2,804 genome-wide <e1>datasets</e1>, among which 166 datasets are gene expression reads (rna-seq <e2>datasets</e2>) and the rest are signal reads of various chromatin marks across 100 different "normal" human cells/tissues [18] ."
sameAs(e1, e2)
Comment:

1498	"remc recently released 2,804 genome-wide datasets, among which 166 <e1>datasets</e1> are gene expression reads (rna-seq <e2>datasets</e2>) and the rest are signal reads of various chromatin marks across 100 different "normal" human cells/tissues [18] ."
sameAs(e1, e2)
Comment:

1499	"however, previous machine learning studies on this <e1>task</e1> either failed to <e2>model</e2> spatial dependencies among marks or required additional feature analysis to explain the predictions (section 4)."
Evaluate-for(e1, e2)
Comment:

1500	"on the <e1>sentiment</e1> modification task, the model successfully transfers the <e2>sentiment</e2> while keeps the content for 41.5% of review sentences according to human evaluation, compared to 41.0% achieved by the control-gen model of hu et al (2017) ."
sameAs(e1, e2)
Comment:

1501	"on the sentiment modification <e1>task</e1>, the <e2>model</e2> successfully transfers the sentiment while keeps the content for 41.5% of review sentences according to human evaluation, compared to 41.0% achieved by the control-gen model of hu et al (2017) ."
Evaluate-for(e1, e2)
Comment:

1502	"on the sentiment modification <e1>task</e1>, the model successfully transfers the sentiment while keeps the content for 41.5% of review sentences according to human evaluation, compared to 41.0% achieved by the control-gen <e2>model</e2> of hu et al (2017) ."
Evaluate-for(e1, e2)
Comment:

1503	"on the sentiment modification task, the <e1>model</e1> successfully transfers the sentiment while keeps the content for 41.5% of review sentences according to human evaluation, compared to 41.0% achieved by the control-gen <e2>model</e2> of hu et al (2017) ."
sameAs(e1, e2)
Comment:

1504	"it achieves strong performance on the decipherment and word order recovery <e1>tasks</e1>, reaching bleu score of 57.4 and 26.1 respectively, obtaining 50.2 and 20.9 gap than a comparable <e2>method</e2> without cross-alignment."
Evaluate-for(e1, e2)
Comment:

1505	"the transferred sentences from <e1>one</e1> style should match example sentences from the <e2>other</e2> style as a population."
Conjunction(e1, e2)
Comment:

1506	"we demonstrate the effectiveness of this cross-alignment <e1>method</e1> on three <e2>tasks</e2>: sentiment modification, decipherment of word substitution ciphers, and recovery of word order."
Used-for(e1, e2)
Comment:

1507	"problems such as decipherment or style transfer are all <e1>instances</e1> of this <e2>family</e2> of tasks."
Part-of(e1, e2)
Comment:

1508	"one way to decouple the two would be to find snow-only or polar-bear-only <e1>images</e1> and evaluate the model's performance on these <e2>images</e2> separately."
sameAs(e1, e2)
Comment:

1509	"instead of iteratively obtaining saliency <e1>maps</e1> for each input image separately, we train a model to predict such a <e2>map</e2> for any input image in a single feed-forward pass."
sameAs(e1, e2)
Comment:

1510	"instead of iteratively obtaining saliency maps for each <e1>input</e1> image separately, we train a model to predict such a map for any <e2>input</e2> image in a single feed-forward pass."
sameAs(e1, e2)
Comment:

1511	"instead of iteratively obtaining saliency maps for each input <e1>image</e1> separately, we train a model to predict such a map for any input <e2>image</e2> in a single feed-forward pass."
sameAs(e1, e2)
Comment:

1512	"we show that this <e1>approach</e1> is not only orders-of-magnitude faster than iterative <e2>methods</e2>, but it also produces higher quality saliency masks and achieves better localisation results."
Compare(e1, e2)
Comment:

1513	"other similar <e1>backpropagation</e1>-based approaches have been proposed, for example guided <e2>backpropagation</e2> [12] or excitation backprop [16] ."
sameAs(e1, e2)
Comment:

1514	"while the gradient-based methods are fast enough to be applied in real-time, <e1>they</e1> produce explanations of limited quality [16] and <e2>they</e2> are hard to improve and build upon."
sameAs(e1, e2)
Comment:

1515	"zhou et al [17] proposed an approach <e1>that</e1> iteratively removes patches of the input image (by setting them to the mean colour) such <e2>that</e2> the class score is preserved."
sameAs(e1, e2)
Comment:

1516	"the maps produced by this method are easily interpretable, but unfortunately, the iterative process is very <e1>time</e1> consuming and not acceptable for real-<e2>time</e2> saliency detection."
sameAs(e1, e2)
Comment:

1517	"in another work, cao et al [1] introduced an optimisation method <e1>that</e1> aims to preserve only a fraction of network activations such <e2>that</e2> the class score is maximised."
sameAs(e1, e2)
Comment:

1518	"this approach is <e1>model</e1> agnostic and the produced maps are easily interpretable because the optimisation is done in the image space and the <e2>model</e2> is treated as a black box."
sameAs(e1, e2)
Comment:

1519	"introduction current state of the art image <e1>classifiers</e1> rival human performance on image <e2>classification tasks</e2>, but often exhibit unexpected and unintuitive behaviour [6, 13] ."
Compare(e1, e2)
Comment:

1520	"therefore, large dimensionalities tend to increase <e1>model</e1> complexity, slow down training speed, and add inferential latency, all of which are constraints that can potentially limit <e2>model</e2> applicability and deployment [wu et al, 2016] ."
sameAs(e1, e2)
Comment:

1521	"in this regard, <e1>we</e1> outline a few major contributions of our paper: 1. <e2>we</e2> introduce the pip loss, a novel metric on the dissimilarity between word embeddings; 2. we develop a mathematical framework that reveals a fundamental bias-variance trade-off in dimensionality selection."
sameAs(e1, e2)
Comment:

1522	"in this regard, <e1>we</e1> outline a few major contributions of our paper: 1. we introduce the pip loss, a novel metric on the dissimilarity between word embeddings; 2. <e2>we</e2> develop a mathematical framework that reveals a fundamental bias-variance trade-off in dimensionality selection."
sameAs(e1, e2)
Comment:

1523	"in this regard, we outline a few major contributions of our paper: 1. <e1>we</e1> introduce the pip loss, a novel metric on the dissimilarity between word embeddings; 2. <e2>we</e2> develop a mathematical framework that reveals a fundamental bias-variance trade-off in dimensionality selection."
sameAs(e1, e2)
Comment:

1524	"we explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations; 3. <e1>we</e1> quantify the robustness of embedding algorithms using the exponent parameter α, and establish that many widely used embedding algorithms, including skip-gram and glove, are robust to over-fitting; 4. <e2>we</e2> propose a mathematically rigorous answer to the open problem of dimensionality selection by minimizing the pip loss."
sameAs(e1, e2)
Comment:

1525	"we explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations; 3. we quantify the robustness of embedding <e1>algorithms</e1> using the exponent parameter α, and establish that many widely used embedding <e2>algorithms</e2>, including skip-gram and glove, are robust to over-fitting; 4. we propose a mathematically rigorous answer to the open problem of dimensionality selection by minimizing the pip loss."
sameAs(e1, e2)
Comment:

1526	"we explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations; 3. we quantify the robustness of embedding <e1>algorithms</e1> using the exponent parameter α, and establish that many widely used embedding algorithms, including skip-gram and glove, are robust to over-fitting; 4. we propose a mathematically rigorous answer to the open <e2>problem</e2> of dimensionality selection by minimizing the pip loss."
Used-for(e1, e2)
Comment:

1527	"we explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations; 3. we quantify the robustness of embedding algorithms using the exponent parameter α, and establish that many widely used embedding <e1>algorithms</e1>, including skip-gram and glove, are robust to over-fitting; 4. we propose a mathematically rigorous answer to the open <e2>problem</e2> of dimensionality selection by minimizing the pip loss."
Used-for(e1, e2)
Comment:

1528	"we perform this procedure and cross-validate the results with grid search for lsa, skip-gram <e1>word2vec</e1> and <e2>glove</e2> on an english corpus."
Conjunction(e1, e2)
Comment:

1529	"personal <e1>data</e1> from, for example, smart phones, wearables and many other mobile devices is sensitive and exposed to a great risk of <e2>data</e2> breaches and abuse when collected by a centralized authority or enterprise."
sameAs(e1, e2)
Comment:

1530	"for example, hendrycks & gimpel [13] proposed the maximum value of posterior distribution from the <e1>classifier</e1> as a baseline <e2>method</e2>, and it is improved by processing the input and output of dnns [21] ."
Part-of(e1, e2)
Comment:

1531	"our high-level idea is to <e1>measure</e1> the probability density of test sample on feature spaces of dnns utilizing the concept of a "generative" (distance-based) <e2>classifier</e2>."
Evaluate-for(e1, e2)
Comment:

1532	"first, for the problem of detecting ood samples, the proposed <e1>method</e1> outperforms the current state-of-the-art <e2>method</e2>, odin [21] , in all tested cases."
sameAs(e1, e2)
Comment:

1533	"since the new class samples are drawn from an out-of-<e1>training</e1> distribution, it is natural to expect that one can classify them using our proposed metric without re-<e2>training</e2> the deep models."
sameAs(e1, e2)
Comment:

1534	"we show that the proposed method outperforms other baseline methods, such as euclidean distance-based <e1>classifier</e1> and re-trained softmax <e2>classifier</e2>."
sameAs(e1, e2)
Comment:

1535	"while most prior <e1>methods</e1> have been evaluated for detecting either out-of-distribution or adversarial samples, but not both, the proposed <e2>method</e2> achieves the state-of-the-art performances for both cases in our experiments."
Compare(e1, e2)
Comment:

1536	"introduction deep neural networks (dnns) have achieved high accuracy on many <e1>classification tasks</e1>, e.g., speech recognition [1] , object detection [9] and <e2>image classification</e2> [12] ."
Used-for(e1, e2)
Comment:

1537	"the set of <e1>objects</e1> and relations is sometimes represented as a graph, connecting <e2>objects</e2> (nodes) with their relations (edges) and is known as a scene graph (figure 1 )."
sameAs(e1, e2)
Comment:

1538	"the set of objects and <e1>relations</e1> is sometimes represented as a graph, connecting objects (nodes) with their <e2>relations</e2> (edges) and is known as a scene graph (figure 1 )."
sameAs(e1, e2)
Comment:

1539	"the set of objects and relations is sometimes represented as a <e1>graph</e1>, connecting objects (nodes) with their relations (edges) and is known as a scene <e2>graph</e2> (figure 1 )."
sameAs(e1, e2)
Comment:

1540	"structured prediction models typically define a score function s(x, y) that quantifies how well a label assignment y is compatible with an input x. in the case of understanding <e1>complex</e1> visual scenes, x is an image, and y is a <e2>complex</e2> label containing the labels of objects detected in an image and the labels of their relations."
sameAs(e1, e2)
Comment:

1541	"structured prediction models typically define a score function s(x, y) that quantifies how well a label assignment y is compatible with an input x. in the case of understanding complex visual scenes, x is an <e1>image</e1>, and y is a complex label containing the labels of objects detected in an <e2>image</e2> and the labels of their relations."
sameAs(e1, e2)
Comment:

1542	"one challenge underlying this task is <e1>that</e1> visual scenes contain multiple interrelated objects, and <e2>that</e2> global context plays an important role in interpreting the scene."
sameAs(e1, e2)
Comment:

1543	"one challenge underlying this task is that visual <e1>scenes</e1> contain multiple interrelated objects, and that global context plays an important role in interpreting the <e2>scene</e2>."
sameAs(e1, e2)
Comment:

1544	"this score-based approach separates a scoring <e1>component</e1> -implemented by a parametric model, from an optimization <e2>component</e2> -aimed at finding a label that maximizes that score."
sameAs(e1, e2)
Comment:

1545	"this score-based approach separates a scoring component -implemented by a parametric model, from an optimization component -aimed at finding a label <e1>that</e1> maximizes <e2>that</e2> score."
sameAs(e1, e2)
Comment:

1546	"for instance, for scene graphs the set of possible object label assignments is too large even for relatively simple images, since the vocabulary of candidate <e1>objects</e1> may contain thousands of <e2>objects</e2>."
sameAs(e1, e2)
Comment:

1547	"an alternative <e1>approach</e1> to score-based <e2>methods</e2> is to map an input x to a structured output y with a "black box" neural network, without explicitly defining a score function."
Compare(e1, e2)
Comment:

1548	"we then prove <e1>that</e1> this invariance is equivalent to imposing certain structural constraints on the architecture of the network, and describe architectures <e2>that</e2> satisfy these constraints."
sameAs(e1, e2)
Comment:

1549	"we then prove that this invariance is equivalent to imposing certain structural <e1>constraints</e1> on the architecture of the network, and describe architectures that satisfy these <e2>constraints</e2>."
sameAs(e1, e2)
Comment:

1550	"we then prove that this invariance is equivalent to imposing certain structural constraints on the <e1>architecture</e1> of the network, and describe <e2>architectures</e2> that satisfy these constraints."
sameAs(e1, e2)
Comment:

1551	"to evaluate our <e1>approach</e1>, we first demonstrate on a synthetic dataset that respecting permutation invariance is important, because <e2>models</e2> that violate this invariance need more training data, despite having a comparable model size."
Used-for(e1, e2)
Comment:

1552	"to evaluate our approach, we first demonstrate on a synthetic dataset <e1>that</e1> respecting permutation invariance is important, because models <e2>that</e2> violate this invariance need more training data, despite having a comparable model size."
sameAs(e1, e2)
Comment:

1553	"a natural <e1>modeling</e1> framework for capturing such effects is structured prediction, which optimizes over complex labels, while <e2>modeling</e2> within-label interactions."
sameAs(e1, e2)
Comment:

1554	"we describe a model <e1>that</e1> satisfies the permutation invariance property, and show <e2>that</e2> it achieves state-of-the-art results on the competitive visual genome benchmark [15] , demonstrating the power of our new design principle."
sameAs(e1, e2)
Comment:

1555	"c) developing a state-of-the-art model for <e1>scene</e1> graph prediction on a large dataset of complex visual <e2>scenes</e2>."
sameAs(e1, e2)
Comment:

1556	"abstract visual <e1>question answering</e1> is a challenging problem requiring a combination of concepts from computer vision and <e2>natural language processing</e2>."
isA(e1, e2)
Comment:

1557	"its attractiveness lies in the fact <e1>that</e1> it combines two fields <e2>that</e2> are typically approached individually (computer vision and natural language processing (nlp))."
sameAs(e1, e2)
Comment:

1558	"given an <e1>image</e1> and a question, the objective of vqa is to answer the question based on the information provided by the <e2>image</e2>."
sameAs(e1, e2)
Comment:

1559	"representing images as <e1>graphs</e1> allows one to explicitly model interactions, so as to seamlessly transfer information between <e2>graph</e2> items (e.g."
sameAs(e1, e2)
Comment:

1560	"bounding box object detections are defined as <e1>graph</e1> nodes, while <e2>graph</e2> edges conditioned on the question are learned via an attention based module."
sameAs(e1, e2)
Comment:

1561	"our intuition is that learning a graph structure not only provides strong predictive power for the vqa <e1>task</e1>, but also interpretability of the <e2>model</e2>'s behaviour by inspecting the most important graph nodes and edges."
Evaluate-for(e1, e2)
Comment:

1562	"combined with a relatively simple baseline, our <e1>graph</e1> learner module achieves 66.18% accuracy on the test set and provides interpretable results via visualisation of the learned <e2>graph</e2> representations."
sameAs(e1, e2)
Comment:

1563	"our method combines a <e1>graph</e1> learner module, which learns a question specific <e2>graph</e2> representation of the input image, with the recent concept of graph convolutions, aiming to learn image representations that capture question specific interactions."
sameAs(e1, e2)
Comment:

1564	"our method combines a <e1>graph</e1> learner module, which learns a question specific graph representation of the input image, with the recent concept of <e2>graph</e2> convolutions, aiming to learn image representations that capture question specific interactions."
sameAs(e1, e2)
Comment:

1565	"our method combines a graph learner module, which <e1>learns</e1> a question specific graph representation of the input image, with the recent concept of graph convolutions, aiming to <e2>learn</e2> image representations that capture question specific interactions."
sameAs(e1, e2)
Comment:

1566	"our method combines a graph learner module, which learns a question specific <e1>graph</e1> representation of the input image, with the recent concept of <e2>graph</e2> convolutions, aiming to learn image representations that capture question specific interactions."
sameAs(e1, e2)
Comment:

1567	"our method combines a graph learner module, which learns a question specific graph representation of the input <e1>image</e1>, with the recent concept of graph convolutions, aiming to learn <e2>image</e2> representations that capture question specific interactions."
sameAs(e1, e2)
Comment:

1568	"we obtain promising results with 66.18% <e1>accuracy</e1> and demonstrate the interpretability of the proposed <e2>method</e2>."
Evaluate-for(e1, e2)
Comment:

1569	"abstract <e1>recurrent neural networks</e1> (<e2>rnns</e2>) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of rnn models which can be trained."
sameAs(e1, e2)
Comment:

1570	"abstract <e1>recurrent neural networks</e1> (rnns) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of <e2>rnn</e2> models which can be trained."
sameAs(e1, e2)
Comment:

1571	"abstract recurrent neural networks (<e1>rnns</e1>) provide state-of-the-art performance in processing sequential data but are memory intensive to train, limiting the flexibility of <e2>rnn</e2> models which can be trained."
sameAs(e1, e2)
Comment:

1572	"abstract recurrent neural networks (rnns) provide state-of-the-art performance in processing sequential <e1>data</e1> but are memory intensive to train, limiting the flexibility of rnn <e2>models</e2> which can be trained."
Used-for(e1, e2)
Comment:

1573	"reversible architectures enable the reconstruction of the hidden <e1>state</e1> at the current timestep given the next hidden <e2>state</e2> and the current input, which would enable us to perform tbptt without storing the hidden states at each timestep."
sameAs(e1, e2)
Comment:

1574	"reversible architectures enable the reconstruction of the hidden <e1>state</e1> at the current timestep given the next hidden state and the current input, which would enable us to perform tbptt without storing the hidden <e2>states</e2> at each timestep."
sameAs(e1, e2)
Comment:

1575	"reversible architectures enable the reconstruction of the hidden state at the current timestep given the next hidden <e1>state</e1> and the current input, which would enable us to perform tbptt without storing the hidden <e2>states</e2> at each timestep."
sameAs(e1, e2)
Comment:

1576	"this <e1>task</e1> is trivial to solve even for vanilla rnns, but perfectly reversible models fail since they need to memorize the input sequence in order to solve the <e2>task</e2>."
sameAs(e1, e2)
Comment:

1577	"depending on the task, dataset, and chosen architecture, reversible <e1>models</e1> (without attention) achieve 10-15-fold memory savings over traditional <e2>models</e2>."
sameAs(e1, e2)
Comment:

1578	"reversible <e1>models</e1> achieve approximately equivalent performance to traditional lstm and gru <e2>models</e2> on word-level language modeling on the penn treebank dataset [marcus et al, 1993] and lag 2-5 perplexity points behind traditional models on the wikitext-2 dataset [merity et al, 2016] ."
sameAs(e1, e2)
Comment:

1579	"reversible <e1>models</e1> achieve approximately equivalent performance to traditional lstm and gru models on word-level language modeling on the penn treebank dataset [marcus et al, 1993] and lag 2-5 perplexity points behind traditional <e2>models</e2> on the wikitext-2 dataset [merity et al, 2016] ."
sameAs(e1, e2)
Comment:

1580	"reversible models achieve approximately equivalent performance to traditional lstm and gru <e1>models</e1> on word-level language modeling on the penn treebank dataset [marcus et al, 1993] and lag 2-5 perplexity points behind traditional <e2>models</e2> on the wikitext-2 dataset [merity et al, 2016] ."
sameAs(e1, e2)
Comment:

1581	"achieving comparable <e1>memory</e1> savings with attention-based recurrent sequence-to-sequence models is difficult, since the encoder hidden states must be kept simultaneously in <e2>memory</e2> in order to perform attention."
sameAs(e1, e2)
Comment:

1582	"with this <e1>technique</e1>, our reversible <e2>models</e2> succeed on neural machine translation tasks, outperforming baseline gru and lstm models on the multi30k dataset [elliott et al, 2016] and achieving competitive performance on the iwslt 2016 [cettolo et al, 2016] benchmark."
Compare(e1, e2)
Comment:

1583	"with this <e1>technique</e1>, our reversible models succeed on neural machine translation tasks, outperforming baseline gru and lstm <e2>models</e2> on the multi30k dataset [elliott et al, 2016] and achieving competitive performance on the iwslt 2016 [cettolo et al, 2016] benchmark."
Compare(e1, e2)
Comment:

1584	"with this technique, our reversible <e1>models</e1> succeed on neural machine translation tasks, outperforming baseline gru and lstm <e2>models</e2> on the multi30k dataset [elliott et al, 2016] and achieving competitive performance on the iwslt 2016 [cettolo et al, 2016] benchmark."
sameAs(e1, e2)
Comment:

1585	"our method achieves comparable performance to traditional <e1>models</e1> while reducing the activation memory cost by a factor of 10-15. we extend our technique to attention-based sequence-to-sequence <e2>models</e2>, where it maintains performance while reducing activation memory cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder."
sameAs(e1, e2)
Comment:

1586	"our method achieves comparable performance to traditional models while reducing the activation <e1>memory</e1> cost by a factor of 10-15. we extend our technique to attention-based sequence-to-sequence models, where it maintains performance while reducing activation <e2>memory</e2> cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder."
sameAs(e1, e2)
Comment:

1587	"our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10-15. <e1>we</e1> extend our technique to attention-based <e2>sequence-to-sequence</e2> models, where it maintains performance while reducing activation memory cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder."
Compare(e1, e2)
Comment:

1588	"our method achieves comparable performance to traditional models while reducing the activation memory cost by a factor of 10-15. we extend our <e1>technique</e1> to attention-based sequence-to-sequence <e2>models</e2>, where it maintains performance while reducing activation memory cost by a factor of 5-10 in the encoder, and a factor of 10-15 in the decoder."
Compare(e1, e2)
Comment:

1589	"introduction <e1>recurrent neural networks</e1> (<e2>rnns</e2>) have attained state-of-the-art performance on a variety of tasks, including speech recognition [graves et al, 2013] , language modeling [melis et al, 2017 , merity et al, 2017 , and machine translation , wu et al, 2016 ."
sameAs(e1, e2)
Comment:

1590	"introduction recurrent neural networks (rnns) have attained state-of-the-art performance on a variety of tasks, including <e1>speech recognition</e1> [graves et al, 2013] , language modeling [melis et al, 2017 , merity et al, 2017 , and <e2>machine translation</e2> , wu et al, 2016 ."
Conjunction(e1, e2)
Comment:

1591	"the standard <e1>training algorithm</e1> is truncated <e2>backpropagation</e2> through time (tbptt) [werbos, 1990 , rumelhart et al, 1986 ."
isA(e1, e2)
Comment:

1592	"improving shape deformation in unsupervised <e1>image</e1>-to-<e2>image</e2> translation"
sameAs(e1, e2)
Comment:

1593	"coping with shape deformation in <e1>image</e1> translation tasks requires the ability to use spatial information from across the <e2>image</e2>."
sameAs(e1, e2)
Comment:

1594	"we propose an <e1>image</e1>-to-<e2>image</e2> translation system, designated ganimorph, to address shortcomings present in current techniques."
sameAs(e1, e2)
Comment:

1595	"unsupervised <e1>image</e1>-to-<e2>image</e2> translation techniques are able to map local texture between two domains, but they are typically unsuccessful when the domains require larger shape change."
sameAs(e1, e2)
Comment:

1596	"unsupervised image-to-image translation techniques are able to map local texture between two <e1>domains</e1>, but they are typically unsuccessful when the <e2>domains</e2> require larger shape change."
sameAs(e1, e2)
Comment:

1597	"we demonstrate that our <e1>approach</e1> is more successful on a challenging shape deformation toy dataset than previous <e2>approaches</e2>."
Compare(e1, e2)
Comment:

1598	"introduction unsupervised <e1>image</e1>-to-<e2>image</e2> translation is the process of learning an arbitrary mapping between image domains without labels or pairings."
sameAs(e1, e2)
Comment:

1599	"introduction unsupervised <e1>image</e1>-to-image translation is the process of learning an arbitrary mapping between <e2>image</e2> domains without labels or pairings."
sameAs(e1, e2)
Comment:

1600	"introduction unsupervised image-to-<e1>image</e1> translation is the process of learning an arbitrary mapping between <e2>image</e2> domains without labels or pairings."
sameAs(e1, e2)
Comment:

1601	"this can be accomplished via deep learning with <e1>generative adversarial networks</e1> (<e2>gans</e2>), through the use of a discriminator network to provide instance-specific generator training, and the use of a cyclic loss to overcome the lack of supervised pairing."
sameAs(e1, e2)
Comment:

1602	"however, we observe <e1>that</e1> prior work in fgvc does not pay much attention to the problems <e2>that</e2> may arise due to the inter-class visual similarity in the feature extraction pipeline."
sameAs(e1, e2)
Comment:

1603	"similar to lsvc <e1>tasks</e1>, neural networks for fgvc <e2>tasks</e2> are typically trained with cross-entropy loss [1, 7, 8, 9] ."
sameAs(e1, e2)
Comment:

1604	"for instance, if <e1>two</e1> samples in the training set have very similar visual content but different class labels, minimizing the cross-entropy loss will force the neural network to learn features that distinguish these <e2>two</e2> images with high confidence-potentially forcing the network to learn sample-specific artifacts for visually confusing classes in order to minimize training error."
sameAs(e1, e2)
Comment:

1605	"for instance, if two samples in the <e1>training</e1> set have very similar visual content but different class labels, minimizing the cross-entropy loss will force the neural network to learn features that distinguish these two images with high confidence-potentially forcing the network to learn sample-specific artifacts for visually confusing classes in order to minimize <e2>training</e2> error."
sameAs(e1, e2)
Comment:

1606	"for instance, if two samples in the training set have very similar visual content but different class labels, minimizing the cross-entropy loss will force the neural network to <e1>learn</e1> features that distinguish these two images with high confidence-potentially forcing the network to <e2>learn</e2> sample-specific artifacts for visually confusing classes in order to minimize training error."
sameAs(e1, e2)
Comment:

1607	"using pairwise confusion with a standard <e1>network</e1> architecture like densenet [11] or resnet [12] as a base <e2>network</e2>, we obtain state-of-the-art performance on six of the most widely-used fine-grained recognition datasets, improving over the previous-best published methods by 1.86% on average."
sameAs(e1, e2)
Comment:

1608	"in addition, pc-trained <e1>networks</e1> show better localization performance as compared to standard <e2>networks</e2>."
sameAs(e1, e2)
Comment:

1609	"pairwise confusion is simple to implement, has no added overhead in <e1>training</e1> or prediction time, and provides performance improvements both in fgvc tasks and other tasks that involve transfer learning with small amounts of <e2>training</e2> data."
sameAs(e1, e2)
Comment:

1610	"pairwise confusion is simple to implement, has no added overhead in training or prediction time, and provides performance improvements both in fgvc <e1>tasks</e1> and other <e2>tasks</e2> that involve transfer learning with small amounts of training data."
sameAs(e1, e2)
Comment:

1611	"in <e1>this</e1> work, we address <e2>this</e2> problem using a novel optimization procedure for the end-to-end neural network training on fgvc tasks."
sameAs(e1, e2)
Comment:

1612	"in particular, we 1) make a few changes to the <e1>neural network</e1> architecture, 2) propose a better learning algorithm to train the model using reinforcement learning, and 3) show how to significantly increase the out-put resolution of the polygon (one of the main limitations of the original model) using a graph <e2>neural network</e2> [31, 17] ."
sameAs(e1, e2)
Comment:

1613	"in particular, we 1) make a few changes to the neural network architecture, 2) propose a better learning algorithm to train the <e1>model</e1> using reinforcement learning, and 3) show how to significantly increase the out-put resolution of the polygon (one of the main limitations of the original <e2>model</e2>) using a graph neural network [31, 17] ."
sameAs(e1, e2)
Comment:

1614	"we further show that a simple online fine-tuning approach achieves high <e1>annotation</e1> speed-ups on out-of-domain dataset <e2>annotation</e2>."
sameAs(e1, e2)
Comment:

1615	"furthermore, we exploit a simple online fine-tuning <e1>method</e1> to adapt our <e2>model</e2> from one dataset to efficiently annotate novel, out-of-domain datasets."
Used-for(e1, e2)
Comment:

1616	"it is well known <e1>that</e1> the amount and variety of data <e2>that</e2> the networks see during training drastically affects their performance at run time."
sameAs(e1, e2)
Comment:

1617	"it is an especially difficult problem, because large variations in viewpoint and lighting across different views can cause two <e1>images</e1> of the same person to look quite different and can cause <e2>images</e2> of different people to look very similar."
sameAs(e1, e2)
Comment:

1618	"a typical re-identification system takes as input <e1>two</e1> images, each of which usually contains a person's full body, and outputs either a similarity score between the <e2>two</e2> images or a classification of the pair of images as same (if the two images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1619	"a typical re-identification system takes as input <e1>two</e1> images, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of images as same (if the <e2>two</e2> images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1620	"a typical re-identification system takes as input two <e1>images</e1>, each of which usually contains a person's full body, and outputs either a similarity score between the two <e2>images</e2> or a classification of the pair of images as same (if the two images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1621	"a typical re-identification system takes as input two <e1>images</e1>, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of <e2>images</e2> as same (if the two images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1622	"a typical re-identification system takes as input two <e1>images</e1>, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of images as same (if the two <e2>images</e2> depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1623	"a typical re-identification system takes as input two <e1>images</e1>, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of images as same (if the two images depict the same person) or different (if the <e2>images</e2> are of different people)."
sameAs(e1, e2)
Comment:

1624	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the <e1>two</e1> images or a classification of the pair of images as same (if the <e2>two</e2> images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1625	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two <e1>images</e1> or a classification of the pair of <e2>images</e2> as same (if the two images depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1626	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two <e1>images</e1> or a classification of the pair of images as same (if the two <e2>images</e2> depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1627	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two <e1>images</e1> or a classification of the pair of images as same (if the two images depict the same person) or different (if the <e2>images</e2> are of different people)."
sameAs(e1, e2)
Comment:

1628	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of <e1>images</e1> as same (if the two <e2>images</e2> depict the same person) or different (if the images are of different people)."
sameAs(e1, e2)
Comment:

1629	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of <e1>images</e1> as same (if the two images depict the same person) or different (if the <e2>images</e2> are of different people)."
sameAs(e1, e2)
Comment:

1630	"a typical re-identification system takes as input two images, each of which usually contains a person's full body, and outputs either a similarity score between the two images or a classification of the pair of images as same (if the two <e1>images</e1> depict the same person) or different (if the <e2>images</e2> are of different people)."
sameAs(e1, e2)
Comment:

1631	"in <e1>this</e1> paper, we follow <e2>this</e2> approach and use a novel deep learning network to assign similarity scores to pairs of images of human bodies."
sameAs(e1, e2)
Comment:

1632	"our network architecture includes two novel layers: a <e1>neighborhood</e1> difference layer that compares convolutional image features in each patch of one input image to the same features computed on nearby patches in the other input image, and a subsequent layer whose features summarize each patch's <e2>neighborhood</e2> differences."
sameAs(e1, e2)
Comment:

1633	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional <e1>image</e1> features in each patch of one input <e2>image</e2> to the same features computed on nearby patches in the other input image, and a subsequent layer whose features summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1634	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional <e1>image</e1> features in each patch of one input image to the same features computed on nearby patches in the other input <e2>image</e2>, and a subsequent layer whose features summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1635	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image <e1>features</e1> in each patch of one input image to the same <e2>features</e2> computed on nearby patches in the other input image, and a subsequent layer whose features summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1636	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image <e1>features</e1> in each patch of one input image to the same features computed on nearby patches in the other input image, and a subsequent layer whose <e2>features</e2> summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1637	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image features in each patch of <e1>one</e1> input image to the same features computed on nearby patches in the <e2>other</e2> input image, and a subsequent layer whose features summarize each patch's neighborhood differences."
Conjunction(e1, e2)
Comment:

1638	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image features in each patch of one <e1>input</e1> image to the same features computed on nearby patches in the other <e2>input</e2> image, and a subsequent layer whose features summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1639	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image features in each patch of one input <e1>image</e1> to the same features computed on nearby patches in the other input <e2>image</e2>, and a subsequent layer whose features summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1640	"our network architecture includes two novel layers: a neighborhood difference layer that compares convolutional image features in each patch of one input image to the same <e1>features</e1> computed on nearby patches in the other input image, and a subsequent layer whose <e2>features</e2> summarize each patch's neighborhood differences."
sameAs(e1, e2)
Comment:

1641	"given a pair of <e1>images</e1> as input, our network outputs a similarity value indicating whether the two input <e2>images</e2> depict the same person."
sameAs(e1, e2)
Comment:

1642	"given a pair of images as <e1>input</e1>, our network outputs a similarity value indicating whether the two <e2>input</e2> images depict the same person."
sameAs(e1, e2)
Comment:

1643	"novel elements of our architecture include a layer that computes cross-<e1>input</e1> neighborhood differences, which capture local relationships between the two <e2>input</e2> images based on midlevel features from each input image."
sameAs(e1, e2)
Comment:

1644	"novel elements of our architecture include a layer that computes cross-<e1>input</e1> neighborhood differences, which capture local relationships between the two input images based on midlevel features from each <e2>input</e2> image."
sameAs(e1, e2)
Comment:

1645	"novel elements of our architecture include a layer that computes cross-input neighborhood differences, which capture local relationships between the two <e1>input</e1> images based on midlevel features from each <e2>input</e2> image."
sameAs(e1, e2)
Comment:

1646	"our method significantly outperforms the state of the art on both a large <e1>data</e1> set (cuhk03) and a medium-sized <e2>data</e2> set (cuhk01), and is resistant to overfitting."
sameAs(e1, e2)
Comment:

1647	"we also demonstrate that by initially training on an unrelated large <e1>data</e1> set before fine-tuning on a small target <e2>data</e2> set, our network can achieve results comparable to the state of the art even on a small data set (viper)."
sameAs(e1, e2)
Comment:

1648	"we also demonstrate that by initially training on an unrelated large <e1>data</e1> set before fine-tuning on a small target data set, our network can achieve results comparable to the state of the art even on a small <e2>data</e2> set (viper)."
sameAs(e1, e2)
Comment:

1649	"we also demonstrate that by initially training on an unrelated large data set before fine-tuning on a small target <e1>data</e1> set, our network can achieve results comparable to the state of the art even on a small <e2>data</e2> set (viper)."
sameAs(e1, e2)
Comment:

1650	"learning pixel-level <e1>semantic</e1> affinity with image-level supervision for weakly supervised <e2>semantic</e2> segmentation"
sameAs(e1, e2)
Comment:

1651	"given an image and its cams, we first build a neighborhood <e1>graph</e1> where each pixel is connected to its neighbors within a certain radius, and estimate semantic affinities of pairs connected in the <e2>graph</e2> through affinitynet."
sameAs(e1, e2)
Comment:

1652	"sparse <e1>activations</e1> in cams are then diffused by random walk [23] on the graph, for each class: the affinities on edges in the graph encourage random walk to propagate the <e2>activations</e2> to nearby and semantically identical areas, and penalize propagation to areas of the other classes."
sameAs(e1, e2)
Comment:

1653	"sparse activations in cams are then diffused by random walk [23] on the <e1>graph</e1>, for each class: the affinities on edges in the <e2>graph</e2> encourage random walk to propagate the activations to nearby and semantically identical areas, and penalize propagation to areas of the other classes."
sameAs(e1, e2)
Comment:

1654	"• unlike most previous weakly supervised methods, our <e1>approach</e1> does not rely heavily on off-the-shelf <e2>techniques</e2>, and takes advantage of representation learning through end-to-end training of affinitynet."
Compare(e1, e2)
Comment:

1655	"• on the pascal voc 2012 [8] , ours achieves stateof-the-art performance among models trained under the same level of <e1>supervision</e1>, and is competitive with those relying on stronger <e2>supervision</e2> or external data."
sameAs(e1, e2)
Comment:

1656	"among various types of weak annotations for semantic segmentation, <e1>image</e1>-level class labels have been widely used [11, 14, 17, 26, 29, 30, 37] since they are already given in existing largescale <e2>image</e2> datasets (e.g., imagenet [7] ) or automatically annotated for image retrieval results by search keywords."
sameAs(e1, e2)
Comment:

1657	"among various types of weak annotations for semantic segmentation, <e1>image</e1>-level class labels have been widely used [11, 14, 17, 26, 29, 30, 37] since they are already given in existing largescale image datasets (e.g., imagenet [7] ) or automatically annotated for <e2>image</e2> retrieval results by search keywords."
sameAs(e1, e2)
Comment:

1658	"among various types of weak annotations for semantic segmentation, image-level class labels have been widely used [11, 14, 17, 26, 29, 30, 37] since they are already given in existing largescale <e1>image</e1> datasets (e.g., imagenet [7] ) or automatically annotated for <e2>image</e2> retrieval results by search keywords."
sameAs(e1, e2)
Comment:

1659	"however, learning semantic segmentation with the imagelevel label <e1>supervision</e1> is a significantly ill-posed problem since such <e2>supervision</e2> indicates only the existence of a certain object class, and does not inform object location and shape that are essential for learning segmentation."
sameAs(e1, e2)
Comment:

1660	"we propose an anatomically coherent approach that is not constrained to a discrete number of <e1>expressions</e1> and can animate a given image and render novel <e2>expressions</e2> in a continuum."
sameAs(e1, e2)
Comment:

1661	"recent advances in <e1>generative adversarial networks</e1> (<e2>gans</e2>) have shown impressive results for task of facial expression synthesis."
sameAs(e1, e2)
Comment:

1662	"ekman and friesen [6] developed the facial <e1>action</e1> coding system (facs) for describing facial expressions in terms of the so-called <e2>action</e2> units (aus), which are anatomically related to the contractions of specific facial muscles."
sameAs(e1, e2)
Comment:

1663	"for this purpose <e1>we</e1> leverage on the recent emotionet dataset [3] , which consists of one million images of facial expressions (<e2>we</e2> use 200,000 of them) of emotion in the wild annotated with discrete aus activations 1 ."
sameAs(e1, e2)
Comment:

1664	"the most successful architecture is stargan, that conditions gans' generation process with <e1>images</e1> of a specific domain, namely a set of <e2>images</e2> of persons sharing the same expression."
sameAs(e1, e2)
Comment:

1665	"this synthesized <e1>image</e1> is then rendered-back to the original pose, hence being directly comparable to the input <e2>image</e2>."
sameAs(e1, e2)
Comment:

1666	"we achieve this by means of an attention layer <e1>that</e1> focuses the action of the network only in those regions of the image <e2>that</e2> are relevant to convey the novel expression."
sameAs(e1, e2)
Comment:

1667	"as a result, we build an anatomically coherent facial expression synthesis method, able to render <e1>images</e1> in a continuous domain, and which can handle <e2>images</e2> in the wild with complex backgrounds and illumination conditions."
sameAs(e1, e2)
Comment:

1668	"as we will show in the <e1>results</e1> section, it compares favorably to other conditioned-gans schemes, both in terms of the visual quality of the <e2>results</e2>, and the possibilities of generation."
sameAs(e1, e2)
Comment:

1669	"figure 1 shows some example of the results <e1>we</e1> obtain, in which given one input image, <e2>we</e2> gradually change the magnitude of activation of the aus used to produce a smile."
sameAs(e1, e2)
Comment:

1670	"to address <e1>this</e1> limitation, in <e2>this</e2> paper, we introduce a novel gan conditioning scheme based on action units (au) annotations, which describes in a continuous manifold the anatomical facial movements defining a human expression."
sameAs(e1, e2)
Comment:

1671	"additionally, we propose a fully unsupervised strategy to train the model, <e1>that</e1> only requires images annotated with their activated aus, and exploit attention mechanisms <e2>that</e2> make our network robust to changing backgrounds and lighting conditions."
sameAs(e1, e2)
Comment:

1672	"the four images to the left show, from left to right, the center view of the input light field, the diffuse <e1>component</e1>, the specular <e2>component</e2> (scaled for better visibility) and the disparity."
sameAs(e1, e2)
Comment:

1673	"from top to bottom, they again show the input, the diffuse <e1>component</e1>, the specular <e2>component</e2> and the disparity."
sameAs(e1, e2)
Comment:

1674	"we achieve model sizes comparable to state-of-the-art <e1>pruning</e1> techniques using our simple architecture design, without any <e2>pruning</e2>."
sameAs(e1, e2)
Comment:

1675	"we explore making <e1>networks</e1> efficient by designing sparse <e2>networks</e2> that preserve connectivity properties."
sameAs(e1, e2)
Comment:

1676	"however, in order to achieve this, they sparsify a <e1>network</e1> by removing several connections from a trained <e2>network</e2>, reducing their accuracies in the process."
sameAs(e1, e2)
Comment:

1677	"we ask a basic question: if <e1>we</e1> try to maximize the connectivity properties and information flow, can <e2>we</e2> achieve the same efficiency gains with minimal loss in accuracy?"
sameAs(e1, e2)
Comment:

1678	"we propose to make the connections between neurons (filters in the case of cnns) according to specific <e1>graph</e1> constructions known as expander <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

1679	"they have been widely studied in spectral <e1>graph</e1> theory [8] and pseudorandomness [9] , and are known to be sparse but highly connected <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

1680	"we also demonstrate the robustness of our <e1>approach</e1> by applying the technique to some of the state of the art <e2>models</e2> like densenet-bc and resnet, obtaining better performance tradeoffs (see section 5.2)."
Used-for(e1, e2)
Comment:

1681	"we also demonstrate the robustness of our approach by applying the <e1>technique</e1> to some of the state of the art <e2>models</e2> like densenet-bc and resnet, obtaining better performance tradeoffs (see section 5.2)."
Compare(e1, e2)
Comment:

1682	"we use a well-studied class of <e1>graphs</e1> from theoretical computer science that satisfies these properties known as expander <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

1683	"we present <e1>two</e1> guarantees on the connectivity of x-nets: each node influences every node in a layer in logarithmic steps, and the number of paths between <e2>two</e2> sets of nodes is proportional to the product of their sizes."
sameAs(e1, e2)
Comment:

1684	"we present two guarantees on the connectivity of x-nets: each <e1>node</e1> influences every <e2>node</e2> in a layer in logarithmic steps, and the number of paths between two sets of nodes is proportional to the product of their sizes."
sameAs(e1, e2)
Comment:

1685	"we then provide experimental results <e1>that</e1> demonstrate the capabilities of the architecture, highlighting potential tasks <e2>that</e2> these architectures can solve, and illustrating improvements upon existing approaches."
sameAs(e1, e2)
Comment:

1686	"we then provide experimental results that demonstrate the capabilities of the <e1>architecture</e1>, highlighting potential tasks that these <e2>architectures</e2> can solve, and illustrating improvements upon existing approaches."
sameAs(e1, e2)
Comment:

1687	"these <e1>layers</e1> encode constraints and complex dependencies between the hidden states that traditional convolutional and fully-connected <e2>layers</e2> often cannot capture."
sameAs(e1, e2)
Comment:

1688	"in this paper, <e1>we</e1> explore the foundations for such an architecture: <e2>we</e2> show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1689	"in this paper, <e1>we</e1> explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; <e2>we</e2> develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1690	"in this paper, <e1>we</e1> explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and <e2>we</e2> highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1691	"in this paper, we explore the foundations for such an architecture: <e1>we</e1> show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; <e2>we</e2> develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1692	"in this paper, we explore the foundations for such an architecture: <e1>we</e1> show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and <e2>we</e2> highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1693	"in this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through <e1>these</e1> layers and with respect to layer parameters; we develop a highly efficient solver for <e2>these</e2> layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1694	"in this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through <e1>these</e1> layers and with respect to layer parameters; we develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of <e2>these</e2> approaches in several problems."
sameAs(e1, e2)
Comment:

1695	"in this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these <e1>layers</e1> and with respect to layer parameters; we develop a highly efficient solver for these <e2>layers</e2> that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1696	"in this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; <e1>we</e1> develop a highly efficient solver for these layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and <e2>we</e2> highlight the application of these approaches in several problems."
sameAs(e1, e2)
Comment:

1697	"in this paper, we explore the foundations for such an architecture: we show how techniques from sensitivity analysis, bilevel optimization, and implicit differentiation can be used to exactly differentiate through these layers and with respect to layer parameters; we develop a highly efficient solver for <e1>these</e1> layers that exploits fast gpubased batch solves within a primal-dual interior point method, and which provides backpropagation gradients with virtually no additional cost on top of the solve; and we highlight the application of <e2>these</e2> approaches in several problems."
sameAs(e1, e2)
Comment:

1698	"in <e1>one</e1> notable example, we show that the method is capable of learning to play mini-sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our architecture to learn hard constraints better than <e2>other</e2> neural architectures."
Conjunction(e1, e2)
Comment:

1699	"in one notable example, we show that the method is capable of learning to play mini-sudoku (4x4) given just input and output games, with no a priori information about the rules of the game; this highlights the ability of our <e1>architecture</e1> to learn hard constraints better than other neural <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

1700	"unlike traditional feedforward <e1>networks</e1>, where the output of each layer is a relatively simple (though non-linear) function of the previous layer, our optimization framework allows for individual layers to capture much richer behavior, expressing complex operations that in to-in order to the make the approach practical for larger <e2>networks</e2>, we develop a custom solver which can simultaneously solve multiple small qps in batch form."
sameAs(e1, e2)
Comment:

1701	"unlike traditional feedforward networks, where the output of each layer is a relatively simple (though non-linear) function of the previous layer, our optimization <e1>framework</e1> allows for individual layers to capture much richer behavior, expressing complex operations that in to-in order to the make the <e2>approach</e2> practical for larger networks, we develop a custom solver which can simultaneously solve multiple small qps in batch form."
Used-for(e1, e2)
Comment:

1702	"one crucial algorithmic insight in the solver is that by using a specific <e1>factorization</e1> of the primal-dual interior point update, we can obtain a backward pass over the optimization layer virtually "for free" (i.e., requiring no additional <e2>factorization</e2> once the optimization problem itself has been solved)."
sameAs(e1, e2)
Comment:

1703	"that is, instead of making <e1>predictions</e1> in a neural network via a purely feedforward process, we can make <e2>predictions</e2> by optimizing a scalar function (which effectively plays the role of an energy function) over some inputs to the function given others."
sameAs(e1, e2)
Comment:

1704	"that is, instead of making predictions in a neural network via a purely feedforward process, we can make predictions by optimizing a scalar <e1>function</e1> (which effectively plays the role of an energy function) over some inputs to the <e2>function</e2> given others."
sameAs(e1, e2)
Comment:

1705	"structured <e1>prediction</e1> as is perhaps apparent from our notation above, a key application of this work is in structured <e2>prediction</e2>."
sameAs(e1, e2)
Comment:

1706	"in our setting, assuming that y is a <e1>convex</e1> space (a common assumption in structured prediction), this optimization problem is <e2>convex</e2>."
sameAs(e1, e2)
Comment:

1707	"data imputation similar to structured prediction but slightly more generic, if <e1>we</e1> are given some space y <e2>we</e2> can learn a network f (y; θ) (removing the additional x inputs, though these can be added as well) that, given an example with some subset i missing, imputes the likely values of these variables by solving the optimization problem as aboveŷ i = argmin y i f (y i , yī; θ) this could be used e.g., in image inpainting where the goal is to fill in some arbitrary set of missing pixels given observed ones."
sameAs(e1, e2)
Comment:

1708	"data imputation similar to structured prediction but slightly more generic, if we are given some space y we can learn a network f (y; θ) (removing the additional x inputs, though <e1>these</e1> can be added as well) that, given an example with some subset i missing, imputes the likely values of <e2>these</e2> variables by solving the optimization problem as aboveŷ i = argmin y i f (y i , yī; θ) this could be used e.g., in image inpainting where the goal is to fill in some arbitrary set of missing pixels given observed ones."
sameAs(e1, e2)
Comment:

1709	"continuous <e1>action</e1> reinforcement learning given a reinforcement learning problem with potentially continuous state and <e2>action</e2> spaces s × a, we can model the (negative) q function, −q(s, a; θ) as an input convex neural network."
sameAs(e1, e2)
Comment:

1710	"continuous action <e1>reinforcement learning</e1> given a <e2>reinforcement learning</e2> problem with potentially continuous state and action spaces s × a, we can model the (negative) q function, −q(s, a; θ) as an input convex neural network."
sameAs(e1, e2)
Comment:

1711	"continuous action reinforcement learning given a reinforcement learning problem with potentially continuous <e1>state</e1> and <e2>action</e2> spaces s × a, we can model the (negative) q function, −q(s, a; θ) as an input convex neural network."
Conjunction(e1, e2)
Comment:

1712	"these are scalar-valued (potentially deep) neural networks with constraints on the <e1>network</e1> parameters such that the output of the <e2>network</e2> is a convex function of (some of) the inputs."
sameAs(e1, e2)
Comment:

1713	"our main contributions are: <e1>we</e1> propose the icnn architecture and a partially convex variant; <e2>we</e2> develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1714	"our main contributions are: <e1>we</e1> propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; <e2>we</e2> propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1715	"our main contributions are: <e1>we</e1> propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and <e2>we</e2> evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1716	"our main contributions are: <e1>we</e1> propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings <e2>we</e2> show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1717	"our main contributions are: we propose the icnn architecture and a partially convex variant; <e1>we</e1> develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; <e2>we</e2> propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1718	"our main contributions are: we propose the icnn architecture and a partially convex variant; <e1>we</e1> develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and <e2>we</e2> evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1719	"our main contributions are: we propose the icnn architecture and a partially convex variant; <e1>we</e1> develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings <e2>we</e2> show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1720	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures <e1>that</e1> are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance <e2>that</e2> improves upon the state of the art."
sameAs(e1, e2)
Comment:

1721	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of <e1>these</e1> specific models; we propose techniques for training <e2>these</e2> models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1722	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of <e1>these</e1> specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of <e2>these</e2> settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1723	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific <e1>models</e1>; we propose techniques for training these <e2>models</e2>, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1724	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; <e1>we</e1> propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and <e2>we</e2> evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1725	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; <e1>we</e1> propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings <e2>we</e2> show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1726	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training <e1>these</e1> models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and we evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of <e2>these</e2> settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1727	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured <e1>prediction</e1> or direct differentiation of the argmin operation; and we evaluate the system on multi-label <e2>prediction</e2>, image completion, and reinforcement learning domains; in many of these settings we show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1728	"our main contributions are: we propose the icnn architecture and a partially convex variant; we develop efficient optimization and inference procedures that are well-suited to the complexity of these specific models; we propose techniques for training these models, based upon either max-margin structured prediction or direct differentiation of the argmin operation; and <e1>we</e1> evaluate the system on multi-label prediction, image completion, and reinforcement learning domains; in many of these settings <e2>we</e2> show performance that improves upon the state of the art."
sameAs(e1, e2)
Comment:

1729	"the networks allow for efficient inference via optimization over some inputs to the network given <e1>others</e1>, and can be applied to settings including structured prediction, data imputation, reinforcement learning, and <e2>others</e2>."
sameAs(e1, e2)
Comment:

1730	"finally, <e1>we</e1> highlight the performance of the methods on multi-label prediction, image completion, and reinforcement learning problems, where <e2>we</e2> show improvement over the existing state of the art in many cases."
sameAs(e1, e2)
Comment:

1731	"introduction in this paper, <e1>we</e1> propose a new neural network architecture that <e2>we</e2> call the input convex neural network (icnn).these are scalar-valued neural networks f (x, y; θ) where x and y denotes inputs to the function and θ denotes the parameters, built in such a way that the network is convex in (a subset of) inputs y."
sameAs(e1, e2)
Comment:

1732	"introduction in this paper, we propose a new <e1>neural network</e1> architecture that we call the input convex <e2>neural network</e2> (icnn).these are scalar-valued neural networks f (x, y; θ) where x and y denotes inputs to the function and θ denotes the parameters, built in such a way that the network is convex in (a subset of) inputs y."
sameAs(e1, e2)
Comment:

1733	"introduction in this paper, we propose a new neural network architecture <e1>that</e1> we call the input convex neural network (icnn).these are scalar-valued neural networks f (x, y; θ) where x and y denotes inputs to the function and θ denotes the parameters, built in such a way <e2>that</e2> the network is convex in (a subset of) inputs y."
sameAs(e1, e2)
Comment:

1734	"introduction in this paper, we propose a new neural network architecture that we call the input <e1>convex</e1> neural network (icnn).these are scalar-valued neural networks f (x, y; θ) where x and y denotes inputs to the function and θ denotes the parameters, built in such a way that the network is <e2>convex</e2> in (a subset of) inputs y."
sameAs(e1, e2)
Comment:

1735	"introduction in this paper, we propose a new neural network architecture that we call the input convex neural network (icnn).these are scalar-valued neural networks f (x, y; θ) where x and y denotes <e1>inputs</e1> to the function and θ denotes the parameters, built in such a way that the network is convex in (a subset of) <e2>inputs</e2> y."
sameAs(e1, e2)
Comment:

1736	"3 the fundamental benefit to these icnns is that we can optimize over the convex <e1>inputs</e1> to the network given some fixed value for other <e2>inputs</e2>."
sameAs(e1, e2)
Comment:

1737	"in this paper we draw from both lines of research, presenting a technique for integrating the representational power of neural networks with the flexible compositional <e1>structure</e1> afforded by symbolic approaches to <e2>semantics</e2>."
Conjunction(e1, e2)
Comment:

1738	"rather than relying on a monolithic <e1>network</e1> structure to answer all questions, our approach assembles a <e2>network</e2> on the fly from a collection of specialized, jointly-learned modules ( figure 1 )."
sameAs(e1, e2)
Comment:

1739	"depending on the underlying structure, these messages passed between modules may be raw <e1>image</e1> features, attentions, or <e2>classification</e2> decisions; each module maps from specific input to output types."
Used-for(e1, e2)
Comment:

1740	"different kinds of <e1>modules</e1> are shown in different colors; attention-producing <e2>modules</e2> (like dog) are shown in green, while labeling modules (like where) are shown in blue."
sameAs(e1, e2)
Comment:

1741	"different kinds of <e1>modules</e1> are shown in different colors; attention-producing modules (like dog) are shown in green, while labeling <e2>modules</e2> (like where) are shown in blue."
sameAs(e1, e2)
Comment:

1742	"different kinds of modules are shown in different colors; attention-producing <e1>modules</e1> (like dog) are shown in green, while labeling <e2>modules</e2> (like where) are shown in blue."
sameAs(e1, e2)
Comment:

1743	"while all the applications considered in this paper involve visual <e1>question answering</e1>, the architecture is much more general, and might easily be applied to visual referring expression resolution [9, 34] or <e2>question answering</e2> about natural language texts [15] ."
sameAs(e1, e2)
Comment:

1744	"next, for the visual qa task specifically, we show how to construct nmns based on the output of a semantic parser, and use <e1>these</e1> to successfully complete established visual question answering <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

1745	"we have released the dataset, as well as code for the <e1>system</e1> described in <e2>this</e2> paper, at http://github.com/jacobandreas/nmn2."
Used-for(e1, e2)
Comment:

1746	"we present 3dmv, a novel method for 3d semantic scene <e1>segmentation</e1> of rgb-d scans in indoor environments using a joint 3d-multi-view <e2>prediction</e2> network."
Conjunction(e1, e2)
Comment:

1747	"we present 3dmv, a novel method for 3d semantic scene <e1>segmentation</e1> of rgb-d scans in indoor environments using a joint 3d-multi-view <e2>prediction</e2> network."
Conjunction(e1, e2)
Comment:

1748	"1 : given an annotated <e1>image</e1> collection of an object category, we learn a predictor f that can map a novel <e2>image</e2> i to its 3d shape, camera pose, and texture."
sameAs(e1, e2)
Comment:

1749	"we can do <e1>this</e1> because all the previously seen birds have enabled us to develop a mental model of what birds are like, and <e2>this</e2> knowledge helps us to recover the 3d structure of this novel instance."
sameAs(e1, e2)
Comment:

1750	"we can do <e1>this</e1> because all the previously seen birds have enabled us to develop a mental model of what birds are like, and this knowledge helps us to recover the 3d structure of <e2>this</e2> novel instance."
sameAs(e1, e2)
Comment:

1751	"we can do this because all the previously seen birds have enabled us to develop a mental model of what birds are like, and <e1>this</e1> knowledge helps us to recover the 3d structure of <e2>this</e2> novel instance."
sameAs(e1, e2)
Comment:

1752	"we represent the shape as a 3d <e1>mesh</e1> in a canonical frame, where the predicted camera transforms the <e2>mesh</e2> from this canonical space to the image coordinates."
sameAs(e1, e2)
Comment:

1753	"the particular <e1>shape</e1> of each instance is instantiated by deforming a learned category-specific mean <e2>shape</e2> with instance-specific predicted deformations."
sameAs(e1, e2)
Comment:

1754	"as we detail in section 2, this allows us to formulate the task of inferring mesh <e1>texture</e1> of different objects as that of predicting pixel values in a common <e2>texture</e2> representation."
sameAs(e1, e2)
Comment:

1755	"unfortunately, all of these approaches require a large <e1>collection</e1> of 3d data to learn the model, preventing their application to categories where such data <e2>collection</e2> is impractical."
sameAs(e1, e2)
Comment:

1756	"unfortunately, all of these approaches require a large collection of 3d <e1>data</e1> to learn the model, preventing their application to categories where such <e2>data</e2> collection is impractical."
sameAs(e1, e2)
Comment:

1757	"the <e1>shape</e1> is represented as a deformable 3d mesh model of an object category where a <e2>shape</e2> is parameterized by a learned mean shape and per-instance predicted deformation."
sameAs(e1, e2)
Comment:

1758	"the <e1>shape</e1> is represented as a deformable 3d mesh model of an object category where a shape is parameterized by a learned mean <e2>shape</e2> and per-instance predicted deformation."
sameAs(e1, e2)
Comment:

1759	"the shape is represented as a deformable 3d mesh model of an object category where a <e1>shape</e1> is parameterized by a learned mean <e2>shape</e2> and per-instance predicted deformation."
sameAs(e1, e2)
Comment:

1760	"sharing our motivation for relaxing the requirement of 3d <e1>data</e1> to learn morphable <e2>models</e2>, some related approaches have examined the use of similarly annotated image collections."
Used-for(e1, e2)
Comment:

1761	"moreover, unlike <e1>these</e1> approaches, we also address the task of texture prediction which cannot be easily incorporated with <e2>these</e2> methods."
sameAs(e1, e2)
Comment:

1762	"relying on ground-truth 3d <e1>supervision</e1> (using synthetic data), some <e2>approaches</e2> have examined learning voxel [4, 8, 39, 33] , point cloud [7] or octree [10, 26] prediction."
Used-for(e1, e2)
Comment:

1763	"in the context of these previous approaches, the proposed <e1>approach</e1> differs primarily in three aspects: -shape representation and inference <e2>method</e2>."
Compare(e1, e2)
Comment:

1764	"using a learned prediction model allows efficient inference from a single unannotated <e1>image</e1> -learning from an <e2>image</e2> collection."
sameAs(e1, e2)
Comment:

1765	"our representation enables us to go beyond existing 3d <e1>prediction</e1> approaches by incorporating texture inference as <e2>prediction</e2> of an image in a canonical appearance space."
sameAs(e1, e2)
Comment:

1766	"an exception is <e1>texture</e1> inference on human faces [2, 22, 23, 28] , but these approaches require a large-set of 3d ground truth data with high quality <e2>texture</e2> maps."
sameAs(e1, e2)
Comment:

1767	"in zero-shot classification, at training time visual examples are provided for some visual <e1>classes</e1> but during testing the model is expected to recognize instances of <e2>classes</e2> which were not seen, with the constraint that the new classes are semantically related to the training classes."
sameAs(e1, e2)
Comment:

1768	"in zero-shot classification, at training time visual examples are provided for some visual <e1>classes</e1> but during testing the model is expected to recognize instances of classes which were not seen, with the constraint that the new <e2>classes</e2> are semantically related to the training classes."
sameAs(e1, e2)
Comment:

1769	"in zero-shot classification, at training time visual examples are provided for some visual <e1>classes</e1> but during testing the model is expected to recognize instances of classes which were not seen, with the constraint that the new classes are semantically related to the training <e2>classes</e2>."
sameAs(e1, e2)
Comment:

1770	"in zero-shot classification, at training time visual examples are provided for some visual classes but during testing the model is expected to recognize instances of <e1>classes</e1> which were not seen, with the constraint that the new <e2>classes</e2> are semantically related to the training classes."
sameAs(e1, e2)
Comment:

1771	"in zero-shot classification, at training time visual examples are provided for some visual classes but during testing the model is expected to recognize instances of <e1>classes</e1> which were not seen, with the constraint that the new classes are semantically related to the training <e2>classes</e2>."
sameAs(e1, e2)
Comment:

1772	"in zero-shot classification, at training time visual examples are provided for some visual classes but during testing the model is expected to recognize instances of classes which were not seen, with the constraint that the new <e1>classes</e1> are semantically related to the training <e2>classes</e2>."
sameAs(e1, e2)
Comment:

1773	"this problem is solved within the framework of transfer learning [13, 40] , where visual models for seen <e1>classes</e1> are transferred to the unknown <e2>classes</e2> by exploiting semantic relationships between the two."
sameAs(e1, e2)
Comment:

1774	"these unseen <e1>classes</e1> are localized by our approach that leverages semantic relationships between seen and unseen <e2>classes</e2> along with the proposed zero-shot detection framework."
sameAs(e1, e2)
Comment:

1775	"in comparison to object classification, which aims to predict the class label of an object in an <e1>image</e1>, object detection aims at predicting bounding box locations for multiple objects in an <e2>image</e2>."
sameAs(e1, e2)
Comment:

1776	"early methods [16, 17] started with an object <e1>proposal</e1> generation step and classified each object <e2>proposal</e2> as belonging to a class from a fixed set of categories."
sameAs(e1, e2)
Comment:

1777	"[45] proposed an <e1>object detection</e1> method that can detect several thousand object classes by using available (image-level) class annotations as weak supervision for <e2>object detection</e2>."
sameAs(e1, e2)
Comment:

1778	"we propose two ways to address this problem: <e1>one</e1> using a fixed background class and the <e2>other</e2> using a large open vocabulary for differentiating different background regions."
Conjunction(e1, e2)
Comment:

1779	"in order to include information from background <e1>regions</e1>, following supervised object detection, we first try to associate the background image <e2>regions</e2> into a single background class embedding."
sameAs(e1, e2)
Comment:

1780	"along with these two enhancements, we provide qualitative and quantitative results to provide insights into the success as well as failure cases of the zero-shot detection <e1>algorithms</e1>, that point us to novel directions towards solving this challenging <e2>problem</e2>."
Used-for(e1, e2)
Comment:

1781	"to summarize, the main contributions of this paper are: (i) <e1>we</e1> introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) <e2>we</e2> discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1782	"to summarize, the main contributions of this paper are: (i) <e1>we</e1> introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) <e2>we</e2> examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1783	"to summarize, the main contributions of this paper are: (i) <e1>we</e1> introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) <e2>we</e2> provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1784	"to summarize, the main contributions of this paper are: (i) we introduce the <e1>problem</e1> of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the <e2>problem</e2> with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1785	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot <e1>object detection</e1> (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised <e2>object detection</e2>; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1786	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline <e1>method</e1> for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two <e2>methods</e2> for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
Compare(e1, e2)
Comment:

1787	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline <e1>method</e1> for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed <e2>methods</e2> and provide useful insights which point to future research directions."
Compare(e1, e2)
Comment:

1788	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) <e1>we</e1> discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) <e2>we</e2> examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1789	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) <e1>we</e1> discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) <e2>we</e2> provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1790	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two <e1>methods</e1> for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed <e2>methods</e2> and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1791	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for <e1>training</e1> background-aware detectors; (iii) we examine the problem with sparse sampling of classes during <e2>training</e2> and propose a solution which densely samples training classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1792	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for <e1>training</e1> background-aware detectors; (iii) we examine the problem with sparse sampling of classes during training and propose a solution which densely samples <e2>training</e2> classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1793	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) <e1>we</e1> examine the problem with sparse sampling of classes during training and propose a solution which densely samples training classes using additional data; and (iv) <e2>we</e2> provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1794	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of <e1>classes</e1> during training and propose a solution which densely samples training <e2>classes</e2> using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1795	"to summarize, the main contributions of this paper are: (i) we introduce the problem of zero-shot object detection (zsd) in real world settings and present a baseline method for zsd that follows existing work on zero-shot image classification using multimodal semantic embeddings and fully supervised object detection; (ii) we discuss some challenges associated with incorporating information from background regions and propose two methods for training background-aware detectors; (iii) we examine the problem with sparse sampling of classes during <e1>training</e1> and propose a solution which densely samples <e2>training</e2> classes using additional data; and (iv) we provide extensive experimental and ablation studies in traditional and generalized zero-shot settings to highlight the benefits and shortcomings of the proposed methods and provide useful insights which point to future research directions."
sameAs(e1, e2)
Comment:

1796	"we also outline the challenge associated with using a limited <e1>number</e1> of training classes and propose a solution based on dense sampling of the semantic label space using auxiliary data with a large <e2>number</e2> of categories."
sameAs(e1, e2)
Comment:

1797	"we propose novel splits of two standard detection datasets -mscoco and visualgenome, and present extensive empirical <e1>results</e1> in both the traditional and generalized zero-shot settings to highlight the benefits of the proposed <e2>methods</e2>."
Compare(e1, e2)
Comment:

1798	"a vqa system takes as input an <e1>image</e1> and a free-form, open-ended, natural-language question about the <e2>image</e2> and produces a natural-language answer as the output."
sameAs(e1, e2)
Comment:

1799	"a vqa system takes as input an image and a free-form, open-ended, natural-<e1>language</e1> question about the image and produces a natural-<e2>language</e2> answer as the output."
sameAs(e1, e2)
Comment:

1800	"in this paper, we present both an open-ended answering <e1>task</e1> and a multiple-choice <e2>task</e2> [38, 27] ."
sameAs(e1, e2)
Comment:

1801	"unlike the open-answer <e1>task</e1> that requires a free-form response, the multiple-choice <e2>task</e2> only requires an algorithm to pick from a predefined list of possible answers."
sameAs(e1, e2)
Comment:

1802	"we present a large dataset <e1>that</e1> contains 204,721 images from the ms coco dataset [26] and a newly created abstract scene dataset [48, 1] <e2>that</e2> contains 50,000 scenes."
sameAs(e1, e2)
Comment:

1803	"we present a large dataset that contains 204,721 images from the ms coco dataset [26] and a newly created abstract <e1>scene</e1> dataset [48, 1] that contains 50,000 <e2>scenes</e2>."
sameAs(e1, e2)
Comment:

1804	"however, the current state of the art demonstrates that a coarse scene-level understanding of an <e1>image</e1> paired with word n-gram statistics suffices to generate reasonable <e2>image</e2> captions, which suggests image captioning may not be as "ai-complete" as desired."
sameAs(e1, e2)
Comment:

1805	"however, the current state of the art demonstrates that a coarse scene-level understanding of an <e1>image</e1> paired with word n-gram statistics suffices to generate reasonable image captions, which suggests <e2>image</e2> captioning may not be as "ai-complete" as desired."
sameAs(e1, e2)
Comment:

1806	"however, the current state of the art demonstrates that a coarse scene-level understanding of an image paired with word n-gram statistics suffices to generate reasonable <e1>image</e1> captions, which suggests <e2>image</e2> captioning may not be as "ai-complete" as desired."
sameAs(e1, e2)
Comment:

1807	"it is then unlikely <e1>that</e1> the model manifold and the true distribution's support have a nonnegligible intersection (see (arjovsky & bottou, 2017) ), and this means <e2>that</e2> the kl distance is not defined (or simply infinite)."
sameAs(e1, e2)
Comment:

1808	"this is a very high amount of <e1>noise</e1>, so much that when papers report the samples of their models, they don't add the <e2>noise</e2> term on which they report likelihood numbers."
sameAs(e1, e2)
Comment:

1809	"this is a very high amount of noise, so much that when papers report the samples of their models, <e1>they</e1> don't add the noise term on which <e2>they</e2> report likelihood numbers."
sameAs(e1, e2)
Comment:

1810	"in this new model, <e1>we</e1> show that <e2>we</e2> can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches."
sameAs(e1, e2)
Comment:

1811	"second, the ability to easily generate samples is often more useful than knowing the numerical value of the density (for example in <e1>image</e1> superresolution or semantic segmentation when considering the conditional distribution of the output <e2>image</e2> given the input image)."
sameAs(e1, e2)
Comment:

1812	"second, the ability to easily generate samples is often more useful than knowing the numerical value of the density (for example in <e1>image</e1> superresolution or semantic segmentation when considering the conditional distribution of the output image given the input <e2>image</e2>)."
sameAs(e1, e2)
Comment:

1813	"second, the ability to easily generate samples is often more useful than knowing the numerical value of the density (for example in image superresolution or semantic segmentation when considering the conditional distribution of the output <e1>image</e1> given the input <e2>image</e2>)."
sameAs(e1, e2)
Comment:

1814	"variational auto-encoders (vaes) (kingma & welling, 2013) and <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) are well known examples of this approach."
sameAs(e1, e2)
Comment:

1815	"in this paper, we direct our attention on the various <e1>ways</e1> to measure how close the model distribution and the real distribution are, or equivalently, on the various <e2>ways</e2> to define a distance or divergence ρ(p θ , p r )."
sameAs(e1, e2)
Comment:

1816	"a sequence of distributions (p t ) t∈n converges if and only if there is a distribution p ∞ such <e1>that</e1> ρ(p t , p ∞ ) tends to zero, something <e2>that</e2> depends on how exactly the distance ρ is defined."
sameAs(e1, e2)
Comment:

1817	"if ρ is our notion of <e1>distance</e1> between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the <e2>distance</e2> between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1818	"if ρ is our notion of <e1>distance</e1> between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the distance between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) <e2>distance</e2> behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1819	"if ρ is our notion of <e1>distance</e1> between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the distance between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability <e2>distances</e2> and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1820	"if ρ is our notion of distance between two distributions, <e1>we</e1> would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the distance between distributions ρ. the contributions of this paper are: • in section 2, <e2>we</e2> provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1821	"if ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is <e1>continuous</e1>, and this is equivalent to having the mapping θ → p θ be <e2>continuous</e2> when using the distance between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1822	"if ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and <e1>this</e1> is equivalent to having the mapping θ → p θ be continuous when using the distance between distributions ρ. the contributions of <e2>this</e2> paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1823	"if ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the <e1>distance</e1> between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) <e2>distance</e2> behaves in comparison to popular probability distances and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1824	"if ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the <e1>distance</e1> between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) distance behaves in comparison to popular probability <e2>distances</e2> and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1825	"if ρ is our notion of distance between two distributions, we would like to have a loss function θ → ρ(p θ , p r ) that is continuous, and this is equivalent to having the mapping θ → p θ be continuous when using the distance between distributions ρ. the contributions of this paper are: • in section 2, we provide a comprehensive theoretical analysis of how the earth mover (em) <e1>distance</e1> behaves in comparison to popular probability <e2>distances</e2> and divergences used in the context of learning distributions."
sameAs(e1, e2)
Comment:

1826	"• in section 3, <e1>we</e1> define a form of gan called wasserstein-gan that minimizes a reasonable and efficient approximation of the em distance, and <e2>we</e2> theoretically show that the corresponding optimization problem is sound."
sameAs(e1, e2)
Comment:

1827	"• in section 3, we define a form of <e1>gan</e1> called wasserstein-<e2>gan</e2> that minimizes a reasonable and efficient approximation of the em distance, and we theoretically show that the corresponding optimization problem is sound."
sameAs(e1, e2)
Comment:

1828	"• in section 3, we define a form of gan called wasserstein-gan <e1>that</e1> minimizes a reasonable and efficient approximation of the em distance, and we theoretically show <e2>that</e2> the corresponding optimization problem is sound."
sameAs(e1, e2)
Comment:

1829	"in particular, <e1>training</e1> wgans does not require maintaining a careful balance in <e2>training</e2> of the discriminator and the generator, does not require a careful design of the network architecture either, and also reduces the mode dropping that is typical in gans."
sameAs(e1, e2)
Comment:

1830	"this is often done by defining a parametric family of densities (p θ ) θ∈r d and finding the one that maximized the likelihood on our data: if <e1>we</e1> have real data examples {x (i) } m i=1 , <e2>we</e2> would solve the problem max θ∈r d 1 m m i=1 log p θ (x (i) ) if the real data distribution p r admits a density and p θ is the distribution of the parametrized density p θ , then, asymptotically, this amounts to minimizing the kullback-leibler divergence kl(p r p θ )."
sameAs(e1, e2)
Comment:

1831	"this is often done by defining a parametric family of densities (p θ ) θ∈r d and finding the one that maximized the likelihood on our data: if we have <e1>real data</e1> examples {x (i) } m i=1 , we would solve the problem max θ∈r d 1 m m i=1 log p θ (x (i) ) if the <e2>real data</e2> distribution p r admits a density and p θ is the distribution of the parametrized density p θ , then, asymptotically, this amounts to minimizing the kullback-leibler divergence kl(p r p θ )."
sameAs(e1, e2)
Comment:

1832	"[12] [12] [12] on <e1>collection</e1> [22] on <e2>collection</e2> ours on collection content (a) (b) (c) (d) (e) fig."
sameAs(e1, e2)
Comment:

1833	"[12] [12] [12] on <e1>collection</e1> [22] on collection ours on <e2>collection</e2> content (a) (b) (c) (d) (e) fig."
sameAs(e1, e2)
Comment:

1834	"[12] [12] [12] on collection [22] on <e1>collection</e1> ours on <e2>collection</e2> content (a) (b) (c) (d) (e) fig."
sameAs(e1, e2)
Comment:

1835	"(a) [12] using van gogh's "road with cypress and star" as reference style image; (b) [12] using van gogh's "starry night"; (c) [12] using the average gram matrix computed across the <e1>collection</e1> of vincent van gogh's artworks; (d) [22] trained on the <e2>collection</e2> of van gogh's artworks alternating target style images every sgd mini-batch; (e) our approach trained on the same collection of van gogh's artworks."
sameAs(e1, e2)
Comment:

1836	"(a) [12] using van gogh's "road with cypress and star" as reference style image; (b) [12] using van gogh's "starry night"; (c) [12] using the average gram matrix computed across the <e1>collection</e1> of vincent van gogh's artworks; (d) [22] trained on the collection of van gogh's artworks alternating target style images every sgd mini-batch; (e) our approach trained on the same <e2>collection</e2> of van gogh's artworks."
sameAs(e1, e2)
Comment:

1837	"(a) [12] using van gogh's "road with cypress and star" as reference style image; (b) [12] using van gogh's "starry night"; (c) [12] using the average gram matrix computed across the collection of vincent van gogh's artworks; (d) [22] trained on the <e1>collection</e1> of van gogh's artworks alternating target style images every sgd mini-batch; (e) our approach trained on the same <e2>collection</e2> of van gogh's artworks."
sameAs(e1, e2)
Comment:

1838	"in essence the stylized output <e1>image</e1> is again run through the encoder and compared with the encoded input content <e2>image</e2>."
sameAs(e1, e2)
Comment:

1839	"however, all these methods cannot make proper use of several style <e1>images</e1>, because combining the gram matrices of several <e2>images</e2> forfeits the details of style, cf."
sameAs(e1, e2)
Comment:

1840	"being able to generate high quality artistic works in high-resolution, our <e1>approach</e1> produces visually more detailed stylizations than the current state of the art style transfer <e2>approaches</e2> and yet shows real-time inference speed."
Compare(e1, e2)
Comment:

1841	"we propose a quantitative <e1>measure</e1> for evaluating the quality of a stylized image and also have art historians rank patches from our <e2>approach</e2> against those from previous work."
Used-for(e1, e2)
Comment:

1842	"this work presents a <e1>method</e1> for adapting a single, fixed deep neural network to multiple <e2>tasks</e2> without affecting performance on already learned tasks."
Used-for(e1, e2)
Comment:

1843	"this work presents a <e1>method</e1> for adapting a single, fixed deep neural network to multiple tasks without affecting performance on already learned <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

1844	"this work presents a method for adapting a single, fixed deep neural network to multiple <e1>tasks</e1> without affecting performance on already learned <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

1845	"by building upon ideas from <e1>network</e1> quantization and pruning, we learn binary masks that "piggyback" on an existing <e2>network</e2>, or are applied to unmodified weights of that network to provide good performance on a new task."
sameAs(e1, e2)
Comment:

1846	"by building upon ideas from <e1>network</e1> quantization and pruning, we learn binary masks that "piggyback" on an existing network, or are applied to unmodified weights of that <e2>network</e2> to provide good performance on a new task."
sameAs(e1, e2)
Comment:

1847	"by building upon ideas from network quantization and pruning, we learn binary masks <e1>that</e1> "piggyback" on an existing network, or are applied to unmodified weights of <e2>that</e2> network to provide good performance on a new task."
sameAs(e1, e2)
Comment:

1848	"by building upon ideas from network quantization and pruning, we learn binary masks that "piggyback" on an existing <e1>network</e1>, or are applied to unmodified weights of that <e2>network</e2> to provide good performance on a new task."
sameAs(e1, e2)
Comment:

1849	"these masks are learned in an end-toend differentiable fashion, and incur a low overhead of 1 bit <e1>per</e1> network parameter, <e2>per</e2> task."
sameAs(e1, e2)
Comment:

1850	"introduction the most popular <e1>method</e1> used in prior work for training a deep network for a new task or dataset is fine-tuning an established pre-trained <e2>model</e2>, such as the vgg-16 [1] trained on imagenet classification [2] ."
Used-for(e1, e2)
Comment:

1851	"introduction the most popular method used in prior work for training a deep network for a new <e1>task</e1> or dataset is fine-tuning an established pre-trained <e2>model</e2>, such as the vgg-16 [1] trained on imagenet classification [2] ."
Evaluate-for(e1, e2)
Comment:

1852	"a major drawback of finetuning is the phenomenon of "catastrophic forgetting" [3] , by which performance on the old <e1>task</e1> degrades significantly as the new <e2>task</e2> is learned, necessitating one to store specialized models for each task or dataset."
sameAs(e1, e2)
Comment:

1853	"a major drawback of finetuning is the phenomenon of "catastrophic forgetting" [3] , by which performance on the old <e1>task</e1> degrades significantly as the new task is learned, necessitating one to store specialized models for each <e2>task</e2> or dataset."
sameAs(e1, e2)
Comment:

1854	"a major drawback of finetuning is the phenomenon of "catastrophic forgetting" [3] , by which performance on the old task degrades significantly as the new <e1>task</e1> is learned, necessitating one to store specialized models for each <e2>task</e2> or dataset."
sameAs(e1, e2)
Comment:

1855	"however, an often overlooked fact is <e1>that</e1> the same technology <e2>that</e2> facilitates training large-scale deep neural networks can also assist in acquiring synthetic data for these neural networks [64, 69] ."
sameAs(e1, e2)
Comment:

1856	"however, an often overlooked fact is that the same technology that facilitates training large-scale <e1>deep neural networks</e1> can also assist in acquiring synthetic data for these <e2>neural networks</e2> [64, 69] ."
Feature-of(e1, e2)
Comment:

1857	"stated precisely, the problem is that: a model trained on <e1>data</e1> from one domain is often incapable of performing well on <e2>data</e2> from another domain due to distinctions in the intrinsic nature of these two domains."
sameAs(e1, e2)
Comment:

1858	"stated precisely, the problem is that: a model trained on data from one <e1>domain</e1> is often incapable of performing well on data from another <e2>domain</e2> due to distinctions in the intrinsic nature of these two domains."
sameAs(e1, e2)
Comment:

1859	"• domain adaptation via style <e1>transfer</e1> -a solution to the issue of domain bias via style <e2>transfer</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

1860	"we show <e1>that</e1> neural network-based classifiers are vulnerable to physical-world adversarial examples <e2>that</e2> remain adversarial over a different viewpoints."
sameAs(e1, e2)
Comment:

1861	"we show that <e1>neural network</e1>-based <e2>classifiers</e2> are vulnerable to physical-world adversarial examples that remain adversarial over a different viewpoints."
Used-for(e1, e2)
Comment:

1862	"we introduce a new algorithm for synthesizing adversarial examples that are robust over a chosen distribution of transformations, which we apply for reliably producing robust <e1>adversarial</e1> images as well as physical-world <e2>adversarial</e2> objects."
sameAs(e1, e2)
Comment:

1863	"figure 1 shows an example of an adversarial object constructed using our <e1>approach</e1>, where a 3d-printed turtle is consistently classified as rifle (a target class that was selected at random) by an imagenet <e2>classifier</e2>."
Used-for(e1, e2)
Comment:

1864	"we demonstrate the existence of robust 3d <e1>adversarial</e1> objects, and we present the first algorithm for synthesizing examples that are <e2>adversarial</e2> over a chosen distribution of transformations."
sameAs(e1, e2)
Comment:

1865	"recent work has demonstrated the applicability of <e1>adversarial examples</e1> in the physical world, showing that <e2>adversarial examples</e2> on a printed page remain adversarial when captured using a cell phone camera in an approximately axis-aligned setting (kurakin et al, 2016) ."
sameAs(e1, e2)
Comment:

1866	"but while minute, carefully-crafted perturbations can cause targeted misclassification in <e1>neural networks</e1>, adversarial examples produced using standard techniques fail to fool <e2>classifiers</e2> in the physical world when the examples are captured over varying viewpoints and affected by natural phenomena such as lighting and camera noise (luo et al, 2016; lu et al, 2017) ."
Used-for(e1, e2)
Comment:

1867	"instead, we propose to train an ornamentation <e1>network</e1> jointly with the glyph generation <e2>network</e2>, enabling our ornament synthesis approach to learn how to decorate automatically generated glyphs with color and texture and also fix issues that arise during glyph generation."
sameAs(e1, e2)
Comment:

1868	"instead, we propose to train an ornamentation network jointly with the glyph <e1>generation</e1> network, enabling our ornament synthesis approach to learn how to decorate automatically generated glyphs with color and texture and also fix issues that arise during glyph <e2>generation</e2>."
sameAs(e1, e2)
Comment:

1869	"instead, we propose to train an ornamentation network jointly with the glyph generation network, enabling our ornament synthesis approach to learn how to decorate automatically generated glyphs with <e1>color</e1> and <e2>texture</e2> and also fix issues that arise during glyph generation."
Conjunction(e1, e2)
Comment:

1870	"to enable this, we develop a novel stacked cgan architecture to predict the coarse glyph shapes, and a novel ornamentation network to predict <e1>color</e1> and <e2>texture</e2> of the final glyphs."
Conjunction(e1, e2)
Comment:

1871	"we use a perceptual evaluation to demonstrate the benefit of our jointly-trained <e1>network</e1> over effect transfer approaches augmented with a baseline glyph-outline inference <e2>network</e2> (section 5.3)."
sameAs(e1, e2)
Comment:

1872	"our multi-content <e1>gan</e1> (mc-<e2>gan</e2>) code and dataset are available at https://github.com/azadis/ mc-gan."
sameAs(e1, e2)
Comment:

1873	"our multi-content <e1>gan</e1> (mc-gan) code and dataset are available at https://github.com/azadis/ mc-<e2>gan</e2>."
sameAs(e1, e2)
Comment:

1874	"our multi-content gan (mc-<e1>gan</e1>) code and dataset are available at https://github.com/azadis/ mc-<e2>gan</e2>."
sameAs(e1, e2)
Comment:

1875	"instead of training a single <e1>network</e1> for all possible typeface ornamentations, we show how to use our multi-content gan architecture to retrain a customized <e2>network</e2> for each observed character set with only a handful of observed glyphs."
sameAs(e1, e2)
Comment:

1876	"our network operates in two stages, first modeling the overall glyph shape and then synthesizing the final appearance with <e1>color</e1> and <e2>texture</e2>, enabling transfer of fine decorative elements."
Conjunction(e1, e2)
Comment:

1877	"our key insight is to leverage the correlation between the <e1>parameters</e1> and represent the space of <e2>parameters</e2> by a compact set of weight vectors, called dictionary."
sameAs(e1, e2)
Comment:

1878	"in this paper, we introduce lcnn, a lookup-based convolutional neural network <e1>that</e1> encodes convolutions by few lookups to a dictionary <e2>that</e2> is trained to cover the space of weights in cnns."
sameAs(e1, e2)
Comment:

1879	"in the resnet-18, the most accurate lcnn offers 5× speedup with 62.2% <e1>accuracy</e1> and the fastest lcnn offers 29.2× speedup with 51.8% <e2>accuracy</e2> in addition, lcnn enables efficient training; almost all the work in efficient deep learning have focused on efficient inference on resource constrained platforms [35] ."
sameAs(e1, e2)
Comment:

1880	"this imposes hard constraints on the number of iterations in <e1>training</e1>.lcnn offers solutions for both of these problems in deep on-device <e2>training</e2>."
sameAs(e1, e2)
Comment:

1881	"lcnn, by virtue of having fewer <e1>parameters</e1> to learn (only around 7% of <e2>parameters</e2> of typical networks), offers a simple solution to this challenge."
sameAs(e1, e2)
Comment:

1882	"lcnn offers a solution: dictionaries in lcnn are <e1>architecture</e1> agnostic and can be transferred across <e2>architectures</e2> or layers."
sameAs(e1, e2)
Comment:

1883	"our experimental evaluations on imagenet challenge show that using lcnn we can train an 18-layer <e1>resnet</e1> with a pre-trained dictionary from a 10-layer <e2>resnet</e2> and achieve 16.2% higher top-1 accuracy on 10k iterations."
sameAs(e1, e2)
Comment:

1884	"in this paper, we 1) introduce lcnn; 2) show state of the art efficient inference in <e1>cnns</e1> using lcnn; 3) demonstrate possibilities of training deep <e2>cnns</e2> using as few as one example per category 4) show results for few iteration learning ."
sameAs(e1, e2)
Comment:

1885	"porting <e1>deep learning methods</e1> to these platforms is challenging mainly due to the gap between what these platforms can offer and what our <e2>deep learning methods</e2> require."
sameAs(e1, e2)
Comment:

1886	"porting deep learning methods to <e1>these</e1> platforms is challenging mainly due to the gap between what <e2>these</e2> platforms can offer and what our deep learning methods require."
sameAs(e1, e2)
Comment:

1887	"porting deep learning methods to these <e1>platforms</e1> is challenging mainly due to the gap between what these <e2>platforms</e2> can offer and what our deep learning methods require."
sameAs(e1, e2)
Comment:

1888	"recent work on efficient <e1>deep learning</e1> have focused on model compression and reducing the computational precision of operations in <e2>neural networks</e2> [3, 15, 35] ."
isA(e1, e2)
Comment:

1889	"if <e1>we</e1> flood this surface from its minima and prevent the merging of the waters coming from different sources, <e2>we</e2> effectively partition the image into different components (i.e., regions)."
sameAs(e1, e2)
Comment:

1890	"this transformation is typically applied to the <e1>image</e1> gradient, thus the basins correspond to homogeneous regions in the <e2>image</e2>."
sameAs(e1, e2)
Comment:

1891	"in <e1>this</e1> paper, we propose a novel approach which combines the strengths of modern deep neural networks with the power of <e2>this</e2> classical bottom-up grouping technique."
sameAs(e1, e2)
Comment:

1892	"current approaches generally use complex pipelines to handle instance extraction involving object proposals [20, 22, 7] , conditional random fields (crf) [32, 33] , large <e1>recurrent neural networks</e1> (<e2>rnn</e2>) [24, 23, 2] , or template matching [28] ."
sameAs(e1, e2)
Comment:

1893	"abstract we lay theoretical foundations for new <e1>database</e1> release mechanisms that allow third-parties to construct consistent estimators of population statistics, while ensuring that the privacy of each individual contributing to the <e2>database</e2> is protected."
sameAs(e1, e2)
Comment:

1894	"abstract we lay theoretical foundations for new database release mechanisms <e1>that</e1> allow third-parties to construct consistent estimators of population statistics, while ensuring <e2>that</e2> the privacy of each individual contributing to the database is protected."
sameAs(e1, e2)
Comment:

1895	"introduction we aim to contribute to the body of research on the trade-off between releasing <e1>datasets</e1> from which publicly beneficial statistical inferences can be drawn, and between protecting the privacy of individuals who contribute to such <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

1896	"currently the most successful formalisation of protecting user <e1>privacy</e1> is provided by differential <e2>privacy</e2> (dwork & roth, 2014) , which is a definition that any algorithm operating on a database may or may not satisfy."
sameAs(e1, e2)
Comment:

1897	"an <e1>algorithm</e1> that does satisfy the definition ensures that a particular individual does not lose too much privacy by deciding to contribute to the database on which the <e2>algorithm</e2> operates."
sameAs(e1, e2)
Comment:

1898	"an algorithm <e1>that</e1> does satisfy the definition ensures <e2>that</e2> a particular individual does not lose too much privacy by deciding to contribute to the database on which the algorithm operates."
sameAs(e1, e2)
Comment:

1899	"for multi-class image generation, the generated samples of one category also need to match the average <e1>feature</e1> of real data of that category, since the <e2>feature</e2> distance and the separability are positively correlated."
sameAs(e1, e2)
Comment:

1900	"moreover, the pixel reconstruction loss is also helpful for maintaining the <e1>structure</e1>, such as a straight line or a facial <e2>structure</e2> in an image."
sameAs(e1, e2)
Comment:

1901	"our approach estimates a good representation of the input <e1>image</e1>, and the generated <e2>image</e2> appears to be more realistic."
sameAs(e1, e2)
Comment:

1902	"in our experiments, we further show that the images synthesized from our <e1>models</e1> can be applied to other <e2>tasks</e2>, such as data augmentation for training better face recognition models."
Used-for(e1, e2)
Comment:

1903	"in our experiments, we further show that the images synthesized from our <e1>models</e1> can be applied to other tasks, such as data augmentation for training better face recognition <e2>models</e2>."
sameAs(e1, e2)
Comment:

1904	"activities are highly correlated with <e1>motion</e1> [47] , and therefore tracking the <e2>motion</e2> of specific points of visual interest is essential, yielding a distributed representation of the collection of glimpses."
sameAs(e1, e2)
Comment:

1905	"appearance and motion <e1>features</e1> need to be collected over time from local points and integrated into a sequential decision <e2>model</e2>."
Used-for(e1, e2)
Comment:

1906	"we summarize our main contributions as follows: • we present a method for human activity recognition that does not require articulated pose during testing, and models activities using two attentional processes; <e1>one</e1> extracting a set of glimpses per frame and <e2>one</e2> reasoning about entities over time."
sameAs(e1, e2)
Comment:

1907	"we avoid the use of articulated <e1>pose</e1> for two reasons: (i) depth data is not always available; for example, in applications involving smaller or otherwise resourceconstrained robots; and (ii) the question of whether articulated <e2>pose</e2> is the optimal intermediate representation for activity recognition is unclear."
sameAs(e1, e2)
Comment:

1908	"long edited <e1>video</e1> can be segmented into short scenes, using descriptive <e2>video</e2> services or with deep learning techniques [21, 3] ; however video scenes contain several shots that, although temporally consistent, have a different appearance."
sameAs(e1, e2)
Comment:

1909	"long edited <e1>video</e1> can be segmented into short scenes, using descriptive video services or with deep learning techniques [21, 3] ; however <e2>video</e2> scenes contain several shots that, although temporally consistent, have a different appearance."
sameAs(e1, e2)
Comment:

1910	"long edited video can be segmented into short <e1>scenes</e1>, using descriptive video services or with deep learning techniques [21, 3] ; however video <e2>scenes</e2> contain several shots that, although temporally consistent, have a different appearance."
sameAs(e1, e2)
Comment:

1911	"long edited video can be segmented into short scenes, using descriptive <e1>video</e1> services or with deep learning techniques [21, 3] ; however <e2>video</e2> scenes contain several shots that, although temporally consistent, have a different appearance."
sameAs(e1, e2)
Comment:

1912	"in this case we want to prevent the <e1>network</e1> from mixing the memory of the two shots; conversely, if the <e2>network</e2> could be aware of the presence of a temporal boundary, it could reset its internal status creating a new output independent to the one of the previous shot."
sameAs(e1, e2)
Comment:

1913	"in this paper, we propose a novel <e1>video</e1> encoding scheme for <e2>video</e2> captioning capable of identifying temporal discontinuities, like action or appearance changes, and exploiting them to get a better representation of the video."
sameAs(e1, e2)
Comment:

1914	"in this paper, we propose a novel <e1>video</e1> encoding scheme for video captioning capable of identifying temporal discontinuities, like action or appearance changes, and exploiting them to get a better representation of the <e2>video</e2>."
sameAs(e1, e2)
Comment:

1915	"in this paper, we propose a novel video encoding scheme for <e1>video</e1> captioning capable of identifying temporal discontinuities, like action or appearance changes, and exploiting them to get a better representation of the <e2>video</e2>."
sameAs(e1, e2)
Comment:

1916	"the awareness of the presence of an appearance or action discontinuity automatically modifies the connectivity through time of the lstm layer: the result is a variable length and adaptive encoding of the <e1>video</e1>, whose length and granularity depends on the input <e2>video</e2> itself."
sameAs(e1, e2)
Comment:

1917	"• the time boundary-aware lstm is used to build a hierarchical encoder for <e1>video</e1> captioning: to the best of our knowledge, this is the first proposal of a <e2>video</e2> captioning network which can learn to adapt its structure to input data."
sameAs(e1, e2)
Comment:

1918	"indeed, bringing together vision and language, <e1>video</e1> captioning can be leveraged for <e2>video</e2> retrieval, to enhance content search on video sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a video."
sameAs(e1, e2)
Comment:

1919	"indeed, bringing together vision and language, <e1>video</e1> captioning can be leveraged for video retrieval, to enhance content search on <e2>video</e2> sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a video."
sameAs(e1, e2)
Comment:

1920	"indeed, bringing together vision and language, <e1>video</e1> captioning can be leveraged for video retrieval, to enhance content search on video sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a <e2>video</e2>."
sameAs(e1, e2)
Comment:

1921	"indeed, bringing together vision and language, video captioning can be leveraged for <e1>video</e1> retrieval, to enhance content search on <e2>video</e2> sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a video."
sameAs(e1, e2)
Comment:

1922	"indeed, bringing together vision and language, video captioning can be leveraged for <e1>video</e1> retrieval, to enhance content search on video sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a <e2>video</e2>."
sameAs(e1, e2)
Comment:

1923	"indeed, bringing together vision and language, video captioning can be leveraged for video retrieval, to enhance content search on <e1>video</e1> sharing and streaming platforms, as well as to generate automatic subtitles and to help visually impaired people to get an insight of the content of a <e2>video</e2>."
sameAs(e1, e2)
Comment:

1924	"later, <e1>image</e1> captioning approaches have been extended to short videos with a single action, object, or scene, initially using very similar approaches to <e2>image</e2> captioning, and then with solutions to account for the temporal evolution of the video [49, 35, 50] ."
sameAs(e1, e2)
Comment:

1925	"later, image captioning <e1>approaches</e1> have been extended to short videos with a single action, object, or scene, initially using very similar <e2>approaches</e2> to image captioning, and then with solutions to account for the temporal evolution of the video [49, 35, 50] ."
sameAs(e1, e2)
Comment:

1926	"after having been applied to highly constrained or user generated videos [28, 6] ,  we propose a novel <e1>video</e1> encoding network which can adaptively modify its structure to improve <e2>video</e2> captioning."
sameAs(e1, e2)
Comment:

1927	"our time boundary-aware <e1>lstm</e1> cell (depicted with dashed rectangles) extends the standard <e2>lstm</e2> unit by adding a trainable boundary detector (bd), which can alter the temporal connections of the network depending on the input video."
sameAs(e1, e2)
Comment:

1928	"to overcome this challenge, additional synthetic <e1>training examples</e1> along with limited real <e2>training examples</e2> can be utilised to train the model."
sameAs(e1, e2)
Comment:

1929	"use additional synthetic <e1>images</e1> generated from <e2>3d models</e2> to train deep networks."
Used-for(e1, e2)
Comment:

1930	"previous adversarial style-transfer methods either supervise their <e1>networks</e1> with a large volume of paired data or train highly under-constrained two-way generative <e2>networks</e2> in an unsupervised fashion."
sameAs(e1, e2)
Comment:

1931	"we formulate this <e1>problem</e1> as a domain adaptation <e2>problem</e2> i.e."
sameAs(e1, e2)
Comment:

1932	"aligning the 3dmm rendered <e1>face</e1> domain into realistic <e2>face</e2> domain."
sameAs(e1, e2)
Comment:

1933	"aligning the 3dmm rendered face <e1>domain</e1> into realistic face <e2>domain</e2>."
sameAs(e1, e2)
Comment:

1934	"cycle-<e1>gan</e1> [71] , another recent <e2>method</e2> and closest to our work, proposes a two-way gan framework for unsupervised image-to-image translation."
Part-of(e1, e2)
Comment:

1935	"cycle-<e1>gan</e1> [71] , another recent method and closest to our work, proposes a two-way <e2>gan</e2> framework for unsupervised image-to-image translation."
sameAs(e1, e2)
Comment:

1936	"cycle-gan [71] , another recent method and closest to our work, proposes a two-way gan framework for unsupervised <e1>image</e1>-to-<e2>image</e2> translation."
sameAs(e1, e2)
Comment:

1937	"although synthetic input images, 3dmm rendered <e1>faces</e1>, contain distinct <e2>face</e2> identities, the distinction between them vanishes as a result of the inherent non-linear transformations induced by the discriminator to encourage realism."
sameAs(e1, e2)
Comment:

1938	"unlike existing works, which are focused on generating new <e1>images</e1> of existing identities, we are interested in generating multiple <e2>images</e2> of new identities."
sameAs(e1, e2)
Comment:

1939	"in this way identity preservation becomes adaptive to the changing feature space during the <e1>training</e1> of the generator network unlike softmax layer that converges very quickly at the beginning of the <e2>training</e2> before meaningful images are generated."
sameAs(e1, e2)
Comment:

1940	"the resulting synthetic <e1>face images</e1> are visually plausible and can be used to boost <e2>face recognition</e2> as additional training data or any other graphical purposes."
isA(e1, e2)
Comment:

1941	"we combine face images generated by the proposed <e1>method</e1> with a real data set to train face recognition algorithms and evaluate the <e2>model</e2> quantitatively on two challenging data sets: lfw and ijb-a."
Used-for(e1, e2)
Comment:

1942	"abstract color constancy is the problem of inferring the <e1>color</e1> of the light that illuminated a scene, usually so that the illumination <e2>color</e2> can be removed."
sameAs(e1, e2)
Comment:

1943	"abstract color constancy is the problem of inferring the color of the light <e1>that</e1> illuminated a scene, usually so <e2>that</e2> the illumination color can be removed."
sameAs(e1, e2)
Comment:

1944	"color constancy is a well studied in both <e1>vision</e1> science and computer <e2>vision</e2>, as it relates to the academic study of human perception as well as practical problems such as designing an object recognition algorithm or a camera."
sameAs(e1, e2)
Comment:

1945	"the simplest such algorihm is "gray world", which assumes <e1>that</e1> the illuminant color is the average color of all image pixels, thereby implicitly assuming <e2>that</e2> object reflectances are, on average, gray [12] ."
sameAs(e1, e2)
Comment:

1946	"the simplest such algorihm is "gray world", which assumes that the illuminant <e1>color</e1> is the average <e2>color</e2> of all image pixels, thereby implicitly assuming that object reflectances are, on average, gray [12] ."
sameAs(e1, e2)
Comment:

1947	"this simple idea can be generalized to <e1>modeling</e1> gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , <e2>modeling</e2> the spatial distribution of colors with a filter bank [13] , modeling the distribution of color histograms [21] , or implicitly reasoning about the moments of colors using pca [14] ."
sameAs(e1, e2)
Comment:

1948	"this simple idea can be generalized to <e1>modeling</e1> gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , modeling the spatial distribution of colors with a filter bank [13] , <e2>modeling</e2> the distribution of color histograms [21] , or implicitly reasoning about the moments of colors using pca [14] ."
sameAs(e1, e2)
Comment:

1949	"this simple idea can be generalized to modeling gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , <e1>modeling</e1> the spatial distribution of colors with a filter bank [13] , <e2>modeling</e2> the distribution of color histograms [21] , or implicitly reasoning about the moments of colors using pca [14] ."
sameAs(e1, e2)
Comment:

1950	"this simple idea can be generalized to modeling gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , modeling the spatial distribution of <e1>colors</e1> with a filter bank [13] , modeling the distribution of <e2>color</e2> histograms [21] , or implicitly reasoning about the moments of colors using pca [14] ."
sameAs(e1, e2)
Comment:

1951	"this simple idea can be generalized to modeling gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , modeling the spatial distribution of <e1>colors</e1> with a filter bank [13] , modeling the distribution of color histograms [21] , or implicitly reasoning about the moments of <e2>colors</e2> using pca [14] ."
sameAs(e1, e2)
Comment:

1952	"this simple idea can be generalized to modeling gradient information or using generalized norms instead of a simple arithmetic mean [4, 36] , modeling the spatial distribution of colors with a filter bank [13] , modeling the distribution of <e1>color</e1> histograms [21] , or implicitly reasoning about the moments of <e2>colors</e2> using pca [14] ."
sameAs(e1, e2)
Comment:

1953	"most of these models can be thought of as statistical, as <e1>they</e1> either assume some distribution of colors or <e2>they</e2> learn some distribution of colors from training data."
sameAs(e1, e2)
Comment:

1954	"most of these models can be thought of as statistical, as they either assume some distribution of <e1>colors</e1> or they learn some distribution of <e2>colors</e2> from training data."
sameAs(e1, e2)
Comment:

1955	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here <e1>we</e1> demonstrate the color constancy problem: the input image i (taken from [24, 33] ) looks green, and <e2>we</e2> want to recover a white-balanced image w and illumination l which reproduces i. below we have our model's solution and error for this image compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1956	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here <e1>we</e1> demonstrate the color constancy problem: the input image i (taken from [24, 33] ) looks green, and we want to recover a white-balanced image w and illumination l which reproduces i. below <e2>we</e2> have our model's solution and error for this image compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1957	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here we demonstrate the color constancy problem: the input <e1>image</e1> i (taken from [24, 33] ) looks green, and we want to recover a white-balanced <e2>image</e2> w and illumination l which reproduces i. below we have our model's solution and error for this image compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1958	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here we demonstrate the color constancy problem: the input <e1>image</e1> i (taken from [24, 33] ) looks green, and we want to recover a white-balanced image w and illumination l which reproduces i. below we have our model's solution and error for this <e2>image</e2> compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1959	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here we demonstrate the color constancy problem: the input image i (taken from [24, 33] ) looks green, and <e1>we</e1> want to recover a white-balanced image w and illumination l which reproduces i. below <e2>we</e2> have our model's solution and error for this image compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1960	"this connection to learning and statistics is sometimes made more explicit, often in a i = w × l ourŵ ,l, err = 0.13°baselineŵ ,l, err = 5.34°f igure 1: here we demonstrate the color constancy problem: the input image i (taken from [24, 33] ) looks green, and we want to recover a white-balanced <e1>image</e1> w and illumination l which reproduces i. below we have our model's solution and error for this <e2>image</e2> compared to a state of the art baseline [19] (recovered illuminations are rendered with respect to ground-truth, so white is correct)."
sameAs(e1, e2)
Comment:

1961	"one thing <e1>that</e1> these statistical or learning-based models have in common is <e2>that</e2> they are all generative models of natural colors -a model is learned from (or assumed of) white-balanced images, and then that model is used to white-balance new images."
sameAs(e1, e2)
Comment:

1962	"one thing <e1>that</e1> these statistical or learning-based models have in common is that they are all generative models of natural colors -a model is learned from (or assumed of) white-balanced images, and then <e2>that</e2> model is used to white-balance new images."
sameAs(e1, e2)
Comment:

1963	"one thing that these statistical or learning-based models have in common is <e1>that</e1> they are all generative models of natural colors -a model is learned from (or assumed of) white-balanced images, and then <e2>that</e2> model is used to white-balance new images."
sameAs(e1, e2)
Comment:

1964	"one thing that these statistical or learning-based models have in common is that they are all generative models of natural colors -a <e1>model</e1> is learned from (or assumed of) white-balanced images, and then that <e2>model</e2> is used to white-balance new images."
sameAs(e1, e2)
Comment:

1965	"one thing that these statistical or learning-based models have in common is that they are all generative models of natural colors -a model is learned from (or assumed of) white-balanced <e1>images</e1>, and then that model is used to white-balance new <e2>images</e2>."
sameAs(e1, e2)
Comment:

1966	"that is, instead of training a generative model to assign high likelihoods to white-balanced <e1>images</e1> under the assumption that such a model will perform well at white-balancing, we will explicitly train a model to distinguish between white-balanced <e2>images</e2> and non-whitebalanced images."
sameAs(e1, e2)
Comment:

1967	"that is, instead of training a generative model to assign high likelihoods to white-balanced <e1>images</e1> under the assumption that such a model will perform well at white-balancing, we will explicitly train a model to distinguish between white-balanced images and non-whitebalanced <e2>images</e2>."
sameAs(e1, e2)
Comment:

1968	"that is, instead of training a generative model to assign high likelihoods to white-balanced images under the assumption that such a <e1>model</e1> will perform well at white-balancing, we will explicitly train a <e2>model</e2> to distinguish between white-balanced images and non-whitebalanced images."
sameAs(e1, e2)
Comment:

1969	"that is, instead of training a generative model to assign high likelihoods to white-balanced images under the assumption that such a model will perform well at white-balancing, we will explicitly train a model to distinguish between white-balanced <e1>images</e1> and non-whitebalanced <e2>images</e2>."
sameAs(e1, e2)
Comment:

1970	"this use of discriminative machine learning appears to be largely unexplored in context of <e1>color constancy</e1>, though similar tools have been used to augment generative <e2>color constancy</e2> models with face detection [9] or scene classification [25] information."
sameAs(e1, e2)
Comment:

1971	"let us contrast the study of color constancy <e1>algorithms</e1> with the seemingly disparate <e2>problem</e2> of object detection."
Used-for(e1, e2)
Comment:

1972	"object detection has seen a tremendous amount of growth and success in the last 20 years owing in large part to standardized challenges [16] and effective machine learning techniques, with <e1>techniques</e1> evolving from simple sliding window classifiers [15, 32, 37] to sophisticated deformable models [17] or segmentation-based <e2>techniques</e2> [28] ."
sameAs(e1, e2)
Comment:

1973	"at first glance, it may seem that all of this has little to teach us about <e1>color constancy</e1>, as most established <e2>color constancy</e2> algorithms are fundamentally incompatible with the discriminative learning techniques used in object detection."
sameAs(e1, e2)
Comment:

1974	"in contrast, in this paper we reformulate the <e1>problem</e1> of color constancy as a 2d spatial localization task in a logchrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the color constancy <e2>problem</e2>."
sameAs(e1, e2)
Comment:

1975	"in contrast, in this paper we reformulate the problem of <e1>color constancy</e1> as a 2d spatial localization task in a logchrominance space, thereby allowing us to apply techniques from object detection and structured prediction to the <e2>color constancy</e2> problem."
sameAs(e1, e2)
Comment:

1976	"but if the <e1>color constancy</e1> problem could be reduced to the problem of localizing a template in some n-dimensional space, then presumably the lessons learned from object detection techniques could be used to design an effective <e2>color constancy</e2> algorithm."
sameAs(e1, e2)
Comment:

1977	"but if the color constancy <e1>problem</e1> could be reduced to the <e2>problem</e2> of localizing a template in some n-dimensional space, then presumably the lessons learned from object detection techniques could be used to design an effective color constancy algorithm."
sameAs(e1, e2)
Comment:

1978	"in this paper we present ccc ("convolutional <e1>color constancy</e1>"), a novel <e2>color constancy</e2> algorithm that has been designed under the assumption that color constancy is a discriminative learning task."
sameAs(e1, e2)
Comment:

1979	"in this paper we present ccc ("convolutional <e1>color constancy</e1>"), a novel color constancy algorithm that has been designed under the assumption that <e2>color constancy</e2> is a discriminative learning task."
sameAs(e1, e2)
Comment:

1980	"in this paper we present ccc ("convolutional color constancy"), a novel <e1>color constancy</e1> algorithm that has been designed under the assumption that <e2>color constancy</e2> is a discriminative learning task."
sameAs(e1, e2)
Comment:

1981	"in this paper we present ccc ("convolutional color constancy"), a novel color constancy algorithm <e1>that</e1> has been designed under the assumption <e2>that</e2> color constancy is a discriminative learning task."
sameAs(e1, e2)
Comment:

1982	"our algorithm is based around the observation <e1>that</e1> scaling the color channels of an image induces a translation in the log-chromaticity histogram of <e2>that</e2> image."
sameAs(e1, e2)
Comment:

1983	"our algorithm is based around the observation that scaling the color channels of an <e1>image</e1> induces a translation in the log-chromaticity histogram of that <e2>image</e2>."
sameAs(e1, e2)
Comment:

1984	"this observation allows us to frame the color constancy <e1>problem</e1> as a discriminative learning <e2>problem</e2>, using tools similar to convolutional neural networks [31] and structured prediction [34] ."
sameAs(e1, e2)
Comment:

1985	"effectively, we are able to reframe the <e1>problem</e1> of color constancy as the <e2>problem</e2> of localizing a template in some two-dimensional space, thereby allowing us to borrow techniques from the well-understood problem of object detection."
sameAs(e1, e2)
Comment:

1986	"effectively, we are able to reframe the <e1>problem</e1> of color constancy as the problem of localizing a template in some two-dimensional space, thereby allowing us to borrow techniques from the well-understood <e2>problem</e2> of object detection."
sameAs(e1, e2)
Comment:

1987	"effectively, we are able to reframe the problem of color constancy as the <e1>problem</e1> of localizing a template in some two-dimensional space, thereby allowing us to borrow techniques from the well-understood <e2>problem</e2> of object detection."
sameAs(e1, e2)
Comment:

1988	"by directly learning how to discriminate between correctly white-balanced <e1>images</e1> and poorly white-balanced <e2>images</e2>, our model is able to improve performance on standard benchmarks by nearly 40%."
sameAs(e1, e2)
Comment:

1989	"in section 6 <e1>we</e1> will evaluate our model on two different color constancy tasks, and in section 7 <e2>we</e2> will conclude."
sameAs(e1, e2)
Comment:

1990	"intro the <e1>color</e1> of a pixel in an image can be described as a product of two quantities: reflectance (the <e2>color</e2> of the paint of the surfaces in the scene) and illumination (the color of the light striking the surfaces in the scene)."
sameAs(e1, e2)
Comment:

1991	"intro the <e1>color</e1> of a pixel in an image can be described as a product of two quantities: reflectance (the color of the paint of the surfaces in the scene) and illumination (the <e2>color</e2> of the light striking the surfaces in the scene)."
sameAs(e1, e2)
Comment:

1992	"intro the color of a pixel in an image can be described as a product of two quantities: reflectance (the <e1>color</e1> of the paint of the surfaces in the scene) and illumination (the <e2>color</e2> of the light striking the surfaces in the scene)."
sameAs(e1, e2)
Comment:

1993	"intro the color of a pixel in an image can be described as a product of two quantities: reflectance (the color of the paint of the <e1>surfaces</e1> in the scene) and illumination (the color of the light striking the <e2>surfaces</e2> in the scene)."
sameAs(e1, e2)
Comment:

1994	"intro the color of a pixel in an image can be described as a product of two quantities: reflectance (the color of the paint of the surfaces in the <e1>scene</e1>) and illumination (the color of the light striking the surfaces in the <e2>scene</e2>)."
sameAs(e1, e2)
Comment:

1995	"when a person stands in a room lit by a colorful light <e1>they</e1> unconsciously "discount the illuminant", in the words of helmholtz [27] , and perceive the objects in the room as though <e2>they</e2> were illuminated by a neutral, white light."
sameAs(e1, e2)
Comment:

1996	"endowing a computer with the same ability is difficult, as this problem is fundamentally underconstrained -given a yellow pixel, how can one discern if it is a white object under a yellow <e1>illuminant</e1>, or a yellow object under a white <e2>illuminant</e2>?"
sameAs(e1, e2)
Comment:

1997	"the most general characterization of this <e1>problem</e1> is the "intrinsic image" <e2>problem</e2> [6] , but the specific problem of inferring and correcting the color of the illumination of an image is commonly referred to as "color constancy" or "white balance"."
sameAs(e1, e2)
Comment:

1998	"the most general characterization of this <e1>problem</e1> is the "intrinsic image" problem [6] , but the specific <e2>problem</e2> of inferring and correcting the color of the illumination of an image is commonly referred to as "color constancy" or "white balance"."
sameAs(e1, e2)
Comment:

1999	"the most general characterization of this problem is the "intrinsic <e1>image</e1>" problem [6] , but the specific problem of inferring and correcting the color of the illumination of an <e2>image</e2> is commonly referred to as "color constancy" or "white balance"."
sameAs(e1, e2)
Comment:

2000	"the most general characterization of this problem is the "intrinsic image" <e1>problem</e1> [6] , but the specific <e2>problem</e2> of inferring and correcting the color of the illumination of an image is commonly referred to as "color constancy" or "white balance"."
sameAs(e1, e2)
Comment:

2001	" introduction in this paper<e1> w</e1>e address the problem of visual attribution, which<e2> w</e2>e define as detecting and visualising evidence of a particular category in an image."
sameAs(e1, e2)
Comment:

2002	"it was recently shown that during training neural networks minimise the mutual information between <e1>input</e1> and output layers, thereby compressing the <e2>input</e2> features [50] ."
sameAs(e1, e2)
Comment:

2003	"these findings suggest that a classifier may ignore <e1>features</e1> with low discriminative power if stronger <e2>features</e2> with redundant information about the target are available."
sameAs(e1, e2)
Comment:

2004	"as a consequence, if there is evidence for a class at multiple <e1>locations</e1> in the image (such as multiple lesions in medical images) some <e2>locations</e2> may not influence the classification result and may thus not be detected."
sameAs(e1, e2)
Comment:

2005	"as a consequence, if there is evidence for a class at multiple locations in the <e1>image</e1> (such as multiple lesions in medical images) some locations may not influence the <e2>classification</e2> result and may thus not be detected."
Used-for(e1, e2)
Comment:

2006	"in contrast to the majority of recent techniques, the <e1>method</e1> does not rely on a classifier but rather aims at finding a map that, when added to an input image of one category, will make it indistinguishable from images from a <e2>baseline</e2> category."
Compare(e1, e2)
Comment:

2007	"we note <e1>that</e1> our method does not tackle the classification problem but rather assumes <e2>that</e2> the category labels of the test images have already been determined (e.g."
sameAs(e1, e2)
Comment:

2008	"furthermore, the <e1>method</e1> requires a <e2>baseline</e2> category, which is not the case for many benchmark recognition datasets in vision, but is in fact the case for many practical detection applications, especially in medical image analysis."
Compare(e1, e2)
Comment:

2009	"furthermore, the <e1>method</e1> requires a baseline category, which is not the case for many benchmark recognition datasets in vision, but is in fact the case for many practical detection <e2>applications</e2>, especially in medical image analysis."
Used-for(e1, e2)
Comment:

2010	"we demonstrate the method on synthetic 2d <e1>data</e1> and on large 3d brain mr <e2>data</e2>, where we aim to predict subjectspecific disease effect maps for alzheimer's disease (ad)."
sameAs(e1, e2)
Comment:

2011	"currently, the most frequently used approach to address the visual attribution problem is training a neural network classifier to predict the categories of a set of images and then following <e1>one</e1> of two <e2>strategies</e2>: analysing the gradients of * data used in preparation of this article were obtained from the alzheimers disease neuroimaging initiative (adni) database (adni.loni.usc.edu)."
isA(e1, e2)
Comment:

2012	"given a test <e1>image</e1>, this function will generate an <e2>image</e2>-specific visual attribution map which highlights the features unique to that category."
sameAs(e1, e2)
Comment:

2013	"the <e1>prediction</e1> with respect to an input image [27, 5, 54] or analysing the activations of the feature maps for the image [65, 41, 43] to determine which part of the image was responsible for making the associated <e2>prediction</e2>."
sameAs(e1, e2)
Comment:

2014	"the prediction with respect to an input <e1>image</e1> [27, 5, 54] or analysing the activations of the feature maps for the <e2>image</e2> [65, 41, 43] to determine which part of the image was responsible for making the associated prediction."
sameAs(e1, e2)
Comment:

2015	"the prediction with respect to an input <e1>image</e1> [27, 5, 54] or analysing the activations of the feature maps for the image [65, 41, 43] to determine which part of the <e2>image</e2> was responsible for making the associated prediction."
sameAs(e1, e2)
Comment:

2016	"the prediction with respect to an input image [27, 5, 54] or analysing the activations of the feature maps for the <e1>image</e1> [65, 41, 43] to determine which part of the <e2>image</e2> was responsible for making the associated prediction."
sameAs(e1, e2)
Comment:

2017	"abstract introduction visual similarities lie at the heart of a large number of computer vision tasks ranging from low-level <e1>image processing</e1> to high-level understanding of <e2>human poses</e2> or object classification."
Used-for(e1, e2)
Comment:

2018	"however, this context is typically highly local (i.e position of patches in the same <e1>image</e1> [5] , object tracks through short number of frames [33] or <e2>image</e2> impainting [22] ), establishing relations between tuples [5] or triplets [20, 38, 33] of images."
sameAs(e1, e2)
Comment:

2019	"hence, these approaches utilize loss functions <e1>that</e1> order a positive i p and a negative i n image with respect to an anchor image i a so <e2>that</e2>, d(i a , i p ) < d(i a , i n )."
sameAs(e1, e2)
Comment:

2020	"hence, these approaches utilize loss functions that order a positive i p and a negative i n <e1>image</e1> with respect to an anchor <e2>image</e2> i a so that, d(i a , i p ) < d(i a , i n )."
sameAs(e1, e2)
Comment:

2021	"(iii) explicitly optimize similarities in a given <e1>representation</e1> space, instead of using the <e2>representation</e2> space indirectly learnt by intermediate layers of a cnn trained for classification."
sameAs(e1, e2)
Comment:

2022	"however, to achieve <e1>this</e1> performance boost, these cnn architectures require millions of samples of supervised train- * both authors contributed equally to <e2>this</e2> work."
sameAs(e1, e2)
Comment:

2023	"abstract the jaccard index, also referred to as the intersectionover-union score, is commonly introduction we consider the task of semantic image segmentation, where each pixel i of a given <e1>image</e1> has to be classified into an object class c ∈ c. most of the deep network based segmentation methods rely on logistic regression, optimizing the cross-entropy loss [10] loss(f ) = − 1 p p i=1 log f i (y * i ), (1) with p the number of pixels in the <e2>image</e2> or minibatch considered, y * i ∈ c the ground truth class of pixel i, f i (y * i ) the network probability estimate of the ground truth probability of pixel i, and f a vector of all network outputs f i (c)."
sameAs(e1, e2)
Comment:

2024	"abstract the jaccard index, also referred to as the intersectionover-union score, is commonly introduction we consider the task of semantic image segmentation, where each pixel i of a given image has to be classified into an object class c ∈ c. most of the deep <e1>network</e1> based segmentation methods rely on logistic regression, optimizing the cross-entropy loss [10] loss(f ) = − 1 p p i=1 log f i (y * i ), (1) with p the number of pixels in the image or minibatch considered, y * i ∈ c the ground truth class of pixel i, f i (y * i ) the <e2>network</e2> probability estimate of the ground truth probability of pixel i, and f a vector of all network outputs f i (c)."
sameAs(e1, e2)
Comment:

2025	"abstract the jaccard index, also referred to as the intersectionover-union score, is commonly introduction we consider the task of semantic image segmentation, where each pixel i of a given image has to be classified into an object class c ∈ c. most of the deep <e1>network</e1> based segmentation methods rely on logistic regression, optimizing the cross-entropy loss [10] loss(f ) = − 1 p p i=1 log f i (y * i ), (1) with p the number of pixels in the image or minibatch considered, y * i ∈ c the ground truth class of pixel i, f i (y * i ) the network probability estimate of the ground truth probability of pixel i, and f a vector of all <e2>network</e2> outputs f i (c)."
sameAs(e1, e2)
Comment:

2026	"abstract the jaccard index, also referred to as the intersectionover-union score, is commonly introduction we consider the task of semantic image segmentation, where each pixel i of a given image has to be classified into an object class c ∈ c. most of the deep network based segmentation methods rely on logistic regression, optimizing the cross-entropy loss [10] loss(f ) = − 1 p p i=1 log f i (y * i ), (1) with p the number of pixels in the image or minibatch considered, y * i ∈ c the ground truth class of pixel i, f i (y * i ) the <e1>network</e1> probability estimate of the ground truth probability of pixel i, and f a vector of all <e2>network</e2> outputs f i (c)."
sameAs(e1, e2)
Comment:

2027	"[4, 12] use iou and related overlap measures to define <e1>training</e1> sets for binary classifiers in a complex multi-stage <e2>training</e2>."
sameAs(e1, e2)
Comment:

2028	"swapping the loss to <e1>this</e1> should be possible...." however, <e2>this</e2> is left to future work."
sameAs(e1, e2)
Comment:

2029	"about pose <e1>tracking</e1>, although there has not been much work [2] , the <e2>system</e2> complexity can be expected to further increase due to the increased problem dimension and solution space."
Evaluate-for(e1, e2)
Comment:

2030	"it is probably the simplest way to estimate heat <e1>maps</e1> from deep and low resolution feature <e2>maps</e2>."
sameAs(e1, e2)
Comment:

2031	"our single <e1>model</e1>'s best result achieves the state-of-the-art at map of 73.7 on coco testdev split, which has an improvement of 1.6% and 0.7% over the winner of coco 2017 keypoint challenge's single <e2>model</e2> and their ensembled model [6, 9] ."
sameAs(e1, e2)
Comment:

2032	"our single <e1>model</e1>'s best result achieves the state-of-the-art at map of 73.7 on coco testdev split, which has an improvement of 1.6% and 0.7% over the winner of coco 2017 keypoint challenge's single model and their ensembled <e2>model</e2> [6, 9] ."
sameAs(e1, e2)
Comment:

2033	"our single model's best result achieves the state-of-the-art at map of 73.7 on coco testdev split, which has an improvement of 1.6% and 0.7% over the winner of coco 2017 keypoint challenge's single <e1>model</e1> and their ensembled <e2>model</e2> [6, 9] ."
sameAs(e1, e2)
Comment:

2034	"at the same time, the overall <e1>algorithm</e1> and system complexity increases as well, making the <e2>algorithm</e2> analysis and comparison more difficult."
sameAs(e1, e2)
Comment:

2035	"more recently, the <e1>image</e1>-based virtual try-on system [10] without resorting to 3d information, provides a more economical solution and shows promising results by reformulating it as a conditional <e2>image</e2> generation problem."
sameAs(e1, e2)
Comment:

2036	"more recently, the image-based virtual try-on system [10] without resorting to 3d information, provides a more economical <e1>solution</e1> and shows promising results by reformulating it as a conditional image generation <e2>problem</e2>."
Used-for(e1, e2)
Comment:

2037	"given two images, <e1>one</e1> of a person and the <e2>other</e2> of an in-shop clothes, such pipeline aims to synthesize a new image that meets the following requirements: a) the person is dressed in the new clothes; b) the original body shape and pose are retained; c) the clothing product with high-fidelity is warped smoothly and seamlessly connected with other parts; d) the characteristics of clothing product, such as texture, logo and text, are well preserved, without any noticeable artifacts and distortions."
Conjunction(e1, e2)
Comment:

2038	"given two images, <e1>one</e1> of a person and the other of an in-shop clothes, such pipeline aims to synthesize a new image that meets the following requirements: a) the person is dressed in the new clothes; b) the original body shape and pose are retained; c) the clothing product with high-fidelity is warped smoothly and seamlessly connected with <e2>other</e2> parts; d) the characteristics of clothing product, such as texture, logo and text, are well preserved, without any noticeable artifacts and distortions."
Conjunction(e1, e2)
Comment:

2039	"given two images, one of a person and the <e1>other</e1> of an in-shop clothes, such pipeline aims to synthesize a new image that meets the following requirements: a) the person is dressed in the new clothes; b) the original body shape and pose are retained; c) the clothing product with high-fidelity is warped smoothly and seamlessly connected with <e2>other</e2> parts; d) the characteristics of clothing product, such as texture, logo and text, are well preserved, without any noticeable artifacts and distortions."
sameAs(e1, e2)
Comment:

2040	"we argue that the main reason lies in the imperfect <e1>shape</e1>-context matching for aligning clothes and body <e2>shape</e2>, and the inferior appearance merging strategy."
sameAs(e1, e2)
Comment:

2041	"to address the aforementioned challenges, we present a new <e1>image</e1>-based method that successfully achieves the plausible try-on <e2>image</e2> syntheses while preserving cloth characteristics, such as texture, logo, text and so on, named as characteristic-preserving image-based virtual try-on network (cp-vton)."
sameAs(e1, e2)
Comment:

2042	"to address the aforementioned challenges, we present a new <e1>image</e1>-based method that successfully achieves the plausible try-on image syntheses while preserving cloth characteristics, such as texture, logo, text and so on, named as characteristic-preserving <e2>image</e2>-based virtual try-on network (cp-vton)."
sameAs(e1, e2)
Comment:

2043	"to address the aforementioned challenges, we present a new image-based method that successfully achieves the plausible try-on <e1>image</e1> syntheses while preserving cloth characteristics, such as texture, logo, text and so on, named as characteristic-preserving <e2>image</e2>-based virtual try-on network (cp-vton)."
sameAs(e1, e2)
Comment:

2044	"second, our model takes the aligned clothes and clothing-agnostic yet descriptive person representation proposed in [10] as inputs, and generates a pose-coherent <e1>image</e1> and a composition mask which indicates the details of aligned clothes kept in the synthesized <e2>image</e2>."
sameAs(e1, e2)
Comment:

2045	"-different from the hand-crafted shape context <e1>matching</e1>, our cp-vton incorporates a full learnable thin-plate spline transformation via a new geometric <e2>matching</e2> module to obtain more robust and powerful alignment."
sameAs(e1, e2)
Comment:

2046	"-given aligned images, a new try-on module is performed to dynamically merge rendered <e1>results</e1> and warped <e2>results</e2>."
sameAs(e1, e2)
Comment:

2047	"however, previous <e1>image</e1>-conditioned generation works fail to meet these critical requirements towards the plausible virtual try-on performance since they fail to handle large spatial misalignment between the input <e2>image</e2> and target clothes."
sameAs(e1, e2)
Comment:

2048	"in <e1>this</e1> work, we propose a new fully-learnable characteristic-preserving virtual try-on network (cp-vton) for addressing all real-world challenges in <e2>this</e2> task."
sameAs(e1, e2)
Comment:

2049	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) have achieved remarkable results in the task of generating realistic natural images."
sameAs(e1, e2)
Comment:

2050	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) are a powerful framework to learn models capable of generating natural images."
sameAs(e1, e2)
Comment:

2051	"this transition is especially notable in computer vision, where <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) significantly advanced the state of the art over the classic prescribed approaches like mixtures of gaussians (blanken et al, 2007) ."
sameAs(e1, e2)
Comment:

2052	"we pose the problem of <e1>graph</e1> generation as learning the distribution of biased random walks over the input <e2>graph</e2>."
sameAs(e1, e2)
Comment:

2053	"additionally, any model operating on a <e1>graph</e1> necessarily has to be permutation invariant, as <e2>graphs</e2> are isomorphic under node reordering."
sameAs(e1, e2)
Comment:

2054	"we formulate the problem of learning the <e1>graph</e1> topology as learning the distribution of biased random walks over the <e2>graph</e2>."
sameAs(e1, e2)
Comment:

2055	"like in the typical gan setting, the generator g -in our case defined as a stochastic neural network with discrete output samples -learns to generate random walks <e1>that</e1> are plausible in the real graph, while the discriminator d then has to distinguish them from the true ones <e2>that</e2> are sampled from the original graph."
sameAs(e1, e2)
Comment:

2056	"like in the typical gan setting, the generator g -in our case defined as a stochastic neural network with discrete output samples -learns to generate random walks that are plausible in the real <e1>graph</e1>, while the discriminator d then has to distinguish them from the true ones that are sampled from the original <e2>graph</e2>."
sameAs(e1, e2)
Comment:

2057	"the main requirement for a <e1>graph</e1> generative model is the ability to generate realistic <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

2058	"we observe that our proposed <e1>method</e1> consistently reproduces most known patterns inherent to real-world networks without explicitly specifying any of them in the <e2>model</e2> definition (e.g., degree distribution, as seen in fig."
Used-for(e1, e2)
Comment:

2059	"recently, several neural network based generative models such as variational autoencoders (vaes) (kingma & welling, 2013) and <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) have found success at modeling data distributions."
sameAs(e1, e2)
Comment:

2060	"while <e1>training</e1>, this mapping is encouraged to produce vectors that resemble the vectors in the <e2>training</e2> dataset."
sameAs(e1, e2)
Comment:

2061	"introduction compressive or compressed sensing is the problem of reconstructing an unknown vector x ⇤ 2 r n after observing m < n linear measurements of its entries, possibly with added <e1>noise</e1>: y = ax ⇤ + ⌘, where a 2 r m⇥n is called the measurement matrix and ⌘ 2 r m is <e2>noise</e2>."
sameAs(e1, e2)
Comment:

2062	"specifically, given a <e1>proposal</e1>, while the probability for each class label naturally acts as an "classification confidence" of the <e2>proposal</e2>, the bounding box regression module finds the optimal transformation for the proposal to best fit the groundtruth."
sameAs(e1, e2)
Comment:

2063	"specifically, given a <e1>proposal</e1>, while the probability for each class label naturally acts as an "classification confidence" of the proposal, the bounding box regression module finds the optimal transformation for the <e2>proposal</e2> to best fit the groundtruth."
sameAs(e1, e2)
Comment:

2064	"specifically, given a proposal, while the probability for each class label naturally acts as an "classification confidence" of the <e1>proposal</e1>, the bounding box regression module finds the optimal transformation for the <e2>proposal</e2> to best fit the groundtruth."
sameAs(e1, e2)
Comment:

2065	"in <e1>this</e1> paper, we take a step towards answering <e2>this</e2> question."
sameAs(e1, e2)
Comment:

2066	"we propose to represent <e1>maps</e1> as a dnn, called mapnet, which learns the <e2>map</e2> representation directly from input data, with the flexibility to fuse multiple sensory inputs and to improve over time using unlabeled data."
sameAs(e1, e2)
Comment:

2067	"we are inspired by both the recent dnnbased camera localization work (e.g., posenet [32] and its variants [11, 31, 40, 55] ) in the context of structure-frommotion, as well as the traditional map <e1>optimization</e1> methods (e.g., bundle adjustment (ba) [22, 23, 39] , pose graph <e2>optimization</e2> (pgo) [8, 17, 37] ) in the context of visual slam."
sameAs(e1, e2)
Comment:

2068	"these constraints can come from a variety of sources: pose constraint from the visual odometry (vo) between pairs of images, translation constraint from <e1>two</e1> gps readings, rotation constraint from <e2>two</e2> imu readings, etc."
sameAs(e1, e2)
Comment:

2069	"pure dnn-based <e1>methods</e1> (e.g., posenet [32, 30, 31] ) result in noisy estimations, and the traditional vo-based <e2>methods</e2> (e.g., stereo vo or dso [18] ) often drift significantly over time."
sameAs(e1, e2)
Comment:

2070	"as shown, prior dnn-based <e1>methods</e1> (e.g., posenet [32, 30, 31] ) result in noisy estimations, while traditional visual odometry based <e2>methods</e2> (e.g., stereo vo or dso [18] ) often drift over time."
sameAs(e1, e2)
Comment:

2071	"examples include 3d landmarks for general visual slam <e1>methods</e1> [34, 35, 42] , 3d lines and patches in semi-dense slam <e2>methods</e2> and indoor scenes [20, 43, 56] , object-level context in semantic slam methods [10, 45] , bag of visual word features on key frames for camera relocalization [12, 46, 47] ."
sameAs(e1, e2)
Comment:

2072	"examples include 3d landmarks for general visual slam <e1>methods</e1> [34, 35, 42] , 3d lines and patches in semi-dense slam methods and indoor scenes [20, 43, 56] , object-level context in semantic slam <e2>methods</e2> [10, 45] , bag of visual word features on key frames for camera relocalization [12, 46, 47] ."
sameAs(e1, e2)
Comment:

2073	"examples include 3d landmarks for general visual slam methods [34, 35, 42] , 3d lines and patches in semi-dense slam <e1>methods</e1> and indoor scenes [20, 43, 56] , object-level context in semantic slam <e2>methods</e2> [10, 45] , bag of visual word features on key frames for camera relocalization [12, 46, 47] ."
sameAs(e1, e2)
Comment:

2074	"each conditional embedding can <e1>learn</e1> a representation specific to a our cite model separates phrases into different groups and <e2>learns</e2> conditional embeddings for these groups in a single end-to-end model."
sameAs(e1, e2)
Comment:

2075	"each conditional embedding can learn a representation specific to a our cite <e1>model</e1> separates phrases into different groups and learns conditional embeddings for these groups in a single end-to-end <e2>model</e2>."
sameAs(e1, e2)
Comment:

2076	"similarly colored <e1>blocks</e1> refer to layers of the same type, with purple <e2>blocks</e2> representing fully connected layers."
sameAs(e1, e2)
Comment:

2077	"similarly colored blocks refer to <e1>layers</e1> of the same type, with purple blocks representing fully connected <e2>layers</e2>."
sameAs(e1, e2)
Comment:

2078	"best viewed in color subset of <e1>phrases</e1> while also taking advantage of weights that are shared across <e2>phrases</e2>."
sameAs(e1, e2)
Comment:

2079	"in order to differentiate text <e1>phrases</e1> into semantically distinct subspaces, we propose a concept weight branch that automatically assigns <e2>phrases</e2> to embeddings, whereas prior works predefine such assignments."
sameAs(e1, e2)
Comment:

2080	"[3] also took into account the predictions made by other <e1>phrases</e1> when localizing <e2>phrases</e2> and incorporated bounding box regression to improve their region proposals."
sameAs(e1, e2)
Comment:

2081	"the contributions of our paper are summarized below: -by conditioning the embedding used by our <e1>model</e1> on the input phrase we simplify the representation requirements for each embedding, leading to a more generalizable <e2>model</e2>."
sameAs(e1, e2)
Comment:

2082	"-<e1>we</e1> perform extensive experiments over three datasets, flickr30k entities [25] , referit game [15] , and visual genome [18] , where <e2>we</e2> report a (resp.)"
sameAs(e1, e2)
Comment:

2083	"this constituent task has applications to <e1>image</e1> captioning [6, 12, 14, 19, 34] , <e2>image</e2> retrieval [9, 26] , and visual question answering [1, 29, 7] ."
sameAs(e1, e2)
Comment:

2084	"recent work has leveraged temporal action proposals [29, 2, 9] for efficient <e1>action</e1> detection, where the proposals identify relevant temporal windows that are then independently classified by an <e2>action</e2> classifier in a second stage."
sameAs(e1, e2)
Comment:

2085	"alternatively, one can adopt an architecture that runs a sliding window at a fixed temporal <e1>scale</e1> but outputs proposals at varied temporal <e2>scales</e2> [9] ."
sameAs(e1, e2)
Comment:

2086	"this approach still needs to perform <e1>computations</e1> on overlapping temporal windows, which results in possibly redundant <e2>computations</e2> as each frame is processed more than once."
sameAs(e1, e2)
Comment:

2087	"(ii) we demonstrate that our new temporal <e1>proposal</e1> generation architecture achieves state-of-the-art performance on the <e2>proposal</e2> generation task."
sameAs(e1, e2)
Comment:

2088	"(ii) we demonstrate that our new temporal proposal <e1>generation</e1> architecture achieves state-of-the-art performance on the proposal <e2>generation</e2> task."
sameAs(e1, e2)
Comment:

2089	"in practice, applications (such as smart surveillance, robotics or autonomous driving) require <e1>cameras</e1> to record <e2>video streams</e2> continuously and vision algorithms to perform temporal action detection in such long streams."
Used-for(e1, e2)
Comment:

2090	"deep textspotter: an end-to-end trainable <e1>scene</e1> <e2>text</e2> localization and recognition framework"
Conjunction(e1, e2)
Comment:

2091	"abstract a method for <e1>scene</e1> <e2>text</e2> localization and recognition is proposed."
Conjunction(e1, e2)
Comment:

2092	"the novelties include: training of both <e1>text</e1> detection and recognition in a single end-to-end pass, the structure of the recognition cnn and the geometry of its input layer that preserves the aspect of the <e2>text</e2> and adapts its resolution to the data."
sameAs(e1, e2)
Comment:

2093	"the proposed <e1>method</e1> achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets -icdar 2013 and icdar 2015, whilst being an order of magnitude faster than competing <e2>methods</e2> -the whole pipeline runs at 10 frames per second on an nvidia k80 gpu."
Compare(e1, e2)
Comment:

2094	"abstract this paper tackles the task of semi-supervised <e1>video</e1> object segmentation, i.e., the separation of an object from the background in a <e2>video</e2>, given the mask of the first frame."
sameAs(e1, e2)
Comment:

2095	"we present <e1>one</e1>-shot video object segmentation (osvos), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on imagenet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence <e2>one</e2>-shot)."
sameAs(e1, e2)
Comment:

2096	"we present one-shot video object <e1>segmentation</e1> (osvos), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on imagenet, to the task of foreground <e2>segmentation</e2>, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot)."
sameAs(e1, e2)
Comment:

2097	"most of the current techniques focus on finding the optimal <e1>architecture</e1> in a designated search space starting from scratch while training each designed <e2>architecture</e2> on the real data (from random initialization) to get a validation performance to guide exploration."
sameAs(e1, e2)
Comment:

2098	"though such methods have shown the ability to discover network structures <e1>that</e1> outperform human-designed architectures when vast computational resources are used, such as  <e2>that</e2> employed 500 p100 gpus across 4 days, they are also likely to fail to beat best human-designed architectures real et al, 2017; liu et al, 2018) , especially when the computational resources are restricted."
sameAs(e1, e2)
Comment:

2099	"though such methods have shown the ability to discover network structures that outperform human-designed <e1>architectures</e1> when vast computational resources are used, such as  that employed 500 p100 gpus across 4 days, they are also likely to fail to beat best human-designed <e2>architectures</e2> real et al, 2017; liu et al, 2018) , especially when the computational resources are restricted."
sameAs(e1, e2)
Comment:

2100	"though such methods have shown the ability to discover network structures that outperform human-designed architectures when vast computational <e1>resources</e1> are used, such as  that employed 500 p100 gpus across 4 days, they are also likely to fail to beat best human-designed architectures real et al, 2017; liu et al, 2018) , especially when the computational <e2>resources</e2> are restricted."
sameAs(e1, e2)
Comment:

2101	"furthermore, insufficient training epochs during the <e1>architecture</e1> search process (much fewer epochs than normal to save time) may cause models to underperform (baker et al, 2017) , which would harm the efficiency of the <e2>architecture</e2> search process."
sameAs(e1, e2)
Comment:

2102	"furthermore, insufficient training epochs during the architecture <e1>search</e1> process (much fewer epochs than normal to save time) may cause models to underperform (baker et al, 2017) , which would harm the efficiency of the architecture <e2>search</e2> process."
sameAs(e1, e2)
Comment:

2103	"alternatively, some efforts have been made to explore the architecture space by <e1>network</e1> transformation, starting from an existing <e2>network</e2> trained on the target task and reusing its weights."
sameAs(e1, e2)
Comment:

2104	"for example, cai et al (2018) utilized net2net (chen et al, 2016) <e1>operations</e1>, a class of function-preserving transformation <e2>operations</e2>, to further find high-performance architectures based on a given network, while ashok et al (2018) used network compression operations to compress well-trained networks."
sameAs(e1, e2)
Comment:

2105	"for example, cai et al (2018) utilized net2net (chen et al, 2016) <e1>operations</e1>, a class of function-preserving transformation operations, to further find high-performance architectures based on a given network, while ashok et al (2018) used network compression <e2>operations</e2> to compress well-trained networks."
sameAs(e1, e2)
Comment:

2106	"for example, cai et al (2018) utilized net2net (chen et al, 2016) operations, a class of function-preserving transformation <e1>operations</e1>, to further find high-performance architectures based on a given network, while ashok et al (2018) used network compression <e2>operations</e2> to compress well-trained networks."
sameAs(e1, e2)
Comment:

2107	"for example, cai et al (2018) utilized net2net (chen et al, 2016) operations, a class of function-preserving transformation operations, to further find high-performance architectures based on a given <e1>network</e1>, while ashok et al (2018) used <e2>network</e2> compression operations to compress well-trained networks."
sameAs(e1, e2)
Comment:

2108	"they would always lead to chain-<e1>structured</e1> networks when given a chain-<e2>structured</e2> start point."
sameAs(e1, e2)
Comment:

2109	"in this paper, we present a new kind of transformation <e1>operations</e1> for neural networks, phrased as path-level network transformation <e2>operations</e2>, which allows modifying the path topologies in a given network while allowing weight reusing to preserve the functionality like net2net operations (chen et al, 2016) ."
sameAs(e1, e2)
Comment:

2110	"in this paper, we present a new kind of transformation <e1>operations</e1> for neural networks, phrased as path-level network transformation operations, which allows modifying the path topologies in a given network while allowing weight reusing to preserve the functionality like net2net <e2>operations</e2> (chen et al, 2016) ."
sameAs(e1, e2)
Comment:

2111	"in this paper, we present a new kind of transformation operations for neural networks, phrased as path-level <e1>network</e1> transformation operations, which allows modifying the path topologies in a given <e2>network</e2> while allowing weight reusing to preserve the functionality like net2net operations (chen et al, 2016) ."
sameAs(e1, e2)
Comment:

2112	"in this paper, we present a new kind of transformation operations for neural networks, phrased as path-level network transformation <e1>operations</e1>, which allows modifying the path topologies in a given network while allowing weight reusing to preserve the functionality like net2net <e2>operations</e2> (chen et al, 2016) ."
sameAs(e1, e2)
Comment:

2113	"to efficiently explore the introduced <e1>tree</e1>-structured architecture space, we further propose a bidirectional <e2>tree</e2>-structured (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input tree, instead of simply using the chain-structured recurrent neural network ."
sameAs(e1, e2)
Comment:

2114	"to efficiently explore the introduced <e1>tree</e1>-structured architecture space, we further propose a bidirectional tree-structured (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input <e2>tree</e2>, instead of simply using the chain-structured recurrent neural network ."
sameAs(e1, e2)
Comment:

2115	"to efficiently explore the introduced tree-<e1>structured</e1> architecture space, we further propose a bidirectional tree-<e2>structured</e2> (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input tree, instead of simply using the chain-structured recurrent neural network ."
sameAs(e1, e2)
Comment:

2116	"to efficiently explore the introduced tree-<e1>structured</e1> architecture space, we further propose a bidirectional tree-structured (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input tree, instead of simply using the chain-<e2>structured</e2> recurrent neural network ."
sameAs(e1, e2)
Comment:

2117	"to efficiently explore the introduced tree-structured architecture space, we further propose a bidirectional <e1>tree</e1>-structured (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input <e2>tree</e2>, instead of simply using the chain-structured recurrent neural network ."
sameAs(e1, e2)
Comment:

2118	"to efficiently explore the introduced tree-structured architecture space, we further propose a bidirectional tree-<e1>structured</e1> (tai et al, 2015) reinforcement learning meta-controller that can naturally encode the input tree, instead of simply using the chain-<e2>structured</e2> recurrent neural network ."
sameAs(e1, e2)
Comment:

2119	"specifically, without any additional regularization techniques, it achieves 3.14% test error with 5.7m <e1>parameters</e1>, while densnets give a best test error rate of 3.46% with 25.6m <e2>parameters</e2> and pyramidnets give 3.31% with 26.0m parameters."
sameAs(e1, e2)
Comment:

2120	"specifically, without any additional regularization techniques, it achieves 3.14% test error with 5.7m <e1>parameters</e1>, while densnets give a best test error rate of 3.46% with 25.6m parameters and pyramidnets give 3.31% with 26.0m <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

2121	"specifically, without any additional regularization techniques, it achieves 3.14% test error with 5.7m parameters, while densnets give a best test error rate of 3.46% with 25.6m <e1>parameters</e1> and pyramidnets give 3.31% with 26.0m <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

2122	"and with additional regularization techniques (droppath  and cutout (devries & taylor, 2017)), it reaches 2.30% test error with 14.3m <e1>parameters</e1>, surpassing 2.40% given by nasnet-a  with 27.6m <e2>parameters</e2> and a similar training scheme."
sameAs(e1, e2)
Comment:

2123	"we further propose a bidirectional treestructured reinforcement learning meta-controller to explore a simple yet highly expressive treestructured <e1>architecture</e1> space that can be viewed as a generalization of multi-branch <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

2124	"we experimented on the image classification datasets with limited computational resources (about 200 gpu-hours), where we observed improved parameter efficiency and better test results (97.70% test <e1>accuracy</e1> on cifar-10 with 14.3m parameters and 74.6% top-1 <e2>accuracy</e2> on imagenet in the mobile setting), demonstrating the effectiveness and transferability of our designed architectures."
sameAs(e1, e2)
Comment:

2125	"abstract sign <e1>language</e1> recognition (slr) introduction sign languages are the primary <e2>language</e2> of the deaf community."
sameAs(e1, e2)
Comment:

2126	"most of the research <e1>that</e1> has been conducted in slr to date has approached the task as a basic gesture recognition problem, ignoring the linguistic properties of the sign language and assuming <e2>that</e2> there is a one-to-one mapping of sign to spoken words."
sameAs(e1, e2)
Comment:

2127	"most of the research that has been conducted in slr to date has approached the task as a basic gesture recognition problem, ignoring the linguistic properties of the sign language and assuming that there is a <e1>one</e1>-to-<e2>one</e2> mapping of sign to spoken words."
sameAs(e1, e2)
Comment:

2128	"we use state-of-the-art sequence-to-sequence (seq2seq) based deep learning methods to learn: the spatio-temporal representation of the signs, the relation between <e1>these</e1> signs (in other words the language model) and how <e2>these</e2> signs map to the spoken or written language."
sameAs(e1, e2)
Comment:

2129	"abstract <e1>we</e1> propose a novel deep learning approach to solve simultaneous alignment and recognition problems (referred  to as "<e2>sequence-to-sequence</e2>" learning introduction perception is a hierarchical process; our understanding of the world as a whole, is based on recognising different parts of the world and understanding their spatio-temporal interactions."
Compare(e1, e2)
Comment:

2130	"abstract we propose a novel deep learning approach to solve simultaneous alignment and recognition problems (referred  to as "sequence-to-sequence" learning introduction perception is a hierarchical process; our <e1>understanding</e1> of the world as a whole, is based on recognising different parts of the world and <e2>understanding</e2> their spatio-temporal interactions."
sameAs(e1, e2)
Comment:

2131	"in this paper, <e1>we</e1> present subunets 1 , a novel deep learning architecture for <e2>sequence-to-sequence</e2> learning tasks, where the systems are expected to produce a sequence of outputs from a given video."
Compare(e1, e2)
Comment:

2132	"contrary to other video to text approaches, our method explicitly models the contextual subunits of the <e1>task</e1> while training the network for the main <e2>task</e2>."
sameAs(e1, e2)
Comment:

2133	"the rest of the paper is organized as follows: in section 2 <e1>we</e1> go over the related work on <e2>sequence-to-sequence</e2> modelling, and continuous sign language recognition."
Compare(e1, e2)
Comment:

2134	"then <e1>we</e1> describe our application of subunets to the challenge of continuous sign language recognition in section 5. here <e2>we</e2> demonstrate how subunets can be combined to model the asynchronous relationship between different channels of information and that combining different loss layers allows expert knowledge to be incorporated which increases recognition performance."
sameAs(e1, e2)
Comment:

2135	"more generally, most spatio-temporal learning <e1>problems</e1> can be broken down into meaningful "subunit" <e2>problems</e2>."
sameAs(e1, e2)
Comment:

2136	"abstract <e1>generative adversarial networks</e1> (<e2>gans</e2>) aim to generate realistic data from some prior distribution (e.g., gaussian noises)."
sameAs(e1, e2)
Comment:

2137	"in this paper, relying on the manifold assumption on <e1>images</e1> (tenenbaum et al, 2000; roweis & saul, 2000) , we propose a novel generative model using local coordinate coding (lcc) (yu et al, 2009 ) to improve gans in generating perceptually convincing <e2>images</e2>."
sameAs(e1, e2)
Comment:

2138	"in this paper, rather than <e1>sampling</e1> from the pre-defined prior distribution, we propose a local coordinate coding (lcc) based <e2>sampling</e2> method to improve gans."
sameAs(e1, e2)
Comment:

2139	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) have been successfully applied to many tasks, such as video prediction (ranzato et al, 2014; mathieu et al, 2016) , image translation (isola et al, 2017; kim et al, 2017) , etc."
sameAs(e1, e2)
Comment:

2140	"specifically, gans learn to generate <e1>data</e1> by playing a two-player game: a generator tries to produce samples from a simple latent distribution, and a discriminator distinguishes between the generated <e2>data</e2> and real data."
sameAs(e1, e2)
Comment:

2141	" introduction in the big<e1> dat</e1>a era, large-scale and high-dimensional media<e2> dat</e2>a has been pervasive in search engines and social networks."
sameAs(e1, e2)
Comment:

2142	"in particular, it proves crucial to jointly learn similarity-preserving <e1>representations</e1> and control quantization error of converting continuous <e2>representations</e2> to binary codes [42, 22, 24, 6] ."
sameAs(e1, e2)
Comment:

2143	"this paper presents hashgan, a novel deep architecture for deep learning to hash, which learns compact binary hash codes from both real <e1>images</e1> and large-scale synthesized <e2>images</e2>."
sameAs(e1, e2)
Comment:

2144	"well-specified loss functions including cosine cross-entropy loss and cosine <e1>quantization</e1> loss are proposed for similarity-preserving learning and <e2>quantization</e2> error control."
sameAs(e1, e2)
Comment:

2145	"parallel to the traditional indexing <e1>methods</e1> [21] , another advantageous solution is hashing <e2>methods</e2> [37] , which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items."
sameAs(e1, e2)
Comment:

2146	"parallel to the traditional indexing methods [21] , another advantageous solution is hashing methods [37] , which transform high-dimensional media <e1>data</e1> into compact binary codes and generate similar binary codes for similar <e2>data</e2> items."
sameAs(e1, e2)
Comment:

2147	"parallel to the traditional indexing methods [21] , another advantageous solution is hashing methods [37] , which transform high-dimensional media data into compact binary <e1>codes</e1> and generate similar binary <e2>codes</e2> for similar data items."
sameAs(e1, e2)
Comment:

2148	"this paper will focus on the learning to hash <e1>methods</e1> [37] that build data-dependent hash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing <e2>methods</e2>, e.g."
sameAs(e1, e2)
Comment:

2149	"this paper will focus on the learning to hash methods [37] that build <e1>data</e1>-dependent hash encoding schemes for efficient image retrieval, which have shown better performance than <e2>data</e2>-independent hashing methods, e.g."
sameAs(e1, e2)
Comment:

2150	" introduction in the big<e1> dat</e1>a era, large-scale and high-dimensional media<e2> dat</e2>a has been pervasive in search engines and social networks."
sameAs(e1, e2)
Comment:

2151	"in particular, it proves crucial to jointly learn similarity-preserving <e1>representations</e1> and control quantization error of binarizing continuous <e2>representations</e2> to binary codes [44, 22, 43, 24] ."
sameAs(e1, e2)
Comment:

2152	"solving the <e1>discrete</e1> optimization of hash codes with <e2>continuous</e2> optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their optimization procedure."
Conjunction(e1, e2)
Comment:

2153	"solving the discrete <e1>optimization</e1> of hash codes with continuous <e2>optimization</e2>, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their optimization procedure."
sameAs(e1, e2)
Comment:

2154	"solving the discrete <e1>optimization</e1> of hash codes with continuous optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their <e2>optimization</e2> procedure."
sameAs(e1, e2)
Comment:

2155	"solving the discrete optimization of hash <e1>codes</e1> with continuous optimization, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash <e2>codes</e2> in their optimization procedure."
sameAs(e1, e2)
Comment:

2156	"solving the discrete optimization of hash codes with continuous <e1>optimization</e1>, all these methods essentially solve an optimization problem that deviates significantly from the hashing objective as they cannot learn exactly binary hash codes in their <e2>optimization</e2> procedure."
sameAs(e1, e2)
Comment:

2157	"first, converting deep representations, which are continuous in nature, to exactly binary hash <e1>codes</e1>, we need to adopt the sign function h = sgn (z) as activation function when generating binary hash <e2>codes</e2> using similarity-preserving learning in deep neural networks."
sameAs(e1, e2)
Comment:

2158	"first, converting deep representations, which are continuous in nature, to exactly binary hash codes, we need to adopt the sign <e1>function</e1> h = sgn (z) as activation <e2>function</e2> when generating binary hash codes using similarity-preserving learning in deep neural networks."
sameAs(e1, e2)
Comment:

2159	"second, the similarity information is usually very sparse in real retrieval systems, i.e., the <e1>number</e1> of similar pairs is much smaller than the <e2>number</e2> of dissimilar pairs."
sameAs(e1, e2)
Comment:

2160	"specifically, we attack the ill-posed gradient <e1>problem</e1> in the non-convex optimization of the deep networks with non-smooth sign activation by the continuation methods [1] , which address a complex optimization problem by smoothing the original function, turning it into a different <e2>problem</e2> that is easier to optimize."
sameAs(e1, e2)
Comment:

2161	"by gradually reducing the amount of smoothing during the training, it results in a sequence of <e1>optimization problems</e1> converging to the original <e2>optimization problem</e2>."
sameAs(e1, e2)
Comment:

2162	"parallel to the traditional indexing <e1>methods</e1> [21] , another advantageous solution is hashing <e2>methods</e2> [38] , which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items."
sameAs(e1, e2)
Comment:

2163	"parallel to the traditional indexing methods [21] , another advantageous solution is hashing methods [38] , which transform high-dimensional media <e1>data</e1> into compact binary codes and generate similar binary codes for similar <e2>data</e2> items."
sameAs(e1, e2)
Comment:

2164	"parallel to the traditional indexing methods [21] , another advantageous solution is hashing methods [38] , which transform high-dimensional media data into compact binary <e1>codes</e1> and generate similar binary <e2>codes</e2> for similar data items."
sameAs(e1, e2)
Comment:

2165	"thus, a natural ambition is to directly transfer both the representation and classification models from large-<e1>scale</e1> dataset to our target dataset, such as caltech-256, which are usually small-<e2>scale</e2> and with unknown categories at training and testing time."
sameAs(e1, e2)
Comment:

2166	"from big data viewpoint, we can assume that the large-<e1>scale</e1> dataset is diverse enough to subsume all categories of the small-<e2>scale</e2> dataset."
sameAs(e1, e2)
Comment:

2167	"as shown in figure 1 , partial <e1>transfer learning</e1> problem is more general and challenging than standard <e2>transfer learning</e2>, since outlier source classes ("sofa") will result in negative transfer when discriminating the target classes ("soccer-ball" and "binoculars")."
sameAs(e1, e2)
Comment:

2168	"as shown in figure 1 , partial transfer learning problem is more general and challenging than standard transfer learning, since outlier source <e1>classes</e1> ("sofa") will result in negative transfer when discriminating the target <e2>classes</e2> ("soccer-ball" and "binoculars")."
sameAs(e1, e2)
Comment:

2169	"negative <e1>transfer</e1> is the phenomenon that a <e2>transfer</e2> learner performs even worse than a supervised classifier trained solely on the source domain, which is the key challenge of transfer learning [23] ."
sameAs(e1, e2)
Comment:

2170	"this paper presents selective <e1>adversarial</e1> networks (san), which largely extends the ability of deep <e2>adversarial</e2> adaptation [7] to address partial transfer learning from large- scale domains to small-scale domains."
sameAs(e1, e2)
Comment:

2171	"this paper presents selective adversarial networks (san), which largely extends the ability of deep adversarial adaptation [7] to address partial transfer learning from large- <e1>scale</e1> domains to small-<e2>scale</e2> domains."
sameAs(e1, e2)
Comment:

2172	"this paper presents selective adversarial networks (san), which largely extends the ability of deep adversarial adaptation [7] to address partial transfer learning from large- scale <e1>domains</e1> to small-scale <e2>domains</e2>."
sameAs(e1, e2)
Comment:

2173	"san aligns the distributions of source and target <e1>data</e1> in the shared label space and more importantly, selects out the source <e2>data</e2> in the outlier source classes."
sameAs(e1, e2)
Comment:

2174	"a key improvement over previous methods is the capability to simultaneously promote positive <e1>transfer</e1> of relevant data and alleviate negative <e2>transfer</e2> of irrelevant data, which can be trained in an end-to-end framework."
sameAs(e1, e2)
Comment:

2175	"a key improvement over previous methods is the capability to simultaneously promote positive transfer of relevant <e1>data</e1> and alleviate negative transfer of irrelevant <e2>data</e2>, which can be trained in an end-to-end framework."
sameAs(e1, e2)
Comment:

2176	"since manual labeling of sufficient training <e1>data</e1> for diverse application domains on-the-fly is often prohibitive, for problems short of labeled <e2>data</e2>, there is strong motivation to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled data from a different but related source domain."
sameAs(e1, e2)
Comment:

2177	"since manual labeling of sufficient training <e1>data</e1> for diverse application domains on-the-fly is often prohibitive, for problems short of labeled data, there is strong motivation to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled <e2>data</e2> from a different but related source domain."
sameAs(e1, e2)
Comment:

2178	"since manual labeling of sufficient training data for diverse application domains on-the-fly is often prohibitive, for problems short of labeled <e1>data</e1>, there is strong motivation to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled <e2>data</e2> from a different but related source domain."
sameAs(e1, e2)
Comment:

2179	"poses a major obstacle in adapting classification <e1>models</e1> to target <e2>tasks</e2> [23] ."
Used-for(e1, e2)
Comment:

2180	"yet, bottom-up approaches do not directly use global contextual cues from <e1>other</e1> body parts and <e2>other</e2> people."
sameAs(e1, e2)
Comment:

2181	"however, solving the integer linear programming <e1>problem</e1> over a fully connected graph is an np-hard <e2>problem</e2> and the average processing time is on the order of hours."
sameAs(e1, e2)
Comment:

2182	"insafutdinov et al [11] built on [22] with stronger part detectors based on resnet [10] and <e1>image</e1>-dependent pairwise scores, and vastly improved the runtime, but the method still takes several minutes per <e2>image</e2>, with a limit on the number of part proposals."
sameAs(e1, e2)
Comment:

2183	"we present the first bottom-up representation of association scores via part affinity <e1>fields</e1> (pafs), a set of 2d vector <e2>fields</e2> that encode the location and orientation of limbs over the image domain."
sameAs(e1, e2)
Comment:

2184	"conditioning on preceding word <e1>nodes</e1>, our acmn alternatively mines visual evidence for <e2>nodes</e2> with modifier relations via an adversarial attention module and integrates features of child nodes of nodes with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2185	"conditioning on preceding word <e1>nodes</e1>, our acmn alternatively mines visual evidence for nodes with modifier relations via an adversarial attention module and integrates features of child <e2>nodes</e2> of nodes with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2186	"conditioning on preceding word <e1>nodes</e1>, our acmn alternatively mines visual evidence for nodes with modifier relations via an adversarial attention module and integrates features of child nodes of <e2>nodes</e2> with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2187	"conditioning on preceding word nodes, our acmn alternatively mines visual evidence for <e1>nodes</e1> with modifier relations via an adversarial attention module and integrates features of child <e2>nodes</e2> of nodes with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2188	"conditioning on preceding word nodes, our acmn alternatively mines visual evidence for <e1>nodes</e1> with modifier relations via an adversarial attention module and integrates features of child nodes of <e2>nodes</e2> with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2189	"conditioning on preceding word nodes, our acmn alternatively mines visual evidence for nodes with modifier relations via an adversarial attention <e1>module</e1> and integrates features of child nodes of nodes with clausal predicate relation via a residual composition <e2>module</e2>."
sameAs(e1, e2)
Comment:

2190	"conditioning on preceding word nodes, our acmn alternatively mines visual evidence for nodes with modifier relations via an adversarial attention module and integrates features of child <e1>nodes</e1> of <e2>nodes</e2> with clausal predicate relation via a residual composition module."
sameAs(e1, e2)
Comment:

2191	"our <e1>method</e1> is designed to facilitation <e2>annotation</e2>, and easily incorporates user corrections of points to improve the overall object's polygon."
Used-for(e1, e2)
Comment:

2192	"our <e1>method</e1> cuts down the number of required <e2>annotation</e2> clicks by a factor of 4.74. proaches hold promise, their performance is not yet competitive with fully supervised approaches."
Used-for(e1, e2)
Comment:

2193	"in this paper, <e1>we</e1> propose an interactive segmentation method that produces highly accurate and structurally coherent object annotations, and reduces annotation time by a factor of 4.7. given a ground-truth bounding box, our method generates a polygon outlining the object instance using a recurrent neural network, which <e2>we</e2> call polygon-rnn."
sameAs(e1, e2)
Comment:

2194	"in this paper, we propose an interactive segmentation <e1>method</e1> that produces highly accurate and structurally coherent object annotations, and reduces <e2>annotation</e2> time by a factor of 4.7. given a ground-truth bounding box, our method generates a polygon outlining the object instance using a recurrent neural network, which we call polygon-rnn."
Used-for(e1, e2)
Comment:

2195	"in this paper, we propose an interactive segmentation method that produces highly accurate and structurally coherent object annotations, and reduces annotation time by a factor of 4.7. given a ground-truth bounding box, our method generates a polygon outlining the object instance using a <e1>recurrent neural network</e1>, which we call polygon-<e2>rnn</e2>."
sameAs(e1, e2)
Comment:

2196	"we show that our <e1>annotation</e1> approach speeds up <e2>annotation</e2> process by factor of 4.7, while achieving 78.4% agreement with original groundtruth, matching the typical agreement of human annotators."
sameAs(e1, e2)
Comment:

2197	"most of the recent approaches are based on neural networks, achieving impressive performance for <e1>these</e1> <e2>tasks</e2> [5, 17, 10, 21] ."
Used-for(e1, e2)
Comment:

2198	"deep learning approaches are, however, <e1>data</e1> hungry and their performance is strongly correlated with the amount of available training <e2>data</e2>."
sameAs(e1, e2)
Comment:

2199	"our goal in <e1>this</e1> paper is to make <e2>this</e2> process faster, while yielding ground-truth as precise as the one available in the current datasets."
sameAs(e1, e2)
Comment:

2200	"sketchyscene contains more than 29,000 <e1>scene</e1>-level sketches, 7,000+ pairs of <e2>scene</e2> templates and photos, and 11,000+ object sketches."
sameAs(e1, e2)
Comment:

2201	"nevertheless, <e1>these</e1> datasets have all been formed by object sketches and the sketch analysis and processing <e2>tasks</e2> have mostly been at the stroke or object level."
Used-for(e1, e2)
Comment:

2202	"while <e1>scene</e1> understanding is one of the hallmark tasks of computer vision, the problem of understanding <e2>scene</e2> sketches have not been well studied."
sameAs(e1, e2)
Comment:

2203	"while scene <e1>understanding</e1> is one of the hallmark tasks of computer vision, the problem of <e2>understanding</e2> scene sketches have not been well studied."
sameAs(e1, e2)
Comment:

2204	"in this paper, <e1>we</e1> introduce the first large-scale dataset of scene sketches, which <e2>we</e2> refer to as sketchyscene, to facilitate sketch understanding and processing at both the object and scene level."
sameAs(e1, e2)
Comment:

2205	"in this paper, we introduce the first large-scale dataset of <e1>scene</e1> sketches, which we refer to as sketchyscene, to facilitate sketch understanding and processing at both the object and <e2>scene</e2> level."
sameAs(e1, e2)
Comment:

2206	"there are four people, a basket, <e1>two</e1> apples, one cup,<e2>two</e2> bananas, and on a picnic rug."
sameAs(e1, e2)
Comment:

2207	"instead of asking the users to draw entire <e1>scene</e1> sketches from scratch, which can be tedious and intimidating, we provide object sketches so that the <e2>scene</e2> sketches can be created via simple interactive operations such as drag-n-dropping and scaling the object sketches."
sameAs(e1, e2)
Comment:

2208	"(a) reference <e1>image</e1>; (b) response of edge detector; (c) synthesized scene using object sketches from sketchy and tu-berlin (using the same pipeline as (f)); (d) nonartist's drawing with the hint of a short description; (e) artist's drawing with the hint of reference <e2>image</e2>; (f) synthesized scene using our system."
sameAs(e1, e2)
Comment:

2209	"(a) reference image; (b) response of edge detector; (c) synthesized <e1>scene</e1> using object sketches from sketchy and tu-berlin (using the same pipeline as (f)); (d) nonartist's drawing with the hint of a short description; (e) artist's drawing with the hint of reference image; (f) synthesized <e2>scene</e2> using our system."
sameAs(e1, e2)
Comment:

2210	"foremost, the dataset provides a springboard to investigate an assortment of problems related to <e1>scene</e1> sketches (a quick google image search on "<e2>scene</e2> sketches" returns millions of results)."
sameAs(e1, e2)
Comment:

2211	"we contribute the first large-scale dataset of <e1>scene</e1> sketches, sketchyscene, with the goal of advancing research on sketch understanding at both the object and <e2>scene</e2> level."
sameAs(e1, e2)
Comment:

2212	"many applications such as content-based <e1>image</e1> annotation [19, 20, 22, 23] and <e2>image</e2> retrieval [12, 24, 34] can be viewed as different instances of image clustering."
sameAs(e1, e2)
Comment:

2213	"many applications such as content-based <e1>image</e1> annotation [19, 20, 22, 23] and image retrieval [12, 24, 34] can be viewed as different instances of <e2>image</e2> clustering."
sameAs(e1, e2)
Comment:

2214	"many applications such as content-based image annotation [19, 20, 22, 23] and <e1>image</e1> retrieval [12, 24, 34] can be viewed as different instances of <e2>image</e2> clustering."
sameAs(e1, e2)
Comment:

2215	"technically, image clustering is the process of grouping <e1>images</e1> into <e2>clusters</e2> such that the images within the same clusters are similar to each other, while those in different clusters are dissimilar."
Used-for(e1, e2)
Comment:

2216	"technically, image clustering is the process of grouping <e1>images</e1> into clusters such that the <e2>images</e2> within the same clusters are similar to each other, while those in different clusters are dissimilar."
sameAs(e1, e2)
Comment:

2217	"technically, image clustering is the process of grouping <e1>images</e1> into clusters such that the images within the same <e2>clusters</e2> are similar to each other, while those in different clusters are dissimilar."
Used-for(e1, e2)
Comment:

2218	"technically, image clustering is the process of grouping <e1>images</e1> into clusters such that the images within the same clusters are similar to each other, while those in different <e2>clusters</e2> are dissimilar."
Used-for(e1, e2)
Comment:

2219	"technically, image clustering is the process of grouping images into <e1>clusters</e1> such that the images within the same <e2>clusters</e2> are similar to each other, while those in different clusters are dissimilar."
sameAs(e1, e2)
Comment:

2220	"technically, image clustering is the process of grouping images into <e1>clusters</e1> such that the images within the same clusters are similar to each other, while those in different <e2>clusters</e2> are dissimilar."
sameAs(e1, e2)
Comment:

2221	"technically, image clustering is the process of grouping images into clusters such that the <e1>images</e1> within the same <e2>clusters</e2> are similar to each other, while those in different clusters are dissimilar."
Used-for(e1, e2)
Comment:

2222	"technically, image clustering is the process of grouping images into clusters such that the <e1>images</e1> within the same clusters are similar to each other, while those in different <e2>clusters</e2> are dissimilar."
Used-for(e1, e2)
Comment:

2223	"technically, image clustering is the process of grouping images into clusters such that the images within the same <e1>clusters</e1> are similar to each other, while those in different <e2>clusters</e2> are dissimilar."
sameAs(e1, e2)
Comment:

2224	"to tackle this <e1>problem</e1>, we propose deep adaptive clustering (dac) that recasts the clustering <e2>problem</e2> into a binary pairwise-classification framework to judge whether pairs of images belong to the same clusters."
sameAs(e1, e2)
Comment:

2225	"to tackle this problem, we propose deep adaptive clustering (dac) that recasts the clustering problem into a binary pairwise-classification framework to judge whether pairs of <e1>images</e1> belong to the same <e2>clusters</e2>."
Used-for(e1, e2)
Comment:

2226	"for the pixel (x, y) in the left <e1>image</e1>, if its corresponding point is found at (x − d, y) in the right <e2>image</e2>, then the depth of this pixel is calculated by fb d , where f is the camera's focal length and b is the distance between two camera centers."
sameAs(e1, e2)
Comment:

2227	"early approaches using <e1>cnns</e1> treated the problem of correspondence estimation as similarity computation [27, 30] , where <e2>cnns</e2> compute the similarity score for a pair of image patches to further determine whether they are matched."
sameAs(e1, e2)
Comment:

2228	"although cnn yields significant gains compared to conventional approaches in terms of both accuracy and speed, it is still difficult to find accurate corresponding points in inherently ill-posed <e1>regions</e1> such as occlusion areas, repeated patterns, textureless <e2>regions</e2>, and reflective surfaces."
sameAs(e1, e2)
Comment:

2229	"solely applying the intensity-consistency constraint between different viewpoints is generally insufficient for accurate correspondence estimation in such ill-posed <e1>regions</e1>, and is useless in textureless <e2>regions</e2>."
sameAs(e1, e2)
Comment:

2230	"in this work, we propose a novel pyramid <e1>stereo matching</e1> network (psmnet) to exploit global context information in <e2>stereo matching</e2>."
sameAs(e1, e2)
Comment:

2231	"in this way, psmnet extends pixel-level <e1>features</e1> to region-level <e2>features</e2> with different scales of receptive fields; the resultant combined global and local feature clues are used to form the cost volume for reliable disparity estimation."
sameAs(e1, e2)
Comment:

2232	"abstract modern health <e1>data</e1> science applications leverage abundant molecular and electronic health <e2>data</e2>, providing opportunities for machine learning to build statistical models to support clinical practice."
sameAs(e1, e2)
Comment:

2233	"time is estimated parametrically or nonparametrically, the <e1>former</e1> by assuming an underlying time distribution and the <e2>latter</e2> as proportional to observed event times."
Conjunction(e1, e2)
Comment:

2234	"time is estimated parametrically or nonparametrically, the former by assuming an underlying <e1>time</e1> distribution and the latter as proportional to observed event <e2>times</e2>."
sameAs(e1, e2)
Comment:

2235	"we present a deep-network-based approach that leverages adversarial learning to address a key challenge in modern <e1>time</e1>-to-event modeling: nonparametric estimation of event-<e2>time</e2> distributions."
sameAs(e1, e2)
Comment:

2236	"we present a deep-network-based approach that leverages adversarial learning to address a key challenge in modern time-to-<e1>event</e1> modeling: nonparametric estimation of <e2>event</e2>-time distributions."
sameAs(e1, e2)
Comment:

2237	"we also introduce a principled cost function to exploit information from censored <e1>events</e1> (<e2>events</e2> that occur subsequent to the observation window)."
sameAs(e1, e2)
Comment:

2238	"unlike most <e1>time</e1>-to-event models, we focus on the estimation of <e2>time</e2>-to-event distributions, rather than time ordering."
sameAs(e1, e2)
Comment:

2239	"unlike most <e1>time</e1>-to-event models, we focus on the estimation of time-to-event distributions, rather than <e2>time</e2> ordering."
sameAs(e1, e2)
Comment:

2240	"unlike most time-to-<e1>event</e1> models, we focus on the estimation of time-to-<e2>event</e2> distributions, rather than time ordering."
sameAs(e1, e2)
Comment:

2241	"unlike most time-to-event models, we focus on the estimation of <e1>time</e1>-to-event distributions, rather than <e2>time</e2> ordering."
sameAs(e1, e2)
Comment:

2242	"for a given subject, these models estimate either a risk score or the <e1>time</e1>-to-event distribution, from a pre-specified point in <e2>time</e2> at which a set of covariates (predictors) are observed."
sameAs(e1, e2)
Comment:

2243	"the objective of the embedding module in neural networks is to represent a <e1>discrete</e1> symbol, such as a word or an entity, with some <e2>continuous</e2> embedding vector v ∈ r d ."
Conjunction(e1, e2)
Comment:

2244	"this is equivalent to the following: first we encode each symbol with an "<e1>one</e1>-hot" encoding vector b ∈ [0, 1] n where j b j = 1 (n is the total number of symbols), and then generate the embedding vector v by simply multiplying the "<e2>one</e2>-hot" vector b with the embedding matrix w ∈ r n ×d , i.e."
sameAs(e1, e2)
Comment:

2245	"the major issue is that the <e1>number</e1> of parameters grows linearly with the <e2>number</e2> of symbols."
sameAs(e1, e2)
Comment:

2246	"in the proposed approach, we use a k-way ddimensional code to represent each symbol, where each code has d dimensions, and each dimension has a cardinality of k. for example, a <e1>concept</e1> of cat may be encoded as (5-1-3-7), and a <e2>concept</e2> of dog may be encoded as (5-1-3-9)."
sameAs(e1, e2)
Comment:

2247	"the code allocation for each symbol is based on data and specific tasks such that the <e1>codes</e1> can capture semantics of symbols, and similar <e2>codes</e2> should reflect similar mean-ings."
sameAs(e1, e2)
Comment:

2248	"by adopting the new approach, we can reduce the <e1>number</e1> of embedding parameters from o(n d) to o( k log k d log n + c) , where d is the code embedding size, and c is the <e2>number</e2> of neural network parameters."
sameAs(e1, e2)
Comment:

2249	"by adopting the new approach, we can reduce the number of embedding <e1>parameters</e1> from o(n d) to o( k log k d log n + c) , where d is the code embedding size, and c is the number of neural network <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

2250	"in this work, we derive a relaxed discrete optimization <e1>approach</e1> based on stochastic gradient descent (sgd), and propose two guided <e2>methods</e2> to assist the end-to-end code learning."
Compare(e1, e2)
Comment:

2251	"in this work, we derive a relaxed discrete optimization approach based on <e1>stochastic gradient descent</e1> (<e2>sgd</e2>), and propose two guided methods to assist the end-to-end code learning."
sameAs(e1, e2)
Comment:

2252	"abstract distributed model training is vulnerable to byzantine system failures and adversarial compute <e1>nodes</e1>, i.e., <e2>nodes</e2> that use malicious updates to corrupt the global model stored at a parameter server (ps)."
sameAs(e1, e2)
Comment:

2253	"instead of having each compute <e1>node</e1> i evaluate a single gradient gi, draco assigns each <e2>node</e2> redundant gradients."
sameAs(e1, e2)
Comment:

2254	"instead of having each compute node i evaluate a single <e1>gradient</e1> gi, draco assigns each node redundant <e2>gradients</e2>."
sameAs(e1, e2)
Comment:

2255	"in this example, the replication ratio is 3, and the parameter server can recover the sum of the <e1>gradients</e1> from any 2 of the encoded <e2>gradient</e2> updates."
sameAs(e1, e2)
Comment:

2256	"thus, draco incurs a computational redundancy ratio of r. while this may seem sub-optimal, we show <e1>that</e1> under a worst-case adversarial setup, it is information-theoretically impossible to design a system <e2>that</e2> obtains identical models to the adversary-free setup with less redundancy."
sameAs(e1, e2)
Comment:

2257	"upon receiving the p <e1>gradient</e1> sums, the ps uses a "decoding" function to remove the effect of the adversarial nodes and reconstruct the original desired sum of the b <e2>gradients</e2>."
sameAs(e1, e2)
Comment:

2258	"however, in realistic regimes where only a constant number of nodes are malicious, draco is significantly faster as <e1>we</e1> show in experiments in section 4. <e2>we</e2> implement draco in pytorch and deploy it on distributed setups on amazon ec2, where we compare against median-based training algorithms on several real world datasets and various ml models."
sameAs(e1, e2)
Comment:

2259	"however, in realistic regimes where only a constant number of nodes are malicious, draco is significantly faster as <e1>we</e1> show in experiments in section 4. we implement draco in pytorch and deploy it on distributed setups on amazon ec2, where <e2>we</e2> compare against median-based training algorithms on several real world datasets and various ml models."
sameAs(e1, e2)
Comment:

2260	"however, in realistic regimes where only a constant number of nodes are malicious, draco is significantly faster as we show in experiments in section 4. <e1>we</e1> implement draco in pytorch and deploy it on distributed setups on amazon ec2, where <e2>we</e2> compare against median-based training algorithms on several real world datasets and various ml models."
sameAs(e1, e2)
Comment:

2261	"we show that draco is up to orders of magnitude faster compared to gm-based <e1>approaches</e1> across a range of neural networks, e.g., lenet, alexnet, , and always converges to the correct adversary-free model, while in some cases median-based <e2>approaches</e2> do not converge."
sameAs(e1, e2)
Comment:

2262	"abstract introduction <e1>image</e1> colorization assigns a color to each pixel of a target grayscale <e2>image</e2>."
sameAs(e1, e2)
Comment:

2263	"intuitively, one reference <e1>image</e1> cannot include all possible scenarios in the target grayscale <e2>image</e2>."
sameAs(e1, e2)
Comment:

2264	"a more reliable solution is locating the most similar <e1>image</e1> patch/pixel in a huge reference <e2>image</e2> database and then transferring color information from the matched patch/pixel to the target patch/pixel."
sameAs(e1, e2)
Comment:

2265	"[7] ) and deep learning techniques have been demonstrated to be very effective on various computer vision and <e1>image processing</e1> applications including <e2>image classification</e2> [12] , pedestrian detection [17, 26] , image super-resolution [4] , photo adjustment [24] etc."
Used-for(e1, e2)
Comment:

2266	"this paper formulates image colorization as a regression <e1>problem</e1> and deep neural networks are used to solve the <e2>problem</e2>."
sameAs(e1, e2)
Comment:

2267	"because image colorization is typically semanticaware, we propose a new <e1>semantic</e1> feature descriptor to incorporate the <e2>semantic</e2>-awareness into our colorization model."
sameAs(e1, e2)
Comment:

2268	"when using reinforcement learning (rl), the agent performs a sequence of actions, which specifies the structure of the <e1>model</e1>; this <e2>model</e2> is then trained and its validation performance is returned as the reward, which is used to update the rnn controller."
sameAs(e1, e2)
Comment:

2269	"in this paper, we describe a <e1>method</e1> that is able to learn a cnn which matches previous state of the art in terms of accuracy, while requiring 5 times fewer <e2>model</e2> evaluations during the architecture search."
Used-for(e1, e2)
Comment:

2270	"in this paper, we describe a <e1>method</e1> that is able to learn a cnn which matches previous state of the art in terms of accuracy, while requiring 5 times fewer model evaluations during the architecture <e2>search</e2>."
Used-for(e1, e2)
Comment:

2271	"this cell structure is then stacked a certain number of <e1>times</e1>, depending on the size of the training set, and the desired running <e2>time</e2> of the final cnn (see section 3 for details)."
sameAs(e1, e2)
Comment:

2272	"we propose to use heuristic <e1>search</e1> to <e2>search</e2> the space of cell structures, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising structures as we go."
sameAs(e1, e2)
Comment:

2273	"we propose to use heuristic search to search the space of cell <e1>structures</e1>, starting with simple (shallow) models and progressing to complex ones, pruning out unpromising <e2>structures</e2> as we go."
sameAs(e1, e2)
Comment:

2274	"at iteration b of the algorithm, <e1>we</e1> have a set of k candidate cells (each of size b blocks), which <e2>we</e2> train and evaluate on a dataset of interest."
sameAs(e1, e2)
Comment:

2275	"we propose a new <e1>method</e1> for learning the structure of convolutional neural networks (cnns) that is more efficient than recent state-of-the-art <e2>methods</e2> based on reinforcement learning and evolutionary algorithms."
Compare(e1, e2)
Comment:

2276	"our progressive (simple to complex) <e1>approach</e1> has several advantages over other <e2>techniques</e2> that directly search in the space of fully-specified structures."
Compare(e1, e2)
Comment:

2277	"third, we factorize the <e1>search space</e1> into a product of smaller <e2>search spaces</e2>, allowing us to potentially search models with many more blocks."
sameAs(e1, e2)
Comment:

2278	"in section 5 we show that our <e1>approach</e1> is 5 times more efficient than the rl <e2>method</e2> of [41] in terms of number of models evaluated, and 8 times faster in terms of total compute."
Compare(e1, e2)
Comment:

2279	"in section 5 we show that our <e1>approach</e1> is 5 times more efficient than the rl method of [41] in terms of number of <e2>models</e2> evaluated, and 8 times faster in terms of total compute."
Used-for(e1, e2)
Comment:

2280	"in section 5 we show that our approach is 5 <e1>times</e1> more efficient than the rl method of [41] in terms of number of models evaluated, and 8 <e2>times</e2> faster in terms of total compute."
sameAs(e1, e2)
Comment:

2281	"our approach uses a sequential model-based optimization (smbo) strategy, in which we <e1>search</e1> for structures in order of increasing complexity, while simultaneously learning a surrogate model to guide the <e2>search</e2> through structure space."
sameAs(e1, e2)
Comment:

2282	"direct comparison under the same search space shows that our method is up to 5 <e1>times</e1> more efficient than the rl method of zoph et al (2018) in terms of number of models evaluated, and 8 <e2>times</e2> faster in terms of total compute."
sameAs(e1, e2)
Comment:

2283	"these details are valuable for the further improvement of <e1>accuracy</e1> and robustness of our <e2>algorithm</e2>."
Evaluate-for(e1, e2)
Comment:

2284	"these details are valuable for the further improvement of accuracy and <e1>robustness</e1> of our <e2>algorithm</e2>."
Evaluate-for(e1, e2)
Comment:

2285	"in summary, our contributions are three-fold as follows: • <e1>we</e1> propose a novel and effective network called cascaded pyramid network (cpn), which integrates global pyramid network (globalnet) and pyramid refined network based on online hard keypoints min-ing (refinenet) • <e2>we</e2> explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2286	"in summary, our contributions are three-fold as follows: • we propose a novel and effective <e1>network</e1> called cascaded pyramid <e2>network</e2> (cpn), which integrates global pyramid network (globalnet) and pyramid refined network based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2287	"in summary, our contributions are three-fold as follows: • we propose a novel and effective <e1>network</e1> called cascaded pyramid network (cpn), which integrates global pyramid <e2>network</e2> (globalnet) and pyramid refined network based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2288	"in summary, our contributions are three-fold as follows: • we propose a novel and effective <e1>network</e1> called cascaded pyramid network (cpn), which integrates global pyramid network (globalnet) and pyramid refined <e2>network</e2> based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2289	"in summary, our contributions are three-fold as follows: • we propose a novel and effective network called cascaded pyramid <e1>network</e1> (cpn), which integrates global pyramid <e2>network</e2> (globalnet) and pyramid refined network based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2290	"in summary, our contributions are three-fold as follows: • we propose a novel and effective network called cascaded pyramid <e1>network</e1> (cpn), which integrates global pyramid network (globalnet) and pyramid refined <e2>network</e2> based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2291	"in summary, our contributions are three-fold as follows: • we propose a novel and effective network called cascaded pyramid network (cpn), which integrates global pyramid <e1>network</e1> (globalnet) and pyramid refined <e2>network</e2> based on online hard keypoints min-ing (refinenet) • we explore the effects of various factors contributing to muti-person pose estimation involved in top-down pipeline."
sameAs(e1, e2)
Comment:

2292	"the main reasons lie at two points: 1) <e1>these</e1> "hard" joints cannot be simply recognized based on their appearance features only, for example, the torso point; 2) <e2>these</e2> "hard" joints are not explicitly addressed during the training process."
sameAs(e1, e2)
Comment:

2293	"to address these "hard" joints, in this paper, we present a novel <e1>network</e1> structure called cascaded pyramid <e2>network</e2> (cpn)."
sameAs(e1, e2)
Comment:

2294	"our globalnet learns a good <e1>feature</e1> representation based on <e2>feature</e2> pyramid network [24] ."
sameAs(e1, e2)
Comment:

2295	"we treat the <e1>image</e1> enhancement problem as an <e2>image</e2>-to-image translation problem in which an input image is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2296	"we treat the <e1>image</e1> enhancement problem as an image-to-<e2>image</e2> translation problem in which an input image is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2297	"we treat the <e1>image</e1> enhancement problem as an image-to-image translation problem in which an input <e2>image</e2> is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2298	"we treat the <e1>image</e1> enhancement problem as an image-to-image translation problem in which an input image is transformed into an enhanced <e2>image</e2> with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2299	"we treat the image enhancement <e1>problem</e1> as an image-to-image translation <e2>problem</e2> in which an input image is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2300	"we treat the image enhancement problem as an <e1>image</e1>-to-<e2>image</e2> translation problem in which an input image is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2301	"we treat the image enhancement problem as an <e1>image</e1>-to-image translation problem in which an input <e2>image</e2> is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2302	"we treat the image enhancement problem as an <e1>image</e1>-to-image translation problem in which an input image is transformed into an enhanced <e2>image</e2> with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2303	"we treat the image enhancement problem as an image-to-<e1>image</e1> translation problem in which an input <e2>image</e2> is transformed into an enhanced image with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2304	"we treat the image enhancement problem as an image-to-<e1>image</e1> translation problem in which an input image is transformed into an enhanced <e2>image</e2> with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2305	"we treat the image enhancement problem as an image-to-image translation problem in which an input <e1>image</e1> is transformed into an enhanced <e2>image</e2> with the characteristics embedded in the set of training photographs."
sameAs(e1, e2)
Comment:

2306	"for addressing the issue and obtaining high-quality results, we propose a few improvements along the <e1>way</e1> of constructing our two-<e2>way</e2> gan."
sameAs(e1, e2)
Comment:

2307	"however, we found that, although in the same domain, the inputs actually come from different sources, <e1>one</e1> from the input data and the <e2>other</e2> from the generated data."
Conjunction(e1, e2)
Comment:

2308	"however, we found that, although in the same domain, the inputs actually come from different sources, one from the input <e1>data</e1> and the other from the generated <e2>data</e2>."
sameAs(e1, e2)
Comment:

2309	"the <e1>results</e1> often look more natural than previous <e2>methods</e2>."
Compare(e1, e2)
Comment:

2310	"to find correct mappings between the query and the <e1>proposals</e1>, [34] proposes to associate the query with successive <e2>proposals</e2>; once a proposal is selected, a phrase is reconstructed from it and evaluated for language consistency with the input query."
sameAs(e1, e2)
Comment:

2311	"to find correct mappings between the query and the <e1>proposals</e1>, [34] proposes to associate the query with successive proposals; once a <e2>proposal</e2> is selected, a phrase is reconstructed from it and evaluated for language consistency with the input query."
sameAs(e1, e2)
Comment:

2312	"to find correct mappings between the query and the proposals, [34] proposes to associate the query with successive <e1>proposals</e1>; once a <e2>proposal</e2> is selected, a phrase is reconstructed from it and evaluated for language consistency with the input query."
sameAs(e1, e2)
Comment:

2313	"it is a fundamental building block for many high-level computer vision tasks such as <e1>image</e1> retrieval [3] , <e2>image</e2> qa [6, 11, 12] and video qa [13, 14] ."
sameAs(e1, e2)
Comment:

2314	"to address this problem, typically a <e1>proposal</e1> generation system is applied to the input image to produce a set of candidate regions (i.e., <e2>proposals</e2>)."
sameAs(e1, e2)
Comment:

2315	"therefore, some computer vision researchers [5, 20, 21, 27] attempt to <e1>model</e1> it as a perception-action <e2>model</e2>, which is an end-to-end system that maps from pixels to actuation."
sameAs(e1, e2)
Comment:

2316	"however, for the <e1>task</e1> of 3d object detection, which is more challenging, a well-designed <e2>model</e2> is required to make use of the strength of multiple modalities."
Evaluate-for(e1, e2)
Comment:

2317	"however, for the task of 3d object detection, which is more challenging, a well-designed <e1>model</e1> is required to make use of the strength of multiple <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

2318	"1 , the multi-view 3d detection <e1>network</e1> consists of two parts: a 3d proposal <e2>network</e2> and a regionbased fusion network."
sameAs(e1, e2)
Comment:

2319	"1 , the multi-view 3d detection <e1>network</e1> consists of two parts: a 3d proposal network and a regionbased fusion <e2>network</e2>."
sameAs(e1, e2)
Comment:

2320	"1 , the multi-view 3d detection network consists of two parts: a 3d proposal <e1>network</e1> and a regionbased fusion <e2>network</e2>."
sameAs(e1, e2)
Comment:

2321	"we design a deep fusion <e1>approach</e1> to enable interactions of intermediate <e2>layers</e2> from different views."
Used-for(e1, e2)
Comment:

2322	"experiments show that our 3d <e1>proposals</e1> significantly outperforms recent 3d <e2>proposal</e2> methods 3dop [4] and mono3d [3] ."
sameAs(e1, e2)
Comment:

2323	"the figure 1 : multi-view 3d object detection <e1>network</e1> (mv3d): the <e2>network</e2> takes the bird's eye view and front view of lidar point cloud as well as an image as input."
sameAs(e1, e2)
Comment:

2324	"lidar-based variant of our approach achieves around 25% higher accuracy in 3d localization <e1>task</e1> and 30% higher 3d average precision (ap) in the <e2>task</e2> of 3d object detection."
sameAs(e1, e2)
Comment:

2325	"first, <e1>proposals</e1> generated by independent systems may not always cover all mentioned objects given various queries; since retrieval based methods localize objects by choosing one of these <e2>proposals</e2>, they are bounded by the performance limits from proposal generation systems."
sameAs(e1, e2)
Comment:

2326	"first, <e1>proposals</e1> generated by independent systems may not always cover all mentioned objects given various queries; since retrieval based methods localize objects by choosing one of these proposals, they are bounded by the performance limits from <e2>proposal</e2> generation systems."
sameAs(e1, e2)
Comment:

2327	"first, proposals generated by independent systems may not always cover all mentioned <e1>objects</e1> given various queries; since retrieval based methods localize <e2>objects</e2> by choosing one of these proposals, they are bounded by the performance limits from proposal generation systems."
sameAs(e1, e2)
Comment:

2328	"first, proposals generated by independent systems may not always cover all mentioned objects given various queries; since retrieval based methods localize objects by choosing one of these <e1>proposals</e1>, they are bounded by the performance limits from <e2>proposal</e2> generation systems."
sameAs(e1, e2)
Comment:

2329	"given <e1>one</e1> query phrase, we evaluate predicted proposals and down-weight those which cover objects mentioned by <e2>other</e2> phrases (i.e., context)."
Conjunction(e1, e2)
Comment:

2330	"for example, we assign lower rewards for <e1>proposals</e1> containing "a guitar" and "a little girl" in fig 1 to guide system to select more discriminative <e2>proposals</e2> containing "a man"."
sameAs(e1, e2)
Comment:

2331	"it is an important building block in computer vision with natural language interaction, which can be utilized in high-level tasks, such as <e1>image</e1> retrieval [3, 26] , <e2>image</e2> captioning [1, 7] and visual question answering [2, 4, 8] ."
sameAs(e1, e2)
Comment:

2332	"in implementation, we propose a novel query-guided regression <e1>network</e1> with context policy (qrc net) which consists of a proposal generation <e2>network</e2> (pgn), a queryguided regression network (qrn) and a context policy network (cpn)."
sameAs(e1, e2)
Comment:

2333	"in implementation, we propose a novel query-guided regression <e1>network</e1> with context policy (qrc net) which consists of a proposal generation network (pgn), a queryguided regression <e2>network</e2> (qrn) and a context policy network (cpn)."
sameAs(e1, e2)
Comment:

2334	"in implementation, we propose a novel query-guided regression <e1>network</e1> with context policy (qrc net) which consists of a proposal generation network (pgn), a queryguided regression network (qrn) and a context policy <e2>network</e2> (cpn)."
sameAs(e1, e2)
Comment:

2335	"in implementation, we propose a novel query-guided regression network with context <e1>policy</e1> (qrc net) which consists of a proposal generation network (pgn), a queryguided regression network (qrn) and a context <e2>policy</e2> network (cpn)."
sameAs(e1, e2)
Comment:

2336	"in implementation, we propose a novel query-guided regression network with context policy (qrc net) which consists of a proposal generation <e1>network</e1> (pgn), a queryguided regression <e2>network</e2> (qrn) and a context policy network (cpn)."
sameAs(e1, e2)
Comment:

2337	"in implementation, we propose a novel query-guided regression network with context policy (qrc net) which consists of a proposal generation <e1>network</e1> (pgn), a queryguided regression network (qrn) and a context policy <e2>network</e2> (cpn)."
sameAs(e1, e2)
Comment:

2338	"in implementation, we propose a novel query-guided regression network with context policy (qrc net) which consists of a proposal generation network (pgn), a queryguided regression <e1>network</e1> (qrn) and a context policy <e2>network</e2> (cpn)."
sameAs(e1, e2)
Comment:

2339	"pgn is a <e1>proposal</e1> generator which provides candidate <e2>proposals</e2> given an input image (red boxes in fig."
sameAs(e1, e2)
Comment:

2340	"flickr30k entities contains more than 30k <e1>images</e1> and 170k query phrases, while referit game has 19k <e2>images</e2> referred by 130k query phrases."
sameAs(e1, e2)
Comment:

2341	"flickr30k entities contains more than 30k images and 170k query <e1>phrases</e1>, while referit game has 19k images referred by 130k query <e2>phrases</e2>."
sameAs(e1, e2)
Comment:

2342	"to address this problem, typically a <e1>proposal</e1> generation system is first applied to produce a set of <e2>proposals</e2> as grounding candidates."
sameAs(e1, e2)
Comment:

2343	"the main difficulties lie in how to learn the <e1>correlation</e1> between language (query) and visual (proposals) modalities, and how to localize objects based on multimodal <e2>correlation</e2>."
sameAs(e1, e2)
Comment:

2344	"among these, phrase-region cca [25] and scrc [14] models learn a multimodal subspace via canonical correlation analysis (cca) and a <e1>recurrent neural network</e1> (<e2>rnn</e2>) respectively."
sameAs(e1, e2)
Comment:

2345	"stargan: unified generative adversarial networks for multi-domain <e1>image</e1>-to-<e2>image</e2> translation"
sameAs(e1, e2)
Comment:

2346	"multi-domain <e1>image</e1>-to-<e2>image</e2> translation results on the celeba dataset via transferring knowledge learned from the rafd dataset."
sameAs(e1, e2)
Comment:

2347	"the first and sixth columns show input <e1>images</e1> while the remaining columns are <e2>images</e2> generated by stargan."
sameAs(e1, e2)
Comment:

2348	"while inception modules are conceptually similar to convolutions (<e1>they</e1> are convolutional feature extractors), <e2>they</e2> empirically appear to be capable of learning richer representations with less parameters."
sameAs(e1, e2)
Comment:

2349	"how do <e1>they</e1> work, and how do <e2>they</e2> differ from regular convolutions?"
sameAs(e1, e2)
Comment:

2350	"in 2012, these ideas were refined into the alexnet architecture [9] , where convolution <e1>operations</e1> were being repeated multiple times in-between max-pooling <e2>operations</e2>, allowing the network to learn richer features at every spatial scale."
sameAs(e1, e2)
Comment:

2351	"at this point a new style of network emerged, the <e1>inception</e1> architecture, introduced by szegedy et al in 2014 [20] as googlenet (<e2>inception</e2> v1), later refined as inception v2 [7] , inception v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2352	"at this point a new style of network emerged, the <e1>inception</e1> architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as <e2>inception</e2> v2 [7] , inception v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2353	"at this point a new style of network emerged, the <e1>inception</e1> architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as inception v2 [7] , <e2>inception</e2> v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2354	"at this point a new style of network emerged, the <e1>inception</e1> architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as inception v2 [7] , inception v3 [21] , and most recently <e2>inception</e2>-resnet [19] ."
sameAs(e1, e2)
Comment:

2355	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (<e1>inception</e1> v1), later refined as <e2>inception</e2> v2 [7] , inception v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2356	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (<e1>inception</e1> v1), later refined as inception v2 [7] , <e2>inception</e2> v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2357	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (<e1>inception</e1> v1), later refined as inception v2 [7] , inception v3 [21] , and most recently <e2>inception</e2>-resnet [19] ."
sameAs(e1, e2)
Comment:

2358	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as <e1>inception</e1> v2 [7] , <e2>inception</e2> v3 [21] , and most recently inception-resnet [19] ."
sameAs(e1, e2)
Comment:

2359	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as <e1>inception</e1> v2 [7] , inception v3 [21] , and most recently <e2>inception</e2>-resnet [19] ."
sameAs(e1, e2)
Comment:

2360	"at this point a new style of network emerged, the inception architecture, introduced by szegedy et al in 2014 [20] as googlenet (inception v1), later refined as inception v2 [7] , <e1>inception</e1> v3 [21] , and most recently <e2>inception</e2>-resnet [19] ."
sameAs(e1, e2)
Comment:

2361	"the fundamental building block of <e1>inception</e1>-style models is the <e2>inception</e2> module, of which several different versions exist."
sameAs(e1, e2)
Comment:

2362	"in figure 1 we show the canonical form of an <e1>inception</e1> module, as found in the <e2>inception</e2> v3 architecture."
sameAs(e1, e2)
Comment:

2363	"to investigate this aspect we train a model to recognize characters from both <e1>audio</e1> and visual input, and then systematically disturb the <e2>audio</e2> channel or remove the visual channel."
sameAs(e1, e2)
Comment:

2364	"our model (section 2) outputs at the character level, is able to learn a language model, and has a novel dual attention mechanism that can operate over visual <e1>input</e1> only, audio <e2>input</e2> only, or both."
sameAs(e1, e2)
Comment:

2365	"the use of <e1>hmms</e1> together with hand-crafted or pre-trained visual features have proved popular - [36] encodes input images using dbf; [14] used dct; and [28] uses a cnn pre-trained to classify phonemes; all three combine these features with <e2>hmms</e2> to classify spoken digits or isolated words."
sameAs(e1, e2)
Comment:

2366	"the use of hmms together with hand-crafted or pre-trained visual <e1>features</e1> have proved popular - [36] encodes input images using dbf; [14] used dct; and [28] uses a cnn pre-trained to classify phonemes; all three combine these <e2>features</e2> with hmms to classify spoken digits or isolated words."
sameAs(e1, e2)
Comment:

2367	"a number of papers have adopted this <e1>approach</e1> for speech recognition [7, 8] , and the most related work to ours is that of chan et al [5] which proposes an elegant sequence-to-sequence <e2>method</e2> to transcribe audio signal to characters."
Compare(e1, e2)
Comment:

2368	"in this case the model is based on the recent sequence-tosequence (encoder-decoder with attention) translater architectures that have been developed for <e1>speech recognition</e1> and <e2>machine translation</e2> [3, 5, 15, 16, 34] ."
Conjunction(e1, e2)
Comment:

2369	"the most common means of performing 6-dof pose estimation using visual <e1>data</e1> is to make use of specially-built <e2>models</e2>, which are constructed from a vast number of local features that have been extracted from the images captured during mapping."
Used-for(e1, e2)
Comment:

2370	"the <e1>pose</e1> is then found using ransac to reject outlier correspondences and optimize the camera <e2>pose</e2> on inliers."
sameAs(e1, e2)
Comment:

2371	"these methods rely on local and unintuitive hand-crafted <e1>features</e1>, such as <e2>sift</e2> keypoints."
Compare(e1, e2)
Comment:

2372	"therefore, in this paper <e1>we</e1> consider ways in which <e2>we</e2> can leverage the temporal dependencies in image-sequences to improve the accuracy of 6-dof camera re-localization."
sameAs(e1, e2)
Comment:

2373	"furthermore, <e1>we</e1> show how <e2>we</e2> can in essence unify map-matching, model-based localization, and temporal filtering all in one, extremely compact model."
sameAs(e1, e2)
Comment:

2374	"a critical step in learning a large multi-layer neural network for a specific task is the choice of its architecture, which includes the <e1>number</e1> of layers and the <e2>number</e2> of units within each layer."
sameAs(e1, e2)
Comment:

2375	"the amortization gap refers to the difference caused by amortizing the variational parameters over the entire <e1>training</e1> set, instead of optimizing for each <e2>training</e2> example individually."
sameAs(e1, e2)
Comment:

2376	"our experiments investigate how the choice of encoder, posterior <e1>approximation</e1>, decoder, and optimization affect the <e2>approximation</e2> and amortization gaps."
sameAs(e1, e2)
Comment:

2377	"we train vae models in a number of settings on the <e1>mnist</e1> (lecun et al, 1998) , <e2>fashion-mnist</e2> (xiao et al, 2017) , and cifar-10 ( krizhevsky & hinton, 2009 ) datasets."
Conjunction(e1, e2)
Comment:

2378	"we train vae models in a number of settings on the <e1>mnist</e1> (lecun et al, 1998) , fashion-mnist (xiao et al, 2017) , and cifar-10 ( krizhevsky & hinton, 2009 ) <e2>datasets</e2>."
isA(e1, e2)
Comment:

2379	"we train vae models in a number of settings on the mnist (lecun et al, 1998) , <e1>fashion-mnist</e1> (xiao et al, 2017) , and <e2>cifar-10</e2> ( krizhevsky & hinton, 2009 ) datasets."
Conjunction(e1, e2)
Comment:

2380	"our contributions are: a) <e1>we</e1> investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) <e2>we</e2> quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2381	"our contributions are: a) <e1>we</e1> investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) <e2>we</e2> demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2382	"our contributions are: a) we investigate inference suboptimality in terms of the <e1>approximation</e1> and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of <e2>approximation</e2>, and c) we demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2383	"our contributions are: a) we investigate inference suboptimality in terms of the <e1>approximation</e1> and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions that improve the expressiveness of the <e2>approximation</e2> play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2384	"our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) <e1>we</e1> quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) <e2>we</e2> demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2385	"our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate <e1>that</e1> the learned generative model accommodates the choice of approximation, and c) we demonstrate <e2>that</e2> parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2386	"our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate <e1>that</e1> the learned generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions <e2>that</e2> improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2387	"our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of <e1>approximation</e1>, and c) we demonstrate that parameterized functions that improve the expressiveness of the <e2>approximation</e2> play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2388	"our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in vae inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) we demonstrate <e1>that</e1> parameterized functions <e2>that</e2> improve the expressiveness of the approximation play a significant role in reducing amortization error."
sameAs(e1, e2)
Comment:

2389	"furthermore, we show that the parameters used to increase the expressiveness of the <e1>approximation</e1> play a role in generalizing inference rather than simply improving the complexity of the <e2>approximation</e2>."
sameAs(e1, e2)
Comment:

2390	"although other large <e1>scale</e1> generic visual datasets like imagenet contain some fine-grained categories, their images are usually iconic web images that contain objects in the center with similar <e2>scale</e2> and simple backgrounds."
sameAs(e1, e2)
Comment:

2391	"although other large scale generic visual datasets like imagenet contain some fine-grained categories, their <e1>images</e1> are usually iconic web <e2>images</e2> that contain objects in the center with similar scale and simple backgrounds."
sameAs(e1, e2)
Comment:

2392	"with the limited availability of large <e1>scale</e1> fgvc datasets, how to design models that perform well on large <e2>scale</e2> non-iconic images with fine-grained categories remains an underdeveloped area."
sameAs(e1, e2)
Comment:

2393	"examples include recognizing natural <e1>categories</e1> such as species of birds [58, 54] , dogs [28] and plants [39, 59] ; or man-made <e2>categories</e2> such as car make & model [32, 63] ."
sameAs(e1, e2)
Comment:

2394	"in addition, along with imagenet, inat enables us to study the transfer of knowledge learned on large <e1>scale</e1> datasets to small <e2>scale</e2> fine-grained domains."
sameAs(e1, e2)
Comment:

2395	"a successful fgvc model should be able to discriminate categories with subtle differences, which presents formidable challenges for the model design yet also provides insights to a wide range of applications such as rich <e1>image</e1> captioning [3] , <e2>image</e2> generation [5] , and machine teaching [27, 37] ."
sameAs(e1, e2)
Comment:

2396	"secondly, we study how to transfer from knowledge learned on large <e1>scale</e1> datasets to small <e2>scale</e2> fine-grained domains."
sameAs(e1, e2)
Comment:

2397	"on cub200 birds [58] , inat pre-trained <e1>networks</e1> perform much better than imagenet pre-trained ones; whereas on stanford-dogs [28] , imagenet pre-trained <e2>networks</e2> yield better performance."
sameAs(e1, e2)
Comment:

2398	"on cub200 birds [58] , inat pre-trained networks perform much better than <e1>imagenet</e1> pre-trained ones; whereas on stanford-dogs [28] , <e2>imagenet</e2> pre-trained networks yield better performance."
sameAs(e1, e2)
Comment:

2399	"this is because there are more visually similar bird <e1>categories</e1> in inat and dog <e2>categories</e2> in imagenet."
sameAs(e1, e2)
Comment:

2400	"given the <e1>target domain</e1> of interest, we pre-train a cnn on the selected subset from the source domain based on the proposed domain similarity measure, and then fine-tune on the <e2>target domain</e2>."
sameAs(e1, e2)
Comment:

2401	"the proposed approach could be served as a learning based evaluation <e1>metric</e1> that is complementary to existing rule-based <e2>metrics</e2>."
sameAs(e1, e2)
Comment:

2402	"secondly, each <e1>metric</e1> has well known blind spots to pathological caption constructions, and rulebased <e2>metrics</e2> lack provisions to repair such blind spots once identified."
sameAs(e1, e2)
Comment:

2403	"secondly, each metric has well known blind <e1>spots</e1> to pathological caption constructions, and rulebased metrics lack provisions to repair such blind <e2>spots</e2> once identified."
sameAs(e1, e2)
Comment:

2404	"our <e1>metric</e1> outperforms other <e2>metrics</e2> on both caption level human correlation in flickr 8k and system level human correlation in coco."
sameAs(e1, e2)
Comment:

2405	"our metric outperforms other metrics on both caption level human <e1>correlation</e1> in flickr 8k and system level human <e2>correlation</e2> in coco."
sameAs(e1, e2)
Comment:

2406	" introduction the problem of super resolution entails artificially enlarging a low<e1> resolutio</e1>n photograph to recover a corresponding plausible image with higher<e2> resolutio</e2>n [31] ."
sameAs(e1, e2)
Comment:

2407	"thus, the problem is underspecified and many plausible, high <e1>resolution</e1> images may match a given low <e2>resolution</e2> input image."
sameAs(e1, e2)
Comment:

2408	"to alleviate the time and storage bottlenecks, two research directions have been studied extensively: (1) partition the dataset so <e1>that</e1> only a subset of data points is searched; (2) represent the data as codes so <e2>that</e2> similarity computation can be carried out more efficiently."
sameAs(e1, e2)
Comment:

2409	"to alleviate the time and storage bottlenecks, two research directions have been studied extensively: (1) partition the dataset so that only a subset of <e1>data</e1> points is searched; (2) represent the <e2>data</e2> as codes so that similarity computation can be carried out more efficiently."
sameAs(e1, e2)
Comment:

2410	"the <e1>former</e1> often resorts to search-tree or bucket-based lookup; while the <e2>latter</e2> relies on binary hashing or quantization."
Conjunction(e1, e2)
Comment:

2411	"however, due to the requirement of discrete outputs for the hash <e1>functions</e1>, learning such <e2>functions</e2> is known to be very challenging."
sameAs(e1, e2)
Comment:

2412	"we propose a generative model which captures both the encoding of binary codes h from <e1>input</e1> x and the decoding of <e2>input</e2> x from h. this provides a principled hash learning framework, where the hash function is learned by minimum description length (mdl) principle."
sameAs(e1, e2)
Comment:

2413	"such a generative model also enables us to optimize distributions over <e1>discrete</e1> hash codes without the necessity to handle <e2>discrete</e2> variables."
sameAs(e1, e2)
Comment:

2414	"extensive experiments on a variety of large-scale datasets show that the proposed <e1>method</e1> achieves better retrieval results than the existing state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

2415	"extensive experiments on a variety of large-scale datasets show that the proposed method achieves better retrieval <e1>results</e1> than the existing state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

2416	"when the negative euclidean distance is used, i.e., sim(x, y) = kx yk 2 , this corresponds to l 2 nearest neighbor <e1>search</e1> (l2nns) problem; when the inner product is used, i.e., sim(x, y) = x > y, it becomes a maximum inner product <e2>search</e2> (mips) problem."
sameAs(e1, e2)
Comment:

2417	"when the negative euclidean distance is used, i.e., sim(x, y) = kx yk 2 , this corresponds to l 2 nearest neighbor search (l2nns) <e1>problem</e1>; when the inner product is used, i.e., sim(x, y) = x > y, it becomes a maximum inner product search (mips) <e2>problem</e2>."
sameAs(e1, e2)
Comment:

2418	"we use both synthetic and real-world data to show that, a family of <e1>graph</e1> neural network models are vulnerable to these attacks, in both <e2>graph</e2>-level and node-level classification tasks."
sameAs(e1, e2)
Comment:

2419	"representation learning on the structured data with deep learning methods has shown promising results in various <e1>applications</e1>, including drug screening (duvenaud et al, 2015) , protein analysis (hamilton et al, 2017) , knowledge graph completion (trivedi et al, 2017) , etc.. despite the success of deep graph networks, the lack of interpretability and robustness of these models make it risky for some financial or security related <e2>applications</e2>."
sameAs(e1, e2)
Comment:

2420	"representation learning on the structured data with deep learning methods has shown promising results in various applications, including drug screening (duvenaud et al, 2015) , protein analysis (hamilton et al, 2017) , knowledge <e1>graph</e1> completion (trivedi et al, 2017) , etc.. despite the success of deep <e2>graph</e2> networks, the lack of interpretability and robustness of these models make it risky for some financial or security related applications."
sameAs(e1, e2)
Comment:

2421	"abstract convolutional neural networks (cnns) introduction a key challenge in visual recognition is how to accommodate geometric variations or model geometric transformations in object <e1>scale</e1>, pose, <e2>viewpoint</e2>, and part deformation."
Conjunction(e1, e2)
Comment:

2422	"the limitation originates from the fixed geometric structures of cnn modules: a convolution unit samples the input feature map at fixed locations; a <e1>pooling</e1> layer reduces the spatial resolution at a fixed ratio; a roi (region-of-interest) <e2>pooling</e2> layer separates a roi into fixed spatial bins, etc."
sameAs(e1, e2)
Comment:

2423	"because different locations may correspond to objects with different <e1>scales</e1> or deformation, adaptive determination of <e2>scales</e2> or receptive field sizes is desirable for visual recognition with fine localization, e.g., semantic segmentation using fully convolutional networks [37] ."
sameAs(e1, e2)
Comment:

2424	"our <e1>approach</e1> shares similar high level spirit with spatial transform networks [23] and deformable part <e2>models</e2> [10] ."
Used-for(e1, e2)
Comment:

2425	"they all have internal transformation <e1>parameters</e1> and learn such <e2>parameters</e2> purely from data."
sameAs(e1, e2)
Comment:

2426	"our <e1>approach</e1> outperforms existing <e2>approaches</e2> both in terms of completion and semantic labeling accuracy by a significant margin."
Compare(e1, e2)
Comment:

2427	"language models are a critical part of systems for <e1>speech recognition</e1> (yu & deng, 2014) and <e2>machine translation</e2> (koehn, 2010) ."
Conjunction(e1, e2)
Comment:

2428	"the current state of the art for <e1>language modeling</e1> is based on long short term memory networks (lstm; hochreiter et al, 1997 ) which can theoretically <e2>model</e2> arbitrarily long dependencies."
isA(e1, e2)
Comment:

2429	"convolutional networks can be stacked to represent large context sizes and extract hierarchical <e1>features</e1> over larger and larger contexts with more abstractive <e2>features</e2> (lecun & bengio, 1995) ."
sameAs(e1, e2)
Comment:

2430	"this allows them to model long-term dependencies by applying o( n k ) <e1>operations</e1> over a context of size n and kernel width k. in contrast, recurrent networks view the input as a chain structure and therefore require a linear number o(n ) of <e2>operations</e2>."
sameAs(e1, e2)
Comment:

2431	"analyzing the input hierarchically bears resemblance to classical grammar formalisms which build syntactic tree structures of increasing granuality, e.g., sentences consist of noun <e1>phrases</e1> and verb <e2>phrases</e2> each comprising further internal structure (manning & schütze, 1999; steedman, 2002) ."
sameAs(e1, e2)
Comment:

2432	"hierarchical <e1>structure</e1> also eases learning since the number of non-linearities for a given context size is reduced compared to a chain <e2>structure</e2>, thereby mitigating the vanishing gradient problem (glorot & bengio, 2010) ."
sameAs(e1, e2)
Comment:

2433	"our gated linear units reduce the vanishing <e1>gradient</e1> problem for deep architectures by providing a linear path for the <e2>gradients</e2> while retaining non-linear capabilities ( §5.2)."
sameAs(e1, e2)
Comment:

2434	"to our knowledge, this is the first time a non-recurrent <e1>approach</e1> is competitive with strong recurrent <e2>models</e2> on these large scale language tasks."
Used-for(e1, e2)
Comment:

2435	"to our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent <e1>models</e1> on these large scale language <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

2436	"to our knowledge, this is the first time a non-recurrent approach is competitive with strong recurrent models on <e1>these</e1> large scale language <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

2437	"introduction statistical language models estimate the probability distribution of a sequence of <e1>words</e1> by modeling the probability of the next word given preceding <e2>words</e2>, i.e."
sameAs(e1, e2)
Comment:

2438	"abstract we propose a novel approach for unsupervised zero-shot learning (zsl) introduction zero-shot learning (zsl) enables identification of <e1>classes</e1> that are not seen before by means of transferring knowledge from seen <e2>classes</e2> to unseen classes."
sameAs(e1, e2)
Comment:

2439	"abstract we propose a novel approach for unsupervised zero-shot learning (zsl) introduction zero-shot learning (zsl) enables identification of <e1>classes</e1> that are not seen before by means of transferring knowledge from seen classes to unseen <e2>classes</e2>."
sameAs(e1, e2)
Comment:

2440	"abstract we propose a novel approach for unsupervised zero-shot learning (zsl) introduction zero-shot learning (zsl) enables identification of classes that are not seen before by means of transferring knowledge from seen <e1>classes</e1> to unseen <e2>classes</e2>."
sameAs(e1, e2)
Comment:

2441	"almost all attribute-based zsl works, however, have an important disadvantage: attribute-class relations need to be precisely annotated not only for the seen (training) <e1>classes</e1>, but also for the unseen (zero-shot) <e2>classes</e2> (e.g."
sameAs(e1, e2)
Comment:

2442	"to address this issue, we propose to use the <e1>names</e1> of visual attributes as an intermediate layer that connects the image features and the class <e2>names</e2> in an unsupervised way for the unseen classes."
sameAs(e1, e2)
Comment:

2443	"we provide an extensive experimental evaluation on two zsl <e1>object recognition</e1> and one zsl <e2>action recognition</e2> benchmark datasets."
Used-for(e1, e2)
Comment:

2444	"our unsupervised zsl <e1>model</e1> also provides competitive performance compared to the state-ofthe-art supervised zsl <e2>methods</e2>."
Compare(e1, e2)
Comment:

2445	"recently, the ability to train <e1>image</e1> manipulation convnets has been shown in the unaligned training scenario [42, 43, 5] , where the training is based on sets of images annotated with the presence/absence of a certain attribute, rather than based on aligned datasets containing {input,output} <e2>image</e2> pairs."
sameAs(e1, e2)
Comment:

2446	"recently, the ability to train image manipulation convnets has been shown in the unaligned <e1>training</e1> scenario [42, 43, 5] , where the <e2>training</e2> is based on sets of images annotated with the presence/absence of a certain attribute, rather than based on aligned datasets containing {input,output} image pairs."
sameAs(e1, e2)
Comment:

2447	"the second group consists of adversarial losses, where the loss function is defined implicitly using a separate discriminator <e1>network</e1> that is trained adversarially in parallel with the main generative <e2>network</e2>."
sameAs(e1, e2)
Comment:

2448	"at the same time, the use of adversarial <e1>training</e1> allows to avoid the need for aligned <e2>training</e2> data."
sameAs(e1, e2)
Comment:

2449	"generally, we have found that the suggested architecture can be trained with little tuning to impose complex image manipulations, such as adding and removing smile to human <e1>faces</e1>, <e2>face</e2> ageing and rejuvenation, gender change, hair style change, etc."
sameAs(e1, e2)
Comment:

2450	"in the experiments, we show that our <e1>architecture</e1> can be used to perform complex manipulations at medium and high resolutions, and compare the proposed <e2>architecture</e2> with several adversarial learning-based baselines and recent methods for learning-based image manipulation."
sameAs(e1, e2)
Comment:

2451	"in this work, we show how <e1>these</e1> two ideas can be combined in a principled and non-additive manner for unaligned image translation <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

2452	"the new architecture, that we call a perceptual discriminator, embeds the convolutional parts of a pre-trained deep classification <e1>network</e1> inside the discriminator <e2>network</e2>."
sameAs(e1, e2)
Comment:

2453	"introduction generative convolutional neural networks have achieved remarkable success in <e1>image</e1> manipulation tasks both due to their ability to train on large amount of data [20, 23, 12] and due to natural <e2>image</e2> priors associated with such architectures [38] ."
sameAs(e1, e2)
Comment:

2454	"the disadvantage of such approach is <e1>that</e1> it could lead to features <e2>that</e2> are, while being repeatable, not necessarily suited for the matching task (see section 2.2)."
sameAs(e1, e2)
Comment:

2455	"on top of <e1>that</e1>, the common drawback of the yi et al [34] and lenc and vedaldi [30] methods is <e2>that</e2> they require to know the exact geometric relationship between patches which increases the amount of work needed to prepare the training dataset."
sameAs(e1, e2)
Comment:

2456	"zhang et al [29] proposed to "anchor" the detected <e1>features</e1> to some pre-defined <e2>features</e2> with known good discriminability like tilde [28] ."
sameAs(e1, e2)
Comment:

2457	"we remark that despite showing images of affine-covariant <e1>features</e1>, the results presented in the paper are for translation-covariant <e2>features</e2> only."
sameAs(e1, e2)
Comment:

2458	"finally, choy et al [35] trained a "universal <e1>correspondence</e1> network" (ucn) for a direct <e2>correspondence</e2> estimation with contrastive loss on a patch descriptor distance."
sameAs(e1, e2)
Comment:

2459	"this <e1>approach</e1> is related to the current work, yet the two <e2>methods</e2> differ in several important aspects."
Compare(e1, e2)
Comment:

2460	"while this could be a good setup for short <e1>baseline</e1> stereo, it does not work well for wide <e2>baseline</e2>, where affine features are usually sought."
sameAs(e1, e2)
Comment:

2461	"players perform steps atop a dance <e1>platform</e1>, following prompts from an on-screen step chart to step on the <e2>platform</e2>'s buttons at specific, musically salient points in time."
sameAs(e1, e2)
Comment:

2462	"step <e1>charts</e1> vary in difficulty with harder <e2>charts</e2> containing more steps and more complex sequences."
sameAs(e1, e2)
Comment:

2463	"step charts exhibit rich <e1>structure</e1> and complex <e2>semantics</e2> to ensure that step sequences are both challenging and enjoyable."
Conjunction(e1, e2)
Comment:

2464	"even when <e1>charts</e1> are available, players may tire of repeatedly performing the same <e2>charts</e2>."
sameAs(e1, e2)
Comment:

2465	"in this paper, we seek to automate the process of step <e1>chart</e1> generation so that players can dance to a wider variety of <e2>charts</e2> on any song of their choosing."
sameAs(e1, e2)
Comment:

2466	"although this <e1>task</e1> has previously been approached via ad-hoc methods, we are the first to cast it as a learning <e2>task</e2> in which we seek to mimic the semantics of human-generated charts."
sameAs(e1, e2)
Comment:

2467	"although this task has previously been approached via ad-hoc methods, <e1>we</e1> are the first to cast it as a learning task in which <e2>we</e2> seek to mimic the semantics of human-generated charts."
sameAs(e1, e2)
Comment:

2468	"while many step <e1>charts</e1> are available in standardized packs, players may grow tired of existing <e2>charts</e2>, or wish to dance to a song for which no chart exists."
sameAs(e1, e2)
Comment:

2469	"while many step <e1>charts</e1> are available in standardized packs, players may grow tired of existing charts, or wish to dance to a song for which no <e2>chart</e2> exists."
sameAs(e1, e2)
Comment:

2470	"while many step charts are available in standardized packs, players may grow tired of existing <e1>charts</e1>, or wish to dance to a song for which no <e2>chart</e2> exists."
sameAs(e1, e2)
Comment:

2471	"in addition to introducing a novel task and methodology, <e1>we</e1> contribute two large public datasets, which <e2>we</e2> consider to be of notably high quality and consistency."
sameAs(e1, e2)
Comment:

2472	"our best model for step placement jointly learns a convolutional neural network (cnn) representation and a <e1>recurrent neural network</e1> (<e2>rnn</e2>), which integrates information across consecutive time slices."
sameAs(e1, e2)
Comment:

2473	"introduction the goal of image generation is to construct <e1>images</e1> that are as barely distinguishable from target <e2>images</e2> which may contain general objects, diverse scenes, or human drawings."
sameAs(e1, e2)
Comment:

2474	"synthesized <e1>images</e1> can contribute to a number of applications such as the image to image translation [7] , image super-resolution [13] , 3d object modeling [36] , unsupervised domain adaptation [15] , domain transfer [39] , future frame prediction [33] , image inpainting [38] , image editing [43] , and feature recovering of astrophysical <e2>images</e2> [29] ."
sameAs(e1, e2)
Comment:

2475	"synthesized images can contribute to a number of applications such as the <e1>image</e1> to <e2>image</e2> translation [7] , image super-resolution [13] , 3d object modeling [36] , unsupervised domain adaptation [15] , domain transfer [39] , future frame prediction [33] , image inpainting [38] , image editing [43] , and feature recovering of astrophysical images [29] ."
sameAs(e1, e2)
Comment:

2476	"synthesized images can contribute to a number of applications such as the <e1>image</e1> to image translation [7] , image super-resolution [13] , 3d object modeling [36] , unsupervised domain adaptation [15] , domain transfer [39] , future frame prediction [33] , image inpainting [38] , <e2>image</e2> editing [43] , and feature recovering of astrophysical images [29] ."
sameAs(e1, e2)
Comment:

2477	"synthesized images can contribute to a number of applications such as the image to <e1>image</e1> translation [7] , image super-resolution [13] , 3d object modeling [36] , unsupervised domain adaptation [15] , domain transfer [39] , future frame prediction [33] , image inpainting [38] , <e2>image</e2> editing [43] , and feature recovering of astrophysical images [29] ."
sameAs(e1, e2)
Comment:

2478	"in this paper, we introduce a new <e1>image</e1> generation problem: a holistic <e2>image</e2> generation conditioned on a small number of local patches of objects or scenes without any geometry prior."
sameAs(e1, e2)
Comment:

2479	"in this paper, we introduce a new image <e1>generation</e1> problem: a holistic image <e2>generation</e2> conditioned on a small number of local patches of objects or scenes without any geometry prior."
sameAs(e1, e2)
Comment:

2480	"while the problem is related to <e1>image completion</e1> and scene understanding <e2>tasks</e2>, it is more general and challenging than each of these problems due to following reasons."
isA(e1, e2)
Comment:

2481	"our approach obtains key regions without any supervision such that the whole <e1>algorithm</e1> is developed within the unsupervised learning <e2>framework</e2>."
Part-of(e1, e2)
Comment:

2482	"third, the generated <e1>image</e1> should look closely to a real <e2>image</e2> in the target category."
sameAs(e1, e2)
Comment:

2483	"the generative adversarial network (gan) contains <e1>two</e1> networks which are trained based on the min-max game of <e2>two</e2> players."
sameAs(e1, e2)
Comment:

2484	"a generator network typically generates fake <e1>images</e1> and aims to fool a discriminator, while a discriminator network seeks to distinguish fake <e2>images</e2> from real images."
sameAs(e1, e2)
Comment:

2485	"a generator network typically generates fake <e1>images</e1> and aims to fool a discriminator, while a discriminator network seeks to distinguish fake images from real <e2>images</e2>."
sameAs(e1, e2)
Comment:

2486	"a generator network typically generates fake images and aims to fool a discriminator, while a discriminator network seeks to distinguish fake <e1>images</e1> from real <e2>images</e2>."
sameAs(e1, e2)
Comment:

2487	"such labeling <e1>task</e1> is labor intensive since gan-based algorithms need a large amount of training <e2>data</e2> to achieve high-quality results."
Conjunction(e1, e2)
Comment:

2488	"in contrast, experiments on seven challenging datasets <e1>that</e1> contain different objects and scenes, such as faces, cars, flowers, ceramics, and waterfalls, demonstrate <e2>that</e2> the proposed unsupervised algorithm generates realistic images and predict part locations well."
sameAs(e1, e2)
Comment:

2489	"among these approaches, quantization based <e1>methods</e1> represent the network weights with very low precision, thus yielding highly compact dnn <e2>models</e2> compared to their floating-point counterparts."
Compare(e1, e2)
Comment:

2490	"moreover, it has been shown that if both the network weights and activations are properly quantized, the convolution <e1>operations</e1> can be efficiently computed via bitwise <e2>operations</e2> [39, 21] , enabling fast inference without gpu."
sameAs(e1, e2)
Comment:

2491	"notwithstanding the promising <e1>results</e1> achieved by the existing quantizationbased <e2>methods</e2> [6, 34, 29, 54, 52, 21, 39, 53, 22, 3, 55, 9, 18] , there is still a sizeable accuracy gap between the quantized dnns and their full-precision counterparts, especially when quantized with extremely low bit-widths such as 1 bit or 2 bits."
Compare(e1, e2)
Comment:

2492	"notwithstanding the promising results achieved by the existing quantizationbased methods [6, 34, 29, 54, 52, 21, 39, 53, 22, 3, 55, 9, 18] , there is still a sizeable <e1>accuracy</e1> gap between the quantized dnns and their full-<e2>precision</e2> counterparts, especially when quantized with extremely low bit-widths such as 1 bit or 2 bits."
Conjunction(e1, e2)
Comment:

2493	"notwithstanding the promising results achieved by the existing quantizationbased methods [6, 34, 29, 54, 52, 21, 39, 53, 22, 3, 55, 9, 18] , there is still a sizeable <e1>accuracy</e1> gap between the quantized dnns and their full-precision <e2>counterparts</e2>, especially when quantized with extremely low bit-widths such as 1 bit or 2 bits."
Evaluate-for(e1, e2)
Comment:

2494	"for example, using the state-of-the-art <e1>method</e1> of [3] , a 50-layer resnet <e2>model</e2> [15] with 1-bit weights and 2-bit activations can achieve 64.6% top-1 image classification accuracy on imagenet validation set [40] ."
Used-for(e1, e2)
Comment:

2495	"this work is devoted to pushing the limit of network quantization algorithms to achieve better <e1>accuracy</e1> with low <e2>precision</e2> weights and activations."
Conjunction(e1, e2)
Comment:

2496	"although weight and activation quantization is an effective approach for deep neural network (dnn) compression and has a lot of potentials to increase inference speed leveraging bit-operations, there is still a noticeable gap in terms of prediction accuracy between the quantized <e1>model</e1> and the full-precision <e2>model</e2>."
sameAs(e1, e2)
Comment:

2497	"to address this gap, we propose to jointly train a quantized, bit-operation-compatible dnn and its associated quantizers, as opposed to using fixed, handcrafted <e1>quantization</e1> schemes such as uniform or logarithmic <e2>quantization</e2>."
sameAs(e1, e2)
Comment:

2498	"for example, facial landmark detection can be applied to a large variety of tasks, including <e1>face recognition</e1> [74, 30] , head <e2>pose estimation</e2> [58] , facial reenactment [53] and 3d face reconstruction [28] , to name a few."
Used-for(e1, e2)
Comment:

2499	"a very typical framework is to construct <e1>features</e1> to depict the facial appearance and shape information by the convolutional neural networks (convnets) or hand-crafted <e2>features</e2>, and then learn a model, i.e., a regressor, to map the features to the landmark locations [64, 10, 7, 42, 72, 67, 40] ."
sameAs(e1, e2)
Comment:

2500	"a very typical framework is to construct <e1>features</e1> to depict the facial appearance and shape information by the convolutional neural networks (convnets) or hand-crafted features, and then learn a <e2>model</e2>, i.e., a regressor, to map the features to the landmark locations [64, 10, 7, 42, 72, 67, 40] ."
Used-for(e1, e2)
Comment:

2501	"a very typical framework is to construct <e1>features</e1> to depict the facial appearance and shape information by the convolutional neural networks (convnets) or hand-crafted features, and then learn a model, i.e., a regressor, to map the <e2>features</e2> to the landmark locations [64, 10, 7, 42, 72, 67, 40] ."
sameAs(e1, e2)
Comment:

2502	"a very typical framework is to construct features to depict the facial appearance and shape information by the convolutional neural networks (convnets) or hand-crafted <e1>features</e1>, and then learn a <e2>model</e2>, i.e., a regressor, to map the features to the landmark locations [64, 10, 7, 42, 72, 67, 40] ."
Used-for(e1, e2)
Comment:

2503	"a very typical framework is to construct features to depict the facial appearance and shape information by the convolutional neural networks (convnets) or hand-crafted <e1>features</e1>, and then learn a model, i.e., a regressor, to map the <e2>features</e2> to the landmark locations [64, 10, 7, 42, 72, 67, 40] ."
sameAs(e1, e2)
Comment:

2504	"motivated by the issue of large <e1>variance</e1> of different image styles, we propose a style-aggregated network (san) for facial landmark detection, which is insensitive to the large <e2>variance</e2> of image styles."
sameAs(e1, e2)
Comment:

2505	"motivated by the issue of large variance of different <e1>image</e1> styles, we propose a style-aggregated network (san) for facial landmark detection, which is insensitive to the large variance of <e2>image</e2> styles."
sameAs(e1, e2)
Comment:

2506	"in <e1>face</e1> animation and reenactment methods, 2d landmarks are used as anchors to deform 3d <e2>face</e2> meshes toward realistic facial performances, so temporal jittering of 2d facial landmark detections in video will be propagated to the 3d face mesh and could generate perceptually jarring results."
sameAs(e1, e2)
Comment:

2507	"in <e1>face</e1> animation and reenactment methods, 2d landmarks are used as anchors to deform 3d face meshes toward realistic facial performances, so temporal jittering of 2d facial landmark detections in video will be propagated to the 3d <e2>face</e2> mesh and could generate perceptually jarring results."
sameAs(e1, e2)
Comment:

2508	"in face animation and reenactment methods, 2d landmarks are used as anchors to deform 3d <e1>face</e1> meshes toward realistic facial performances, so temporal jittering of 2d facial landmark detections in video will be propagated to the 3d <e2>face</e2> mesh and could generate perceptually jarring results."
sameAs(e1, e2)
Comment:

2509	"other <e1>methods</e1> that focus on video facial landmark detection [13, 22, 23] utilize both detections and tracking to combat jittering and increase precision, but these <e2>methods</e2> require per-frame annotations in video, which are (1) tedious to annotate due to the sheer volume of video frames and (2) difficult to annotate consistently across frames, even for temporally adjacent frames."
sameAs(e1, e2)
Comment:

2510	"other methods that focus on <e1>video</e1> facial landmark detection [13, 22, 23] utilize both detections and tracking to combat jittering and increase precision, but these methods require per-frame annotations in <e2>video</e2>, which are (1) tedious to annotate due to the sheer volume of video frames and (2) difficult to annotate consistently across frames, even for temporally adjacent frames."
sameAs(e1, e2)
Comment:

2511	"other methods that focus on <e1>video</e1> facial landmark detection [13, 22, 23] utilize both detections and tracking to combat jittering and increase precision, but these methods require per-frame annotations in video, which are (1) tedious to annotate due to the sheer volume of <e2>video</e2> frames and (2) difficult to annotate consistently across frames, even for temporally adjacent frames."
sameAs(e1, e2)
Comment:

2512	"other methods that focus on video facial landmark detection [13, 22, 23] utilize both detections and tracking to combat jittering and increase precision, but these methods require per-frame annotations in <e1>video</e1>, which are (1) tedious to annotate due to the sheer volume of <e2>video</e2> frames and (2) difficult to annotate consistently across frames, even for temporally adjacent frames."
sameAs(e1, e2)
Comment:

2513	"instead of completely relying on human annotations, we present <e1>supervision</e1>-by-registration (sbr), which augments the training loss function with <e2>supervision</e2> automatically extracted from unlabeled videos."
sameAs(e1, e2)
Comment:

2514	"to ensure that the <e1>supervision</e1> from registration is reasonable, <e2>supervision</e2> is only enforced for landmarks whose optical flow pass the forward-backward check [12] ."
sameAs(e1, e2)
Comment:

2515	"the final output of our method is an enhanced image-based facial landmark detector which has leveraged large amounts of unlabeled video to achieve higher precision in both images and <e1>videos</e1>, and more stable predictions in <e2>videos</e2>."
sameAs(e1, e2)
Comment:

2516	"the supervision-by-registration (sbr) framework takes labeled <e1>images</e1> and unlabeled video as input to train an image-based facial landmark detector which is more precise on <e2>images</e2>/video and also more stable on video."
sameAs(e1, e2)
Comment:

2517	"the supervision-by-registration (sbr) framework takes labeled images and unlabeled <e1>video</e1> as input to train an image-based facial landmark detector which is more precise on images/<e2>video</e2> and also more stable on video."
sameAs(e1, e2)
Comment:

2518	"the supervision-by-registration (sbr) framework takes labeled images and unlabeled <e1>video</e1> as input to train an image-based facial landmark detector which is more precise on images/video and also more stable on <e2>video</e2>."
sameAs(e1, e2)
Comment:

2519	"the supervision-by-registration (sbr) framework takes labeled images and unlabeled video as input to train an image-based facial landmark detector which is more precise on images/<e1>video</e1> and also more stable on <e2>video</e2>."
sameAs(e1, e2)
Comment:

2520	"essentially, supervisionby-<e1>registration</e1> augments the training loss function with a <e2>registration</e2> loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos."
sameAs(e1, e2)
Comment:

2521	"essentially, supervisionby-<e1>registration</e1> augments the training loss function with a registration loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with <e2>registration</e2> on large amounts of unlabeled videos."
sameAs(e1, e2)
Comment:

2522	"essentially, supervisionby-registration augments the <e1>training</e1> loss function with a registration loss, thus <e2>training</e2> the detector to have output that is not only close to the annotations in labeled images, but also consistent with registration on large amounts of unlabeled videos."
sameAs(e1, e2)
Comment:

2523	"essentially, supervisionby-registration augments the training loss function with a <e1>registration</e1> loss, thus training the detector to have output that is not only close to the annotations in labeled images, but also consistent with <e2>registration</e2> on large amounts of unlabeled videos."
sameAs(e1, e2)
Comment:

2524	"end-to-end training with the <e1>registration</e1> loss is made possible by a differentiable lucas-kanade operation, which computes optical flow <e2>registration</e2> in the forward pass, and back-propagates gradients that encourage temporal coherency in the detector."
sameAs(e1, e2)
Comment:

2525	"with supervision-by-registration, we demonstrate (1) improvements in facial landmark detection on both images (300w, alfw) and <e1>video</e1> (300vw, youtube-celebrities), and (2) significant reduction of jittering in <e2>video</e2> detections."
sameAs(e1, e2)
Comment:

2526	"introduction precise facial landmark detection lays the foundation for high quality performance of many computer vision and computer graphics tasks, such as face recognition [15] , <e1>face</e1> animation [2] and <e2>face</e2> reenactment [32] ."
sameAs(e1, e2)
Comment:

2527	"while these models continue to achieve steadily increasing <e1>accuracies</e1>, their robustness has not been thoroughly studied, thus little is known if the high <e2>accuracies</e2> come at the price of reduced robustness."
sameAs(e1, e2)
Comment:

2528	"while these models continue to achieve steadily increasing accuracies, their <e1>robustness</e1> has not been thoroughly studied, thus little is known if the high accuracies come at the price of reduced <e2>robustness</e2>."
sameAs(e1, e2)
Comment:

2529	"a common approach to evaluate the robustness of <e1>dnns</e1> is via adversarial attacks [3, 4, 5, 6, 7, 8, 9, 10, 11] , where imperceptible adversarial examples are crafted to mislead <e2>dnns</e2>."
sameAs(e1, e2)
Comment:

2530	"[16] proposes a robustness lower bound based on linear approximations of <e1>relu</e1> <e2>activations</e2>."
Feature-of(e1, e2)
Comment:

2531	"we also note that the adversarial <e1>robustness</e1> studied in this paper is different from [17] , where "<e2>robustness</e2>" is studied in the context of label semantics and accuracy."
sameAs(e1, e2)
Comment:

2532	"since the last <e1>imagenet</e1> challenge has ended in 2017, we are now at the beginning of post-<e2>imagenet</e2> era."
sameAs(e1, e2)
Comment:

2533	"in the course of evaluation, <e1>we</e1> have gained a number of insights and <e2>we</e2> summarize our contributions as follows: -tested on a large number of well-trained deep image classifiers, we find that robustness is scarified when solely pursuing a higher classification performance."
sameAs(e1, e2)
Comment:

2534	"in the course of evaluation, <e1>we</e1> have gained a number of insights and we summarize our contributions as follows: -tested on a large number of well-trained deep image classifiers, <e2>we</e2> find that robustness is scarified when solely pursuing a higher classification performance."
sameAs(e1, e2)
Comment:

2535	"in the course of evaluation, we have gained a <e1>number</e1> of insights and we summarize our contributions as follows: -tested on a large <e2>number</e2> of well-trained deep image classifiers, we find that robustness is scarified when solely pursuing a higher classification performance."
sameAs(e1, e2)
Comment:

2536	"in the course of evaluation, we have gained a number of insights and <e1>we</e1> summarize our contributions as follows: -tested on a large number of well-trained deep image classifiers, <e2>we</e2> find that robustness is scarified when solely pursuing a higher classification performance."
sameAs(e1, e2)
Comment:

2537	"in the course of evaluation, we have gained a number of insights and we summarize our contributions as follows: -tested on a large number of well-trained deep image <e1>classifiers</e1>, we find that robustness is scarified when solely pursuing a higher <e2>classification performance</e2>."
Compare(e1, e2)
Comment:

2538	"we advocate that imagenet network designers should evaluate model <e1>robustness</e1> via our disclosed accuracy-<e2>robustness</e2> pareto frontier."
sameAs(e1, e2)
Comment:

2539	"-the <e1>adversarial examples</e1> generated by the vgg family can transfer very well to all the other 17 models, while most <e2>adversarial examples</e2> of other models can only transfer within the same model family."
sameAs(e1, e2)
Comment:

2540	"-the adversarial examples generated by the vgg <e1>family</e1> can transfer very well to all the other 17 models, while most adversarial examples of other models can only transfer within the same model <e2>family</e2>."
sameAs(e1, e2)
Comment:

2541	"-the adversarial examples generated by the vgg family can <e1>transfer</e1> very well to all the other 17 models, while most adversarial examples of other models can only <e2>transfer</e2> within the same model family."
sameAs(e1, e2)
Comment:

2542	"-the adversarial examples generated by the vgg family can transfer very well to all the <e1>other</e1> 17 models, while most adversarial examples of <e2>other</e2> models can only transfer within the same model family."
sameAs(e1, e2)
Comment:

2543	"-the adversarial examples generated by the vgg family can transfer very well to all the other 17 <e1>models</e1>, while most adversarial examples of other <e2>models</e2> can only transfer within the same model family."
sameAs(e1, e2)
Comment:

2544	"to demystify the trade-offs between <e1>robustness</e1> and accuracy, in this paper we thoroughly benchmark 18 imagenet models using multiple <e2>robustness</e2> metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of models."
sameAs(e1, e2)
Comment:

2545	"to demystify the trade-offs between robustness and accuracy, in this paper we thoroughly benchmark 18 imagenet <e1>models</e1> using multiple robustness metrics, including the distortion, success rate and transferability of adversarial examples between 306 pairs of <e2>models</e2>."
sameAs(e1, e2)
Comment:

2546	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) <e1>model</e1> architecture is a more critical factor to robustness than <e2>model</e2> size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2547	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) <e1>model</e1> architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet <e2>model</e2> designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2548	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model <e1>architecture</e1> is a more critical factor to robustness than model size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network <e2>architecture</e2>, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2549	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to <e1>robustness</e1> than model size, and the disclosed accuracy-<e2>robustness</e2> pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2550	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to <e1>robustness</e1> than model size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network architecture, increasing network depth slightly improves <e2>robustness</e2> in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2551	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than <e1>model</e1> size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet <e2>model</e2> designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2552	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-<e1>robustness</e1> pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network architecture, increasing network depth slightly improves <e2>robustness</e2> in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2553	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar <e1>network</e1> architecture, increasing <e2>network</e2> depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg family) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same family."
sameAs(e1, e2)
Comment:

2554	"our extensive experimental results reveal several new insights: (1) linear scaling law -the empirical ℓ2 and ℓ∞ distortion metrics scale linearly with the logarithm of classification error; (2) model architecture is a more critical factor to robustness than model size, and the disclosed accuracy-robustness pareto frontier can be used as an evaluation criterion for imagenet model designers; (3) for a similar network architecture, increasing network depth slightly improves robustness in ℓ∞ distortion; (4) there exist models (in vgg <e1>family</e1>) that exhibit high adversarial transferability, while most adversarial examples crafted from one model can only be transferred within the same <e2>family</e2>."
sameAs(e1, e2)
Comment:

2555	"introduction image classification is a fundamental problem in computer vision and serves as the foundation of multiple tasks such as object detection, <e1>image segmentation</e1>, object tracking, <e2>action recognition</e2>, and autonomous driving."
Used-for(e1, e2)
Comment:

2556	"we postulate that the residuals are highly <e1>structured</e1> and reflect limitations in model capacity -we therefore propose to estimate the reconstruction uncertainty using a <e2>structured</e2> gaussian model, to capture pixel-wise correlations which will in turn improve the sampled reconstructions, as shown in fig."
sameAs(e1, e2)
Comment:

2557	"we postulate that the residuals are highly structured and reflect limitations in <e1>model</e1> capacity -we therefore propose to estimate the reconstruction uncertainty using a structured gaussian <e2>model</e2>, to capture pixel-wise correlations which will in turn improve the sampled reconstructions, as shown in fig."
sameAs(e1, e2)
Comment:

2558	"for the rest of the <e1>graph</e1>, the edges are bidirectional (i.e., the <e2>graph</e2> matrix is symmetric)."
sameAs(e1, e2)
Comment:

2559	"finally, by comparing the <e1>results</e1> of these <e2>methods</e2> on imagenet and the yfcc100m dataset [33] , we highlight how these methods exhibit some artificial aspects of imagenet that can influence the performance of low shot learning algorithms."
Compare(e1, e2)
Comment:

2560	"finally, by comparing the <e1>results</e1> of these methods on imagenet and the yfcc100m dataset [33] , we highlight how these <e2>methods</e2> exhibit some artificial aspects of imagenet that can influence the performance of low shot learning algorithms."
Compare(e1, e2)
Comment:

2561	"finally, by comparing the results of <e1>these</e1> methods on imagenet and the yfcc100m dataset [33] , we highlight how <e2>these</e2> methods exhibit some artificial aspects of imagenet that can influence the performance of low shot learning algorithms."
sameAs(e1, e2)
Comment:

2562	"finally, by comparing the results of these <e1>methods</e1> on imagenet and the yfcc100m dataset [33] , we highlight how these <e2>methods</e2> exhibit some artificial aspects of imagenet that can influence the performance of low shot learning algorithms."
sameAs(e1, e2)
Comment:

2563	"finally, by comparing the results of these methods on <e1>imagenet</e1> and the yfcc100m dataset [33] , we highlight how these methods exhibit some artificial aspects of <e2>imagenet</e2> that can influence the performance of low shot learning algorithms."
sameAs(e1, e2)
Comment:

2564	"in more detail, <e1>we</e1> make the following contributions: • <e2>we</e2> carry out a large-scale evaluation for diffusion methods for semi-supervised learning and compare it to recent low-shot learning papers."
sameAs(e1, e2)
Comment:

2565	"• <e1>we</e1> show that our approach is efficient and that the diffusion process scales up to hundreds of millions of images, which is order(s) of magnitude larger than what <e2>we</e2> are aware in the literature on image-based diffusion [19, 18] ."
sameAs(e1, e2)
Comment:

2566	"• we show <e1>that</e1> our approach is efficient and <e2>that</e2> the diffusion process scales up to hundreds of millions of images, which is order(s) of magnitude larger than what we are aware in the literature on image-based diffusion [19, 18] ."
sameAs(e1, e2)
Comment:

2567	"we propose a simple way to estimate it without <e1>this</e1> prior knowledge, and extend <e2>this</e2> assumption to a multiclass setting by introducing a probabilistic projection step derived from sinkhorn-knopp algorithm."
sameAs(e1, e2)
Comment:

2568	"• our experimental study shows that a simple propagation process significantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i) the <e1>number</e1> of annotated images per class is small and when (ii) the <e2>number</e2> of unlabeled images is large or the unlabeled images come form the same domain as the test images."
sameAs(e1, e2)
Comment:

2569	"• our experimental study shows that a simple propagation process significantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i) the number of annotated images per class is small and when (ii) the number of unlabeled <e1>images</e1> is large or the unlabeled <e2>images</e2> come form the same domain as the test images."
sameAs(e1, e2)
Comment:

2570	"• our experimental study shows that a simple propagation process significantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i) the number of annotated images per class is small and when (ii) the number of unlabeled <e1>images</e1> is large or the unlabeled images come form the same domain as the test <e2>images</e2>."
sameAs(e1, e2)
Comment:

2571	"• our experimental study shows that a simple propagation process significantly outperforms some state-ofthe-art approaches in low-shot visual learning when (i) the number of annotated images per class is small and when (ii) the number of unlabeled images is large or the unlabeled <e1>images</e1> come form the same domain as the test <e2>images</e2>."
sameAs(e1, e2)
Comment:

2572	"thus if <e1>we</e1> want to learn them, <e2>we</e2> must do so with few labeled examples."
sameAs(e1, e2)
Comment:

2573	"in order to learn new <e1>classes</e1> with little supervision, a standard approach is to leverage classifiers already learned for the most frequent <e2>classes</e2>, employing a so-called transfer learning strategy."
sameAs(e1, e2)
Comment:

2574	"in order to learn new classes with little supervision, a standard <e1>approach</e1> is to leverage <e2>classifiers</e2> already learned for the most frequent classes, employing a so-called transfer learning strategy."
Used-for(e1, e2)
Comment:

2575	"in this paper, <e1>we</e1> consider the low-shot learning problem described above, where the goal is to learn to detect new visual classes with only a few annotated images per class, but <e2>we</e2> also assume that we have many unlabelled images."
sameAs(e1, e2)
Comment:

2576	"in this paper, <e1>we</e1> consider the low-shot learning problem described above, where the goal is to learn to detect new visual classes with only a few annotated images per class, but we also assume that <e2>we</e2> have many unlabelled images."
sameAs(e1, e2)
Comment:

2577	"in this paper, we consider the low-shot learning problem described above, where the goal is to learn to detect new visual classes with only a few annotated images per class, but <e1>we</e1> also assume that <e2>we</e2> have many unlabelled images."
sameAs(e1, e2)
Comment:

2578	"in this paper, <e1>we</e1> propose a graphbit method to eliminate the ambiguity through bitwise interaction mining, where <e2>we</e2> represent binary codes in a directed acyclic graph."
sameAs(e1, e2)
Comment:

2579	"specifically, we design a deep reinforcement learning model to learn the structure of the <e1>graph</e1> for bitwise interaction mining, reducing the uncertainty of binary codes by maximizing the mutual information with inputs and related bits, so that the ambiguous bits receive additional instruction from the <e2>graph</e2> for confident binarization."
sameAs(e1, e2)
Comment:

2580	"introduction extracting effective descriptors is <e1>one</e1> of the most active issues in computer vision, which is widely applicable in numerous applications, such as face recognition [31, 42, 33] , image classification [17, 27] , object recognition [15, 30] and many <e2>others</e2>."
Conjunction(e1, e2)
Comment:

2581	"abstract introduction over the last few years, <e1>deep learning</e1> and convolutional neural networks (cnns) have become state-of-theart methods for visual recognition, including image classification [34, 56, 28] , <e2>object detection</e2> [21, 20, 10] or semantic segmentation [8, 42, 9] ."
Used-for(e1, e2)
Comment:

2582	"we propose a new <e1>pooling</e1> strategy (right of figure 2 ) which generalizes several approaches in the literature, including (top) max <e2>pooling</e2> [44, 39] , global average pooling [70] or negative evidence models [47, 12, 13] ."
sameAs(e1, e2)
Comment:

2583	"we propose a new <e1>pooling</e1> strategy (right of figure 2 ) which generalizes several approaches in the literature, including (top) max pooling [44, 39] , global average <e2>pooling</e2> [70] or negative evidence models [47, 12, 13] ."
sameAs(e1, e2)
Comment:

2584	"we propose a new pooling strategy (right of figure 2 ) which generalizes several approaches in the literature, including (top) max <e1>pooling</e1> [44, 39] , global average <e2>pooling</e2> [70] or negative evidence models [47, 12, 13] ."
sameAs(e1, e2)
Comment:

2585	"abstract recent studies demonstrate the effectiveness of <e1>recurrent neural networks</e1> (<e2>rnns</e2>) for action recognition in videos."
sameAs(e1, e2)
Comment:

2586	"experimental <e1>results</e1> show that rpan outperforms the recent state-of-the-art <e2>methods</e2> on these challenging datasets."
Compare(e1, e2)
Comment:

2587	"however, most existing attention approaches only utilize video-level category as supervision to train <e1>rnns</e1>, which may lack a detailed and dynamical guidance (such as human movement over time), and consequently restrict their capacity of <e2>modeling</e2> complex motions in videos."
Used-for(e1, e2)
Comment:

2588	"however, previous works mainly utilize video-level category as supervision to train <e1>rnns</e1>, which may prohibit <e2>rnns</e2> to learn complex motion structures along time."
sameAs(e1, e2)
Comment:

2589	"1 , <e1>human poses</e1> of different actors are closely related to the saliency regions in the average of convolutional feature maps estimated by cnn, and different joints of <e2>human pose</e2> can also be highly activated in certain individual feature maps."
sameAs(e1, e2)
Comment:

2590	"1 , human poses of different actors are closely related to the saliency regions in the average of convolutional <e1>feature</e1> maps estimated by cnn, and different joints of human pose can also be highly activated in certain individual <e2>feature</e2> maps."
sameAs(e1, e2)
Comment:

2591	"1 , human poses of different actors are closely related to the saliency regions in the average of convolutional feature <e1>maps</e1> estimated by cnn, and different joints of human pose can also be highly activated in certain individual feature <e2>maps</e2>."
sameAs(e1, e2)
Comment:

2592	"inspired by <e1>this</e1> analysis, <e2>this</e2> paper proposes a novel recurrent pose-attention network (rpan) for action recognition in videos, which can adaptively learn a highlydiscriminative pose-related feature for every-step action prediction of lstm."
sameAs(e1, e2)
Comment:

2593	"inspired by this analysis, this paper proposes a novel recurrent <e1>pose</e1>-attention network (rpan) for action recognition in videos, which can adaptively learn a highlydiscriminative <e2>pose</e2>-related feature for every-step action prediction of lstm."
sameAs(e1, e2)
Comment:

2594	"firstly, unlike the previous works on pose-related action recognition, our rpan is an end-to-end recurrent network, which allows to take advantage of dy- we generate convolutional cube from the 5a layer (9 × 15 × 1024) in the spatial-stream of temporal segment net [47] , and then sum the convolutional cube over <e1>feature</e1> channels to obtain this averaged <e2>feature</e2> map."
sameAs(e1, e2)
Comment:

2595	"finally, the <e1>feature</e1> map with the highest-activated value at the joint location is selected as the highest-activated <e2>feature</e2> map for the corresponding joint."
sameAs(e1, e2)
Comment:

2596	"finally, the feature <e1>map</e1> with the highest-activated value at the joint location is selected as the highest-activated feature <e2>map</e2> for the corresponding joint."
sameAs(e1, e2)
Comment:

2597	"in <e1>this</e1> paper, we propose a recurrent pose-attention network (rpan) to address <e2>this</e2> challenge, where we introduce a novel pose-attention mechanism to adaptively learn pose-related features at every time-step action prediction of rnns."
sameAs(e1, e2)
Comment:

2598	"in this paper, <e1>we</e1> propose a recurrent pose-attention network (rpan) to address this challenge, where <e2>we</e2> introduce a novel pose-attention mechanism to adaptively learn pose-related features at every time-step action prediction of rnns."
sameAs(e1, e2)
Comment:

2599	"in this paper, we propose a recurrent <e1>pose</e1>-attention network (rpan) to address this challenge, where we introduce a novel <e2>pose</e2>-attention mechanism to adaptively learn pose-related features at every time-step action prediction of rnns."
sameAs(e1, e2)
Comment:

2600	"in this paper, we propose a recurrent <e1>pose</e1>-attention network (rpan) to address this challenge, where we introduce a novel pose-attention mechanism to adaptively learn <e2>pose</e2>-related features at every time-step action prediction of rnns."
sameAs(e1, e2)
Comment:

2601	"in this paper, we propose a recurrent pose-attention network (rpan) to address this challenge, where we introduce a novel <e1>pose</e1>-attention mechanism to adaptively learn <e2>pose</e2>-related features at every time-step action prediction of rnns."
sameAs(e1, e2)
Comment:

2602	"(d) the <e1>pose</e1>-attention-related heat maps and estimated <e2>poses</e2> of sampled video frames by our recurrent pose attention network (rpan)."
sameAs(e1, e2)
Comment:

2603	"(d) the <e1>pose</e1>-attention-related heat maps and estimated poses of sampled video frames by our recurrent <e2>pose</e2> attention network (rpan)."
sameAs(e1, e2)
Comment:

2604	"(d) the pose-attention-related heat maps and estimated <e1>poses</e1> of sampled video frames by our recurrent <e2>pose</e2> attention network (rpan)."
sameAs(e1, e2)
Comment:

2605	"firstly, unlike previous works on pose-related <e1>action recognition</e1>, our rpan is an end-toend recurrent network which can exploit important spatialtemporal evolutions of <e2>human pose</e2> to assist action recognition in a unified framework."
Used-for(e1, e2)
Comment:

2606	"firstly, unlike previous works on pose-related <e1>action recognition</e1>, our rpan is an end-toend recurrent network which can exploit important spatialtemporal evolutions of human pose to assist <e2>action recognition</e2> in a unified framework."
sameAs(e1, e2)
Comment:

2607	"secondly, instead of learning individual human-joint <e1>features</e1> separately, our poseattention mechanism learns robust human-part <e2>features</e2> by sharing attention parameters partially on the semanticallyrelated human joints."
sameAs(e1, e2)
Comment:

2608	"in the more general case, it is easy to conduct a simple experiment showing that ground-truth <e1>segmentation</e1> is a meaningful clue for detection, using for instance ground-truth <e2>segmentation</e2> as the input of an object detection pipeline."
sameAs(e1, e2)
Comment:

2609	"current techniques tackle this task by either (i) directly or recursively merging linguistic and visual information in the channel dimension and then performing <e1>convolutions</e1>; or by (ii) mapping the expression to a space in which it can be thought of as a filter, whose response is directly related to the presence of the object at a given spatial coordinate in the image, so that a <e2>convolution</e2> can be applied to look for the object."
sameAs(e1, e2)
Comment:

2610	"in contrast to traditional instance segmentation, in which the goal is to label all pixels belonging to <e1>instances</e1> in the image for a set of predefined semantic classes [1, 2] , segmenting <e2>instances</e2> described by a natural language expression is a task that humans are able to perform without specifically focusing on a limited set of categories: we simply associate a referring expression such as "man on the right" with what we see, as shown in fig."
sameAs(e1, e2)
Comment:

2611	"in contrast to traditional instance segmentation, in which the goal is to label all pixels belonging to instances in the image for a set of predefined semantic classes [1, 2] , segmenting instances described by a natural language expression is a task that humans are able to perform without specifically focusing on a limited set of categories: <e1>we</e1> simply associate a referring expression such as "man on the right" with what <e2>we</e2> see, as shown in fig."
sameAs(e1, e2)
Comment:

2612	"in particular, we show strong results on the <e1>task</e1> of walkable surface estimation and scene classification by using this dog modelling <e2>task</e2> as representation learning."
sameAs(e1, e2)
Comment:

2613	"introduction computer vision research typically focuses on a few well defined tasks including image classification, <e1>object recognition</e1>, object detection, <e2>image segmentation</e2>, etc."
isA(e1, e2)
Comment:

2614	"we value the undeniable impact of <e1>these</e1> proxy <e2>tasks</e2> in computer vision research and advocate the continuation of research on these fundamental problems."
Used-for(e1, e2)
Comment:

2615	"we value the undeniable impact of <e1>these</e1> proxy tasks in computer vision research and advocate the continuation of research on <e2>these</e2> fundamental problems."
sameAs(e1, e2)
Comment:

2616	"we value the undeniable impact of these proxy tasks in computer vision <e1>research</e1> and advocate the continuation of <e2>research</e2> on these fundamental problems."
sameAs(e1, e2)
Comment:

2617	"there is, however, a gap between the ideal outcome of <e1>these</e1> proxy <e2>tasks</e2> and the expected functionality of visually intelligent systems."
Used-for(e1, e2)
Comment:

2618	"inspired by recent work <e1>that</e1> explores the role of action and interaction in visual understanding [56, 3, 31] , we define the problem of visual intelligence as understanding visual data to the extent <e2>that</e2> an agent can take actions and perform tasks in the visual world."
sameAs(e1, e2)
Comment:

2619	"inspired by recent work that explores the role of action and interaction in visual <e1>understanding</e1> [56, 3, 31] , we define the problem of visual intelligence as <e2>understanding</e2> visual data to the extent that an agent can take actions and perform tasks in the visual world."
sameAs(e1, e2)
Comment:

2620	"thus <e1>we</e1> are modelling a black box where <e2>we</e2> only know the inputs and outputs of the system."
sameAs(e1, e2)
Comment:

2621	"in learning to plan like a dog, we address the problem of estimating a sequence of movements that take the <e1>state</e1> of the dog's world from what is observed at a given time to a desired observed <e2>state</e2>."
sameAs(e1, e2)
Comment:

2622	"first, <e1>applications</e1> to new tasks may be quickly developed, with much of the new work lying in defining an appropriate training set and loss function; in this light, our work is a step towards building offthe-shelf regressor models that can be used for many <e2>applications</e2>."
sameAs(e1, e2)
Comment:

2623	"in this paper, we address three of <e1>these</e1> <e2>tasks</e2>, depth prediction, surface normal estimation and semantic segmentation -all using a single common architecture."
Used-for(e1, e2)
Comment:

2624	"our multiscale approach generates pixel-maps directly from an input <e1>image</e1>, without the need for low-level superpixels or contours, and is able to align to many <e2>image</e2> details using a series of convolutional network stacks applied at increasing resolution."
sameAs(e1, e2)
Comment:

2625	"at test <e1>time</e1>, all three outputs can be generated in real <e2>time</e2> (∼30hz)."
sameAs(e1, e2)
Comment:

2626	"abstract recent studies show that the <e1>state</e1>-of-the-art deep neural networks (dnns) introduction deep neural networks (dnns) have achieved <e2>state</e2>-ofthe-art, and sometimes human-competitive, performance on many computer vision tasks [11, 14, 36] ."
sameAs(e1, e2)
Comment:

2627	"abstract recent studies show that the state-of-the-art <e1>deep neural networks</e1> (dnns) introduction <e2>deep neural networks</e2> (dnns) have achieved state-ofthe-art, and sometimes human-competitive, performance on many computer vision tasks [11, 14, 36] ."
sameAs(e1, e2)
Comment:

2628	"abstract recent studies show that the state-of-the-art deep neural networks (<e1>dnns</e1>) introduction deep neural networks (<e2>dnns</e2>) have achieved state-ofthe-art, and sometimes human-competitive, performance on many computer vision tasks [11, 14, 36] ."
sameAs(e1, e2)
Comment:

2629	"(4) a reasonable threat model for transportation is <e1>that</e1> an attacker might not have control over a vehicle's systems, but is able to modify the objects in the physical world <e2>that</e2> a vehicle might depend on to make crucial safety decisions."
sameAs(e1, e2)
Comment:

2630	"additionally, other practicality challenges exist: (1) perturbations in the digital world can be so small in magnitude <e1>that</e1> it is likely <e2>that</e2> a camera will not be able to perceive them due to sensor imperfections."
sameAs(e1, e2)
Comment:

2631	"using the proposed <e1>algorithm</e1>, we evaluate the effectiveness of perturbations on physical objects, and show that adversaries can physically modify objects using low-cost <e2>techniques</e2> to reliably cause classification errors in dnnbased classifiers under widely varying distances and angles."
Compare(e1, e2)
Comment:

2632	"using the proposed algorithm, we evaluate the effectiveness of perturbations on physical <e1>objects</e1>, and show that adversaries can physically modify <e2>objects</e2> using low-cost techniques to reliably cause classification errors in dnnbased classifiers under widely varying distances and angles."
sameAs(e1, e2)
Comment:

2633	"rp 2 samples from a distribution <e1>that</e1> models physical dynamics (in this case, varying distances and angles), and uses a mask to project computed perturbations to a shape <e2>that</e2> resembles graffiti."
sameAs(e1, e2)
Comment:

2634	"physical attacks, <e1>we</e1> draw on standard techniques from the physical sciences and propose a two-stage experiment design: (1) a lab test where the viewing camera is kept at various distance/angle configurations; and (2) a field test where <e2>we</e2> drive a car towards an intersection in uncontrolled conditions to simulate an autonomous vehicle."
sameAs(e1, e2)
Comment:

2635	"2. given the lack of a standardized <e1>methodology</e1> in evaluating physical adversarial perturbations, we propose an evaluation <e2>methodology</e2> to study the effectiveness of physical perturbations in real world scenarios (section 4.2)."
sameAs(e1, e2)
Comment:

2636	"3. <e1>we</e1> evaluate our attacks against two standardarchitecture classifiers that <e2>we</e2> built: lisa-cnn with 91% accuracy on the lisa test set and gtsrb-cnn with 95.7% accuracy on the gtsrb test set."
sameAs(e1, e2)
Comment:

2637	"3. we evaluate our attacks against two standardarchitecture classifiers that we built: lisa-<e1>cnn</e1> with 91% accuracy on the lisa test set and gtsrb-<e2>cnn</e2> with 95.7% accuracy on the gtsrb test set."
sameAs(e1, e2)
Comment:

2638	"3. we evaluate our attacks against two standardarchitecture classifiers that we built: lisa-cnn with 91% <e1>accuracy</e1> on the lisa test set and gtsrb-cnn with 95.7% <e2>accuracy</e2> on the gtsrb test set."
sameAs(e1, e2)
Comment:

2639	"using two types of attacks (object-constrained poster and sticker attacks) <e1>that</e1> we introduce, we show <e2>that</e2> rp 2 produces robust perturbations for real road signs."
sameAs(e1, e2)
Comment:

2640	"using two types of attacks (object-constrained poster and sticker attacks) that <e1>we</e1> introduce, <e2>we</e2> show that rp 2 produces robust perturbations for real road signs."
sameAs(e1, e2)
Comment:

2641	"for example, poster attacks are successful in 100% of stationary and drive-by tests against lisa-<e1>cnn</e1>, and sticker attacks are successful in 80% of stationary testing conditions and in 87.5% of the extracted video frames against gtsrb-<e2>cnn</e2>."
sameAs(e1, e2)
Comment:

2642	"to which degree an <e1>image</e1> is later remembered or forgotten is expressed as <e2>image</e2> memorability."
sameAs(e1, e2)
Comment:

2643	"an application of a great interest is to measure a decline in memory capacity of patients affected by de- prior research [13] has shown <e1>that</e1> image memorability has a stable property, <e2>that</e2> is, individuals tend to remember the same images with the same probability regardless of delays, and that it can be quantified and measured."
sameAs(e1, e2)
Comment:

2644	"an application of a great interest is to measure a decline in memory capacity of patients affected by de- prior research [13] has shown <e1>that</e1> image memorability has a stable property, that is, individuals tend to remember the same images with the same probability regardless of delays, and <e2>that</e2> it can be quantified and measured."
sameAs(e1, e2)
Comment:

2645	"an application of a great interest is to measure a decline in memory capacity of patients affected by de- prior research [13] has shown that image memorability has a stable property, <e1>that</e1> is, individuals tend to remember the same images with the same probability regardless of delays, and <e2>that</e2> it can be quantified and measured."
sameAs(e1, e2)
Comment:

2646	"however, they generally suffer performance decline in the challenging conditional few-shot learning scenario, where <e1>training</e1> samples for each condition are limited due to the high dimension of the condition space although the total number of <e2>training</e2> samples can be large."
sameAs(e1, e2)
Comment:

2647	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this <e1>task</e1> would help alleviate expensive and labourintensive <e2>data</e2> collection and labeling as they would not require massive labelled training data to achieve reasonable performance; 2) the target data in practice usually have a large number of different categories but very few examples per category."
Conjunction(e1, e2)
Comment:

2648	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this <e1>task</e1> would help alleviate expensive and labourintensive data collection and labeling as they would not require massive labelled training <e2>data</e2> to achieve reasonable performance; 2) the target data in practice usually have a large number of different categories but very few examples per category."
Conjunction(e1, e2)
Comment:

2649	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this <e1>task</e1> would help alleviate expensive and labourintensive data collection and labeling as they would not require massive labelled training data to achieve reasonable performance; 2) the target <e2>data</e2> in practice usually have a large number of different categories but very few examples per category."
Conjunction(e1, e2)
Comment:

2650	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this task would help alleviate expensive and labourintensive <e1>data</e1> collection and labeling as they would not require massive labelled training <e2>data</e2> to achieve reasonable performance; 2) the target data in practice usually have a large number of different categories but very few examples per category."
sameAs(e1, e2)
Comment:

2651	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this task would help alleviate expensive and labourintensive <e1>data</e1> collection and labeling as they would not require massive labelled training data to achieve reasonable performance; 2) the target <e2>data</e2> in practice usually have a large number of different categories but very few examples per category."
sameAs(e1, e2)
Comment:

2652	"few-shot learning is of great significance both academically and industrially, since 1) models excelling at this task would help alleviate expensive and labourintensive data collection and labeling as they would not require massive labelled training <e1>data</e1> to achieve reasonable performance; 2) the target <e2>data</e2> in practice usually have a large number of different categories but very few examples per category."
sameAs(e1, e2)
Comment:

2653	"in this paper, we mainly focus on improving two kinds of models in the conditional few-shot learning scenario, i.e., the discriminative <e1>one</e1> and the generative <e2>one</e2>."
sameAs(e1, e2)
Comment:

2654	"the <e1>generative models</e1> often leverage data <e2>generative models</e2>, e.g., generative adversarial networks (gans) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2655	"the generative models often leverage <e1>data</e1> generative models, e.g., generative adversarial networks (gans) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training <e2>data</e2> for data augmentation."
sameAs(e1, e2)
Comment:

2656	"the generative models often leverage data generative models, e.g., <e1>generative adversarial networks</e1> (<e2>gans</e2>) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2657	"the generative models often leverage data generative models, e.g., <e1>generative adversarial networks</e1> (gans) [10] , conditional generative adversarial networks (conditional-<e2>gans</e2>) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2658	"the generative models often leverage data generative models, e.g., <e1>generative adversarial networks</e1> (gans) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium <e2>generative adversarial networks</e2> (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2659	"the generative models often leverage data generative models, e.g., <e1>generative adversarial networks</e1> (gans) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-<e2>gans</e2>) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2660	"the generative models often leverage data generative models, e.g., generative adversarial networks (<e1>gans</e1>) [10] , conditional generative adversarial networks (conditional-<e2>gans</e2>) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2661	"the generative models often leverage data generative models, e.g., generative adversarial networks (<e1>gans</e1>) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-<e2>gans</e2>) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2662	"the generative models often leverage data generative models, e.g., generative adversarial networks (gans) [10] , <e1>conditional generative adversarial networks</e1> (conditional-<e2>gans</e2>) [24] , boundary equilibrium generative adversarial networks (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
Feature-of(e1, e2)
Comment:

2663	"the generative models often leverage data generative models, e.g., generative adversarial networks (gans) [10] , <e1>conditional generative adversarial networks</e1> (conditional-gans) [24] , boundary equilibrium <e2>generative adversarial networks</e2> (be-gans) [2] , etc., for synthesizing auxiliary training data for data augmentation."
Feature-of(e1, e2)
Comment:

2664	"the generative models often leverage data generative models, e.g., generative adversarial networks (gans) [10] , <e1>conditional generative adversarial networks</e1> (conditional-gans) [24] , boundary equilibrium generative adversarial networks (be-<e2>gans</e2>) [2] , etc., for synthesizing auxiliary training data for data augmentation."
Feature-of(e1, e2)
Comment:

2665	"the generative models often leverage data generative models, e.g., generative adversarial networks (gans) [10] , conditional generative adversarial networks (conditional-<e1>gans</e1>) [24] , boundary equilibrium generative adversarial networks (be-<e2>gans</e2>) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2666	"the generative models often leverage data generative models, e.g., generative adversarial networks (gans) [10] , conditional generative adversarial networks (conditional-gans) [24] , boundary equilibrium <e1>generative adversarial networks</e1> (be-<e2>gans</e2>) [2] , etc., for synthesizing auxiliary training data for data augmentation."
sameAs(e1, e2)
Comment:

2667	"the conditions could be based on category labels, on some part of <e1>data</e1>, or even on <e2>data</e2> from different modalities."
sameAs(e1, e2)
Comment:

2668	"the proposed dccn outperforms other discriminative and generative conditional <e1>models</e1> for all the <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

2669	"(2) the dynamic convolution is achieved through linearly combining the basis filters of the filter bank in the dyconvnet with a set of adaptive weights predicted by the condinet from conditional <e1>inputs</e1>, which is different from existing conditional learning approaches that combine the two <e2>inputs</e2> through direct concatenation."
sameAs(e1, e2)
Comment:

2670	"for the ucf-101 dataset [34] which contains about 10 thousands videos, extracting optical flows for all data via the tv-l1 method takes <e1>one</e1> gpu-day, and storing them costs more than <e2>one</e2> terabyte of storage for the original fields as floats (often a linear jpeg normalization is required to save storage cost [33] )."
sameAs(e1, e2)
Comment:

2671	"to sum up, this paper makes the following contributions: • we develop a novel <e1>neural network</e1> to learn motions from videos by unfolding the iterations of the tv-l1 <e2>method</e2> to customized neural layers."
Used-for(e1, e2)
Comment:

2672	"we believe <e1>that</e1> the major obstacle is <e2>that</e2> the distinctive motion cues in videos demand some new network designs, which are yet to be found and tested."
sameAs(e1, e2)
Comment:

2673	"a currently successful example of applying optical <e1>flow</e1> to video understanding is the twostream model [33] , where a cnn is trained on the optical <e2>flow</e2> data to learn action patterns."
sameAs(e1, e2)
Comment:

2674	"various extensions of the two-stream model have been proposed and achieved <e1>state</e1>-ofthe-art results on serval tasks including action recognition [28, 9, 10, 40] and <e2>action</e2> detection [15, 29] ."
Conjunction(e1, e2)
Comment:

2675	"meanwhile, [16] , [9] , [11] take a learning-based approach to predict the <e1>orientations</e1> of local image patches, and then group the patches with similar <e2>orientations</e2> to form planar regions."
sameAs(e1, e2)
Comment:

2676	"meanwhile, [16] , [9] , [11] take a learning-based approach to predict the orientations of local image <e1>patches</e1>, and then group the <e2>patches</e2> with similar orientations to form planar regions."
sameAs(e1, e2)
Comment:

2677	"1 , the network takes a single <e1>image</e1> as input, and outputs (i) a segmentation map that identifies the planar surfaces in the <e2>image</e2> and (ii) the parameters of each plane in the 3d space, thus effectively creating a piecewise planar model for the scene."
sameAs(e1, e2)
Comment:

2678	"our key insight here is that, if <e1>we</e1> can correctly identify the planar regions in the image and predict the plane parameters, then <e2>we</e2> can also accurately infer the depth in these regions."
sameAs(e1, e2)
Comment:

2679	"our key insight here is that, if we can correctly identify the planar <e1>regions</e1> in the image and predict the plane parameters, then we can also accurately infer the depth in these <e2>regions</e2>."
sameAs(e1, e2)
Comment:

2680	"further, our method achieves real-<e1>time</e1> performance at the testing <e2>time</e2>, thus is suitable for a wide range of applications such as visual localization and mapping, and human-robot interaction."
sameAs(e1, e2)
Comment:

2681	"experiment <e1>results</e1> demonstrate that our method significantly outperforms existing <e2>methods</e2>, both qualitatively and quantitatively."
Compare(e1, e2)
Comment:

2682	"we build upon the adaptive computation time (act) [12] mechanism which was recently proposed for <e1>recurrent neural networks</e1> (<e2>rnns</e2>)."
sameAs(e1, e2)
Comment:

2683	"finally, we demonstrate <e1>that</e1> the obtained computation time maps are well-correlated with human eye fixations positions, suggesting <e2>that</e2> a reasonable attention model arises in the model automatically without any explicit supervision."
sameAs(e1, e2)
Comment:

2684	"finally, we demonstrate that the obtained computation time maps are well-correlated with human eye fixations positions, suggesting that a reasonable attention <e1>model</e1> arises in the <e2>model</e2> automatically without any explicit supervision."
sameAs(e1, e2)
Comment:

2685	"this makes such models unsuitable for multi-output <e1>problems</e1> (generating box proposals in object detection) and per-pixel prediction <e2>problems</e2> (image segmentation, image generation)."
sameAs(e1, e2)
Comment:

2686	"abstract we propose an algorithm for meta-learning <e1>that</e1> is model-agnostic, in the sense <e2>that</e2> it is compatible with any model trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning."
sameAs(e1, e2)
Comment:

2687	"abstract we propose an algorithm for meta-learning that is <e1>model</e1>-agnostic, in the sense that it is compatible with any <e2>model</e2> trained with gradient descent and applicable to a variety of different learning problems, including classification, regression, and reinforcement learning."
sameAs(e1, e2)
Comment:

2688	"the goal of meta-learning is to train a model on a variety of learning <e1>tasks</e1>, such that it can solve new learning <e2>tasks</e2> using only a small number of training samples."
sameAs(e1, e2)
Comment:

2689	"in our approach, the parameters of the model are explicitly trained such <e1>that</e1> a small number of gradient steps with a small amount of training data from a new task will produce good generalization performance on <e2>that</e2> task."
sameAs(e1, e2)
Comment:

2690	"in our approach, the parameters of the model are explicitly trained such that a small number of gradient steps with a small amount of training data from a new <e1>task</e1> will produce good generalization performance on that <e2>task</e2>."
sameAs(e1, e2)
Comment:

2691	"this simple form allows the rnn to be analyzed via straightforward linear methods: <e1>we</e1> can exactly characterize the linear contribution of each input to the model predictions; <e2>we</e2> can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task."
sameAs(e1, e2)
Comment:

2692	"this simple form allows the rnn to be analyzed via straightforward linear methods: <e1>we</e1> can exactly characterize the linear contribution of each input to the model predictions; we can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; <e2>we</e2> can fully reverse-engineer the architecture's solution to a simple task."
sameAs(e1, e2)
Comment:

2693	"this simple form allows the rnn to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each <e1>input</e1> to the model predictions; we can use a change-of-basis to disentangle <e2>input</e2>, output, and computational hidden unit subspaces; we can fully reverse-engineer the architecture's solution to a simple task."
sameAs(e1, e2)
Comment:

2694	"this simple form allows the rnn to be analyzed via straightforward linear methods: we can exactly characterize the linear contribution of each input to the model predictions; <e1>we</e1> can use a change-of-basis to disentangle input, output, and computational hidden unit subspaces; <e2>we</e2> can fully reverse-engineer the architecture's solution to a simple task."
sameAs(e1, e2)
Comment:

2695	"an example of a mask learned (right) by blurring an <e1>image</e1> (middle) to suppress the softmax probability of its target class (left: original <e2>image</e2>; softmax scores above images)."
sameAs(e1, e2)
Comment:

2696	"for example, one rule could be <e1>that</e1> f is rotation invariant, in the sense <e2>that</e2> "f (x) = f (x ′ ) whenever images x and x ′ are related by a rotation"."
sameAs(e1, e2)
Comment:

2697	"for example, one rule could be that f is <e1>rotation</e1> invariant, in the sense that "f (x) = f (x ′ ) whenever images x and x ′ are related by a <e2>rotation</e2>"."
sameAs(e1, e2)
Comment:

2698	"while artifacts are informative since they explain part of the <e1>network</e1> behavior, characterizing other properties of the <e2>network</e2> requires careful calibration of the generality and interpretability of explanations."
sameAs(e1, e2)
Comment:

2699	"a <e1>neural network</e1> object <e2>classifier</e2>."
Used-for(e1, e2)
Comment:

2700	"however, in <e1>this</e1> paper, we ignore <e2>this</e2> fine change in far structure, and represent the books in terms of their contribution to texture."
sameAs(e1, e2)
Comment:

2701	"when <e1>we</e1> see this image, <e2>we</e2> can easily recognize and compensate for the underlying 3d structure: for example, we have no trouble recognizing the orientation of the bookshelves and the floor."
sameAs(e1, e2)
Comment:

2702	"when <e1>we</e1> see this image, we can easily recognize and compensate for the underlying 3d structure: for example, <e2>we</e2> have no trouble recognizing the orientation of the bookshelves and the floor."
sameAs(e1, e2)
Comment:

2703	"when we see this image, <e1>we</e1> can easily recognize and compensate for the underlying 3d structure: for example, <e2>we</e2> have no trouble recognizing the orientation of the bookshelves and the floor."
sameAs(e1, e2)
Comment:

2704	"in <e1>this</e1> paper, we take <e2>this</e2> argument one step further: we claim that there is enough regularity in indoor scenes to learn a model for 3d scene understanding without ever seeing an explicit 3d label."
sameAs(e1, e2)
Comment:

2705	"in this paper, <e1>we</e1> take this argument one step further: <e2>we</e2> claim that there is enough regularity in indoor scenes to learn a model for 3d scene understanding without ever seeing an explicit 3d label."
sameAs(e1, e2)
Comment:

2706	"in this paper, we take this argument one step further: we claim that there is enough regularity in indoor <e1>scenes</e1> to learn a model for 3d <e2>scene</e2> understanding without ever seeing an explicit 3d label."
sameAs(e1, e2)
Comment:

2707	"recent gradient-based techniques for ho, however, have significantly increased the number of <e1>hyperparameters</e1> that can be optimized (domke, 2012; maclaurin et al, 2015; pedregosa, 2016; franceschi et al, 2017) and it is now possible to tune as <e2>hyperparameters</e2> entire weight vectors associated with a neural network layer."
sameAs(e1, e2)
Comment:

2708	"while in ho the available <e1>data</e1> is associated with a single task and split into a training set (used to tune the parameters) and a validation set (used to tune the hyperparameters), in ml we are often interested in the so-called few-shot learning setting where <e2>data</e2> comes in the form of short episodes (small datasets with few examples per class) sampled from a common probability distribution over supervised tasks."
sameAs(e1, e2)
Comment:

2709	"while in ho the available data is associated with a single <e1>task</e1> and split into a training set (used to tune the parameters) and a validation set (used to tune the hyperparameters), in ml we are often interested in the so-called few-shot learning setting where <e2>data</e2> comes in the form of short episodes (small datasets with few examples per class) sampled from a common probability distribution over supervised tasks."
Conjunction(e1, e2)
Comment:

2710	"a main contribution of this paper is a unified view of ho and ml within the natural mathematical framework of bilevel programming, where an outer <e1>optimization problem</e1> is solved subject to the optimality of an inner <e2>optimization problem</e2>."
sameAs(e1, e2)
Comment:

2711	"in ho the outer <e1>problem</e1> involves hyperparameters while the inner <e2>problem</e2> is usually the minimization of an empirical loss."
sameAs(e1, e2)
Comment:

2712	"in ml the outer <e1>problem</e1> could involve a shared representation among tasks while the inner <e2>problem</e2> could concern classifiers for individual tasks."
sameAs(e1, e2)
Comment:

2713	"in ml the outer problem could involve a shared representation among <e1>tasks</e1> while the inner problem could concern classifiers for individual <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

2714	"bilevel programming (bard, 2013) has been suggested before in machine learning in the context of <e1>kernel methods</e1> and <e2>support vector machines</e2> (keerthi et al, 2007; kunapuli et al, 2008) , multitask learning (flamary et al, 2014) , and more recently ho (pedregosa, 2016) , but never in the context of ml."
isA(e1, e2)
Comment:

2715	"a technical difficulty arises when the <e1>solution</e1> to the inner <e2>problem</e2> cannot be written analytically (for example this happens when using the log-loss for training neural networks) and one needs to resort to iterative optimization approaches."
Used-for(e1, e2)
Comment:

2716	"we provide sufficient conditions under which solutions of the approximate <e1>problem</e1> converge to those of the exact <e2>problem</e2>."
sameAs(e1, e2)
Comment:

2717	"4, by taking inspiration on early work on representation learning in the context of multi-task and meta-learning (baxter, 1995; caruana, 1998) , we instantiate the framework for ml in a simple way treating the <e1>weights</e1> of the last layer of a neural network as the inner variables and the remaining <e2>weights</e2>, which parametrize the representation mapping, as the outer variables."
sameAs(e1, e2)
Comment:

2718	"4, by taking inspiration on early work on representation learning in the context of multi-task and meta-learning (baxter, 1995; caruana, 1998) , we instantiate the framework for ml in a simple way treating the weights of the last layer of a neural network as the inner <e1>variables</e1> and the remaining weights, which parametrize the representation mapping, as the outer <e2>variables</e2>."
sameAs(e1, e2)
Comment:

2719	"we instantiate our <e1>approach</e1> for meta-learning in the case of deep learning where representation <e2>layers</e2> are treated as hyperparameters shared across a set of training episodes."
Used-for(e1, e2)
Comment:

2720	"in experiments, we confirm our theoretical findings, present encouraging results for few-shot learning and contrast the bilevel <e1>approach</e1> against classical <e2>approaches</e2> for learning-to-learn."
Compare(e1, e2)
Comment:

2721	"introduction while in standard supervised learning problems <e1>we</e1> seek the best hypothesis in a given space and with a given learning algorithm, in hyperparameter optimization (ho) and metalearning (ml) <e2>we</e2> seek a configuration so that the optimized learning algorithm will produce a model that generalizes well to new data."
sameAs(e1, e2)
Comment:

2722	"introduction while in standard supervised learning problems we seek the best hypothesis in a given space and with a given <e1>learning algorithm</e1>, in hyperparameter optimization (ho) and metalearning (ml) we seek a configuration so that the optimized <e2>learning algorithm</e2> will produce a model that generalizes well to new data."
sameAs(e1, e2)
Comment:

2723	"introduction while in standard supervised learning problems we seek the best hypothesis in a given space and with a given learning algorithm, in hyperparameter optimization (ho) and metalearning (ml) we seek a configuration so <e1>that</e1> the optimized learning algorithm will produce a model <e2>that</e2> generalizes well to new data."
sameAs(e1, e2)
Comment:

2724	"under this common perspective, both ho and ml essentially boil down to nesting two <e1>search</e1> problems: at the inner level we seek a good hypothesis (as in standard supervised learning) while at the outer level we seek a good configuration (including a good hypothesis space) where the inner <e2>search</e2> takes place."
sameAs(e1, e2)
Comment:

2725	"under this common perspective, both ho and ml essentially boil down to nesting two search problems: at the inner level <e1>we</e1> seek a good hypothesis (as in standard supervised learning) while at the outer level <e2>we</e2> seek a good configuration (including a good hypothesis space) where the inner search takes place."
sameAs(e1, e2)
Comment:

2726	"this has been explored recently in methods combining the feature extraction architecture of the cnn <e1>paradigm</e1>, with the pooling & encoding steps from the bovw <e2>paradigm</e2> [23, 8] ."
sameAs(e1, e2)
Comment:

2727	"however, their final <e1>representation</e1> is very high-dimensional; in their paper the encoded feature dimension, d, is more than 250, 000. such <e2>representation</e2> is impractical for several reasons: (1) if used with a standard one-vs-rest linear classifier for k classes, the number of model parameters becomes kd, which for e.g."
sameAs(e1, e2)
Comment:

2728	"however, their final representation is very high-dimensional; in their paper the encoded feature dimension, d, is more than 250, 000. such representation is impractical for several reasons: (1) if used with a standard <e1>one</e1>-vs-rest linear <e2>classifier</e2> for k classes, the number of model parameters becomes kd, which for e.g."
isA(e1, e2)
Comment:

2729	"the main contribution of this work is a pair of bilinear <e1>pooling</e1> methods, each able to reduce the feature dimensionality three orders of magnitude with little-to-no loss in performance compared to a full bilinear <e2>pooling</e2>."
sameAs(e1, e2)
Comment:

2730	"we show <e1>that</e1> bilinear features are closely related to polynomial kernels and propose new methods for compact bilinear features based on algorithms for the polynomial kernel first proposed by kar [15] and pham [27] ; a key aspect of our contribution is <e2>that</e2> we show how to back-propagate through such representations."
sameAs(e1, e2)
Comment:

2731	"we show that bilinear <e1>features</e1> are closely related to polynomial kernels and propose new methods for compact bilinear <e2>features</e2> based on algorithms for the polynomial kernel first proposed by kar [15] and pham [27] ; a key aspect of our contribution is that we show how to back-propagate through such representations."
sameAs(e1, e2)
Comment:

2732	"first, we propose <e1>two</e1> compact bilinear pooling methods, which can reduce the feature dimensionality <e2>two</e2> orders of magnitude with little-to-no loss in performance compared to a full bilinear pooling."
sameAs(e1, e2)
Comment:

2733	"first, we propose two compact bilinear <e1>pooling</e1> methods, which can reduce the feature dimensionality two orders of magnitude with little-to-no loss in performance compared to a full bilinear <e2>pooling</e2>."
sameAs(e1, e2)
Comment:

2734	"third, we provide a novel kernelized viewpoint of bilinear <e1>pooling</e1> which not only motivates the proposed compact methods, but also provides theoretical insights into bilinear <e2>pooling</e2>."
sameAs(e1, e2)
Comment:

2735	"while the distinction of the steps is less clear in a cnn than in a bovw pipeline, one can view the first several convolutional <e1>layers</e1> as a feature extractor and the later fully connected <e2>layers</e2> as a pooling and encoding mechanism."
sameAs(e1, e2)
Comment:

2736	"such a method can be scaled up to large <e1>training</e1> datasets using mini-batch <e2>training</e2>."
sameAs(e1, e2)
Comment:

2737	"running persistent chains may make the synthesized <e1>examples</e1> less biased by the observed <e2>examples</e2>, although the persistent chains may still have difficulty traversing different modes of the learned model."
sameAs(e1, e2)
Comment:

2738	"our <e1>method</e1> learns a separate generative convnet <e2>model</e2> at each grid."
Used-for(e1, e2)
Comment:

2739	"specifically, we initialize the finite-step <e1>mcmc</e1> sampling from the minimal 1 × 1 version of the training image, and the synthesized image at each grid serves to initialize the finite-step <e2>mcmc</e2> that samples from the model of the subsequent finer grid."
sameAs(e1, e2)
Comment:

2740	"specifically, we initialize the finite-step mcmc sampling from the minimal 1 × 1 version of the training <e1>image</e1>, and the synthesized <e2>image</e2> at each grid serves to initialize the finite-step mcmc that samples from the model of the subsequent finer grid."
sameAs(e1, e2)
Comment:

2741	"after obtaining the synthesized <e1>images</e1> at the multiple grids, the models at the multiple grids are updated separately and simultaneously based on the differences between the synthesized <e2>images</e2> and the observed training images at different grids."
sameAs(e1, e2)
Comment:

2742	"after obtaining the synthesized <e1>images</e1> at the multiple grids, the models at the multiple grids are updated separately and simultaneously based on the differences between the synthesized images and the observed training <e2>images</e2> at different grids."
sameAs(e1, e2)
Comment:

2743	"after obtaining the synthesized images at the multiple grids, the models at the multiple grids are updated separately and simultaneously based on the differences between the synthesized <e1>images</e1> and the observed training <e2>images</e2> at different grids."
sameAs(e1, e2)
Comment:

2744	"(1) the finite-step mcmc is initialized from the 1 × 1 version of the observed <e1>image</e1>, instead of the original observed <e2>image</e2>."
sameAs(e1, e2)
Comment:

2745	"thus the synthesized <e1>image</e1> is much less biased by the observed <e2>image</e2> compared to the original cd."
sameAs(e1, e2)
Comment:

2746	"(2) the learned <e1>models</e1> at coarser grids are expected to be smoother than the <e2>models</e2> at finer grids."
sameAs(e1, e2)
Comment:

2747	"(3) unlike the original cd or persistent cd, the learned models are equipped with a fixed budget <e1>mcmc</e1> to generate new synthesized images from scratch, because we only need to initialize the <e2>mcmc</e2> by sampling from the onedimensional histogram of the 1 × 1 version of the training images."
sameAs(e1, e2)
Comment:

2748	"(3) unlike the original cd or persistent cd, the learned models are equipped with a fixed budget mcmc to generate new synthesized <e1>images</e1> from scratch, because we only need to initialize the mcmc by sampling from the onedimensional histogram of the 1 × 1 version of the training <e2>images</e2>."
sameAs(e1, e2)
Comment:

2749	"the maximum likelihood learning of the energy-based generative convnet <e1>model</e1> follows an "analysis by synthesis" scheme: we sample the synthesized examples from the current <e2>model</e2>, usually by markov chain monte carlo (mcmc), and then update the model parameters based on the difference between the observed training examples and the synthesized examples."
sameAs(e1, e2)
Comment:

2750	"the maximum likelihood learning of the energy-based generative convnet model follows an "analysis by synthesis" scheme: we sample the synthesized <e1>examples</e1> from the current model, usually by markov chain monte carlo (mcmc), and then update the model parameters based on the difference between the observed training examples and the synthesized <e2>examples</e2>."
sameAs(e1, e2)
Comment:

2751	"the maximum likelihood learning of the energy-based generative convnet model follows an "analysis by synthesis" scheme: we sample the synthesized examples from the current model, usually by <e1>markov chain monte carlo</e1> (<e2>mcmc</e2>), and then update the model parameters based on the difference between the observed training examples and the synthesized examples."
sameAs(e1, e2)
Comment:

2752	"compared to recurrent <e1>layers</e1>, convolutions create representations for fixed size contexts, however, the effective context size of the network can easily be made larger by stacking several <e2>layers</e2> on top of each other."
sameAs(e1, e2)
Comment:

2753	"multi-layer convolutional neural networks create hierarchical representations over the <e1>input</e1> sequence in which nearby <e2>input</e2> elements interact at lower layers while distant elements interact at higher layers."
sameAs(e1, e2)
Comment:

2754	"multi-layer convolutional neural networks create hierarchical representations over the input sequence in which nearby input elements interact at lower <e1>layers</e1> while distant elements interact at higher <e2>layers</e2>."
sameAs(e1, e2)
Comment:

2755	"hierarchical <e1>structure</e1> provides a shorter path to capture long-range dependencies compared to the chain <e2>structure</e2> modeled by recurrent networks, e.g."
sameAs(e1, e2)
Comment:

2756	"inputs to a convolutional network are fed through a constant number of kernels and <e1>non-linearities</e1>, whereas recurrent networks apply up to n operations and <e2>non-linearities</e2> to the first word and only a single set of operations to the last word."
sameAs(e1, e2)
Comment:

2757	"inputs to a convolutional network are fed through a constant number of kernels and non-linearities, whereas recurrent networks apply up to n <e1>operations</e1> and non-linearities to the first word and only a single set of <e2>operations</e2> to the last word."
sameAs(e1, e2)
Comment:

2758	"gated convolutions have been previously explored for machine translation by meng et al (2015) but their evaluation was restricted to a small dataset and the <e1>model</e1> was used in tandem with a traditional count-based <e2>model</e2>."
sameAs(e1, e2)
Comment:

2759	"on wmt'14 english-german <e1>we</e1> outperform the strong lstm setup of wu et al (2016) by 0.5 bleu and on wmt'14 english-french <e2>we</e2> outperform the likelihood trained system of wu et al (2016) by 1.6 bleu."
sameAs(e1, e2)
Comment:

2760	"on wmt'14 english-german we outperform the strong lstm setup of wu et al (2016) by 0.5 <e1>bleu</e1> and on wmt'14 english-french we outperform the likelihood trained system of wu et al (2016) by 1.6 <e2>bleu</e2>."
sameAs(e1, e2)
Comment:

2761	"the dominant approach to date encodes the input sequence with a series of bi-directional <e1>recurrent neural networks</e1> (<e2>rnn</e2>) and generates a variable length output with another set of decoder rnns, both of which interface via a soft-attention mechanism luong et al, 2015) ."
sameAs(e1, e2)
Comment:

2762	"the dominant approach to date encodes the input sequence with a series of bi-directional <e1>recurrent neural networks</e1> (rnn) and generates a variable length output with another set of decoder <e2>rnns</e2>, both of which interface via a soft-attention mechanism luong et al, 2015) ."
sameAs(e1, e2)
Comment:

2763	"the dominant approach to date encodes the input sequence with a series of bi-directional recurrent neural networks (<e1>rnn</e1>) and generates a variable length output with another set of decoder <e2>rnns</e2>, both of which interface via a soft-attention mechanism luong et al, 2015) ."
sameAs(e1, e2)
Comment:

2764	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel <e1>categories</e1> needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial <e2>categories</e2> that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial categories by calling them base categories)."
sameAs(e1, e2)
Comment:

2765	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel <e1>categories</e1> needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial categories that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial <e2>categories</e2> by calling them base categories)."
sameAs(e1, e2)
Comment:

2766	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel <e1>categories</e1> needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial categories that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial categories by calling them base <e2>categories</e2>)."
sameAs(e1, e2)
Comment:

2767	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel categories needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial <e1>categories</e1> that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial <e2>categories</e2> by calling them base categories)."
sameAs(e1, e2)
Comment:

2768	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel categories needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial <e1>categories</e1> that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial categories by calling them base <e2>categories</e2>)."
sameAs(e1, e2)
Comment:

2769	"however, most prior methods neglect to fulfill two very important requirements for a good few-shot learning system: (a) the learning of the novel categories needs to be fast, and (b) to not sacrifice any recognition accuracy on the initial categories that the convnet was trained on, i.e., to not "forget" (from now on we will refer to those initial <e1>categories</e1> by calling them base <e2>categories</e2>)."
sameAs(e1, e2)
Comment:

2770	"motivated by <e1>this</e1> observation, in <e2>this</e2> work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2771	"motivated by this observation, in this work <e1>we</e1> propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, <e2>we</e2> want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2772	"motivated by this observation, in this work we propose to tackle the problem of <e1>few-shot learning</e1> under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic <e2>few-shot learning</e2> without forgetting)."
sameAs(e1, e2)
Comment:

2773	"motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training <e1>data</e1> is assumed to exist for a set of base categories and, using these <e2>data</e2> as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2774	"motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base <e1>categories</e1> and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base <e2>categories</e2>, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2775	"motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base <e1>categories</e1> and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base categories, but also learns to dynamically recognize novel <e2>categories</e2> from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2776	"motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using <e1>these</e1> data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize <e2>these</e2> base categories, but also learns to dynamically recognize novel categories from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2777	"motivated by this observation, in this work we propose to tackle the problem of few-shot learning under a more realistic setting, where a large set of training data is assumed to exist for a set of base categories and, using these data as the sole input, we want to develop an object recognition learning system that, not only is able to recognize these base <e1>categories</e1>, but also learns to dynamically recognize novel <e2>categories</e2> from only a few training examples (provided only at test time) while also not forgetting the base ones or requiring to be re-trained on them (dynamic few-shot learning without forgetting)."
sameAs(e1, e2)
Comment:

2778	"a typical convnet based recognition model, in order to classify an <e1>image</e1>, first extracts a high level feature representation from it and then computes per category <e2>classification</e2> scores by applying a set of classification weight vectors (one per category) to the feature."
Used-for(e1, e2)
Comment:

2779	"a typical convnet based recognition model, in order to classify an <e1>image</e1>, first extracts a high level feature representation from it and then computes per category classification scores by applying a set of <e2>classification</e2> weight vectors (one per category) to the feature."
Used-for(e1, e2)
Comment:

2780	"a typical convnet based recognition model, in order to classify an image, first extracts a high level <e1>feature</e1> representation from it and then computes per category classification scores by applying a set of classification weight vectors (one per category) to the <e2>feature</e2>."
sameAs(e1, e2)
Comment:

2781	"a typical convnet based recognition model, in order to classify an image, first extracts a high level feature representation from it and then computes <e1>per</e1> category classification scores by applying a set of classification weight vectors (one <e2>per</e2> category) to the feature."
sameAs(e1, e2)
Comment:

2782	"a typical convnet based recognition model, in order to classify an image, first extracts a high level feature representation from it and then computes per category <e1>classification</e1> scores by applying a set of <e2>classification</e2> weight vectors (one per category) to the feature."
sameAs(e1, e2)
Comment:

2783	"in this context, the first technical novelty of our work is <e1>that</e1> we enhance a typical object recognition system with an extra component, called few-shot classification weight generator <e2>that</e2> accepts as input a few training examples of a novel category (e.g., no more than five examples) and, based on them, generates a classification weight vector for that novel category."
sameAs(e1, e2)
Comment:

2784	"in this context, the first technical novelty of our work is <e1>that</e1> we enhance a typical object recognition system with an extra component, called few-shot classification weight generator that accepts as input a few training examples of a novel category (e.g., no more than five examples) and, based on them, generates a classification weight vector for <e2>that</e2> novel category."
sameAs(e1, e2)
Comment:

2785	"in this context, the first technical novelty of our work is that we enhance a typical object recognition system with an extra component, called few-shot <e1>classification</e1> weight generator that accepts as input a few training examples of a novel category (e.g., no more than five examples) and, based on them, generates a <e2>classification</e2> weight vector for that novel category."
sameAs(e1, e2)
Comment:

2786	"in this context, the first technical novelty of our work is that we enhance a typical object recognition system with an extra component, called few-shot classification weight generator <e1>that</e1> accepts as input a few training examples of a novel category (e.g., no more than five examples) and, based on them, generates a classification weight vector for <e2>that</e2> novel category."
sameAs(e1, e2)
Comment:

2787	"its key characteristic is that in order to compose novel <e1>classification</e1> weight vectors, it explicitly exploits the acquired past knowledge about the visual world by incorporating an attention mechanism over the <e2>classification</e2> weight vectors of the base categories."
sameAs(e1, e2)
Comment:

2788	"in order for the few-shot <e1>classification</e1> weight generator to be successfully incorporated into the rest of the recognition system, it is essential the convnet model to be able to simultaneously handle the <e2>classification</e2> weight vectors of both base and novel categories."
sameAs(e1, e2)
Comment:

2789	"apart from unifying the recognition of both base and novel <e1>categories</e1>, features learned with the cosine-similarity based classifier turn out to generalize significantly better on novel <e2>categories</e2> than those learned with a dot-product based classifier."
sameAs(e1, e2)
Comment:

2790	"apart from unifying the recognition of both base and novel categories, features learned with the cosine-similarity based <e1>classifier</e1> turn out to generalize significantly better on novel categories than those learned with a dot-product based <e2>classifier</e2>."
sameAs(e1, e2)
Comment:

2791	"moreover, <e1>we</e1> demonstrate in the experimental section that, by simply training a cosine-similarity based convnet recognition model, <e2>we</e2> are able to learn feature extractors that when used for image matching they surpass prior stateof-the-art approaches on the few-shot recognition task."
sameAs(e1, e2)
Comment:

2792	"moreover, we demonstrate in the experimental section <e1>that</e1>, by simply training a cosine-similarity based convnet recognition model, we are able to learn feature extractors <e2>that</e2> when used for image matching they surpass prior stateof-the-art approaches on the few-shot recognition task."
sameAs(e1, e2)
Comment:

2793	"to sum up, our contributions are: (1) we propose a fewshot object recognition system that is capable of dynamically learning novel <e1>categories</e1> from only a few training data while at the same time does not forget the base <e2>categories</e2> on which it was trained."
sameAs(e1, e2)
Comment:

2794	"(2) in order to achieve that we introduced two technical novelties, an attention based few-shot <e1>classification</e1> weight generator, and to implement the classifier of a convnet model as a cosine similarity function between feature representations and <e2>classification</e2> vectors."
sameAs(e1, e2)
Comment:

2795	"(4) finally, <e1>we</e1> apply our approach on the recently introduced fews-shot benchmark of bharath and girshick [4] where <e2>we</e2> achieve state-of-the-art results."
sameAs(e1, e2)
Comment:

2796	"in the following sections, <e1>we</e1> provide related work in §2, <e2>we</e2> describe our few-shot object learning methodology in §3, we provide experimental results in §4, and finally we conclude in §5."
sameAs(e1, e2)
Comment:

2797	"in the following sections, <e1>we</e1> provide related work in §2, we describe our few-shot object learning methodology in §3, <e2>we</e2> provide experimental results in §4, and finally we conclude in §5."
sameAs(e1, e2)
Comment:

2798	"in the following sections, <e1>we</e1> provide related work in §2, we describe our few-shot object learning methodology in §3, we provide experimental results in §4, and finally <e2>we</e2> conclude in §5."
sameAs(e1, e2)
Comment:

2799	"in the following sections, we provide related work in §2, <e1>we</e1> describe our few-shot object learning methodology in §3, <e2>we</e2> provide experimental results in §4, and finally we conclude in §5."
sameAs(e1, e2)
Comment:

2800	"in the following sections, we provide related work in §2, <e1>we</e1> describe our few-shot object learning methodology in §3, we provide experimental results in §4, and finally <e2>we</e2> conclude in §5."
sameAs(e1, e2)
Comment:

2801	"in the following sections, we provide related work in §2, we describe our few-shot object learning methodology in §3, <e1>we</e1> provide experimental results in §4, and finally <e2>we</e2> conclude in §5."
sameAs(e1, e2)
Comment:

2802	"in case <e1>we</e1> would like to expand the set of categories that the convnet can recognize, then <e2>we</e2> need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2803	"in case <e1>we</e1> would like to expand the set of categories that the convnet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that <e2>we</e2> will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2804	"in case we would like to expand the set of <e1>categories</e1> that the convnet can recognize, then we need to collect training data for the novel <e2>categories</e2> (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2805	"in case we would like to expand the set of categories <e1>that</e1> the convnet can recognize, then we need to collect training data for the novel categories (i.e., those <e2>that</e2> they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2806	"in case we would like to expand the set of categories <e1>that</e1> the convnet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such <e2>that</e2> we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2807	"in case we would like to expand the set of categories that the convnet can recognize, then <e1>we</e1> need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that <e2>we</e2> will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2808	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect <e1>training</e1> data for the novel categories (i.e., those that they were not in the initial <e2>training</e2> set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2809	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect <e1>training</e1> data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly <e2>training</e2> procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2810	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect <e1>training</e1> data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced <e2>training</e2> set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2811	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect training data for the novel categories (i.e., those <e1>that</e1> they were not in the initial training set) and restart the aforementioned computationally costly training procedure this time on the enhanced training set such <e2>that</e2> we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2812	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial <e1>training</e1> set) and restart the aforementioned computationally costly <e2>training</e2> procedure this time on the enhanced training set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2813	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial <e1>training</e1> set) and restart the aforementioned computationally costly training procedure this time on the enhanced <e2>training</e2> set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2814	"in case we would like to expand the set of categories that the convnet can recognize, then we need to collect training data for the novel categories (i.e., those that they were not in the initial training set) and restart the aforementioned computationally costly <e1>training</e1> procedure this time on the enhanced <e2>training</e2> set such that we will avoid catastrophic interference."
sameAs(e1, e2)
Comment:

2815	"it is assumed <e1>that</e1> the reason the human visual system is so efficient when learning novel concepts is <e2>that</e2> it exploits its past experiences about the (visual) world."
sameAs(e1, e2)
Comment:

2816	"however, since <e1>we</e1> use an agent with recurrent structure, <e2>we</e2> acknowledge that memory is also important."
sameAs(e1, e2)
Comment:

2817	"we introduce a method for generating useful saliency maps and use it to show 1) what strong <e1>agents</e1> attend to, 2) whether <e2>agents</e2> are making decisions for the right or wrong reasons, and 3) how agents evolve during learning."
sameAs(e1, e2)
Comment:

2818	"we introduce a method for generating useful saliency maps and use it to show 1) what strong <e1>agents</e1> attend to, 2) whether agents are making decisions for the right or wrong reasons, and 3) how <e2>agents</e2> evolve during learning."
sameAs(e1, e2)
Comment:

2819	"we introduce a method for generating useful saliency maps and use it to show 1) what strong agents attend to, 2) whether <e1>agents</e1> are making decisions for the right or wrong reasons, and 3) how <e2>agents</e2> evolve during learning."
sameAs(e1, e2)
Comment:

2820	"a simple example is an agent which has learned to reason about the velocity of a ball; it uses <e1>information</e1> about previous frames in addition to <e2>information</e2> from the current frame."
sameAs(e1, e2)
Comment:

2821	"that multiple, calibrated views of thousands of <e1>objects</e1> are available is also biologically implausible: a human infant must physically interact with <e2>objects</e2> to acquire such training data, but most humans can understand airplane shape very easily despite having played with very few airplanes."
sameAs(e1, e2)
Comment:

2822	"first, we design a unified model architecture and loss functions that combine pose <e1>supervision</e1> with weaker <e2>supervision</e2> from unlabeled images."
sameAs(e1, e2)
Comment:

2823	"then, we use our model and <e1>training</e1> framework to evaluate and compare many <e2>training</e2> paradigms and forms of supervision to come up with the best way of using a small number of pose annotations effectively."
sameAs(e1, e2)
Comment:

2824	"2. category-agnostic <e1>priors</e1> obtained by pooling training data across classes work just as well as, but not better than, category-specific <e2>priors</e2> trained on each class individually."
sameAs(e1, e2)
Comment:

2825	"in contrast, unlabeled <e1>images</e1> or <e2>images</e2> with just category labels are easy to acquire, but few current models can use this weak supervision."
sameAs(e1, e2)
Comment:

2826	"we present a unified framework that can combine both types of supervision: a small amount of camera <e1>pose</e1> annotations are used to enforce <e2>pose</e2>-invariance and view-point consistency, and unlabeled images combined with an adversarial loss are used to enforce the realism of rendered, generated models."
sameAs(e1, e2)
Comment:

2827	"we show <e1>that</e1> with a combination of these ideas, we can train single-view reconstruction models <e2>that</e2> improve up to 7 points in performance (ap) when using only 1% pose annotated training data."
sameAs(e1, e2)
Comment:

2828	"our <e1>model</e1> outperforms other <e2>methods</e2> for irregular masks."
Compare(e1, e2)
Comment:

2829	"for example, it can be used in <e1>image</e1> editing to remove unwanted (a) <e2>image</e2> with hole (b) patchmatch (c) iizuka et al [10] (d) yu et al [36] (e) hole=127."
sameAs(e1, e2)
Comment:

2830	"2(e) and 2(f) are using the same <e1>network</e1> architecture as section 3.2 but using typical convolutional <e2>network</e2>, 2(e) uses the pixel value 127.5 to initialize the holes."
sameAs(e1, e2)
Comment:

2831	"recent image inpainting approaches that do not use deep learning use <e1>image</e1> statistics of the remaining <e2>image</e2> to fill in the hole."
sameAs(e1, e2)
Comment:

2832	"for example, in figure 2 (b), patchmatch was able to smoothly fill in the missing components of the painting using image <e1>patches</e1> from the surrounding shadow and wall, but a semantically-aware approach would make use of <e2>patches</e2> from the painting instead."
sameAs(e1, e2)
Comment:

2833	"for example, iizuka et al [10] uses fast marching [30] and poisson image blending [21] , while yu et al [36] employ a following-up refinement <e1>network</e1> to refine their raw <e2>network</e2> predictions."
sameAs(e1, e2)
Comment:

2834	"we find <e1>these</e1> limitations may lead to overfitting to the rectangular holes, and ultimately limit the utility of <e2>these</e2> models in application."
sameAs(e1, e2)
Comment:

2835	"iizuka et al [10] and yu et al [36] remove the centered hole assumption and can handle irregular shaped holes, but do not perform an extensive quantitative analysis on a large number of <e1>images</e1> with irregular masks (51 test <e2>images</e2> in [8] )."
sameAs(e1, e2)
Comment:

2836	"the concept of a masked and re-normalized <e1>convolution</e1> is also referred to as segmentation-aware <e2>convolutions</e2> in [7] for the image segmentation task, however they did not make modifications to the input mask."
sameAs(e1, e2)
Comment:

2837	"in summary, <e1>we</e1> make the following contributions: -<e2>we</e2> propose the the use of partial convolutions with an automatic mask update step for achieving state-of-the-art on image inpainting."
sameAs(e1, e2)
Comment:

2838	"-while previous works fail to achieve good inpainting <e1>results</e1> with skip links in a u-net [32] with typical convolutions, we demonstrate that substituting convolutional layers with partial convolutions and mask updates can achieve state-of-the-art inpainting <e2>results</e2>."
sameAs(e1, e2)
Comment:

2839	"-while previous works fail to achieve good inpainting results with skip links in a u-net [32] with typical <e1>convolutions</e1>, we demonstrate that substituting convolutional layers with partial <e2>convolutions</e2> and mask updates can achieve state-of-the-art inpainting results."
sameAs(e1, e2)
Comment:

2840	"we propose the use of partial <e1>convolutions</e1>, where the <e2>convolution</e2> is masked and renormalized to be conditioned on only valid pixels."
sameAs(e1, e2)
Comment:

2841	"for example, one can improve performance by combining network outputs with a <e1>language model</e1> in <e2>speech recognition</e2> (hannun et al, 2014; xiong et al, 2016) , or with camera information for object detection (kendall & cipolla, 2016) ."
isA(e1, e2)
Comment:

2842	"in 2005 , niculescu-mizil & caruana (2005 showed that <e1>neural networks</e1> typically produce well-calibrated probabilities on <e2>binary classification</e2> tasks."
Used-for(e1, e2)
Comment:

2843	"while <e1>neural networks</e1> today are undoubtedly more accurate than they were a decade ago, we discover with great surprise that modern <e2>neural networks</e2> are no longer well-calibrated."
sameAs(e1, e2)
Comment:

2844	"the average confidence of lenet closely matches its <e1>accuracy</e1>, while the average confidence of the resnet is substantially higher than its <e2>accuracy</e2>."
sameAs(e1, e2)
Comment:

2845	"in <e1>this</e1> paper, we demonstrate on several computer vision and <e2>nlp tasks</e2> that neural networks produce confidences that cannot represent true probabilities."
Used-for(e1, e2)
Comment:

2846	"in this paper, we demonstrate on several computer vision and nlp tasks <e1>that</e1> neural networks produce confidences <e2>that</e2> cannot represent true probabilities."
sameAs(e1, e2)
Comment:

2847	"surprisingly, <e1>we</e1> find that a single-parameter variant of platt scaling (platt et al, 1999 ) -which <e2>we</e2> refer to as temperature scaling -is often the most effective method at obtaining calibrated probabilities."
sameAs(e1, e2)
Comment:

2848	"introduction recent advances in <e1>deep learning</e1> have dramatically improved <e2>neural network</e2> accuracy (simonyan & zisserman, 2015; srivastava et al, 2015; he et al, 2016; huang et al, 2016; 2017) ."
isA(e1, e2)
Comment:

2849	"as a result, <e1>neural networks</e1> are now entrusted with making complex decisions in applications, such as object detection (girshick, 2015) , <e2>speech recognition</e2> (hannun et al, 2014) , and medical diagnosis (caruana et al, 2015) ."
Used-for(e1, e2)
Comment:

2850	"a proper <e1>training</e1> regime ensures that at end of <e2>training</e2>, g generates images which are essentially indistinguishable from real images, i.e."
sameAs(e1, e2)
Comment:

2851	"a proper training regime ensures that at end of training, g generates <e1>images</e1> which are essentially indistinguishable from real <e2>images</e2>, i.e."
sameAs(e1, e2)
Comment:

2852	"in particular, <e1>generative adversarial networks</e1> (<e2>gans</e2>) [8] and variational auto-encoders (vae) [13] have shown a lot of promise in this regard."
sameAs(e1, e2)
Comment:

2853	"however, <e1>training</e1> high-capacity generators requires a large amount of <e2>training</e2> data."
sameAs(e1, e2)
Comment:

2854	"the rest of the paper is organised as follows: <e1>we</e1> give an overview of the related work in section 2, review gan in section 3 and then go on to describe our model deligan in section 4. in section 5, <e2>we</e2> discuss experimental results which showcase the capabilities of our model."
sameAs(e1, e2)
Comment:

2855	"the rest of the paper is organised as follows: we give an overview of the related work in section 2, review gan in section 3 and then go on to describe our <e1>model</e1> deligan in section 4. in section 5, we discuss experimental results which showcase the capabilities of our <e2>model</e2>."
sameAs(e1, e2)
Comment:

2856	"towards the end of the paper, <e1>we</e1> discuss these results and the implications of our design decisions in section 6. <e2>we</e2> conclude with some pointers for future work in section 7."
sameAs(e1, e2)
Comment:

2857	"g is trained to generate <e1>images</e1> i which are indistinguishable from a sampling of the true distribution, i.e i ∼ p data , where p data is the true distribution of <e2>images</e2>."
sameAs(e1, e2)
Comment:

2858	"g is trained to generate images i which are indistinguishable from a sampling of the true distribution, i.e i ∼ p <e1>data</e1> , where p <e2>data</e2> is the true distribution of images."
sameAs(e1, e2)
Comment:

2859	"the discriminator d takes an <e1>image</e1> as input and outputs the probability that the <e2>image</e2> is from the true data distribution p data ."
sameAs(e1, e2)
Comment:

2860	"the discriminator d takes an image as input and outputs the probability that the image is from the true <e1>data</e1> distribution p <e2>data</e2> ."
sameAs(e1, e2)
Comment:

2861	"the difficulty, from a compositional point of view comes from the intricate interplay between harmony (notes sounding at the same <e1>time</e1>) and voice movements (how a single voice evolves through <e2>time</e2>)."
sameAs(e1, e2)
Comment:

2862	"from the point of view of automatic music generation, the first <e1>solution</e1> to this apparently highly combinatorial <e2>problem</e2> was proposed by (ebcioglu, 1988) in 1988. this problem is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
Used-for(e1, e2)
Comment:

2863	"from the point of view of automatic music generation, the first <e1>solution</e1> to this apparently highly combinatorial problem was proposed by (ebcioglu, 1988) in 1988. this <e2>problem</e2> is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
Used-for(e1, e2)
Comment:

2864	"from the point of view of automatic music generation, the first <e1>solution</e1> to this apparently highly combinatorial problem was proposed by (ebcioglu, 1988) in 1988. this problem is seen as a constraint satisfaction <e2>problem</e2>, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
Used-for(e1, e2)
Comment:

2865	"from the point of view of automatic music generation, the first solution to <e1>this</e1> apparently highly combinatorial problem was proposed by (ebcioglu, 1988) in 1988. <e2>this</e2> problem is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
sameAs(e1, e2)
Comment:

2866	"from the point of view of automatic music generation, the first solution to this apparently highly combinatorial <e1>problem</e1> was proposed by (ebcioglu, 1988) in 1988. this <e2>problem</e2> is seen as a constraint satisfaction problem, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
sameAs(e1, e2)
Comment:

2867	"from the point of view of automatic music generation, the first solution to this apparently highly combinatorial <e1>problem</e1> was proposed by (ebcioglu, 1988) in 1988. this problem is seen as a constraint satisfaction <e2>problem</e2>, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
sameAs(e1, e2)
Comment:

2868	"from the point of view of automatic music generation, the first solution to this apparently highly combinatorial problem was proposed by (ebcioglu, 1988) in 1988. this <e1>problem</e1> is seen as a constraint satisfaction <e2>problem</e2>, where the system must fulfill numerous hand-crafted constraints characterizing the style of bach."
sameAs(e1, e2)
Comment:

2869	"it is a <e1>rule</e1>-based expert system which contains no less than 300 <e2>rules</e2> and tries to reharmonize a given melody with a generate-and-test method and intelligent backtracking."
sameAs(e1, e2)
Comment:

2870	"it is a rule-based expert <e1>system</e1> which contains no less than 300 rules and tries to reharmonize a given melody with a generate-and-test <e2>method</e2> and intelligent backtracking."
Used-for(e1, e2)
Comment:

2871	"in (whorley et al, 2013; whorley & conklin, 2016) , authors elaborate on those <e1>methods</e1> by introducing multiple viewpoints and variations on the sampling <e2>method</e2> (generated sequences which violate "rules of harmony" are put aside for instance)."
Compare(e1, e2)
Comment:

2872	"their temporal dependencies are learned using <e1>recurrent neural networks</e1> (<e2>rnns</e2>)."
sameAs(e1, e2)
Comment:

2873	"this is in contrast with many automatic <e1>music</e1> composition approaches which tend to compose <e2>music</e2> sequentially."
sameAs(e1, e2)
Comment:

2874	"a method that addresses the rigidity of sequential generation in <e1>music</e1> was first proposed in (sakellariou et al, 2015; sakellariou et al, 2016) for monophonic <e2>music</e2> and later generalized to polyphony in (hadjeres et al, 2016) ."
sameAs(e1, e2)
Comment:

2875	"similarly to our work, the authors evaluate their <e1>model</e1> with an online turing test to assess the efficiency of their <e2>model</e2>."
sameAs(e1, e2)
Comment:

2876	"contrary to models based on rnns, we do not sample from left to right which allows us to enforce positional, unary user-defined constraints such as <e1>rhythm</e1>, notes, parts, <e2>chords</e2> and cadences."
Conjunction(e1, e2)
Comment:

2877	"we refer to domain adaptation as the <e1>task</e1> to train a <e2>model</e2> on labeled data from a source domain while minimizing test error on a target domain, for which no labels are available at training time."
Evaluate-for(e1, e2)
Comment:

2878	"we refer to domain adaptation as the <e1>task</e1> to train a model on labeled <e2>data</e2> from a source domain while minimizing test error on a target domain, for which no labels are available at training time."
Conjunction(e1, e2)
Comment:

2879	"once a child has seen <e1>one</e1> dog, she or he will be able to recognize <e2>other</e2> dogs and becomes better at recognition with subsequent exposure to more variety."
Conjunction(e1, e2)
Comment:

2880	"however, these networks have been trained dramatically differently from a learning child, requiring labels for every <e1>training</e1> example, following a purely supervised <e2>training</e2> scheme."
sameAs(e1, e2)
Comment:

2881	"recently, researchers have annotated clothes with <e1>semantic</e1> attributes [9, 2, 8, 16, 11] (e.g., material, pattern) as intermediate representations or supervisory signals to bridge the <e2>semantic</e2> gap."
sameAs(e1, e2)
Comment:

2882	"for example, the neckline attribute usually corresponds to the top part in <e1>images</e1> while the sleeve attribute ordinarily relates to the left and right side of <e2>images</e2>."
sameAs(e1, e2)
Comment:

2883	"in addition, we learn a subspace embedding for each discovered <e1>concept</e1> to facilitate a structured exploration of the dataset based on the <e2>concept</e2> of interest (figure 1(b) )."
sameAs(e1, e2)
Comment:

2884	"by fine-tuning googlenet in an endto-end fashion, <e1>we</e1> train a discriminative model that contains localization information of attributes; (2) <e2>we</e2> then obtain the spatial representation for each attribute, indicating where in images the attribute mostly corresponds to, from the attribute activation maps."
sameAs(e1, e2)
Comment:

2885	"these spatial <e1>representations</e1> are further utilized to augment their corresponding semantic word <e2>representations</e2> (word vectors) produced from a skipgram model."
sameAs(e1, e2)
Comment:

2886	"further, clustering is performed to discover concepts, each of which contains semantically related attributes (e.g., maxi, midi, mini are all different dress length); (3) we further disentangle the trained visual-semantic embedding by training a subspace embedding for each discovered <e1>concept</e1>, in which the similarities among items can be measured based on the corresponding <e2>concept</e2> only."
sameAs(e1, e2)
Comment:

2887	"the transformation of <e1>images</e1> into a subspace embedding facilitates attribute-feedback clothing search and structured browsing of fashion <e2>images</e2>."
sameAs(e1, e2)
Comment:

2888	"the discovered spatially-aware <e1>concepts</e1> are further utilized for (b) structured product browsing (visualizing images according to selected <e2>concepts</e2>) and (c) attribute-feedback product retrieval (refining search results by providing a desired attribute)."
sameAs(e1, e2)
Comment:

2889	"various feedback, including the relevance of displayed images [20, 4] , or tuning parameters like <e1>color</e1> and <e2>texture</e2>, and then results are updated correspondingly."
Conjunction(e1, e2)
Comment:

2890	"in addition to <e1>color</e1> and <e2>texture</e2>, customers often wish to exploit higher-level features, such as neckline, sleeve length, dress length, etc."
Conjunction(e1, e2)
Comment:

2891	"abstract <e1>deep convolutional neural networks</e1> (dcnns) introduction the emergence of <e2>deep convolutional neural networks</e2> (dcnns) has greatly contributed to advancements in solving complex tasks [13, 23, 2, 3, 19] in computer vision with significantly improved performance."
sameAs(e1, e2)
Comment:

2892	"these researchers showed <e1>that</e1> the deletion of an individual residual unit from resnets, i.e., such <e2>that</e2> only a shortcut connection remains, does not significantly affect the overall performance, proving that deleting a residual unit is equivalent to deleting some shallow networks in the ensemble networks."
sameAs(e1, e2)
Comment:

2893	"these researchers showed <e1>that</e1> the deletion of an individual residual unit from resnets, i.e., such that only a shortcut connection remains, does not significantly affect the overall performance, proving <e2>that</e2> deleting a residual unit is equivalent to deleting some shallow networks in the ensemble networks."
sameAs(e1, e2)
Comment:

2894	"these researchers showed that the deletion of an individual residual unit from resnets, i.e., such <e1>that</e1> only a shortcut connection remains, does not significantly affect the overall performance, proving <e2>that</e2> deleting a residual unit is equivalent to deleting some shallow networks in the ensemble networks."
sameAs(e1, e2)
Comment:

2895	"these researchers showed that the deletion of an individual residual unit from resnets, i.e., such that only a shortcut connection remains, does not significantly affect the overall performance, proving that deleting a residual unit is equivalent to deleting some shallow <e1>networks</e1> in the ensemble <e2>networks</e2>."
sameAs(e1, e2)
Comment:

2896	"contrary to this, deleting a single layer in plain <e1>network</e1> architectures such as a vgg-<e2>network</e2> [25] damages the network by causing additional severe errors."
sameAs(e1, e2)
Comment:

2897	"contrary to this, deleting a single layer in plain <e1>network</e1> architectures such as a vgg-network [25] damages the <e2>network</e2> by causing additional severe errors."
sameAs(e1, e2)
Comment:

2898	"contrary to this, deleting a single layer in plain network architectures such as a vgg-<e1>network</e1> [25] damages the <e2>network</e2> by causing additional severe errors."
sameAs(e1, e2)
Comment:

2899	"in our paper, we refer to this <e1>network</e1> architecture as a deep "pyramidal" <e2>network</e2> and a "pyramidal" residual network with a residual-type network architecture."
sameAs(e1, e2)
Comment:

2900	"in our paper, we refer to this <e1>network</e1> architecture as a deep "pyramidal" network and a "pyramidal" residual <e2>network</e2> with a residual-type network architecture."
sameAs(e1, e2)
Comment:

2901	"in our paper, we refer to this <e1>network</e1> architecture as a deep "pyramidal" network and a "pyramidal" residual network with a residual-type <e2>network</e2> architecture."
sameAs(e1, e2)
Comment:

2902	"in our paper, we refer to this network <e1>architecture</e1> as a deep "pyramidal" network and a "pyramidal" residual network with a residual-type network <e2>architecture</e2>."
sameAs(e1, e2)
Comment:

2903	"in our paper, we refer to this network architecture as a deep "pyramidal" <e1>network</e1> and a "pyramidal" residual <e2>network</e2> with a residual-type network architecture."
sameAs(e1, e2)
Comment:

2904	"in our paper, we refer to this network architecture as a deep "pyramidal" <e1>network</e1> and a "pyramidal" residual network with a residual-type <e2>network</e2> architecture."
sameAs(e1, e2)
Comment:

2905	"in our paper, we refer to this network architecture as a deep "pyramidal" network and a "pyramidal" residual <e1>network</e1> with a residual-type <e2>network</e2> architecture."
sameAs(e1, e2)
Comment:

2906	"this reflects the fact <e1>that</e1> the shape of the network architecture can be compared to <e2>that</e2> of a pyramid."
sameAs(e1, e2)
Comment:

2907	"• a novel residual unit is also proposed, which can further improve the performance of resnet-based <e1>architectures</e1> (compared with state-of-the-art network <e2>architectures</e2>)."
sameAs(e1, e2)
Comment:

2908	"vgg [25] , googlenet [31] , residual <e1>networks</e1> [7, 8] , and inception residual <e2>networks</e2> [30] were successively proposed to demonstrate advances in network architectures."
sameAs(e1, e2)
Comment:

2909	"deeper <e1>network</e1> architectures are known for their superior performance, and these <e2>network</e2> architectures commonly have deeply stacked convolutional filters with nonlinearity [25, 31] ."
sameAs(e1, e2)
Comment:

2910	"deeper network <e1>architectures</e1> are known for their superior performance, and these network <e2>architectures</e2> commonly have deeply stacked convolutional filters with nonlinearity [25, 31] ."
sameAs(e1, e2)
Comment:

2911	"with respect to <e1>feature</e1> map dimension, the conventional method of stacking several convolutional filters is to increase the dimension while decreasing the size of <e2>feature</e2> maps by increasing the strides of the filters or poolings."
sameAs(e1, e2)
Comment:

2912	"with respect to feature <e1>map</e1> dimension, the conventional method of stacking several convolutional filters is to increase the dimension while decreasing the size of feature <e2>maps</e2> by increasing the strides of the filters or poolings."
sameAs(e1, e2)
Comment:

2913	"this is the widely adopted method of controlling the size of <e1>feature</e1> maps, because extracting the diversified highlevel attributes with the increased <e2>feature</e2> map dimension is very effective for classification tasks."
sameAs(e1, e2)
Comment:

2914	"this is the widely adopted method of controlling the size of feature <e1>maps</e1>, because extracting the diversified highlevel attributes with the increased feature <e2>map</e2> dimension is very effective for classification tasks."
sameAs(e1, e2)
Comment:

2915	"can spatiotemporal 3d <e1>cnns</e1> retrace the history of 2d <e2>cnns</e2> and imagenet?"
sameAs(e1, e2)
Comment:

2916	"for action recognition, <e1>cnns</e1> with spatio-temporal threedimensional (3d) convolutional kernels (3d <e2>cnns</e2>) are recently more effective than cnns with two-dimensional (2d) kernels [2] ."
sameAs(e1, e2)
Comment:

2917	"for action recognition, <e1>cnns</e1> with spatio-temporal threedimensional (3d) convolutional kernels (3d cnns) are recently more effective than <e2>cnns</e2> with two-dimensional (2d) kernels [2] ."
sameAs(e1, e2)
Comment:

2918	"for action recognition, cnns with spatio-temporal threedimensional (3d) convolutional <e1>kernels</e1> (3d cnns) are recently more effective than cnns with two-dimensional (2d) <e2>kernels</e2> [2] ."
sameAs(e1, e2)
Comment:

2919	"for action recognition, cnns with spatio-temporal threedimensional (3d) convolutional kernels (3d <e1>cnns</e1>) are recently more effective than <e2>cnns</e2> with two-dimensional (2d) kernels [2] ."
sameAs(e1, e2)
Comment:

2920	"the primary reason for this failure has been the relatively small data-scale of video datasets that are available for optimizing the immense number of parameters in 3d <e1>cnns</e1>, which are much larger than those of 2d <e2>cnns</e2>."
sameAs(e1, e2)
Comment:

2921	"in addition, basically, 3d <e1>cnns</e1> can only be trained on video datasets whereas 2d <e2>cnns</e2> can be pretrained on imagenet."
sameAs(e1, e2)
Comment:

2922	"however, can 3d <e1>cnns</e1> retrace the successful history of 2d <e2>cnns</e2> and imagenet?"
sameAs(e1, e2)
Comment:

2923	"to achieve such progress, we consider that kinetics for 3d <e1>cnns</e1> should be as large-scale as imagenet for 2d <e2>cnns</e2>, though no previous work has examined enough about the scale of kinetics."
sameAs(e1, e2)
Comment:

2924	"to achieve such progress, we consider that kinetics for 3d cnns should be as large-<e1>scale</e1> as imagenet for 2d cnns, though no previous work has examined enough about the <e2>scale</e2> of kinetics."
sameAs(e1, e2)
Comment:

2925	"if using the kinetics dataset enables very deep 3d <e1>cnns</e1> at a level similar to imagenet, which can train 152-layer 2d <e2>cnns</e2> [10] , that question could be answered in the affirmative."
sameAs(e1, e2)
Comment:

2926	"accordingly, using <e1>those</e1> datasets, we performed several experiments aimed at training and testing <e2>those</e2> architectures from scratch, as well as their fine-tuning."
sameAs(e1, e2)
Comment:

2927	"the results of those experiments (see section 4 for details) show the kinetics dataset can train 3d <e1>resnet</e1>-152 from scratch to a level that is similar to the training accomplished by 2d <e2>resnets</e2> on imagenet, as shown in figure 2 ."
sameAs(e1, e2)
Comment:

2928	"in addition to such large-scale datasets, a large number of algorithms, such as residual learning [10] , have been used to improve image classification performance by adding increased depth to <e1>cnns</e1>, and the use of very deep <e2>cnns</e2> trained on imagenet have facilitated the acquisition of generic feature representation."
sameAs(e1, e2)
Comment:

2929	"to date, the video <e1>datasets</e1> available for action recognition have been relatively small when compared with image recognition <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

2930	"representative video <e1>datasets</e1>, such as ucf-101 [21] and hmdb-51 [17] , can be used to provide realistic videos with sizes around 10 k, but even though they are still used as standard benchmarks, such <e2>datasets</e2> are obviously too small to be used for optimizing cnn representations from scratch."
sameAs(e1, e2)
Comment:

2931	"more than 300 k videos have been collected for the kinetics dataset, which means <e1>that</e1> the scale of video datasets has begun to approach <e2>that</e2> of image datasets."
sameAs(e1, e2)
Comment:

2932	"more than 300 k videos have been collected for the kinetics dataset, which means that the scale of video <e1>datasets</e1> has begun to <e2>approach</e2> that of image datasets."
Evaluate-for(e1, e2)
Comment:

2933	"more than 300 k videos have been collected for the kinetics dataset, which means that the scale of video <e1>datasets</e1> has begun to approach that of image <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

2934	"feed-forward architectures, which is considered as a oneway mapping, only map rich representations of the <e1>input</e1> to [6] , vdsr [21] , drrn [42] ) commonly uses the conventional interpolation, such as bicubic, to upscale lr <e2>input</e2> images before entering the network."
sameAs(e1, e2)
Comment:

2935	"single <e1>image</e1> sr is an ill-posed inverse problem where the aim is to recover a high-resolution (hr) <e2>image</e2> from a low-resolution (lr) image."
sameAs(e1, e2)
Comment:

2936	"single <e1>image</e1> sr is an ill-posed inverse problem where the aim is to recover a high-resolution (hr) image from a low-resolution (lr) <e2>image</e2>."
sameAs(e1, e2)
Comment:

2937	"single image sr is an ill-posed inverse problem where the aim is to recover a high-<e1>resolution</e1> (hr) image from a low-<e2>resolution</e2> (lr) image."
sameAs(e1, e2)
Comment:

2938	"single image sr is an ill-posed inverse problem where the aim is to recover a high-resolution (hr) <e1>image</e1> from a low-resolution (lr) <e2>image</e2>."
sameAs(e1, e2)
Comment:

2939	"therefore, our networks focus not only generating variants of the hr features using upsampling <e1>layers</e1> but also projecting it back to the lr spaces using downsampling <e2>layers</e2>."
sameAs(e1, e2)
Comment:

2940	"these networks compute a sequence of feature maps from the lr <e1>image</e1>, culminating with one or more upsampling layers to increase resolution and finally construct the hr <e2>image</e2>."
sameAs(e1, e2)
Comment:

2941	"we consider the dynamics of a <e1>system</e1> governed by multi- variate ordinary differential functions: x(t) = dx(t) dt = f (x(t)) (1) where x(t) ∈ x = r d is the state vector of a ddimensional dynamical <e2>system</e2> at time t, and theẋ(t) ∈ x = r d is the first order time derivative of x(t) that drives the state x(t) forward, and where f : r d → r d is the vector-valued derivative function."
sameAs(e1, e2)
Comment:

2942	"we consider the dynamics of a system governed by multi- variate ordinary differential functions: x(t) = dx(t) dt = f (x(t)) (1) where x(t) ∈ x = r d is the <e1>state</e1> vector of a ddimensional dynamical system at time t, and theẋ(t) ∈ x = r d is the first order time derivative of x(t) that drives the <e2>state</e2> x(t) forward, and where f : r d → r d is the vector-valued derivative function."
sameAs(e1, e2)
Comment:

2943	"we consider the dynamics of a system governed by multi- variate ordinary differential functions: x(t) = dx(t) dt = f (x(t)) (1) where x(t) ∈ x = r d is the state vector of a ddimensional dynamical system at <e1>time</e1> t, and theẋ(t) ∈ x = r d is the first order <e2>time</e2> derivative of x(t) that drives the state x(t) forward, and where f : r d → r d is the vector-valued derivative function."
sameAs(e1, e2)
Comment:

2944	"the ode solution is determined by x(t) = x 0 + t 0 f (x(τ ))dτ, (2) where we integrate the system <e1>state</e1> from an initial <e2>state</e2> x(0) = x 0 for time t forward."
sameAs(e1, e2)
Comment:

2945	"there is a vast literature on conventional odes (butcher, 2016) where a parametric form for function f (x; θ, t) is assumed to be known, and its <e1>parameters</e1> θ are subsequently optimised with least squares or bayesian approach, where the expensive forward solution x θ (t i ) = ti 0 f (x(τ ); θ, t)dτ is required to evaluate the system responses x θ (t i ) from <e2>parameters</e2> θ against observations y(t i )."
sameAs(e1, e2)
Comment:

2946	"to overcome the computationally intensive forward <e1>solution</e1>, a family of methods denoted as gradient matching (varah, 1982; ellner et al, 2002; ramsay et al, 2007) have proposed to replace the forward <e2>solution</e2> by matching f (y i ) ≈ẏ i to empirical gradientsẏ i of the data instead, which do not require the costly integration step."
sameAs(e1, e2)
Comment:

2947	"to overcome the computationally intensive forward solution, a family of methods denoted as gradient <e1>matching</e1> (varah, 1982; ellner et al, 2002; ramsay et al, 2007) have proposed to replace the forward solution by <e2>matching</e2> f (y i ) ≈ẏ i to empirical gradientsẏ i of the data instead, which do not require the costly integration step."
sameAs(e1, e2)
Comment:

2948	"äijö and lähdesmäki (2009) proposed estimating the unknown nonlinear <e1>function</e1> with gps using either finite time differences, or analytically solving the derivative <e2>function</e2> as a function of only time,ẋ(t) = f (t) (äijö et al, 2013) ."
sameAs(e1, e2)
Comment:

2949	"äijö and lähdesmäki (2009) proposed estimating the unknown nonlinear <e1>function</e1> with gps using either finite time differences, or analytically solving the derivative function as a <e2>function</e2> of only time,ẋ(t) = f (t) (äijö et al, 2013) ."
sameAs(e1, e2)
Comment:

2950	"äijö and lähdesmäki (2009) proposed estimating the unknown nonlinear function with gps using either finite <e1>time</e1> differences, or analytically solving the derivative function as a function of only <e2>time</e2>,ẋ(t) = f (t) (äijö et al, 2013) ."
sameAs(e1, e2)
Comment:

2951	"äijö and lähdesmäki (2009) proposed estimating the unknown nonlinear function with gps using either finite time differences, or analytically solving the derivative <e1>function</e1> as a <e2>function</e2> of only time,ẋ(t) = f (t) (äijö et al, 2013) ."
sameAs(e1, e2)
Comment:

2952	"for example, methods based on articulated 3d skeletons can be trained not only with actual 3d <e1>annotations</e1> but also using 2d <e2>annotations</e2> [21, 54] and multi-view footage [25, 47] ."
sameAs(e1, e2)
Comment:

2953	"however, these methods still require a good enough 3d <e1>training</e1> set to initialize (a) during <e2>training</e2>, we first learn a geometry-aware representation using unlabeled multi-view images."
sameAs(e1, e2)
Comment:

2954	"we then use a small amount of <e1>supervision</e1> to learn a mapping from our representation to actual 3d poses, which only requires a shallow network and therefore a limited amount of <e2>supervision</e2>."
sameAs(e1, e2)
Comment:

2955	"(c) by contrast, most state-of-the-art approaches train a <e1>network</e1> to regress directly from the images to the 3d poses, which requires a much deeper <e2>network</e2> and therefore more training data."
sameAs(e1, e2)
Comment:

2956	"instead, we train an encoder-decoder to predict an <e1>image</e1> seen from one viewpoint from an <e2>image</e2> captured from a different one."
sameAs(e1, e2)
Comment:

2957	"instead, we train an encoder-decoder to predict an image seen from <e1>one</e1> viewpoint from an image captured from a different <e2>one</e2>."
sameAs(e1, e2)
Comment:

2958	"the crux of our <e1>approach</e1>, however, is that because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing <e2>methods</e2> that rely on multiview supervision [31, 55] , and more generally most state-of-the-art methods that attempt to regress directly from the image to the 3d pose."
Compare(e1, e2)
Comment:

2959	"the crux of our <e1>approach</e1>, however, is that because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing methods that rely on multiview supervision [31, 55] , and more generally most state-of-the-art <e2>methods</e2> that attempt to regress directly from the image to the 3d pose."
Compare(e1, e2)
Comment:

2960	"the crux of our approach, however, is <e1>that</e1> because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing methods <e2>that</e2> rely on multiview supervision [31, 55] , and more generally most state-of-the-art methods that attempt to regress directly from the image to the 3d pose."
sameAs(e1, e2)
Comment:

2961	"the crux of our approach, however, is <e1>that</e1> because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing methods that rely on multiview supervision [31, 55] , and more generally most state-of-the-art methods <e2>that</e2> attempt to regress directly from the image to the 3d pose."
sameAs(e1, e2)
Comment:

2962	"the crux of our approach, however, is that because our latent representation already captures 3d geometry, the mapping to 3d <e1>pose</e1> is much simpler and can be learned using much fewer examples than existing methods that rely on multiview supervision [31, 55] , and more generally most state-of-the-art methods that attempt to regress directly from the image to the 3d <e2>pose</e2>."
sameAs(e1, e2)
Comment:

2963	"the crux of our approach, however, is that because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing <e1>methods</e1> that rely on multiview supervision [31, 55] , and more generally most state-of-the-art <e2>methods</e2> that attempt to regress directly from the image to the 3d pose."
sameAs(e1, e2)
Comment:

2964	"the crux of our approach, however, is that because our latent representation already captures 3d geometry, the mapping to 3d pose is much simpler and can be learned using much fewer examples than existing methods <e1>that</e1> rely on multiview supervision [31, 55] , and more generally most state-of-the-art methods <e2>that</e2> attempt to regress directly from the image to the 3d pose."
sameAs(e1, e2)
Comment:

2965	"while weaklysupervised methods require less supervision, by utilizing 2d poses or multi-view imagery without <e1>annotations</e1>, they still need a sufficiently large set of samples with 3d <e2>annotations</e2> for learning to succeed."
sameAs(e1, e2)
Comment:

2966	"in <e1>this</e1> paper, we propose to overcome <e2>this</e2> problem by learning a geometry-aware body representation from multi-view images without annotations."
sameAs(e1, e2)
Comment:

2967	"to this end, we use an encoder-decoder that predicts an <e1>image</e1> from one viewpoint given an <e2>image</e2> from another viewpoint."
sameAs(e1, e2)
Comment:

2968	"to this end, we use an encoder-decoder that predicts an image from one <e1>viewpoint</e1> given an image from another <e2>viewpoint</e2>."
sameAs(e1, e2)
Comment:

2969	"as evidenced by our experiments, our approach significantly outperforms fully-supervised methods given the same amount of labeled <e1>data</e1>, and improves over other semi-supervised methods while using as little as 1% of the labeled <e2>data</e2>."
sameAs(e1, e2)
Comment:

2970	"global <e1>pooling</e1> module in parsenet [24] , different-dilation based atrous spatial pyramid <e2>pooling</e2> (aspp) module in deeplab [5] and different-region based pyramid pooling module (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2971	"global <e1>pooling</e1> module in parsenet [24] , different-dilation based atrous spatial pyramid pooling (aspp) module in deeplab [5] and different-region based pyramid <e2>pooling</e2> module (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2972	"global pooling <e1>module</e1> in parsenet [24] , different-dilation based atrous spatial pyramid pooling (aspp) <e2>module</e2> in deeplab [5] and different-region based pyramid pooling module (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2973	"global pooling <e1>module</e1> in parsenet [24] , different-dilation based atrous spatial pyramid pooling (aspp) module in deeplab [5] and different-region based pyramid pooling <e2>module</e2> (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2974	"global pooling module in parsenet [24] , different-dilation based atrous spatial pyramid <e1>pooling</e1> (aspp) module in deeplab [5] and different-region based pyramid <e2>pooling</e2> module (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2975	"global pooling module in parsenet [24] , different-dilation based atrous spatial pyramid pooling (aspp) <e1>module</e1> in deeplab [5] and different-region based pyramid pooling <e2>module</e2> (ppm) in pspnet [45] can help extract the context information to a certain degree."
sameAs(e1, e2)
Comment:

2976	"besides, <e1>recurrent neural network</e1> (<e2>rnn</e2>) is introduced in reseg [38] for its capability to capture long-range dependencies."
sameAs(e1, e2)
Comment:

2977	"each position in the feature <e1>map</e1> is connected with all other ones through self-adaptively predicted attention <e2>maps</e2>, thus harvesting various information nearby and far away."
sameAs(e1, e2)
Comment:

2978	"each <e1>position</e1> collects information from all others to help the prediction of itself and vice versa, the information at each <e2>position</e2> can be distributed globally, assisting the prediction of all other positions."
sameAs(e1, e2)
Comment:

2979	"each position collects <e1>information</e1> from all others to help the prediction of itself and vice versa, the <e2>information</e2> at each position can be distributed globally, assisting the prediction of all other positions."
sameAs(e1, e2)
Comment:

2980	"each position collects information from all <e1>others</e1> to help the prediction of itself and vice versa, the information at each position can be distributed globally, assisting the prediction of all <e2>other</e2> positions."
sameAs(e1, e2)
Comment:

2981	"each position collects information from all others to help the <e1>prediction</e1> of itself and vice versa, the information at each position can be distributed globally, assisting the <e2>prediction</e2> of all other positions."
sameAs(e1, e2)
Comment:

2982	"information at <e1>other</e1> positions can be collected to help the prediction of the current position and vice versa, information at the current position can be distributed to assist the prediction of <e2>other</e2> ones."
sameAs(e1, e2)
Comment:

2983	"information at other positions can be collected to help the <e1>prediction</e1> of the current position and vice versa, information at the current position can be distributed to assist the <e2>prediction</e2> of other ones."
sameAs(e1, e2)
Comment:

2984	"information at other positions can be collected to help the prediction of the current <e1>position</e1> and vice versa, information at the current <e2>position</e2> can be distributed to assist the prediction of other ones."
sameAs(e1, e2)
Comment:

2985	"our proposed <e1>approach</e1> achieves top performance on various competitive scene <e2>parsing</e2> datasets, including ade20k, pascal voc 2012 and cityscapes, demonstrating its effectiveness and generality."
Used-for(e1, e2)
Comment:

2986	"deep residual nets are foundations of our submissions to ilsvrc & coco 2015 competitions 1 , where we also won the 1st places on the tasks of <e1>imagenet</e1> detection, <e2>imagenet</e2> localization, coco detection, and coco segmentation."
sameAs(e1, e2)
Comment:

2987	"deep networks naturally integrate low/mid/highlevel <e1>features</e1> [49] and classifiers in an end-to-end multilayer fashion, and the "levels" of <e2>features</e2> can be enriched by the number of stacked layers (depth)."
sameAs(e1, e2)
Comment:

2988	"this problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization <e1>layers</e1> [16] , which enable networks with tens of <e2>layers</e2> to start converging for stochastic gradient descent (sgd) with backpropagation [22] ."
sameAs(e1, e2)
Comment:

2989	"this problem, however, has been largely addressed by normalized initialization [23, 8, 36, 12] and intermediate normalization layers [16] , which enable networks with tens of layers to start converging for <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) with backpropagation [22] ."
sameAs(e1, e2)
Comment:

2990	"the degradation (of training <e1>accuracy</e1>) indicates that not all <e2>systems</e2> are similarly easy to optimize."
Evaluate-for(e1, e2)
Comment:

2991	"there exists a solution by construction to the deeper <e1>model</e1>: the added layers are identity mapping, and the other layers are copied from the learned shallower <e2>model</e2>."
sameAs(e1, e2)
Comment:

2992	"there exists a solution by construction to the deeper model: the added <e1>layers</e1> are identity mapping, and the other <e2>layers</e2> are copied from the learned shallower model."
sameAs(e1, e2)
Comment:

2993	"but experiments show <e1>that</e1> our current solvers on hand are unable to find solutions <e2>that</e2> are comparably good or better than the constructed solution (or unable to do so in feasible time)."
sameAs(e1, e2)
Comment:

2994	"we explicitly reformulate the layers as learning residual <e1>functions</e1> with reference to the layer inputs, instead of learning unreferenced <e2>functions</e2>."
sameAs(e1, e2)
Comment:

2995	"instead of hoping each few stacked <e1>layers</e1> directly fit a desired underlying mapping, we explicitly let these <e2>layers</e2> fit a residual mapping."
sameAs(e1, e2)
Comment:

2996	"in our case, the shortcut connections simply perform identity mapping, and their <e1>outputs</e1> are added to the <e2>outputs</e2> of the stacked layers (fig."
sameAs(e1, e2)
Comment:

2997	"we show <e1>that</e1>: 1) our extremely deep residual nets are easy to optimize, but the counterpart "plain" nets (<e2>that</e2> simply stack layers) exhibit higher training error when the depth increases; 2) our deep residual nets can easily enjoy accuracy gains from greatly increased depth, producing results substantially better than previous networks."
sameAs(e1, e2)
Comment:

2998	"we present successfully trained <e1>models</e1> on this dataset with over 100 layers, and explore <e2>models</e2> with over 1000 layers."
sameAs(e1, e2)
Comment:

2999	"we present successfully trained models on this dataset with over 100 <e1>layers</e1>, and explore models with over 1000 <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3000	"the extremely deep representations also have excellent generalization performance on other recognition tasks, and lead us to further win the 1st places on: <e1>imagenet</e1> detection, <e2>imagenet</e2> localization, coco detection, and coco segmentation in ilsvrc & coco 2015 competitions."
sameAs(e1, e2)
Comment:

3001	"this strong evidence shows <e1>that</e1> the residual learning principle is generic, and we expect <e2>that</e2> it is applicable in other vision and non-vision problems."
sameAs(e1, e2)
Comment:

3002	"this strong evidence shows that the residual learning principle is generic, and we expect that it is applicable in other <e1>vision</e1> and non-<e2>vision</e2> problems."
sameAs(e1, e2)
Comment:

3003	"our method, called mask r-<e1>cnn</e1>, extends faster r-<e2>cnn</e2> [29] by adding a branch for predicting segmentation masks on each region of interest (roi), in parallel with the existing branch for classification and bounding box regression ( figure 1 )."
sameAs(e1, e2)
Comment:

3004	"mask r-<e1>cnn</e1> is simple to implement and train given the faster r-<e2>cnn</e2> framework, which facilitates a wide range of flexible architecture designs."
sameAs(e1, e2)
Comment:

3005	"in principle mask r-<e1>cnn</e1> is an intuitive extension of faster r-<e2>cnn</e2>, yet constructing the mask branch properly is critical for good results."
sameAs(e1, e2)
Comment:

3006	"second, <e1>we</e1> found it essential to decouple mask and class prediction: <e2>we</e2> predict a binary mask for each class independently, without competition among classes, and rely on the network's roi classification branch to predict the category."
sameAs(e1, e2)
Comment:

3007	"in contrast, fcns usually perform per-pixel multi-class categorization, which couples <e1>segmentation</e1> and classification, and based on our experiments works poorly for instance <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

3008	"deep learning approaches to embedding are trained with data from many <e1>classes</e1> and optimize loss functions based on pairs of images from the same or different <e2>classes</e2> [3, 5] , triplets of images where inputs from the same class are forced to be closer than inputs from different classes [16] , or functions of large collections of images [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3009	"deep learning approaches to embedding are trained with data from many <e1>classes</e1> and optimize loss functions based on pairs of images from the same or different classes [3, 5] , triplets of images where inputs from the same class are forced to be closer than inputs from different <e2>classes</e2> [16] , or functions of large collections of images [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3010	"deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of <e1>images</e1> from the same or different classes [3, 5] , triplets of <e2>images</e2> where inputs from the same class are forced to be closer than inputs from different classes [16] , or functions of large collections of images [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3011	"deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of <e1>images</e1> from the same or different classes [3, 5] , triplets of images where inputs from the same class are forced to be closer than inputs from different classes [16] , or functions of large collections of <e2>images</e2> [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3012	"deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of images from the same or different <e1>classes</e1> [3, 5] , triplets of images where inputs from the same class are forced to be closer than inputs from different <e2>classes</e2> [16] , or functions of large collections of images [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3013	"deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of images from the same or different classes [3, 5] , triplets of <e1>images</e1> where inputs from the same class are forced to be closer than inputs from different classes [16] , or functions of large collections of <e2>images</e2> [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3014	"deep learning approaches to embedding are trained with data from many classes and optimize loss functions based on pairs of images from the same or different classes [3, 5] , triplets of images where <e1>inputs</e1> from the same class are forced to be closer than <e2>inputs</e2> from different classes [16] , or functions of large collections of images [9, 2, 20, 7] ."
sameAs(e1, e2)
Comment:

3015	"some recent work [13] suggests an <e1>approach</e1> to high-dimensional embedding with an ensemble <e2>approach</e2>, learning to map images to a collection of independent output spaces, using boosting to re-weight input examples to make each output space independent."
sameAs(e1, e2)
Comment:

3016	"we learn a collection of embeddings all trained with the same input <e1>data</e1>, but differing in the label assigned to the <e2>data</e2> points."
sameAs(e1, e2)
Comment:

3017	"we group <e1>classes</e1> into metaclasses (each containing a few <e2>classes</e2>), and learn embeddings with inputs labelled by their meta-class."
sameAs(e1, e2)
Comment:

3018	"the <e1>first</e1> meta-class groups together images from particular models of porches and audi, so the <e2>first</e2> embedding will seek to map all these images to the same location."
sameAs(e1, e2)
Comment:

3019	"the first meta-class groups together <e1>images</e1> from particular models of porches and audi, so the first embedding will seek to map all these <e2>images</e2> to the same location."
sameAs(e1, e2)
Comment:

3020	"for each grouping of <e1>classes</e1> into meta-<e2>classes</e2>, an embedding is learned that embeds all elements of the same meta-class similarly."
sameAs(e1, e2)
Comment:

3021	"we train many such embeddings, with different random groupings of <e1>classes</e1> into meta-<e2>classes</e2>."
sameAs(e1, e2)
Comment:

3022	"-we introduce the idea of randomly grouping labels as an <e1>approach</e1> to making a large family of related embedding <e2>models</e2> that can be used as an ensemble."
Used-for(e1, e2)
Comment:

3023	"-we demonstrate improvement over the state of the art for <e1>retrieval</e1> tasks for the cub-200-2011 [21] , cars-196 [8] , in-shop clothes <e2>retrieval</e2> [24] and vehicleid [10] datasets."
sameAs(e1, e2)
Comment:

3024	"the ensemble output defines a metric space that improves state of the art performance for image <e1>retrieval</e1> on cub-200-2011, cars-196, in-shop clothes <e2>retrieval</e2>  and vehicleid."
sameAs(e1, e2)
Comment:

3025	"the holistically-nested <e1>edge</e1> detector (hed) [49] model, which explicitly deals with the scale space problem, has lead to large improvements over generic fcn models in the context of <e2>edge</e2> detection."
sameAs(e1, e2)
Comment:

3026	"1 , we observe that i) deeper side <e1>outputs</e1> encodes high level knowledge and can better locate salient objects; ii) shallower side <e2>outputs</e2> capture rich spatial information."
sameAs(e1, e2)
Comment:

3027	"by having a series of short connections from deeper side outputs to shallower ones, our new framework offers two advantages: i) high-level features can be transformed to shallower side-output <e1>layers</e1> and thus can help them better locate the most salient region; shallower sideoutput <e2>layers</e2> can learn rich low-level features that can help refine the sparse and irregular prediction maps from deeper side-output layers."
sameAs(e1, e2)
Comment:

3028	"by having a series of short connections from deeper side outputs to shallower ones, our new framework offers two advantages: i) high-level features can be transformed to shallower side-output <e1>layers</e1> and thus can help them better locate the most salient region; shallower sideoutput layers can learn rich low-level features that can help refine the sparse and irregular prediction maps from deeper side-output <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3029	"by having a series of short connections from deeper side outputs to shallower ones, our new framework offers two advantages: i) high-level features can be transformed to shallower side-output layers and thus can help them better locate the most salient region; shallower sideoutput <e1>layers</e1> can learn rich low-level features that can help refine the sparse and irregular prediction maps from deeper side-output <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3030	"experimental <e1>results</e1> show that our method produces state-of-the-art <e2>results</e2> on 5 widely tested salient object detection benchmarks, with advantages in terms of efficiency (0.08 seconds per image), effectiveness, and simplicity over the existing algorithms."
sameAs(e1, e2)
Comment:

3031	"though saliency <e1>maps</e1> produced by deeper (4-6) side output (s-out) look similar, because of the introduced short connections, each shallower (1-3) side output can generate satisfactory saliency <e2>maps</e2> and hence a better output result."
sameAs(e1, e2)
Comment:

3032	"holisitcally-nested <e1>edge</e1> detector (hed) provides a skip-layer structure with deep supervision for <e2>edge</e2> and boundary detection, but the performance gain of hed on saliency detection is not obvious."
sameAs(e1, e2)
Comment:

3033	"salient <e1>object detection</e1> methods commonly serve as the first step for a variety of computer vision applications including image and video compression [15, 20] , image segmentation [10] , content-aware image editing [9] , <e2>object recognition</e2> [44, 48] , visual tracking [2] , non-photo-realist rendering [43, 16] , photo synthesis [6, 14, 19] , information discovery [35, 53, 8] , image retrieval [18, 12, 8] , etc."
isA(e1, e2)
Comment:

3034	"salient object detection methods commonly serve as the first step for a variety of computer vision applications including <e1>image</e1> and video compression [15, 20] , image segmentation [10] , content-aware <e2>image</e2> editing [9] , object recognition [44, 48] , visual tracking [2] , non-photo-realist rendering [43, 16] , photo synthesis [6, 14, 19] , information discovery [35, 53, 8] , image retrieval [18, 12, 8] , etc."
sameAs(e1, e2)
Comment:

3035	"salient object detection methods commonly serve as the first step for a variety of computer vision applications including <e1>image</e1> and video compression [15, 20] , image segmentation [10] , content-aware image editing [9] , object recognition [44, 48] , visual tracking [2] , non-photo-realist rendering [43, 16] , photo synthesis [6, 14, 19] , information discovery [35, 53, 8] , <e2>image</e2> retrieval [18, 12, 8] , etc."
sameAs(e1, e2)
Comment:

3036	"salient object detection methods commonly serve as the first step for a variety of computer vision applications including image and video compression [15, 20] , image segmentation [10] , content-aware <e1>image</e1> editing [9] , object recognition [44, 48] , visual tracking [2] , non-photo-realist rendering [43, 16] , photo synthesis [6, 14, 19] , information discovery [35, 53, 8] , <e2>image</e2> retrieval [18, 12, 8] , etc."
sameAs(e1, e2)
Comment:

3037	"despite their impressive advances in visual domain, such as <e1>image</e1> generation (radford et al, 2015) , learning interpretable <e2>image</e2> representations , and image editing (zhu et al, 2016) , applications to natural language generation have been relatively less studied."
sameAs(e1, e2)
Comment:

3038	"despite their impressive advances in visual domain, such as <e1>image</e1> generation (radford et al, 2015) , learning interpretable image representations , and <e2>image</e2> editing (zhu et al, 2016) , applications to natural language generation have been relatively less studied."
sameAs(e1, e2)
Comment:

3039	"despite their impressive advances in visual domain, such as image generation (radford et al, 2015) , learning interpretable <e1>image</e1> representations , and <e2>image</e2> editing (zhu et al, 2016) , applications to natural language generation have been relatively less studied."
sameAs(e1, e2)
Comment:

3040	"abstract normalizing <e1>flows</e1> and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via masked autoregressive <e2>flows</e2> (maf) (papamakarios et al, 2017) , and to accelerate stateof-the-art wavenet-based speech synthesis to 20x faster than real-time (oord et al, 2017) , via inverse autoregressive flows (iaf) ."
sameAs(e1, e2)
Comment:

3041	"abstract normalizing <e1>flows</e1> and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via masked autoregressive flows (maf) (papamakarios et al, 2017) , and to accelerate stateof-the-art wavenet-based speech synthesis to 20x faster than real-time (oord et al, 2017) , via inverse autoregressive <e2>flows</e2> (iaf) ."
sameAs(e1, e2)
Comment:

3042	"abstract normalizing flows and <e1>autoregressive models</e1> have been successfully combined to produce state-of-the-art results in density estimation, via masked autoregressive flows (maf) (papamakarios et al, 2017) , and to accelerate stateof-the-art <e2>wavenet</e2>-based speech synthesis to 20x faster than real-time (oord et al, 2017) , via inverse autoregressive flows (iaf) ."
Used-for(e1, e2)
Comment:

3043	"abstract normalizing flows and autoregressive models have been successfully combined to produce state-of-the-art results in density estimation, via masked autoregressive <e1>flows</e1> (maf) (papamakarios et al, 2017) , and to accelerate stateof-the-art wavenet-based speech synthesis to 20x faster than real-time (oord et al, 2017) , via inverse autoregressive <e2>flows</e2> (iaf) ."
sameAs(e1, e2)
Comment:

3044	"we unify and generalize these approaches, replacing the (conditionally) affine univariate <e1>transformations</e1> of maf/iaf with a more general class of invertible univariate <e2>transformations</e2> expressed as monotonic neural networks."
sameAs(e1, e2)
Comment:

3045	"tractable density is an appealing property for these <e1>models</e1>, since it allows the objective of interest to be directly optimized; whereas other mainstream methods rely on alternative losses, in the case of intractable density <e2>models</e2> (kingma & welling, 2014; rezende et al, 2014) , or * equal contribution 1 mila, university of montreal 2 element ai 3 cifar fellow."
sameAs(e1, e2)
Comment:

3046	"tractable density is an appealing property for these models, since it allows the objective of interest to be directly optimized; whereas other mainstream <e1>methods</e1> rely on alternative losses, in the case of intractable density <e2>models</e2> (kingma & welling, 2014; rezende et al, 2014) , or * equal contribution 1 mila, university of montreal 2 element ai 3 cifar fellow."
Compare(e1, e2)
Comment:

3047	" introduction benefiting from the rapid development of deep learning methods and the easy access to a large amount of annotated<e1> face image</e1>s, unconstrained<e2> face recognitio</e2>n techniques [28, 29] have made significant advances in recent years."
isA(e1, e2)
Comment:

3048	"these methods are good at normalizing small <e1>pose</e1> faces, but their performance decreases under large face <e2>poses</e2> due to severe texture loss."
sameAs(e1, e2)
Comment:

3049	"these methods are good at normalizing small pose <e1>faces</e1>, but their performance decreases under large <e2>face</e2> poses due to severe texture loss."
sameAs(e1, e2)
Comment:

3050	"moreover, from an optimization point of view, recovering the frontal view from incompletely observed profile is an ill-posed or under-defined <e1>problem</e1>, and there exist multiple solutions to this <e2>problem</e2> if no prior knowledge or constraints are considered."
sameAs(e1, e2)
Comment:

3051	"although some deep learning methods have been proposed for <e1>face</e1> synthesis, our method is the first attempt to be effective for the recognition task with synthesized <e2>faces</e2>."
sameAs(e1, e2)
Comment:

3052	"one category tries to adopt hand-crafted or learned <e1>pose</e1>-invariant features [4, 25] , while the other resorts to synthesis techniques to recover a frontal view image from a large <e2>pose</e2> face image and then use the recovered face images for face recognition [41, 42] ."
sameAs(e1, e2)
Comment:

3053	"one category tries to adopt hand-crafted or learned pose-invariant features [4, 25] , while the other resorts to synthesis techniques to recover a frontal view <e1>image</e1> from a large pose face <e2>image</e2> and then use the recovered face images for face recognition [41, 42] ."
sameAs(e1, e2)
Comment:

3054	"one category tries to adopt hand-crafted or learned pose-invariant features [4, 25] , while the other resorts to synthesis techniques to recover a frontal view image from a large pose face image and then use the recovered <e1>face images</e1> for <e2>face recognition</e2> [41, 42] ."
isA(e1, e2)
Comment:

3055	" introduction the high accuracy of<e1> convolutional network</e1>s (cnns) in visual recognition tasks, such as image classification [12, 19, 38] , has fueled the desire to deploy these<e2> network</e2>s on platforms with limited computational resources, e.g., in robotics, self-driving cars, and on mobile devices."
isA(e1, e2)
Comment:

3056	" introduction the high accuracy of convolutional networks (cnns) in visual recognition tasks, such as image classification [12, 19, 38] , has fueled the desire to deploy these networks on platforms with<e1> limited computational resource</e1>s, e.g., in robotics, self-driving cars, and on mobile<e2> device</e2>s."
Feature-of(e1, e2)
Comment:

3057	"the densenet architecture [19] alleviates the need for <e1>feature</e1> replication by directly connecting each layer with all layers before it, which induces <e2>feature</e2> re-use."
sameAs(e1, e2)
Comment:

3058	"our <e1>image</e1>-<e2>classification</e2> experiments show that condensenets consistently outperform alternative network architectures."
Used-for(e1, e2)
Comment:

3059	"compared to densenets, condensenets use only 1/10 of the <e1>computation</e1> at comparable <e2>accuracy</e2> levels."
Conjunction(e1, e2)
Comment:

3060	"a typical set-up for deep learning on mobile <e1>devices</e1> is one where cnns are trained on multi-gpu machines but deployed on <e2>devices</e2> with limited compute."
sameAs(e1, e2)
Comment:

3061	"b) we produce a set of plane-sweep volumes for a reference view and feed these into a convolutional neural network that predicts a disparity <e1>map</e1>; (c) our network produces high-quality disparity <e2>maps</e2> even for challenging cases containing poorly textured regions and thin structures."
sameAs(e1, e2)
Comment:

3062	"although these different approaches vary in network topology and training procedure, <e1>they</e1> all share a key characteristic: <e2>they</e2> create short paths from early layers to later layers."
sameAs(e1, e2)
Comment:

3063	"although these different approaches vary in network topology and training procedure, they all share a key characteristic: they create short paths from early <e1>layers</e1> to later <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3064	"in <e1>this</e1> paper, we propose an architecture that distills <e2>this</e2> insight into a simple connectivity pattern: to ensure maximum information flow between layers in the network, we connect all layers (with matching feature-map sizes) directly with each other."
sameAs(e1, e2)
Comment:

3065	"in this paper, <e1>we</e1> propose an architecture that distills this insight into a simple connectivity pattern: to ensure maximum information flow between layers in the network, <e2>we</e2> connect all layers (with matching feature-map sizes) directly with each other."
sameAs(e1, e2)
Comment:

3066	"in this paper, we propose an architecture that distills this insight into a simple connectivity pattern: to ensure maximum information flow between <e1>layers</e1> in the network, we connect all <e2>layers</e2> (with matching feature-map sizes) directly with each other."
sameAs(e1, e2)
Comment:

3067	"to preserve the feed-forward nature, each layer obtains additional inputs from all preceding <e1>layers</e1> and passes on its own feature-maps to all subsequent <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3068	"crucially, in contrast to resnets, <e1>we</e1> never combine features through summation before they are passed into a layer; instead, <e2>we</e2> combine features by concatenating them."
sameAs(e1, e2)
Comment:

3069	"crucially, in contrast to resnets, we never combine <e1>features</e1> through summation before they are passed into a layer; instead, we combine <e2>features</e2> by concatenating them."
sameAs(e1, e2)
Comment:

3070	"this makes the state of <e1>resnets</e1> similar to (unrolled) recurrent neural networks [21] , but the number of parameters of <e2>resnets</e2> is substantially larger because each layer has its own weights."
sameAs(e1, e2)
Comment:

3071	"our proposed densenet architecture explicitly differentiates between <e1>information</e1> that is added to the network and <e2>information</e2> that is preserved."
sameAs(e1, e2)
Comment:

3072	"our proposed densenet architecture explicitly differentiates between information <e1>that</e1> is added to the network and information <e2>that</e2> is preserved."
sameAs(e1, e2)
Comment:

3073	"densenet layers are very narrow (e.g., 12 filters per layer), adding only a small set of <e1>feature</e1>-maps to the "collective knowledge" of the network and keep the remaining featuremaps unchanged-and the final classifier makes a decision based on all <e2>feature</e2>-maps in the network."
sameAs(e1, e2)
Comment:

3074	"densenet layers are very narrow (e.g., 12 filters per layer), adding only a small set of feature-<e1>maps</e1> to the "collective knowledge" of the network and keep the remaining featuremaps unchanged-and the final classifier makes a decision based on all feature-<e2>maps</e2> in the network."
sameAs(e1, e2)
Comment:

3075	"densenet layers are very narrow (e.g., 12 filters per layer), adding only a small set of feature-maps to the "collective knowledge" of the <e1>network</e1> and keep the remaining featuremaps unchanged-and the final classifier makes a decision based on all feature-maps in the <e2>network</e2>."
sameAs(e1, e2)
Comment:

3076	"the original lenet5 [19] consisted of 5 layers, vgg featured 19 [28] , and only last year highway * authors contributed equally <e1>networks</e1> [33] and residual <e2>networks</e2> (resnets) [11] have surpassed the 100-layer barrier."
sameAs(e1, e2)
Comment:

3077	"abstract <e1>data</e1> in vision domain often exhibit highly-skewed class distribution, i.e., most <e2>data</e2> belong to a few majority classes, while the minority classes only contain a scarce amount of instances."
sameAs(e1, e2)
Comment:

3078	"abstract data in vision domain often exhibit highly-skewed class distribution, i.e., most data belong to a few majority <e1>classes</e1>, while the minority <e2>classes</e2> only contain a scarce amount of instances."
sameAs(e1, e2)
Comment:

3079	"for <e1>image</e1> edge detection, the <e2>image</e2> edge structures intrinsically follow a power-law distribution, e.g., horizontal and vertical edges outnumber those with "y" shape."
sameAs(e1, e2)
Comment:

3080	"for image <e1>edge</e1> detection, the image <e2>edge</e2> structures intrinsically follow a power-law distribution, e.g., horizontal and vertical edges outnumber those with "y" shape."
sameAs(e1, e2)
Comment:

3081	"the first option is re-<e1>sampling</e1>, which aims to balance the class priors by under-<e2>sampling</e2> the majority class or over-sampling the minority class (or both)."
sameAs(e1, e2)
Comment:

3082	"the first option is re-<e1>sampling</e1>, which aims to balance the class priors by under-sampling the majority class or over-<e2>sampling</e2> the minority class (or both)."
sameAs(e1, e2)
Comment:

3083	"the first option is re-sampling, which aims to balance the class priors by under-<e1>sampling</e1> the majority class or over-<e2>sampling</e2> the minority class (or both)."
sameAs(e1, e2)
Comment:

3084	"for instance, oquab et al [32] resample the number of foreground and background <e1>image</e1> patches for learning a convolutional neural network (cnn) for object <e2>classification</e2>."
Used-for(e1, e2)
Comment:

3085	"in image <e1>edge</e1> detection, for example, shen et al [35] regularize the softmax loss to cope with the imbalanced <e2>edge</e2> class distribution."
sameAs(e1, e2)
Comment:

3086	"for instance, over-<e1>sampling</e1> can easily introduce undesirable noise with overfitting risks, and under-<e2>sampling</e2> is often preferred [11] but may remove valuable information."
sameAs(e1, e2)
Comment:

3087	"to this end, 1 an imposter neighbor of a <e1>data</e1> point x i is another <e2>data</e2> point x j with a different class label, y i = y j ."
sameAs(e1, e2)
Comment:

3088	"our key contributions are as follows: (1) <e1>we</e1> show how to learn deep feature embeddings for imbalanced data classification, which is understudied in the literature; (2) <e2>we</e2> formulate a new quintuplet sampling method with the associated triple-header loss that preserves locality across clusters and discrimination between classes."
sameAs(e1, e2)
Comment:

3089	"the representation learned by our <e1>approach</e1>, when combined with a simple k-nearest neighbor (knn) algorithm, shows significant improvements over existing <e2>methods</e2> on both high-and low-level vision classification tasks that exhibit imbalanced class distribution."
Compare(e1, e2)
Comment:

3090	"for instance, the number of positive and negative <e1>face</e1> pairs in <e2>face</e2> verification is highly skewed since it is easier to obtain face images of different identities (negative) than faces with matched identity (positive) during data collection."
sameAs(e1, e2)
Comment:

3091	"for instance, the number of positive and negative <e1>face</e1> pairs in face verification is highly skewed since it is easier to obtain face images of different identities (negative) than <e2>faces</e2> with matched identity (positive) during data collection."
sameAs(e1, e2)
Comment:

3092	"for instance, the number of positive and negative face pairs in <e1>face</e1> verification is highly skewed since it is easier to obtain face images of different identities (negative) than <e2>faces</e2> with matched identity (positive) during data collection."
sameAs(e1, e2)
Comment:

3093	"most previous <e1>methods</e1> have formulated gaze prediction as the problem of saliency detection, and computational <e2>models</e2> of visual saliency have been studied to the find image regions that are likely to attract human attention."
Compare(e1, e2)
Comment:

3094	"in a natural dynamic scene, a person perceives the surrounding <e1>environment</e1> with a series of gaze fixations which point to the objects/regions related to the person's interactions with the <e2>environment</e2>."
sameAs(e1, e2)
Comment:

3095	"we present a new computational model for <e1>gaze</e1> prediction in egocentric videos by exploring patterns in temporal shift of <e2>gaze</e2> fixations (attention transition) that are dependent on egocentric manipulation tasks."
sameAs(e1, e2)
Comment:

3096	"in this paper, we propose a hybrid gaze prediction model that combines bottom-up visual saliency with <e1>task</e1>-dependent attention transition learned from successively attended image regions in training <e2>data</e2>."
Conjunction(e1, e2)
Comment:

3097	"firstly, a person's <e1>gaze</e1> tends to be located on the same object during each fixation, and a large <e2>gaze</e2> shift almost always occurs along with large head motion [23] ."
sameAs(e1, e2)
Comment:

3098	"secondly, patterns in the temporal shift between regions of attention are dependent on the performed <e1>task</e1> and can be learned from <e2>data</e2>."
Conjunction(e1, e2)
Comment:

3099	"the last module is based on a fully convolutional network which fuses the saliency <e1>map</e1> and the attention <e2>map</e2> from the first two modules and generates a final gaze map, from which the final prediction of 2d gaze position is made."
sameAs(e1, e2)
Comment:

3100	"the last module is based on a fully convolutional network which fuses the saliency <e1>map</e1> and the attention map from the first two modules and generates a final gaze <e2>map</e2>, from which the final prediction of 2d gaze position is made."
sameAs(e1, e2)
Comment:

3101	"the last module is based on a fully convolutional network which fuses the saliency map and the attention <e1>map</e1> from the first two modules and generates a final gaze <e2>map</e2>, from which the final prediction of 2d gaze position is made."
sameAs(e1, e2)
Comment:

3102	"the last module is based on a fully convolutional network which fuses the saliency map and the attention map from the first two modules and generates a final <e1>gaze</e1> map, from which the final prediction of 2d <e2>gaze</e2> position is made."
sameAs(e1, e2)
Comment:

3103	"experiments on public egocentric activity datasets show that our <e1>model</e1> significantly outperforms state-of-the-art gaze prediction <e2>methods</e2> and is able to learn meaningful transition of human attention."
Compare(e1, e2)
Comment:

3104	"introduction with the increasing popularity of wearable or action cameras in recording our life experience, egocentric <e1>vision</e1> [1] , which aims at automatic analysis of videos captured from a first-person perspective [21] [4] [6] , has become an emerging field in computer <e2>vision</e2>."
sameAs(e1, e2)
Comment:

3105	"in particular, as the camera wearer's point-of-<e1>gaze</e1> in egocentric video contains important information about interacted objects and the camera wearer's intent [17] , <e2>gaze</e2> prediction can be used to infer important regions in images and videos to reduce the amount of computation needed in learning and inference of various analysis tasks [11] [36] [5] [7] ."
sameAs(e1, e2)
Comment:

3106	"abstract introduction recently, great progress has been achieved by applying deep convolutional neural networks (cnns) to <e1>image</e1> transformation tasks, where a feed-forward cnn receives an input <e2>image</e2>, possibly equipped with some auxiliary information, and transforms it into a desired output image."
sameAs(e1, e2)
Comment:

3107	"abstract introduction recently, great progress has been achieved by applying deep convolutional neural networks (cnns) to <e1>image</e1> transformation tasks, where a feed-forward cnn receives an input image, possibly equipped with some auxiliary information, and transforms it into a desired output <e2>image</e2>."
sameAs(e1, e2)
Comment:

3108	"abstract introduction recently, great progress has been achieved by applying deep convolutional neural networks (cnns) to image transformation tasks, where a feed-forward cnn receives an input <e1>image</e1>, possibly equipped with some auxiliary information, and transforms it into a desired output <e2>image</e2>."
sameAs(e1, e2)
Comment:

3109	"a draw-back of post-processing is <e1>that</e1> it can only deal with image transformations whose edited results have pixel-wise correspondences to their inputs, which is not the case <e2>that</e2> style transfer obeys."
sameAs(e1, e2)
Comment:

3110	"in view of the efficacy of feed-forward networks on image transformation <e1>tasks</e1>, a natural thinking will be whether a feed-forward network can be adapted to video transformation <e2>tasks</e2> by including temporal consistency."
sameAs(e1, e2)
Comment:

3111	"in <e1>this</e1> paper, we testify <e2>this</e2> idea on the problem of video style transfer."
sameAs(e1, e2)
Comment:

3112	"we demonstrate that a feed-forward network can not only capture content and style information in the spatial <e1>domain</e1>, but also encourage consistency in the temporal <e2>domain</e2>."
sameAs(e1, e2)
Comment:

3113	"1 shows an example of directly applying the feed-forward network based <e1>image</e1> style transfer method style <e2>image</e2> figure 1 : video style transfer without and with temporal consistency."
sameAs(e1, e2)
Comment:

3114	"1 shows an example of directly applying the feed-forward network based image style <e1>transfer</e1> method style image figure 1 : video style <e2>transfer</e2> without and with temporal consistency."
sameAs(e1, e2)
Comment:

3115	"in this paper, we seek to explore the speed/<e1>accuracy</e1> trade-off of modern detection <e2>systems</e2> in an exhaustive and fair way."
Evaluate-for(e1, e2)
Comment:

3116	"• <e1>we</e1> describe our flexible and unified implementation of three meta-architectures (faster r-cnn, r-fcn and ssd) in tensorflow which <e2>we</e2> use to do extensive experiments that trace the accuracy/speed tradeoff curve for different detection systems, varying metaarchitecture, feature extractor, image resolution, etc."
sameAs(e1, e2)
Comment:

3117	"• we describe our flexible and unified implementation of three meta-architectures (faster r-cnn, r-fcn and ssd) in tensorflow which we use to do extensive experiments that trace the <e1>accuracy</e1>/speed tradeoff curve for different detection <e2>systems</e2>, varying metaarchitecture, feature extractor, image resolution, etc."
Evaluate-for(e1, e2)
Comment:

3118	"and we identify "sweet spots" on the <e1>accuracy</e1>/speed trade-off curve where gains in <e2>accuracy</e2> are only possible by sacrificing speed (within our family of detectors)."
sameAs(e1, e2)
Comment:

3119	"while the <e1>methods</e1> that win competitions, such as the coco challenge [24] , are optimized for accuracy, they often rely on model ensembling and multicrop <e2>methods</e2> which are too slow for practical usage."
sameAs(e1, e2)
Comment:

3120	"while the methods that win competitions, such as the coco challenge [24] , are optimized for accuracy, they often rely on <e1>model</e1> ensembling and multicrop <e2>methods</e2> which are too slow for practical usage."
Compare(e1, e2)
Comment:

3121	"abstract most modern <e1>face</e1> super-resolution methods resort to convolutional neural networks (cnn) introduction <e2>face</e2> super-resolution (sr), also known as face hallucination, refers to reconstructing high resolution (hr) face images from their corresponding low resolution (lr) inputs."
sameAs(e1, e2)
Comment:

3122	"abstract most modern <e1>face</e1> super-resolution methods resort to convolutional neural networks (cnn) introduction face super-resolution (sr), also known as <e2>face</e2> hallucination, refers to reconstructing high resolution (hr) face images from their corresponding low resolution (lr) inputs."
sameAs(e1, e2)
Comment:

3123	"abstract most modern face <e1>super-resolution</e1> methods resort to convolutional neural networks (cnn) introduction face <e2>super-resolution</e2> (sr), also known as face hallucination, refers to reconstructing high resolution (hr) face images from their corresponding low resolution (lr) inputs."
sameAs(e1, e2)
Comment:

3124	"abstract most modern face super-resolution methods resort to convolutional neural networks (cnn) introduction <e1>face</e1> super-resolution (sr), also known as <e2>face</e2> hallucination, refers to reconstructing high resolution (hr) face images from their corresponding low resolution (lr) inputs."
sameAs(e1, e2)
Comment:

3125	"abstract most modern face super-resolution methods resort to convolutional neural networks (cnn) introduction face super-resolution (sr), also known as face hallucination, refers to reconstructing high <e1>resolution</e1> (hr) <e2>face images</e2> from their corresponding low resolution (lr) inputs."
Feature-of(e1, e2)
Comment:

3126	"abstract most modern face super-resolution methods resort to convolutional neural networks (cnn) introduction face super-resolution (sr), also known as face hallucination, refers to reconstructing high <e1>resolution</e1> (hr) face images from their corresponding low <e2>resolution</e2> (lr) inputs."
sameAs(e1, e2)
Comment:

3127	"we assume that a high-quality hr <e1>image</e1> with abundant textural details and global topology information can be reconstructed via a lr <e2>image</e2> as long as the corresponding wavelet coefficients are accurately predicted."
sameAs(e1, e2)
Comment:

3128	"the wavelet <e1>prediction</e1> loss and texture loss correspond with the wavelet <e2>prediction</e2> subnetwork, imposing constraint in wavelet domain."
sameAs(e1, e2)
Comment:

3129	"the full-<e1>image</e1> loss is used after the reconstruction subnetwork to add a traditional mse constraint in <e2>image</e2> space."
sameAs(e1, e2)
Comment:

3130	"besides, as each wavelet coefficient shares the same size with the low-resolution <e1>input</e1>, we use a network configuration to make every feature map keep the same size with the <e2>input</e2>, which reduces the difficulty of training."
sameAs(e1, e2)
Comment:

3131	"experimental <e1>results</e1> show that our proposed approach outperforms state-of-the-art face sr <e2>methods</e2>."
Compare(e1, e2)
Comment:

3132	"experimental results show that our proposed <e1>approach</e1> outperforms state-of-the-art face sr <e2>methods</e2>."
Compare(e1, e2)
Comment:

3133	" introduction deep learning with 3d data has received great research interests recently, which leads to noticeable advances in typical applications including scene understanding,<e1> shap</e1>e completion, and<e2> shap</e2>e matching."
sameAs(e1, e2)
Comment:

3134	"at the core of our network is a new <e1>convolution</e1> operator, called pointwise <e2>convolution</e2>, which can be applied at each point in a point cloud to learn pointwise features."
sameAs(e1, e2)
Comment:

3135	"among <e1>these</e1>, scene understanding is considered as one of the most important <e2>tasks</e2> for robots and drones as it can assist exploratory scene navigations."
Used-for(e1, e2)
Comment:

3136	"among these, <e1>scene</e1> understanding is considered as one of the most important tasks for robots and drones as it can assist exploratory <e2>scene</e2> navigations."
sameAs(e1, e2)
Comment:

3137	"tasks such as semantic <e1>scene</e1> segmentation and object recognition are often performed to predict contextual information about objects for both indoor and outdoor <e2>scenes</e2>."
sameAs(e1, e2)
Comment:

3138	"volume <e1>representation</e1> is a true 3d <e2>representation</e2> and straightforward to implement but often requires a large amount of memory for data storage."
sameAs(e1, e2)
Comment:

3139	"by contrast, multi-view <e1>representation</e1> is not a true 3d <e2>representation</e2> but shows promising prediction accuracy as existing pre-trained weights from 2d networks can be utilized."
sameAs(e1, e2)
Comment:

3140	"abstract recently, deep convolutional neural networks (cnns) introduction single image super-resolution (sisr) is a classical problem in low-level computer vision, which reconstructs a high-<e1>resolution</e1> (hr) image from a low-<e2>resolution</e2> (lr) image."
sameAs(e1, e2)
Comment:

3141	"abstract recently, deep convolutional neural networks (cnns) introduction single image super-resolution (sisr) is a classical problem in low-level computer vision, which reconstructs a high-resolution (hr) <e1>image</e1> from a low-resolution (lr) <e2>image</e2>."
sameAs(e1, e2)
Comment:

3142	"tai et al propose a deep recursive residual network (drrn) [22] , which employs <e1>parameters</e1> sharing strategy to alleviate the requirement of enormous <e2>parameters</e2> of the very deep networks."
sameAs(e1, e2)
Comment:

3143	"to get a hr <e1>image</e1>, we implement an element-wise addition operation on the residual <e2>image</e2> and the upsampled lr image."
sameAs(e1, e2)
Comment:

3144	"to get a hr <e1>image</e1>, we implement an element-wise addition operation on the residual image and the upsampled lr <e2>image</e2>."
sameAs(e1, e2)
Comment:

3145	"to get a hr image, we implement an element-wise addition operation on the residual <e1>image</e1> and the upsampled lr <e2>image</e2>."
sameAs(e1, e2)
Comment:

3146	"one part represents reserved shortpath <e1>features</e1> and another expresses the short-path <e2>features</e2> that will be enhanced."
sameAs(e1, e2)
Comment:

3147	"in each dblock, the enhancement unit gathers more <e1>information</e1> as much as possible and the compression unit distills more useful <e2>information</e2>."
sameAs(e1, e2)
Comment:

3148	"in order to mitigate this problem, numerous sis-r <e1>methods</e1> have been proposed in the literature, including * corresponding author interpolation-based <e2>methods</e2>, reconstruction-based methods and example-based methods."
sameAs(e1, e2)
Comment:

3149	"in order to mitigate this problem, numerous sis-r <e1>methods</e1> have been proposed in the literature, including * corresponding author interpolation-based methods, reconstruction-based <e2>methods</e2> and example-based methods."
sameAs(e1, e2)
Comment:

3150	"in order to mitigate this problem, numerous sis-r <e1>methods</e1> have been proposed in the literature, including * corresponding author interpolation-based methods, reconstruction-based methods and example-based <e2>methods</e2>."
sameAs(e1, e2)
Comment:

3151	"in order to mitigate this problem, numerous sis-r methods have been proposed in the literature, including * corresponding author interpolation-based <e1>methods</e1>, reconstruction-based <e2>methods</e2> and example-based methods."
sameAs(e1, e2)
Comment:

3152	"in order to mitigate this problem, numerous sis-r methods have been proposed in the literature, including * corresponding author interpolation-based <e1>methods</e1>, reconstruction-based methods and example-based <e2>methods</e2>."
sameAs(e1, e2)
Comment:

3153	"in order to mitigate this problem, numerous sis-r methods have been proposed in the literature, including * corresponding author interpolation-based methods, reconstruction-based <e1>methods</e1> and example-based <e2>methods</e2>."
sameAs(e1, e2)
Comment:

3154	"since the former two kinds of <e1>methods</e1> usually suffer dramatically drop in restoration performance with larger upscaling factors, the recent sr <e2>methods</e2> fall into the example-based methods which try to learn prior knowledge from lr and hr pairs."
sameAs(e1, e2)
Comment:

3155	"since the former two kinds of <e1>methods</e1> usually suffer dramatically drop in restoration performance with larger upscaling factors, the recent sr methods fall into the example-based <e2>methods</e2> which try to learn prior knowledge from lr and hr pairs."
sameAs(e1, e2)
Comment:

3156	"since the former two kinds of methods usually suffer dramatically drop in restoration performance with larger upscaling factors, the recent sr <e1>methods</e1> fall into the example-based <e2>methods</e2> which try to learn prior knowledge from lr and hr pairs."
sameAs(e1, e2)
Comment:

3157	"to mitigate training difficulty, mao et al propose a very deep residual encoder-decoder network (red) [17] , which consists of a series of convolutional and subsequent transposed convolution <e1>layers</e1> to learn end-to-end mappings from the lr <e2>images</e2> to the ground truths."
Part-of(e1, e2)
Comment:

3158	"flownet2 and spynet warp the second <e1>image</e1> towards the first <e2>image</e2> in the pair using the previous flow estimate, and then refine the estimate using the feature maps generated by the warped and the first images."
sameAs(e1, e2)
Comment:

3159	"flownet2 and spynet warp the second image towards the <e1>first</e1> image in the pair using the previous flow estimate, and then refine the estimate using the feature maps generated by the warped and the <e2>first</e2> images."
sameAs(e1, e2)
Comment:

3160	"warping an <e1>image</e1> and then generating the feature maps of the warped <e2>image</e2> are two ordered steps."
sameAs(e1, e2)
Comment:

3161	"this one-step <e1>feature</e1> warping process reduces the more discriminative <e2>feature</e2>-space distance instead of the rgb-space distance between the two images."
sameAs(e1, e2)
Comment:

3162	"this one-step feature warping process reduces the more discriminative feature-space <e1>distance</e1> instead of the rgb-space <e2>distance</e2> between the two images."
sameAs(e1, e2)
Comment:

3163	"we now highlight the more specific differences between our network and existing cnn-based optical <e1>flow</e1> estimation frameworks: 1) cascaded <e2>flow</e2> inference -at each pyramid level, we introduce a novel cascade of two lightweight networks."
sameAs(e1, e2)
Comment:

3164	"each of them has a <e1>feature</e1> warping (f-warp) layer to displace the <e2>feature</e2> maps of the second image towards the first image using the flow estimate from the previous level."
sameAs(e1, e2)
Comment:

3165	"each of them has a feature warping (f-warp) layer to displace the feature maps of the second <e1>image</e1> towards the first <e2>image</e2> using the flow estimate from the previous level."
sameAs(e1, e2)
Comment:

3166	"since at each pyramid level the <e1>feature</e1>-space distance between the images has been reduced by <e2>feature</e2> warping, we can use a rather short displacement than [9, 14] to establish the cost volume."
sameAs(e1, e2)
Comment:

3167	"2) <e1>flow</e1> regularization -the cascaded <e2>flow</e2> inference resembles the role of data fidelity in energy minimization methods."
sameAs(e1, e2)
Comment:

3168	"to tackle this problem, local <e1>flow</e1> consistency and co-occurrence between <e2>flow</e2> boundaries and intensity edges are commonly used as the cues to regularize flow field."
sameAs(e1, e2)
Comment:

3169	"some of the representative methods include anisotropic <e1>image</e1>-driven [32] , <e2>image</e2>-and flow-driven [28] , and complementary [36] regularizations."
sameAs(e1, e2)
Comment:

3170	"this makes the <e1>flow</e1> regularization to be both <e2>flow</e2>-and image-aware."
sameAs(e1, e2)
Comment:

3171	"e.g., brightness constraint in data fidelity to pyramidal <e1>cnn</e1> features and image warping to <e2>cnn</e2> feature warping."
sameAs(e1, e2)
Comment:

3172	"more specifically, we present a cascaded <e1>flow</e1> inference with feature warping and <e2>flow</e2> regularization in each pyramid level, which are new in the literature."
sameAs(e1, e2)
Comment:

3173	"overall, our network outperforms flownet [9] and spynet [21] and is on par with the recent flownet2 [14] , while having 30 <e1>times</e1> fewer parameters and being 1.36 <e2>times</e2> faster than flownet2."
sameAs(e1, e2)
Comment:

3174	"to push the envelop of accuracy, flownet2 is designed as a <e1>cascade</e1> of variants of flownet that each network in the <e2>cascade</e2> refines the preceding flow field by contributing on the flow increment between the first image and the warped second image."
sameAs(e1, e2)
Comment:

3175	"to push the envelop of accuracy, flownet2 is designed as a cascade of variants of flownet that each network in the cascade refines the preceding flow field by contributing on the flow increment between the first <e1>image</e1> and the warped second <e2>image</e2>."
sameAs(e1, e2)
Comment:

3176	"a recent <e1>network</e1> termed spynet [21] attempts a <e2>network</e2> with smaller size of 1.2m parameters by adopting image warping in each pyramid level."
sameAs(e1, e2)
Comment:

3177	"abstract <e1>backpropagation algorithm</e1> is indispensable for the training of feedforward <e2>neural networks</e2>."
isA(e1, e2)
Comment:

3178	"most neural networks are trained using <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) or its variants in which the gradients of the networks are computed by backpropagation algorithm (rumelhart et al, 1988) ."
sameAs(e1, e2)
Comment:

3179	"as shown in figure 1 , the <e1>backpropagation algorithm</e1> consists of two processes, the  we split a multilayer feedforward <e2>neural network</e2> into three modules."
isA(e1, e2)
Comment:

3180	"for example, <e1>module</e1> a cannot perform step 6 before receiving δ t a which is an output of step 5 in <e2>module</e2> b. forward pass to compute prediction and the backward pass to compute gradient and update the model."
sameAs(e1, e2)
Comment:

3181	"from alexnet with 8 <e1>layers</e1> (krizhevsky et al, 2012) to resnet-101 with more than one hundred <e2>layers</e2> (he et al, 2016) , the forward and backward time grow from (4.31ms and 9.58ms) to (53.38ms and 103.06ms) when we train the networks on titan x with the input size of 16×3×224×224 (johnson, 2017) ."
sameAs(e1, e2)
Comment:

3182	"therefore, parallelizing the backward pass can greatly reduce the <e1>training time</e1> when the backward <e2>time</e2> is about twice of the forward time."
Used-for(e1, e2)
Comment:

3183	"therefore, parallelizing the backward pass can greatly reduce the <e1>training time</e1> when the backward time is about twice of the forward <e2>time</e2>."
Used-for(e1, e2)
Comment:

3184	"therefore, parallelizing the backward pass can greatly reduce the training time when the backward <e1>time</e1> is about twice of the forward <e2>time</e2>."
sameAs(e1, e2)
Comment:

3185	"for example, (jaderberg et al, 2016; czarnecki et al, 2017) proposed to remove the lockings in <e1>backpropagation</e1> by employing additional <e2>neural networks</e2> to approximate error gradients."
isA(e1, e2)
Comment:

3186	"in this paper, <e1>we</e1> focus on breaking the backward locking in backpropagtion algorithm for training feedforward neural networks, such that <e2>we</e2> can update models in parallel without loss of accuracy."
sameAs(e1, e2)
Comment:

3187	"• we also provide <e1>convergence</e1> analysis for the proposed method in section 4 and prove that it guarantees <e2>convergence</e2> to critical points for the non-convex problem."
sameAs(e1, e2)
Comment:

3188	"• finally, we perform experiments for <e1>training</e1> deep convolutional neural networks in section 5, experimental results verifying that the proposed method can significantly speed up the <e2>training</e2> without loss of accuracy."
sameAs(e1, e2)
Comment:

3189	"firstly, <e1>we</e1> decouple the backpropagation algorithm using delayed gradients, and show that the backward locking is removed when <e2>we</e2> split the networks into multiple modules."
sameAs(e1, e2)
Comment:

3190	"this form of <e1>color</e1> correction can benefit downstream applications such as visual recognition, where <e2>color</e2> is an important feature for distinguishing objects."
sameAs(e1, e2)
Comment:

3191	"in particular, when compared to <e1>prior</e1> art on wider face, our results reduce error by a factor of 2 (our models produce an ap of 81% while <e2>prior</e2> art ranges from 29-64%)."
sameAs(e1, e2)
Comment:

3192	"abstract we propose a novel probabilistic model for visual <e1>question answering</e1> (visual qa introduction visual <e2>question answering</e2> (visual qa) has made significant progress in the last few years."
sameAs(e1, e2)
Comment:

3193	"in this paper, we propose end-to-end module networks (n2nmns): a class of <e1>models</e1> capable of predicting novel modular network architectures directly from textual input and applying them to images in order to solve question answering <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

3194	"this is a serious limitation, because off-the-shelf <e1>language</e1> parsers are not designed for <e2>language</e2> and vision tasks and must therefore be modified using handcrafted rules that often fail to predict valid layouts [15] ."
sameAs(e1, e2)
Comment:

3195	"this is a serious limitation, because off-the-shelf <e1>language</e1> parsers are not designed for language and <e2>vision</e2> tasks and must therefore be modified using handcrafted rules that often fail to predict valid layouts [15] ."
Conjunction(e1, e2)
Comment:

3196	"this is a serious limitation, because off-the-shelf language parsers are not designed for <e1>language</e1> and <e2>vision</e2> tasks and must therefore be modified using handcrafted rules that often fail to predict valid layouts [15] ."
Conjunction(e1, e2)
Comment:

3197	"our contributions are 1) a method for learning a layout policy <e1>that</e1> dynamically predicts a network structure for each instance, without the aid of external linguistic resources at test time and 2) a new module parameterization <e2>that</e2> uses a soft attention over question words rather than hard-coded word assignments."
sameAs(e1, e2)
Comment:

3198	"it uses these to assemble a concrete <e1>network</e1> architecture, and then executes the assembled neural module <e2>network</e2> to output an answer for visual question answering."
sameAs(e1, e2)
Comment:

3199	"because there are multiple women in the image, resolving <e1>this</e1> referential expression requires both finding a bounding box that contains a person, and ensuring that <e2>this</e2> bounding box relates in the right way to other objects in the scene."
sameAs(e1, e2)
Comment:

3200	"because there are multiple women in the image, resolving this referential expression requires both finding a bounding box <e1>that</e1> contains a person, and ensuring <e2>that</e2> this bounding box relates in the right way to other objects in the scene."
sameAs(e1, e2)
Comment:

3201	"previous work on grounding referential <e1>expressions</e1> either (1) treats referential <e2>expressions</e2> holistically, thus failing to model explicit correspondence between textual components and visual entities in the image [19, 10, 24, 32, 20] , or else (2) relies on a fixed set of entity and relationship categories defined a priori [17] ."
sameAs(e1, e2)
Comment:

3202	"in this paper, we present a joint <e1>approach</e1> that explicitly <e2>models</e2> the compositional linguistic structure of referential expressions and their groundings, but which nonetheless supports interpretation of arbitrary language."
Used-for(e1, e2)
Comment:

3203	"there are <e1>two</e1> types of modules in our model, one used for localizing specific textual components by outputting unary scores over regions for that component, and one for determining the relationship between <e2>two</e2> pairs of bounding boxes by outputting pairwise scores over region-region pairs."
sameAs(e1, e2)
Comment:

3204	"there are two types of modules in our model, <e1>one</e1> used for localizing specific textual components by outputting unary scores over regions for that component, and <e2>one</e2> for determining the relationship between two pairs of bounding boxes by outputting pairwise scores over region-region pairs."
sameAs(e1, e2)
Comment:

3205	"there are two types of modules in our model, one used for localizing specific textual <e1>components</e1> by outputting unary scores over regions for that <e2>component</e2>, and one for determining the relationship between two pairs of bounding boxes by outputting pairwise scores over region-region pairs."
isA(e1, e2)
Comment:

3206	"there are two types of modules in our model, one used for localizing specific textual components by outputting unary scores over <e1>regions</e1> for that component, and one for determining the relationship between two pairs of bounding boxes by outputting pairwise scores over <e2>region</e2>-region pairs."
sameAs(e1, e2)
Comment:

3207	"we evaluate our <e1>model</e1> on multiple datasets, and show that our <e2>model</e2> outperforms both natural baselines and previous work."
sameAs(e1, e2)
Comment:

3208	"given an <e1>image</e1> and a natural language expression referring to a visual entity, such as the young man wearing green shirt and riding a black bicycle, these approaches localize the <e2>image</e2> region corresponding to the entity that the expression refers to with a bounding box."
sameAs(e1, e2)
Comment:

3209	"referential expressions often describe relationships be-  given an <e1>image</e1> and an expression, we learn to parse the expression into vector representation of subject q subj , relationship q rel and object q obj with attention, and align these textual components to <e2>image</e2> regions with two types of modules."
sameAs(e1, e2)
Comment:

3210	"the localization <e1>module</e1> outputs scores over each individual region while the relationship <e2>module</e2> produces scores over region pairs."
sameAs(e1, e2)
Comment:

3211	"the localization module outputs scores over each individual <e1>region</e1> while the relationship module produces scores over <e2>region</e2> pairs."
sameAs(e1, e2)
Comment:

3212	"these outputs are integrated into final scores over <e1>region</e1> pairs, producing the top <e2>region</e2> pair as grounding result."
sameAs(e1, e2)
Comment:

3213	"using text to express <e1>colors</e1> can allow ample room for creativity, and it would be useful to visualize the <e2>colors</e2> of a certain semantic concept."
sameAs(e1, e2)
Comment:

3214	"for instance, since <e1>colors</e1> can leave a strong impression on people [19] , corporations often decide upon the season's <e2>color</e2> theme from marketing concepts such as 'passion.'"
sameAs(e1, e2)
Comment:

3215	"since our model uses <e1>text</e1> to visualize aesthetic concepts, its range of future applications can encompass <e2>text</e2> to even speech."
sameAs(e1, e2)
Comment:

3216	"previous methods have a limited range of applications as they only take a single word as input and can recommend only a single <e1>color</e1> or a <e2>color</e2> palette in pre-existing datasets [12, 8, 15, 25] ."
sameAs(e1, e2)
Comment:

3217	"other studies have further attempted to link a single word with a multi-<e1>color</e1> palette [21, 36] since multi-<e2>color</e2> palettes are highly expressive in conveying semantics [18] ."
sameAs(e1, e2)
Comment:

3218	"in this paper, we propose a novel method to generate multiple <e1>color</e1> palettes that convey the semantics of rich text and then colorize a given grayscale image according to the generated <e2>color</e2> palette."
sameAs(e1, e2)
Comment:

3219	"perception of <e1>color</e1> is inherently multimodal [4] , meaning that a particular text input can be mapped to multiple possible <e2>color</e2> palettes."
sameAs(e1, e2)
Comment:

3220	"this paper proposes a novel approach to generate multiple <e1>color</e1> palettes that reflect the semantics of input text and then colorize a given grayscale image according to the generated <e2>color</e2> palette."
sameAs(e1, e2)
Comment:

3221	"motivated from previous user-guided colorizations that utilize <e1>color</e1> hints given by users [42, 44] , we design our colorization networks to utilize <e2>color</e2> palettes during the colorization process."
sameAs(e1, e2)
Comment:

3222	"(3) we introduce our manually curated dataset called palette-and-<e1>text</e1> (pat), which includes 10,183 pairs of a multi-word <e2>text</e2> and a multi-color palette."
sameAs(e1, e2)
Comment:

3223	"our proposed model called text2colors consists of two conditional generative adversarial networks: the text-topalette generation <e1>networks</e1> and the palette-based colorization <e2>networks</e2>."
sameAs(e1, e2)
Comment:

3224	"our evaluation results show <e1>that</e1> people preferred our generated palettes over ground truth palettes and <e2>that</e2> our model can effectively reflect the given palette when colorizing an image."
sameAs(e1, e2)
Comment:

3225	"larger <e1>sensors</e1> and high-aperture optics yield better photo resolution, color rendition and less noise, whereas their additional <e2>sensors</e2> help to fine-tune shooting parameters."
sameAs(e1, e2)
Comment:

3226	"besides <e1>that</e1>, they are usually based on a pre-defined set of rules <e2>that</e2> do not always consider the specifics of a particular device."
sameAs(e1, e2)
Comment:

3227	"an additional challenge is to discover key <e1>instances</e1> (liu et al, 2012) , i.e., the <e2>instances</e2> that trigger the bag label."
sameAs(e1, e2)
Comment:

3228	"in order to solve the primary task of a bag classification different methods are proposed, such as utilizing similarities among bags (cheplygina et al, 2015b) , embedding instances to a compact low-dimensional representation that is further fed to a bag-level <e1>classifier</e1> (andrews et al, 2003; chen et al, 2006) , and combining responses of an instance-level <e2>classifier</e2> (ramon & de raedt, 2000; raykar et al, 2008; zhang et al, 2006) ."
sameAs(e1, e2)
Comment:

3229	"however, it was shown that the instance level accuracy of such <e1>methods</e1> is low (kandemir & hamprecht, 2015) and in general there is a disagreement among mil <e2>methods</e2> at the instance level (cheplygina et al, 2015a) ."
sameAs(e1, e2)
Comment:

3230	"we show <e1>that</e1> the application of the fundamental theorem of symmetric functions provides a general procedure for modeling the bag label probability (the bag score function) <e2>that</e2> consists of three steps: (i) a transformation of instances to a low-dimensional embedding, (ii) a permutation-invariant (symmetric) aggregation function, and (iii) a final transformation to the bag probability."
sameAs(e1, e2)
Comment:

3231	"we show that the application of the fundamental theorem of symmetric functions provides a general procedure for modeling the bag label probability (the bag score <e1>function</e1>) that consists of three steps: (i) a transformation of instances to a low-dimensional embedding, (ii) a permutation-invariant (symmetric) aggregation <e2>function</e2>, and (iii) a final transformation to the bag probability."
sameAs(e1, e2)
Comment:

3232	"in the experiments we show <e1>that</e1> our model is on a par with the best classical mil methods on common benchmark mil datasets, and <e2>that</e2> it outperforms other methods on a mnist-based mil problem as well as two real-life histopathology image datasets."
sameAs(e1, e2)
Comment:

3233	"in the experiments we show that our <e1>model</e1> is on a par with the best classical mil <e2>methods</e2> on common benchmark mil datasets, and that it outperforms other methods on a mnist-based mil problem as well as two real-life histopathology image datasets."
Compare(e1, e2)
Comment:

3234	"in the experiments we show that our <e1>model</e1> is on a par with the best classical mil methods on common benchmark mil datasets, and that it outperforms other <e2>methods</e2> on a mnist-based mil problem as well as two real-life histopathology image datasets."
Compare(e1, e2)
Comment:

3235	"in the experiments we show that our model is on a par with the best classical mil <e1>methods</e1> on common benchmark mil datasets, and that it outperforms other <e2>methods</e2> on a mnist-based mil problem as well as two real-life histopathology image datasets."
sameAs(e1, e2)
Comment:

3236	"in the experiments we show that our model is on a par with the best classical mil methods on common benchmark mil <e1>datasets</e1>, and that it outperforms other methods on a mnist-based mil problem as well as two real-life histopathology image <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

3237	"in the experiments we show that our model is on a par with the best classical mil methods on common benchmark mil datasets, and that it outperforms other methods on a <e1>mnist</e1>-based mil problem as well as two real-life histopathology image <e2>datasets</e2>."
isA(e1, e2)
Comment:

3238	"we show empirically that our <e1>approach</e1> achieves comparable performance to the best mil <e2>methods</e2> on benchmark mil datasets and it outperforms other methods on a mnist-based mil dataset and two real-life histopathology datasets without sacrificing interpretability."
Compare(e1, e2)
Comment:

3239	"we show empirically that our <e1>approach</e1> achieves comparable performance to the best mil methods on benchmark mil datasets and it outperforms other <e2>methods</e2> on a mnist-based mil dataset and two real-life histopathology datasets without sacrificing interpretability."
Compare(e1, e2)
Comment:

3240	"we show empirically that our approach achieves comparable performance to the best mil <e1>methods</e1> on benchmark mil datasets and it outperforms other <e2>methods</e2> on a mnist-based mil dataset and two real-life histopathology datasets without sacrificing interpretability."
sameAs(e1, e2)
Comment:

3241	"we show empirically that our approach achieves comparable performance to the best mil methods on benchmark mil <e1>datasets</e1> and it outperforms other methods on a mnist-based mil dataset and two real-life histopathology <e2>datasets</e2> without sacrificing interpretability."
sameAs(e1, e2)
Comment:

3242	"we show empirically that our approach achieves comparable performance to the best mil methods on benchmark mil datasets and it outperforms other methods on a <e1>mnist</e1>-based mil dataset and two real-life histopathology <e2>datasets</e2> without sacrificing interpretability."
isA(e1, e2)
Comment:

3243	"abstract current <e1>neural network</e1>-based <e2>classifiers</e2> are susceptible to adversarial examples even in the black-box setting, where the attacker only has query access to the model."
Used-for(e1, e2)
Comment:

3244	"one <e1>approach</e1> to attacking a <e2>classifier</e2> in this setting trains * equal contribution 1 massachusetts institute of technology 2 labsix."
Used-for(e1, e2)
Comment:

3245	"in practice, the threat <e1>model</e1> for real-world systems is often more restrictive than the typical black-box <e2>model</e2> where the adversary can observe the full output of the network on arbitrarily many chosen inputs."
sameAs(e1, e2)
Comment:

3246	"introduction <e1>neural network</e1>-based image <e2>classifiers</e2> are susceptible to adversarial examples, minutely perturbed inputs that fool classifiers (szegedy et al, 2013; biggio et al, 2013) ."
Used-for(e1, e2)
Comment:

3247	"introduction <e1>neural network</e1>-based image classifiers are susceptible to adversarial examples, minutely perturbed inputs that fool <e2>classifiers</e2> (szegedy et al, 2013; biggio et al, 2013) ."
Used-for(e1, e2)
Comment:

3248	"introduction neural network-based image <e1>classifiers</e1> are susceptible to adversarial examples, minutely perturbed inputs that fool <e2>classifiers</e2> (szegedy et al, 2013; biggio et al, 2013) ."
sameAs(e1, e2)
Comment:

3249	"2 : anatomy of a splice: one of the most common ways of creative fake <e1>images</e1> is splicing together content from two different real source <e2>images</e2>."
sameAs(e1, e2)
Comment:

3250	"the two input images above might look plausible, but our model correctly determined that <e1>they</e1> have been manipulated because <e2>they</e2> lack self-consistency: the visual information within the predicted splice region was found to be inconsistent with the rest of the image."
sameAs(e1, e2)
Comment:

3251	"all layers, or more generally, modules, of the <e1>network</e1> are therefore locked, in the sense that they must wait for the remainder of the <e2>network</e2> to execute forwards and propagate error backwards before they can be updated."
sameAs(e1, e2)
Comment:

3252	"all layers, or more generally, modules, of the network are therefore locked, in the sense that <e1>they</e1> must wait for the remainder of the network to execute forwards and propagate error backwards before <e2>they</e2> can be updated."
sameAs(e1, e2)
Comment:

3253	"in <e1>this</e1> work we break <e2>this</e2> constraint by decoupling modules by introducing a model of the future computation of the network graph."
sameAs(e1, e2)
Comment:

3254	"in particular <e1>we</e1> focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients <e2>we</e2> decouple subgraphs, and can update them independently and asynchronously i.e."
sameAs(e1, e2)
Comment:

3255	"in particular we focus on modelling error <e1>gradients</e1>: by using the modelled synthetic <e2>gradient</e2> in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e."
sameAs(e1, e2)
Comment:

3256	"in particular we focus on modelling error <e1>gradients</e1>: by using the modelled synthetic gradient in place of true backpropagated error <e2>gradients</e2> we decouple subgraphs, and can update them independently and asynchronously i.e."
sameAs(e1, e2)
Comment:

3257	"in particular we focus on modelling error gradients: by using the modelled synthetic <e1>gradient</e1> in place of true backpropagated error <e2>gradients</e2> we decouple subgraphs, and can update them independently and asynchronously i.e."
sameAs(e1, e2)
Comment:

3258	"we show results for feed-forward models, where every layer is trained asynchronously, <e1>recurrent neural networks</e1> (<e2>rnns</e2>) where predicting one's future gradient extends the time over which the rnn can effectively model, and also a hierarchical rnn system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3259	"we show results for feed-forward models, where every layer is trained asynchronously, <e1>recurrent neural networks</e1> (rnns) where predicting one's future gradient extends the time over which the <e2>rnn</e2> can effectively model, and also a hierarchical rnn system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3260	"we show results for feed-forward models, where every layer is trained asynchronously, <e1>recurrent neural networks</e1> (rnns) where predicting one's future gradient extends the time over which the rnn can effectively model, and also a hierarchical <e2>rnn</e2> system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3261	"we show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (<e1>rnns</e1>) where predicting one's future gradient extends the time over which the <e2>rnn</e2> can effectively model, and also a hierarchical rnn system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3262	"we show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (<e1>rnns</e1>) where predicting one's future gradient extends the time over which the rnn can effectively model, and also a hierarchical <e2>rnn</e2> system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3263	"we show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (rnns) where predicting one's future gradient extends the time over which the <e1>rnn</e1> can effectively model, and also a hierarchical <e2>rnn</e2> system with ticking at different timescales."
sameAs(e1, e2)
Comment:

3264	"finally, we demonstrate <e1>that</e1> in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -amounting to independent networks which co-learn such <e2>that</e2> they can be composed into a single functioning corporation."
sameAs(e1, e2)
Comment:

3265	"after receiving the message ha from a, b can use its model of a, mb, to send back synthetic gradientsˆ a which are trained to approximate real error <e1>gradients</e1> a. note that a does not need to wait for any extra computation after itself to get the correct error <e2>gradients</e2>, hence decoupling the backward computation."
sameAs(e1, e2)
Comment:

3266	"after receiving the message ha from a, b can use its model of a, mb, to send back synthetic gradientsˆ a which are trained to approximate real error gradients a. note that a does not need to wait for any extra <e1>computation</e1> after itself to get the correct error gradients, hence decoupling the backward <e2>computation</e2>."
sameAs(e1, e2)
Comment:

3267	"we analyze <e1>image</e1> specificity with respect to image content and properties to better understand what makes an <e2>image</e2> specific."
sameAs(e1, e2)
Comment:

3268	"we then train models to automatically predict the specificity of an <e1>image</e1> from <e2>image</e2> features alone without requiring textual descriptions of the image."
sameAs(e1, e2)
Comment:

3269	"we then train models to automatically predict the specificity of an <e1>image</e1> from image features alone without requiring textual descriptions of the <e2>image</e2>."
sameAs(e1, e2)
Comment:

3270	"we then train models to automatically predict the specificity of an image from <e1>image</e1> features alone without requiring textual descriptions of the <e2>image</e2>."
sameAs(e1, e2)
Comment:

3271	"finally, we show that modeling <e1>image</e1> specificity leads to improvements in a text-based <e2>image</e2> retrieval application."
sameAs(e1, e2)
Comment:

3272	"one such task <e1>that</e1> is receiving increased attention in recent years is <e2>that</e2> of automatically generating textual descriptions of images [5, 8, 13, 14, 23, 25, 28, 31, 35, 37, 38, 47, 51] and evaluating these descriptions [11, 32, 38, 39] ."
sameAs(e1, e2)
Comment:

3273	"one such task that is receiving increased attention in recent years is that of automatically generating textual <e1>descriptions</e1> of images [5, 8, 13, 14, 23, 25, 28, 31, 35, 37, 38, 47, 51] and evaluating these <e2>descriptions</e2> [11, 32, 38, 39] ."
sameAs(e1, e2)
Comment:

3274	"in fact, early works <e1>that</e1> tackled the image description problem [14] or reasoned about what image content is important and frequently described [3] claimed <e2>that</e2> human descriptions are consistent."
sameAs(e1, e2)
Comment:

3275	"we introduce the notion of <e1>image</e1> specificity which measures the amount of variance in multiple viable descriptions of the same <e2>image</e2>."
sameAs(e1, e2)
Comment:

3276	"in <e1>other</e1> words, some images are specific -they elicit consistent descriptions from different people -while <e2>other</e2> images are ambiguous."
sameAs(e1, e2)
Comment:

3277	"in other words, some <e1>images</e1> are specific -they elicit consistent descriptions from different people -while other <e2>images</e2> are ambiguous."
sameAs(e1, e2)
Comment:

3278	"may want to pick specific <e1>images</e1> -<e2>images</e2> that are likely to have a single (intended) interpretation across viewers."
sameAs(e1, e2)
Comment:

3279	"given multiple human-generated <e1>descriptions</e1> of an image, we measure specificity using two different mechanisms: one requiring human judgement of similarities between two <e2>descriptions</e2>, and the other using an automatic textual similarity measure."
sameAs(e1, e2)
Comment:

3280	"given multiple human-generated descriptions of an image, we measure specificity using <e1>two</e1> different mechanisms: one requiring human judgement of similarities between <e2>two</e2> descriptions, and the other using an automatic textual similarity measure."
sameAs(e1, e2)
Comment:

3281	"given multiple human-generated descriptions of an image, we measure specificity using two different mechanisms: <e1>one</e1> requiring human judgement of similarities between two descriptions, and the <e2>other</e2> using an automatic textual similarity measure."
Conjunction(e1, e2)
Comment:

3282	"we find that <e1>images</e1> with people tend to be specific, while mundane <e2>images</e2> of generic buildings or blue skies do not tend to be specific."
sameAs(e1, e2)
Comment:

3283	"applications involving <e1>images</e1> and text can benefit from an understanding of which <e2>images</e2> are specific and which ones are ambiguous."
sameAs(e1, e2)
Comment:

3284	"we then train models that can predict the specificity of an <e1>image</e1> just by using <e2>image</e2> features (without associated human-generated descriptions)."
sameAs(e1, e2)
Comment:

3285	"finally, we leverage <e1>image</e1> specificity to improve performance in a real-world application: text-based <e2>image</e2> retrieval."
sameAs(e1, e2)
Comment:

3286	"if a query description is moderately similar to the caption (or reference description) of an ambiguous <e1>image</e1>, that query may be considered a decent match to the <e2>image</e2>."
sameAs(e1, e2)
Comment:

3287	"but if the <e1>image</e1> is very specific, a moderate similarity between the query and the reference description may not be sufficient to retrieve the <e2>image</e2>."
sameAs(e1, e2)
Comment:

3288	"we present two mechanisms to <e1>measure</e1> specificity given multiple descriptions of an image: an automated <e2>measure</e2> and a measure that relies on human judgement."
sameAs(e1, e2)
Comment:

3289	"we present two mechanisms to <e1>measure</e1> specificity given multiple descriptions of an image: an automated measure and a <e2>measure</e2> that relies on human judgement."
sameAs(e1, e2)
Comment:

3290	"we present two mechanisms to measure specificity given multiple descriptions of an image: an automated <e1>measure</e1> and a <e2>measure</e2> that relies on human judgement."
sameAs(e1, e2)
Comment:

3291	"to address this, practitioners in large scale retrieval and recommendation systems often resort to a separate post-processing step where the learned embedding representation is run through <e1>quantization</e1> pipelines such as sketches, hashing, and vector <e2>quantization</e2> in order to significantly reduce the number of data to compare during the inference while trading off the accuracy."
sameAs(e1, e2)
Comment:

3292	"in contrast to some of the recent <e1>methods</e1> (cao et al, 2016; liu & lu, 2017) , our proposed <e2>method</e2> avoids ever having to cluster the entire dataset, offers the modularity to accommodate any existing deep embedding learning techniques (schroff et al, 2015; sohn, 2016) , and is efficiently trained in a mini-batch stochastic gradient descent setting."
Compare(e1, e2)
Comment:

3293	"the proposed method alternates between finding the optimal hash codes of the given embedding <e1>representations</e1> in the mini-batch and adjusting the embedding <e2>representations</e2> indexed at the activated hash code dimensions via deep metric learning methods."
sameAs(e1, e2)
Comment:

3294	"our end-to-end learning <e1>method</e1> outperforms state-of-theart deep metric learning approaches (schroff et al, 2015; sohn, 2016) in retrieval and clustering <e2>tasks</e2> on the cifar-100 (krizhevsky et al, 2009 ) and the imagenet (russakovsky et al, 2015) datasets while providing up to several orders of magnitude speedup during inference."
Used-for(e1, e2)
Comment:

3295	"our method utilizes efficient off-the-shelf implementations from or-<e1>tools</e1> (google optimization <e2>tools</e2> for combinatorial optimization problems) (or-tools, 2018) , the deep metric learning library implementation in tensorflow (abadi et al, 2015) , and is efficient to train."
sameAs(e1, e2)
Comment:

3296	"our method utilizes efficient off-the-shelf implementations from or-<e1>tools</e1> (google optimization tools for combinatorial optimization problems) (or-<e2>tools</e2>, 2018) , the deep metric learning library implementation in tensorflow (abadi et al, 2015) , and is efficient to train."
sameAs(e1, e2)
Comment:

3297	"our method utilizes efficient off-the-shelf implementations from or-tools (google optimization <e1>tools</e1> for combinatorial optimization problems) (or-<e2>tools</e2>, 2018) , the deep metric learning library implementation in tensorflow (abadi et al, 2015) , and is efficient to train."
sameAs(e1, e2)
Comment:

3298	"the state of the art deep metric learning <e1>approaches</e1> (schroff et al, 2015; sohn, 2016) use the class labels during training (for the hard negative mining procedure) and since our method utilizes the <e2>approaches</e2> as a component, we focus on the settings where the class labels are available during training."
sameAs(e1, e2)
Comment:

3299	"the state of the art deep metric learning approaches (schroff et al, 2015; sohn, 2016) use the class labels during <e1>training</e1> (for the hard negative mining procedure) and since our method utilizes the approaches as a component, we focus on the settings where the class labels are available during <e2>training</e2>."
sameAs(e1, e2)
Comment:

3300	"while much effort has been put in developing algorithms for learning binary hamming code representations for <e1>search</e1> efficiency, this still requires a linear scan of the entire dataset per each query and trades off the <e2>search</e2> accuracy through binarization."
sameAs(e1, e2)
Comment:

3301	"to this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant <e1>search</e1> reduction in the number of data but also achieving the state of the art <e2>search</e2> accuracy outperforming previous state of the art deep metric learning methods."
sameAs(e1, e2)
Comment:

3302	"to this end, we consider the problem of directly learning a quantizable embedding representation and the sparse binary hash code end-to-end which can be used to construct an efficient hash table not only providing significant search reduction in the number of data but also achieving the <e1>state</e1> of the art search accuracy outperforming previous <e2>state</e2> of the art deep metric learning methods."
sameAs(e1, e2)
Comment:

3303	"our results on cifar-100 and on imagenet datasets show the state of the art <e1>search</e1> accuracy in precision@k and nmi metrics while providing up to 98× and 478× <e2>search</e2> speedup respectively over exhaustive linear search."
sameAs(e1, e2)
Comment:

3304	"our results on cifar-100 and on imagenet datasets show the state of the art <e1>search</e1> accuracy in precision@k and nmi metrics while providing up to 98× and 478× search speedup respectively over exhaustive linear <e2>search</e2>."
sameAs(e1, e2)
Comment:

3305	"our results on cifar-100 and on imagenet datasets show the state of the art search <e1>accuracy</e1> in <e2>precision</e2>@k and nmi metrics while providing up to 98× and 478× search speedup respectively over exhaustive linear search."
Conjunction(e1, e2)
Comment:

3306	"our results on cifar-100 and on imagenet datasets show the state of the art search accuracy in precision@k and nmi metrics while providing up to 98× and 478× <e1>search</e1> speedup respectively over exhaustive linear <e2>search</e2>."
sameAs(e1, e2)
Comment:

3307	"to this end, deep metric learning methods (hadsell et al, 2006; weinberger et al, 2006; schroff et al, 2015; song et al, 2016; sohn, 2016; song et al, 2017; bell & bala, 2015; sener et al, 2016) aim to learn an embedding representation space such that similar <e1>data</e1> are close to each other and vice versa for dissimilar <e2>data</e2>."
sameAs(e1, e2)
Comment:

3308	"in steganography, the goal is secret communication: a sender (alice) encodes a message in an <e1>image</e1> such that the recipient (bob) can decode the message, but an adversary (eve) cannot tell whether any given <e2>image</e2> contains a message or not; eve's task of detecting encoded images is called steganalysis."
sameAs(e1, e2)
Comment:

3309	"in digital watermarking, the goal is to encode information robustly: alice wishes to encode a fingerprint in an <e1>image</e1>; eve will then somehow distort the <e2>image</e2> (by cropping, blurring, etc), and bob should be able to detect the fingerprint in the distorted image."
sameAs(e1, e2)
Comment:

3310	"in digital watermarking, the goal is to encode information robustly: alice wishes to encode a fingerprint in an <e1>image</e1>; eve will then somehow distort the image (by cropping, blurring, etc), and bob should be able to detect the fingerprint in the distorted <e2>image</e2>."
sameAs(e1, e2)
Comment:

3311	"in digital watermarking, the goal is to encode information robustly: alice wishes to encode a fingerprint in an image; eve will then somehow distort the <e1>image</e1> (by cropping, blurring, etc), and bob should be able to detect the fingerprint in the distorted <e2>image</e2>."
sameAs(e1, e2)
Comment:

3312	"digital watermarking can be used to identify image ownership: if alice is a photographer, then by embedding digital watermarks in her <e1>images</e1> she can prove ownership of those <e2>images</e2> even if versions posted online are modified."
sameAs(e1, e2)
Comment:

3313	"given a cover <e1>image</e1> and a binary message, the hidden encoder produces a visually indistinguishable encoded <e2>image</e2> that contains the message, which can be recovered with high accuracy by the decoder."
sameAs(e1, e2)
Comment:

3314	"recent research have showed <e1>that</e1> neural networks are susceptible to adversarial examples: given an image and a target class, the pixels of the image can be imperceptibly modified such <e2>that</e2> it is confidently classified as the target class [1, 2] ."
sameAs(e1, e2)
Comment:

3315	"recent research have showed that neural networks are susceptible to adversarial examples: given an <e1>image</e1> and a target class, the pixels of the <e2>image</e2> can be imperceptibly modified such that it is confidently classified as the target class [1, 2] ."
sameAs(e1, e2)
Comment:

3316	"recent research have showed that neural networks are susceptible to adversarial examples: given an image and a <e1>target</e1> class, the pixels of the image can be imperceptibly modified such that it is confidently classified as the <e2>target</e2> class [1, 2] ."
sameAs(e1, e2)
Comment:

3317	"while the existence of adversarial examples is usually seen as a disadvantage of neural networks, it can be desirable for <e1>information</e1> hiding: if a network can be fooled with small perturbations into making incorrect class predictions, it should be possible to extract meaningful <e2>information</e2> from similar perturbations."
sameAs(e1, e2)
Comment:

3318	"an encoder <e1>network</e1> receives a cover image and a message (encoded as a bit string) and outputs an encoded image; a decoder <e2>network</e2> receives the encoded image and attempts to reconstruct the message."
sameAs(e1, e2)
Comment:

3319	"an encoder network receives a cover <e1>image</e1> and a message (encoded as a bit string) and outputs an encoded <e2>image</e2>; a decoder network receives the encoded image and attempts to reconstruct the message."
sameAs(e1, e2)
Comment:

3320	"an encoder network receives a cover <e1>image</e1> and a message (encoded as a bit string) and outputs an encoded image; a decoder network receives the encoded <e2>image</e2> and attempts to reconstruct the message."
sameAs(e1, e2)
Comment:

3321	"an encoder network receives a cover image and a message (encoded as a bit string) and outputs an encoded <e1>image</e1>; a decoder network receives the encoded <e2>image</e2> and attempts to reconstruct the message."
sameAs(e1, e2)
Comment:

3322	"we <e1>model</e1> this by inserting optional noise layers between the encoder and decoder, which apply different image transformations and force the <e2>model</e2> to learn encodings that can survive noisy transmission."
sameAs(e1, e2)
Comment:

3323	"we model the data hiding objective by minimizing (1) the difference between the cover and encoded <e1>images</e1>, (2) the difference between the input and decoded messages, and (3) the ability of an adversary to detect encoded <e2>images</e2>."
sameAs(e1, e2)
Comment:

3324	"we show <e1>that</e1> our methods outperform prior work in deep-learning-based steganography, and <e2>that</e2> our methods can also produce robust blind watermarks."
sameAs(e1, e2)
Comment:

3325	"we show that <e1>our methods</e1> outperform prior work in deep-learning-based steganography, and that <e2>our methods</e2> can also produce robust blind watermarks."
sameAs(e1, e2)
Comment:

3326	"these heuristics are effective in the domains for which <e1>they</e1> are designed, but <e2>they</e2> are fundamentally static."
sameAs(e1, e2)
Comment:

3327	"for watermarking, one can simply retrain the model to gain <e1>robustness</e1> against a new type of noise instead of inventing a new <e2>algorithm</e2>."
Evaluate-for(e1, e2)
Comment:

3328	"in fact, one can exploit this capability for the <e1>task</e1> of <e2>data</e2> hiding."
Conjunction(e1, e2)
Comment:

3329	"we jointly train <e1>encoder</e1> and decoder networks, where given an input message and cover image, the <e2>encoder</e2> produces a visually indistinguishable encoded image, from which the decoder can recover the original message."
sameAs(e1, e2)
Comment:

3330	"we jointly train encoder and <e1>decoder</e1> networks, where given an input message and cover image, the encoder produces a visually indistinguishable encoded image, from which the <e2>decoder</e2> can recover the original message."
sameAs(e1, e2)
Comment:

3331	"we jointly train encoder and decoder networks, where given an input message and cover <e1>image</e1>, the encoder produces a visually indistinguishable encoded <e2>image</e2>, from which the decoder can recover the original message."
sameAs(e1, e2)
Comment:

3332	"we show <e1>that</e1> these encodings are competitive with existing data hiding algorithms, and further <e2>that</e2> they can be made robust to noise: our models learn to reconstruct hidden information in an encoded image despite the presence of gaussian blurring, pixelwise dropout, cropping, and jpeg compression."
sameAs(e1, e2)
Comment:

3333	"we show that these encodings are competitive with existing <e1>data</e1> hiding algorithms, and further that they can be made robust to noise: our <e2>models</e2> learn to reconstruct hidden information in an encoded image despite the presence of gaussian blurring, pixelwise dropout, cropping, and jpeg compression."
Used-for(e1, e2)
Comment:

3334	"we are interested in learning a random variable transformation g(z) so <e1>that</e1> the generated data g(z) has a probability density function <e2>that</e2> is close to the real distribution p * ."
sameAs(e1, e2)
Comment:

3335	"while <e1>gan</e1> has been widely used, it is also known that <e2>gan</e2> is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3336	"while <e1>gan</e1> has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein <e2>gan</e2> (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3337	"while <e1>gan</e1> has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized <e2>gan</e2> to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3338	"while <e1>gan</e1> has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled <e2>gan</e2> , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3339	"while <e1>gan</e1> has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd <e2>gan</e2> , and references therein."
sameAs(e1, e2)
Comment:

3340	"while gan has been widely used, it is also known that <e1>gan</e1> is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein <e2>gan</e2> (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3341	"while gan has been widely used, it is also known that <e1>gan</e1> is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized <e2>gan</e2> to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3342	"while gan has been widely used, it is also known that <e1>gan</e1> is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled <e2>gan</e2> , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3343	"while gan has been widely used, it is also known that <e1>gan</e1> is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd <e2>gan</e2> , and references therein."
sameAs(e1, e2)
Comment:

3344	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein <e1>gan</e1> (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized <e2>gan</e2> to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3345	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein <e1>gan</e1> (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled <e2>gan</e2> , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3346	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein <e1>gan</e1> (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd <e2>gan</e2> , and references therein."
sameAs(e1, e2)
Comment:

3347	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized <e1>gan</e1> to tackle the issue of mode collapse (che et al, 2017) , unrolled <e2>gan</e2> , adagan (tolstikhin et al, 2017) , mmd gan , and references therein."
sameAs(e1, e2)
Comment:

3348	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized <e1>gan</e1> to tackle the issue of mode collapse (che et al, 2017) , unrolled gan , adagan (tolstikhin et al, 2017) , mmd <e2>gan</e2> , and references therein."
sameAs(e1, e2)
Comment:

3349	"while gan has been widely used, it is also known that gan is difficult to train due to its instability, which has led to numerous studies, e.g., wasserstein gan (wgan) and its extensions to pursue a different minimax objective gulrajani et al, 2017; mroueh & sercu, 2017) , mode-regularized gan to tackle the issue of mode collapse (che et al, 2017) , unrolled <e1>gan</e1> , adagan (tolstikhin et al, 2017) , mmd <e2>gan</e2> , and references therein."
sameAs(e1, e2)
Comment:

3350	"(1) parameterizing d and g, (1) can be viewed as a saddle point problem in optimization, which can be solved using a stochastic <e1>gradient</e1> method, where one takes a <e2>gradient</e2> step with respect to the parameters in d and g (see algorithm 4 below)."
sameAs(e1, e2)
Comment:

3351	"it shows <e1>that</e1> with a strong discriminator, a good generator can be learned so <e2>that</e2> the kl divergence between the distributions of real data and generated data improves after each functional gradient step until it converges to zero."
sameAs(e1, e2)
Comment:

3352	"notation throughout the paper, <e1>we</e1> use x to denote data in r r , and in particular, <e2>we</e2> use x * to denote real data."
sameAs(e1, e2)
Comment:

3353	"object detection is hence recovered as a special case when the target labels consist of <e1>one</e1> word, and image captioning is recovered when all images consist of <e2>one</e2> region that spans the full image."
sameAs(e1, e2)
Comment:

3354	"object detection is hence recovered as a special case when the target labels consist of one word, and <e1>image</e1> captioning is recovered when all images consist of one region that spans the full <e2>image</e2>."
sameAs(e1, e2)
Comment:

3355	"internally, the localization layer predicts a set of <e1>regions</e1> of interest in the image and then uses bilinear interpolation [19, 16] to smoothly crop the activations in each <e2>region</e2>."
sameAs(e1, e2)
Comment:

3356	"in this work we take a step towards unifying <e1>these</e1> two inter-connected <e2>tasks</e2> into one joint framework."
Used-for(e1, e2)
Comment:

3357	"first, we introduce the dense captioning <e1>task</e1> (see figure 1 ), which requires a <e2>model</e2> to predict a set of descriptions across regions of an image."
Evaluate-for(e1, e2)
Comment:

3358	"we fit the "frank" <e1>model</e1> to a capture of 70 people, and learn a new deformation <e2>model</e2>, named "adam", capable of additionally capturing variations of hair and clothing with a simplified parameterization."
sameAs(e1, e2)
Comment:

3359	"we present a <e1>method</e1> to capture the total body motion of multiple people with the 3d deformable <e2>model</e2>."
Used-for(e1, e2)
Comment:

3360	"these rich signals layer upon goal-directed <e1>activity</e1> in constructing the behavior of humans, and are therefore crucial for the machine perception of human <e2>activity</e2>."
sameAs(e1, e2)
Comment:

3361	"each area has its own preferred capture configuration: (1) torso and limb <e1>motions</e1> are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial <e2>motion</e2> is captured at close range, mostly frontal, and assuming little global head motion [7, 26, 8, 11, 54] ; (3) finger motion is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3362	"each area has its own preferred capture configuration: (1) torso and limb <e1>motions</e1> are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial motion is captured at close range, mostly frontal, and assuming little global head <e2>motion</e2> [7, 26, 8, 11, 54] ; (3) finger motion is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3363	"each area has its own preferred capture configuration: (1) torso and limb <e1>motions</e1> are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial motion is captured at close range, mostly frontal, and assuming little global head motion [7, 26, 8, 11, 54] ; (3) finger <e2>motion</e2> is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3364	"each area has its own preferred capture configuration: (1) torso and limb motions are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial <e1>motion</e1> is captured at close range, mostly frontal, and assuming little global head <e2>motion</e2> [7, 26, 8, 11, 54] ; (3) finger motion is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3365	"each area has its own preferred capture configuration: (1) torso and limb motions are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial <e1>motion</e1> is captured at close range, mostly frontal, and assuming little global head motion [7, 26, 8, 11, 54] ; (3) finger <e2>motion</e2> is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3366	"each area has its own preferred capture configuration: (1) torso and limb motions are captured in a sufficiently large working volume where people can freely move [19, 23, 47, 21] ; (2) facial motion is captured at close range, mostly frontal, and assuming little global head <e1>motion</e1> [7, 26, 8, 11, 54] ; (3) finger <e2>motion</e2> is also captured at very close distances from hands, where the hand regions are dominant in the sensor measurements [37, 52, 45, 53] ."
sameAs(e1, e2)
Comment:

3367	"mobilenets [10] introduced efficient reparameterization of standard 3 × 3 convolutional weights, in terms of depth-wise <e1>convolutions</e1> and 1 × 1 <e2>convolutions</e2>."
sameAs(e1, e2)
Comment:

3368	"recent work has also demonstrated that sparse convolutional <e1>weights</e1> [20, 21, 18] perform comparably to dense convolutional <e2>weights</e2> while also being computationally efficient."
sameAs(e1, e2)
Comment:

3369	"the success of a wide range of approaches <e1>that</e1> utilize convolutional layers <e2>that</e2> have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional weights with binary weights, motivates our hypothesis that one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3370	"the success of a wide range of approaches <e1>that</e1> utilize convolutional layers that have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional weights with binary weights, motivates our hypothesis <e2>that</e2> one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3371	"the success of a wide range of approaches that utilize convolutional <e1>layers</e1> that have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional weights with binary weights, motivates our hypothesis that one can perhaps completely do away with convolutional <e2>layers</e2> for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3372	"the success of a wide range of approaches that utilize convolutional layers <e1>that</e1> have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional weights with binary weights, motivates our hypothesis <e2>that</e2> one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3373	"the success of a wide range of approaches that utilize convolutional layers that have, a) very small receptive fields (3 × 3), b) sparse convolutional <e1>weights</e1>, and c) convolutional <e2>weights</e2> with binary weights, motivates our hypothesis that one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3374	"the success of a wide range of approaches that utilize convolutional layers that have, a) very small receptive fields (3 × 3), b) sparse convolutional <e1>weights</e1>, and c) convolutional weights with binary <e2>weights</e2>, motivates our hypothesis that one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3375	"the success of a wide range of approaches that utilize convolutional layers that have, a) very small receptive fields (3 × 3), b) sparse convolutional weights, and c) convolutional <e1>weights</e1> with binary <e2>weights</e2>, motivates our hypothesis that one can perhaps completely do away with convolutional layers for learning high performance image classification models."
sameAs(e1, e2)
Comment:

3376	"given an <e1>input</e1>, the perturbation layer first perturbs the <e2>input</e2> additively through random, but fixed, noise followed by a weighted combination of non-linear activations of the input perturbations."
sameAs(e1, e2)
Comment:

3377	"given an <e1>input</e1>, the perturbation layer first perturbs the input additively through random, but fixed, noise followed by a weighted combination of non-linear activations of the <e2>input</e2> perturbations."
sameAs(e1, e2)
Comment:

3378	"given an input, the perturbation layer first perturbs the <e1>input</e1> additively through random, but fixed, noise followed by a weighted combination of non-linear activations of the <e2>input</e2> perturbations."
sameAs(e1, e2)
Comment:

3379	"the weighted linear combinations of activated perturbations are conceptually similar to 1×1 convolutions, but are not strictly convolutional since their receptive <e1>field</e1> is just one pixel, as opposed to the receptive <e2>fields</e2> of standard convolutional weights."
sameAs(e1, e2)
Comment:

3380	"avoiding <e1>convolutions</e1> with receptive fields larger than one offers immediate statistical savings in the form of fewer learnable network parameters, computational savings from more efficient operations (weighted sum vs. <e2>convolution</e2>) and more importantly allows us to rethink the premise and utility of convolutional layers in the context of image classification models."
sameAs(e1, e2)
Comment:

3381	"in addition, we empirically demonstrate that deep neural networks with the perturbation <e1>layers</e1> as replacements for standard convolutional <e2>layers</e2> perform as well as an equivalent network with convolutional layers across a variety of datasets of varying difficulty and scale, mnist, cifar-10, pascal voc, and imagenet."
sameAs(e1, e2)
Comment:

3382	"in addition, we empirically demonstrate that deep neural networks with the perturbation <e1>layers</e1> as replacements for standard convolutional layers perform as well as an equivalent network with convolutional <e2>layers</e2> across a variety of datasets of varying difficulty and scale, mnist, cifar-10, pascal voc, and imagenet."
sameAs(e1, e2)
Comment:

3383	"in addition, we empirically demonstrate that deep neural networks with the perturbation layers as replacements for standard convolutional <e1>layers</e1> perform as well as an equivalent network with convolutional <e2>layers</e2> across a variety of datasets of varying difficulty and scale, mnist, cifar-10, pascal voc, and imagenet."
sameAs(e1, e2)
Comment:

3384	"convolutional <e1>layers</e1> are characterized by two main properties [17, 5] , namely, local connectivity and weight sharing, both of which afford these <e2>layers</e2> with significant computational and statistical efficiency over densely connected layers."
sameAs(e1, e2)
Comment:

3385	"convolutional <e1>layers</e1> are characterized by two main properties [17, 5] , namely, local connectivity and weight sharing, both of which afford these layers with significant computational and statistical efficiency over densely connected <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3386	"convolutional layers are characterized by two main properties [17, 5] , namely, local connectivity and weight sharing, both of which afford these <e1>layers</e1> with significant computational and statistical efficiency over densely connected <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3387	"as convolutional <e1>layers</e1> are often the main computational bottleneck of cnns, there has been steady developments in improving the computational efficiency of convolutional <e2>layers</e2>."
sameAs(e1, e2)
Comment:

3388	"introduction <e1>image super-resolution</e1> (sr), particularly single-<e2>image super-resolution</e2> (sisr), has attracted more and more attention in academia and industry."
sameAs(e1, e2)
Comment:

3389	"sisr aims to reconstruct a high-<e1>resolution</e1> (hr) image from a low-<e2>resolution</e2> (lr) image which is an ill-posed problem since the mapping between lr and hr has multiple solutions."
sameAs(e1, e2)
Comment:

3390	"sisr aims to reconstruct a high-resolution (hr) <e1>image</e1> from a low-resolution (lr) <e2>image</e2> which is an ill-posed problem since the mapping between lr and hr has multiple solutions."
sameAs(e1, e2)
Comment:

3391	"in 2014, dong et al proposed a <e1>model</e1> for sisr problem termed srcnn [1] , which was the first successful <e2>model</e2> adopting cnns to sr problem."
sameAs(e1, e2)
Comment:

3392	"in 2014, dong et al proposed a model for sisr <e1>problem</e1> termed srcnn [1] , which was the first successful model adopting cnns to sr <e2>problem</e2>."
sameAs(e1, e2)
Comment:

3393	"srcnn was an efficient network <e1>that</e1> could learn a kind of end-to-end mapping between the lr and hr images without requiring any engineered features and reached the most satisfactory performance at <e2>that</e2> time."
sameAs(e1, e2)
Comment:

3394	"it based on srresnet [8] while enhanced the <e1>network</e1> by removing the normalization layers as well as using deeper and wider <e2>network</e2> structures."
sameAs(e1, e2)
Comment:

3395	"nevertheless, all of <e1>these</e1> models tend to construct deeper and more complex network structures, which means training <e2>these</e2> models consumes more resources, time, and tricks."
sameAs(e1, e2)
Comment:

3396	"nevertheless, all of these <e1>models</e1> tend to construct deeper and more complex network structures, which means training these <e2>models</e2> consumes more resources, time, and tricks."
sameAs(e1, e2)
Comment:

3397	"during the reconstruction experiments, we find most existing sr <e1>models</e1> have the following problems: (a) hard to reproduce: the experimental results manifest that most sr <e2>models</e2> are sensitive to the subtle network architectural changes and some of them are difficult to reach the level of the original paper due to the lack of the network configuration."
sameAs(e1, e2)
Comment:

3398	"during the reconstruction experiments, we find most existing sr models have the following problems: (a) hard to reproduce: the experimental results manifest that most sr models are sensitive to the subtle <e1>network</e1> architectural changes and some of them are difficult to reach the level of the original paper due to the lack of the <e2>network</e2> configuration."
sameAs(e1, e2)
Comment:

3399	"(b) inadequate of <e1>features</e1> utilization: most methods blindly increase the depth of the network in order to enhance the the performance of the network but ignore taking full use of the lr image <e2>features</e2>."
sameAs(e1, e2)
Comment:

3400	"(b) inadequate of features utilization: most methods blindly increase the depth of the <e1>network</e1> in order to enhance the the performance of the <e2>network</e2> but ignore taking full use of the lr image features."
sameAs(e1, e2)
Comment:

3401	"as a result, it is difficult to find a simple sr model that can accommodate to any upscaling <e1>factors</e1>, or can migrate to any upscaling <e2>factors</e2> with only minor adjustments to the network architecture."
sameAs(e1, e2)
Comment:

3402	"firstly, we use the msrb to acquire the image <e1>features</e1> on different scales, which is considered as local multi-scale <e2>features</e2>."
sameAs(e1, e2)
Comment:

3403	"firstly, we use the msrb to acquire the image features on different <e1>scales</e1>, which is considered as local multi-<e2>scale</e2> features."
sameAs(e1, e2)
Comment:

3404	"finally, the combination of local multi-scale <e1>features</e1> and global <e2>features</e2> can maximize the use of the lr image features and completely solve the problem that features disappear in the transmission process."
sameAs(e1, e2)
Comment:

3405	"finally, the combination of local multi-scale <e1>features</e1> and global features can maximize the use of the lr image <e2>features</e2> and completely solve the problem that features disappear in the transmission process."
sameAs(e1, e2)
Comment:

3406	"finally, the combination of local multi-scale <e1>features</e1> and global features can maximize the use of the lr image features and completely solve the problem that <e2>features</e2> disappear in the transmission process."
sameAs(e1, e2)
Comment:

3407	"finally, the combination of local multi-scale features and global <e1>features</e1> can maximize the use of the lr image <e2>features</e2> and completely solve the problem that features disappear in the transmission process."
sameAs(e1, e2)
Comment:

3408	"finally, the combination of local multi-scale features and global <e1>features</e1> can maximize the use of the lr image features and completely solve the problem that <e2>features</e2> disappear in the transmission process."
sameAs(e1, e2)
Comment:

3409	"finally, the combination of local multi-scale features and global features can maximize the use of the lr image <e1>features</e1> and completely solve the problem that <e2>features</e2> disappear in the transmission process."
sameAs(e1, e2)
Comment:

3410	"however, blindly increasing the depth of the <e1>network</e1> cannot ameliorate the <e2>network</e2> effectively."
sameAs(e1, e2)
Comment:

3411	"our base-<e1>model</e1> shows superior performance over most state-of-the-art <e2>methods</e2> on benchmark test-datasets."
Compare(e1, e2)
Comment:

3412	"contributions of this paper are as follows: -different from previous works, we propose a novel multi-<e1>scale</e1> residual block (msrb), which can not only adaptively detect the image features, but also achieve feature fusion at different <e2>scales</e2>."
sameAs(e1, e2)
Comment:

3413	"-we extend our work to computer vision tasks and the <e1>results</e1> exceed those of the state-of-the-art <e2>methods</e2> in sisr without deep network structure."
Compare(e1, e2)
Comment:

3414	"meanwhile, <e1>we</e1> let these features interact with each other to get the most efficacious image information, <e2>we</e2> call this structure multi-scale residual block (msrb)."
sameAs(e1, e2)
Comment:

3415	"there have been two main lines of efforts at incorporating structural reasoning into semantic segmentation: conditional random field (crf) <e1>methods</e1> [15, 37] and generative adversarial network (gan) <e2>methods</e2> [12, 22] ."
sameAs(e1, e2)
Comment:

3416	"specifically, the predicted label <e1>map</e1> is tested by a discriminator network on whether it resembles ground truth label <e2>maps</e2> in the training set."
sameAs(e1, e2)
Comment:

3417	"how the semantic label of each pixel is related to those of neighboring pixels, e.g., whether <e1>they</e1> are same or different, provides a distributed and pixel-centric description of semantic relations in the space and collectively <e2>they</e2> describe motorcycle wheels are round with thin radial spokes."
sameAs(e1, e2)
Comment:

3418	"matching the affinity fields at a fixed size would not work well for all semantic categories, e.g., thin <e1>structures</e1> are needed for persons seen at a distance whereas large <e2>structures</e2> are for cows seen close-up."
sameAs(e1, e2)
Comment:

3419	"specifically, <e1>we</e1> formulate our aaf as a minimax problem where <e2>we</e2> simultaneously maximize the affinity errors over multiple kernel sizes and minimize the overall matching loss."
sameAs(e1, e2)
Comment:

3420	"one drawback of <e1>these</e1> second-order cnns is that vectorizing the covariance descriptor to pass it to the classiication layer, as done in [36, 24, 32, 34] , yields a vector representation that is orders of magnitude larger than that of irst-order cnns, thus making <e2>these</e2> networks memory-intensive and subject to overitting."
sameAs(e1, e2)
Comment:

3421	"one drawback of these second-order <e1>cnns</e1> is that vectorizing the covariance descriptor to pass it to the classiication layer, as done in [36, 24, 32, 34] , yields a vector representation that is orders of magnitude larger than that of irst-order <e2>cnns</e2>, thus making these networks memory-intensive and subject to overitting."
sameAs(e1, e2)
Comment:

3422	"one drawback of these second-order cnns is <e1>that</e1> vectorizing the covariance descriptor to pass it to the classiication layer, as done in [36, 24, 32, 34] , yields a vector representation <e2>that</e2> is orders of magnitude larger than that of irst-order cnns, thus making these networks memory-intensive and subject to overitting."
sameAs(e1, e2)
Comment:

3423	"one drawback of these second-order cnns is <e1>that</e1> vectorizing the covariance descriptor to pass it to the classiication layer, as done in [36, 24, 32, 34] , yields a vector representation that is orders of magnitude larger than <e2>that</e2> of irst-order cnns, thus making these networks memory-intensive and subject to overitting."
sameAs(e1, e2)
Comment:

3424	"one drawback of these second-order cnns is that vectorizing the covariance descriptor to pass it to the classiication layer, as done in [36, 24, 32, 34] , yields a vector representation <e1>that</e1> is orders of magnitude larger than <e2>that</e2> of irst-order cnns, thus making these networks memory-intensive and subject to overitting."
sameAs(e1, e2)
Comment:

3425	"however, the resulting second-order networks yield a inal representation <e1>that</e1> is orders of magnitude larger than <e2>that</e2> of standard, irst-order ones, making them memory-intensive and cumbersome to deploy."
sameAs(e1, e2)
Comment:

3426	"here, we introduce a general, parametric <e1>compression</e1> strategy that can produce more compact representations than existing <e2>compression</e2> techniques, yet outperform both compressed and uncompressed second-order models."
sameAs(e1, e2)
Comment:

3427	"abstract we propose a probabilistic <e1>video</e1> model, the <e2>video</e2> pixel network (vpn), that estimates the discrete joint distribution of the raw pixel values in a video."
sameAs(e1, e2)
Comment:

3428	"abstract we propose a probabilistic <e1>video</e1> model, the video pixel network (vpn), that estimates the discrete joint distribution of the raw pixel values in a <e2>video</e2>."
sameAs(e1, e2)
Comment:

3429	"abstract we propose a probabilistic video model, the <e1>video</e1> pixel network (vpn), that estimates the discrete joint distribution of the raw pixel values in a <e2>video</e2>."
sameAs(e1, e2)
Comment:

3430	"this makes it possible to model the stochastic transitions locally from <e1>one</e1> pixel to the next and more globally from <e2>one</e2> frame to the next without introducing independence assumptions in the conditional factors."
sameAs(e1, e2)
Comment:

3431	"the factorization further ensures <e1>that</e1> the model stays fully tractable; the likelihood <e2>that</e2> the model assigns to a video can be computed exactly."
sameAs(e1, e2)
Comment:

3432	"the factorization further ensures that the <e1>model</e1> stays fully tractable; the likelihood that the <e2>model</e2> assigns to a video can be computed exactly."
sameAs(e1, e2)
Comment:

3433	"the <e1>model</e1> operates on pixels without preprocessing and predicts discrete multinomial distributions over raw pixel intensities, allowing the <e2>model</e2> to estimate distributions of any shape."
sameAs(e1, e2)
Comment:

3434	"the architecture of the vpn consists of two parts: resolution preserving <e1>cnn</e1> encoders and pixelcnn <e2>decoders</e2> (van den oord et al, 2016b) ."
Used-for(e1, e2)
Comment:

3435	"5 we show <e1>that</e1> the vpn achieves 87.6 nats/frame, a score <e2>that</e2> is near the lower bound on the loss (calculated to be 86.3 nats/frame); this constitutes a significant improvement over the previous best result of 179.8 nats/frame (patraucean et al, 2015) ."
sameAs(e1, e2)
Comment:

3436	"we show that the vpn not only generalizes to new <e1>action</e1> sequences with objects seen during training, but also to new <e2>action</e2> sequences involving novel objects not seen during training."
sameAs(e1, e2)
Comment:

3437	"we show that the vpn not only generalizes to new action sequences with <e1>objects</e1> seen during training, but also to new action sequences involving novel <e2>objects</e2> not seen during training."
sameAs(e1, e2)
Comment:

3438	"we show that the vpn not only generalizes to new action sequences with objects seen during <e1>training</e1>, but also to new action sequences involving novel objects not seen during <e2>training</e2>."
sameAs(e1, e2)
Comment:

3439	"r g b f t f <t x <e1>f1</e1> f0 f2 f0f3 <e2>f1</e2> f1 f0 f2 f3 r g b f t f <t x f1 f0 f2"
sameAs(e1, e2)
Comment:

3440	"r g b f t f <t x <e1>f1</e1> f0 f2 f0f3 f1 f1 f0 f2 f3 r g b f t f <t x <e2>f1</e2> f0 f2"
sameAs(e1, e2)
Comment:

3441	"r g b f t f <t x f1 f0 f2 f0f3 <e1>f1</e1> f1 f0 f2 f3 r g b f t f <t x <e2>f1</e2> f0 f2"
sameAs(e1, e2)
Comment:

3442	"introduction <e1>video</e1> modelling has remained a challenging problem due to the complexity and ambiguity inherent in <e2>video</e2> data."
sameAs(e1, e2)
Comment:

3443	"current approaches range from mean squared error <e1>models</e1> based on deep neural networks (srivastava et al, 2015a; oh et al, 2015) , to <e2>models</e2> that predict quantized image patches (ranzato et al, 2014) , incorporate motion priors (patraucean et al, 2015; finn et al, 2016) or use adversarial losses (mathieu et al, 2015; vondrick et al, 2016) ."
sameAs(e1, e2)
Comment:

3444	"we propose the <e1>video</e1> pixel network (vpn), a generative <e2>video</e2> model based on deep neural networks, that reflects the factorization of the joint distribution of the pixel values in a video."
sameAs(e1, e2)
Comment:

3445	"we propose the <e1>video</e1> pixel network (vpn), a generative video model based on deep neural networks, that reflects the factorization of the joint distribution of the pixel values in a <e2>video</e2>."
sameAs(e1, e2)
Comment:

3446	"we propose the video pixel network (vpn), a generative <e1>video</e1> model based on deep neural networks, that reflects the factorization of the joint distribution of the pixel values in a <e2>video</e2>."
sameAs(e1, e2)
Comment:

3447	"we find that, for a constant number of parameters, large sparse <e1>networks</e1> perform better than small dense <e2>networks</e2> and this relationship holds for sparsity levels beyond 96%."
sameAs(e1, e2)
Comment:

3448	"end-to-end recovery of human shape and pose <e1>input</e1> reconstruction side and top down view part segmentation <e2>input</e2> reconstruction side and top down view part segmentation"
sameAs(e1, e2)
Comment:

3449	"the first two rows show <e1>results</e1> from our model trained with some 2d-to-3d supervision, the bottom row shows <e2>results</e2> from a model that is trained in a fully weakly-supervised manner without using any paired 2d-to-3d supervision."
sameAs(e1, e2)
Comment:

3450	"the first two rows show results from our <e1>model</e1> trained with some 2d-to-3d supervision, the bottom row shows results from a <e2>model</e2> that is trained in a fully weakly-supervised manner without using any paired 2d-to-3d supervision."
sameAs(e1, e2)
Comment:

3451	"the first two rows show results from our model trained with some 2d-to-3d <e1>supervision</e1>, the bottom row shows results from a model that is trained in a fully weakly-supervised manner without using any paired 2d-to-3d <e2>supervision</e2>."
sameAs(e1, e2)
Comment:

3452	"abstract in this work we describe a convolutional neural network (cnn) to accurately predict <e1>image</e1> quality without a reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

3453	"nr-iqa measures can directly quantify <e1>image</e1> degradations by exploiting features that are discriminant for <e2>image</e2> degradations."
sameAs(e1, e2)
Comment:

3454	"cornia demonstrates that it is possible to learn discriminant <e1>image</e1> features directly from the raw <e2>image</e2> pixels, instead of using handcrafted features."
sameAs(e1, e2)
Comment:

3455	"in the object recognition domain good <e1>features</e1> generally encode local invariant parts, however, for the nr-iqa task, good <e2>features</e2> should be able to capture nss properties."
sameAs(e1, e2)
Comment:

3456	"one of our contributions is <e1>that</e1> we modified the network structure, such <e2>that</e2> it can learn image quality features more effectively and estimate the image quality more accurately."
sameAs(e1, e2)
Comment:

3457	"one of our contributions is that we modified the network structure, such that it can learn <e1>image</e1> quality features more effectively and estimate the <e2>image</e2> quality more accurately."
sameAs(e1, e2)
Comment:

3458	"one of our contributions is that we modified the network structure, such that it can learn image <e1>quality</e1> features more effectively and estimate the image <e2>quality</e2> more accurately."
sameAs(e1, e2)
Comment:

3459	"another contribution of our paper is <e1>that</e1> we propose a novel framework <e2>that</e2> allows learning and prediction of image quality on local regions."
sameAs(e1, e2)
Comment:

3460	"previous approaches typically accumulate features over the entire image to obtain statistics for estimating overall <e1>quality</e1>, and have rarely shown the ability to estimate local <e2>quality</e2>, except for a simple example in [18] ."
sameAs(e1, e2)
Comment:

3461	"the work focuses on the most challenging category of objective image quality assessment (iqa) tasks: general-purpose no-reference iqa (nr-iqa), which evaluates the visual quality of digital <e1>images</e1> without access to reference <e2>images</e2> and without prior knowledge of the types of distortions present."
sameAs(e1, e2)
Comment:

3462	"this reduces the human labor in creating labeled <e1>video</e1> datasets to just recording the <e2>video</e2>."
sameAs(e1, e2)
Comment:

3463	"relocalization results for an input <e1>image</e1> (top), the predicted camera pose of a visual reconstruction (middle), shown again overlaid in red on the original <e2>image</e2> (bottom)."
sameAs(e1, e2)
Comment:

3464	"simultaneous localization and mapping (slam) is a traditional <e1>solution</e1> to this <e2>problem</e2>."
Used-for(e1, e2)
Comment:

3465	"we empirically show that this representation is a smoothly varying injective (<e1>one</e1>-to-<e2>one</e2>) function of pose, allowing us to regress pose directly from the image without need of tracking."
sameAs(e1, e2)
Comment:

3466	"we empirically show that this representation is a smoothly varying injective (one-to-one) function of <e1>pose</e1>, allowing us to regress <e2>pose</e2> directly from the image without need of tracking."
sameAs(e1, e2)
Comment:

3467	"we employ two techniques to overcome this limitation: • an automated method of labeling data using structure from motion to generate large regression <e1>datasets</e1> of camera pose • transfer learning which trains a pose regressor, pretrained as a classifier, on immense image recognition <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

3468	"we employ two techniques to overcome this limitation: • an automated method of labeling data using structure from motion to generate large regression datasets of camera <e1>pose</e1> • transfer learning which trains a <e2>pose</e2> regressor, pretrained as a classifier, on immense image recognition datasets."
sameAs(e1, e2)
Comment:

3469	"this converges to a lower error in less time, even with a very sparse <e1>training</e1> set, as compared to <e2>training</e2> from scratch."
sameAs(e1, e2)
Comment:

3470	"we show top <e1>results</e1> for (bounding box) weakly supervised semantic labelling and, to the best of our knowledge, for the first time report <e2>results</e2> for weakly supervised instance segmentation."
sameAs(e1, e2)
Comment:

3471	"we explore recursive <e1>training</e1> as a de-noising strategy, where convnet predictions of the previous <e2>training</e2> round are used as supervision for the next round."
sameAs(e1, e2)
Comment:

3472	"− we show that state of the art quality can be reached in a single <e1>training</e1> round when properly employing grabcut-like algorithms to generate <e2>training</e2> labels from given bounding boxes, instead of modifying the segmentation convnet training procedure or using recursive training (section 3.2)."
sameAs(e1, e2)
Comment:

3473	"− we show that state of the art quality can be reached in a single <e1>training</e1> round when properly employing grabcut-like algorithms to generate training labels from given bounding boxes, instead of modifying the segmentation convnet <e2>training</e2> procedure or using recursive training (section 3.2)."
sameAs(e1, e2)
Comment:

3474	"− we show that state of the art quality can be reached in a single <e1>training</e1> round when properly employing grabcut-like algorithms to generate training labels from given bounding boxes, instead of modifying the segmentation convnet training procedure or using recursive <e2>training</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

3475	"− we show that state of the art quality can be reached in a single training round when properly employing grabcut-like algorithms to generate <e1>training</e1> labels from given bounding boxes, instead of modifying the segmentation convnet <e2>training</e2> procedure or using recursive training (section 3.2)."
sameAs(e1, e2)
Comment:

3476	"− we show that state of the art quality can be reached in a single training round when properly employing grabcut-like algorithms to generate <e1>training</e1> labels from given bounding boxes, instead of modifying the segmentation convnet training procedure or using recursive <e2>training</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

3477	"− we show that state of the art quality can be reached in a single training round when properly employing grabcut-like algorithms to generate training labels from given bounding boxes, instead of modifying the segmentation convnet <e1>training</e1> procedure or using recursive <e2>training</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

3478	"− we report the best known results when <e1>training</e1> using bounding boxes only, both using pascal voc12 and voc12+coco <e2>training</e2> data, reaching comparable quality with the fully supervised regime (section 4.2)."
sameAs(e1, e2)
Comment:

3479	"with ∼ 10 6 training samples for imagenet classification [37] ), but still thousands of samples are needed to shift from the pre-training <e1>domain</e1> to the application <e2>domain</e2>."
sameAs(e1, e2)
Comment:

3480	"compared to object bounding box <e1>annotations</e1>, pixelwise mask <e2>annotations</e2> are far more expensive, requiring ∼ 15× more time [25] ."
sameAs(e1, e2)
Comment:

3481	"cheaper and easier to define, box <e1>annotations</e1> are more pervasive than pixel-wise <e2>annotations</e2>."
sameAs(e1, e2)
Comment:

3482	"abstract while humans easily recognize <e1>relations</e1> between data from different domains without any supervision, learning to automatically discover them is in general very challenging and needs many ground-truth pairs that illustrate the <e2>relations</e2>."
sameAs(e1, e2)
Comment:

3483	"in <e1>other</e1> words, finding a mapping function from one domain to the <e2>other</e2> can be thought as generating an image in one domain given another image in the other domain."
sameAs(e1, e2)
Comment:

3484	"in <e1>other</e1> words, finding a mapping function from one domain to the other can be thought as generating an image in one domain given another image in the <e2>other</e2> domain."
sameAs(e1, e2)
Comment:

3485	"in other words, finding a mapping function from <e1>one</e1> domain to the <e2>other</e2> can be thought as generating an image in one domain given another image in the other domain."
Conjunction(e1, e2)
Comment:

3486	"in other words, finding a mapping function from <e1>one</e1> domain to the other can be thought as generating an image in <e2>one</e2> domain given another image in the other domain."
sameAs(e1, e2)
Comment:

3487	"in other words, finding a mapping function from <e1>one</e1> domain to the other can be thought as generating an image in one domain given another image in the <e2>other</e2> domain."
Conjunction(e1, e2)
Comment:

3488	"in other words, finding a mapping function from one <e1>domain</e1> to the other can be thought as generating an image in one <e2>domain</e2> given another image in the other domain."
sameAs(e1, e2)
Comment:

3489	"in other words, finding a mapping function from one <e1>domain</e1> to the other can be thought as generating an image in one domain given another image in the other <e2>domain</e2>."
sameAs(e1, e2)
Comment:

3490	"in other words, finding a mapping function from one domain to the <e1>other</e1> can be thought as generating an image in one domain given another image in the <e2>other</e2> domain."
sameAs(e1, e2)
Comment:

3491	"in other words, finding a mapping function from one domain to the other can be thought as generating an <e1>image</e1> in one domain given another <e2>image</e2> in the other domain."
sameAs(e1, e2)
Comment:

3492	"in other words, finding a mapping function from one domain to the other can be thought as generating an image in <e1>one</e1> domain given another image in the <e2>other</e2> domain."
Conjunction(e1, e2)
Comment:

3493	"in other words, finding a mapping function from one domain to the other can be thought as generating an image in one <e1>domain</e1> given another image in the other <e2>domain</e2>."
sameAs(e1, e2)
Comment:

3494	"to avoid costly pairing, we address the <e1>task</e1> of discovering cross-domain relations when given unpaired <e2>data</e2>."
Conjunction(e1, e2)
Comment:

3495	"splitnet: learning to semantically split deep networks for <e1>parameter reduction</e1> and <e2>model</e2> parallelization"
Feature-of(e1, e2)
Comment:

3496	"we focus on the observation that as the number of <e1>classes</e1> increases, semantically disparate <e2>classes</e2> (or tasks) can be represented by largely disjoint sets of features."
sameAs(e1, e2)
Comment:

3497	"such grouping of <e1>concepts</e1> based on semantic proximity also agrees with the way that our brain stores semantic <e2>concepts</e2>, where semantically related concepts activate similar part of the brain (huth et al, 2012) , in a highly localized manner."
sameAs(e1, e2)
Comment:

3498	"such grouping of <e1>concepts</e1> based on semantic proximity also agrees with the way that our brain stores semantic concepts, where semantically related <e2>concepts</e2> activate similar part of the brain (huth et al, 2012) , in a highly localized manner."
sameAs(e1, e2)
Comment:

3499	"such grouping of concepts based on <e1>semantic</e1> proximity also agrees with the way that our brain stores <e2>semantic</e2> concepts, where semantically related concepts activate similar part of the brain (huth et al, 2012) , in a highly localized manner."
sameAs(e1, e2)
Comment:

3500	"such grouping of concepts based on semantic proximity also agrees with the way that our brain stores semantic <e1>concepts</e1>, where semantically related <e2>concepts</e2> activate similar part of the brain (huth et al, 2012) , in a highly localized manner."
sameAs(e1, e2)
Comment:

3501	"our <e1>network</e1>, which we name as splitnet, automatically learns to split the <e2>network</e2> weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the network weights."
sameAs(e1, e2)
Comment:

3502	"our <e1>network</e1>, which we name as splitnet, automatically learns to split the network weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the <e2>network</e2> weights."
sameAs(e1, e2)
Comment:

3503	"our network, which we name as splitnet, automatically learns to split the <e1>network</e1> weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the <e2>network</e2> weights."
sameAs(e1, e2)
Comment:

3504	"our network, which we name as splitnet, automatically learns to split the network <e1>weights</e1> into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the network <e2>weights</e2>."
sameAs(e1, e2)
Comment:

3505	"our <e1>network</e1> automatically learns to split the classes and associated features into multiple groups at multiple <e2>network</e2> layers, obtaining a tree-structured network."
sameAs(e1, e2)
Comment:

3506	"our <e1>network</e1> automatically learns to split the classes and associated features into multiple groups at multiple network layers, obtaining a tree-structured <e2>network</e2>."
sameAs(e1, e2)
Comment:

3507	"our network automatically learns to split the classes and associated features into multiple groups at multiple <e1>network</e1> layers, obtaining a tree-structured <e2>network</e2>."
sameAs(e1, e2)
Comment:

3508	"given a base <e1>network</e1> in (a), (b) our algorithm optimizes <e2>network</e2> weights as well as class-to-group and feature-to-group assignments."
sameAs(e1, e2)
Comment:

3509	"deep splitting of the network into a set of subnetworks or a hierarchy of subnetworks sharing common lower layers, such <e1>that</e1> classes in each group share a common subset of features, <e2>that</e2> is completely disjoint from the features for other class groups."
sameAs(e1, e2)
Comment:

3510	"deep splitting of the network into a set of subnetworks or a hierarchy of subnetworks sharing common lower layers, such that classes in each group share a common subset of <e1>features</e1>, that is completely disjoint from the <e2>features</e2> for other class groups."
sameAs(e1, e2)
Comment:

3511	"we train such a <e1>network</e1> by additionally learning classes-to-group assignments and feature-togroup assignments along with the <e2>network</e2> weights, while regularizing them to be disjoint across groups."
sameAs(e1, e2)
Comment:

3512	"• we propose an efficient algorithm for automatically training such a tree-structured <e1>network</e1>, which learns class-to-group and feature-to-group assignments along with the <e2>network</e2> weights."
sameAs(e1, e2)
Comment:

3513	"• the <e1>networks</e1> trained by our approach are embarrassingly model-parallelizable; in distributed learning setting, we show that our <e2>networks</e2> scale well to an increased number of processors."
sameAs(e1, e2)
Comment:

3514	"we validate <e1>our method</e1> with two different deep network models (resnet and alexnet) on two datasets (cifar-100 and ilsvrc 2012) for image classification, on which <e2>our method</e2> obtains networks with significantly reduced number of parameters while achieving comparable or superior accuracies over original full deep networks, and accelerated test speed with multiple gpus."
sameAs(e1, e2)
Comment:

3515	"we validate our method with <e1>two</e1> different deep network models (resnet and alexnet) on <e2>two</e2> datasets (cifar-100 and ilsvrc 2012) for image classification, on which our method obtains networks with significantly reduced number of parameters while achieving comparable or superior accuracies over original full deep networks, and accelerated test speed with multiple gpus."
sameAs(e1, e2)
Comment:

3516	"introduction recently, deep neural networks have shown impressive performances on a multitude of tasks, including visual recognition (krizhevsky et al, 2012; szegedy et al, 2015; he et al, 2016) , <e1>speech recognition</e1> , and <e2>natural language processing</e2> (bengio et al, 2003; sutskever et al, 2014) ."
Conjunction(e1, e2)
Comment:

3517	"moreover, we highlight the problems of a commonly used disentanglement <e1>metric</e1> and introduce a new <e2>metric</e2> that does not suffer from them."
sameAs(e1, e2)
Comment:

3518	"such representations are useful not only for standard downstream <e1>tasks</e1> such as supervised learning and reinforcement learning, but also for <e2>tasks</e2> such as transfer learning and zero-shot learning where humans excel but machines struggle (lake et al, 2016) ."
sameAs(e1, e2)
Comment:

3519	"while there is no canonical definition for this term, we adopt the <e1>one</e1> due to bengio et al (2013) : a representation where a change in <e2>one</e2> dimension corresponds to a change in one factor of variation, while being relatively invariant to changes in other factors."
sameAs(e1, e2)
Comment:

3520	"while there is no canonical definition for this term, we adopt the <e1>one</e1> due to bengio et al (2013) : a representation where a change in one dimension corresponds to a change in <e2>one</e2> factor of variation, while being relatively invariant to changes in other factors."
sameAs(e1, e2)
Comment:

3521	"while there is no canonical definition for this term, we adopt the <e1>one</e1> due to bengio et al (2013) : a representation where a change in one dimension corresponds to a change in one factor of variation, while being relatively invariant to changes in <e2>other</e2> factors."
Conjunction(e1, e2)
Comment:

3522	"while there is no canonical definition for this term, we adopt the one due to bengio et al (2013) : a representation where a change in <e1>one</e1> dimension corresponds to a change in <e2>one</e2> factor of variation, while being relatively invariant to changes in other factors."
sameAs(e1, e2)
Comment:

3523	"while there is no canonical definition for this term, we adopt the one due to bengio et al (2013) : a representation where a change in <e1>one</e1> dimension corresponds to a change in one factor of variation, while being relatively invariant to changes in <e2>other</e2> factors."
Conjunction(e1, e2)
Comment:

3524	"while there is no canonical definition for this term, we adopt the one due to bengio et al (2013) : a representation where a change in one dimension corresponds to a change in <e1>one</e1> factor of variation, while being relatively invariant to changes in <e2>other</e2> factors."
Conjunction(e1, e2)
Comment:

3525	" introduction we address the problem of generating a high<e1>-resolutio</e1>n (hr) image given a low<e2>-resolutio</e2>n (lr) image, commonly referred as single image super-resolution (sisr) [12] , [8] , [9] ."
sameAs(e1, e2)
Comment:

3526	" introduction we address the problem of generating a high-resolution (hr)<e1> imag</e1>e given a low-resolution (lr)<e2> imag</e2>e, commonly referred as single image super-resolution (sisr) [12] , [8] , [9] ."
sameAs(e1, e2)
Comment:

3527	"their <e1>method</e1>, termed srcnn, does not require any engineered features that are typically necessary in other <e2>methods</e2> [25, 26, 21, 22] and shows the stateof-the-art performance."
Compare(e1, e2)
Comment:

3528	"as lr <e1>image</e1> and hr <e2>image</e2> share the same information to a large extent, explicitly modelling the residual image, which is the difference between hr and lr images, is advantageous."
sameAs(e1, e2)
Comment:

3529	"as lr <e1>image</e1> and hr image share the same information to a large extent, explicitly modelling the residual <e2>image</e2>, which is the difference between hr and lr images, is advantageous."
sameAs(e1, e2)
Comment:

3530	"as lr image and hr <e1>image</e1> share the same information to a large extent, explicitly modelling the residual <e2>image</e2>, which is the difference between hr and lr images, is advantageous."
sameAs(e1, e2)
Comment:

3531	"boosting convergence rate with high learning rates lead to exploding <e1>gradients</e1> and we resolve the issue with residual-learning and <e2>gradient</e2> clipping."
sameAs(e1, e2)
Comment:

3532	"our <e1>method</e1> is relatively accurate and fast in comparison to state-of-the-art <e2>methods</e2> as illustrated in figure 1 ."
Compare(e1, e2)
Comment:

3533	"early <e1>methods</e1> include interpolation such as bicubic interpolation and lanczos resampling [7] more powerful <e2>methods</e2> utilizing statistical image priors [20, 13] or internal patch recurrence [9] ."
sameAs(e1, e2)
Comment:

3534	"the workers' decision for which visual attribute they use to compare the <e1>images</e1> can be explained by two factors: 1) the workers have an innate preference towards certain attributes based on their past experiences and 2) the set of related <e2>images</e2> that a worker observes biases them towards certain attributes."
sameAs(e1, e2)
Comment:

3535	"we call this first <e1>bias</e1> the worker prior and the second <e2>bias</e2> the context."
sameAs(e1, e2)
Comment:

3536	"we introduce context embedding networks (cens), an efficient end-to-end model <e1>that</e1> learns interpretable, low dimensional, image embeddings <e2>that</e2> respect the varied similarity estimates provided by different crowd workers."
sameAs(e1, e2)
Comment:

3537	"this approach, however, presents its own challenges: 1) different workers may use different criteria when estimating the similarity between pairs of <e1>images</e1>, and 2) workers may be influenced by the set of <e2>images</e2> that they see when making their decisions i.e."
sameAs(e1, e2)
Comment:

3538	"abstract we propose an <e1>image super-resolution</e1> method (sr) introduction for <e2>image super-resolution</e2> (sr), receptive field of a convolutional network determines the amount of contextual information that can be exploited to infer missing highfrequency components."
sameAs(e1, e2)
Comment:

3539	"for example, if there exists a pattern with smoothed <e1>edges</e1> contained in a receptive field, it is plausible that the pattern is recognized and <e2>edges</e2> are appropriately sharpened."
sameAs(e1, e2)
Comment:

3540	"as each recursion leads to a different hr <e1>prediction</e1>, we combine all predictions resulting from different levels of recursions to deliver a more accurate final <e2>prediction</e2>."
sameAs(e1, e2)
Comment:

3541	"in sr, a low-<e1>resolution</e1> image (input) and a high-<e2>resolution</e2> image (output) share the same information to a large extent."
sameAs(e1, e2)
Comment:

3542	"in sr, a low-resolution <e1>image</e1> (input) and a high-resolution <e2>image</e2> (output) share the same information to a large extent."
sameAs(e1, e2)
Comment:

3543	"it consists of three parts: embedding <e1>network</e1>, inference <e2>network</e2> and reconstruction network."
sameAs(e1, e2)
Comment:

3544	"it consists of three parts: embedding <e1>network</e1>, inference network and reconstruction <e2>network</e2>."
sameAs(e1, e2)
Comment:

3545	"it consists of three parts: embedding network, inference <e1>network</e1> and reconstruction <e2>network</e2>."
sameAs(e1, e2)
Comment:

3546	"abstract interacting <e1>systems</e1> are prevalent in nature, from dynamical <e2>systems</e2> in physics to complex societal dynamics."
sameAs(e1, e2)
Comment:

3547	"it is clear that the <e1>dynamics</e1> of a single basketball player are influenced by the other players, and observing these <e2>dynamics</e2> as a human, we are"
sameAs(e1, e2)
Comment:

3548	"the interplay of <e1>components</e1> can give rise to complex behavior, which can often be explained using a simple model of the <e2>system</e2>'s constituent parts."
Part-of(e1, e2)
Comment:

3549	"in this work, we introduce the neural relational inference (nri) <e1>model</e1>: an unsupervised <e2>model</e2> that learns to infer interactions while simultaneously learning the dynamics purely from observational data."
sameAs(e1, e2)
Comment:

3550	"our model takes the form of a variational auto-encoder, in which the latent code represents the underlying interaction <e1>graph</e1> and the reconstruction is based on <e2>graph</e2> neural networks."
sameAs(e1, e2)
Comment:

3551	"introduction a wide range of dynamical systems in physics, biology, sports, and other areas can be seen as groups of interacting <e1>components</e1>, giving rise to complex dynamics at the level of individual constituents and in the <e2>system</e2> as a whole."
Part-of(e1, e2)
Comment:

3552	"at the same <e1>time</e1>, kdnetworks come with smaller memory footprints and more efficient computations at train and at test <e2>time</e2> thanks to the improved ability of kd-trees to index and structure 3d data as compared to uniform voxel grids."
sameAs(e1, e2)
Comment:

3553	"below, <e1>we</e1> first review the related work on convolutional networks for 3d models in section 2. <e2>we</e2> then discuss the kd-network architecture in section 3. an extensive evaluation on toy data (a variation of mnist) and standard benchmarks (modelnet10, modelnet40, shrec'16, shapenet part datasets) is presented in section 4. we summarize the work in section 5."
sameAs(e1, e2)
Comment:

3554	"below, <e1>we</e1> first review the related work on convolutional networks for 3d models in section 2. we then discuss the kd-network architecture in section 3. an extensive evaluation on toy data (a variation of mnist) and standard benchmarks (modelnet10, modelnet40, shrec'16, shapenet part datasets) is presented in section 4. <e2>we</e2> summarize the work in section 5."
sameAs(e1, e2)
Comment:

3555	"below, we first review the related work on convolutional networks for 3d models in section 2. <e1>we</e1> then discuss the kd-network architecture in section 3. an extensive evaluation on toy data (a variation of mnist) and standard benchmarks (modelnet10, modelnet40, shrec'16, shapenet part datasets) is presented in section 4. <e2>we</e2> summarize the work in section 5."
sameAs(e1, e2)
Comment:

3556	"below, we first review the related work on convolutional networks for 3d models in section 2. we then discuss the kd-network architecture in section 3. an extensive evaluation on toy data (a variation of <e1>mnist</e1>) and standard benchmarks (modelnet10, modelnet40, shrec'16, shapenet part <e2>datasets</e2>) is presented in section 4. we summarize the work in section 5."
isA(e1, e2)
Comment:

3557	"64 × 64 × 64), which clearly lag behind grid resolutions typical for processing 2d <e1>data</e1>, and is likely to be insufficient for the recognition tasks that require attention to fine details in the <e2>models</e2>."
Used-for(e1, e2)
Comment:

3558	"to solve this problem, we take inspiration from the long history of research in computer graphics and computational geometry communities [25, 10] , where a large number of indexing structures that are far more scalable than uniform grids have been proposed, including kd-<e1>trees</e1> [1] , octrees [19] , binary spatial partition <e2>trees</e2> [28] , r-trees [11] , constructive solid geometry [22] , etc."
sameAs(e1, e2)
Comment:

3559	"to solve this problem, we take inspiration from the long history of research in computer graphics and computational geometry communities [25, 10] , where a large number of indexing structures that are far more scalable than uniform grids have been proposed, including kd-<e1>trees</e1> [1] , octrees [19] , binary spatial partition trees [28] , r-<e2>trees</e2> [11] , constructive solid geometry [22] , etc."
sameAs(e1, e2)
Comment:

3560	"to solve this problem, we take inspiration from the long history of research in computer graphics and computational geometry communities [25, 10] , where a large number of indexing structures that are far more scalable than uniform grids have been proposed, including kd-trees [1] , octrees [19] , binary spatial partition <e1>trees</e1> [28] , r-<e2>trees</e2> [11] , constructive solid geometry [22] , etc."
sameAs(e1, e2)
Comment:

3561	"recent advances in sign <e1>language</e1> research have given rise to many publicly available sign <e2>language</e2> lexicons that allow searching of the videos by the index of hand shapes."
sameAs(e1, e2)
Comment:

3562	"as such, <e1>this</e1> manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline <e2>this</e2> paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3563	"as such, this manuscript provides the following contributions: • <e1>formulation</e1> of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem <e2>formulation</e2> and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3564	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-<e1>models</e1> (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing <e2>tasks</e2> using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
Used-for(e1, e2)
Comment:

3565	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment <e1>problem</e1> in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise <e2>problem</e2> formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3566	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in <e1>continuous</e1> video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a <e2>continuous</e2> sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3567	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of <e1>cnn</e1> architectures • robust fine grained single frame hand shape recognition based on a <e2>cnn</e2>-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3568	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand <e1>shape</e1> recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand <e2>shape</e2> data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3569	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand <e1>shape</e1> recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand <e2>shape</e2> subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3570	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across <e1>data</e1> sets without retraining • making an articulated sign language hand shape <e2>data</e2> set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3571	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign <e1>language</e1> hand shape data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand shape subunits into a continuous sign <e2>language</e2> recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3572	"as such, this manuscript provides the following contributions: • formulation of an em-based algorithm integrating cnns with hidden-markov-models (hmms) for weak supervision and overcoming the temporal alignment problem in continuous video processing tasks using the strong discriminative capabilities of cnn architectures • robust fine grained single frame hand shape recognition based on a cnn-model, trained on over 1 million hand shapes and shown to generalise across data sets without retraining • making an articulated sign language hand <e1>shape</e1> data set publicly available comprising 3361 manual labelled frames in 45 classes 1 • and integration of pose-independent hand <e2>shape</e2> subunits into a continuous sign language recognition pipeline this paper is organised as follows: after introducing the related literature in section 2 we give a precise problem formulation and the solution in section 3. section 4 introduces the employed data sources."
sameAs(e1, e2)
Comment:

3573	"abstract <e1>we</e1> present ron, an introduction <e2>we</e2> are witnessing significant advances in object detection area, mainly thanks to the deep networks."
sameAs(e1, e2)
Comment:

3574	"in fast r-cnn [10] , the <e1>region</e1>-wise subnetwork repeatedly evaluates thousands of <e2>region</e2> proposals to generate detection scores."
sameAs(e1, e2)
Comment:

3575	"under fast r-<e1>cnn</e1> pipeline, faster r-<e2>cnn</e2> shares full-image convolutional features with the detection network to enable nearly costfree region proposals."
sameAs(e1, e2)
Comment:

3576	"nevertheless, r-fcn still needs <e1>region</e1> proposals generated from <e2>region</e2> proposal networks [23] ."
sameAs(e1, e2)
Comment:

3577	"nevertheless, r-fcn still needs region <e1>proposals</e1> generated from region <e2>proposal</e2> networks [23] ."
sameAs(e1, e2)
Comment:

3578	"it is somewhat resource/<e1>time</e1> consuming when feeding the image into deep networks, both in training and inference <e2>time</e2>."
sameAs(e1, e2)
Comment:

3579	"current top deep-networks-based object detection frameworks could be grouped into two main streams: the <e1>region</e1>-based methods [11] [23] [10] [16] and the <e2>region</e2>-free methods [22] [19] ."
sameAs(e1, e2)
Comment:

3580	"current top deep-networks-based object detection frameworks could be grouped into two main streams: the region-based <e1>methods</e1> [11] [23] [10] [16] and the region-free <e2>methods</e2> [22] [19] ."
sameAs(e1, e2)
Comment:

3581	"we answer this question by trying to bridge the gap between the <e1>region</e1>-based and <e2>region</e2>-free methodologies."
sameAs(e1, e2)
Comment:

3582	"objects of various <e1>scales</e1> could appear at any position of an image, so tens of thousands of regions with different positions/<e2>scales</e2>/aspect ratios should be considered."
sameAs(e1, e2)
Comment:

3583	"prior works [16] [3] show that multi-<e1>scale</e1> representation will significantly improve object detection of various <e2>scales</e2>."
sameAs(e1, e2)
Comment:

3584	"as a result, we propose ron (reverse connection with objectness prior networks) object detection framework, which could associate the merits of <e1>region</e1>-based and <e2>region</e2>-free approaches."
sameAs(e1, e2)
Comment:

3585	"in this example, sofa is responded at <e1>scales</e1> (a) and (b), the brown dog is responded at <e2>scale</e2> (c) and the white spotted dog is responded at scale (d)."
sameAs(e1, e2)
Comment:

3586	"in this example, sofa is responded at <e1>scales</e1> (a) and (b), the brown dog is responded at scale (c) and the white spotted dog is responded at <e2>scale</e2> (d)."
sameAs(e1, e2)
Comment:

3587	"in this example, sofa is responded at scales (a) and (b), the brown dog is responded at <e1>scale</e1> (c) and the white spotted dog is responded at <e2>scale</e2> (d)."
sameAs(e1, e2)
Comment:

3588	"despite its advantages in terms of ease of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classification and <e1>segmentation</e1> tasks, one may wonder whether this simplification comes at a loss of precision as one considers more challenging <e2>prediction</e2> tasks."
Conjunction(e1, e2)
Comment:

3589	"despite its advantages in terms of ease of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classification and segmentation <e1>tasks</e1>, one may wonder whether this simplification comes at a loss of precision as one considers more challenging prediction <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

3590	"despite its advantages in terms of ease of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classification and segmentation tasks, <e1>one</e1> may wonder whether this simplification comes at a loss of precision as <e2>one</e2> considers more challenging prediction tasks."
sameAs(e1, e2)
Comment:

3591	"in their basic form, these models learn a deep <e1>representation</e1> over the discretized surface by combining a latent <e2>representation</e2> at a given node with a local linear combination of its neighbors' latent representations, and a point-wise nonlinearity."
sameAs(e1, e2)
Comment:

3592	"different <e1>models</e1> vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph laplacian, leading to spectral interpretations of those <e2>models</e2>."
sameAs(e1, e2)
Comment:

3593	"more specifically, we exploit the fact <e1>that</e1> surfaces in r 3 admit a first-order differential operator, the dirac operator, <e2>that</e2> is stable to discretization, provides a direct generalization of laplacianbased propagation models, and is able to detect principal curvature directions [8, 16] ."
sameAs(e1, e2)
Comment:

3594	"by combining the dirac operator with input coordinates, <e1>we</e1> obtain a fully differentiable, end-to-end feature representation that <e2>we</e2> apply to several challenging tasks."
sameAs(e1, e2)
Comment:

3595	"• we prove <e1>that</e1> surface networks define shape representations <e2>that</e2> are stable to deformation and to discretization."
sameAs(e1, e2)
Comment:

3596	"corresponding author: bruna@cims.nyu.edu vast amount of high-quality 3d geometric <e1>data</e1> available, <e2>data</e2>-driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data representation regularity which is required for traditional convolutional neural network approaches."
sameAs(e1, e2)
Comment:

3597	"corresponding author: bruna@cims.nyu.edu vast amount of high-quality 3d geometric data available, data-driven <e1>approaches</e1> to problems involving complex geometry have yet to become mainstream, in part due to the lack of data representation regularity which is required for traditional convolutional neural network <e2>approaches</e2>."
sameAs(e1, e2)
Comment:

3598	"with the advent of large datasets pairing <e1>images</e1> with natural language descriptions [20, 34, 10, 16] it has recently become possible to generate novel sentences describing <e2>images</e2> [4, 6, 12, 22, 30] ."
sameAs(e1, e2)
Comment:

3599	"abstract introduction <e1>image deblurring</e1> is a classic <e2>image restoration</e2> problem with a vast body of work in computer vision, signal processing and related fields (see [25] for a fairly recent survey)."
isA(e1, e2)
Comment:

3600	"since <e1>image</e1> deconvolution is mathematically ill-posed in the presence of noise, some form of regularization has to be used to recover a restored <e2>image</e2>."
sameAs(e1, e2)
Comment:

3601	"a classic fast fftbased deconvolution method is the wiener filter [27] , which uses quadratic regularization of the expected <e1>image</e1> spectrum to obtain the restored <e2>image</e2> in closed form."
sameAs(e1, e2)
Comment:

3602	"in this work, we focus on the case of uniform <e1>blur</e1>, where the observed blurred image y = k ⊗ x + η is obtained via convolution of the true image x with known <e2>blur</e2> kernel (point spread function) k and additive gaussian noise η. the task of recovering x is then called (non-blind) image deconvolution."
sameAs(e1, e2)
Comment:

3603	"in this work, we focus on the case of uniform blur, where the observed blurred <e1>image</e1> y = k ⊗ x + η is obtained via convolution of the true <e2>image</e2> x with known blur kernel (point spread function) k and additive gaussian noise η. the task of recovering x is then called (non-blind) image deconvolution."
sameAs(e1, e2)
Comment:

3604	"in this work, we focus on the case of uniform blur, where the observed blurred <e1>image</e1> y = k ⊗ x + η is obtained via convolution of the true image x with known blur kernel (point spread function) k and additive gaussian noise η. the task of recovering x is then called (non-blind) <e2>image</e2> deconvolution."
sameAs(e1, e2)
Comment:

3605	"in this work, we focus on the case of uniform blur, where the observed blurred image y = k ⊗ x + η is obtained via convolution of the true <e1>image</e1> x with known blur kernel (point spread function) k and additive gaussian noise η. the task of recovering x is then called (non-blind) <e2>image</e2> deconvolution."
sameAs(e1, e2)
Comment:

3606	"based on this analysis, <e1>we</e1> propose a new, generalized learning-based approach utilizing the power of convolutional neural networks in section 3. in order to improve the quality of the restored image even further, <e2>we</e2> address the often neglected topic of image boundary handling."
sameAs(e1, e2)
Comment:

3607	"based on this analysis, we propose a new, generalized learning-based approach utilizing the power of convolutional neural networks in section 3. in order to improve the quality of the restored <e1>image</e1> even further, we address the often neglected topic of <e2>image</e2> boundary handling."
sameAs(e1, e2)
Comment:

3608	"we show the efficacy of our proposed model and boundary adjustment method in various non-blind deconvolution experiments in section 4, before <e1>we</e1> conclude in section 5. in this work, <e2>we</e2> solely focus on non-blind deconvolution, while recent research in the field has arguably shifted its focus towards blind deconvolution, which aims to estimate both the blur kernel k and the restored image x. however, most of these approaches make use of non-blind deconvolution steps [e.g., 4, 29] ."
sameAs(e1, e2)
Comment:

3609	"note that although the assumption of uniform <e1>blur</e1> is often not accurate [12, 15] , such image deconvolution techniques can in fact outperform methods which assume a more realistic non-uniform <e2>blur</e2> model [cf."
sameAs(e1, e2)
Comment:

3610	"• we obtain state-of-the-art <e1>results</e1> on non-blind deconvolution benchmarks, even when including <e2>methods</e2> that are computationally considerably more expensive."
Compare(e1, e2)
Comment:

3611	"when it comes to image deconvolution methods, it is useful to broadly separate them into two classes: (1) <e1>those</e1> where the most costly computational operations are a fixed number of fourier transforms or convolutions, and (2) <e2>those</e2> which require more expensive computation, often due to (iterative) solvers for large linear systems of equations."
sameAs(e1, e2)
Comment:

3612	"inferring the latent correspondence between <e1>image</e1> regions and words is a key to more interpretable <e2>image</e2>-text matching by capturing the fine-grained interplay between vision and language."
sameAs(e1, e2)
Comment:

3613	"these models often detect <e1>image</e1> regions at object/stuff level and simply aggregate the similarity of all possible pairs of <e2>image</e2> regions and words in sentence to infer the global image-text similarity; e.g."
sameAs(e1, e2)
Comment:

3614	"these models often detect <e1>image</e1> regions at object/stuff level and simply aggregate the similarity of all possible pairs of image regions and words in sentence to infer the global <e2>image</e2>-text similarity; e.g."
sameAs(e1, e2)
Comment:

3615	"these models often detect image <e1>regions</e1> at object/stuff level and simply aggregate the similarity of all possible pairs of image <e2>regions</e2> and words in sentence to infer the global image-text similarity; e.g."
sameAs(e1, e2)
Comment:

3616	"these models often detect image regions at object/stuff level and simply aggregate the similarity of all possible pairs of <e1>image</e1> regions and words in sentence to infer the global <e2>image</e2>-text similarity; e.g."
sameAs(e1, e2)
Comment:

3617	"we strive to take a step towards attending differentially to important <e1>image</e1> regions and words with each other as context for inferring the <e2>image</e2>-text similarity."
sameAs(e1, e2)
Comment:

3618	"in the proposed imagetext formulation, given an <e1>image</e1> and a sentence, it first attends to words in the sentence with respect to each <e2>image</e2> region, and compares each image region to the attended information from the sentence to decide the importance of the image regions (e.g."
sameAs(e1, e2)
Comment:

3619	"in the proposed imagetext formulation, given an <e1>image</e1> and a sentence, it first attends to words in the sentence with respect to each image region, and compares each <e2>image</e2> region to the attended information from the sentence to decide the importance of the image regions (e.g."
sameAs(e1, e2)
Comment:

3620	"in the proposed imagetext formulation, given an <e1>image</e1> and a sentence, it first attends to words in the sentence with respect to each image region, and compares each image region to the attended information from the sentence to decide the importance of the <e2>image</e2> regions (e.g."
sameAs(e1, e2)
Comment:

3621	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each <e1>image</e1> region, and compares each <e2>image</e2> region to the attended information from the sentence to decide the importance of the image regions (e.g."
sameAs(e1, e2)
Comment:

3622	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each <e1>image</e1> region, and compares each image region to the attended information from the sentence to decide the importance of the <e2>image</e2> regions (e.g."
sameAs(e1, e2)
Comment:

3623	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each image <e1>region</e1>, and compares each image <e2>region</e2> to the attended information from the sentence to decide the importance of the image regions (e.g."
sameAs(e1, e2)
Comment:

3624	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each image <e1>region</e1>, and compares each image region to the attended information from the sentence to decide the importance of the image <e2>regions</e2> (e.g."
sameAs(e1, e2)
Comment:

3625	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each image region, and compares each <e1>image</e1> region to the attended information from the sentence to decide the importance of the <e2>image</e2> regions (e.g."
sameAs(e1, e2)
Comment:

3626	"in the proposed imagetext formulation, given an image and a sentence, it first attends to words in the sentence with respect to each image region, and compares each image <e1>region</e1> to the attended information from the sentence to decide the importance of the image <e2>regions</e2> (e.g."
sameAs(e1, e2)
Comment:

3627	"likewise, in the proposed text-<e1>image</e1> formulation, it first attends to <e2>image</e2> regions with respect to each word and then decides to pay more or less attention to each word."
sameAs(e1, e2)
Comment:

3628	"compared to models that perform fixed-step attentional reasoning and thus only focus on limited semantic <e1>alignments</e1> (one at a time) [31, 16] , stacked cross attention discovers all possible <e2>alignments</e2> simultaneously."
sameAs(e1, e2)
Comment:

3629	"to identify the salient <e1>regions</e1> in image, we follow anderson et al [1] to analogize the detection of salient <e2>regions</e2> at object/stuff level to the spontaneous bottom-up attention in the human vision system [4, 6, 21] , and practically imple-ment bottom-up attention using faster r-cnn [34] , which represents a natural expression of a bottom-up attention mechanism."
sameAs(e1, e2)
Comment:

3630	"on flickr30k, our <e1>approach</e1> outperforms the current best <e2>methods</e2> by 22.1% relatively in text retrivel from image query, and 18.2% relatively in image retrieval with text query (based on recall@1)."
Compare(e1, e2)
Comment:

3631	"on flickr30k, our approach outperforms the current best methods by 22.1% relatively in <e1>text</e1> retrivel from image query, and 18.2% relatively in image retrieval with <e2>text</e2> query (based on recall@1)."
sameAs(e1, e2)
Comment:

3632	"on flickr30k, our approach outperforms the current best methods by 22.1% relatively in text retrivel from <e1>image</e1> query, and 18.2% relatively in <e2>image</e2> retrieval with text query (based on recall@1)."
sameAs(e1, e2)
Comment:

3633	"on ms-coco, it improves sentence <e1>retrieval</e1> by 17.8% relatively and image <e2>retrieval</e2> by 16.6% relatively (based on recall@1 using the 5k test set)."
sameAs(e1, e2)
Comment:

3634	"prior work either simply aggregates the similarity of all possible pairs of <e1>regions</e1> and words without attending differentially to more and less important words or <e2>regions</e2>, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable."
sameAs(e1, e2)
Comment:

3635	"prior work either simply aggregates the similarity of all possible pairs of regions and <e1>words</e1> without attending differentially to more and less important <e2>words</e2> or regions, or uses a multi-step attentional process to capture limited number of semantic alignments which is less interpretable."
sameAs(e1, e2)
Comment:

3636	"in this paper, we present stacked cross attention to discover the full latent alignments using both <e1>image</e1> regions and words in a sentence as context and infer <e2>image</e2>-text similarity."
sameAs(e1, e2)
Comment:

3637	"on flickr30k, our <e1>approach</e1> outperforms the current best <e2>methods</e2> by 22.1% relatively in text retrieval from image query, and 18.2% relatively in image retrieval with text query (based on recall@1)."
Compare(e1, e2)
Comment:

3638	"on flickr30k, our approach outperforms the current best methods by 22.1% relatively in <e1>text</e1> retrieval from image query, and 18.2% relatively in image retrieval with <e2>text</e2> query (based on recall@1)."
sameAs(e1, e2)
Comment:

3639	"on flickr30k, our approach outperforms the current best methods by 22.1% relatively in text <e1>retrieval</e1> from image query, and 18.2% relatively in image <e2>retrieval</e2> with text query (based on recall@1)."
sameAs(e1, e2)
Comment:

3640	"on flickr30k, our approach outperforms the current best methods by 22.1% relatively in text retrieval from <e1>image</e1> query, and 18.2% relatively in <e2>image</e2> retrieval with text query (based on recall@1)."
sameAs(e1, e2)
Comment:

3641	"on ms-coco, our approach improves sentence <e1>retrieval</e1> by 17.8% relatively and image <e2>retrieval</e2> by 16.6% relatively (based on recall@1 using the 5k test set)."
sameAs(e1, e2)
Comment:

3642	"abstract it is desirable to train <e1>convolutional networks</e1> (cnns) introduction <e2>convolutional networks</e2> (cnns) [7] have greatly accelerated the progress of many computer vision areas and applications in recent years."
sameAs(e1, e2)
Comment:

3643	"abstract it is desirable to train convolutional networks (<e1>cnns</e1>) introduction convolutional networks (<e2>cnns</e2>) [7] have greatly accelerated the progress of many computer vision areas and applications in recent years."
sameAs(e1, e2)
Comment:

3644	"as a simple solution to such a concern, <e1>one</e1> could train several cnn models such that each has a different inference cost, and then select the <e2>one</e2> that matches the given budget at inference time."
sameAs(e1, e2)
Comment:

3645	"as a simple solution to such a concern, one could train several cnn models such <e1>that</e1> each has a different inference cost, and then select the one <e2>that</e2> matches the given budget at inference time."
sameAs(e1, e2)
Comment:

3646	"however, it is extremely time-consuming to train many <e1>models</e1>, not to mention the computational storage required to store the weights of many <e2>models</e2>."
sameAs(e1, e2)
Comment:

3647	"a sdpoint instance is a <e1>network</e1> configuration consisting of a unique downsampling point (layer index) in the <e2>network</e2> layer hierarchy as well as a unique downsampling ratio."
sameAs(e1, e2)
Comment:

3648	"existing approaches [22, 38, 20 ] to achieve cost-adjustable inference in cnns work by evaluating just subparts of the <e1>network</e1> (e.g., skipping layers or skipping subpaths), and therefore not all <e2>network</e2> parameters are utilized during cheaper inference."
sameAs(e1, e2)
Comment:

3649	"in contrast to existing approaches, sdpoint makes full use of all <e1>network</e1> parameters regardless of the inference costs, thus making better use of <e2>network</e2> representational capacity."
sameAs(e1, e2)
Comment:

3650	"furthermore, as the amount of visual <e1>data</e1> grows, we need increasingly higher-capacity (thus higher complexity) cnns which have shown to better utilize these large visual <e2>data</e2> compared to their lower-capacity counterparts [35] ."
sameAs(e1, e2)
Comment:

3651	"these <e1>methods</e1> result in more efficient <e2>models</e2> which have fixed inference costs (measured in floating-point operations or flops)."
Compare(e1, e2)
Comment:

3652	"models with fixed inference costs cannot work effectively in certain <e1>resource</e1>-constrained vision systems, where the computational budget that can be allocated to cnn inference depends on the real-time <e2>resource</e2> availability."
sameAs(e1, e2)
Comment:

3653	"our <e1>method</e1> was extensively evaluated and outperformed other <e2>methods</e2> with a large margin in most settings."
Compare(e1, e2)
Comment:

3654	"moreover, if <e1>robots</e1> working in the real-world can detect unknown objects and ask annotators to give labels to them, these <e2>robots</e2> will be able to easily expand their knowledge."
sameAs(e1, e2)
Comment:

3655	"in domain adaptation, we aim to train a classifier from a label-rich <e1>domain</e1> (source domain) and apply it to a label-scarce <e2>domain</e2> (target domain)."
sameAs(e1, e2)
Comment:

3656	"it is assumed <e1>that</e1> access is granted to the unknown source samples although the class of unknown source does not overlap with <e2>that</e2> of unknown target."
sameAs(e1, e2)
Comment:

3657	"numerous algorithms have been proposed for transferring knowledge from a label-rich <e1>domain</e1> (source) to a label-scarce <e2>domain</e2> (target)."
sameAs(e1, e2)
Comment:

3658	"the goal of the task is to classify unknown <e1>target</e1> samples as "unknown" and to classify known <e2>target</e2> samples into correct known categories."
sameAs(e1, e2)
Comment:

3659	"then, in this paper, <e1>we</e1> present a more challenging open set domain adaptation (osda) that does not provide any unknown source samples, and <e2>we</e2> propose a method for it."
sameAs(e1, e2)
Comment:

3660	"that is, <e1>we</e1> propose a method where <e2>we</e2> have access to only known source samples and unlabeled target samples for open set domain adaptation as shown in the right of fig."
sameAs(e1, e2)
Comment:

3661	"although we need to align <e1>target</e1> samples with source samples to reduce this domain's difference, unknown <e2>target</e2> samples cannot be aligned due to the absence of unknown samples in the source domain."
sameAs(e1, e2)
Comment:

3662	"the existing distribution <e1>matching</e1> method is aimed at <e2>matching</e2> the distribution of the target with that of the source."
sameAs(e1, e2)
Comment:

3663	"unlike the existing distribution <e1>alignment</e1> methods that only match the source and target distribution, our method facilitates the rejection of unknown target samples with high accuracy as well as the <e2>alignment</e2> of known target samples with known source samples."
sameAs(e1, e2)
Comment:

3664	"unlike the existing distribution alignment methods that only match the source and <e1>target</e1> distribution, our method facilitates the rejection of unknown <e2>target</e2> samples with high accuracy as well as the alignment of known target samples with known source samples."
sameAs(e1, e2)
Comment:

3665	"unlike the existing distribution alignment methods that only match the source and <e1>target</e1> distribution, our method facilitates the rejection of unknown target samples with high accuracy as well as the alignment of known <e2>target</e2> samples with known source samples."
sameAs(e1, e2)
Comment:

3666	"unlike the existing distribution alignment methods that only match the source and target distribution, our method facilitates the rejection of unknown <e1>target</e1> samples with high accuracy as well as the alignment of known <e2>target</e2> samples with known source samples."
sameAs(e1, e2)
Comment:

3667	"the feature generator generates <e1>features</e1> from inputs, and the classifier takes the <e2>features</e2> and outputs k + 1 dimension probability, where k indicates the number of known classes."
sameAs(e1, e2)
Comment:

3668	"the classifier is trained to make a boundary between source and <e1>target</e1> samples whereas the feature generator is trained to make <e2>target</e2> samples far from the boundary."
sameAs(e1, e2)
Comment:

3669	"specifically, <e1>we</e1> train the classifier to output probability t for unknown class, where 0 < t < 1. <e2>we</e2> can build a decision boundary for unknown samples by weakly training a classifier to classify target samples as unknown."
sameAs(e1, e2)
Comment:

3670	"specifically, we train the <e1>classifier</e1> to output probability t for unknown class, where 0 < t < 1. we can build a decision boundary for unknown samples by weakly training a <e2>classifier</e2> to classify target samples as unknown."
sameAs(e1, e2)
Comment:

3671	"as such, we assign two options to the feature generator: aligning <e1>them</e1> with samples in the source domain or rejecting <e2>them</e2> as unknown."
sameAs(e1, e2)
Comment:

3672	"this approach allows to extract features that separate unknown <e1>target</e1> from known <e2>target</e2> samples."
sameAs(e1, e2)
Comment:

3673	"during training, we assign two options to the feature generator: aligning <e1>target</e1> samples with source known ones or rejecting them as unknown <e2>target</e2> ones."
sameAs(e1, e2)
Comment:

3674	"during training, we assign two options to the feature generator: aligning target samples with source known <e1>ones</e1> or rejecting them as unknown target <e2>ones</e2>."
sameAs(e1, e2)
Comment:

3675	"to train generative models for <e1>these</e1> <e2>tasks</e2>, these objects are often first represented as strings."
Used-for(e1, e2)
Comment:

3676	"to train generative models for <e1>these</e1> tasks, <e2>these</e2> objects are often first represented as strings."
sameAs(e1, e2)
Comment:

3677	"for instance, molecules can be represented by so-called smiles <e1>strings</e1> (weininger, 1988) and gómez-bombarelli et al (2016b) has recently developed a generative model for molecules based on smiles <e2>strings</e2> that uses grus and dcnns."
sameAs(e1, e2)
Comment:

3678	"however, one immediate difficulty in using strings to represent discrete <e1>objects</e1> is that the representation is very brittle: small changes in the string can lead to completely different <e2>objects</e2>, or often do not correspond to valid objects at all."
sameAs(e1, e2)
Comment:

3679	"however, one immediate difficulty in using strings to represent discrete <e1>objects</e1> is that the representation is very brittle: small changes in the string can lead to completely different objects, or often do not correspond to valid <e2>objects</e2> at all."
sameAs(e1, e2)
Comment:

3680	"however, one immediate difficulty in using strings to represent discrete objects is that the representation is very brittle: small changes in the string can lead to completely different <e1>objects</e1>, or often do not correspond to valid <e2>objects</e2> at all."
sameAs(e1, e2)
Comment:

3681	"specifically, gómez-bombarelli et al (2016b) described that while searching for new molecules, the probabilistic decoder -the distribution which maps from the continuous latent space into the space of molecular structures -would sometimes accidentally put high probability on <e1>strings</e1> which are not valid smiles <e2>strings</e2> or do not encode plausible molecules."
sameAs(e1, e2)
Comment:

3682	"given a <e1>grammar</e1>, every valid discrete object can be described as a parse tree from the <e2>grammar</e2>."
sameAs(e1, e2)
Comment:

3683	"we also show <e1>that</e1> this learned latent space is effective for searching for arithmetic expressions <e2>that</e2> fit data, for finding better drug-like molecules, and for making accurate predictions about target properties."
sameAs(e1, e2)
Comment:

3684	"surprisingly, we show that not only does our model more often generate valid <e1>outputs</e1>, it also learns a more coherent latent space in which nearby points decode to similar discrete <e2>outputs</e2>."
sameAs(e1, e2)
Comment:

3685	"the <e1>network</e1> is extended to embed a sparse coding-based <e2>network</e2> [33] or use a deeper structure [17] ."
sameAs(e1, e2)
Comment:

3686	"several algorithms accelerate srcnn by performing <e1>convolution</e1> on lr images and replacing the pre-defined upsampling operator with sub-pixel <e2>convolution</e2> [28] or transposed convolution [8] (also named as deconvolution in some of the literature)."
sameAs(e1, e2)
Comment:

3687	"several algorithms accelerate srcnn by performing <e1>convolution</e1> on lr images and replacing the pre-defined upsampling operator with sub-pixel convolution [28] or transposed <e2>convolution</e2> [8] (also named as deconvolution in some of the literature)."
sameAs(e1, e2)
Comment:

3688	"several algorithms accelerate srcnn by performing convolution on lr images and replacing the pre-defined upsampling operator with sub-pixel <e1>convolution</e1> [28] or transposed <e2>convolution</e2> [8] (also named as deconvolution in some of the literature)."
sameAs(e1, e2)
Comment:

3689	"since the ℓ 2 loss fails to capture the underlying multi-modal distributions of hr <e1>patches</e1> (i.e., the same lr patch may have many corresponding hr <e2>patches</e2>), the reconstructed hr images are often overlysmooth and not close to human visual perception on natural images."
sameAs(e1, e2)
Comment:

3690	"finally, we use a convolutional layer to predict the subband residuals (the differences between the upsampled <e1>image</e1> and the ground truth hr <e2>image</e2> at the respective level)."
sameAs(e1, e2)
Comment:

3691	"introduction single-image super-resolution (sr) aims to reconstruct a high-<e1>resolution</e1> (hr) image from a single low-<e2>resolution</e2> (lr) input image."
sameAs(e1, e2)
Comment:

3692	"introduction single-image super-resolution (sr) aims to reconstruct a high-resolution (hr) <e1>image</e1> from a single low-resolution (lr) input <e2>image</e2>."
sameAs(e1, e2)
Comment:

3693	"in recent years, example-based sr methods have demonstrated the state-of-the-art performance by learning a mapping from lr to hr <e1>image</e1> patches using large <e2>image</e2> databases."
sameAs(e1, e2)
Comment:

3694	"introduction the foveation <e1>mechanism</e1> in the human visual <e2>system</e2> (hvs) indicates that only a small fovea region captures most visual attention at high resolution, while other peripheral regions receive little attention at low resolution."
Part-of(e1, e2)
Comment:

3695	"introduction the foveation mechanism in the human visual system (hvs) indicates that only a small fovea <e1>region</e1> captures most visual attention at high resolution, while other peripheral <e2>regions</e2> receive little attention at low resolution."
sameAs(e1, e2)
Comment:

3696	"introduction the foveation mechanism in the human visual system (hvs) indicates that only a small fovea region captures most visual attention at high <e1>resolution</e1>, while other peripheral regions receive little attention at low <e2>resolution</e2>."
sameAs(e1, e2)
Comment:

3697	"to predict human attention, saliency prediction has been widely studied in recent years, with multiple applications [5, 21, 22, 38] in <e1>object recognition</e1>, object segmentation, <e2>action recognition</e2>, image caption, and image/video compression, among others."
Used-for(e1, e2)
Comment:

3698	"to predict human attention, saliency prediction has been widely studied in recent years, with multiple applications [5, 21, 22, 38] in object recognition, object segmentation, action recognition, <e1>image</e1> caption, and <e2>image</e2>/video compression, among others."
sameAs(e1, e2)
Comment:

3699	"in this paper, we focus on predicting <e1>video</e1> saliency at the pixel level, which models attention on each <e2>video</e2> frame."
sameAs(e1, e2)
Comment:

3700	"the traditional <e1>video</e1> saliency prediction methods mainly focus on the feature integration theory [16, 19, 20, 26] , in which some spatial and temporal features were developed for <e2>video</e2> saliency prediction."
sameAs(e1, e2)
Comment:

3701	"the traditional video saliency <e1>prediction</e1> methods mainly focus on the feature integration theory [16, 19, 20, 26] , in which some spatial and temporal features were developed for video saliency <e2>prediction</e2>."
sameAs(e1, e2)
Comment:

3702	"however, only a few works have managed to apply dl in <e1>video</e1> (1) the regions with object can draw a majority of human attention, (2) the moving objects or the moving parts of objects attract more human attention, and (3) a dynamic pixel-wise transition of human attention occurs across <e2>video</e2> frames."
sameAs(e1, e2)
Comment:

3703	"however, only a few works have managed to apply dl in video (1) the regions with object can draw a majority of human attention, (2) the moving <e1>objects</e1> or the moving parts of <e2>objects</e2> attract more human attention, and (3) a dynamic pixel-wise transition of human attention occurs across video frames."
sameAs(e1, e2)
Comment:

3704	"bazzani et al [2] leveraged a deep convolutional 3d (c3d) <e1>network</e1> to learn the representations of human attention on 16 consecutive frames, and then a long shortterm memory (lstm) <e2>network</e2> connected to a mixture density network was learned to generate saliency maps in a gaussian mixture distribution."
sameAs(e1, e2)
Comment:

3705	"bazzani et al [2] leveraged a deep convolutional 3d (c3d) <e1>network</e1> to learn the representations of human attention on 16 consecutive frames, and then a long shortterm memory (lstm) network connected to a mixture density <e2>network</e2> was learned to generate saliency maps in a gaussian mixture distribution."
sameAs(e1, e2)
Comment:

3706	"bazzani et al [2] leveraged a deep convolutional 3d (c3d) network to learn the representations of human attention on 16 consecutive frames, and then a long shortterm memory (lstm) <e1>network</e1> connected to a mixture density <e2>network</e2> was learned to generate saliency maps in a gaussian mixture distribution."
sameAs(e1, e2)
Comment:

3707	"for training the dl networks, we establish a large-scale eye-tracking database of <e1>videos</e1> (ledov) that contains the free-view fixation data of 32 subjects viewing 538 diverse-content <e2>videos</e2>."
sameAs(e1, e2)
Comment:

3708	"we find from figure 1 that people tend to be attracted by the moving <e1>objects</e1> or the moving parts of <e2>objects</e2>, and this finding is also verified in the analysis of our ledov database."
sameAs(e1, e2)
Comment:

3709	"in deepvs, a novel object-to-<e1>motion</e1> convolutional neural network (om-cnn) is constructed to learn the features of object <e2>motion</e2>, in which the cross-net mask and hierarchical feature normalization (fn) are proposed to combine the subnets of objectness and motion."
sameAs(e1, e2)
Comment:

3710	"in deepvs, a novel object-to-<e1>motion</e1> convolutional neural network (om-cnn) is constructed to learn the features of object motion, in which the cross-net mask and hierarchical feature normalization (fn) are proposed to combine the subnets of objectness and <e2>motion</e2>."
sameAs(e1, e2)
Comment:

3711	"in deepvs, a novel object-to-motion convolutional neural network (om-cnn) is constructed to learn the features of object <e1>motion</e1>, in which the cross-net mask and hierarchical feature normalization (fn) are proposed to combine the subnets of objectness and <e2>motion</e2>."
sameAs(e1, e2)
Comment:

3712	"specifically, we establish a large-scale eye-tracking database of <e1>videos</e1> (ledov), which includes 32 subjects' fixations on 538 <e2>videos</e2>."
sameAs(e1, e2)
Comment:

3713	"the traditional <e1>lstm</e1> networks for video saliency prediction [2, 23] assume that human attention follows the gaussian mixture distribution, since these <e2>lstm</e2> networks cannot generate structured output."
sameAs(e1, e2)
Comment:

3714	"the traditional lstm <e1>networks</e1> for video saliency prediction [2, 23] assume that human attention follows the gaussian mixture distribution, since these lstm <e2>networks</e2> cannot generate structured output."
sameAs(e1, e2)
Comment:

3715	"we find from ledov that human attention is more likely to be attracted by <e1>objects</e1>, particularly the moving <e2>objects</e2> or the moving parts of objects."
sameAs(e1, e2)
Comment:

3716	"we find from ledov that human attention is more likely to be attracted by <e1>objects</e1>, particularly the moving objects or the moving parts of <e2>objects</e2>."
sameAs(e1, e2)
Comment:

3717	"we find from ledov that human attention is more likely to be attracted by objects, particularly the moving <e1>objects</e1> or the moving parts of <e2>objects</e2>."
sameAs(e1, e2)
Comment:

3718	"hence, an object-to-<e1>motion</e1> convolutional neural network (om-cnn) is developed to predict the intra-frame saliency for deepvs, which is composed of the objectness and <e2>motion</e2> subnets."
sameAs(e1, e2)
Comment:

3719	"in om-cnn, cross-net mask and hierarchical feature normalization are proposed to combine the spatial <e1>features</e1> of the objectness subnet and the temporal <e2>features</e2> of the motion subnet."
sameAs(e1, e2)
Comment:

3720	"consequently, the interframe saliency <e1>maps</e1> of a video can be generated, which consider both structured output with center-bias and cross-frame transitions of human attention <e2>maps</e2>."
sameAs(e1, e2)
Comment:

3721	"we consider a hypothesis class such that each hypothesis is a combination of two functions -namely, a deterministic <e1>function</e1> taking x as an input and a stochastic <e2>function</e2> taking x ⋆ as an input."
sameAs(e1, e2)
Comment:

3722	"we consider a hypothesis class such that each hypothesis is a combination of two functions -namely, a deterministic function taking x as an <e1>input</e1> and a stochastic function taking x ⋆ as an <e2>input</e2>."
sameAs(e1, e2)
Comment:

3723	"this structure is natural in the context of convolutional neural networks (cnns) and <e1>recurrent neural networks</e1> (<e2>rnns</e2>) thanks to the dropout."
sameAs(e1, e2)
Comment:

3724	"dropout is a widely adopted tool to regularize <e1>neural networks</e1> by multiplying the activations of a <e2>neural network</e2> at some layer with a random vector."
sameAs(e1, e2)
Comment:

3725	"in other words, dropout becomes the stochastic <e1>function</e1> taking x ⋆ as an input and marginalizing the <e2>function</e2> corresponds to not utilizing dropout in the test phase."
sameAs(e1, e2)
Comment:

3726	"in order to be able to train the heteroscedastic dropout, <e1>we</e1> use gaussian dropout instead of bernoulli because the key technical tool <e2>we</e2> use is the reparameterization trick [21] which is only available for some specific distributions, including the gaussian."
sameAs(e1, e2)
Comment:

3727	"in our proposed heteroscedastic dropout, the privileged information is used to estimate this uncertainty so that hard <e1>examples</e1> and easy <e2>examples</e2> are treated accordingly during training."
sameAs(e1, e2)
Comment:

3728	"our method is <e1>problem</e1>-and modality-agnostic and can be incorporated as long as dropout can be utilized in the original <e2>problem</e2> and the privileged information can be encoded with an appropriate neural network."
sameAs(e1, e2)
Comment:

3729	"in a typical <e1>machine learning</e1> setup, we present tuples {(x i , y i )} n i=1 to a <e2>machine learning</e2> model."
sameAs(e1, e2)
Comment:

3730	"in <e1>this</e1> work, we propose to utilize <e2>this</e2> information in order to control the variance of the dropout."
sameAs(e1, e2)
Comment:

3731	"ti-<e1>pooling</e1>: transformation-invariant <e2>pooling</e2> for feature learning in convolutional neural networks"
sameAs(e1, e2)
Comment:

3732	"abstract <e1>deep convolutional neural networks</e1> take gpu- introduction <e2>deep convolutional neural networks</e2> (convnets) achieve state of the art results on image recognition problems [12] [8] ."
sameAs(e1, e2)
Comment:

3733	"larger <e1>data</e1> sets and <e2>models</e2> lead to better accuracy but also increase computation time."
Used-for(e1, e2)
Comment:

3734	"distributed training of convnets can be achieved by partitioning each batch of examples across the <e1>nodes</e1> of a cluster and accumulating weight updates across the <e2>nodes</e2>."
sameAs(e1, e2)
Comment:

3735	"abstract clustering is the task of grouping a set of <e1>examples</e1> so that similar <e2>examples</e2> are grouped into the same cluster while dissimilar examples are in different clusters."
sameAs(e1, e2)
Comment:

3736	"abstract clustering is the task of grouping a set of <e1>examples</e1> so that similar examples are grouped into the same cluster while dissimilar <e2>examples</e2> are in different clusters."
sameAs(e1, e2)
Comment:

3737	"abstract clustering is the task of grouping a set of examples so that similar <e1>examples</e1> are grouped into the same cluster while dissimilar <e2>examples</e2> are in different clusters."
sameAs(e1, e2)
Comment:

3738	"it consists in grouping a set of <e1>examples</e1> so that "similar" <e2>examples</e2> are in the same cluster while "dissimilar" examples are in different clusters."
sameAs(e1, e2)
Comment:

3739	"it consists in grouping a set of <e1>examples</e1> so that "similar" examples are in the same cluster while "dissimilar" <e2>examples</e2> are in different clusters."
sameAs(e1, e2)
Comment:

3740	"it consists in grouping a set of examples so that "similar" <e1>examples</e1> are in the same cluster while "dissimilar" <e2>examples</e2> are in different clusters."
sameAs(e1, e2)
Comment:

3741	"we derive a closed-form expression for the <e1>gradient</e1> that is efficient to compute: the complexity to compute the <e2>gradient</e2> is linear in the size of the training mini-batch and quadratic in the representation dimensionality."
sameAs(e1, e2)
Comment:

3742	"unsupervised learning <e1>models</e1> are often generative and supervised classifiers are often discriminative; generative model learning has been traditionally considered to be a much harder task than discriminative learning [12] due to its intrinsic learning complexity, as well as many assumptions and simplifications made about the underlying <e2>models</e2>."
sameAs(e1, e2)
Comment:

3743	"in [45] , a self supervised <e1>boosting algorithm</e1> was proposed to train a <e2>boosting algorithm</e2> by sequentially adding features as weak classifiers on additionally self-generated negative samples."
sameAs(e1, e2)
Comment:

3744	"in [45] , a self supervised <e1>boosting algorithm</e1> was proposed to train a boosting algorithm by sequentially adding features as weak <e2>classifiers</e2> on additionally self-generated negative samples."
isA(e1, e2)
Comment:

3745	"in [45] , a self supervised boosting algorithm was proposed to train a <e1>boosting algorithm</e1> by sequentially adding features as weak <e2>classifiers</e2> on additionally self-generated negative samples."
isA(e1, e2)
Comment:

3746	"inspired by the prior work on generative <e1>modeling</e1> [51, 45, 40] and development of convolutional neural networks [27, 26, 13] , we develop an image <e2>modeling</e2> algorithm, introspective neural networks for generative modeling (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn classifier learning stage (classification) for self-evaluation and model updating from the previous synthesis."
sameAs(e1, e2)
Comment:

3747	"inspired by the prior work on generative <e1>modeling</e1> [51, 45, 40] and development of convolutional neural networks [27, 26, 13] , we develop an image modeling algorithm, introspective neural networks for generative <e2>modeling</e2> (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn classifier learning stage (classification) for self-evaluation and model updating from the previous synthesis."
sameAs(e1, e2)
Comment:

3748	"inspired by the prior work on generative modeling [51, 45, 40] and development of <e1>convolutional neural networks</e1> [27, 26, 13] , we develop an image modeling algorithm, introspective <e2>neural networks</e2> for generative modeling (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn classifier learning stage (classification) for self-evaluation and model updating from the previous synthesis."
isA(e1, e2)
Comment:

3749	"inspired by the prior work on generative modeling [51, 45, 40] and development of convolutional neural networks [27, 26, 13] , we develop an <e1>image</e1> modeling algorithm, introspective neural networks for generative modeling (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn classifier learning stage (<e2>classification</e2>) for self-evaluation and model updating from the previous synthesis."
Used-for(e1, e2)
Comment:

3750	"inspired by the prior work on generative modeling [51, 45, 40] and development of convolutional neural networks [27, 26, 13] , we develop an image <e1>modeling</e1> algorithm, introspective neural networks for generative <e2>modeling</e2> (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn classifier learning stage (classification) for self-evaluation and model updating from the previous synthesis."
sameAs(e1, e2)
Comment:

3751	"inspired by the prior work on generative modeling [51, 45, 40] and development of convolutional neural networks [27, 26, 13] , we develop an image modeling algorithm, introspective <e1>neural networks</e1> for generative modeling (inng) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a cnn <e2>classifier</e2> learning stage (classification) for self-evaluation and model updating from the previous synthesis."
Used-for(e1, e2)
Comment:

3752	"there are a number of interesting properties about inng worth highlighting: • <e1>cnn</e1> classifier as generator: no special conditions on the <e2>cnn</e2> architecture are needed in inng and existing cnn classifiers can be directly made into generators, if trained properly."
sameAs(e1, e2)
Comment:

3753	"there are a number of interesting properties about inng worth highlighting: • <e1>cnn</e1> classifier as generator: no special conditions on the cnn architecture are needed in inng and existing <e2>cnn</e2> classifiers can be directly made into generators, if trained properly."
sameAs(e1, e2)
Comment:

3754	"there are a number of interesting properties about inng worth highlighting: • cnn classifier as generator: no special conditions on the <e1>cnn</e1> architecture are needed in inng and existing <e2>cnn</e2> classifiers can be directly made into generators, if trained properly."
sameAs(e1, e2)
Comment:

3755	"• all <e1>backpropagation</e1>: our synthesis-by-classification algorithm performs efficient training using <e2>backpropagation</e2> in both stages: the sampling stage for the input images and the classification training stage for the cnn parameters."
sameAs(e1, e2)
Comment:

3756	"• all backpropagation: our synthesis-by-<e1>classification</e1> algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the <e2>classification</e2> training stage for the cnn parameters."
sameAs(e1, e2)
Comment:

3757	"• all backpropagation: our synthesis-by-classification algorithm performs efficient <e1>training</e1> using backpropagation in both stages: the sampling stage for the input images and the classification <e2>training</e2> stage for the cnn parameters."
sameAs(e1, e2)
Comment:

3758	"• model-based anysize-<e1>image</e1>-generation: since we model the input <e2>image</e2>, we can train on images of a given size and generate an image of a larger size while maintaining coherence of the entire image."
sameAs(e1, e2)
Comment:

3759	"• model-based anysize-<e1>image</e1>-generation: since we model the input image, we can train on images of a given size and generate an <e2>image</e2> of a larger size while maintaining coherence of the entire image."
sameAs(e1, e2)
Comment:

3760	"• model-based anysize-<e1>image</e1>-generation: since we model the input image, we can train on images of a given size and generate an image of a larger size while maintaining coherence of the entire <e2>image</e2>."
sameAs(e1, e2)
Comment:

3761	"• model-based anysize-image-generation: since <e1>we</e1> model the input image, <e2>we</e2> can train on images of a given size and generate an image of a larger size while maintaining coherence of the entire image."
sameAs(e1, e2)
Comment:

3762	"• model-based anysize-image-generation: since we model the input <e1>image</e1>, we can train on images of a given size and generate an <e2>image</e2> of a larger size while maintaining coherence of the entire image."
sameAs(e1, e2)
Comment:

3763	"• model-based anysize-image-generation: since we model the input <e1>image</e1>, we can train on images of a given size and generate an image of a larger size while maintaining coherence of the entire <e2>image</e2>."
sameAs(e1, e2)
Comment:

3764	"• model-based anysize-image-generation: since we model the input image, we can train on images of a given size and generate an <e1>image</e1> of a larger size while maintaining coherence of the entire <e2>image</e2>."
sameAs(e1, e2)
Comment:

3765	"in addition to the applications shown here, extension of the objective (loss) function within inng is expected to work for other tasks such as "<e1>image</e1>-to-<e2>image</e2> translation" [21] ."
sameAs(e1, e2)
Comment:

3766	"unsupervised learning, where no <e1>task</e1>-specific labeling/feedback is provided on top of the input <e2>data</e2>, still remains one of the most difficult problems in machine learning but holds a bright future since a large number of tasks have little to no supervision."
Conjunction(e1, e2)
Comment:

3767	"in a nutshell, <e1>unsupervised learning</e1> techniques are mostly guided by the minimum description length principle (mdl) [35] to best reconstruct the data whereas <e2>supervised learning</e2> methods are primarily driven by minimizing error metrics to best fit the input labeling."
Conjunction(e1, e2)
Comment:

3768	"the central question <e1>we</e1> address in this paper is: when experts are available, how can <e2>we</e2> most effectively leverage their feedback?"
sameAs(e1, e2)
Comment:

3769	"focusing on relevant parts of the state space speeds up learning (improves sample <e1>efficiency</e1>), while omitting feedback on the already mastered subtasks reduces expert effort (improves label <e2>efficiency</e2>)."
sameAs(e1, e2)
Comment:

3770	"introduction learning good agent behavior from <e1>reward</e1> signals alonethe goal of reinforcement learning (rl)-is particularly difficult when the planning horizon is long and <e2>rewards</e2> are sparse."
sameAs(e1, e2)
Comment:

3771	"abstract <e1>ensemble methods</e1> are arguably the most trustworthy techniques for boosting the performance of <e2>machine learning</e2> models."
Used-for(e1, e2)
Comment:

3772	"despite continued efforts <e1>that</e1> apply various ensemble methods such as bagging and boosting to deep models, it has been observed <e2>that</e2> traditional independent ensembles (ie) which train models independently with random initialization achieve the best performance (ciregan et al, 2012; lee et al, 2015) ."
sameAs(e1, e2)
Comment:

3773	"popular independent <e1>ensembles</e1> (ie) relying on naïve averaging/voting scheme have been of typical choice for most applications involving deep neural networks, but they do not consider advanced collaboration among <e2>ensemble</e2> models."
sameAs(e1, e2)
Comment:

3774	"introduction <e1>ensemble methods</e1> have played a critical role in the <e2>machine learning</e2> community to obtain better predictive performance than what could be obtained from any of the constituent learning models alone, e.g., bayesian model/parameter averaging (domingos, 2000) , boosting (freund et al, 1999) and bagging (breiman, 1996) ."
Used-for(e1, e2)
Comment:

3775	"in <e1>this</e1> work, we strive to reconcile <e2>this</e2> gap."
sameAs(e1, e2)
Comment:

3776	"first, we develop a reference set encoder with the attention mechanism to encode a set of reference images of a class to an embedding vector <e1>that</e1> represents <e2>that</e2> class."
sameAs(e1, e2)
Comment:

3777	"since class embeddings generated from different reference sets represents different <e1>classes</e1> where we wish the model to adapt to, cleannet can generalize to <e2>classes</e2> without explicit human supervision."
sameAs(e1, e2)
Comment:

3778	"simple thresholding based on the similarity between the reference set and the query image lead to good <e1>results</e1> compared with existing <e2>methods</e2>."
Compare(e1, e2)
Comment:

3779	"label noise detection not only is useful for training <e1>image</e1> classifiers with noisy data, but also has important values in applications like <e2>image</e2> search result filtering and linking images to knowledge graph entities."
sameAs(e1, e2)
Comment:

3780	"therefore, we propose to use cleannet to assign weights to <e1>image</e1> samples according to the <e2>image</e2>-to-label relevance to guide training of the image classifier."
sameAs(e1, e2)
Comment:

3781	"therefore, we propose to use cleannet to assign weights to <e1>image</e1> samples according to the image-to-label relevance to guide training of the <e2>image</e2> classifier."
sameAs(e1, e2)
Comment:

3782	"therefore, we propose to use cleannet to assign weights to image samples according to the <e1>image</e1>-to-label relevance to guide training of the <e2>image</e2> classifier."
sameAs(e1, e2)
Comment:

3783	"on the other hand, as a better <e1>classifier</e1> provides more discriminative convolutional image features for learning cleannet, we refresh the cleannet using the newly trained <e2>classifier</e2>."
sameAs(e1, e2)
Comment:

3784	"we carried out comprehensive experimentation to evaluate our method for label <e1>noise</e1> detection and image classification on three large datasets with real-world label <e2>noise</e2>: clothing1m [35] , webvision [13] , and food-101n."
sameAs(e1, e2)
Comment:

3785	"food-101n contains 310k images <e1>we</e1> collected from internet with the food-101 taxonomy [2] , and <e2>we</e2> added "verification label" that verifies whether a noisy class label is correct for an image 1 ."
sameAs(e1, e2)
Comment:

3786	"it also achieves 47% of the performance gain of verifying all <e1>images</e1> with only 3.2% <e2>images</e2> verified on an image classification task."
sameAs(e1, e2)
Comment:

3787	"yet many studies have shown that label <e1>noise</e1> can affect accuracy of the induced classifiers significantly [7, 19, 22, 27] , making it desirable to develop algorithms for learning in presence of label <e2>noise</e2>."
sameAs(e1, e2)
Comment:

3788	"yet many studies have shown that label noise can affect <e1>accuracy</e1> of the induced classifiers significantly [7, 19, 22, 27] , making it desirable to develop <e2>algorithms</e2> for learning in presence of label noise."
Evaluate-for(e1, e2)
Comment:

3789	"learning with label noise can be categorized by type of <e1>supervision</e1>: methods that rely on human <e2>supervision</e2> and methods that do not."
sameAs(e1, e2)
Comment:

3790	"learning with label noise can be categorized by type of supervision: <e1>methods</e1> that rely on human supervision and <e2>methods</e2> that do not."
sameAs(e1, e2)
Comment:

3791	"learning with label noise can be categorized by type of supervision: methods <e1>that</e1> rely on human supervision and methods <e2>that</e2> do not."
sameAs(e1, e2)
Comment:

3792	"under our scheme, a taxonomy is built with the hypernymhyponym relationships between known <e1>classes</e1> such that objects from novel <e2>classes</e2> are expected to be classified into the most relevant label, i.e., the closest class in the taxonomy."
sameAs(e1, e2)
Comment:

3793	"in contrast to standard object recognition tasks with a closed set of <e1>classes</e1>, our proposed framework can be useful for extending the domain of <e2>classes</e2> to an open set with taxonomy information (i.e., dealing with any objects unseen in training)."
sameAs(e1, e2)
Comment:

3794	"in practical application scenarios, our framework can be potentially useful for automatically or interactively organizing a customized taxonomy (e.g., company's prod-uct catalog, wildlife monitoring, personal photo library) by suggesting closest <e1>categories</e1> for an image from novel <e2>categories</e2> (e.g., new consumer products, unregistered animal species, untagged scenes or places)."
sameAs(e1, e2)
Comment:

3795	"although the flatten <e1>method</e1> simplifies the full hierarchical structure, it outperforms the top-down <e2>method</e2> for datasets of a large hierarchical depth."
sameAs(e1, e2)
Comment:

3796	"furthermore, we combine two <e1>methods</e1> for utilizing their complementary benefits: top-down <e2>methods</e2> naturally leverage the hierarchical structure information, but the classification performance might be degraded due to the error aggregation."
sameAs(e1, e2)
Comment:

3797	"our <e1>method</e1> can also be useful for generalized zero-shot learning (gzsl) [4, 33] <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

3798	"gzsl is a classification task with <e1>classes</e1> both seen and unseen during training, given that semantic side-information for all test <e2>classes</e2> is provided."
sameAs(e1, e2)
Comment:

3799	"we show <e1>that</e1> our method can generate a hierarchical embedding <e2>that</e2> leads to improved gzsl performance in combination with other commonly-used semantic embeddings."
sameAs(e1, e2)
Comment:

3800	"for example, suppose <e1>one</e1> trains a <e2>classifier</e2> on an animal image dataset as in figure 1 ."
isA(e1, e2)
Comment:

3801	"however, despite the improved <e1>results</e1>, these <e2>methods</e2> use cnns to generate a new set of "low-level" features and fall short of exploiting the end-to-end learning ability of cnns."
Compare(e1, e2)
Comment:

3802	"however, despite the improved results, these methods use <e1>cnns</e1> to generate a new set of "low-level" features and fall short of exploiting the end-to-end learning ability of <e2>cnns</e2>."
sameAs(e1, e2)
Comment:

3803	"the field of room layout estimation has been primarily focused on using bottom-up image features such as local <e1>color</e1>, <e2>texture</e2>, and edge cues followed by vanishing point detection."
Conjunction(e1, e2)
Comment:

3804	"a separate post-processing stage is used to clean up feature outliers and generate/rank a large set of room layout hypotheses with structured svms or <e1>conditional random fields</e1> (<e2>crfs</e2>) [15, 11, 16, 36, 49] ."
sameAs(e1, e2)
Comment:

3805	"abstract we apply basic statistical reasoning to signal reconstruction by machine learning -learning to map corrupted observations to clean signals -with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean <e1>data</e1>, without explicit image priors or likelihood <e2>models</e2> of the corruption."
Used-for(e1, e2)
Comment:

3806	"this happens by training a regression model, e.g., a convolutional neural network (cnn), with a large number of pairs (x i , y i ) of corrupted inputsx i and clean <e1>targets</e1> y i and minimizing the empirical risk argmin θ i l (f θ (x i ), y i ) , (1) where f θ is a parametric family of mappings (e.g., cnns), under the loss function l. we use the notationx to underline the fact that the corrupted inputx ∼ p(x|y i ) is a random variable distributed according to the clean <e2>target</e2>."
sameAs(e1, e2)
Comment:

3807	"priors are typically over-simplified <e1>models</e1> of the actual geometry behavior, thus the prior-based techniques tend to work well for specific class of <e2>models</e2> rather than being general."
sameAs(e1, e2)
Comment:

3808	"to implicitly <e1>model</e1> and characterize the geometry behavior, one common way is to take a data-driven approach and <e2>model</e2> the complex behavior using explicit examples."
sameAs(e1, e2)
Comment:

3809	"more importantly, our method is <e1>edge</e1>-aware, in the sense that the network learns the geometry of edges from the training set, and during the test time, it identifies <e2>edge</e2> points and generates more points along the edges (and over the surface) to facilitate a 3d reconstruction that preserves sharp features."
sameAs(e1, e2)
Comment:

3810	"more importantly, our method is edge-aware, in the sense <e1>that</e1> the network learns the geometry of edges from the training set, and during the test time, it identifies edge points and generates more points along the edges (and over the surface) to facilitate a 3d reconstruction <e2>that</e2> preserves sharp features."
sameAs(e1, e2)
Comment:

3811	"more importantly, our method is edge-aware, in the sense that the network learns the geometry of <e1>edges</e1> from the training set, and during the test time, it identifies edge points and generates more points along the <e2>edges</e2> (and over the surface) to facilitate a 3d reconstruction that preserves sharp features."
sameAs(e1, e2)
Comment:

3812	"to do so, <e1>we</e1> develop a patch extraction scheme that solely works on points, so that <e2>we</e2> can extract patches of points for use consistently in both training and testing phases."
sameAs(e1, e2)
Comment:

3813	"to do so, we develop a patch extraction scheme <e1>that</e1> solely works on points, so <e2>that</e2> we can extract patches of points for use consistently in both training and testing phases."
sameAs(e1, e2)
Comment:

3814	"in addition, to train the <e1>network</e1> to be edge-aware, we associate edge and mesh triangle information with the training patches, and train the <e2>network</e2> to learn features from the patches by regressing point-to-edge distances and then the point coordinates."
sameAs(e1, e2)
Comment:

3815	"in addition, to train the network to be <e1>edge</e1>-aware, we associate <e2>edge</e2> and mesh triangle information with the training patches, and train the network to learn features from the patches by regressing point-to-edge distances and then the point coordinates."
sameAs(e1, e2)
Comment:

3816	"in addition, to train the network to be <e1>edge</e1>-aware, we associate edge and mesh triangle information with the training patches, and train the network to learn features from the patches by regressing point-to-<e2>edge</e2> distances and then the point coordinates."
sameAs(e1, e2)
Comment:

3817	"in addition, to train the network to be edge-aware, we associate <e1>edge</e1> and mesh triangle information with the training patches, and train the network to learn features from the patches by regressing point-to-<e2>edge</e2> distances and then the point coordinates."
sameAs(e1, e2)
Comment:

3818	"in addition, to train the network to be edge-aware, we associate edge and mesh triangle information with the training <e1>patches</e1>, and train the network to learn features from the <e2>patches</e2> by regressing point-to-edge distances and then the point coordinates."
sameAs(e1, e2)
Comment:

3819	"our loss function encourages the output points to be located close to the underlying <e1>surface</e1> and to the edges, as well as distributed more evenly on <e2>surface</e2>."
sameAs(e1, e2)
Comment:

3820	"since it is difficult to annotate edges directly in real scanned <e1>point clouds</e1>, we train our network on synthesized virtual scanned <e2>point clouds</e2>, and show the performance of our method on both real and virtual scanned point clouds."
sameAs(e1, e2)
Comment:

3821	"since it is difficult to annotate edges directly in real scanned <e1>point clouds</e1>, we train our network on synthesized virtual scanned point clouds, and show the performance of our method on both real and virtual scanned <e2>point clouds</e2>."
sameAs(e1, e2)
Comment:

3822	"since it is difficult to annotate edges directly in real scanned point clouds, we train our network on synthesized virtual scanned <e1>point clouds</e1>, and show the performance of our method on both real and virtual scanned <e2>point clouds</e2>."
sameAs(e1, e2)
Comment:

3823	"by using our trained network, <e1>we</e1> show through various experiments that <e2>we</e2> can improve not only the point cloud consolidation results (see figures 1(b) & (c) ), but also the surface reconstruction quality, compared to various state-of-the-art methods."
sameAs(e1, e2)
Comment:

3824	"by using our trained network, we show through various experiments that we can improve not only the point cloud consolidation <e1>results</e1> (see figures 1(b) & (c) ), but also the surface reconstruction quality, compared to various state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

3825	"to consolidate a point set in an edge-aware manner, some methods detected/sensed the sharp <e1>edges</e1> and arranged points deliberatively along <e2>edges</e2> to preserve their sharpness [16] [17] [18] [19] ."
sameAs(e1, e2)
Comment:

3826	"motivated by the promising results that deep learning methods have achieved for image and video <e1>problems</e1>, there has been increasing effort to leverage neural networks for geometry and 3d shape <e2>problems</e2>."
sameAs(e1, e2)
Comment:

3827	"recently, <e1>point clouds</e1> have drawn more attention, and there are some works to utilize neural networks to directly process <e2>point clouds</e2>."
sameAs(e1, e2)
Comment:

3828	"to achieve this, we formulate a regression component to simultaneously recover 3d point coordinates and pointto-<e1>edge</e1> distances from upsampled features, and an <e2>edge</e2>-aware joint loss function to directly minimize distances from output points to 3d meshes and to edges."
sameAs(e1, e2)
Comment:

3829	"to achieve this, we formulate a regression component to simultaneously recover 3d point coordinates and pointto-edge <e1>distances</e1> from upsampled features, and an edge-aware joint loss function to directly minimize <e2>distances</e2> from output points to 3d meshes and to edges."
sameAs(e1, e2)
Comment:

3830	"for each point in an input patch, we first encode its local geometry into a <e1>feature</e1> vector f (size: n × d) using pointnet++, and expand f into f ′ (size: rn × d 2 ) using a <e2>feature</e2> expansion mechanism."
sameAs(e1, e2)
Comment:

3831	"however, our method is <e1>edge</e1>-aware, and we extract local patches and train the network to learn edges in patches with a novel <e2>edge</e2>-aware joint loss function."
sameAs(e1, e2)
Comment:

3832	"however, our method is edge-aware, and we extract local <e1>patches</e1> and train the network to learn edges in <e2>patches</e2> with a novel edge-aware joint loss function."
sameAs(e1, e2)
Comment:

3833	"also, we trained our network on virtual scanned <e1>point clouds</e1>, demonstrated the performance of our method on both synthetic and real <e2>point clouds</e2>, presented various surface reconstruction results, and showed how our method outperforms the state-of-the-arts."
sameAs(e1, e2)
Comment:

3834	"also, we trained our network on virtual scanned point clouds, demonstrated the performance of <e1>our method</e1> on both synthetic and real point clouds, presented various surface reconstruction results, and showed how <e2>our method</e2> outperforms the state-of-the-arts."
sameAs(e1, e2)
Comment:

3835	"introduction point cloud consolidation is a process of "massaging" a point set into a <e1>surface</e1> [1] , for enhancing the <e2>surface</e2> reconstruction quality."
sameAs(e1, e2)
Comment:

3836	"alpha-<e1>divergences</e1> are alternative <e2>divergences</e2> to vi's kl objective, which are able to avoid vi's uncertainty underestimation."
sameAs(e1, e2)
Comment:

3837	"we propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference <e1>technique</e1> which, together with dropout, can be easily implemented with existing <e2>models</e2> by simply changing the loss of the model."
Compare(e1, e2)
Comment:

3838	"we study our <e1>model</e1>'s epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our <e2>model</e2>'s uncertainty."
sameAs(e1, e2)
Comment:

3839	"we study our model's epistemic <e1>uncertainty</e1> far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's <e2>uncertainty</e2>."
sameAs(e1, e2)
Comment:

3840	"we study our model's epistemic uncertainty far away from the data using <e1>adversarial</e1> images, showing that these can be distinguished from non-<e2>adversarial</e2> images by examining our model's uncertainty."
sameAs(e1, e2)
Comment:

3841	"we study our model's epistemic uncertainty far away from the data using adversarial <e1>images</e1>, showing that these can be distinguished from non-adversarial <e2>images</e2> by examining our model's uncertainty."
sameAs(e1, e2)
Comment:

3842	"but unlike deep learning models, bayesian probabilistic models can capture parameter <e1>uncertainty</e1> and its induced effects over predictions, capturing the models' ignorance about the world, and able to convey their increased <e2>uncertainty</e2> on out-of-data examples."
sameAs(e1, e2)
Comment:

3843	"when using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the <e1>target</e1> task, but would be difficult to extract from the limited amount of data available on the <e2>target</e2> task."
sameAs(e1, e2)
Comment:

3844	"when using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the target <e1>task</e1>, but would be difficult to extract from the limited amount of <e2>data</e2> available on the target task."
Conjunction(e1, e2)
Comment:

3845	"when using finetuning, the underlying assumption is that the pretrained model extracts generic features, which are at least partially relevant for solving the target <e1>task</e1>, but would be difficult to extract from the limited amount of data available on the target <e2>task</e2>."
sameAs(e1, e2)
Comment:

3846	"we show the benefit of having an explicit inductive bias towards the initial <e1>model</e1>, and we eventually recommend a simple l 2 penalty with the pre-trained <e2>model</e2> being a reference as the baseline of penalty for transfer learning tasks."
sameAs(e1, e2)
Comment:

3847	"we show the benefit of having an explicit inductive bias towards the initial model, and we eventually recommend a simple l 2 <e1>penalty</e1> with the pre-trained model being a reference as the baseline of <e2>penalty</e2> for transfer learning tasks."
sameAs(e1, e2)
Comment:

3848	"according to (hochreiter & schmidhuber, 1997a; haussler et al, 1997; keskar et al, 2016; chaudhari et al, 2016) , a <e1>model</e1> lying in a flat region of the loss surface is likely to generalize well, since any small perturbation to the <e2>model</e2> makes little fluctuation to the loss."
sameAs(e1, e2)
Comment:

3849	"training lstm towards binary-valued gates means seeking a set of parameters to make the values of the gates approaching zero or one, namely residing in the flat <e1>region</e1> of the sigmoid function, which corresponds to the flat <e2>region</e2> of the overall loss surface."
sameAs(e1, e2)
Comment:

3850	"it aims to use gates to control <e1>information</e1> flow (e.g., whether to skip some <e2>information</e2> or not) in the recurrent computations, although its practical implementation based on soft gates only partially achieves this goal."
sameAs(e1, e2)
Comment:

3851	"however, <e1>this</e1> is equivalent to rescaling the input and cannot guarantee the values of the learned gates to be close to 0 or 1. to tackle <e2>this</e2> challenge, in this paper, we leverage the gumbel-softmax estimator developed for variational methods (jang et al, 2016; maddison et al, 2016) ."
sameAs(e1, e2)
Comment:

3852	"however, <e1>this</e1> is equivalent to rescaling the input and cannot guarantee the values of the learned gates to be close to 0 or 1. to tackle this challenge, in <e2>this</e2> paper, we leverage the gumbel-softmax estimator developed for variational methods (jang et al, 2016; maddison et al, 2016) ."
sameAs(e1, e2)
Comment:

3853	"however, this is equivalent to rescaling the input and cannot guarantee the values of the learned gates to be close to 0 or 1. to tackle <e1>this</e1> challenge, in <e2>this</e2> paper, we leverage the gumbel-softmax estimator developed for variational methods (jang et al, 2016; maddison et al, 2016) ."
sameAs(e1, e2)
Comment:

3854	"specifically, during training, we apply the gumbel-softmax estimator to the gates to approximate the values sampled from the bernoulli distribution given by the parameters, and train the lstm <e1>model</e1> with standard backpropagation <e2>methods</e2>."
Compare(e1, e2)
Comment:

3855	"we call the learned model gumbel-gate <e1>lstm</e1> (g 2 -<e2>lstm</e2>)."
sameAs(e1, e2)
Comment:

3856	"furthermore, our <e1>model</e1> achieves better or comparable results compared to the baseline <e2>model</e2>."
sameAs(e1, e2)
Comment:

3857	"in this paper, <e1>we</e1> propose a new way for lstm training, which pushes the output values of the gates towards 0 or 1. by doing so, <e2>we</e2> can better control the information flow: the gates are mostly open or closed, instead of in a middle state, which makes the results more interpretable."
sameAs(e1, e2)
Comment:

3858	"we apply several <e1>model</e1> compression algorithms to the parameters in the gates, including low-precision approximation and low-rank approximation, and results show that our compressed <e2>model</e2> can be even better than the baseline model without compression."
sameAs(e1, e2)
Comment:

3859	"we apply several <e1>model</e1> compression algorithms to the parameters in the gates, including low-precision approximation and low-rank approximation, and results show that our compressed model can be even better than the baseline <e2>model</e2> without compression."
sameAs(e1, e2)
Comment:

3860	"we apply several model <e1>compression</e1> algorithms to the parameters in the gates, including low-precision approximation and low-rank approximation, and results show that our compressed model can be even better than the baseline model without <e2>compression</e2>."
sameAs(e1, e2)
Comment:

3861	"we apply several model compression algorithms to the parameters in the gates, including low-precision <e1>approximation</e1> and low-rank <e2>approximation</e2>, and results show that our compressed model can be even better than the baseline model without compression."
sameAs(e1, e2)
Comment:

3862	"we apply several model compression algorithms to the parameters in the gates, including low-precision approximation and low-rank approximation, and results show that our compressed <e1>model</e1> can be even better than the baseline <e2>model</e2> without compression."
sameAs(e1, e2)
Comment:

3863	"empirical studies show <e1>that</e1> (1) although it seems <e2>that</e2> we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) the outputs of gates are not sensitive to their inputs: we can easily compress the lstm unit in multiple ways, e.g., low-rank approximation and low-precision approximation."
sameAs(e1, e2)
Comment:

3864	"empirical studies show that (1) although it seems that <e1>we</e1> restrict the model capacity, there is no performance drop: <e2>we</e2> achieve better or comparable performances due to its better generalization ability; (2) the outputs of gates are not sensitive to their inputs: we can easily compress the lstm unit in multiple ways, e.g., low-rank approximation and low-precision approximation."
sameAs(e1, e2)
Comment:

3865	"empirical studies show that (1) although it seems that <e1>we</e1> restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) the outputs of gates are not sensitive to their inputs: <e2>we</e2> can easily compress the lstm unit in multiple ways, e.g., low-rank approximation and low-precision approximation."
sameAs(e1, e2)
Comment:

3866	"empirical studies show that (1) although it seems that we restrict the model capacity, there is no performance drop: <e1>we</e1> achieve better or comparable performances due to its better generalization ability; (2) the outputs of gates are not sensitive to their inputs: <e2>we</e2> can easily compress the lstm unit in multiple ways, e.g., low-rank approximation and low-precision approximation."
sameAs(e1, e2)
Comment:

3867	"empirical studies show that (1) although it seems that we restrict the model capacity, there is no performance drop: we achieve better or comparable performances due to its better generalization ability; (2) the outputs of gates are not sensitive to their inputs: we can easily compress the lstm unit in multiple ways, e.g., low-rank <e1>approximation</e1> and low-precision <e2>approximation</e2>."
sameAs(e1, e2)
Comment:

3868	"the compressed <e1>models</e1> are even better than the baseline <e2>models</e2> without compression."
sameAs(e1, e2)
Comment:

3869	"introduction <e1>recurrent neural networks</e1> (<e2>rnns</e2>) (hochreiter, 1998) are widely used in sequence modeling tasks, such as language modeling (kim et al, 2016; jozefowicz et al, 2016) , speech recognition (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , machine translation britz et al, 2017; he et al, 2016) , image captioning (vinyals et al, 2015; xu et al, 2015) , and image generation (villegas et al, 2017) ."
sameAs(e1, e2)
Comment:

3870	"introduction <e1>recurrent neural networks</e1> (rnns) (hochreiter, 1998) are widely used in sequence <e2>modeling</e2> tasks, such as language modeling (kim et al, 2016; jozefowicz et al, 2016) , speech recognition (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , machine translation britz et al, 2017; he et al, 2016) , image captioning (vinyals et al, 2015; xu et al, 2015) , and image generation (villegas et al, 2017) ."
Used-for(e1, e2)
Comment:

3871	"introduction recurrent neural networks (<e1>rnns</e1>) (hochreiter, 1998) are widely used in sequence <e2>modeling</e2> tasks, such as language modeling (kim et al, 2016; jozefowicz et al, 2016) , speech recognition (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , machine translation britz et al, 2017; he et al, 2016) , image captioning (vinyals et al, 2015; xu et al, 2015) , and image generation (villegas et al, 2017) ."
Used-for(e1, e2)
Comment:

3872	"introduction recurrent neural networks (rnns) (hochreiter, 1998) are widely used in sequence modeling tasks, such as <e1>language modeling</e1> (kim et al, 2016; jozefowicz et al, 2016) , <e2>speech recognition</e2> (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , machine translation britz et al, 2017; he et al, 2016) , image captioning (vinyals et al, 2015; xu et al, 2015) , and image generation (villegas et al, 2017) ."
isA(e1, e2)
Comment:

3873	"introduction recurrent neural networks (rnns) (hochreiter, 1998) are widely used in sequence modeling tasks, such as language modeling (kim et al, 2016; jozefowicz et al, 2016) , <e1>speech recognition</e1> (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , <e2>machine translation</e2> britz et al, 2017; he et al, 2016) , image captioning (vinyals et al, 2015; xu et al, 2015) , and image generation (villegas et al, 2017) ."
Conjunction(e1, e2)
Comment:

3874	"introduction recurrent neural networks (rnns) (hochreiter, 1998) are widely used in sequence modeling tasks, such as language modeling (kim et al, 2016; jozefowicz et al, 2016) , speech recognition (zhang et al, 2016) , time series prediction (xingjian et al, 2015) , machine translation britz et al, 2017; he et al, 2016) , <e1>image</e1> captioning (vinyals et al, 2015; xu et al, 2015) , and <e2>image</e2> generation (villegas et al, 2017) ."
sameAs(e1, e2)
Comment:

3875	"to address the long-term dependency and gradient vanish- ing problem of conventional rnns, long short-term memory (lstm) (gers et al, 1999; hochreiter & schmidhuber, 1997b) networks were proposed, which introduce gate functions to control the <e1>information</e1> flow in a recurrent unit: a forget gate function to determine how much previous <e2>information</e2> should be excluded for the current step, an input gate function to find relevant signals to be absorbed into the hidden context, and an output gate function for prediction and decision making."
sameAs(e1, e2)
Comment:

3876	"to address the long-term dependency and gradient vanish- ing problem of conventional rnns, long short-term memory (lstm) (gers et al, 1999; hochreiter & schmidhuber, 1997b) networks were proposed, which introduce gate functions to control the information flow in a recurrent unit: a forget gate <e1>function</e1> to determine how much previous information should be excluded for the current step, an input gate <e2>function</e2> to find relevant signals to be absorbed into the hidden context, and an output gate function for prediction and decision making."
sameAs(e1, e2)
Comment:

3877	"to address the long-term dependency and gradient vanish- ing problem of conventional rnns, long short-term memory (lstm) (gers et al, 1999; hochreiter & schmidhuber, 1997b) networks were proposed, which introduce gate functions to control the information flow in a recurrent unit: a forget gate <e1>function</e1> to determine how much previous information should be excluded for the current step, an input gate function to find relevant signals to be absorbed into the hidden context, and an output gate <e2>function</e2> for prediction and decision making."
sameAs(e1, e2)
Comment:

3878	"to address the long-term dependency and gradient vanish- ing problem of conventional rnns, long short-term memory (lstm) (gers et al, 1999; hochreiter & schmidhuber, 1997b) networks were proposed, which introduce gate functions to control the information flow in a recurrent unit: a forget gate function to determine how much previous information should be excluded for the current step, an input gate <e1>function</e1> to find relevant signals to be absorbed into the hidden context, and an output gate <e2>function</e2> for prediction and decision making."
sameAs(e1, e2)
Comment:

3879	"however, when looking deep into the unit, we empirically find <e1>that</e1> the values of the gates are not <e2>that</e2> meaningful as the design logic."
sameAs(e1, e2)
Comment:

3880	"in this work, we consider two types of neural networks that use spatial pyramid <e1>pooling</e1> module [18, 19, 20] or encoder-decoder structure [21, 22] for semantic segmentation, where the former one captures rich contextual information by <e2>pooling</e2> features at different resolution while the latter one is able to obtain sharp object boundaries."
sameAs(e1, e2)
Comment:

3881	"in this work, we consider two types of neural networks that use spatial pyramid pooling module [18, 19, 20] or encoder-decoder structure [21, 22] for semantic segmentation, where the <e1>former</e1> one captures rich contextual information by pooling features at different resolution while the <e2>latter</e2> one is able to obtain sharp object boundaries."
Conjunction(e1, e2)
Comment:

3882	"in this work, we consider two types of neural networks that use spatial pyramid pooling module [18, 19, 20] or encoder-decoder structure [21, 22] for semantic segmentation, where the former <e1>one</e1> captures rich contextual information by pooling features at different resolution while the latter <e2>one</e2> is able to obtain sharp object boundaries."
sameAs(e1, e2)
Comment:

3883	"the proposed model, deeplabv3+, contains rich semantic information from the encoder <e1>module</e1>, while the detailed object boundaries are recovered by the simple yet effective decoder <e2>module</e2>."
sameAs(e1, e2)
Comment:

3884	"spatial pyramid <e1>pooling</e1>, or aspp), while pspnet [24] performs <e2>pooling</e2> operations at different grid scales."
sameAs(e1, e2)
Comment:

3885	"taking resnet-101 [25] for example, when applying atrous convolution to extract output <e1>features</e1> that are 16 times smaller than input resolution, <e2>features</e2> within the last 3 residual blocks (9 layers) have to be dilated."
sameAs(e1, e2)
Comment:

3886	"motivated by the recent success of depthwise separable <e1>convolution</e1> [27, 28, 26, 29, 30] , we also explore this operation and show improvement in terms of both speed and accuracy by adapting the xception model [26] , similar to [31] , for the task of semantic segmentation, and applying the atrous separable <e2>convolution</e2> to both the aspp and decoder modules."
sameAs(e1, e2)
Comment:

3887	"the <e1>former</e1> networks are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the <e2>latter</e2> networks can capture sharper object boundaries by gradually recovering the spatial information."
Conjunction(e1, e2)
Comment:

3888	"the former <e1>networks</e1> are able to encode multi-scale contextual information by probing the incoming features with filters or pooling operations at multiple rates and multiple effective fields-of-view, while the latter <e2>networks</e2> can capture sharper object boundaries by gradually recovering the spatial information."
sameAs(e1, e2)
Comment:

3889	"finally, we demonstrate the effectiveness of the proposed <e1>model</e1> on pascal voc 2012 and cityscapes datasts and attain the test set performance of 89.0% and 82.1% without any <e2>post-processing</e2>, setting a new state-of-the-art."
Used-for(e1, e2)
Comment:

3890	"in summary, our contributions are: -we propose a novel encoder-decoder structure which employs deeplabv3 as a powerful encoder <e1>module</e1> and a simple yet effective decoder <e2>module</e2>."
sameAs(e1, e2)
Comment:

3891	"-we adapt the xception model for the segmentation task and apply depthwise separable convolution to both aspp <e1>module</e1> and decoder <e2>module</e2>, resulting in a faster and stronger encoder-decoder network."
sameAs(e1, e2)
Comment:

3892	"-we make our <e1>tensorflow</e1>-based implementation of the proposed model publicly available at https://github.com/<e2>tensorflow</e2>/models/tree/master/ research/deeplab."
sameAs(e1, e2)
Comment:

3893	"we demonstrate the effectiveness of the proposed <e1>model</e1> on pascal voc 2012 and cityscapes datasets, achieving the test set performance of 89% and 82.1% without any <e2>post-processing</e2>."
Used-for(e1, e2)
Comment:

3894	"our paper is accompanied with a publicly available reference implementation of the proposed <e1>models</e1> in tensorflow at https: //github.com/tensorflow/<e2>models</e2>/tree/master/research/deeplab."
sameAs(e1, e2)
Comment:

3895	"our paper is accompanied with a publicly available reference implementation of the proposed models in <e1>tensorflow</e1> at https: //github.com/<e2>tensorflow</e2>/models/tree/master/research/deeplab."
sameAs(e1, e2)
Comment:

3896	"these results demonstrate the advantage of the recurrent <e1>structure</e1> over purely feed-forward <e2>structure</e2> for object recognition."
sameAs(e1, e2)
Comment:

3897	"a feed-forward model can only capture the context (e.g., the face in figure 1 ) in higher <e1>layers</e1> where units have larger rfs, but this information cannot modulate the activities of units in lower <e2>layers</e2> responsible for recognizing smaller ob- figure 1 ."
sameAs(e1, e2)
Comment:

3898	"though the input is static, the <e1>activities</e1> of rcnn units evolve over time so that the activity of each unit is modulated by the <e2>activities</e2> of its neighboring units."
sameAs(e1, e2)
Comment:

3899	"abstract existing learning based solutions to 3d <e1>surface</e1> prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., tsdf) from which 3d <e2>surface</e2> meshes must be extracted in a post-processing step (e.g., via the marching cubes algorithm)."
sameAs(e1, e2)
Comment:

3900	"abstract existing learning based solutions to 3d <e1>surface</e1> prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., tsdf) from which 3d surface meshes must be extracted in a post-processing step (e.g., via the marching cubes <e2>algorithm</e2>)."
Used-for(e1, e2)
Comment:

3901	"abstract existing learning based solutions to 3d surface prediction cannot be trained end-to-end as they operate on intermediate representations (e.g., tsdf) from which 3d <e1>surface</e1> meshes must be extracted in a post-processing step (e.g., via the marching cubes <e2>algorithm</e2>)."
Used-for(e1, e2)
Comment:

3902	"unfortunately, the expressiveness of such prior <e1>models</e1> is limited to simple local smoothness assumptions [2, 17, 21, 47] or very specialized shape <e2>models</e2> [1, 15, 16, 42] ."
sameAs(e1, e2)
Comment:

3903	"hence, existing 3d reconstruction systems work either in narrow domains where specialized shape knowledge is available, or figure 1 : illustration comparing point <e1>prediction</e1> (a), implicit surface <e2>prediction</e2> (b) and explicit surface prediction (c)."
sameAs(e1, e2)
Comment:

3904	"hence, existing 3d reconstruction systems work either in narrow domains where specialized shape knowledge is available, or figure 1 : illustration comparing point <e1>prediction</e1> (a), implicit surface prediction (b) and explicit surface <e2>prediction</e2> (c)."
sameAs(e1, e2)
Comment:

3905	"hence, existing 3d reconstruction systems work either in narrow domains where specialized shape knowledge is available, or figure 1 : illustration comparing point prediction (a), implicit <e1>surface</e1> prediction (b) and explicit <e2>surface</e2> prediction (c)."
sameAs(e1, e2)
Comment:

3906	"hence, existing 3d reconstruction systems work either in narrow domains where specialized shape knowledge is available, or figure 1 : illustration comparing point prediction (a), implicit surface <e1>prediction</e1> (b) and explicit surface <e2>prediction</e2> (c)."
sameAs(e1, e2)
Comment:

3907	"and indeed, recent advances in <e1>this</e1> area [7, 12, 18, 24, 34, 36, 39, 40] suggest that <e2>this</e2> goal can ultimately be achieved."
sameAs(e1, e2)
Comment:

3908	"existing 3d representation learning approaches can be classified into two categories: voxel based <e1>methods</e1> and point based <e2>methods</e2>, see fig."
sameAs(e1, e2)
Comment:

3909	"as both techniques cannot be trained end-to-end for the 3d surface prediction task, an auxiliary loss (e.g., chamfer <e1>distance</e1> on point sets, ℓ 1 loss on signed <e2>distance</e2> field) must be used during training."
sameAs(e1, e2)
Comment:

3910	"our model is flexible and can be combined with a variety of <e1>shape</e1> encoder and <e2>shape</e2> inference techniques."
sameAs(e1, e2)
Comment:

3911	"conditional <e1>image</e1>-to-<e2>image</e2> translation"
sameAs(e1, e2)
Comment:

3912	" introduction<e1> imag</e1>e-to<e2>-imag</e2>e translation covers a large variety of computer vision problems, including image stylization [4] , segmentation [13] and saliency detection [5] ."
sameAs(e1, e2)
Comment:

3913	" introduction<e1> imag</e1>e-to-image translation covers a large variety of computer vision problems, including<e2> imag</e2>e stylization [4] , segmentation [13] and saliency detection [5] ."
sameAs(e1, e2)
Comment:

3914	" introduction image-to<e1>-imag</e1>e translation covers a large variety of computer vision problems, including<e2> imag</e2>e stylization [4] , segmentation [13] and saliency detection [5] ."
sameAs(e1, e2)
Comment:

3915	"to fulfill such a blank in <e1>image</e1> translation, we propose the concept of conditional <e2>image</e2>-to-image translation, which can specify domainspecific features in the target domain, carried by another input image from the target domain."
sameAs(e1, e2)
Comment:

3916	"to fulfill such a blank in <e1>image</e1> translation, we propose the concept of conditional image-to-<e2>image</e2> translation, which can specify domainspecific features in the target domain, carried by another input image from the target domain."
sameAs(e1, e2)
Comment:

3917	"to fulfill such a blank in <e1>image</e1> translation, we propose the concept of conditional image-to-image translation, which can specify domainspecific features in the target domain, carried by another input <e2>image</e2> from the target domain."
sameAs(e1, e2)
Comment:

3918	"to fulfill such a blank in image translation, we propose the concept of conditional <e1>image</e1>-to-<e2>image</e2> translation, which can specify domainspecific features in the target domain, carried by another input image from the target domain."
sameAs(e1, e2)
Comment:

3919	"to fulfill such a blank in image translation, we propose the concept of conditional <e1>image</e1>-to-image translation, which can specify domainspecific features in the target domain, carried by another input <e2>image</e2> from the target domain."
sameAs(e1, e2)
Comment:

3920	"to fulfill such a blank in image translation, we propose the concept of conditional image-to-<e1>image</e1> translation, which can specify domainspecific features in the target domain, carried by another input <e2>image</e2> from the target domain."
sameAs(e1, e2)
Comment:

3921	"to fulfill such a blank in image translation, we propose the concept of conditional image-to-image translation, which can specify domainspecific features in the <e1>target domain</e1>, carried by another input image from the <e2>target domain</e2>."
sameAs(e1, e2)
Comment:

3922	"an example of conditional <e1>image</e1>-to-<e2>image</e2> translation is shown in figure 1 , in which we want to convert hillary's photo to a man's photo."
sameAs(e1, e2)
Comment:

3923	"particularly, the generative adversarial networks (gan) [6] and dual learning [7, 21] are extensively studied in <e1>image</e1>-to-<e2>image</e2> translations."
sameAs(e1, e2)
Comment:

3924	"[22, 9, 25] tackle imageto-<e1>image</e1> translation by the aforementioned two techniques, where the gans are used to ensure the generated images belonging to the target domain, and dual learning can help improve <e2>image</e2> qualities by minimizing reconstruction loss."
sameAs(e1, e2)
Comment:

3925	"an implicit assumption of <e1>image</e1>-to-<e2>image</e2> translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3926	"an implicit assumption of <e1>image</e1>-to-image translation is that an <e2>image</e2> contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3927	"an implicit assumption of <e1>image</e1>-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face <e2>image</e2> translation)."
sameAs(e1, e2)
Comment:

3928	"an implicit assumption of image-to-<e1>image</e1> translation is that an <e2>image</e2> contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3929	"an implicit assumption of image-to-<e1>image</e1> translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face <e2>image</e2> translation)."
sameAs(e1, e2)
Comment:

3930	"an implicit assumption of image-to-image translation is that an <e1>image</e1> contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face <e2>image</e2> translation)."
sameAs(e1, e2)
Comment:

3931	"an implicit assumption of image-to-image translation is that an image contains two kinds of <e1>features</e1> 1 : domainindependent <e2>features</e2>, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3932	"an implicit assumption of image-to-image translation is that an image contains two kinds of <e1>features</e1> 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific <e2>features</e2>, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3933	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent <e1>features</e1>, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific <e2>features</e2>, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3934	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of <e1>face</e1>, eyes, nose and mouse while translating a man' <e2>face</e2> to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3935	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of <e1>face</e1>, eyes, nose and mouse while translating a man' face to a woman' <e2>face</e2>), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3936	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of <e1>face</e1>, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for <e2>face</e2> image translation)."
sameAs(e1, e2)
Comment:

3937	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' <e1>face</e1> to a woman' <e2>face</e2>), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation)."
sameAs(e1, e2)
Comment:

3938	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' <e1>face</e1> to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for <e2>face</e2> image translation)."
sameAs(e1, e2)
Comment:

3939	"an implicit assumption of image-to-image translation is that an image contains two kinds of features 1 : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' <e1>face</e1>), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for <e2>face</e2> image translation)."
sameAs(e1, e2)
Comment:

3940	"image-to-image translation aims at transferring images from the source domain to the target domain by preserving <e1>domain</e1>-independent features while replacing <e2>domain</e2>-specific features."
sameAs(e1, e2)
Comment:

3941	"image-to-image translation aims at transferring images from the source domain to the target domain by preserving domain-independent <e1>features</e1> while replacing domain-specific <e2>features</e2>."
sameAs(e1, e2)
Comment:

3942	"while it is not difficult for existing <e1>image</e1>-to-<e2>image</e2> translation methods to convert an image from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated image in the target domain."
sameAs(e1, e2)
Comment:

3943	"while it is not difficult for existing <e1>image</e1>-to-image translation methods to convert an <e2>image</e2> from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated image in the target domain."
sameAs(e1, e2)
Comment:

3944	"while it is not difficult for existing <e1>image</e1>-to-image translation methods to convert an image from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated <e2>image</e2> in the target domain."
sameAs(e1, e2)
Comment:

3945	"while it is not difficult for existing image-to-<e1>image</e1> translation methods to convert an <e2>image</e2> from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated image in the target domain."
sameAs(e1, e2)
Comment:

3946	"while it is not difficult for existing image-to-<e1>image</e1> translation methods to convert an image from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated <e2>image</e2> in the target domain."
sameAs(e1, e2)
Comment:

3947	"while it is not difficult for existing image-to-image translation methods to convert an <e1>image</e1> from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated <e2>image</e2> in the target domain."
sameAs(e1, e2)
Comment:

3948	"while it is not difficult for existing image-to-image translation methods to convert an image from a source domain to a <e1>target domain</e1>, it is not easy for them to control or manipulate the style in fine granularity of the generated image in the <e2>target domain</e2>."
sameAs(e1, e2)
Comment:

3949	"spatial transformer networks (stns) [7] are <e1>one</e1> of the first notable attempts to integrate low capacity and computationally efficient <e2>strategies</e2> for resolving -instead of tolerating -misalignment with classical cnns."
isA(e1, e2)
Comment:

3950	"jaderberg et al presented a novel strategy for integrating <e1>image</e1> warping within a neural network and showed that such operations are (sub-)differentiable, allowing for the application of canonical backpropagation to an <e2>image</e2> warping framework."
sameAs(e1, e2)
Comment:

3951	"the lk algorithm can be interpreted as a feed forward network of multiple <e1>alignment</e1> modules; specifically, each <e2>alignment</e2> module contains a low-capacity predictor (typically linear) for predicting geometric distortion from relative image appearance, followed by an image resampling/warp operation."
sameAs(e1, e2)
Comment:

3952	"the lk algorithm can be interpreted as a feed forward network of multiple alignment modules; specifically, each alignment module contains a low-capacity predictor (typically linear) for predicting geometric distortion from relative <e1>image</e1> appearance, followed by an <e2>image</e2> resampling/warp operation."
sameAs(e1, e2)
Comment:

3953	"the lk algorithm differs fundamentally, however, to stns in their application: <e1>image</e1>/object alignment instead of <e2>classification</e2>."
Used-for(e1, e2)
Comment:

3954	"in an stn with multiple feed-forward <e1>alignment</e1> modules, the output image of the previous <e2>alignment</e2> module is directly fed into the next."
sameAs(e1, e2)
Comment:

3955	"as we will demonstate in <e1>this</e1> paper, <e2>this</e2> is problematic as it can create unwanted boundary effects as the number of geometric prediction layers increase."
sameAs(e1, e2)
Comment:

3956	"the lk algorithm does not suffer from such problems; instead, it feeds the warp parameters through the <e1>network</e1> (instead of the warped image) such that each subsequent alignment module in the <e2>network</e2> resamples the original input source image."
sameAs(e1, e2)
Comment:

3957	"the lk algorithm does not suffer from such problems; instead, it feeds the warp parameters through the network (instead of the warped <e1>image</e1>) such that each subsequent alignment module in the network resamples the original input source <e2>image</e2>."
sameAs(e1, e2)
Comment:

3958	"furthermore, the inverse <e1>compositional</e1> (ic) variant of the lk algorithm [2] has demonstrated to achieve equivalently ef-fective alignment by reusing the same geometric predictor in a <e2>compositional</e2> update form."
sameAs(e1, e2)
Comment:

3959	"inspired by the ic-lk <e1>algorithm</e1>, we advocate an improved extension to the stn <e2>framework</e2> that (a) propagates warp parameters, rather than image intensities, through the network, and (b) employs the same geometric predictor that could be reapplied for all alignment modules."
Part-of(e1, e2)
Comment:

3960	"inspired by the ic-lk algorithm, we advocate an improved extension to the stn framework <e1>that</e1> (a) propagates warp parameters, rather than image intensities, through the network, and (b) employs the same geometric predictor <e2>that</e2> could be reapplied for all alignment modules."
sameAs(e1, e2)
Comment:

3961	"we propose inverse compositional spatial transformer networks (icstns) and show its superior performance over the original stns across a myriad of tasks, including pure <e1>image</e1> alignment and joint alignment/<e2>classification</e2> problems."
Used-for(e1, e2)
Comment:

3962	"we propose inverse compositional spatial transformer networks (icstns) and show its superior performance over the original stns across a myriad of tasks, including pure image <e1>alignment</e1> and joint <e2>alignment</e2>/classification problems."
sameAs(e1, e2)
Comment:

3963	"however, <e1>they</e1> have an intrinsic limited range of tolerance to geometric variation <e2>they</e2> can provide; furthermore, such pooling operations destroy spatial details within the images that could be crucial to the performance of subsequent tasks."
sameAs(e1, e2)
Comment:

3964	"instead of designing a <e1>network</e1> to solely give tolerance to spatial variation, another option is to have the <e2>network</e2> solve for some of the geometric misalignment in the input images [12, 6] ."
sameAs(e1, e2)
Comment:

3965	"another type of <e1>methods</e1> exploits features from interme-diate layers for generating high-resolution prediction, e.g., the fcn <e2>method</e2> in [36] and hypercolumns in [22] ."
Compare(e1, e2)
Comment:

3966	"the task here is to assign a unique label (or category) to every single pixel in the <e1>image</e1>, which can be considered as a dense <e2>classification</e2> problem."
Used-for(e1, e2)
Comment:

3967	"this information is thought to be complementary to the <e1>features</e1> from early convolution layers which encode low-level spatial visual information like edges, corners, circles, etc., and also complementary to high-level <e2>features</e2> from deeper layers which encode high-level semantic information, including object-or category-level evidence, but which lack strong spatial information."
sameAs(e1, e2)
Comment:

3968	"this information is thought to be complementary to the features from early convolution <e1>layers</e1> which encode low-level spatial visual information like edges, corners, circles, etc., and also complementary to high-level features from deeper <e2>layers</e2> which encode high-level semantic information, including object-or category-level evidence, but which lack strong spatial information."
sameAs(e1, e2)
Comment:

3969	"refinenet refines low-<e1>resolution</e1> (coarse) semantic features with fine-grained low-level features in a recursive manner to generate high-<e2>resolution</e2> semantic feature maps."
sameAs(e1, e2)
Comment:

3970	"3. <e1>we</e1> propose a new network component <e2>we</e2> call chained residual pooling which is able to capture background context from a large image region."
sameAs(e1, e2)
Comment:

3971	"however, <e1>these</e1> approaches exhibit clear limitations when it comes to dense prediction in <e2>tasks</e2> like dense depth or normal estimation [13, 33, 34] and semantic segmentation [36, 5] ."
Used-for(e1, e2)
Comment:

3972	"multiple stages of spatial pooling and convolution strides reduce the final <e1>image</e1> prediction typically by a factor of 32 in each dimension, thereby losing much of the finer <e2>image</e2> structure."
sameAs(e1, e2)
Comment:

3973	"st-<e1>gan</e1>: spatial transformer <e2>generative adversarial networks</e2> for image compositing"
sameAs(e1, e2)
Comment:

3974	"as the space of all images is very high-dimensional and <e1>image</e1> generation methods are limited by finite network capacity, direct <e2>image</e2> generation methods currently work well only on restricted domains (e.g."
sameAs(e1, e2)
Comment:

3975	"as the space of all images is very high-dimensional and image <e1>generation</e1> methods are limited by finite network capacity, direct image <e2>generation</e2> methods currently work well only on restricted domains (e.g."
sameAs(e1, e2)
Comment:

3976	"as the space of all images is very high-dimensional and image generation <e1>methods</e1> are limited by finite network capacity, direct image generation <e2>methods</e2> currently work well only on restricted domains (e.g."
sameAs(e1, e2)
Comment:

3977	"in this work, we leverage spatial transformer networks (stns) [11] , a special type of cnns capable of performing geometric transformations on <e1>images</e1>, to provide a simpler way to generate realistic looking <e2>images</e2> -by restricting the space of possible outputs to a well-defined lowdimensional geometric transformation of real images."
sameAs(e1, e2)
Comment:

3978	"in this work, we leverage spatial transformer networks (stns) [11] , a special type of cnns capable of performing geometric transformations on <e1>images</e1>, to provide a simpler way to generate realistic looking images -by restricting the space of possible outputs to a well-defined lowdimensional geometric transformation of real <e2>images</e2>."
sameAs(e1, e2)
Comment:

3979	"in this work, we leverage spatial transformer networks (stns) [11] , a special type of cnns capable of performing geometric transformations on images, to provide a simpler way to generate realistic looking <e1>images</e1> -by restricting the space of possible outputs to a well-defined lowdimensional geometric transformation of real <e2>images</e2>."
sameAs(e1, e2)
Comment:

3980	"we propose spatial transformer <e1>generative adversarial networks</e1> (st-<e2>gans</e2>), which learn spatial transformer generators within a gan framework."
sameAs(e1, e2)
Comment:

3981	"the adversarial loss enables us to learn geometric corrections resulting in a warped <e1>image</e1> that lies at the intersection of the natural <e2>image</e2> man-ifold and the geometric manifold -the space of geometric manipulations specific to the target image (fig."
sameAs(e1, e2)
Comment:

3982	"the adversarial loss enables us to learn geometric corrections resulting in a warped <e1>image</e1> that lies at the intersection of the natural image man-ifold and the geometric manifold -the space of geometric manipulations specific to the target <e2>image</e2> (fig."
sameAs(e1, e2)
Comment:

3983	"the adversarial loss enables us to learn geometric corrections resulting in a warped image that lies at the intersection of the natural <e1>image</e1> man-ifold and the geometric manifold -the space of geometric manipulations specific to the target <e2>image</e2> (fig."
sameAs(e1, e2)
Comment:

3984	"to achieve this, we advocate a sequential adversarial training strategy to learn iterative spatial <e1>transformations</e1> that serve to break large <e2>transformations</e2> down into smaller ones."
sameAs(e1, e2)
Comment:

3985	"we evaluate st-gans in the context <e1>image</e1> compositing, where a source foreground <e2>image</e2> and its mask are warped by the spatial transformer generator g, and the resulting composite is assessed by the discriminator d. in this setup, d tries to distinguish warped composites from real images, while g tries to fool d by generating as realistic looking as possible composites."
sameAs(e1, e2)
Comment:

3986	"our main contributions are as follows: • we integrate the stn and <e1>gan</e1> frameworks and introduce st-<e2>gan</e2>, a novel gan framework for finding realistic-looking geometric warps."
sameAs(e1, e2)
Comment:

3987	"our main contributions are as follows: • we integrate the stn and <e1>gan</e1> frameworks and introduce st-gan, a novel <e2>gan</e2> framework for finding realistic-looking geometric warps."
sameAs(e1, e2)
Comment:

3988	"our main contributions are as follows: • we integrate the stn and gan <e1>frameworks</e1> and introduce st-gan, a novel gan <e2>framework</e2> for finding realistic-looking geometric warps."
sameAs(e1, e2)
Comment:

3989	"our main contributions are as follows: • we integrate the stn and gan frameworks and introduce st-<e1>gan</e1>, a novel <e2>gan</e2> framework for finding realistic-looking geometric warps."
sameAs(e1, e2)
Comment:

3990	"most approaches constrain the possible appearance variations within an <e1>image</e1> by learning a low-dimensional embedding as an encoding of the natural <e2>image</e2> subspace and making predictions from this at the pixel level."
sameAs(e1, e2)
Comment:

3991	"they consist of a generator network (g) <e1>that</e1> produces images from codes, and a discriminator network (d) <e2>that</e2> distinguishes real images from fake ones."
sameAs(e1, e2)
Comment:

3992	"they consist of a generator network (g) that produces <e1>images</e1> from codes, and a discriminator network (d) that distinguishes real <e2>images</e2> from fake ones."
sameAs(e1, e2)
Comment:

3993	"these <e1>two</e1> networks play a minimax game that results in g generating realistic looking images and d being unable to distinguish between the <e2>two</e2> when equilibrium is reached."
sameAs(e1, e2)
Comment:

3994	"estimating the success of unsupervised <e1>image</e1> to <e2>image</e2> translation"
sameAs(e1, e2)
Comment:

3995	"practitioners are often uncertain regarding the <e1>results</e1> obtained when evaluating gan-based <e2>methods</e2>, and many avoid using these altogether."
Compare(e1, e2)
Comment:

3996	"while in supervised learning, the validation error is an unbiased estimator of the <e1>generalization</e1> (test) error and complexity-based <e2>generalization</e2> bounds are abundant, no such bounds exist for learning a mapping in an unsupervised way."
sameAs(e1, e2)
Comment:

3997	"while in supervised learning, the validation error is an unbiased estimator of the generalization (test) error and complexity-based generalization <e1>bounds</e1> are abundant, no such <e2>bounds</e2> exist for learning a mapping in an unsupervised way."
sameAs(e1, e2)
Comment:

3998	"this includes a unique combination of the hyperband <e1>method</e1> [16] , which is perhaps the leading <e2>method</e2> in hyperparameter optimization, in the supervised setting, with our bound."
sameAs(e1, e2)
Comment:

3999	"as a result, when training <e1>gans</e1> and specifically when using <e2>gans</e2> for learning to map between domains in a completely unsupervised way, one is forced to select the hyperparameters and the stopping epoch by subjectively examining multiple options."
sameAs(e1, e2)
Comment:

4000	"in a zero-inflated embedding (zie), a zero in the <e1>data</e1> can come from an interaction to other <e2>data</e2> (i.e., an embedding) or from a separate process by which many observations are equal to zero (i.e."
sameAs(e1, e2)
Comment:

4001	"on top of the idea to decouple the norm and the angle in an inner product, we propose a novel decoupled network (dcnet) by generalizing traditional inner productbased convolution <e1>operators</e1> ( w x cos(θ (w,x) )) to decoupled <e2>operators</e2>."
sameAs(e1, e2)
Comment:

4002	"to this end, we define such operator as multiplication of a <e1>function</e1> of norms h( w , x ) and a <e2>function</e2> of angle g(θ (w,x) )."
sameAs(e1, e2)
Comment:

4003	"the magnitude <e1>function</e1> h( w , x ) models the intra-class variation while the angular <e2>function</e2> g(θ (w,x) ) models the semantic difference."
sameAs(e1, e2)
Comment:

4004	"the magnitude function h( w , x ) <e1>models</e1> the intra-class variation while the angular function g(θ (w,x) ) <e2>models</e2> the semantic difference."
sameAs(e1, e2)
Comment:

4005	"with the decoupled learning framework, we can either design the decoupled operators based on the <e1>task</e1> itself or learn them directly from <e2>data</e2>."
Conjunction(e1, e2)
Comment:

4006	"first, dcnets not only allow us to use some alternative <e1>functions</e1> to better model the intraclass variation and the semantic difference, but they also enable us to directly learn these <e2>functions</e2> rather than fixing them."
sameAs(e1, e2)
Comment:

4007	"specifically, we propose two different types of decoupled convolution <e1>operators</e1>: bounded <e2>operators</e2> and unbounded operators."
sameAs(e1, e2)
Comment:

4008	"specifically, we propose two different types of decoupled convolution <e1>operators</e1>: bounded operators and unbounded <e2>operators</e2>."
sameAs(e1, e2)
Comment:

4009	"specifically, we propose two different types of decoupled convolution operators: bounded <e1>operators</e1> and unbounded <e2>operators</e2>."
sameAs(e1, e2)
Comment:

4010	"empirically, the bounded <e1>operators</e1> may yield faster convergence and better robustness against adversarial attacks, and the unbounded <e2>operators</e2> may have better representational power."
sameAs(e1, e2)
Comment:

4011	"empirically, the bounded operators may yield <e1>faster convergence</e1> and better <e2>robustness</e2> against adversarial attacks, and the unbounded operators may have better representational power."
Conjunction(e1, e2)
Comment:

4012	" introduction in recent years, hundreds of thousands of<e1> image</e1>s are uploaded to the internet every day, making it extremely difficult to find relevant<e2> image</e2>s according to different users' request."
sameAs(e1, e2)
Comment:

4013	"the successful applications of <e1>cnns</e1> in various tasks imply that the features learned by <e2>cnns</e2> can well capture the underlying semantic structure of images in spite of significant appearance variations."
sameAs(e1, e2)
Comment:

4014	"inspired by the robustness of <e1>cnn</e1> features, we propose a binary code learning framework by exploiting the <e2>cnn</e2> structure, named deep supervised hashing (dsh)."
sameAs(e1, e2)
Comment:

4015	"in practice, we generate <e1>image</e1> pairs online so that many more <e2>image</e2> pairs can be utilized in the training stage."
sameAs(e1, e2)
Comment:

4016	"the loss function is designed to pull the network <e1>outputs</e1> of similar images together and push the <e2>outputs</e2> of dissimilar ones far away, so that the learned hamming space can well approximate the semantic structure of images."
sameAs(e1, e2)
Comment:

4017	"the loss function is designed to pull the network outputs of similar <e1>images</e1> together and push the outputs of dissimilar ones far away, so that the learned hamming space can well approximate the semantic structure of <e2>images</e2>."
sameAs(e1, e2)
Comment:

4018	"the network consists of 3 convolution-pooling <e1>layers</e1> and 2 fully connected <e2>layers</e2>."
sameAs(e1, e2)
Comment:

4019	"the filters in <e1>convolution</e1> layers are of size 5 × 5 with stride 1 (32, 32, and 64 filters in the three <e2>convolution</e2> layers respectively), and pooling over 3 × 3 patches with stride 2. the first fully connected layer contains 500 nodes, and the second (output layer) has k (the code length) nodes."
sameAs(e1, e2)
Comment:

4020	"the filters in convolution <e1>layers</e1> are of size 5 × 5 with stride 1 (32, 32, and 64 filters in the three convolution <e2>layers</e2> respectively), and pooling over 3 × 3 patches with stride 2. the first fully connected layer contains 500 nodes, and the second (output layer) has k (the code length) nodes."
sameAs(e1, e2)
Comment:

4021	"the filters in convolution layers are of size 5 × 5 with stride 1 (32, 32, and 64 filters in the three convolution layers respectively), and pooling over 3 × 3 patches with stride 2. the first fully connected layer contains 500 <e1>nodes</e1>, and the second (output layer) has k (the code length) <e2>nodes</e2>."
sameAs(e1, e2)
Comment:

4022	"for example, content based <e1>image</e1> retrieval retrieves images that are similar to a given query <e2>image</e2>, where "similar" may refer to visually similar or semantically similar."
sameAs(e1, e2)
Comment:

4023	"under this framework, images can be easily encoded by first propagating through the <e1>network</e1> and then quantizing the <e2>network</e2> outputs to binary codes representation."
sameAs(e1, e2)
Comment:

4024	"suppose that both the <e1>images</e1> in the database and the query image are represented by real-valued features, the simplest way of looking for relevant <e2>images</e2> is by ranking the database images according to their distances to the query image in the feature space, and returning the closest ones."
sameAs(e1, e2)
Comment:

4025	"suppose that both the <e1>images</e1> in the database and the query image are represented by real-valued features, the simplest way of looking for relevant images is by ranking the database <e2>images</e2> according to their distances to the query image in the feature space, and returning the closest ones."
sameAs(e1, e2)
Comment:

4026	"suppose that both the images in the <e1>database</e1> and the query image are represented by real-valued features, the simplest way of looking for relevant images is by ranking the <e2>database</e2> images according to their distances to the query image in the feature space, and returning the closest ones."
sameAs(e1, e2)
Comment:

4027	"suppose that both the images in the database and the query <e1>image</e1> are represented by real-valued features, the simplest way of looking for relevant images is by ranking the database images according to their distances to the query <e2>image</e2> in the feature space, and returning the closest ones."
sameAs(e1, e2)
Comment:

4028	"suppose that both the images in the database and the query image are represented by real-valued features, the simplest way of looking for relevant <e1>images</e1> is by ranking the database <e2>images</e2> according to their distances to the query image in the feature space, and returning the closest ones."
sameAs(e1, e2)
Comment:

4029	"however, for a <e1>database</e1> with millions of images, which is quite common nowadays, even a linear search through the <e2>database</e2> would cost a great deal of time and memory."
sameAs(e1, e2)
Comment:

4030	"however, the retrieval performance of most existing hashing methods heavily depends on the features they use, which are basically extracted in an unsupervised manner, thus more suitable for dealing with the visual similarity <e1>search</e1> rather than the semantic similarity <e2>search</e2>."
sameAs(e1, e2)
Comment:

4031	"on the <e1>other</e1> hand, recent progress in image classification [12, 25, 8] , object detection [26] , face recognition [24] , and many <e2>other</e2> vision tasks [18, 2] demonstrate the impressive learning power of cnns."
sameAs(e1, e2)
Comment:

4032	"in <e1>these</e1> different <e2>tasks</e2>, the cnns can be viewed as a feature extractor guided by the objective functions specifically designed for the individual tasks."
Used-for(e1, e2)
Comment:

4033	"in <e1>these</e1> different tasks, the cnns can be viewed as a feature extractor guided by the objective functions specifically designed for the individual <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

4034	"in these different <e1>tasks</e1>, the cnns can be viewed as a feature extractor guided by the objective functions specifically designed for the individual <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

4035	"in contrast to previous two-stage <e1>text</e1> spotting, our method learns better features through convolutional neural network, which are shared between <e2>text</e2> detection and text recognition."
sameAs(e1, e2)
Comment:

4036	"although previous works have made significant progress in both <e1>text</e1> detection and text recognition, it is still challenging due to the large variance of <e2>text</e2> patterns and highly complicated background."
sameAs(e1, e2)
Comment:

4037	"the text proposal features are then fed into <e1>recurrent neural network</e1> (<e2>rnn</e2>) encoder and connectionist temporal classification (ctc) decoder [9] for text recognition."
sameAs(e1, e2)
Comment:

4038	"the most common way in <e1>scene</e1> <e2>text</e2> reading is to divide it into text detection and text recognition, which are handled as two separate tasks [20, 34] ."
Conjunction(e1, e2)
Comment:

4039	"the most common way in <e1>scene</e1> text reading is to divide it into <e2>text</e2> detection and text recognition, which are handled as two separate tasks [20, 34] ."
Conjunction(e1, e2)
Comment:

4040	"the most common way in scene <e1>text</e1> reading is to divide it into <e2>text</e2> detection and text recognition, which are handled as two separate tasks [20, 34] ."
sameAs(e1, e2)
Comment:

4041	"• fots significantly surpasses state-of-the-art methods on a number of <e1>text</e1> detection and <e2>text</e2> spotting benchmarks, including icdar 2015 [26] , icdar 2017 mlt [1] and icdar 2013 [27] ."
sameAs(e1, e2)
Comment:

4042	"while in text recognition, a network for sequential prediction is conducted on top of text regions, <e1>one</e1> by <e2>one</e2> [44, 14] ."
sameAs(e1, e2)
Comment:

4043	"further, based on the <e1>features</e1> used, all existing methods can be roughly categorized into two categories: i) hand-crafted <e2>features</e2> based methods [6] [22] ."
sameAs(e1, e2)
Comment:

4044	"further, based on the features used, all existing <e1>methods</e1> can be roughly categorized into two categories: i) hand-crafted features based <e2>methods</e2> [6] [22] ."
sameAs(e1, e2)
Comment:

4045	"we presume it is probable <e1>that</e1> the video frame prediction is far from satisfactory at <e2>that</e2> time."
sameAs(e1, e2)
Comment:

4046	"however, it is extremely challenging because abnormal <e1>events</e1> are unbounded in real applications, and it is almost infeasible to gather all kinds of abnormal <e2>events</e2> and tackle the problem with a classification method."
sameAs(e1, e2)
Comment:

4047	"we implement our predictor with an u-net [30] network architecture given its good performance at <e1>image</e1>-to-<e2>image</e2> translation [17] ."
sameAs(e1, e2)
Comment:

4048	"first, we impose a constraint on the appearance by enforcing the intensity and gradient maps of the predicted frame to be close to its ground truth; then, <e1>motion</e1> is another important feature for video characterization [32] , and a good prediction should be consistent with real object <e2>motion</e2>."
sameAs(e1, e2)
Comment:

4049	"further, we also add a generative adversarial network (gan) [13] module into our framework in light of its success for image <e1>generation</e1> [9] and video <e2>generation</e2> [27] ."
sameAs(e1, e2)
Comment:

4050	"to the best of our knowledge, it is the first work that leverages <e1>video</e1> prediction for anomaly detection; ii) for the <e2>video</e2> frame prediction framework, other than enforcing predicted frames to be close to their ground truth in spatial space, we also enforce the optical flow between predicted frames to be close to their optical flow ground truth."
sameAs(e1, e2)
Comment:

4051	"to the best of our knowledge, it is the first work that leverages video <e1>prediction</e1> for anomaly detection; ii) for the video frame <e2>prediction</e2> framework, other than enforcing predicted frames to be close to their ground truth in spatial space, we also enforce the optical flow between predicted frames to be close to their optical flow ground truth."
sameAs(e1, e2)
Comment:

4052	"to the best of our knowledge, it is the first work that leverages video prediction for anomaly detection; ii) for the video frame prediction framework, other than enforcing predicted frames to be close to their ground truth in spatial space, we also enforce the optical <e1>flow</e1> between predicted frames to be close to their optical <e2>flow</e2> ground truth."
sameAs(e1, e2)
Comment:

4053	"such a temporal constraint is shown to be crucial for video frame prediction, and it is also the first work that leverages a motion constraint for anomaly detection; iii) experiments on toy dataset <e1>validate</e1> the robustness to the uncertainty for normal events, which <e2>validates</e2> the robustness of our method."
sameAs(e1, e2)
Comment:

4054	"such a temporal constraint is shown to be crucial for video frame prediction, and it is also the first work that leverages a motion constraint for anomaly detection; iii) experiments on toy dataset validate the <e1>robustness</e1> to the uncertainty for normal events, which validates the <e2>robustness</e2> of our method."
sameAs(e1, e2)
Comment:

4055	"abstract the deployment of deep <e1>convolutional neural networks (cnns)</e1> introduction in recent years, <e2>convolutional neural networks (cnns)</e2> have become the dominant approach for a variety of computer vision tasks, e.g., image classification [22] , object detection [8] , semantic segmentation [26] ."
sameAs(e1, e2)
Comment:

4056	"for instance, from alexnet [22] , vggnet [31] and googlenet [34] to resnets [14] , the imagenet classification challenge winner models have evolved from 8 <e1>layers</e1> to more than 100 <e2>layers</e2>."
sameAs(e1, e2)
Comment:

4057	"for instance, a 152-layer resnet [14] has more than 60 million parameters and requires more than 20 giga float-point-operations (flops) when inferencing an image with resolution 224× 224. this is unlikely to be affordable on resource constrained platforms such as mobile <e1>devices</e1>, wearables or internet of things (iot) <e2>devices</e2>."
sameAs(e1, e2)
Comment:

4058	"the deployment of <e1>cnns</e1> in real world applications are mostly constrained by 1) model size: <e2>cnns</e2>' strong representation power comes from their millions of trainable parameters."
sameAs(e1, e2)
Comment:

4059	"if <e1>we</e1> want to check whether it belongs to the foreground dog for uniformly highlighting the whole dog, <e2>we</e2> need to refer to other parts of the dog."
sameAs(e1, e2)
Comment:

4060	"thus, we construct the attended contextual features from the global view to local contexts, from coarse <e1>scale</e1> to fine <e2>scales</e2>, and use them to enhance the convolutional features to facilitate saliency inference at each pixel."
sameAs(e1, e2)
Comment:

4061	"in these models, the first school extracts contextual features from each input <e1>image</e1> region, while the second one extracts features at each <e2>image</e2> location from its corresponding receptive field."
sameAs(e1, e2)
Comment:

4062	"predicting salient <e1>face</e1> in multiple-<e2>face</e2> videos"
sameAs(e1, e2)
Comment:

4063	"the representative <e1>method</e1> on predicting image saliency is itti's <e2>model</e2> [20] , which combines centersurround features of color, intensity and orientation together."
Used-for(e1, e2)
Comment:

4064	"for example, judd et al combined high-level <e1>features</e1> (e.g., face and text), middle-level <e2>features</e2> (e.g., gist) and low-level features together, via learning their corresponding weights with the support vector machine (svm)."
sameAs(e1, e2)
Comment:

4065	"for example, judd et al combined high-level <e1>features</e1> (e.g., face and text), middle-level features (e.g., gist) and low-level features together, via learning their corresponding weights with the <e2>support vector machine</e2> (svm)."
Used-for(e1, e2)
Comment:

4066	"for example, judd et al combined high-level <e1>features</e1> (e.g., face and text), middle-level features (e.g., gist) and low-level features together, via learning their corresponding weights with the support vector machine (<e2>svm</e2>)."
Used-for(e1, e2)
Comment:

4067	"for example, judd et al combined high-level features (e.g., face and text), middle-level <e1>features</e1> (e.g., gist) and low-level features together, via learning their corresponding weights with the <e2>support vector machine</e2> (svm)."
Used-for(e1, e2)
Comment:

4068	"for example, judd et al combined high-level features (e.g., face and text), middle-level <e1>features</e1> (e.g., gist) and low-level features together, via learning their corresponding weights with the support vector machine (<e2>svm</e2>)."
Used-for(e1, e2)
Comment:

4069	"for example, judd et al combined high-level features (e.g., face and text), middle-level features (e.g., gist) and low-level features together, via learning their corresponding weights with the <e1>support vector machine</e1> (<e2>svm</e2>)."
sameAs(e1, e2)
Comment:

4070	"to predict visual attention in face images, xu et al [41] proposed to precisely model saliency of <e1>face</e1> region, via learning the fixation distributions of <e2>face</e2> and facial features."
sameAs(e1, e2)
Comment:

4071	"besides, jiang et al [21] explored several <e1>face</e1>-related features to predict saliency in a scene with multiple <e2>faces</e2>."
sameAs(e1, e2)
Comment:

4072	"for video saliency prediction, earlier methods [6, 8, 12, [17] [18] [19] have investigated several dynamic <e1>features</e1> to <e2>model</e2> visual attention on videos, in light of the hvs."
Used-for(e1, e2)
Comment:

4073	"this figure mainly demonstrates transition of salient <e1>faces</e1> and characters of long/short-term correlation between salient <e2>faces</e2> across frames."
sameAs(e1, e2)
Comment:

4074	"however, to our best knowledge, the existing video saliency <e1>prediction</e1> methods rely on the handcrafted features, despite cnn being applied to automatically learn features for image saliency <e2>prediction</e2> in the most recent works of [14, 24, 25, 28, 32] ."
sameAs(e1, e2)
Comment:

4075	"more importantly, both long-and short-term correlation of salient <e1>faces</e1> across frames, which is critical in modeling attention transition across frames for multiple-<e2>face</e2> videos (see figure 1) , is not taken into account in these methods."
sameAs(e1, e2)
Comment:

4076	"in this paper, we propose a dl-based method to predict salient <e1>face</e1> in multiple-<e2>face</e2> videos, which learns both image features and saliency transition for modeling attention on multiple faces across frames."
sameAs(e1, e2)
Comment:

4077	"in this paper, we propose a dl-based method to predict salient <e1>face</e1> in multiple-face videos, which learns both image features and saliency transition for modeling attention on multiple <e2>faces</e2> across frames."
sameAs(e1, e2)
Comment:

4078	"in this paper, we propose a dl-based method to predict salient face in multiple-<e1>face</e1> videos, which learns both image features and saliency transition for modeling attention on multiple <e2>faces</e2> across frames."
sameAs(e1, e2)
Comment:

4079	"built on the long short-term memory (<e1>lstm</e1>) of recurrent neural network (rnn), we develop a multiplestream <e2>lstm</e2> (m-lstm) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted cnn features as the input."
sameAs(e1, e2)
Comment:

4080	"built on the long short-term memory (<e1>lstm</e1>) of recurrent neural network (rnn), we develop a multiplestream lstm (m-<e2>lstm</e2>) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted cnn features as the input."
sameAs(e1, e2)
Comment:

4081	"built on the long short-term memory (lstm) of <e1>recurrent neural network</e1> (<e2>rnn</e2>), we develop a multiplestream lstm (m-lstm) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted cnn features as the input."
sameAs(e1, e2)
Comment:

4082	"built on the long short-term memory (lstm) of recurrent neural network (rnn), we develop a multiplestream <e1>lstm</e1> (m-<e2>lstm</e2>) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted cnn features as the input."
sameAs(e1, e2)
Comment:

4083	"finally, saliency maps of multiple-<e1>face</e1> videos can be generated upon transited <e2>face</e2> saliency."
sameAs(e1, e2)
Comment:

4084	"we provide a detailed analysis on the collected data, which shows <e1>that</e1> typically only one face (among multiple faces) in a video frame receives attention of viewing subjects, and <e2>that</e2> attention shifts across frames consistently for different subjects."
sameAs(e1, e2)
Comment:

4085	"we provide a detailed analysis on the collected data, which shows that typically only one <e1>face</e1> (among multiple <e2>faces</e2>) in a video frame receives attention of viewing subjects, and that attention shifts across frames consistently for different subjects."
sameAs(e1, e2)
Comment:

4086	"(3) we propose a dl-based <e1>method</e1> to predict the salient face with transition across frames, which integrates a cnn and an lstm-based rnn <e2>model</e2>."
Used-for(e1, e2)
Comment:

4087	"our goal in this work is to capture both static and dynamic properties of the attention on <e1>faces</e1> in multiple-<e2>face</e2> videos."
sameAs(e1, e2)
Comment:

4088	"abstract most <e1>video</e1>-based action recognition approaches choose to extract features from the whole <e2>video</e2> to recognize actions."
sameAs(e1, e2)
Comment:

4089	"with recent advances of <e1>human pose</e1> estimation, this work presents a novel method to recognize human action as the evolution of <e2>pose estimation</e2> maps."
isA(e1, e2)
Comment:

4090	"with recent advances of human <e1>pose estimation</e1>, this work presents a novel method to recognize human action as the evolution of <e2>pose estimation</e2> maps."
sameAs(e1, e2)
Comment:

4091	"instead of relying on the inaccurate <e1>human poses</e1> estimated from videos, we observe that <e2>pose estimation</e2> maps, the byproduct of pose estimation, preserve richer cues of human body to benefit action recognition."
isA(e1, e2)
Comment:

4092	"instead of relying on the inaccurate <e1>human poses</e1> estimated from videos, we observe that pose estimation maps, the byproduct of <e2>pose estimation</e2>, preserve richer cues of human body to benefit action recognition."
isA(e1, e2)
Comment:

4093	"instead of relying on the inaccurate human poses estimated from videos, we observe that <e1>pose estimation</e1> maps, the byproduct of <e2>pose estimation</e2>, preserve richer cues of human body to benefit action recognition."
sameAs(e1, e2)
Comment:

4094	"specifically, the evolution of pose estimation <e1>maps</e1> can be decomposed as an evolution of heatmaps, e.g., probabilistic <e2>maps</e2>, and an evolution of estimated 2d human poses, which denote the changes of body shape and body pose, respectively."
sameAs(e1, e2)
Comment:

4095	"as body shape evolution <e1>image</e1> does not differentiate body parts, we design body guided sampling to aggregate the evolution of poses as a body pose evolution <e2>image</e2>."
sameAs(e1, e2)
Comment:

4096	"as body shape evolution image does not differentiate body parts, we design body guided sampling to aggregate the evolution of <e1>poses</e1> as a body <e2>pose</e2> evolution image."
sameAs(e1, e2)
Comment:

4097	"abstract in this paper, we propose an accurate <e1>edge</e1> detector using richer convolutional features (rcf) introduction <e2>edge</e2> detection, which aims to extract visually salient edges and object boundaries from natural images, has remained as one of the main challenges in computer vision for several decades."
sameAs(e1, e2)
Comment:

4098	"some well-known cnn-based methods have pushed forward this <e1>field</e1> significantly, such as deepedge [4] , n 4 -<e2>fields</e2> [19] , cscnn [26] , deepcontour [47] , and hed [58] ."
sameAs(e1, e2)
Comment:

4099	"conv) <e1>layers</e1> in edge detection, we build a simple network to produce side outputs of intermediate <e2>layers</e2> using vgg16 [50] which has five conv stages."
sameAs(e1, e2)
Comment:

4100	"so why don't <e1>we</e1> make full use the cnn features <e2>we</e2> have now?"
sameAs(e1, e2)
Comment:

4101	"unlike previous <e1>cnn</e1> methods, the proposed novel network uses the <e2>cnn</e2> features of all the conv layers to perform the pixelwise prediction in an image-to-image fashion, and thus is able to obtain accurate representations for objects or object parts in different scales."
sameAs(e1, e2)
Comment:

4102	"unlike previous cnn methods, the proposed novel network uses the cnn features of all the conv layers to perform the pixelwise prediction in an <e1>image</e1>-to-<e2>image</e2> fashion, and thus is able to obtain accurate representations for objects or object parts in different scales."
sameAs(e1, e2)
Comment:

4103	"typically, traditional methods first extract local cues of brightness, <e1>colors</e1>, gradients and <e2>textures</e2>, or other manually designed features like pb [40] , gpb [2] , and sketch tokens [36] , then sophisticated learning paradigms [14, 57] are used to classify edge and non-edge pixels."
Conjunction(e1, e2)
Comment:

4104	"typically, traditional methods first extract local cues of brightness, colors, gradients and textures, or other manually designed features like pb [40] , gpb [2] , and sketch tokens [36] , then sophisticated learning paradigms [14, 57] are used to classify <e1>edge</e1> and non-<e2>edge</e2> pixels."
sameAs(e1, e2)
Comment:

4105	"we build a simple network based on vgg16 [50] to produce side outputs of conv3 1, conv3 2, conv3 3, conv4 1, conv4 2 and conv4 3. <e1>one</e1> can clearly see that convolutional features become coarser gradually, and the intermediate layers conv3 1, conv3 2, conv4 1, and conv4 2 contain lots of useful fine details that do not appear in <e2>other</e2> layers."
Conjunction(e1, e2)
Comment:

4106	"we build a simple network based on vgg16 [50] to produce side outputs of conv3 1, conv3 2, conv3 3, conv4 1, conv4 2 and conv4 3. one can clearly see <e1>that</e1> convolutional features become coarser gradually, and the intermediate layers conv3 1, conv3 2, conv4 1, and conv4 2 contain lots of useful fine details <e2>that</e2> do not appear in other layers."
sameAs(e1, e2)
Comment:

4107	"we build a simple network based on vgg16 [50] to produce side outputs of conv3 1, conv3 2, conv3 3, conv4 1, conv4 2 and conv4 3. one can clearly see that convolutional features become coarser gradually, and the intermediate <e1>layers</e1> conv3 1, conv3 2, conv4 1, and conv4 2 contain lots of useful fine details that do not appear in other <e2>layers</e2>."
sameAs(e1, e2)
Comment:

4108	"abstract this paper addresses deep <e1>face recognition</e1> (fr) introduction recent years have witnessed the great success of convolutional neural networks (cnns) in <e2>face recognition</e2> (fr)."
sameAs(e1, e2)
Comment:

4109	"yellow dots represent the first class <e1>face</e1> features, while purple dots represent the second class <e2>face</e2> features."
sameAs(e1, e2)
Comment:

4110	"yellow dots represent the first class face <e1>features</e1>, while purple dots represent the second class face <e2>features</e2>."
sameAs(e1, e2)
Comment:

4111	"oneering work [30, 26] <e1>learn</e1> face features via the softmax loss 1 , but softmax loss only <e2>learns</e2> separable features that are not discriminative enough."
sameAs(e1, e2)
Comment:

4112	"oneering work [30, 26] learn face <e1>features</e1> via the softmax loss 1 , but softmax loss only learns separable <e2>features</e2> that are not discriminative enough."
sameAs(e1, e2)
Comment:

4113	"it seems to be a widely recognized choice to impose euclidean margin to <e1>learned features</e1>, but a question arises: is euclidean margin always suitable for learning discriminative face <e2>features</e2>?"
isA(e1, e2)
Comment:

4114	"typically, face recognition can be categorized as <e1>face</e1> identification and <e2>face</e2> verification [8, 11] ."
sameAs(e1, e2)
Comment:

4115	"the <e1>former</e1> classifies a face to a specific identity, while the <e2>latter</e2> determines whether a pair of faces belongs to the same identity."
Conjunction(e1, e2)
Comment:

4116	"the former classifies a <e1>face</e1> to a specific identity, while the latter determines whether a pair of <e2>faces</e2> belongs to the same identity."
sameAs(e1, e2)
Comment:

4117	"in the light of this, we derive lower bounds for the parameter m to approximate the desired open-set fr criterion that the maximal intra-class <e1>distance</e1> should be smaller than the minimal inter-class <e2>distance</e2>."
sameAs(e1, e2)
Comment:

4118	"(2) we derive lower bounds for m such <e1>that</e1> a-softmax loss can approximate the learning task <e2>that</e2> minimal interclass distance is larger than maximal intra-class distance."
sameAs(e1, e2)
Comment:

4119	"(2) we derive lower bounds for m such that a-softmax loss can approximate the learning task that minimal interclass <e1>distance</e1> is larger than maximal intra-class <e2>distance</e2>."
sameAs(e1, e2)
Comment:

4120	"trained on publicly available casia dataset [37] , sphereface achieves competitive results on several benchmarks, including labeled <e1>face</e1> in the wild (lfw), youtube <e2>faces</e2> (ytf) and megaface challenge 1."
sameAs(e1, e2)
Comment:

4121	"in this scenario, <e1>face</e1> verification is equivalent to performing identification for a pair of <e2>faces</e2> respectively (see left side of fig."
sameAs(e1, e2)
Comment:

4122	" introduction understanding human motion is extremely important for various<e1> application</e1>s in computer vision and robotics, particularly for<e2> application</e2>s that require interaction with humans."
sameAs(e1, e2)
Comment:

4123	"however, their residual unit based model often converges to an undesired mean pose in the <e1>long-term</e1> <e2>predictions</e2>, i.e., the predictor gives static predictions similar to the mean of the ground truth of future sequences (see figure 1 )."
Feature-of(e1, e2)
Comment:

4124	"however, their residual unit based model often converges to an undesired mean pose in the <e1>long-term</e1> predictions, i.e., the predictor gives static <e2>predictions</e2> similar to the mean of the ground truth of future sequences (see figure 1 )."
Feature-of(e1, e2)
Comment:

4125	"however, their residual unit based model often converges to an undesired mean pose in the long-term <e1>predictions</e1>, i.e., the predictor gives static <e2>predictions</e2> similar to the mean of the ground truth of future sequences (see figure 1 )."
sameAs(e1, e2)
Comment:

4126	"we believe <e1>that</e1> the mean pose problem is caused by the fact <e2>that</e2> it is difficult for recurrent models to learn to keep track of long-term information; the mean pose becomes a good prediction of the future pose when the model loses track of information from the distant past."
sameAs(e1, e2)
Comment:

4127	"we believe that the mean <e1>pose</e1> problem is caused by the fact that it is difficult for recurrent models to learn to keep track of long-term information; the mean <e2>pose</e2> becomes a good prediction of the future pose when the model loses track of information from the distant past."
sameAs(e1, e2)
Comment:

4128	"we believe that the mean <e1>pose</e1> problem is caused by the fact that it is difficult for recurrent models to learn to keep track of long-term information; the mean pose becomes a good prediction of the future <e2>pose</e2> when the model loses track of information from the distant past."
sameAs(e1, e2)
Comment:

4129	"we believe that the mean pose problem is caused by the fact that it is difficult for recurrent models to learn to keep track of long-term <e1>information</e1>; the mean pose becomes a good prediction of the future pose when the model loses track of <e2>information</e2> from the distant past."
sameAs(e1, e2)
Comment:

4130	"we believe that the mean pose problem is caused by the fact that it is difficult for recurrent models to learn to keep track of long-term information; the mean <e1>pose</e1> becomes a good prediction of the future <e2>pose</e2> when the model loses track of information from the distant past."
sameAs(e1, e2)
Comment:

4131	"for a chainstructured <e1>rnn</e1> model, it takes n steps for two elements that are n time steps apart to interact with each other; this may make it difficult for an <e2>rnn</e2> to learn a structure that is able to exploit long-term correlations [6] ."
sameAs(e1, e2)
Comment:

4132	"for a chainstructured rnn model, it takes n steps for two elements <e1>that</e1> are n time steps apart to interact with each other; this may make it difficult for an rnn to learn a structure <e2>that</e2> is able to exploit long-term correlations [6] ."
sameAs(e1, e2)
Comment:

4133	"the sequenceto-sequence model consists of an <e1>encoder</e1> and a decoder, in which the <e2>encoder</e2> maps a given seed sequence to a hidden variable, and the decoder maps the hidden variable to the target sequence."
sameAs(e1, e2)
Comment:

4134	"the sequenceto-sequence model consists of an encoder and a <e1>decoder</e1>, in which the encoder maps a given seed sequence to a hidden variable, and the <e2>decoder</e2> maps the hidden variable to the target sequence."
sameAs(e1, e2)
Comment:

4135	"the sequenceto-sequence model consists of an encoder and a decoder, in which the encoder <e1>maps</e1> a given seed sequence to a hidden variable, and the decoder <e2>maps</e2> the hidden variable to the target sequence."
sameAs(e1, e2)
Comment:

4136	"a major difference between human motion prediction and other sequence-to-sequence tasks is that human <e1>motion</e1> is a highly constrained system by <e2>environment</e2> properties, human body properties and newton's laws."
Conjunction(e1, e2)
Comment:

4137	"a major difference between human motion prediction and other sequence-to-sequence tasks is that human motion is a highly constrained system by environment <e1>properties</e1>, human body <e2>properties</e2> and newton's laws."
sameAs(e1, e2)
Comment:

4138	"however, rnn may not be able to learn <e1>these</e1> constraints accurately, and the accumulation of <e2>these</e2> errors in decoder may result in larger error in long-term prediction."
sameAs(e1, e2)
Comment:

4139	"furthermore, the human body is often not static stable during <e1>motion</e1>, and our central neural system must make multiple parts of our body coordinate with each other to stabilize the <e2>motion</e2> under gravity and other loads [20] ."
sameAs(e1, e2)
Comment:

4140	"furthermore, the human body is often not static stable during motion, and our central neural system must make multiple parts of our body coordinate with each <e1>other</e1> to stabilize the motion under gravity and <e2>other</e2> loads [20] ."
sameAs(e1, e2)
Comment:

4141	"despite these limitations, the rnn based <e1>method</e1> [14] is considered to be the current state-of-the-art as its performance is superior to other human motion prediction <e2>methods</e2> in terms of accuracy."
Compare(e1, e2)
Comment:

4142	"experimental results show that our method can better avoid the <e1>long-term</e1> mean pose problem, and give more realistic <e2>predictions</e2>."
Feature-of(e1, e2)
Comment:

4143	"the quantitative <e1>results</e1> also show that our algorithm outperforms state-ofthe-art <e2>methods</e2> in terms of accuracy."
Compare(e1, e2)
Comment:

4144	"in this paper, we focus on the human motion prediction <e1>task</e1>, using learning based methods from motion capture <e2>data</e2>."
Conjunction(e1, e2)
Comment:

4145	"in earlier works [5, 10] , it is often observed that there is a significant discontinuity between the first predicted frame of <e1>motion</e1> and the last frame of the true observed <e2>motion</e2>."
sameAs(e1, e2)
Comment:

4146	"the additional bonus of using <e1>dnns</e1> comes from the enthusiastic hardware community where <e2>dnns</e2> are rapidly investigated and implemented on gpus [13] , fpgas [14, 15, 16] , and asics [17] ."
sameAs(e1, e2)
Comment:

4147	"unlike the latest works such as [4, 5] which use the deep <e1>cnn</e1> for ancillary, we focus on designing a <e2>cnn</e2>-based density map generator."
sameAs(e1, e2)
Comment:

4148	"our model uses pure convolutional <e1>layers</e1> as the backbone to support input <e2>images</e2> with flexible resolutions."
Part-of(e1, e2)
Comment:

4149	"we deploy the first 10 <e1>layers</e1> from vgg-16 [21] as the front-end and dilated convolution <e2>layers</e2> as the back-end to enlarge receptive fields and extract deeper features without losing resolutions (since pooling layers are not used)."
sameAs(e1, e2)
Comment:

4150	"we deploy the first 10 <e1>layers</e1> from vgg-16 [21] as the front-end and dilated convolution layers as the back-end to enlarge receptive fields and extract deeper features without losing resolutions (since pooling <e2>layers</e2> are not used)."
sameAs(e1, e2)
Comment:

4151	"we deploy the first 10 layers from vgg-16 [21] as the front-end and dilated convolution <e1>layers</e1> as the back-end to enlarge receptive fields and extract deeper features without losing resolutions (since pooling <e2>layers</e2> are not used)."
sameAs(e1, e2)
Comment:

4152	"one major difficulty comes from the <e1>prediction</e1> manner: since the generated density values follow the pixel-by-pixel <e2>prediction</e2>, output density maps must include spatial coherence so that they can present the smooth transition between nearest pixels."
sameAs(e1, e2)
Comment:

4153	"the recent development of congested scene analysis relays on dnn-based methods because of the high accuracy <e1>they</e1> have achieved in semantic segmentation tasks [7, 8, 9, 10, 11] and the significant progress <e2>they</e2> have made in visual saliency [12] ."
sameAs(e1, e2)
Comment:

4154	"while many time-lapse datasets primarily capture outdoor <e1>scenes</e1>, we explicitly wanted representation from indoor <e2>scenes</e2> as well."
sameAs(e1, e2)
Comment:

4155	"we collected 145 sequences from indoor <e1>scenes</e1> and 50 from outdoor <e2>scenes</e2>, yielding a total of ∼6,500 training images."
sameAs(e1, e2)
Comment:

4156	"most outdoor scenes in our dataset are from <e1>time</e1>-lapse sequences where the sun moves evenly over <e2>time</e2>."
sameAs(e1, e2)
Comment:

4157	"however, we found that indoor image sequences are much more challenging because illumination changes in indoor <e1>scenes</e1> tend to be less even or continuous compared to outdoor <e2>scenes</e2>."
sameAs(e1, e2)
Comment:

4158	"in particular, we observed <e1>that</e1>: 1. most relevant video clips cover a short period of time and do not show large changes in light direction, 2. several video clips are comprised of a light turning on/off in a room, producing a limited number (<8) of valid images with different lighting conditions, and 3. the dynamic range of indoor scenes can be high, with strong sunlight or shadows leading to saturation/clipping <e2>that</e2> can break intrinsic image algorithms."
sameAs(e1, e2)
Comment:

4159	"in particular, we observed that: 1. most relevant <e1>video clips</e1> cover a short period of time and do not show large changes in light direction, 2. several <e2>video clips</e2> are comprised of a light turning on/off in a room, producing a limited number (<8) of valid images with different lighting conditions, and 3. the dynamic range of indoor scenes can be high, with strong sunlight or shadows leading to saturation/clipping that can break intrinsic image algorithms."
sameAs(e1, e2)
Comment:

4160	"we found that prior intrinsic image methods designed for <e1>image sequences</e1> often fail on our indoor videos, as their assumptions tend to hold only for outdoor we applied a state-of-the-art multi-image intrinsic image decomposition estimation <e2>algorithm</e2> [13] to our dataset."
Used-for(e1, e2)
Comment:

4161	"we found that prior intrinsic image methods designed for image sequences often fail on our indoor videos, as their assumptions tend to hold only for outdoor we applied a state-of-the-art multi-<e1>image</e1> intrinsic <e2>image</e2> decomposition estimation algorithm [13] to our dataset."
sameAs(e1, e2)
Comment:

4162	"our network has <e1>one</e1> encoder and two decoders, <e2>one</e2> for log-reflectance and one for logshading, with skip connections for both decoders."
sameAs(e1, e2)
Comment:

4163	"our network has <e1>one</e1> encoder and two decoders, one for log-reflectance and <e2>one</e2> for logshading, with skip connections for both decoders."
sameAs(e1, e2)
Comment:

4164	"our network has one encoder and two <e1>decoders</e1>, one for log-reflectance and one for logshading, with skip connections for both <e2>decoders</e2>."
sameAs(e1, e2)
Comment:

4165	"our network has one encoder and two decoders, <e1>one</e1> for log-reflectance and <e2>one</e2> for logshading, with skip connections for both decoders."
sameAs(e1, e2)
Comment:

4166	"single shot <e1>scene</e1> <e2>text</e2> retrieval"
Conjunction(e1, e2)
Comment:

4167	"apart from the scientific interest, a key motivation comes by the plethora of potential applications enabled by automated <e1>scene</e1> text understanding, such as improved <e2>scene</e2>-text based image search, image geo-localization, human-computer interaction, assisted reading for the visually-impaired, robot navigation and industrial automation to mention just a few."
sameAs(e1, e2)
Comment:

4168	"apart from the scientific interest, a key motivation comes by the plethora of potential applications enabled by automated <e1>scene</e1> text understanding, such as improved scene-<e2>text</e2> based image search, image geo-localization, human-computer interaction, assisted reading for the visually-impaired, robot navigation and industrial automation to mention just a few."
Conjunction(e1, e2)
Comment:

4169	"apart from the scientific interest, a key motivation comes by the plethora of potential applications enabled by automated scene text understanding, such as improved <e1>scene</e1>-<e2>text</e2> based image search, image geo-localization, human-computer interaction, assisted reading for the visually-impaired, robot navigation and industrial automation to mention just a few."
Conjunction(e1, e2)
Comment:

4170	"apart from the scientific interest, a key motivation comes by the plethora of potential applications enabled by automated scene text understanding, such as improved scene-text based <e1>image</e1> search, <e2>image</e2> geo-localization, human-computer interaction, assisted reading for the visually-impaired, robot navigation and industrial automation to mention just a few."
sameAs(e1, e2)
Comment:

4171	"it seems impossible to correctly label <e1>them</e1> without reading the text within <e2>them</e2>."
sameAs(e1, e2)
Comment:

4172	"our <e1>scene</e1> <e2>text</e2> retrieval method returns all the images shown here within the top-10 ranked results among more than 10, 000 distractors for the text query "tea"."
Conjunction(e1, e2)
Comment:

4173	"our <e1>scene</e1> text retrieval method returns all the images shown here within the top-10 ranked results among more than 10, 000 distractors for the <e2>text</e2> query "tea"."
Conjunction(e1, e2)
Comment:

4174	"our scene <e1>text</e1> retrieval method returns all the images shown here within the top-10 ranked results among more than 10, 000 distractors for the <e2>text</e2> query "tea"."
sameAs(e1, e2)
Comment:

4175	"textual information found in <e1>scene</e1> images provides high level semantic information about the image and its context and it can be leveraged for better <e2>scene</e2> understanding."
sameAs(e1, e2)
Comment:

4176	"mishra et al [7] introduced the task of <e1>scene</e1> <e2>text</e2> retrieval, where, given a text query, the system must return all images that are likely to contain such text."
Conjunction(e1, e2)
Comment:

4177	"mishra et al [7] introduced the task of <e1>scene</e1> text retrieval, where, given a <e2>text</e2> query, the system must return all images that are likely to contain such text."
Conjunction(e1, e2)
Comment:

4178	"mishra et al [7] introduced the task of <e1>scene</e1> text retrieval, where, given a text query, the system must return all images that are likely to contain such <e2>text</e2>."
Conjunction(e1, e2)
Comment:

4179	"mishra et al [7] introduced the task of scene <e1>text</e1> retrieval, where, given a <e2>text</e2> query, the system must return all images that are likely to contain such text."
sameAs(e1, e2)
Comment:

4180	"mishra et al [7] introduced the task of scene <e1>text</e1> retrieval, where, given a text query, the system must return all images that are likely to contain such <e2>text</e2>."
sameAs(e1, e2)
Comment:

4181	"mishra et al [7] introduced the task of scene text retrieval, where, given a <e1>text</e1> query, the system must return all images that are likely to contain such <e2>text</e2>."
sameAs(e1, e2)
Comment:

4182	"a possible approach to implement <e1>scene</e1> <e2>text</e2> retrieval is to use an end-toend reading system and simply look for the occurrences of the query word within its outputs."
Conjunction(e1, e2)
Comment:

4183	"first, it is worth noting that end-to-end reading <e1>systems</e1> are evaluated on a different <e2>task</e2>, and optimized on different metrics, opting for high precision, and more often than not making use of explicit information about each of the images (for example, short dictionaries given for each image)."
Used-for(e1, e2)
Comment:

4184	"we demonstrate state of the art performance in most <e1>scene</e1> <e2>text</e2> retrieval benchmarks."
Conjunction(e1, e2)
Comment:

4185	"in this paper we address the problem of <e1>scene</e1> <e2>text</e2> retrieval: given a text query, the system must return all images containing the queried text."
Conjunction(e1, e2)
Comment:

4186	"in this paper we address the problem of <e1>scene</e1> text retrieval: given a <e2>text</e2> query, the system must return all images containing the queried text."
Conjunction(e1, e2)
Comment:

4187	"in this paper we address the problem of <e1>scene</e1> text retrieval: given a text query, the system must return all images containing the queried <e2>text</e2>."
Conjunction(e1, e2)
Comment:

4188	"in this paper we address the problem of scene <e1>text</e1> retrieval: given a <e2>text</e2> query, the system must return all images containing the queried text."
sameAs(e1, e2)
Comment:

4189	"in this paper we address the problem of scene <e1>text</e1> retrieval: given a text query, the system must return all images containing the queried <e2>text</e2>."
sameAs(e1, e2)
Comment:

4190	"in this paper we address the problem of scene text retrieval: given a <e1>text</e1> query, the system must return all images containing the queried <e2>text</e2>."
sameAs(e1, e2)
Comment:

4191	"moreover, we show that our <e1>scene</e1> <e2>text</e2> retrieval method yields equally good results for in-dictionary and out-of-dictionary (never before seen) text queries."
Conjunction(e1, e2)
Comment:

4192	"moreover, we show that our <e1>scene</e1> text retrieval method yields equally good results for in-dictionary and out-of-dictionary (never before seen) <e2>text</e2> queries."
Conjunction(e1, e2)
Comment:

4193	"moreover, we show that our scene <e1>text</e1> retrieval method yields equally good results for in-dictionary and out-of-dictionary (never before seen) <e2>text</e2> queries."
sameAs(e1, e2)
Comment:

4194	"by learning to predict phoc representations of <e1>words</e1> the proposed model is able to transfer the knowledge acquired from training data to represent <e2>words</e2> it has never seen before."
sameAs(e1, e2)
Comment:

4195	"section 2 presents an overview of the state of the art in <e1>scene</e1> text understanding tasks, section 3 describes the proposed architecture for single shot <e2>scene</e2> text retrieval."
sameAs(e1, e2)
Comment:

4196	"section 2 presents an overview of the state of the art in <e1>scene</e1> text understanding tasks, section 3 describes the proposed architecture for single shot scene <e2>text</e2> retrieval."
Conjunction(e1, e2)
Comment:

4197	"section 2 presents an overview of the state of the art in scene text understanding tasks, section 3 describes the proposed architecture for single shot <e1>scene</e1> <e2>text</e2> retrieval."
Conjunction(e1, e2)
Comment:

4198	"section 4 reports the experiments and results obtained on different benchmarks for <e1>scene</e1> <e2>text</e2> based image retrieval."
Conjunction(e1, e2)
Comment:

4199	"in this way, the <e1>text</e1> based image retrieval task can be casted as a simple nearest neighbor search of the query <e2>text</e2> representation over the outputs of the cnn over the entire image database."
sameAs(e1, e2)
Comment:

4200	"in this way, the text based <e1>image</e1> retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the cnn over the entire <e2>image</e2> database."
sameAs(e1, e2)
Comment:

4201	"a large percentage of everyday <e1>scene</e1> images contain <e2>text</e2>, especially in urban scenarios [1, 2] ."
Conjunction(e1, e2)
Comment:

4202	"many existing methods use convolutional neural networks (cnns) or <e1>recurrent neural networks</e1> (<e2>rnns</e2>) based on such local feature sequences to capture the temporal interactions within a video."
sameAs(e1, e2)
Comment:

4203	"secondly, an attention model may assign higher weights to significant local features so as to focus on a small <e1>number</e1> of key signals, and their local identifiability determines to what extent the classification results on a small <e2>number</e2> of key frames can be taken as a class label for the entire video."
sameAs(e1, e2)
Comment:

4204	"finally, the inputs to an attention model are naturally unordered sets of varying sizes, which fits the properties of the <e1>local features</e1>, and also facilitates a generalization ability to varying numbers of <e2>local features</e2>."
sameAs(e1, e2)
Comment:

4205	"in the following, <e1>we</e1> first review pertinent related work in section 2. then, in section 3, <e2>we</e2> present our proposed attention clusters approach with the shifting operation, as well as our overall architecture for video classification."
sameAs(e1, e2)
Comment:

4206	"patchwise <e1>training</e1> is common [27, 2, 7, 28, 9] , but lacks the efficiency of fully convolutional <e2>training</e2>."
sameAs(e1, e2)
Comment:

4207	"our <e1>approach</e1> does not make use of pre-and post-processing complications, including superpixels [7, 15] , proposals [15, 13] , or post-hoc refinement by random fields or local <e2>classifiers</e2> [7, 15] ."
Used-for(e1, e2)
Comment:

4208	"our model transfers recent success in <e1>classification</e1> [20, 31, 32] to dense prediction by reinterpreting <e2>classification</e2> nets as fully convolutional and fine-tuning from their learned representations."
sameAs(e1, e2)
Comment:

4209	"semantic segmentation faces an inherent tension between semantics and location: global <e1>information</e1> resolves what while local <e2>information</e2> resolves where."
sameAs(e1, e2)
Comment:

4210	"figure 1 : given a reference style <e1>image</e1> (a) and an input <e2>image</e2> (b), we seek to create an output image of the same scene as the input, but with the style of the reference image."
sameAs(e1, e2)
Comment:

4211	"figure 1 : given a reference style <e1>image</e1> (a) and an input image (b), we seek to create an output <e2>image</e2> of the same scene as the input, but with the style of the reference image."
sameAs(e1, e2)
Comment:

4212	"figure 1 : given a reference style <e1>image</e1> (a) and an input image (b), we seek to create an output image of the same scene as the input, but with the style of the reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

4213	"figure 1 : given a reference style image (a) and an <e1>input</e1> image (b), we seek to create an output image of the same scene as the <e2>input</e2>, but with the style of the reference image."
sameAs(e1, e2)
Comment:

4214	"figure 1 : given a reference style image (a) and an input <e1>image</e1> (b), we seek to create an output <e2>image</e2> of the same scene as the input, but with the style of the reference image."
sameAs(e1, e2)
Comment:

4215	"figure 1 : given a reference style image (a) and an input <e1>image</e1> (b), we seek to create an output image of the same scene as the input, but with the style of the reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

4216	"figure 1 : given a reference style image (a) and an input image (b), we seek to create an output <e1>image</e1> of the same scene as the input, but with the style of the reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

4217	"for example, consider an <e1>image</e1> with less sky visible in the input <e2>image</e2>; a transfer that ignores the difference in context between style and input may cause the style of the sky to "spill over" the rest of the picture."
sameAs(e1, e2)
Comment:

4218	"for example, consider an image with less sky visible in the <e1>input</e1> image; a transfer that ignores the difference in context between style and <e2>input</e2> may cause the style of the sky to "spill over" the rest of the picture."
sameAs(e1, e2)
Comment:

4219	"we demonstrate the effectiveness of our approach with satisfying photorealistic style <e1>transfers</e1> for a broad variety of scenarios including <e2>transfer</e2> of the time of day, weather, season, and artistic edits."
sameAs(e1, e2)
Comment:

4220	"the neural style algorithm [5] (c) successfully <e1>transfers</e1> colors, but also introduces distortions that make the output look like a painting, which is undesirable in the context of photo style <e2>transfer</e2>."
sameAs(e1, e2)
Comment:

4221	"on the right (e), we show 3 insets of (b), (c), and ( introduction photographic style <e1>transfer</e1> is a long-standing problem that seeks to <e2>transfer</e2> the style of a reference style photo onto another input picture."
sameAs(e1, e2)
Comment:

4222	"so far, existing techniques are either limited in the diversity of scenes or transfers that <e1>they</e1> can handle or in the faithfulness of the stylistic match <e2>they</e2> achieve."
sameAs(e1, e2)
Comment:

4223	"our discriminability loss is derived from the ability of a (pre-trained) retrieval model to match the correct <e1>image</e1> to its caption significantly stronger than any other <e2>image</e2> in a set, and vice versa (caption to correct image above other captions)."
sameAs(e1, e2)
Comment:

4224	"our discriminability loss is derived from the ability of a (pre-trained) retrieval model to match the correct <e1>image</e1> to its caption significantly stronger than any other image in a set, and vice versa (caption to correct <e2>image</e2> above other captions)."
sameAs(e1, e2)
Comment:

4225	"our discriminability loss is derived from the ability of a (pre-trained) retrieval model to match the correct image to its caption significantly stronger than any <e1>other</e1> image in a set, and vice versa (caption to correct image above <e2>other</e2> captions)."
sameAs(e1, e2)
Comment:

4226	"our discriminability loss is derived from the ability of a (pre-trained) retrieval model to match the correct image to its caption significantly stronger than any other <e1>image</e1> in a set, and vice versa (caption to correct <e2>image</e2> above other captions)."
sameAs(e1, e2)
Comment:

4227	"language-based <e1>measures</e1> like bleu reward machine captions for mimicking human captions, and so since, as we state above, human captions are discriminative, one could expect these <e2>measures</e2> to be correlated with descriptiveness."
sameAs(e1, e2)
Comment:

4228	"because <e1>these</e1> properties are somewhat vaguely defined, objective evaluation of caption quality remains a challenge, more so than evaluation of earlier established <e2>tasks</e2> like object detection or depth estimation."
Used-for(e1, e2)
Comment:

4229	"comparison to human ("gold standard") captions collected for test images is done by means of <e1>metrics</e1> borrowed from machine translation, such as bleu [1] , as well as new <e2>metrics</e2> introduced for the captioning task, such as cider [2] and spice [3] ."
sameAs(e1, e2)
Comment:

4230	"example captions generated by human, an existing automatic <e1>system</e1> (attn+cider [6] ), and a model trained with our proposed <e2>method</e2> (attn+cider+disc(1), see section 5) sider referring expressions [4] : captions for an image region, produced with the goal of unambiguously identifying the region within the image to the recipient."
Used-for(e1, e2)
Comment:

4231	"example captions generated by human, an existing automatic system (attn+cider [6] ), and a model trained with our proposed method (attn+cider+disc(1), see section 5) sider referring expressions [4] : captions for an <e1>image</e1> region, produced with the goal of unambiguously identifying the region within the <e2>image</e2> to the recipient."
sameAs(e1, e2)
Comment:

4232	"example captions generated by human, an existing automatic system (attn+cider [6] ), and a model trained with our proposed method (attn+cider+disc(1), see section 5) sider referring expressions [4] : captions for an image <e1>region</e1>, produced with the goal of unambiguously identifying the <e2>region</e2> within the image to the recipient."
sameAs(e1, e2)
Comment:

4233	"current approaches learn the parameters of the matching network by treating the problem as binary classification; given a patch in the left <e1>image</e1>, the task is to predict if a patch in the right <e2>image</e2> is the correct match."
sameAs(e1, e2)
Comment:

4234	"we demonstrate the effectiveness of our <e1>approach</e1> on the challenging kitti benchmark and show competitive results when exploiting smoothing <e2>techniques</e2>."
Compare(e1, e2)
Comment:

4235	"in this paper, we show that the overarching objectives of state-of-the-art cnn models (enforcing spatial coherence of the predicted saliency map and using both the local and global <e1>features</e1> in the optimization) can be achieved with a much simplified non-local deep feature (nldf) <e2>model</e2>."
Used-for(e1, e2)
Comment:

4236	"several applications benefit from saliency detection including <e1>image</e1> and video compression [14] , context aware <e2>image</e2> re-targeting [25] , scene parsing [50] , image resizing [3] , object detection [44] and segmentation [33] ."
sameAs(e1, e2)
Comment:

4237	"several applications benefit from saliency detection including <e1>image</e1> and video compression [14] , context aware image re-targeting [25] , scene parsing [50] , <e2>image</e2> resizing [3] , object detection [44] and segmentation [33] ."
sameAs(e1, e2)
Comment:

4238	"several applications benefit from saliency detection including image and video compression [14] , context aware <e1>image</e1> re-targeting [25] , scene parsing [50] , <e2>image</e2> resizing [3] , object detection [44] and segmentation [33] ."
sameAs(e1, e2)
Comment:

4239	"the nldf model evaluates an input image in 0.08s, a speed gain of 18 to 100 times as compared to other <e1>state</e1>-of-the-art deep learning methods, while being on par with <e2>state</e2>-of-the-art evaluation performance on the msra-b [30] , hku-is [25] , pascal-s [27] , dut-omron [49] , ecssd [48] and sod [32] benchmark datasets."
sameAs(e1, e2)
Comment:

4240	"traditional methods typically extract local pixel-wise or region-wise <e1>features</e1> and compare it with global <e2>features</e2>."
sameAs(e1, e2)
Comment:

4241	"unsupervised approaches is <e1>that</e1> they can be trained end-toend using simple optimization functions <e2>that</e2> combine local and deep features."
sameAs(e1, e2)
Comment:

4242	"while some methods apply a straight forward convolutional neural net (cnn) <e1>model</e1> [36] , others have proposed a <e2>model</e2> tailored to the saliency detection problem [25, 26, 29, 43, 50] ."
sameAs(e1, e2)
Comment:

4243	"1 a neural <e1>image</e1> captioning approach [20] describes the <e2>image</e2> as "a dog is sitting on a couch with a toy.""
sameAs(e1, e2)
Comment:

4244	"while this may suffice for common <e1>scenes</e1>, images that differ from canonical <e2>scenes</e2> -given the diversity in our visual world, there are plenty of such images -tend to be underserved by these models."
sameAs(e1, e2)
Comment:

4245	"while this may suffice for common scenes, <e1>images</e1> that differ from canonical scenes -given the diversity in our visual world, there are plenty of such <e2>images</e2> -tend to be underserved by these models."
sameAs(e1, e2)
Comment:

4246	"if <e1>we</e1> take a step back -do <e2>we</e2> really need the language model to do the heavy lifting in image captioning?"
sameAs(e1, e2)
Comment:

4247	"the unprecedented progress we are seeing in <e1>object recognition</e1> 2 (e.g., object detection, semantic segmentation, instance segmentation, <e2>pose estimation</e2>), it seems like the vision pipeline can certainly do better than rely on just a firstglance gist of the scene."
Conjunction(e1, e2)
Comment:

4248	"today's <e1>approaches</e1> fall at the other extreme on the spectrum -the language generated by modern neural image captioning <e2>approaches</e2> is much more natural but tends to be much less grounded in the image."
sameAs(e1, e2)
Comment:

4249	"today's approaches fall at the other extreme on the spectrum -the language generated by modern neural <e1>image</e1> captioning approaches is much more natural but tends to be much less grounded in the <e2>image</e2>."
sameAs(e1, e2)
Comment:

4250	"the visual word is essentially a token <e1>that</e1> will hold the slot for a word <e2>that</e2> is to describe a specific region in the image."
sameAs(e1, e2)
Comment:

4251	"images are encoded into a vector with a convolutional neural network (cnn), and captions are decoded from this vector using a <e1>recurrent neural network</e1> (<e2>rnn</e2>), with the entire system trained end-to-end."
sameAs(e1, e2)
Comment:

4252	"1 , the generated sequence may be "a <<e1>region</e1>−17> is sitting at a <<e2>region</e2>−123> with a <region−3>.""
sameAs(e1, e2)
Comment:

4253	"1 , the generated sequence may be "a <<e1>region</e1>−17> is sitting at a <region−123> with a <<e2>region</e2>−3>.""
sameAs(e1, e2)
Comment:

4254	"1 , the generated sequence may be "a <region−17> is sitting at a <<e1>region</e1>−123> with a <<e2>region</e2>−3>.""
sameAs(e1, e2)
Comment:

4255	"]>'s) are then filled in during a second stage that classifies each of the indicated <e1>regions</e1> (e.g., <<e2>region</e2>−17>→puppy, <region−123>→table), resulting in a final description of "a puppy is sitting at a table with a cake.""
sameAs(e1, e2)
Comment:

4256	"]>'s) are then filled in during a second stage that classifies each of the indicated <e1>regions</e1> (e.g., <region−17>→puppy, <<e2>region</e2>−123>→table), resulting in a final description of "a puppy is sitting at a table with a cake.""
sameAs(e1, e2)
Comment:

4257	"]>'s) are then filled in during a second stage that classifies each of the indicated regions (e.g., <<e1>region</e1>−17>→puppy, <<e2>region</e2>−123>→table), resulting in a final description of "a puppy is sitting at a table with a cake.""
sameAs(e1, e2)
Comment:

4258	"contributions: our contributions are as follows: • we present neural baby talk -a novel framework for visually grounded <e1>image</e1> captioning that explicitly localizes objects in the <e2>image</e2> while generating free-form natural language descriptions."
sameAs(e1, e2)
Comment:

4259	"• ours is a two-stage approach <e1>that</e1> first generates a hybrid template <e2>that</e2> contains a mix of (text) words and slots explicitly associated with image regions, and then fills in the slots with (text) words by recognizing the content in the corresponding image regions."
sameAs(e1, e2)
Comment:

4260	"• ours is a two-stage approach that first generates a hybrid template that contains a mix of (<e1>text</e1>) words and slots explicitly associated with image regions, and then fills in the slots with (<e2>text</e2>) words by recognizing the content in the corresponding image regions."
sameAs(e1, e2)
Comment:

4261	"• ours is a two-stage approach that first generates a hybrid template that contains a mix of (text) <e1>words</e1> and slots explicitly associated with image regions, and then fills in the slots with (text) <e2>words</e2> by recognizing the content in the corresponding image regions."
sameAs(e1, e2)
Comment:

4262	"• ours is a two-stage approach that first generates a hybrid template that contains a mix of (text) words and slots explicitly associated with <e1>image</e1> regions, and then fills in the slots with (text) words by recognizing the content in the corresponding <e2>image</e2> regions."
sameAs(e1, e2)
Comment:

4263	"• ours is a two-stage approach that first generates a hybrid template that contains a mix of (text) words and slots explicitly associated with image <e1>regions</e1>, and then fills in the slots with (text) words by recognizing the content in the corresponding image <e2>regions</e2>."
sameAs(e1, e2)
Comment:

4264	"• we propose a robust <e1>image</e1> captioning task to benchmark compositionality of <e2>image</e2> captioning algorithms where at test time the model encounters images containing known objects but in novel combinations (e.g., the model has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)."
sameAs(e1, e2)
Comment:

4265	"• we propose a robust image captioning <e1>task</e1> to benchmark compositionality of image captioning algorithms where at test time the <e2>model</e2> encounters images containing known objects but in novel combinations (e.g., the model has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)."
Evaluate-for(e1, e2)
Comment:

4266	"• we propose a robust image captioning <e1>task</e1> to benchmark compositionality of image captioning algorithms where at test time the model encounters images containing known objects but in novel combinations (e.g., the <e2>model</e2> has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)."
Evaluate-for(e1, e2)
Comment:

4267	"• we propose a robust image captioning task to benchmark compositionality of image captioning algorithms where at test <e1>time</e1> the model encounters images containing known objects but in novel combinations (e.g., the model has seen dogs on couches and people at tables during training, but at test <e2>time</e2> encounters a dog at a table)."
sameAs(e1, e2)
Comment:

4268	"• we propose a robust image captioning task to benchmark compositionality of image captioning algorithms where at test time the <e1>model</e1> encounters images containing known objects but in novel combinations (e.g., the <e2>model</e2> has seen dogs on couches and people at tables during training, but at test time encounters a dog at a table)."
sameAs(e1, e2)
Comment:

4269	"• our proposed <e1>method</e1> achieves state-of-the-art performance on coco and flickr30k datasets on the standard image captioning task, and significantly outperforms existing approaches on the robust image captioning and novel object captioning <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

4270	"• our proposed method achieves state-of-the-art performance on coco and flickr30k <e1>datasets</e1> on the standard image captioning task, and significantly outperforms existing <e2>approaches</e2> on the robust image captioning and novel object captioning tasks."
Used-for(e1, e2)
Comment:

4271	"• our proposed method achieves state-of-the-art performance on coco and flickr30k datasets on the standard <e1>image</e1> captioning task, and significantly outperforms existing approaches on the robust <e2>image</e2> captioning and novel object captioning tasks."
sameAs(e1, e2)
Comment:

4272	"our method generates the sentence "template" with slot locations (illustrated with filled boxes) explicitly tied to <e1>image</e1> regions (drawn in the <e2>image</e2> in corresponding colors)."
sameAs(e1, e2)
Comment:

4273	"we empirically demonstrate <e1>that</e1> our approach is highly tolerant to significant proportions of noisy labels, and can effectively learn low-dimensional local subspaces <e2>that</e2> capture the data distribution."
sameAs(e1, e2)
Comment:

4274	"given a <e1>distance</e1> between the deep representations of the video and selected key frames, our goal is to optimize the frame selector such that this <e2>distance</e2> is minimized over training examples."
sameAs(e1, e2)
Comment:

4275	"hence, we resort to the generative adversarial framework [8] , which extends the aforementioned video summarization <e1>network</e1> with an additional discriminator <e2>network</e2>."
sameAs(e1, e2)
Comment:

4276	"the auto-encoder <e1>lstm</e1> and the frame selector are jointly trained so as to maximally confuse the discriminator <e2>lstm</e2> -i.e., they are cast in a role of the discriminator's adversary -such that the discriminator has a high error rate in recognizing between the original and reconstructed videos."
sameAs(e1, e2)
Comment:

4277	"evaluation on four benchmark <e1>datasets</e1>, consisting of videos showing diverse events in first-and third-person views, demonstrates our competitive performance in comparison to fully supervised state-of-the-art <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

4278	"given a sequence of <e1>video</e1> frames, our goal is to select a sparse subset of frames such that a representation error between the <e2>video</e2> and its summary is minimal."
sameAs(e1, e2)
Comment:

4279	"domain of <e1>videos</e1> to be summarized is a priori known (e.g., first-person <e2>videos</e2>) [18] , or when ground-truth annotations of key frames are provided in training data based on attention, aesthetics, quality, landmark presence, and certain object occurrences and motions [9] ."
sameAs(e1, e2)
Comment:

4280	"keeping the surviving <e1>parameters</e1> fixed, the freed up <e2>parameters</e2> are modified for learning a new task."
sameAs(e1, e2)
Comment:

4281	"by using the <e1>task</e1>-specific parameter masks generated by pruning, our models are able to maintain the same level of accuracy even after the addition of multiple tasks, and incur a very low storage overhead per each new <e2>task</e2>."
sameAs(e1, e2)
Comment:

4282	"by using the task-specific parameter masks generated by pruning, our <e1>models</e1> are able to maintain the same level of accuracy even after the addition of multiple <e2>tasks</e2>, and incur a very low storage overhead per each new task."
Used-for(e1, e2)
Comment:

4283	"under this setting, the agent is required to acquire expertise on new <e1>tasks</e1> while maintaining its performance on previously learned <e2>tasks</e2>, ideally without the need to store large specialized models for each individual task."
sameAs(e1, e2)
Comment:

4284	"however, as features relevant to the new task are learned through modification of the network <e1>weights</e1>, <e2>weights</e2> important for prior tasks might be altered, leading to deterioration in performance referred to as "catastrophic forgetting" [4] ."
sameAs(e1, e2)
Comment:

4285	"without access to older training <e1>data</e1> due to the lack of storage space, <e2>data</e2> rights, or deployed nature of the agent, which are all very realistic constraints, naïve fine-tuning is not a viable option for continual learning."
sameAs(e1, e2)
Comment:

4286	"the <e1>former</e1> tries to preserve activations of the initial network while training on new data, while the <e2>latter</e2> penalizes the modification of parameters deemed to be important to prior tasks."
Conjunction(e1, e2)
Comment:

4287	"distinct from such prior work, we draw inspiration from approaches in <e1>network</e1> compression that have shown impressive results for reducing <e2>network</e2> size and computational footprint by eliminating redundant parameters [8, 17, 19, 20] ."
sameAs(e1, e2)
Comment:

4288	"we propose an <e1>approach</e1> that uses weight-based pruning <e2>techniques</e2> [7, 8] to free up redundant parameters across all layers of a deep network after it has been trained for a task, with minimal loss in accuracy."
Compare(e1, e2)
Comment:

4289	"we propose an <e1>approach</e1> that uses weight-based pruning techniques [7, 8] to free up redundant parameters across all <e2>layers</e2> of a deep network after it has been trained for a task, with minimal loss in accuracy."
Used-for(e1, e2)
Comment:

4290	"while deep neural networks tend to produce more transferable and <e1>domain</e1>-invariant features, previous works [8] have shown that the <e2>domain</e2> shift is only alleviated but not removed."
sameAs(e1, e2)
Comment:

4291	"to address this problem, known in the literature as latent domain discovery, previous works have proposed <e1>methods</e1> which simultaneously discover hidden source domains and use them to learn the target classification <e2>models</e2> [16, 21, 41] ."
Compare(e1, e2)
Comment:

4292	"this paper introduces the first deep approach able to automatically discover latent <e1>source domains</e1> in multi-<e2>source domain</e2> adaptation settings."
sameAs(e1, e2)
Comment:

4293	"our method is inspired by the recent works [6, 7] , which revisit batch normalization <e1>layers</e1> [23] for the purpose of domain adaptation, introducing specific domain alignment <e2>layers</e2> (da-layers)."
sameAs(e1, e2)
Comment:

4294	"our method is inspired by the recent works [6, 7] , which revisit batch normalization <e1>layers</e1> [23] for the purpose of domain adaptation, introducing specific domain alignment layers (da-<e2>layers</e2>)."
sameAs(e1, e2)
Comment:

4295	"our method is inspired by the recent works [6, 7] , which revisit batch normalization layers [23] for the purpose of <e1>domain adaptation</e1>, introducing specific domain alignment layers (<e2>da</e2>-layers)."
isA(e1, e2)
Comment:

4296	"our method is inspired by the recent works [6, 7] , which revisit batch normalization layers [23] for the purpose of domain adaptation, introducing specific domain alignment <e1>layers</e1> (da-<e2>layers</e2>)."
sameAs(e1, e2)
Comment:

4297	"however, to address the additional challenges of discovering and handling multiple latent domains, we propose a novel architecture which is able to (i) <e1>learn</e1> a set of assignment variables which associate source samples to a latent domain and (ii) exploit this information for aligning the distributions of the internal cnn feature representations and <e2>learn</e2> a robust target classifier (fig.2 )."
sameAs(e1, e2)
Comment:

4298	"our experimental evaluation shows that the proposed <e1>approach</e1> alleviates the domain discrepancy and outperforms previous multi-source da <e2>techniques</e2> on popular benchmarks, such as office-31 [36] and office-caltech [17] ."
Compare(e1, e2)
Comment:

4299	"throughout this article we will make use of the terms equivariance, invariance and <e1>covariance</e1> of a function f (·) with respect to a transformation g(·) in the following sense: -equivariance: f (g(·)) = g(f (·)), -invariance: f (g(·)) = f (·), -<e2>covariance</e2>: f (g(·)) = g ′ (f (·)), where g ′ (·) is a second transformation, which is itself a function of g(·)."
sameAs(e1, e2)
Comment:

4300	"throughout this article we will make use of the terms equivariance, invariance and covariance of a <e1>function</e1> f (·) with respect to a transformation g(·) in the following sense: -equivariance: f (g(·)) = g(f (·)), -invariance: f (g(·)) = f (·), -covariance: f (g(·)) = g ′ (f (·)), where g ′ (·) is a second transformation, which is itself a <e2>function</e2> of g(·)."
sameAs(e1, e2)
Comment:

4301	"by following a different flightpath, we would expect the car detector to score the exact same values over the same cars, just in their new position on the rotated <e1>image</e1>, independently from their new orientation along the <e2>image</e2> axes."
sameAs(e1, e2)
Comment:

4302	"in this case, we say that the problem is <e1>rotation</e1> equivariant: rotating the input is expected to result in the same <e2>rotation</e2> in the output."
sameAs(e1, e2)
Comment:

4303	"on the other hand, if <e1>we</e1> were confronted with a classification setting in which <e2>we</e2> are only interested in the presence or absence of cars in the whole scene, the classification score should remain the same, no matter the absolute orientation of the input scene."
sameAs(e1, e2)
Comment:

4304	"on the other hand, if we were confronted with a <e1>classification</e1> setting in which we are only interested in the presence or absence of cars in the whole scene, the <e2>classification</e2> score should remain the same, no matter the absolute orientation of the input scene."
sameAs(e1, e2)
Comment:

4305	"on the other hand, if we were confronted with a classification setting in which we are only interested in the presence or absence of cars in the whole <e1>scene</e1>, the classification score should remain the same, no matter the absolute orientation of the input <e2>scene</e2>."
sameAs(e1, e2)
Comment:

4306	"the more general case would be <e1>rotation</e1> covariance, in which the output changes as a function of the <e2>rotation</e2> of the input, with some predefined behavior."
sameAs(e1, e2)
Comment:

4307	"taking again the cars example, a <e1>rotation</e1> covariant problem would be to retrieve the absolute orientation of cars with respect to longitude and latitude: in this case, a <e2>rotation</e2> of the image should produce a change of the predicted angle."
sameAs(e1, e2)
Comment:

4308	"objects <e1>that</e1> are compatible must lie close; as a result, all shoes <e2>that</e2> match a given top are obliged to be close."
sameAs(e1, e2)
Comment:

4309	"this means <e1>that</e1> all shoes <e2>that</e2> match a given top must be close in shoe-top space, but can be very different in the general embedding space."
sameAs(e1, e2)
Comment:

4310	"this enables us to search for two pairs of shoes <e1>that</e1> 1) match the same top, and 2) look very different from one another <e2>that</e2> takes input items to an embedding space (e.g."
sameAs(e1, e2)
Comment:

4311	"the training process tries to ensure <e1>that</e1> similar items are embedded nearby, and items <e2>that</e2> are different have widely separated embeddings (i.e."
sameAs(e1, e2)
Comment:

4312	"worse, this strategy encourages improper triangles: if a pair of shoes match a hat, and <e1>that</e1> hat in turn matches a blouse, then a natural consequence of models without type-respecting embeddings is <e2>that</e2> the shoes are forced to also match the blouse."
sameAs(e1, e2)
Comment:

4313	"a representation for building outfits requires a method <e1>that</e1> can learn both notions of similarity (for example, when two tops are interchangeable) and compatibility (items of possibly different type <e2>that</e2> can go together in an outfit)."
sameAs(e1, e2)
Comment:

4314	"since many of the current fashion datasets either do not contain outfit compatibility <e1>annotations</e1> [20] , or are limited in size and the type of <e2>annotations</e2> they provide [12] , we collect our own dataset which we describe in section 3. in section 4 we discuss our type-aware embedding model, which enables us to perform complex queries on our data."
sameAs(e1, e2)
Comment:

4315	"since many of the current fashion datasets either do not contain outfit compatibility annotations [20] , or are limited in size and the type of annotations they provide [12] , <e1>we</e1> collect our own dataset which <e2>we</e2> describe in section 3. in section 4 we discuss our type-aware embedding model, which enables us to perform complex queries on our data."
sameAs(e1, e2)
Comment:

4316	"since many of the current fashion datasets either do not contain outfit compatibility annotations [20] , or are limited in size and the type of annotations they provide [12] , <e1>we</e1> collect our own dataset which we describe in section 3. in section 4 <e2>we</e2> discuss our type-aware embedding model, which enables us to perform complex queries on our data."
sameAs(e1, e2)
Comment:

4317	"since many of the current fashion datasets either do not contain outfit compatibility annotations [20] , or are limited in size and the type of annotations they provide [12] , we collect our own dataset which <e1>we</e1> describe in section 3. in section 4 <e2>we</e2> discuss our type-aware embedding model, which enables us to perform complex queries on our data."
sameAs(e1, e2)
Comment:

4318	"our experiments outlined in section 5 demonstrate the effectiveness of our approach, reporting a 4% <e1>improvement</e1> in a fill-in-the-blank outfit completion experiment, and a 5% <e2>improvement</e2> in an outfit compatibility prediction task over the prior state-of-the-art."
sameAs(e1, e2)
Comment:

4319	"an intentional <e1>motion</e1>), <e2>motion</e2> modeling is a complex task that should be ideally learned from observations."
sameAs(e1, e2)
Comment:

4320	"our focus in this paper is to learn models of human <e1>motion</e1> from <e2>motion</e2> capture (mocap) data."
sameAs(e1, e2)
Comment:

4321	"more specifically, <e1>we</e1> are interested in human motion prediction, where <e2>we</e2> forecast the most likely future 3d poses of a person given their past motion."
sameAs(e1, e2)
Comment:

4322	"this problem has received interest in a wide variety of fields, such as action prediction for sociallyaware robotics [21] , 3d people tracking within computer vision [13, 43] , <e1>motion</e1> generation for computer graphics [22] or modeling biological <e2>motion</e2> in psychology [42] ."
sameAs(e1, e2)
Comment:

4323	"traditional <e1>approaches</e1> have typically imposed expert knowledge about motion in their <e2>systems</e2> in the form of markovian assumptions [25, 32] , smoothness, or low dimensional embeddings [48] ."
Evaluate-for(e1, e2)
Comment:

4324	"recently, a family of methods based on deep <e1>recurrent neural networks</e1> (<e2>rnns</e2>) have shown good performance on this task while trying to be more agnostic in their assumptions."
sameAs(e1, e2)
Comment:

4325	"recent work has validated its performance via two complementary methods: (1) quantitative prediction error in the short-term, typically measured as a mean-squared loss in angle-space, and (2) qualitative <e1>motion</e1> synthesis for longer time horizons, where the goal is to generate feasible <e2>motion</e2>."
sameAs(e1, e2)
Comment:

4326	"the second criterion, most relevant for open-loop <e1>motion</e1> generation in graphics, is hard to evaluate quantitatively, because human <e2>motion</e2> is a highly non-deterministic process over long time horizons."
sameAs(e1, e2)
Comment:

4327	"as a result, their long-term <e1>results</e1> suffer from occasional unrealistic artifacts such as foot sliding, while their short-term <e2>results</e2> are not practical for tracking due to clear discontinuities in the first prediction."
sameAs(e1, e2)
Comment:

4328	"in fact, the discontinuity problem is so severe, <e1>that</e1> we have found <e2>that</e2> state-of-the-art methods are quantitatively outperformed by a range of simple baselines, including a constant pose predictor."
sameAs(e1, e2)
Comment:

4329	"in this work, we argue that (a) the results achieved by recent work are not fully satisfactory for either of these <e1>problems</e1>, and (b) trying to address both <e2>problems</e2> at once is very challenging, especially in the absence of a proper quantitative evaluation for long-term plausibility."
sameAs(e1, e2)
Comment:

4330	"we investigate the reasons for the poor performance of recent <e1>methods</e1> on this task by analyzing several factors such as the network architectures and the training procedures used in state-of-theart rnn <e2>methods</e2>."
sameAs(e1, e2)
Comment:

4331	"instead, <e1>we</e1> propose a simple approach that introduces realistic error in training time without any scheduling; <e2>we</e2> simply feed the predictions of the net, as it is done in test time."
sameAs(e1, e2)
Comment:

4332	"instead, we propose a simple approach that introduces realistic error in <e1>training time</e1> without any scheduling; we simply feed the predictions of the net, as it is done in test <e2>time</e2>."
Used-for(e1, e2)
Comment:

4333	"therefore, we propose a residual architecture that models first-order motion derivatives, which results in smooth and much more accurate <e1>short-term</e1> <e2>predictions</e2>."
Feature-of(e1, e2)
Comment:

4334	"in order to generate such frames, we follow a recurrent attentive approach similar to [3] , which focuses on one part of the frame at each <e1>time</e1> step for generation, and completes the frame generation over multiple <e2>time</e2> steps."
sameAs(e1, e2)
Comment:

4335	"in order to generate such frames, we follow a recurrent attentive approach similar to [3] , which focuses on one part of the frame at each time step for <e1>generation</e1>, and completes the frame <e2>generation</e2> over multiple time steps."
sameAs(e1, e2)
Comment:

4336	"in order to achieve the aforementioned objectives, the proposed network is required to not only account for the local correlations between any two consecutively generated frames but also has to ensure <e1>that</e1> the long-term spatiotemporal nature of the video is preserved so <e2>that</e2> the generated video is not just a loosely coupled set of frames but exhibits a holistic portrayal of the given caption."
sameAs(e1, e2)
Comment:

4337	"in order to achieve the aforementioned objectives, the proposed network is required to not only account for the local correlations between any two consecutively generated frames but also has to ensure that the long-term spatiotemporal nature of the <e1>video</e1> is preserved so that the generated <e2>video</e2> is not just a loosely coupled set of frames but exhibits a holistic portrayal of the given caption."
sameAs(e1, e2)
Comment:

4338	"in <e1>this</e1> work, we adopt a fresh perspective in <e2>this</e2> direction, by devising a network that learns these long-term and short-term video semantics separately but simultaneously."
sameAs(e1, e2)
Comment:

4339	"this paper makes the following contributions: (1) a novel <e1>methodology</e1> that can perform variable length semantic video generation with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) a <e2>methodology</e2> for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video generation and action recognition."
sameAs(e1, e2)
Comment:

4340	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic <e1>video</e1> generation with captions by separately and simultaneously learning the long-term and short-term context of the <e2>video</e2>; (2) a methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video generation and action recognition."
sameAs(e1, e2)
Comment:

4341	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic <e1>video</e1> generation with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) a methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised <e2>video</e2> generation and action recognition."
sameAs(e1, e2)
Comment:

4342	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic video <e1>generation</e1> with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) a methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on <e2>tasks</e2> such as unsupervised video generation and action recognition."
isA(e1, e2)
Comment:

4343	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic video <e1>generation</e1> with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) a methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video <e2>generation</e2> and action recognition."
sameAs(e1, e2)
Comment:

4344	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic video generation with captions by separately and simultaneously learning the long-term and short-term context of the <e1>video</e1>; (2) a methodology for selectively combining in-formation for conditioning at various levels of the architecture using appropriate attention mechanisms; and (3) a network architecture which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised <e2>video</e2> generation and action recognition."
sameAs(e1, e2)
Comment:

4345	"this paper makes the following contributions: (1) a novel methodology that can perform variable length semantic video generation with captions by separately and simultaneously learning the long-term and short-term context of the video; (2) a methodology for selectively combining in-formation for conditioning at various levels of the <e1>architecture</e1> using appropriate attention mechanisms; and (3) a network <e2>architecture</e2> which learns a robust latent representation of videos and is able to perform competently on tasks such as unsupervised video generation and action recognition."
sameAs(e1, e2)
Comment:

4346	"in <e1>this</e1> work, we propose an approach that attempts to provide <e2>this</e2> control and streamlines video generation by using captions."
sameAs(e1, e2)
Comment:

4347	"since a video can be arbitrarily long, <e1>generation</e1> of such videos necessitates a step-by-step <e2>generation</e2> mechanism."
sameAs(e1, e2)
Comment:

4348	"* equal contribution therefore, our model approaches video <e1>generation</e1> iteratively by creating one frame at a time, and conditioning the <e2>generation</e2> of the subsequent frames by the frames generated so far."
sameAs(e1, e2)
Comment:

4349	" introduction<e1> on</e1>e of the important factors in the success of deep learning for computer vision is the ease with which features, pre-trained on large datasets such as imagenet [6, 31] and places2 [37] , can be transferred to<e2> othe</e2>r computer vision figure 1 ."
Conjunction(e1, e2)
Comment:

4350	" introduction one of the important factors in the success of deep learning for computer<e1> visio</e1>n is the ease with which features, pre-trained on large datasets such as imagenet [6, 31] and places2 [37] , can be transferred to other computer<e2> visio</e2>n figure 1 ."
sameAs(e1, e2)
Comment:

4351	"however, one must question whether a target domain task requires such a large <e1>network</e1> and whether the resulting <e2>network</e2> is not highly redundant."
sameAs(e1, e2)
Comment:

4352	"approaches include methods based on weight quantization [13, 27] , weight removal from fully-connected [38] or convolutional <e1>layers</e1> [2] , compact representations of convolutional <e2>layers</e2> through tensor decompositions [7, 18, 1] , as well as training of thinner networks from predictions of a larger teacher network [16, 30] ."
sameAs(e1, e2)
Comment:

4353	"in <e1>these</e1> networks the truncated svd approach is applied to the fc6 and fc7 layers, and the authors showed that with only a small drop in performance <e2>these</e2> layers can be compressed to 25% of their original sizes."
sameAs(e1, e2)
Comment:

4354	"in these networks the truncated svd <e1>approach</e1> is applied to the fc6 and fc7 <e2>layers</e2>, and the authors showed that with only a small drop in performance these layers can be compressed to 25% of their original sizes."
Used-for(e1, e2)
Comment:

4355	"in these networks the truncated svd <e1>approach</e1> is applied to the fc6 and fc7 layers, and the authors showed that with only a small drop in performance these <e2>layers</e2> can be compressed to 25% of their original sizes."
Used-for(e1, e2)
Comment:

4356	"in these networks the truncated svd approach is applied to the fc6 and fc7 <e1>layers</e1>, and the authors showed that with only a small drop in performance these <e2>layers</e2> can be compressed to 25% of their original sizes."
sameAs(e1, e2)
Comment:

4357	"in this work we investigate <e1>network</e1> compression in the context of domain transfer from a <e2>network</e2> pre-trained on a large dataset to a smaller dataset representing the target domain."
sameAs(e1, e2)
Comment:

4358	"we first adapt a pretrained <e1>network</e1> with fine-tuning to a new target domain and then proceed to compress this <e2>network</e2>."
sameAs(e1, e2)
Comment:

4359	"most current <e1>compression</e1> methods do not consider activation statistics and base <e2>compression</e2> solely on the values in the weight matrices [20, 21, 7, 13, 27] ."
sameAs(e1, e2)
Comment:

4360	"we call our approach domain adaptive low <e1>rank</e1> (dalr) compression, since it is a low-<e2>rank</e2> approximation technique that takes into ac- 1 with activation statistics we refer to their direct usage during compression, but we do not explicitly model the statistical distribution."
sameAs(e1, e2)
Comment:

4361	"we call our approach domain adaptive low rank (dalr) <e1>compression</e1>, since it is a low-rank approximation technique that takes into ac- 1 with activation statistics we refer to their direct usage during <e2>compression</e2>, but we do not explicitly model the statistical distribution."
sameAs(e1, e2)
Comment:

4362	"we call our approach domain adaptive low rank (dalr) compression, since it is a low-rank approximation technique that takes into ac- 1 with activation statistics <e1>we</e1> refer to their direct usage during compression, but <e2>we</e2> do not explicitly model the statistical distribution."
sameAs(e1, e2)
Comment:

4363	"in section 3 <e1>we</e1> discuss in detail the motivation behind our network compression approach, and in section 4 <e2>we</e2> show how network compression can be formulated as a rank constraint regression problem."
sameAs(e1, e2)
Comment:

4364	"in section 3 we discuss in detail the motivation behind our <e1>network</e1> compression approach, and in section 4 we show how <e2>network</e2> compression can be formulated as a rank constraint regression problem."
sameAs(e1, e2)
Comment:

4365	"in section 3 we discuss in detail the motivation behind our network <e1>compression</e1> approach, and in section 4 we show how network <e2>compression</e2> can be formulated as a rank constraint regression problem."
sameAs(e1, e2)
Comment:

4366	"the most popular method to transfer the representations is by means of fine-tuning, where the <e1>network</e1> is initialized with the pre-trained <e2>network</e2> weights, after which they are further adjusted on the smaller target domain [26] ."
sameAs(e1, e2)
Comment:

4367	"", a model must identify which <e1>sphere</e1> is the large metal one, understand what it means for an object to be to the right of another, and apply this concept spatially to the attended <e2>sphere</e2>."
sameAs(e1, e2)
Comment:

4368	"this design closely <e1>models</e1> the compositional nature of visual reasoning <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

4369	"answering detailed questions about an image is a type of <e1>task</e1> which requires more sophisticated patterns of reasoning, and there has been a rapid recent proliferation of computer vision approaches for tackling the visual question answering (visual qa) <e2>task</e2> [6, 7] ."
sameAs(e1, e2)
Comment:

4370	"successful visual qa <e1>architectures</e1> must be able given a natural image and a textual question as input, our visual qa <e2>architecture</e2> outputs an answer."
sameAs(e1, e2)
Comment:

4371	"we base our architecture on the premise <e1>that</e1> the norm of the visual features correlates with their relevance, and <e2>that</e2> those feature vectors with high magnitudes correspond to image regions which contain important semantic content."
sameAs(e1, e2)
Comment:

4372	"we base our architecture on the premise that the norm of the visual <e1>features</e1> correlates with their relevance, and that those feature vectors with high <e2>magnitudes</e2> correspond to image regions which contain important semantic content."
Feature-of(e1, e2)
Comment:

4373	"this can improve accuracy by isolating important <e1>information</e1> and avoiding interference from unimportant <e2>information</e2>."
sameAs(e1, e2)
Comment:

4374	"however, there is a key downside of hard attention within a gradientbased learning framework, such as deep learning: because the choice of which information to process is discrete and thus non-differentiable, <e1>gradients</e1> cannot be backpropagated into the selection mechanism to support <e2>gradient</e2>-based optimization."
sameAs(e1, e2)
Comment:

4375	"there have been various efforts to address <e1>this</e1> shortcoming in visual attention [15] , attention to text [16] , and more general machine learning domains [17, 18, 19] , but <e2>this</e2> is still a very active area of research."
sameAs(e1, e2)
Comment:

4376	"in computer vision, however, there has been relatively little exploration of hard attention, where some <e1>information</e1> is selectively ignored, in spite of the success of soft attention, where <e2>information</e2> is re-weighted and aggregated, but never filtered out."
sameAs(e1, e2)
Comment:

4377	"this algorithm computes <e1>features</e1> over pairs of input <e2>features</e2> and thus scale quadratically with number of vectors in the feature map, highlighting the importance of feature selection."
sameAs(e1, e2)
Comment:

4378	"this algorithm computes features over pairs of input features and thus scale quadratically with number of vectors in the <e1>feature</e1> map, highlighting the importance of <e2>feature</e2> selection."
sameAs(e1, e2)
Comment:

4379	"for example, the "bag of <e1>features</e1>" <e2>model</e2> uses clustering on handcrafted local descriptors to produce good image-level features [11] ."
Used-for(e1, e2)
Comment:

4380	"for example, the "bag of <e1>features</e1>" model uses clustering on handcrafted local descriptors to produce good image-level <e2>features</e2> [11] ."
sameAs(e1, e2)
Comment:

4381	"an issue is that clustering methods have been primarily designed for linear models on top of fixed <e1>features</e1>, and they scarcely work if the <e2>features</e2> have to be learned simultaneously."
sameAs(e1, e2)
Comment:

4382	"in this paper, we make the following contributions: (i) a novel unsupervised <e1>method</e1> for the end-to-end learning of convnets that works with any standard clustering algorithm, like k-means, and requires minimal additional steps; (ii) state-of-the-art performance on many standard transfer <e2>tasks</e2> used in unsupervised learning; (iii) performance above the previous state of the art when trained on an uncured image distribution; (iv) a discussion about the current evaluation protocol in unsupervised feature learning."
Used-for(e1, e2)
Comment:

4383	"in this paper, we make the following contributions: (i) a novel unsupervised method for the end-to-end learning of convnets that works with any standard clustering algorithm, like k-means, and requires minimal additional steps; (ii) <e1>state</e1>-of-the-art performance on many standard transfer tasks used in unsupervised learning; (iii) performance above the previous <e2>state</e2> of the art when trained on an uncured image distribution; (iv) a discussion about the current evaluation protocol in unsupervised feature learning."
sameAs(e1, e2)
Comment:

4384	"to achieve this, fake embedding featuresẽ are learned in an adversarial manner to match the distribution of the real embedding <e1>features</e1> e, where the encoded <e2>features</e2> from the input image are treated as real whilst the ones generated from the gaussian noise as fake (fig."
sameAs(e1, e2)
Comment:

4385	"in stage-i, the encoder of the multi-branched reconstruction network serves conditional image <e1>generation</e1> <e2>tasks</e2>, whereas in stage-ii the mapping functions learned through adversarial training (i.e."
isA(e1, e2)
Comment:

4386	"4 constructs a virtual market re-id dataset by fixing foreground <e1>features</e1> and changing background <e2>features</e2> and pose keypoints to generate samples of one identity."
sameAs(e1, e2)
Comment:

4387	"ation models, such as variational autoencoders (vae) [12] , <e1>generative adversarial networks</e1> (<e2>gans</e2>) [7] and autoregressive models (arms) (e.g."
sameAs(e1, e2)
Comment:

4388	"recently, ma et al [20] proposed an architecture to synthesize novel person images in arbitrary <e1>poses</e1> given as input an image of that person and a new <e2>pose</e2>."
sameAs(e1, e2)
Comment:

4389	" introduction many important<e1> problem</e1>s in image processing and computer vision can be phrased as linear inverse<e2> problem</e2>s where the desired quantity u cannot be observed directly but needs to be determined from measurements f that relate to u via a linear operator a, i.e."
sameAs(e1, e2)
Comment:

4390	"the free <e1>parameters</e1> of g are learned by using large amounts of training data and fitting the <e2>parameters</e2> to the ground truth data via a large-scale optimization problem."
sameAs(e1, e2)
Comment:

4391	"the free parameters of g are learned by using large amounts of training <e1>data</e1> and fitting the parameters to the ground truth <e2>data</e2> via a large-scale optimization problem."
sameAs(e1, e2)
Comment:

4392	"a sufficient amount of training <e1>data</e1> needs to be acquired in such a way that it generalizes well enough to the test <e2>data</e2> the network is finally used for."
sameAs(e1, e2)
Comment:

4393	"f = au + n for some noise n. in almost all practically relevant applications the <e1>solution</e1> is very sensitive to the input data, and the underlying continuous <e2>problem</e2> is ill-posed."
Used-for(e1, e2)
Comment:

4394	"finally, while it is very quick and easy to change the linear operator a in variational <e1>methods</e1> like equation (1), learning based <e2>methods</e2> require a costly training as soon as the operator a changes."
sameAs(e1, e2)
Comment:

4395	"the latter motivates the idea to combine the advantages of energy minimization methods <e1>that</e1> are flexible to changes of the data term with the powerful representation of natural images <e2>that</e2> can be obtained via deep learning."
sameAs(e1, e2)
Comment:

4396	"it was observed in [40, 18] that modern convex optimization algorithms for solving equation (1) merely depend on the proximal operator of the regularization r, which motivated the authors to replace this step by general designed denoising <e1>algorithms</e1> such as the non-local means (nlm) [3] or bm3d [7] <e2>algorithms</e2>."
sameAs(e1, e2)
Comment:

4397	"for the sake of completeness, we have to mention <e1>methods</e1> such as [41] who apply the contrary approach and use variational <e2>methods</e2> as boilerplate models to design their network architecture."
sameAs(e1, e2)
Comment:

4398	"for the sake of completeness, we have to mention <e1>methods</e1> such as [41] who apply the contrary approach and use variational methods as boilerplate <e2>models</e2> to design their network architecture."
Compare(e1, e2)
Comment:

4399	"for the sake of completeness, we have to mention methods such as [41] who apply the contrary <e1>approach</e1> and use variational <e2>methods</e2> as boilerplate models to design their network architecture."
Compare(e1, e2)
Comment:

4400	"for the sake of completeness, we have to mention methods such as [41] who apply the contrary <e1>approach</e1> and use variational methods as boilerplate <e2>models</e2> to design their network architecture."
Used-for(e1, e2)
Comment:

4401	"for the sake of completeness, we have to mention methods such as [41] who apply the contrary approach and use variational <e1>methods</e1> as boilerplate <e2>models</e2> to design their network architecture."
Compare(e1, e2)
Comment:

4402	"our contributions are: • we demonstrate <e1>that</e1> using a fixed denoising network as a proximal operator in the primal-dual hybrid gradient (pdhg) method yields state-of-the-art results close to the performance of methods <e2>that</e2> trained a problem-specific network."
sameAs(e1, e2)
Comment:

4403	"our contributions are: • we demonstrate that using a fixed denoising <e1>network</e1> as a proximal operator in the primal-dual hybrid gradient (pdhg) method yields state-of-the-art results close to the performance of methods that trained a problem-specific <e2>network</e2>."
sameAs(e1, e2)
Comment:

4404	"our contributions are: • we demonstrate that using a fixed denoising network as a proximal operator in the primal-dual hybrid gradient (pdhg) <e1>method</e1> yields state-of-the-art results close to the performance of <e2>methods</e2> that trained a problem-specific network."
Compare(e1, e2)
Comment:

4405	"our contributions are: • we demonstrate that using a fixed denoising network as a proximal operator in the primal-dual hybrid gradient (pdhg) method yields state-of-the-art <e1>results</e1> close to the performance of <e2>methods</e2> that trained a problem-specific network."
Compare(e1, e2)
Comment:

4406	"our proposed method is based on leveraging context <e1>models</e1>, which were previously used as techniques to im-prove coding rates for already-trained <e2>models</e2> [4, 20, 9, 14] , directly as an entropy term in the optimization."
sameAs(e1, e2)
Comment:

4407	"we concurrently train the auto-<e1>encoder</e1> and the context model with respect to each other, where the context model learns a convolutional probabilistic model of the image representation in the auto-<e2>encoder</e2>, while the auto-encoder uses it for entropy estimation to navigate the rate-distortion trade-off."
sameAs(e1, e2)
Comment:

4408	"we concurrently train the auto-<e1>encoder</e1> and the context model with respect to each other, where the context model learns a convolutional probabilistic model of the image representation in the auto-encoder, while the auto-<e2>encoder</e2> uses it for entropy estimation to navigate the rate-distortion trade-off."
sameAs(e1, e2)
Comment:

4409	"we concurrently train the auto-encoder and the context <e1>model</e1> with respect to each other, where the context <e2>model</e2> learns a convolutional probabilistic model of the image representation in the auto-encoder, while the auto-encoder uses it for entropy estimation to navigate the rate-distortion trade-off."
sameAs(e1, e2)
Comment:

4410	"we concurrently train the auto-encoder and the context model with respect to each other, where the context model learns a convolutional probabilistic model of the image representation in the auto-<e1>encoder</e1>, while the auto-<e2>encoder</e2> uses it for entropy estimation to navigate the rate-distortion trade-off."
sameAs(e1, e2)
Comment:

4411	"furthermore, we generalize our formulation to spatially-aware networks, which use an importance map to spatially attend the bitrate <e1>representation</e1> to the most important regions in the compressed <e2>representation</e2>."
sameAs(e1, e2)
Comment:

4412	"while in lossless <e1>image</e1> compression the compression rate is limited by the requirement that the original <e2>image</e2> should be perfectly reconstructible, in lossy image compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4413	"while in lossless <e1>image</e1> compression the compression rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy <e2>image</e2> compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4414	"while in lossless <e1>image</e1> compression the compression rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy image compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed <e2>image</e2>."
sameAs(e1, e2)
Comment:

4415	"while in lossless image <e1>compression</e1> the <e2>compression</e2> rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy image compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4416	"while in lossless image <e1>compression</e1> the compression rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy image <e2>compression</e2>, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4417	"while in lossless image compression the <e1>compression</e1> rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy image <e2>compression</e2>, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4418	"while in lossless image compression the compression rate is limited by the requirement that the original <e1>image</e1> should be perfectly reconstructible, in lossy <e2>image</e2> compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed image."
sameAs(e1, e2)
Comment:

4419	"while in lossless image compression the compression rate is limited by the requirement that the original <e1>image</e1> should be perfectly reconstructible, in lossy image compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed <e2>image</e2>."
sameAs(e1, e2)
Comment:

4420	"while in lossless image compression the compression rate is limited by the requirement that the original image should be perfectly reconstructible, in lossy <e1>image</e1> compression, a greater reduction in storage is enabled by allowing for some distortion in the reconstructed <e2>image</e2>."
sameAs(e1, e2)
Comment:

4421	"recently, deep neural networks (dnns) trained as <e1>image</e1> autoencoders for this task led to promising results, achieving better performance than many traditional techniques for <e2>image</e2> compression [19, 20, 17, 4, 2, 9] ."
sameAs(e1, e2)
Comment:

4422	"dnn-based learned <e1>compression</e1> systems is their adaptability to specific target domains such as areal images or stereo images, enabling even higher <e2>compression</e2> rates on these domains."
sameAs(e1, e2)
Comment:

4423	"dnn-based learned <e1>compression</e1> systems is their adaptability to specific target domains such as areal images or stereo images, enabling even higher compression rates on these <e2>domains</e2>."
isA(e1, e2)
Comment:

4424	"dnn-based learned compression systems is their adaptability to specific target domains such as areal images or stereo images, enabling even higher <e1>compression</e1> rates on these <e2>domains</e2>."
isA(e1, e2)
Comment:

4425	"however, the quality of the resulting <e1>model</e1> crucially relies on the expressiveness of the inference <e2>model</e2>."
sameAs(e1, e2)
Comment:

4426	"we introduce adversarial variational bayes (avb), a <e1>technique</e1> for training variational autoencoders with arbitrarily expressive inference <e2>models</e2>."
Compare(e1, e2)
Comment:

4427	"we achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihoodproblem as a two-player game, hence establishing a principled connection between vaes and <e1>generative adversarial networks</e1> (<e2>gans</e2>)."
sameAs(e1, e2)
Comment:

4428	"as generating new content requires a good understanding of the training <e1>data</e1> at hand, such <e2>models</e2> are often regarded as a key ingredient to unsupervised learning."
Used-for(e1, e2)
Comment:

4429	"in recent years, generative models have become more and 1 autonomous <e1>vision</e1> group, mpi tübingen 2 microsoft research cambridge 3 computer <e2>vision</e2> and geometry group, eth zürich."
sameAs(e1, e2)
Comment:

4430	"for example, a key <e1>problem</e1> in many applications is to detect, identify, and track (a) third-third <e2>problem</e2>: who can be seen in both cameras?"
sameAs(e1, e2)
Comment:

4431	"we are interested in two specific, related problems: (a) given <e1>one</e1> or more synchronized third-person videos of a scene, segment all the visible people and identify corresponding people across the different videos; and (b) given <e2>one</e2> or more synchronized third-person videos of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person videos."
sameAs(e1, e2)
Comment:

4432	"we are interested in two specific, related problems: (a) given one or more synchronized third-person <e1>videos</e1> of a scene, segment all the visible people and identify corresponding people across the different <e2>videos</e2>; and (b) given one or more synchronized third-person videos of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person videos."
sameAs(e1, e2)
Comment:

4433	"we are interested in two specific, related problems: (a) given one or more synchronized third-person <e1>videos</e1> of a scene, segment all the visible people and identify corresponding people across the different videos; and (b) given one or more synchronized third-person <e2>videos</e2> of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person videos."
sameAs(e1, e2)
Comment:

4434	"we are interested in two specific, related problems: (a) given one or more synchronized third-person <e1>videos</e1> of a scene, segment all the visible people and identify corresponding people across the different videos; and (b) given one or more synchronized third-person videos of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4435	"we are interested in two specific, related problems: (a) given one or more synchronized third-person videos of a <e1>scene</e1>, segment all the visible people and identify corresponding people across the different videos; and (b) given one or more synchronized third-person videos of a <e2>scene</e2> as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person videos."
sameAs(e1, e2)
Comment:

4436	"we are interested in two specific, related problems: (a) given one or more synchronized third-person videos of a scene, segment all the visible people and identify corresponding people across the different <e1>videos</e1>; and (b) given one or more synchronized third-person <e2>videos</e2> of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person videos."
sameAs(e1, e2)
Comment:

4437	"we are interested in two specific, related problems: (a) given one or more synchronized third-person videos of a scene, segment all the visible people and identify corresponding people across the different <e1>videos</e1>; and (b) given one or more synchronized third-person videos of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4438	"we are interested in two specific, related problems: (a) given one or more synchronized third-person videos of a scene, segment all the visible people and identify corresponding people across the different videos; and (b) given one or more synchronized third-person <e1>videos</e1> of a scene as well as a video that was taken by a wearable first-person camera, identify and segment the person who was wearing the camera in the third-person <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4439	"in a world of pervasive <e1>cameras</e1>, public spaces are often captured from multiple perspectives by <e2>cameras</e2> of different types, both fixed and mobile."
sameAs(e1, e2)
Comment:

4440	"investigators later want to reconstruct the incident by combining the first-person wearable camera <e1>videos</e1> with third-person views from surveillance cameras and civilian smartphone <e2>videos</e2> uploaded to social media."
sameAs(e1, e2)
Comment:

4441	"in any given frame of any given camera, they may want to identify: (1) fine-grained, pixel-level segmentation masks for all people of interest, including both the suspect and the officers (e.g., for activity or action recognition), (2) the <e1>instances</e1> in which one of the camera wearers (officers) was visible in another camera's view, and (3) <e2>instances</e2> of the same person appearing in different views at the same time."
sameAs(e1, e2)
Comment:

4442	"an important problem is to organize these heterogeneous collections of <e1>videos</e1> by finding connections between them, such as identifying correspondences between the people appearing in the <e2>videos</e2> and the people holding or wearing the cameras."
sameAs(e1, e2)
Comment:

4443	"we define a first-person camera to be a wearable camera for which <e1>we</e1> care about the identity of the camera wearer, while a third-person camera is either a static or wearable camera for which <e2>we</e2> are not interested in determining the wearer."
sameAs(e1, e2)
Comment:

4444	"our hypothesis is that simultaneous <e1>segmentation</e1> and matching is mutually beneficial: <e2>segmentation</e2> helps refine matching by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while matching helps locate a person of interest and produce better segmentation masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4445	"our hypothesis is that simultaneous <e1>segmentation</e1> and matching is mutually beneficial: segmentation helps refine matching by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while matching helps locate a person of interest and produce better <e2>segmentation</e2> masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4446	"our hypothesis is that simultaneous segmentation and <e1>matching</e1> is mutually beneficial: segmentation helps refine <e2>matching</e2> by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while matching helps locate a person of interest and produce better segmentation masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4447	"our hypothesis is that simultaneous segmentation and <e1>matching</e1> is mutually beneficial: segmentation helps refine matching by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while <e2>matching</e2> helps locate a person of interest and produce better segmentation masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4448	"our hypothesis is that simultaneous segmentation and matching is mutually beneficial: <e1>segmentation</e1> helps refine matching by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while matching helps locate a person of interest and produce better <e2>segmentation</e2> masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4449	"our hypothesis is that simultaneous segmentation and matching is mutually beneficial: segmentation helps refine <e1>matching</e1> by producing finer-grained appearance features (compared to bounding boxes), which are important in crowded scenes with many occlusions, while <e2>matching</e2> helps locate a person of interest and produce better segmentation masks, which in turn help in tasks like activity and action recognition."
sameAs(e1, e2)
Comment:

4450	"in this paper, we wish to solve <e1>two</e1> specific problems: (1) given <e2>two</e2> or more synchronized third-person videos of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera a corresponds with whom in camera b), and (2) given one or more synchronized third-person videos as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person videos."
sameAs(e1, e2)
Comment:

4451	"in this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person <e1>videos</e1> of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera a corresponds with whom in camera b), and (2) given one or more synchronized third-person <e2>videos</e2> as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person videos."
sameAs(e1, e2)
Comment:

4452	"in this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person <e1>videos</e1> of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera a corresponds with whom in camera b), and (2) given one or more synchronized third-person videos as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4453	"in this paper, we wish to solve two specific problems: (1) given two or more synchronized third-person videos of a scene, produce a pixel-level segmentation of each visible person and identify corresponding people across different views (i.e., determine who in camera a corresponds with whom in camera b), and (2) given one or more synchronized third-person <e1>videos</e1> as well as a first-person video taken by a mobile or wearable camera, segment and identify the camera wearer in the third-person <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4454	"we find that solving these two problems simultaneously is mutually beneficial, because better fine-grained <e1>segmentation</e1> allows us to better perform matching across views, and information from multiple views helps us perform more accurate <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

4455	"we evaluate our <e1>approach</e1> on two challenging datasets of interacting people captured from multiple wearable cameras, and show that our proposed <e2>method</e2> performs significantly better than the stateof-the-art on both person segmentation and identification."
Compare(e1, e2)
Comment:

4456	"in a world with so many <e1>cameras</e1>, it will be commonplace for a scene to be simultaneously recorded by multiple <e2>cameras</e2> of different types."
sameAs(e1, e2)
Comment:

4457	"the <e1>method</e1>, illustrated in figure 1 , takes into account information of the environment by performing series of experiments to understand which parts of the <e2>model</e2> should be placed on which device, and how to arrange the computations so that the communication is optimized."
Used-for(e1, e2)
Comment:

4458	"the execution <e1>time</e1> is then used as a reward signal to train the recurrent model so that it gives better proposals over <e2>time</e2>."
sameAs(e1, e2)
Comment:

4459	"single-step measurements show <e1>that</e1> scotch (pellegrini, 2009 ) yields disappointing results on all three benchmarks, suggesting <e2>that</e2> their graph-based heuristics are not flexible enough for them."
sameAs(e1, e2)
Comment:

4460	"our main result is <e1>that</e1> on inception-v3 for imagenet classification, and on rnn lstm, for language modeling and neural machine translation, our model finds non-trivial device placements <e2>that</e2> outperform hand-crafted heuristics and traditional algorithmic methods."
sameAs(e1, e2)
Comment:

4461	"our main result is that on inception-v3 for imagenet classification, and on rnn lstm, for <e1>language modeling</e1> and neural machine translation, our <e2>model</e2> finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic methods."
isA(e1, e2)
Comment:

4462	"our main result is that on inception-v3 for imagenet classification, and on rnn lstm, for language modeling and neural machine translation, our <e1>model</e1> finds non-trivial device placements that outperform hand-crafted heuristics and traditional algorithmic <e2>methods</e2>."
Compare(e1, e2)
Comment:

4463	"introduction over the past few years, <e1>neural networks</e1> have proven to be a general and effective tool for many practical problems, such as <e2>image classification</e2> (krizhevsky et al, 2012; szegedy et al, 2015; he et al, 2016) , speech recognition graves & jaitly, 2014; hannun et al, 2014; chan et al, 2015) , machine translation cho et al, 2014; proceedings of the 34 th international conference on machine learning, sydney, australia, pmlr 70, 2017 ."
Used-for(e1, e2)
Comment:

4464	"introduction over the past few years, <e1>neural networks</e1> have proven to be a general and effective tool for many practical problems, such as image classification (krizhevsky et al, 2012; szegedy et al, 2015; he et al, 2016) , <e2>speech recognition</e2> graves & jaitly, 2014; hannun et al, 2014; chan et al, 2015) , machine translation cho et al, 2014; proceedings of the 34 th international conference on machine learning, sydney, australia, pmlr 70, 2017 ."
Used-for(e1, e2)
Comment:

4465	"introduction over the past few years, <e1>neural networks</e1> have proven to be a general and effective tool for many practical problems, such as image classification (krizhevsky et al, 2012; szegedy et al, 2015; he et al, 2016) , speech recognition graves & jaitly, 2014; hannun et al, 2014; chan et al, 2015) , machine translation cho et al, 2014; proceedings of the 34 th international conference on <e2>machine learning</e2>, sydney, australia, pmlr 70, 2017 ."
isA(e1, e2)
Comment:

4466	"introduction over the past few years, neural networks have proven to be a general and effective tool for many practical problems, such as image classification (krizhevsky et al, 2012; szegedy et al, 2015; he et al, 2016) , <e1>speech recognition</e1> graves & jaitly, 2014; hannun et al, 2014; chan et al, 2015) , <e2>machine translation</e2> cho et al, 2014; proceedings of the 34 th international conference on machine learning, sydney, australia, pmlr 70, 2017 ."
Conjunction(e1, e2)
Comment:

4467	"a key takeaway from <e1>these</e1> works is that multiple <e2>tasks</e2>, and thus multiple types of supervision, helps achieve better performance with the same input."
Used-for(e1, e2)
Comment:

4468	"our proposed <e1>method</e1> generalizes across multiple <e2>tasks</e2> and shows dramatically improved performance over baseline methods for categories with few training examples."
Used-for(e1, e2)
Comment:

4469	"while <e1>action</e1> detection, which quickly decides on the present <e2>action</e2> within a short time window, is fast enough to run in real-time, activity understanding, which is concerned with longer-term activities that can span several seconds, requires the integration of the long-term context to achieve full accuracy."
sameAs(e1, e2)
Comment:

4470	"while action detection, which quickly decides on the present action within a short <e1>time</e1> window, is fast enough to run in real-<e2>time</e2>, activity understanding, which is concerned with longer-term activities that can span several seconds, requires the integration of the long-term context to achieve full accuracy."
sameAs(e1, e2)
Comment:

4471	"the state of the art in <e1>video</e1> understanding suffers from two problems: (1) the major part of reasoning is performed locally in the <e2>video</e2>, therefore, it misses important relationships within actions that span several seconds."
sameAs(e1, e2)
Comment:

4472	"in this paper, <e1>we</e1> also present a way to use our architecture in an online setting, where <e2>we</e2> provide a fast first guess on the action and refine it using the longer term context as a more complex activity establishes."
sameAs(e1, e2)
Comment:

4473	"we conducted experiments on various <e1>video</e1> understanding problems including action recognition and <e2>video</e2> captioning."
sameAs(e1, e2)
Comment:

4474	"although <e1>we</e1> just use rgb images as input, <e2>we</e2> obtain on-par or favorable performance compared to state-of-the-art approaches on most datasets."
sameAs(e1, e2)
Comment:

4475	"(2) while there are local methods with fast perframe <e1>processing</e1>, the <e2>processing</e2> of the whole video is not efficient and hampers fast video retrieval or online classification of long-term activities."
sameAs(e1, e2)
Comment:

4476	"(2) while there are local methods with fast perframe processing, the processing of the whole <e1>video</e1> is not efficient and hampers fast <e2>video</e2> retrieval or online classification of long-term activities."
sameAs(e1, e2)
Comment:

4477	"(2) while there are local methods with fast perframe processing, the processing of the whole video is not efficient and hampers fast video <e1>retrieval</e1> or online <e2>classification</e2> of long-term activities."
Conjunction(e1, e2)
Comment:

4478	"together with a sampling strategy, which exploits that neighboring frames are largely redundant, this yields high-quality action classification and <e1>video</e1> captioning at up to 230 videos per second, where each <e2>video</e2> can consist of a few hundred frames."
sameAs(e1, e2)
Comment:

4479	"the <e1>approach</e1> achieves competitive performance across all datasets while being 10x to 80x faster than state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

4480	"we demonstrate the effectiveness of the developed <e1>approach</e1> on two datasets, outperforming existing state-of-the-art <e2>techniques</e2> on both."
Compare(e1, e2)
Comment:

4481	"such perturbations are named "universal <e1>adversarial</e1> perturbations" (uap), because a single <e2>adversarial</e2> noise can perturb images from multiple classes."
sameAs(e1, e2)
Comment:

4482	"therefore, a more relevant <e1>task</e1> at hand is to <e2>model</e2> the distribution of adversarial perturbations."
Evaluate-for(e1, e2)
Comment:

4483	"the freedom from parametric assumptions on the distribution and the <e1>target</e1> distribution being unknown (no known samples from the <e2>target</e2> distribution of adversarial perturbations) make the gan framework a suitable choice for our task."
sameAs(e1, e2)
Comment:

4484	"• we demonstrate empirically <e1>that</e1> the learned model can capture the distribution of perturbations and generates perturbations <e2>that</e2> exhibit diversity, high fooling capacity and excellent cross model generalizability."
sameAs(e1, e2)
Comment:

4485	"• we demonstrate empirically that the learned <e1>model</e1> can capture the distribution of perturbations and generates perturbations that exhibit diversity, high fooling capacity and excellent cross <e2>model</e2> generalizability."
sameAs(e1, e2)
Comment:

4486	"the rest of the paper is organized as follows: section 2 details the proposed <e1>method</e1>, section 3 presents comprehensive experimentation to validate the utility of the proposed <e2>method</e2>, section 4 briefly discusses the existing related works, and finally section 5 concludes the paper."
sameAs(e1, e2)
Comment:

4487	"that is, the perturbations learned on one model can fool another <e1>model</e1> even if the second <e2>model</e2> has a different architecture or has been trained with different dataset [29, 9] ."
sameAs(e1, e2)
Comment:

4488	"inspired by these considerations, in this work we propose to dynamically increase the <e1>number</e1> of units that are suppressed as a function of the <e2>number</e2> of gradient updates."
sameAs(e1, e2)
Comment:

4489	"indeed, such a gigantic number of parameters is likely to produce weights <e1>that</e1> are so specialized to the training examples <e2>that</e2> the network's generalization capability may be extremely poor."
sameAs(e1, e2)
Comment:

4490	"in this paper, we show <e1>that</e1> supplementing the usual pixel-wise loss by a topology loss <e2>that</e2> promotes results with appropriate topological characteristics yields a substantial performance increase without having to change the network architecture."
sameAs(e1, e2)
Comment:

4491	"we show that, unlike in the recent methods [19, 25] , sharing the same architecture and <e1>parameters</e1> across all refinement steps, instead of instantiating a new network each time, results in state of the art performance and enables keeping the number of <e2>parameters</e2> constant irrespectively of the number of iterations."
sameAs(e1, e2)
Comment:

4492	"we show that, unlike in the recent methods [19, 25] , sharing the same architecture and parameters across all refinement steps, instead of instantiating a new network each time, results in state of the art performance and enables keeping the <e1>number</e1> of parameters constant irrespectively of the <e2>number</e2> of iterations."
sameAs(e1, e2)
Comment:

4493	"in <e1>this</e1> paper we claim that pixel-wise losses alone are unsuitable for <e2>this</e2> problem because of their inability to reflect the topological impact of mistakes in the final prediction."
sameAs(e1, e2)
Comment:

4494	"we also exploit a refinement pipeline that iteratively applies the same <e1>model</e1> over the previous delineation to refine the predictions at each step, while keeping the number of parameters and the complexity of the <e2>model</e2> constant."
sameAs(e1, e2)
Comment:

4495	"we show that our <e1>approach</e1> outperforms state-of-the-art <e2>methods</e2> on a wide range of data, from microscopy to aerial images."
Compare(e1, e2)
Comment:

4496	"we evaluate our method on the <e1>kitti</e1> [2] and pascal 3d+ [26] <e2>datasets</e2>."
isA(e1, e2)
Comment:

4497	"we introduce three additional performance metrics measuring the 3d box accuracy: <e1>distance</e1> to center of box, <e2>distance</e2> to the center of the closest bounding box face, and the overall bounding box overlap with the ground truth box, measured using 3d intersection over union (3d iou) score."
sameAs(e1, e2)
Comment:

4498	"2) a novel <e1>discrete</e1>-<e2>continuous</e2> cnn architecture called multibin regression for estimation of the object's orientation."
Conjunction(e1, e2)
Comment:

4499	"while recently developed 2d detection <e1>algorithms</e1> are capable of handling large variations in viewpoint and clutter, accurate 3d object detection largely remains an open <e2>problem</e2> despite some promising recent work."
Used-for(e1, e2)
Comment:

4500	"they exploit the observation <e1>that</e1> the appearance of objects changes as a function of viewpoint and <e2>that</e2> discretization of viewpoints (parametrized by azimuth and elevation) gives rise to sub-categories which can be trained discriminatively [23] ."
sameAs(e1, e2)
Comment:

4501	"our simple and efficient <e1>method</e1> is suitable for many real world <e2>applications</e2> including selfdriving vehicles."
Used-for(e1, e2)
Comment:

4502	"abstract we address the problem of <e1>distance</e1> metric learning (dml) introduction <e2>distance</e2> metric learning (dml) is a major tool for a variety of problems in computer vision."
sameAs(e1, e2)
Comment:

4503	"abstract we address the problem of distance <e1>metric</e1> learning (dml) introduction distance <e2>metric</e2> learning (dml) is a major tool for a variety of problems in computer vision."
sameAs(e1, e2)
Comment:

4504	"another commonly used family of losses is triplet loss, which is defined by a triplet of <e1>data</e1> points: an anchor point, and a similar and dissimilar <e2>data</e2> points."
sameAs(e1, e2)
Comment:

4505	"on the other hand, top-down <e1>methods</e1> [10, 18, 23, 40] start by detecting people first and then for each person detection, a single-person pose estimation <e2>method</e2> (e.g."
Compare(e1, e2)
Comment:

4506	"detecting body joints conditioned on the information that there is a single person in the given input (the topdown <e1>approach</e1>), is typically a more costly process than grouping the detected joints (the bottom-up <e2>approach</e2>)."
sameAs(e1, e2)
Comment:

4507	"consequently, the top-down <e1>methods</e1> tend to be slower than the bottom-up <e2>methods</e2>, since they need to repeat the singleperson pose estimation for each person detection; however, they usually yield better accuracy than bottom-up methods."
sameAs(e1, e2)
Comment:

4508	"consequently, the top-down <e1>methods</e1> tend to be slower than the bottom-up methods, since they need to repeat the singleperson pose estimation for each person detection; however, they usually yield better accuracy than bottom-up <e2>methods</e2>."
sameAs(e1, e2)
Comment:

4509	"consequently, the top-down methods tend to be slower than the bottom-up <e1>methods</e1>, since they need to repeat the singleperson pose estimation for each person detection; however, they usually yield better accuracy than bottom-up <e2>methods</e2>."
sameAs(e1, e2)
Comment:

4510	"consequently, the top-down methods tend to be slower than the bottom-up methods, since <e1>they</e1> need to repeat the singleperson pose estimation for each person detection; however, <e2>they</e2> usually yield better accuracy than bottom-up methods."
sameAs(e1, e2)
Comment:

4511	"our <e1>method</e1> is based on a multi-task learning <e2>model</e2>, which can jointly handle the person detection, person segmentation and pose estimation problems."
Used-for(e1, e2)
Comment:

4512	"to emphasize its multiperson and multi-<e1>task</e1> aspects of our <e2>model</e2>, we named it as "multiposenet."
Evaluate-for(e1, e2)
Comment:

4513	"in this paper, we present multiposenet, a novel bottom-up multi-person pose estimation architecture that combines a multi-<e1>task</e1> <e2>model</e2> with a novel assignment method."
Evaluate-for(e1, e2)
Comment:

4514	"the novel assignment method is implemented by the <e1>pose</e1> residual network (prn) which receives keypoint and person detections, and produces accurate <e2>poses</e2> by assigning keypoints to person instances."
sameAs(e1, e2)
Comment:

4515	"on the coco keypoints dataset, our pose estimation <e1>method</e1> outperforms all previous bottom-up <e2>methods</e2> both in accuracy (+4-point map over previous best result) and speed; it also performs on par with the best top-down methods while being at least 4x faster."
Compare(e1, e2)
Comment:

4516	"on the coco keypoints dataset, our pose estimation <e1>method</e1> outperforms all previous bottom-up methods both in accuracy (+4-point map over previous best result) and speed; it also performs on par with the best top-down <e2>methods</e2> while being at least 4x faster."
Compare(e1, e2)
Comment:

4517	"on the coco keypoints dataset, our pose estimation method outperforms all previous bottom-up <e1>methods</e1> both in accuracy (+4-point map over previous best result) and speed; it also performs on par with the best top-down <e2>methods</e2> while being at least 4x faster."
sameAs(e1, e2)
Comment:

4518	"any <e1>solution</e1> to this <e2>problem</e2> has to tackle a few subproblems: (i) detecting body joints (or keypoints, as they are called in the widely used coco [36] dataset) such as wrists, ankles, etc., (ii) grouping these joints into person instances, or detecting people and (iii) assigning joints to person instances."
Used-for(e1, e2)
Comment:

4519	"any solution to this problem has to tackle a few subproblems: (i) detecting body joints (or keypoints, as they are called in the widely used coco [36] dataset) such as wrists, ankles, etc., (ii) grouping these joints into person <e1>instances</e1>, or detecting people and (iii) assigning joints to person <e2>instances</e2>."
sameAs(e1, e2)
Comment:

4520	"abstract a key solution to visual <e1>question answering</e1> (vqa) introduction there has been a significant progress in the study of visual <e2>question answering</e2> (vqa) over a short period of time since its introduction, showing rapid boost of performance for common benchmark datasets."
sameAs(e1, e2)
Comment:

4521	"given representations of an <e1>image</e1> and a question, it first generates an attention map on <e2>image</e2> regions for each question word and an attention map on question words for each image region."
sameAs(e1, e2)
Comment:

4522	"given representations of an <e1>image</e1> and a question, it first generates an attention map on image regions for each question word and an attention map on question words for each <e2>image</e2> region."
sameAs(e1, e2)
Comment:

4523	"given representations of an image and a question, it first generates an attention <e1>map</e1> on image regions for each question word and an attention <e2>map</e2> on question words for each image region."
sameAs(e1, e2)
Comment:

4524	"given representations of an image and a question, it first generates an attention map on <e1>image</e1> regions for each question word and an attention map on question words for each <e2>image</e2> region."
sameAs(e1, e2)
Comment:

4525	"given representations of an image and a question, it first generates an attention map on image <e1>regions</e1> for each question word and an attention map on question words for each image <e2>region</e2>."
sameAs(e1, e2)
Comment:

4526	"starting from initial <e1>representations</e1> of an input image and question, each dense co-attention layer in the layer stack updates the <e2>representations</e2>, which are inputted to the next layer."
sameAs(e1, e2)
Comment:

4527	"we call the entire <e1>network</e1> including all these components the dense coattention <e2>network</e2> (dcn)."
sameAs(e1, e2)
Comment:

4528	"since introduced by bahdanau et al [3] , attention has been playing an important role in solutions of various problems of artificial intelligence ranging from <e1>tasks</e1> using single modality (e.g., language, speech, and vision) to multimodal <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

4529	"since introduced by bahdanau et al [3] , attention has been playing an important role in solutions of various problems of artificial intelligence ranging from tasks using single modality (e.g., <e1>language</e1>, speech, and <e2>vision</e2>) to multimodal tasks."
Conjunction(e1, e2)
Comment:

4530	"right: in <e1>this</e1> work, we extend <e2>this</e2> view to structured inference, which consists of optimizing over a polytope m, the convex hull of all possible structures (depicted: the arborescence polytope, whose vertices are trees)."
sameAs(e1, e2)
Comment:

4531	"in this work, <e1>we</e1> make the following contributions: 1. <e2>we</e2> propose sparsemap: a new framework for sparse structured inference ( §3.1)."
sameAs(e1, e2)
Comment:

4532	"sparsemap automatically selects only a few global <e1>structures</e1>: it is situated between map inference, which picks a single structure, and marginal inference, which assigns nonzero probability to all <e2>structures</e2>, including implausible ones."
sameAs(e1, e2)
Comment:

4533	"4. we introduce a novel sparsemap loss for <e1>structured</e1> prediction, placing it into a family of loss functions which generalizes the crf and <e2>structured</e2> svm losses ( §4)."
sameAs(e1, e2)
Comment:

4534	"our experiments demonstrate that sparsemap is useful both for predicting <e1>structured</e1> outputs, as well as for learning latent <e2>structured</e2> representations."
sameAs(e1, e2)
Comment:

4535	"experiments in dependency parsing and natural language inference reveal competitive <e1>accuracy</e1>, improved interpretability, and the ability to capture natural language ambiguities, which is attractive for pipeline <e2>systems</e2>."
Evaluate-for(e1, e2)
Comment:

4536	"abstract <e1>backpropagation</e1>-based visualizations have been proposed to interpret convolutional neural networks (cnns), however a theory is missing to justify their behaviors: guided <e2>backpropagation</e2> (gbp) and deconvolutional network (deconvnet) generate more human-interpretable but less classsensitive visualizations than saliency map."
sameAs(e1, e2)
Comment:

4537	"(springenberg et al, 2014) proposed the guided backpropagation (gbp) <e1>method</e1> which combines the above two <e2>methods</e2>: by considering both the forward and backward relus, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations."
Compare(e1, e2)
Comment:

4538	"our experiments have confirmed previous observations (mahendran & vedaldi, 2016; selvaraju et al, 2016; samek et al, 2017) that saliency <e1>map</e1> is indeed very sensitive to the change of class labels, while gbp and deconvnet, though their visualization results are much cleaner than saliency <e2>map</e2>, remain almost the same given different class labels."
sameAs(e1, e2)
Comment:

4539	"the linear <e1>model</e1> explanation thus cannot answer questions regarding why gbp and deconvnet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear <e2>model</e2>."
sameAs(e1, e2)
Comment:

4540	"the linear model explanation thus cannot answer questions regarding why gbp and deconvnet outperform saliency <e1>map</e1> in terms of visual quality whereas they are less class-sensitive than saliency <e2>map</e2>, as both of them reduce to saliency map in a linear model."
sameAs(e1, e2)
Comment:

4541	"the linear model explanation thus cannot answer questions regarding why gbp and deconvnet outperform saliency <e1>map</e1> in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency <e2>map</e2> in a linear model."
sameAs(e1, e2)
Comment:

4542	"the linear model explanation thus cannot answer questions regarding why gbp and deconvnet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency <e1>map</e1>, as both of them reduce to saliency <e2>map</e2> in a linear model."
sameAs(e1, e2)
Comment:

4543	"therefore, <e1>we</e1> need a more complex model, which should at least capture the impact of both forward relu and backward relu, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, <e2>we</e2> can extract from these visualizations."
sameAs(e1, e2)
Comment:

4544	"therefore, we need a more complex model, which should at least capture the impact of both forward <e1>relu</e1> and backward <e2>relu</e2>, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations."
sameAs(e1, e2)
Comment:

4545	"we also find <e1>that</e1> it is the backward relu introduced by either gbp or deconvnet, together with the local connections in cnns <e2>that</e2> results in crisp visualizations."
sameAs(e1, e2)
Comment:

4546	"second, they only use parsing information for inference other than learning <e1>pose</e1> models and therefore do not strengthen <e2>pose</e2> models essentially."
sameAs(e1, e2)
Comment:

4547	"second, they only use parsing information for inference other than learning pose <e1>models</e1> and therefore do not strengthen pose <e2>models</e2> essentially."
sameAs(e1, e2)
Comment:

4548	"first, the <e1>parsing</e1> representations should be learned towards being beneficial to pose estimation, instead of solely learned from the <e2>parsing</e2> supervision."
sameAs(e1, e2)
Comment:

4549	"according to the above observations, we design a novel <e1>parsing</e1> induced learner (pil) that learns to fast adapt the pose estimation model conditioned on the <e2>parsing</e2> information extracted from a specific sample, and therefore effectively improves both performance and flexibility of the model."
sameAs(e1, e2)
Comment:

4550	"according to the above observations, we design a novel parsing induced learner (pil) that learns to fast adapt the pose estimation <e1>model</e1> conditioned on the parsing information extracted from a specific sample, and therefore effectively improves both performance and flexibility of the <e2>model</e2>."
sameAs(e1, e2)
Comment:

4551	"in particular, pil consists of two components: an encoder <e1>that</e1> encodes an input image into high-level parsing representations, and an adapter <e2>that</e2> learns to adapt parameters of the pose model by leveraging parsing representations."
sameAs(e1, e2)
Comment:

4552	"in particular, pil consists of two components: an encoder that encodes an input image into high-level <e1>parsing</e1> representations, and an adapter that learns to adapt parameters of the pose model by leveraging <e2>parsing</e2> representations."
sameAs(e1, e2)
Comment:

4553	"in particular, pil consists of two components: an encoder that encodes an input image into high-level parsing <e1>representations</e1>, and an adapter that learns to adapt parameters of the pose model by leveraging parsing <e2>representations</e2>."
sameAs(e1, e2)
Comment:

4554	"the adaptive parameters predicted by pil can help the <e1>pose</e1> model learn more tailored representations for estimating <e2>poses</e2> for each specific input, in which body part cues are effectively integrated for constraining joint locations and pose structures."
sameAs(e1, e2)
Comment:

4555	"the adaptive parameters predicted by pil can help the <e1>pose</e1> model learn more tailored representations for estimating poses for each specific input, in which body part cues are effectively integrated for constraining joint locations and <e2>pose</e2> structures."
sameAs(e1, e2)
Comment:

4556	"the adaptive parameters predicted by pil can help the pose model learn more tailored representations for estimating <e1>poses</e1> for each specific input, in which body part cues are effectively integrated for constraining joint locations and <e2>pose</e2> structures."
sameAs(e1, e2)
Comment:

4557	"moreover, pil can efficiently learn to adapt <e1>pose</e1> parameters in one-shot manner, yielding fast adaption of the base <e2>pose</e2> model according to parsing information."
sameAs(e1, e2)
Comment:

4558	"we implement pil by combining a parsing encoder <e1>network</e1> and a parameter adapter <e2>network</e2>, which can be directly applied to various deep pose models and across different datasets."
sameAs(e1, e2)
Comment:

4559	"the parameter adapter network is trained with supervision from <e1>human pose</e1> to learn adaptive parameters for boosting <e2>pose estimation</e2>."
isA(e1, e2)
Comment:

4560	"the <e1>parsing</e1> encoder is learned from both the human <e2>parsing</e2> and pose annotations."
sameAs(e1, e2)
Comment:

4561	"firstly and most importantly, we propose a novel <e1>parsing</e1> induced learner for efficiently learning to adapt pose estimation models by exploiting <e2>parsing</e2> information, achieving better pose estimation performance."
sameAs(e1, e2)
Comment:

4562	"firstly and most importantly, we propose a novel parsing induced learner for efficiently learning to adapt <e1>pose estimation</e1> models by exploiting parsing information, achieving better <e2>pose estimation</e2> performance."
sameAs(e1, e2)
Comment:

4563	"thirdly, with the help of pil, an hourglass network [25] based <e1>pose estimation</e1> model achieves new state-of-the-art on multiple benchmarks for both single-and multi-person <e2>pose estimation</e2>."
sameAs(e1, e2)
Comment:

4564	"however, they generally perform human body <e1>parsing</e1> and pose estimation separately and utilize <e2>parsing</e2> results to refine body joint localization as post processing."
sameAs(e1, e2)
Comment:

4565	"specifically, for a pixel (x, y) in the interpolated frame, <e1>this</e1> deep neural network takes two receptive field patches r 1 and r 2 centered at that pixel as input and estimates a convolution kernel k. <e2>this</e2> convolution kernel is used to convolve with the input patches p 1 and p 2 to synthesize the output pixel, as illustrated in figure 1 ."
sameAs(e1, e2)
Comment:

4566	"specifically, for a pixel (x, y) in the interpolated frame, this deep neural network takes two receptive field <e1>patches</e1> r 1 and r 2 centered at that pixel as input and estimates a convolution kernel k. this convolution kernel is used to convolve with the input <e2>patches</e2> p 1 and p 2 to synthesize the output pixel, as illustrated in figure 1 ."
sameAs(e1, e2)
Comment:

4567	"specifically, for a pixel (x, y) in the interpolated frame, this deep neural network takes two receptive field patches r 1 and r 2 centered at that pixel as <e1>input</e1> and estimates a convolution kernel k. this convolution kernel is used to convolve with the <e2>input</e2> patches p 1 and p 2 to synthesize the output pixel, as illustrated in figure 1 ."
sameAs(e1, e2)
Comment:

4568	"specifically, for a pixel (x, y) in the interpolated frame, this deep neural network takes two receptive field patches r 1 and r 2 centered at that pixel as input and estimates a <e1>convolution</e1> kernel k. this <e2>convolution</e2> kernel is used to convolve with the input patches p 1 and p 2 to synthesize the output pixel, as illustrated in figure 1 ."
sameAs(e1, e2)
Comment:

4569	"the main contribution of this paper is a robust <e1>video</e1> frame interpolation method that employs a fully deep convolutional neural network to produce high-quality <e2>video</e2> interpolation results."
sameAs(e1, e2)
Comment:

4570	"sechttp://graphics.cs.pdx.edu/project/adaconv ond, this frame interpolation deep convolutional neural network can be directly trained end to end using widely available video <e1>data</e1>, without any difficult-to-obtain ground truth <e2>data</e2> like optical flow."
sameAs(e1, e2)
Comment:

4571	"our method considers pixel interpolation as convolution over corresponding image patches in the <e1>two</e1> input video frames, and estimates the spatially-adaptive convolutional kernel using a deep fully convolutional neural * the first <e2>two</e2> authors contributed equally to this paper."
sameAs(e1, e2)
Comment:

4572	" introduction traditional video frame interpolation methods estimate optical<e1> flo</e1>w between input frames and synthesizing intermediate frames guided by optical<e2> flo</e2>w [3] ."
sameAs(e1, e2)
Comment:

4573	"to generate the <e1>kernels</e1> for all pixels in a 1080p video frame, the output <e2>kernels</e2> alone will require 26 gb of memory."
sameAs(e1, e2)
Comment:

4574	"our approach approximates each of these with a pair of 1d kernels, <e1>one</e1> horizontal and <e2>one</e2> vertical."
sameAs(e1, e2)
Comment:

4575	"our deep neural network is fully convolutional and can be trained end-to-end using widely available video <e1>data</e1> without any difficult-to-obtain meta <e2>data</e2> like optical flow."
sameAs(e1, e2)
Comment:

4576	"compared to the recent <e1>convolution</e1> approach that utilizes 2d kernels [36] (b), our separable <e2>convolution</e2> methods, especially the one with perceptual loss (d), incorporate 1d kernels that allow for full-frame interpolation and produce higher-quality results."
sameAs(e1, e2)
Comment:

4577	"compared to the recent convolution <e1>approach</e1> that utilizes 2d kernels [36] (b), our separable convolution <e2>methods</e2>, especially the one with perceptual loss (d), incorporate 1d kernels that allow for full-frame interpolation and produce higher-quality results."
Compare(e1, e2)
Comment:

4578	"compared to the recent convolution approach <e1>that</e1> utilizes 2d kernels [36] (b), our separable convolution methods, especially the one with perceptual loss (d), incorporate 1d kernels <e2>that</e2> allow for full-frame interpolation and produce higher-quality results."
sameAs(e1, e2)
Comment:

4579	"compared to the recent convolution approach that utilizes 2d <e1>kernels</e1> [36] (b), our separable convolution methods, especially the one with perceptual loss (d), incorporate 1d <e2>kernels</e2> that allow for full-frame interpolation and produce higher-quality results."
sameAs(e1, e2)
Comment:

4580	"cally, they estimate spatially-adaptive convolution <e1>kernels</e1> for each output pixel and convolve the <e2>kernels</e2> with the input frames to generate a new frame."
sameAs(e1, e2)
Comment:

4581	" introduction we are interested in learning representations (features) that are discriminative for semantic<e1> imag</e1>e understanding tasks such as<e2> classificatio</e2>n, detection, and segmentation."
Used-for(e1, e2)
Comment:

4582	"to obtain a supervision signal useful to learn to count, <e1>we</e1> exploit the following property: if <e2>we</e2> partition an image into non-overlapping regions, the number of visual primitives in each region should sum up to the number of primitives in the original image (see the example in fig."
sameAs(e1, e2)
Comment:

4583	"to obtain a supervision signal useful to learn to count, we exploit the following property: if we partition an <e1>image</e1> into non-overlapping regions, the number of visual primitives in each region should sum up to the number of primitives in the original <e2>image</e2> (see the example in fig."
sameAs(e1, e2)
Comment:

4584	"to obtain a supervision signal useful to learn to count, we exploit the following property: if we partition an image into non-overlapping <e1>regions</e1>, the number of visual primitives in each <e2>region</e2> should sum up to the number of primitives in the original image (see the example in fig."
sameAs(e1, e2)
Comment:

4585	"to obtain a supervision signal useful to learn to count, we exploit the following property: if we partition an image into non-overlapping regions, the <e1>number</e1> of visual primitives in each region should sum up to the <e2>number</e2> of primitives in the original image (see the example in fig."
sameAs(e1, e2)
Comment:

4586	"we make the hypothesis <e1>that</e1> the model needs to disentangle the image into high-level factors of variation, such <e2>that</e2> the complex relation between the original image and its regions is translated to a simple arithmetic operation [3, 35] ."
sameAs(e1, e2)
Comment:

4587	"we make the hypothesis that the model needs to disentangle the <e1>image</e1> into high-level factors of variation, such that the complex relation between the original <e2>image</e2> and its regions is translated to a simple arithmetic operation [3, 35] ."
sameAs(e1, e2)
Comment:

4588	"our contributions are: 1) <e1>we</e1> introduce a novel method to learn representations from data without manual annotation; 2) <e2>we</e2> propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) we show that the proposed methodology learns representations that perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4589	"our contributions are: 1) <e1>we</e1> introduce a novel method to learn representations from data without manual annotation; 2) we propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) <e2>we</e2> show that the proposed methodology learns representations that perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4590	"our contributions are: 1) we introduce a novel method to <e1>learn</e1> representations from data without manual annotation; 2) we propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) we show that the proposed methodology <e2>learns</e2> representations that perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4591	"our contributions are: 1) we introduce a novel method to learn <e1>representations</e1> from data without manual annotation; 2) we propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) we show that the proposed methodology learns <e2>representations</e2> that perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4592	"our contributions are: 1) we introduce a novel method to learn representations from data without manual annotation; 2) <e1>we</e1> propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) <e2>we</e2> show that the proposed methodology learns representations that perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4593	"our contributions are: 1) we introduce a novel method to learn representations from data without manual annotation; 2) we propose exploiting counting as a pretext task and demonstrate its relation to counting visual primitives; 3) we show <e1>that</e1> the proposed methodology learns representations <e2>that</e2> perform on par or exceed the state of the art in standard transfer learning benchmarks."
sameAs(e1, e2)
Comment:

4594	"a rationale behind self-supervised learning is <e1>that</e1> pretext tasks <e2>that</e2> relate the most to the final problems (e.g., classification and detection) will be more likely to build relevant representations."
sameAs(e1, e2)
Comment:

4595	"however, training such networks has been observed to be difficult in practice due to exploding and vanishing <e1>gradients</e1> when propagating error <e2>gradients</e2> through time (hochreiter et al, 2001) ."
sameAs(e1, e2)
Comment:

4596	"while explod- ing <e1>gradients</e1> can be mitigated with techniques like <e2>gradient</e2> clipping and normalization , vanishing gradients may be harder to deal with."
sameAs(e1, e2)
Comment:

4597	"while explod- ing <e1>gradients</e1> can be mitigated with techniques like gradient clipping and normalization , vanishing <e2>gradients</e2> may be harder to deal with."
sameAs(e1, e2)
Comment:

4598	"while explod- ing gradients can be mitigated with techniques like <e1>gradient</e1> clipping and normalization , vanishing <e2>gradients</e2> may be harder to deal with."
sameAs(e1, e2)
Comment:

4599	"as a result, sophisticated gated architectures like long-short term memory (lstm) <e1>networks</e1> (hochreiter & schmidhuber, 1997) and gated recurrent unit (gru) <e2>networks</e2>  have been developed."
sameAs(e1, e2)
Comment:

4600	"the structure of the paper is as follows: first <e1>we</e1> detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, <e2>we</e2> describe our experiments comparing the sru to popular gated alternatives, and we perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, we discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4601	"the structure of the paper is as follows: first <e1>we</e1> detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, we describe our experiments comparing the sru to popular gated alternatives, and <e2>we</e2> perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, we discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4602	"the structure of the paper is as follows: first <e1>we</e1> detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, we describe our experiments comparing the sru to popular gated alternatives, and we perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, <e2>we</e2> discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4603	"the structure of the paper is as follows: first we detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, <e1>we</e1> describe our experiments comparing the sru to popular gated alternatives, and <e2>we</e2> perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, we discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4604	"the structure of the paper is as follows: first we detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, <e1>we</e1> describe our experiments comparing the sru to popular gated alternatives, and we perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, <e2>we</e2> discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4605	"the structure of the paper is as follows: first we detail the architecture of the sru as well as provide several key intuitions and insights for its design; after, we describe our experiments comparing the sru to popular gated alternatives, and <e1>we</e1> perform a "dissective" study of the sru, gaining further understanding of the unit by exploring how various hyper-parameters affect performance; finally, <e2>we</e2> discuss conclusions from our study."
sameAs(e1, e2)
Comment:

4606	"for example, the simple recurrent unit (elman, 1990 ) contains a hidden <e1>state</e1> that itself depends on the previous hidden <e2>state</e2>."
sameAs(e1, e2)
Comment:

4607	"inspired by recent work in computer vision [15, 23] and robotics [17, 6, 25] <e1>we</e1> consider the task of object stacking and the problem of learning -from passive <e2>we</e2> present a visual classifier which is trained on stacks of diverse shapes to distinguish between stable and unstable structures."
sameAs(e1, e2)
Comment:

4608	"in <e1>this</e1> paper we investigate the passive acquisition of an intuitive understanding of physical principles as well as the active utilisation of <e2>this</e2> intuition in the context of generalised object stacking."
sameAs(e1, e2)
Comment:

4609	"instead, gan <e1>training</e1> can be seen as a minimax game between two models: the generator aims to output images similar to the <e2>training</e2> set given random noise; while the discriminator aims to distinguish the output of the generator from the training set."
sameAs(e1, e2)
Comment:

4610	"instead, gan <e1>training</e1> can be seen as a minimax game between two models: the generator aims to output images similar to the training set given random noise; while the discriminator aims to distinguish the output of the generator from the <e2>training</e2> set."
sameAs(e1, e2)
Comment:

4611	"instead, gan training can be seen as a minimax game between two models: the generator aims to output images similar to the <e1>training</e1> set given random noise; while the discriminator aims to distinguish the output of the generator from the <e2>training</e2> set."
sameAs(e1, e2)
Comment:

4612	"in the lin dataset, each image corresponds to a cell and is composed of signals from <e1>two</e1> independent fluorescence imaging channels ("red" and "green"), corresponding to the <e2>two</e2> different proteins tagged with red or green-emitting fluorophores, respectively."
sameAs(e1, e2)
Comment:

4613	"the green channel signal instead corresponds to any of 41 different "polarity factors", <e1>that</e1> is proteins <e2>that</e2> mark specific areas of the cells' cortex that help define a cell's geometry."
sameAs(e1, e2)
Comment:

4614	"the green channel signal instead corresponds to any of 41 different "polarity factors", <e1>that</e1> is proteins that mark specific areas of the cells' cortex <e2>that</e2> help define a cell's geometry."
sameAs(e1, e2)
Comment:

4615	"the green channel signal instead corresponds to any of 41 different "polarity factors", that is proteins <e1>that</e1> mark specific areas of the cells' cortex <e2>that</e2> help define a cell's geometry."
sameAs(e1, e2)
Comment:

4616	"the need to analyze quantitatively this deluge of <e1>data</e1> has given rise to the field of bioimage informatics [29] and is the source of numerous interesting and novel <e2>data</e2> analysis problems, which current machine learning developments could, in principle, help solve."
sameAs(e1, e2)
Comment:

4617	"specifically, <e1>we</e1> want to tackle two concrete limitations of large scale fluorescent imaging screens: <e2>we</e2> want to use the common information contained in the red channel to learn how to generate a cell with several of the green-labeled proteins together."
sameAs(e1, e2)
Comment:

4618	"we propose <e1>two</e1> approaches to generate multi-channel images: regular wgan-gp trained on multi-channel images, where extra channels for training are mined by nearest neighbor search in the training set, and a novel star-shaped generator trained directly one the <e2>two</e2>-channel images."
sameAs(e1, e2)
Comment:

4619	"we propose two approaches to generate multi-channel <e1>images</e1>: regular wgan-gp trained on multi-channel <e2>images</e2>, where extra channels for training are mined by nearest neighbor search in the training set, and a novel star-shaped generator trained directly one the two-channel images."
sameAs(e1, e2)
Comment:

4620	"we propose two approaches to generate multi-channel <e1>images</e1>: regular wgan-gp trained on multi-channel images, where extra channels for training are mined by nearest neighbor search in the training set, and a novel star-shaped generator trained directly one the two-channel <e2>images</e2>."
sameAs(e1, e2)
Comment:

4621	"we propose two approaches to generate multi-channel images: regular wgan-gp trained on multi-channel <e1>images</e1>, where extra channels for training are mined by nearest neighbor search in the training set, and a novel star-shaped generator trained directly one the two-channel <e2>images</e2>."
sameAs(e1, e2)
Comment:

4622	"we propose two approaches to generate multi-channel images: regular wgan-gp trained on multi-channel images, where extra channels for <e1>training</e1> are mined by nearest neighbor search in the <e2>training</e2> set, and a novel star-shaped generator trained directly one the two-channel images."
sameAs(e1, e2)
Comment:

4623	"we carefully evaluate our models using <e1>two</e1> quantitative techniques: the neural network <e2>two</e2>-sample test (combining ideas from [26] and [15] ) and by reconstructing samples in a held out test set with the optimization approach of [30] ."
sameAs(e1, e2)
Comment:

4624	"we carefully evaluate our models using two quantitative <e1>techniques</e1>: the neural network two-sample test (combining ideas from [26] and [15] ) and by reconstructing samples in a held out test set with the optimization <e2>approach</e2> of [30] ."
Compare(e1, e2)
Comment:

4625	"examples of recent models include denoising <e1>autoencoders</e1> [2] , variational <e2>autoencoders</e2> [20] , pixelcnns [44] and generative adversarial networks (gans) [14] ."
sameAs(e1, e2)
Comment:

4626	"examples of recent models include denoising autoencoders [2] , variational autoencoders [20] , pixelcnns [44] and <e1>generative adversarial networks</e1> (<e2>gans</e2>) [14] ."
sameAs(e1, e2)
Comment:

4627	"proteins real <e1>images</e1> generated <e2>images</e2> figure 1 ."
sameAs(e1, e2)
Comment:

4628	"the pseudocount is defined in terms of a density <e1>model</e1> ρ trained on the sequence of states experienced by an agent: n(x) = ρ(x)n(x), wheren(x) can be thought of as a total pseudo-count computed from the <e2>model</e2>'s recoding probability ρ (x), the probability of x computed immediately after training on x. as a practical application the authors used the pseudocounts derived from the simple cts density model (bellemare et al, 2014) to incentivize exploration in atari 2600 agents."
sameAs(e1, e2)
Comment:

4629	"the pseudocount is defined in terms of a density <e1>model</e1> ρ trained on the sequence of states experienced by an agent: n(x) = ρ(x)n(x), wheren(x) can be thought of as a total pseudo-count computed from the model's recoding probability ρ (x), the probability of x computed immediately after training on x. as a practical application the authors used the pseudocounts derived from the simple cts density <e2>model</e2> (bellemare et al, 2014) to incentivize exploration in atari 2600 agents."
sameAs(e1, e2)
Comment:

4630	"the pseudocount is defined in terms of a density model ρ trained on the sequence of states experienced by an agent: n(x) = ρ(x)n(x), wheren(x) can be thought of as a total pseudo-count computed from the <e1>model</e1>'s recoding probability ρ (x), the probability of x computed immediately after training on x. as a practical application the authors used the pseudocounts derived from the simple cts density <e2>model</e2> (bellemare et al, 2014) to incentivize exploration in atari 2600 agents."
sameAs(e1, e2)
Comment:

4631	"their <e1>method</e1> critically hinged on several assumptions regarding the density <e2>model</e2>: 1) the model should be learning-positive, i.e."
Used-for(e1, e2)
Comment:

4632	"their <e1>method</e1> critically hinged on several assumptions regarding the density model: 1) the <e2>model</e2> should be learning-positive, i.e."
Used-for(e1, e2)
Comment:

4633	"their method critically hinged on several assumptions regarding the density <e1>model</e1>: 1) the <e2>model</e2> should be learning-positive, i.e."
sameAs(e1, e2)
Comment:

4634	"of note, tang et al (2016) maintain an approximate count by means of hash tables over <e1>features</e1>, which in the pseudo-count framework corresponds to a hash-based density <e2>model</e2>."
Used-for(e1, e2)
Comment:

4635	"in <e1>this</e1> paper, we describe a system that achieves state-of-the-art results on <e2>this</e2> challenging task."
sameAs(e1, e2)
Comment:

4636	"in this paper, we describe a <e1>system</e1> that achieves state-of-the-art results on <e2>this</e2> challenging task."
Used-for(e1, e2)
Comment:

4637	"recent work [35, 25, 8, 24] has advocated the bottom-up <e1>approach</e1>; in their experiments, their proposed bottom-up <e2>methods</e2> outperformed the top-down baselines they compared with."
Compare(e1, e2)
Comment:

4638	"the <e1>system</e1> described in <e2>this</e2> paper is an improved version of our g-rmi entry to the coco 2016 keypoints detection challenge."
Used-for(e1, e2)
Comment:

4639	"using only publicly available data for training, our final <e1>system</e1> achieves average precision of 0.649 on the coco test-dev set and 0.643 on the coco test-standard set, outperforming the winner of the 2016 coco keypoints challenge [8] , which gets 0.618 on test-dev and 0.611 on teststandard, as well as the very recent mask-rcnn [21] <e2>methods</e2> which gets 0.631 on test-dev."
Compare(e1, e2)
Comment:

4640	"these results have been attained with single-scale evaluation and using a single <e1>cnn</e1> for box detection and a single <e2>cnn</e2> for pose estimation."
sameAs(e1, e2)
Comment:

4641	"we then perform an experimental study, comparing our <e1>system</e1> to recent stateof-the-art, and we measure the effects of the different parts of our <e2>system</e2> on the ap metric."
sameAs(e1, e2)
Comment:

4642	"in this paper, <e1>we</e1> tackle the more challenging setting of pose detection 'in the wild', in which <e2>we</e2> are not provided with the ground truth location or scale of the person instances."
sameAs(e1, e2)
Comment:

4643	"this is harder because it combines the <e1>problem</e1> of person detection with the <e2>problem</e2> of pose estimation."
sameAs(e1, e2)
Comment:

4644	"our contributions aim to unify these research streams: • <e1>we</e1> introduce two alternative procedures for loss correction, provided that <e2>we</e2> know a stochastic matrix t summarizing the probability of one class being flipped into another under noise."
sameAs(e1, e2)
Comment:

4645	"since we only operate on the <e1>loss function</e1>, the approach is both architecture and application domain independent, as well as viable for any chosen <e2>loss function</e2>."
sameAs(e1, e2)
Comment:

4646	"• we take a further step and extend the noise estimator of [26] to our multi-class setting, thus formulating an end-to-end <e1>solution</e1> to the <e2>problem</e2>."
Used-for(e1, e2)
Comment:

4647	"unsurprisingly, the noise estimator is the bottleneck in obtaining near-perfect robustness, yet in most experiments our <e1>approach</e1> is often the best compared to <e2>prior work</e2>."
Compare(e1, e2)
Comment:

4648	"experimental results not only provide evidence in support of the effectiveness of the proposed architecture and the learned hardness predictor, but also show <e1>that</e1> the realistic classifier always improves performance on the examples <e2>that</e2> it accepts to classify, performing better on these examples than an equivalent nonrealistic classifier."
sameAs(e1, e2)
Comment:

4649	"experimental results not only provide evidence in support of the effectiveness of the proposed architecture and the learned hardness predictor, but also show that the realistic <e1>classifier</e1> always improves performance on the examples that it accepts to classify, performing better on these examples than an equivalent nonrealistic <e2>classifier</e2>."
sameAs(e1, e2)
Comment:

4650	"experimental results not only provide evidence in support of the effectiveness of the proposed architecture and the learned hardness predictor, but also show that the realistic classifier always improves performance on the <e1>examples</e1> that it accepts to classify, performing better on these <e2>examples</e2> than an equivalent nonrealistic classifier."
sameAs(e1, e2)
Comment:

4651	"then <e1>they</e1> work on what <e2>they</e2> can do and gradually overcome their limitations."
sameAs(e1, e2)
Comment:

4652	"one could say <e1>that</e1> humans are realistic predictors, who would rather refuse tasks <e2>that</e2> are too hard than almost surely fail."
sameAs(e1, e2)
Comment:

4653	"instead of annotating all <e1>data</e1> manually, it is undoubtedly efficient to let realistic <e2>models</e2> handle on easy examples so as to guarantee accuracy comparable to humans, and just leave the hard ones aside for human experts."
Used-for(e1, e2)
Comment:

4654	"these are predictors <e1>that</e1>, like humans, assess the difficulty of examples, reject to work on those <e2>that</e2> are deemed too hard, but guarantee good performance on the ones they operate on."
sameAs(e1, e2)
Comment:

4655	"for example, <e1>classifier</e1> <e2>cascades</e2> are composed of stages that implement a series of reject decisions, efficiently zooming in on image region containing the object to detect [28] ."
Used-for(e1, e2)
Comment:

4656	"neural <e1>network</e1> routing [20] , where samples are processed by different <e2>network</e2> paths, according to their difficulty, is a neural variant of this idea."
sameAs(e1, e2)
Comment:

4657	"while useful for gathering difficult <e1>examples</e1>, it can produce a significant percentage of <e2>examples</e2> that are not difficult."
sameAs(e1, e2)
Comment:

4658	"motivated by this, <e1>we</e1> propose to implement the hardness predictor as an auxiliary network, which <e2>we</e2> call the auxiliary hardness prediction network (hpnet)."
sameAs(e1, e2)
Comment:

4659	"motivated by this, we propose to implement the hardness predictor as an auxiliary <e1>network</e1>, which we call the auxiliary hardness prediction <e2>network</e2> (hpnet)."
sameAs(e1, e2)
Comment:

4660	"the two networks are trained in an adversarial setting, <e1>that</e1> resembles <e2>that</e2> of generative adversarial networks (gans)."
sameAs(e1, e2)
Comment:

4661	"the two networks are trained in an adversarial setting, that resembles that of <e1>generative adversarial networks</e1> (<e2>gans</e2>)."
sameAs(e1, e2)
Comment:

4662	"sharing the same inputs as <e1>classifiers</e1>, the hp-net outputs the hardness scores to be fed to the <e2>classifier</e2> as loss weights."
sameAs(e1, e2)
Comment:

4663	"jointly optimize <e1>data augmentation</e1> and network training: adversarial <e2>data augmentation</e2> in human pose estimation"
sameAs(e1, e2)
Comment:

4664	"a common <e1>solution</e1> for this <e2>problem</e2> is to perform random data augmentation [21, 37] ."
Used-for(e1, e2)
Comment:

4665	"prior to being fed into the network <e1>training</e1> be jointly optimized, so that effective augmentations can be generated online to improve the <e2>training</e2>?"
sameAs(e1, e2)
Comment:

4666	"however, data augmentation and <e1>network</e1> training are usually treated as two isolated processes, limiting the effectiveness of <e2>network</e2> training."
sameAs(e1, e2)
Comment:

4667	"however, data augmentation and network <e1>training</e1> are usually treated as two isolated processes, limiting the effectiveness of network <e2>training</e2>."
sameAs(e1, e2)
Comment:

4668	"in other words, the augmentation <e1>network</e1> explores the weaknesses of the pose <e2>network</e2> which, at the same time, learns from adversarial augmentations for better performance."
sameAs(e1, e2)
Comment:

4669	"besides, we propose a novel <e1>reward</e1> and <e2>penalty</e2> policy to address the issue of missing supervisions during the joint training."
Conjunction(e1, e2)
Comment:

4670	"• we propose an augmentation <e1>network</e1> to play a minimax game against the target <e2>network</e2>, by generating adversarial augmentations online."
sameAs(e1, e2)
Comment:

4671	"• we take advantage of the wildly used u-net design and propose a <e1>reward</e1> and <e2>penalty</e2> policy for the efficient joint training of the two networks."
Conjunction(e1, e2)
Comment:

4672	"the main idea is to design an augmentation <e1>network</e1> (generator) that competes against a target <e2>network</e2> (discriminator) by generating "hard" augmentation operations online."
sameAs(e1, e2)
Comment:

4673	"the augmentation <e1>network</e1> explores the weaknesses of the target <e2>network</e2>, while the latter learns from "hard" augmentations to achieve better performance."
sameAs(e1, e2)
Comment:

4674	"we also design a <e1>reward</e1>/<e2>penalty</e2> strategy for effective joint training."
Conjunction(e1, e2)
Comment:

4675	"we demonstrate our <e1>approach</e1> on the problem of human pose estimation and carry out a comprehensive experimental analysis, showing that our method can significantly improve state-of-the-art <e2>models</e2> without additional data efforts."
Used-for(e1, e2)
Comment:

4676	"the idea has apparent complications, as different child <e1>models</e1> might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other <e2>models</e2> on other tasks, with little to no modifications (razavian et al, 2014; zoph et al, 2016; luong et al, 2016) ."
sameAs(e1, e2)
Comment:

4677	"the idea has apparent complications, as different child <e1>models</e1> might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other models on other <e2>tasks</e2>, with little to no modifications (razavian et al, 2014; zoph et al, 2016; luong et al, 2016) ."
Used-for(e1, e2)
Comment:

4678	"the idea has apparent complications, as different child models might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for <e1>other</e1> models on <e2>other</e2> tasks, with little to no modifications (razavian et al, 2014; zoph et al, 2016; luong et al, 2016) ."
sameAs(e1, e2)
Comment:

4679	"the idea has apparent complications, as different child models might utilize their weights differently, but was encouraged by previous work on transfer learning and multitask learning, which established that parameters learned for a particular model on a particular task can be used for other <e1>models</e1> on other <e2>tasks</e2>, with little to no modifications (razavian et al, 2014; zoph et al, 2016; luong et al, 2016) ."
Used-for(e1, e2)
Comment:

4680	"enas constructs a large computational graph, where each subgraph represents a neural network <e1>architecture</e1>, hence forcing all <e2>architectures</e2> to share their parameters."
sameAs(e1, e2)
Comment:

4681	"on <e1>penn treebank</e1>, our method achieves a test perplexity of 56.3, which significantly outperforms nas's test perplexity of 62.4 (zoph & le, 2017) and which is on par with the existing state-of-the-art among <e2>penn treebank</e2>'s approaches that do not utilize post-training processing (56.0; yang et al (2018) )."
sameAs(e1, e2)
Comment:

4682	"abstract in this paper, we introduce the concept of learning latent super-events from <e1>activity</e1> videos, and present introduction <e2>activity</e2> detection is an important computer vision problem with many societal applications, including smart surveillance, monitoring of patients or elderly (e.g., for quality-of-life systems), online video retrieval, and robot perception."
sameAs(e1, e2)
Comment:

4683	"this means that detecting the frames of one <e1>activity</e1> in the video should benefit from information in the frames corresponding to another <e2>activity</e2>, which are often temporally very separated."
sameAs(e1, e2)
Comment:

4684	"the temporal structure filters are applied to the entire <e1>video</e1> and soft-attention is applied to form a super-event representation for each <e2>video</e2>."
sameAs(e1, e2)
Comment:

4685	"we concatenate <e1>per</e1>-frame (or <e2>per</e2>-segment) video features with the super-event representation to make per-frame classifications, annotating each frame with its activity class (or no-activity)."
sameAs(e1, e2)
Comment:

4686	"we concatenate <e1>per</e1>-frame (or per-segment) video features with the super-event representation to make <e2>per</e2>-frame classifications, annotating each frame with its activity class (or no-activity)."
sameAs(e1, e2)
Comment:

4687	"we concatenate per-frame (or <e1>per</e1>-segment) video features with the super-event representation to make <e2>per</e2>-frame classifications, annotating each frame with its activity class (or no-activity)."
sameAs(e1, e2)
Comment:

4688	"we concatenate per-frame (or per-segment) video features with the super-event representation to make per-frame classifications, annotating each frame with its <e1>activity</e1> class (or no-<e2>activity</e2>)."
sameAs(e1, e2)
Comment:

4689	"a block or rebound <e1>event</e1> cannot occur without a shot <e2>event</e2>."
sameAs(e1, e2)
Comment:

4690	"given a continuous <e1>video</e1>, the task is to find the frames corresponding to every event occurring in the <e2>video</e2>."
sameAs(e1, e2)
Comment:

4691	"more specifically, if the <e1>event</e1> we are interested in is a part of a longer-term <e2>event</e2>, we call the longer-term event its super-event."
sameAs(e1, e2)
Comment:

4692	"more specifically, if the <e1>event</e1> we are interested in is a part of a longer-term event, we call the longer-term <e2>event</e2> its super-event."
sameAs(e1, e2)
Comment:

4693	"more specifically, if the <e1>event</e1> we are interested in is a part of a longer-term event, we call the longer-term event its super-<e2>event</e2>."
sameAs(e1, e2)
Comment:

4694	"more specifically, if the event <e1>we</e1> are interested in is a part of a longer-term event, <e2>we</e2> call the longer-term event its super-event."
sameAs(e1, e2)
Comment:

4695	"more specifically, if the event we are interested in is a part of a longer-term <e1>event</e1>, we call the longer-term <e2>event</e2> its super-event."
sameAs(e1, e2)
Comment:

4696	"more specifically, if the event we are interested in is a part of a longer-term <e1>event</e1>, we call the longer-term event its super-<e2>event</e2>."
sameAs(e1, e2)
Comment:

4697	"more specifically, if the event we are interested in is a part of a longer-term event, we call the longer-term <e1>event</e1> its super-<e2>event</e2>."
sameAs(e1, e2)
Comment:

4698	"learning such latent super-<e1>events</e1> allows the model to capture how the <e2>events</e2> are temporally related in their videos."
sameAs(e1, e2)
Comment:

4699	"once learned, when making a prediction (i.e., testing), the super-<e1>events</e1> can serve as temporal context to better detect the <e2>events</e2>."
sameAs(e1, e2)
Comment:

4700	"note <e1>that</e1> such super-events are 'latent', meaning <e2>that</e2> no super-event annotations are provided."
sameAs(e1, e2)
Comment:

4701	"we newly design temporal structure filters, and convolve it with the video <e1>representation</e1> to obtain a super-event <e2>representation</e2>."
sameAs(e1, e2)
Comment:

4702	"this is more challenging compared to the <e1>activity</e1> classification problem of categorizing a pre-segmented video or localizing a single <e2>activity</e2> in a trimmed video."
sameAs(e1, e2)
Comment:

4703	"this is more challenging compared to the activity classification problem of categorizing a pre-segmented <e1>video</e1> or localizing a single activity in a trimmed <e2>video</e2>."
sameAs(e1, e2)
Comment:

4704	"such temporal structure filters are learned for each <e1>event</e1>, optimized based on the training data for the best super-<e2>event</e2> representation construction."
sameAs(e1, e2)
Comment:

4705	"for each frame, we combine the super-<e1>event</e1> representation with the per-frame or per-segment cnn representation for its binary classification per <e2>event</e2>."
sameAs(e1, e2)
Comment:

4706	"for each frame, we combine the super-event <e1>representation</e1> with the per-frame or per-segment cnn <e2>representation</e2> for its binary classification per event."
sameAs(e1, e2)
Comment:

4707	"for each frame, we combine the super-event representation with the <e1>per</e1>-frame or <e2>per</e2>-segment cnn representation for its binary classification per event."
sameAs(e1, e2)
Comment:

4708	"for each frame, we combine the super-event representation with the <e1>per</e1>-frame or per-segment cnn representation for its binary classification <e2>per</e2> event."
sameAs(e1, e2)
Comment:

4709	"for each frame, we combine the super-event representation with the per-frame or <e1>per</e1>-segment cnn representation for its binary classification <e2>per</e2> event."
sameAs(e1, e2)
Comment:

4710	"although activity detection is an important area to study as almost all real-world <e1>videos</e1> contain multiple activities and are rarely segmented (e.g., surveillance systems), it has been investigated much less, particularly for multi-event <e2>videos</e2>."
sameAs(e1, e2)
Comment:

4711	"these approaches successfully modeled <e1>per</e1>-frame (or <e2>per</e2>-local-segment) information in activity videos, such as a single rgb frame or optical flows over a small number of frames [7] ."
sameAs(e1, e2)
Comment:

4712	"however, most current rl-based <e1>approaches</e1> fail to generalize since: (a) the gap between simulation and real world is so large that policy-learning <e2>approaches</e2> fail to transfer; (b) even if policy learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses)."
sameAs(e1, e2)
Comment:

4713	"however, most current rl-based approaches fail to generalize since: (a) the gap between simulation and real world is so large that <e1>policy</e1>-learning approaches fail to transfer; (b) even if <e2>policy</e2> learning is done in real world, the data scarcity leads to failed generalization from training to test scenarios (e.g., due to different friction or object masses)."
sameAs(e1, e2)
Comment:

4714	"extensive experiments in multiple environments (invertedpendulum, halfcheetah, swimmer, hopper, walker2d and ant) conclusively demonstrate that our method (a) improves <e1>training</e1> stability; (b) is robust to differences in <e2>training</e2>/test conditions; and c) outperform the baseline even in the absence of the adversary."
sameAs(e1, e2)
Comment:

4715	"many papers rely on network <e1>architectures</e1> that have already been proven successful for image classification such as variants of the resnet [25] or the vgg <e2>architecture</e2> [50] ."
sameAs(e1, e2)
Comment:

4716	"starting from pre-trained nets, where a large number of weights for the <e1>target</e1> task can be pre-set by an auxiliary classification task, reduces training time and often yields superior performance compared to training a network from scratch using the (possibly limited amount of) data of the <e2>target</e2> application."
sameAs(e1, e2)
Comment:

4717	"starting from pre-trained nets, where a large number of weights for the target <e1>task</e1> can be pre-set by an auxiliary classification task, reduces training time and often yields superior performance compared to training a network from scratch using the (possibly limited amount of) <e2>data</e2> of the target application."
Conjunction(e1, e2)
Comment:

4718	"noh et al [41] learn a mirrored vgg <e1>network</e1> as a decoder, yu and koltun [55] introduce dilated convolutions to reduce the pooling factor of their pre-trained <e2>network</e2>."
sameAs(e1, e2)
Comment:

4719	"an alternative <e1>approach</e1> used by several <e2>methods</e2> is to apply post-processing steps such as crf-smoothing [30] ."
Compare(e1, e2)
Comment:

4720	"in this paper, we propose a novel network <e1>architecture</e1> that achieves state-of-the-art segmentation performance without the need for additional post-processing steps and without the limitations imposed by pre-trained <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

4721	"one stream undergoes a sequence of pooling operations and is responsible for understanding large-scale relationships of <e1>image</e1> elements; the other stream carries feature maps at the full <e2>image</e2> resolution, resulting in precise boundary adherence."
sameAs(e1, e2)
Comment:

4722	"the frrus from the red <e1>pooling</e1> lane act as residual units for the blue stream, but also undergo <e2>pooling</e2> operations and carry high-level information through the network."
sameAs(e1, e2)
Comment:

4723	"this paper makes the following contributions: (i) we propose a novel network <e1>architecture</e1> geared towards precise semantic segmentation in street scenes which is not limited to pre-trained <e2>architectures</e2> and achieves state-ofthe-art results."
sameAs(e1, e2)
Comment:

4724	"(ii) we propose to use two processing streams to realize strong recognition and strong localization performance: <e1>one</e1> stream undergoes a sequence of pooling operations while the <e2>other</e2> stream stays at the full image resolution."
Conjunction(e1, e2)
Comment:

4725	"the residual stream (blue) stays at the full image resolution, the <e1>pooling</e1> stream (red) undergoes a sequence of <e2>pooling</e2> and unpooling operations."
sameAs(e1, e2)
Comment:

4726	"for this <e1>task</e1>, the best performing methods have recently relied heavily on models learned from <e2>data</e2> [12, 37, 46, 47] ."
Conjunction(e1, e2)
Comment:

4727	"for this task, the best performing <e1>methods</e1> have recently relied heavily on <e2>models</e2> learned from data [12, 37, 46, 47] ."
Compare(e1, e2)
Comment:

4728	"even methods which employ a manually created hand <e1>model</e1> to search for a good fit with the observation, often employ such a data-driven part as initialization or for <e2>error correction</e2> [21, 49, 52, 66] ."
Used-for(e1, e2)
Comment:

4729	"unfortunately, <e1>data</e1>-driven <e2>models</e2> require a large amount of labeled data, covering a sufficient part of the pose space, to work well."
Used-for(e1, e2)
Comment:

4730	"unfortunately, <e1>data</e1>-driven models require a large amount of labeled <e2>data</e2>, covering a sufficient part of the pose space, to work well."
sameAs(e1, e2)
Comment:

4731	"we learn to predict a low-dimensional latent <e1>representation</e1> and, subsequently, a different view of the input, solely from the latent <e2>representation</e2>."
sameAs(e1, e2)
Comment:

4732	"however, for the <e1>task</e1> of estimating the pose of articulated objects, like the human hand, it is especially expensive to provide accurate annotations for a sufficient amount of real world <e2>data</e2>."
Conjunction(e1, e2)
Comment:

4733	"the guidance relies on the fact that from any set of <e1>pose</e1> parameters, which accurately specify the <e2>pose</e2> and rough shape of the hand, we necessarily need to be able to predict the hand's appearance in any other view."
sameAs(e1, e2)
Comment:

4734	"hence, by capturing another view, this additional view can be used as a target for <e1>training</e1> a model, which itself guides the <e2>training</e2> of the underlying pose representation."
sameAs(e1, e2)
Comment:

4735	"more specifically, the idea is to train a model whichgiven the first camera view -estimates a small number of latent <e1>parameters</e1>, and subsequently predicts a different view solely from these few <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

4736	"by learning to predict a different view from the latent <e1>parameters</e1>, the latent <e2>parameters</e2> are enforced to capture pose specific information."
sameAs(e1, e2)
Comment:

4737	"framing the problem in this way, a pose representation can be learned just by capturing the hand simultaneously from different viewpoints and learning to predict <e1>one</e1> view given the <e2>other</e2>."
Conjunction(e1, e2)
Comment:

4738	"given the learned low-dimensional pose representation, a rather simple mapping to a specific <e1>target</e1> (e.g., joint positions) can be learned from a much smaller number of training samples than required to learn the full mapping from input to <e2>target</e2>."
sameAs(e1, e2)
Comment:

4739	"thereby, the joint training regularizes the model to ensure that the learned <e1>pose</e1> representation can be mapped to the target <e2>pose</e2> space using the specified mapping."
sameAs(e1, e2)
Comment:

4740	"for the more practical case, where the <e1>number</e1> of unlabeled samples is larger than the <e2>number</e2> of labeled samples, we find that the proposed method performs on par with the baseline, even with one order of magnitude less labeled samples."
sameAs(e1, e2)
Comment:

4741	"for the more practical case, where the number of unlabeled samples is larger than the number of labeled samples, we find that the proposed <e1>method</e1> performs on par with the <e2>baseline</e2>, even with one order of magnitude less labeled samples."
Compare(e1, e2)
Comment:

4742	"in targeted <e1>adversarial</e1> attacks, we seek <e2>adversarial</e2> images that can change the prediction of a model to a specific target label."
sameAs(e1, e2)
Comment:

4743	"our main contributions can be summarized as follows: • we present a unifying framework for creating universal and <e1>image</e1>-dependent perturbations for both <e2>classification</e2> and semantic segmentation tasks, considering targeted and non-targeted attacks with l ∞ and l 2 norms as the metric."
Used-for(e1, e2)
Comment:

4744	"to generate these perturbations, we require a function which takes a natural <e1>image</e1>, and outputs an adversarial <e2>image</e2>."
sameAs(e1, e2)
Comment:

4745	"to address these limitations, some have proposed iqa <e1>methods</e1> based on machine learning to learn more sophisticated <e2>models</e2> [19] ."
Compare(e1, e2)
Comment:

4746	"although many learning-based <e1>methods</e1> use hand-crafted image features [8, 10, 18, 20, 21, 31, 33, 35, 40] , recent <e2>methods</e2> (including ours) apply deep-learning to fr-iqa to learn features automatically [7, 17, 25] ."
sameAs(e1, e2)
Comment:

4747	"although many learning-based methods use hand-crafted image <e1>features</e1> [8, 10, 18, 20, 21, 31, 33, 35, 40] , recent methods (including ours) apply deep-learning to fr-iqa to learn <e2>features</e2> automatically [7, 17, 25] ."
sameAs(e1, e2)
Comment:

4748	"however, the accuracy of all existing learning-based methods depends on the size and quality of the <e1>datasets</e1> they are trained on, and existing iqa <e2>datasets</e2> are small and noisy."
sameAs(e1, e2)
Comment:

4749	"for instance, many datasets [16, 26, 27, 34, 47, 49, 55] are labeled using a <e1>mean opinion score</e1> (<e2>mos</e2>) where each user gives the distorted image a subjective quality rating (e.g., 0 = "bad", 10 = "excellent")."
sameAs(e1, e2)
Comment:

4750	"[35] 3.213 1.504 bosse et al [7] 31.360 36.192 kim et al [25] 0 of people prefer <e1>image</e1> b. despite this simple visual task, 13 <e2>image</e2> quality assessment (iqa) methods-including both popular and stateof-the-art approaches-fail to predict the image that is visually closer to the reference."
sameAs(e1, e2)
Comment:

4751	"[35] 3.213 1.504 bosse et al [7] 31.360 36.192 kim et al [25] 0 of people prefer <e1>image</e1> b. despite this simple visual task, 13 image quality assessment (iqa) methods-including both popular and stateof-the-art approaches-fail to predict the <e2>image</e2> that is visually closer to the reference."
sameAs(e1, e2)
Comment:

4752	"[35] 3.213 1.504 bosse et al [7] 31.360 36.192 kim et al [25] 0 of people prefer image b. despite this simple visual task, 13 <e1>image</e1> quality assessment (iqa) methods-including both popular and stateof-the-art approaches-fail to predict the <e2>image</e2> that is visually closer to the reference."
sameAs(e1, e2)
Comment:

4753	"in <e1>this</e1> paper, we make critical strides towards solving <e2>this</e2> problem by proposing a novel framework for learning perceptual image error as well as a new, corresponding dataset that is larger and of higher quality than previous ones."
sameAs(e1, e2)
Comment:

4754	"a successful <e1>solution</e1> to this <e2>problem</e2> would have many applications, including image compression/coding, restoration, and adaptive reconstruction."
Used-for(e1, e2)
Comment:

4755	"rather than asking people to label <e1>images</e1> with a subjective quality score, we exploit the fact that it is much easier for humans to select which of two <e2>images</e2> is closer to a reference."
sameAs(e1, e2)
Comment:

4756	"however, unlike the tid <e1>datasets</e1> [42, 43] , we do not explicitly convert this preference into a quality score, since <e2>approaches</e2> such as swiss tournaments introduce errors and do not scale."
Used-for(e1, e2)
Comment:

4757	"to do <e1>this</e1>, we input the distorted images (a,b) and the corresponding reference, r, into a pair of identical error-estimation functions which output the perceptualerror scores for a and b. the choice for the error-estimation function is flexible, and in <e2>this</e2> paper we propose a new deep convolutional neural network (dcnn) for it."
sameAs(e1, e2)
Comment:

4758	"to do this, <e1>we</e1> input the distorted images (a,b) and the corresponding reference, r, into a pair of identical error-estimation functions which output the perceptualerror scores for a and b. the choice for the error-estimation function is flexible, and in this paper <e2>we</e2> propose a new deep convolutional neural network (dcnn) for it."
sameAs(e1, e2)
Comment:

4759	"to do this, we input the distorted images (a,b) and the corresponding reference, r, into a pair of identical error-<e1>estimation</e1> functions which output the perceptualerror scores for a and b. the choice for the error-<e2>estimation</e2> function is flexible, and in this paper we propose a new deep convolutional neural network (dcnn) for it."
sameAs(e1, e2)
Comment:

4760	"once our <e1>system</e1>, which we call pieapp, is trained using the pairwise probabilities, we can use the learned error-estimation function on a single image a and a reference r to compute the perceptual error of a with respect to r. <e2>this</e2> trick allows us to quantify the perceived error of a distorted image with respect to a reference, even though our system was never explicitly trained with hand-labeled, perceptual-error scores."
Used-for(e1, e2)
Comment:

4761	"once our <e1>system</e1>, which we call pieapp, is trained using the pairwise probabilities, we can use the learned error-estimation function on a single image a and a reference r to compute the perceptual error of a with respect to r. this trick allows us to quantify the perceived error of a distorted image with respect to a reference, even though our <e2>system</e2> was never explicitly trained with hand-labeled, perceptual-error scores."
sameAs(e1, e2)
Comment:

4762	"once our system, which <e1>we</e1> call pieapp, is trained using the pairwise probabilities, <e2>we</e2> can use the learned error-estimation function on a single image a and a reference r to compute the perceptual error of a with respect to r. this trick allows us to quantify the perceived error of a distorted image with respect to a reference, even though our system was never explicitly trained with hand-labeled, perceptual-error scores."
sameAs(e1, e2)
Comment:

4763	"once our system, which we call pieapp, is trained using the pairwise probabilities, we can use the learned error-estimation function on a single <e1>image</e1> a and a reference r to compute the perceptual error of a with respect to r. this trick allows us to quantify the perceived error of a distorted <e2>image</e2> with respect to a reference, even though our system was never explicitly trained with hand-labeled, perceptual-error scores."
sameAs(e1, e2)
Comment:

4764	"the combination of our novel, pairwise-learning framework and new dataset <e1>results</e1> in a significant improvement in perceptual image-error assessment, and can also be used to further improve existing learning-based iqa <e2>methods</e2>."
Compare(e1, e2)
Comment:

4765	"others have proposed metrics that try to exploit known aspects of the human visual <e1>system</e1> (hvs) such as contrast sensitivity [22] , high-level structural acuity [52] , and masking [48, 51] , or use other statistics/<e2>features</e2> [3, 4, 14, 15, 38, [44] [45] [46] 53, 54, 57] ."
Used-for(e1, e2)
Comment:

4766	"2. environments with a sparse <e1>reward</e1> signal can be difficult for a neural network to model as there may be very few instances where the <e2>reward</e2> is non-zero."
sameAs(e1, e2)
Comment:

4767	"this can be viewed as a form of class imbalance where low-<e1>reward</e1> samples outnumber high-<e2>reward</e2> samples by an unknown number."
sameAs(e1, e2)
Comment:

4768	"3. <e1>reward</e1> signal propagation by value-bootstrapping techniques, such as q-learning, results in <e2>reward</e2> information being propagated one step at a time through the history of previous interactions with the environment."
sameAs(e1, e2)
Comment:

4769	"in this work <e1>we</e1> shall focus on addressing the three concerns listed above; <e2>we</e2> must note, however, that other recent advances in exploration (osband et al, 2016) , hierarchical reinforcement learning (vezhnevets et al, 2016) and transfer learning (rusu et al, 2016; fernando et al, 2017 ) also make substantial contributions to improving data efficiency in deep reinforcement learning over baseline agents."
sameAs(e1, e2)
Comment:

4770	"our agent uses a semi-tabular representation of its experience of the environment possessing several of the features of episodic <e1>memory</e1> such as long term <e2>memory</e2>, sequentiality, and context-based lookups."
sameAs(e1, e2)
Comment:

4771	"thus the agent's memory operates in much the same way that traditional table-based rl methods map from <e1>state</e1> and <e2>action</e2> to value estimates."
Conjunction(e1, e2)
Comment:

4772	"a unique aspect of the <e1>memory</e1> in contrast to other neural <e2>memory</e2> architectures for reinforcement learning (explained in more detail in section 3) is that the values retrieved from the memory can be updated much faster than the rest of the deep neural network."
sameAs(e1, e2)
Comment:

4773	"a unique aspect of the <e1>memory</e1> in contrast to other neural memory architectures for reinforcement learning (explained in more detail in section 3) is that the values retrieved from the <e2>memory</e2> can be updated much faster than the rest of the deep neural network."
sameAs(e1, e2)
Comment:

4774	"a unique aspect of the memory in contrast to other neural <e1>memory</e1> architectures for reinforcement learning (explained in more detail in section 3) is that the values retrieved from the <e2>memory</e2> can be updated much faster than the rest of the deep neural network."
sameAs(e1, e2)
Comment:

4775	"another unique aspect of the <e1>memory</e1> is that unlike other <e2>memory</e2> architectures such as lstm and the differentiable neural computer (dnc; graves et al, 2016) , our architecture does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time."
sameAs(e1, e2)
Comment:

4776	"another unique aspect of the <e1>memory</e1> is that unlike other memory architectures such as lstm and the differentiable neural computer (dnc; graves et al, 2016) , our architecture does not try to learn when to write to <e2>memory</e2>, as this can be slow to learn and take a significant amount of time."
sameAs(e1, e2)
Comment:

4777	"another unique aspect of the memory is that unlike other <e1>memory</e1> architectures such as lstm and the differentiable neural computer (dnc; graves et al, 2016) , our architecture does not try to learn when to write to <e2>memory</e2>, as this can be slow to learn and take a significant amount of time."
sameAs(e1, e2)
Comment:

4778	"another unique aspect of the memory is that unlike other memory <e1>architectures</e1> such as lstm and the differentiable neural computer (dnc; graves et al, 2016) , our <e2>architecture</e2> does not try to learn when to write to memory, as this can be slow to learn and take a significant amount of time."
sameAs(e1, e2)
Comment:

4779	"another unique aspect of the memory is that unlike other memory architectures such as lstm and the differentiable neural computer (dnc; graves et al, 2016) , our architecture does not try to <e1>learn</e1> when to write to memory, as this can be slow to <e2>learn</e2> and take a significant amount of time."
sameAs(e1, e2)
Comment:

4780	"instead, we elect to write all experiences to the <e1>memory</e1>, and allow it to grow very large compared to existing <e2>memory</e2> architectures (in contrast to oh et al (2015) ; graves et al (2016) where the memory is wiped at the end of each episode)."
sameAs(e1, e2)
Comment:

4781	"instead, we elect to write all experiences to the <e1>memory</e1>, and allow it to grow very large compared to existing memory architectures (in contrast to oh et al (2015) ; graves et al (2016) where the <e2>memory</e2> is wiped at the end of each episode)."
sameAs(e1, e2)
Comment:

4782	"instead, we elect to write all experiences to the memory, and allow it to grow very large compared to existing <e1>memory</e1> architectures (in contrast to oh et al (2015) ; graves et al (2016) where the <e2>memory</e2> is wiped at the end of each episode)."
sameAs(e1, e2)
Comment:

4783	"the remainder of the paper is organised as follows: in section 2 <e1>we</e1> review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 <e2>we</e2> report experimental results in the atari learning environment, in section 5 we discuss other methods that use memory for reinforcement learning, and finally in section 6 we outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4784	"the remainder of the paper is organised as follows: in section 2 <e1>we</e1> review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 we report experimental results in the atari learning environment, in section 5 <e2>we</e2> discuss other methods that use memory for reinforcement learning, and finally in section 6 we outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4785	"the remainder of the paper is organised as follows: in section 2 <e1>we</e1> review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 we report experimental results in the atari learning environment, in section 5 we discuss other methods that use memory for reinforcement learning, and finally in section 6 <e2>we</e2> outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4786	"the remainder of the paper is organised as follows: in section 2 we review deep reinforcement learning, in section 3 the neural episodic control <e1>algorithm</e1> is described, in section 4 we report experimental results in the atari learning environment, in section 5 we discuss other methods that use memory for reinforcement learning, and finally in section 6 we outline future work and summarise the main advantages of the nec <e2>algorithm</e2>."
sameAs(e1, e2)
Comment:

4787	"the remainder of the paper is organised as follows: in section 2 we review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 <e1>we</e1> report experimental results in the atari learning environment, in section 5 <e2>we</e2> discuss other methods that use memory for reinforcement learning, and finally in section 6 we outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4788	"the remainder of the paper is organised as follows: in section 2 we review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 <e1>we</e1> report experimental results in the atari learning environment, in section 5 we discuss other methods that use memory for reinforcement learning, and finally in section 6 <e2>we</e2> outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4789	"the remainder of the paper is organised as follows: in section 2 we review deep reinforcement learning, in section 3 the neural episodic control algorithm is described, in section 4 we report experimental results in the atari learning environment, in section 5 <e1>we</e1> discuss other methods that use memory for reinforcement learning, and finally in section 6 <e2>we</e2> outline future work and summarise the main advantages of the nec algorithm."
sameAs(e1, e2)
Comment:

4790	"our agent uses a semi-tabular representation of the value <e1>function</e1>: a buffer of past experience containing slowly changing state representations and rapidly updated estimates of the value <e2>function</e2>."
sameAs(e1, e2)
Comment:

4791	"in <e1>this</e1> paper, we address <e2>this</e2> visibility degradation problem."
sameAs(e1, e2)
Comment:

4792	"to resolve the problem, we use a generative adversarial <e1>network</e1>, where our generated outputs will be assessed by our discriminative <e2>network</e2> to ensure that our outputs look like real images."
sameAs(e1, e2)
Comment:

4793	"to resolve the problem, we use a generative adversarial network, where our generated <e1>outputs</e1> will be assessed by our discriminative network to ensure that our <e2>outputs</e2> look like real images."
sameAs(e1, e2)
Comment:

4794	"unlike non-raindrop <e1>regions</e1>, raindrop <e2>regions</e2> are formed by rays of reflected light from a figure 1 ."
sameAs(e1, e2)
Comment:

4795	"this attention map is the most critical part of our <e1>network</e1>, since it will guide the next process in the generative <e2>network</e2> to focus on raindrop regions."
sameAs(e1, e2)
Comment:

4796	"the second part of our generative network is an autoencoder, which takes both the <e1>input</e1> image and the attention map as the <e2>input</e2>."
sameAs(e1, e2)
Comment:

4797	"section 2 discusses the related work in the <e1>fields</e1> of raindrop detection and removal, and in the <e2>fields</e2> of the cnn-based image inpainting."
sameAs(e1, e2)
Comment:

4798	"since it is known that the activations of images in the same category are spatially clustered together [5] , a reasonable choice of w y is to align withā y in order to maximize the inner product, and <e1>this</e1> argument holds true for all y. to verify <e2>this</e2> intuition, we use t-sne [23] to visualize the neighbor embeddings of the activation statisticā y and the parameters w y for each category of a pre-trained deep neural network, as shown in figure 2 ."
sameAs(e1, e2)
Comment:

4799	"the first one is miniimagenet [29] , a simplified subset of imagenet ilsvrc 2015 [27] , in which c large has 80 <e1>categories</e1> and c few has 20 <e2>categories</e2>."
sameAs(e1, e2)
Comment:

4800	"formally, in the few-shot learning problem [14, 24, 29] , we are provided with a large-scale set d large with <e1>categories</e1> c large and a few-shot set d few with <e2>categories</e2> c few that do not overlap with c large ."
sameAs(e1, e2)
Comment:

4801	"abstract in this work, we study 3d <e1>object detection</e1> from rgb-d data introduction recently, great progress has been made on 2d image understanding tasks, such as <e2>object detection</e2> [10] and instance segmentation [11] ."
sameAs(e1, e2)
Comment:

4802	"while pointnets are capable of classifying a whole <e1>point cloud</e1> or predicting a semantic class for each point in a <e2>point cloud</e2>, it is unclear how this architecture can be used for instance-level 3d object detection."
sameAs(e1, e2)
Comment:

4803	"instead, in this work, <e1>we</e1> reduce the search space following the dimension reduction principle: <e2>we</e2> take the advantage of mature 2d object detectors (fig."
sameAs(e1, e2)
Comment:

4804	"in contrast to previous work that treats rgb-d data as 2d <e1>maps</e1> for cnns, our method is more 3d-centric as we lift depth <e2>maps</e2> to 3d point clouds and process them using 3d tools."
sameAs(e1, e2)
Comment:

4805	"in principle, all objects live in 3d space; therefore, we believe <e1>that</e1> many geometric structures, such as repetition, planarity, and symmetry, are more naturally parameterized and captured by learners <e2>that</e2> directly operate in 3d space."
sameAs(e1, e2)
Comment:

4806	"• <e1>we</e1> show how <e2>we</e2> can train 3d object detectors under our framework and achieve state-of-the-art performance on standard 3d object detection benchmarks."
sameAs(e1, e2)
Comment:

4807	"* majority of the work done as an intern at nuro, inc. given <e1>rgb</e1>-d data, we first generate 2d object region proposals in the <e2>rgb</e2> image using a cnn."
sameAs(e1, e2)
Comment:

4808	"there exist a large amount of methods on depth <e1>estimation</e1> [25, 19, 8, 7, 21, 31, 24, 16, 20, 34, 18] and surface normal <e2>estimation</e2> [7, 33, 3, 2, 18 ] from a single image."
sameAs(e1, e2)
Comment:

4809	"on the <e1>one</e1> hand, surface normal is determined by local surface tangent plane of 3d points, which can be estimated from depth; on the <e2>other</e2> hand, depth is constrained by the local surface tangent plane determined by surface normal."
Conjunction(e1, e2)
Comment:

4810	"on the one hand, <e1>surface</e1> normal is determined by local <e2>surface</e2> tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local surface tangent plane determined by surface normal."
sameAs(e1, e2)
Comment:

4811	"on the one hand, <e1>surface</e1> normal is determined by local surface tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local <e2>surface</e2> tangent plane determined by surface normal."
sameAs(e1, e2)
Comment:

4812	"on the one hand, <e1>surface</e1> normal is determined by local surface tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local surface tangent plane determined by <e2>surface</e2> normal."
sameAs(e1, e2)
Comment:

4813	"on the one hand, surface normal is determined by local <e1>surface</e1> tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local <e2>surface</e2> tangent plane determined by surface normal."
sameAs(e1, e2)
Comment:

4814	"on the one hand, surface normal is determined by local <e1>surface</e1> tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local surface tangent plane determined by <e2>surface</e2> normal."
sameAs(e1, e2)
Comment:

4815	"on the one hand, surface normal is determined by local surface tangent plane of 3d points, which can be estimated from depth; on the other hand, depth is constrained by the local <e1>surface</e1> tangent plane determined by <e2>surface</e2> normal."
sameAs(e1, e2)
Comment:

4816	"the <e1>two</e1> networks manage the <e2>two</e2> streams to model the depth-to-normal and normal-to-depth mapping."
sameAs(e1, e2)
Comment:

4817	"it achieves top performance on surface normal <e1>estimation</e1> and is on par with state-of-theart depth <e2>estimation</e2> methods."
sameAs(e1, e2)
Comment:

4818	"the 2.5d geometric information is beneficial to various computer vision tasks, including structure from motion (sfm), 3d reconstruction, <e1>pose estimation</e1>, <e2>object recognition</e2>, and scene classification."
isA(e1, e2)
Comment:

4819	"our <e1>approach</e1>, called imprinting, is to compute these activations from a training image for a new <e2>object category</e2> and use an appropriately scaled version of these activation values as the final layer weights for the new category while leaving the weights of existing categories unchanged."
Used-for(e1, e2)
Comment:

4820	"our approach, called imprinting, is to compute <e1>these</e1> activations from a training image for a new object category and use an appropriately scaled version of <e2>these</e2> activation values as the final layer weights for the new category while leaving the weights of existing categories unchanged."
sameAs(e1, e2)
Comment:

4821	"our approach, called imprinting, is to compute these activations from a training image for a new object category and use an appropriately scaled version of these activation values as the final layer <e1>weights</e1> for the new category while leaving the <e2>weights</e2> of existing categories unchanged."
sameAs(e1, e2)
Comment:

4822	"we consider a low-shot learning scenario where a learner initially trained on base <e1>classes</e1> with abundant samples is then exposed to previously unseen novel <e2>classes</e2> with a limited amount of training data for each category [5] ."
sameAs(e1, e2)
Comment:

4823	"for example, <e1>training</e1> a deep convnet classifier with stochastic gradient descent requires an extensive fine-tuning process that cycles through all prior <e2>training</e2> data together with examples from additional categories [5] ."
sameAs(e1, e2)
Comment:

4824	"however, semantic embeddings are difficult to train due to the computationally expensive hard-negative mining step and these methods require storing all the embedding vectors of encountered examples at test time for nearest neighbor <e1>retrieval</e1> or <e2>classification</e2>."
Conjunction(e1, e2)
Comment:

4825	"experiments show that the imprinted <e1>weights</e1> provide a better starting point than the usual random initialization for fine-tuning all network <e2>weights</e2> and result in better final classification results for low-shot categories."
sameAs(e1, e2)
Comment:

4826	"our imprinting <e1>method</e1> provides a potential <e2>model</e2> for immediate recognition in biological vision as well as a useful approach for on-line updates for novel training data, as in a mobile device or robot."
Used-for(e1, e2)
Comment:

4827	"since point clouds or meshes are not in a <e1>regular</e1> format, most researchers typically transform such data to <e2>regular</e2> 3d voxel grids or collections of images (e.g, views) before feeding them to a deep net architecture."
sameAs(e1, e2)
Comment:

4828	"this data representation transformation, however, renders the resulting <e1>data</e1> unnecessarily voluminous -while also introducing quantization artifacts that can obscure natural invariances of the <e2>data</e2>."
sameAs(e1, e2)
Comment:

4829	"based on the insight that the alignment between input and output sequence elements is monotonic in many problems of interest, we propose an end-to-end differentiable method for learning monotonic alignments which, at test <e1>time</e1>, enables computing attention online and in linear <e2>time</e2>."
sameAs(e1, e2)
Comment:

4830	"we validate our <e1>approach</e1> on sentence summarization, machine translation, and online speech recognition problems and achieve results competitive with existing sequence-tosequence <e2>models</e2>."
Used-for(e1, e2)
Comment:

4831	"introduction recently, the "sequence-to-sequence" framework (sutskever et al, 2014; cho et al, 2014) has facilitated the use of <e1>recurrent neural networks</e1> (<e2>rnns</e2>) on sequence transduction problems such as machine translation and speech recognition."
sameAs(e1, e2)
Comment:

4832	"in <e1>this</e1> framework, an input sequence is processed with an rnn to produce an "encoding"; <e2>this</e2> encoding is then used by a second rnn to produce the target sequence."
sameAs(e1, e2)
Comment:

4833	"in this framework, an input sequence is processed with an <e1>rnn</e1> to produce an "encoding"; this encoding is then used by a second <e2>rnn</e2> to produce the target sequence."
sameAs(e1, e2)
Comment:

4834	"in practice, this often results in the <e1>model</e1> having difficulty generalizing to longer sequences than <e2>those</e2> seen during training ."
Compare(e1, e2)
Comment:

4835	"our approach is based on an encoderdecoder captioning <e1>model</e1>, and can produce spatial or spatiotemporal heatmaps for either a given input caption or a caption predicted by our <e2>model</e2> (fig."
sameAs(e1, e2)
Comment:

4836	"in addition to facilitating visual search, this allows us to expose the inner workings of deep captioning <e1>models</e1> and provide much needed intuition of what these <e2>models</e2> are actually learning."
sameAs(e1, e2)
Comment:

4837	"previous attempts at such model introspection have analyzed lstms trained on text generation [13] , or cnns trained on <e1>image</e1>-level <e2>classification</e2> [31, 32] ."
Used-for(e1, e2)
Comment:

4838	"our <e1>approach</e1> is inspired by the signal drop-out <e2>methods</e2> used to visualize convolutional activations in [30, 32] , however we study lstm based encoder-decoder models and design a novel approach based on information gain."
Compare(e1, e2)
Comment:

4839	"our <e1>approach</e1> is inspired by the signal drop-out methods used to visualize convolutional activations in [30, 32] , however we study lstm based encoder-decoder <e2>models</e2> and design a novel approach based on information gain."
Used-for(e1, e2)
Comment:

4840	"our approach is inspired by the signal drop-out <e1>methods</e1> used to visualize convolutional activations in [30, 32] , however we study lstm based encoder-decoder <e2>models</e2> and design a novel approach based on information gain."
Compare(e1, e2)
Comment:

4841	"this is done by replacing the input image or video by a single <e1>region</e1> and observing the effect on the word in terms of its generation probability given the single <e2>region</e2> only."
sameAs(e1, e2)
Comment:

4842	"we apply our approach to both still image and <e1>video</e1> description scenarios, adapting a popular encoder-decoder model for <e2>video</e2> captioning [22] as our base model."
sameAs(e1, e2)
Comment:

4843	"we apply our approach to both still image and video description scenarios, adapting a popular encoder-decoder <e1>model</e1> for video captioning [22] as our base <e2>model</e2>."
sameAs(e1, e2)
Comment:

4844	"we also use our approach to "explain" what the base <e1>video</e1> captioning model is learning on the publicly available large scale microsoft <e2>video</e2>-to-text (msr-vtt) video captioning dataset [25] ."
sameAs(e1, e2)
Comment:

4845	"we also use our approach to "explain" what the base <e1>video</e1> captioning model is learning on the publicly available large scale microsoft video-to-text (msr-vtt) <e2>video</e2> captioning dataset [25] ."
sameAs(e1, e2)
Comment:

4846	"we also use our approach to "explain" what the base video captioning model is learning on the publicly available large scale microsoft <e1>video</e1>-to-text (msr-vtt) <e2>video</e2> captioning dataset [25] ."
sameAs(e1, e2)
Comment:

4847	"we compare our <e1>approach</e1> to explicit "soft" attention <e2>models</e2> [27, 28] and show that we can obtain similar text generation performance with less computational overhead, while also enabling more accurate localization of words."
Used-for(e1, e2)
Comment:

4848	"we compare our <e1>approach</e1> to explicit "soft" attention models [27, 28] and show that we can obtain similar <e2>text generation</e2> performance with less computational overhead, while also enabling more accurate localization of words."
Used-for(e1, e2)
Comment:

4849	"to close this gap, the task of learning deep classifiers for unseen <e1>classes</e1> from external domain knowledge we present our neuron importance-aware weight transfer (niwt) approach which maps free-form domain knowledge about unseen <e2>classes</e2> to relevant conceptsensitive neurons within a pretrained deep network."
sameAs(e1, e2)
Comment:

4850	"to close this gap, the task of learning deep classifiers for unseen classes from external <e1>domain knowledge</e1> we present our neuron importance-aware weight transfer (niwt) approach which maps free-form <e2>domain knowledge</e2> about unseen classes to relevant conceptsensitive neurons within a pretrained deep network."
sameAs(e1, e2)
Comment:

4851	"as humans, much of the way we acquire and transfer knowledge about novel <e1>concepts</e1> is in reference to or via composition of <e2>concepts</e2> which are already known."
sameAs(e1, e2)
Comment:

4852	"", <e1>we</e1> can compose our understanding of colors and birds to imagine how <e2>we</e2> might distinguish such an animal from other birds."
sameAs(e1, e2)
Comment:

4853	"individual neurons in convolutional neural networks supervised for image-level classification tasks have been shown to implicitly learn semantically meaningful <e1>concepts</e1> ranging from simple textures and shapes to whole or partial objects -forming a "dictionary" of <e2>concepts</e2> acquired through the learning process."
sameAs(e1, e2)
Comment:

4854	"however, this annotation process is <e1>model</e1> dependent and needs to be re-executed for each <e2>model</e2> trained, which makes it expensive and impractical."
sameAs(e1, e2)
Comment:

4855	"these higher-level <e1>features</e1> collapse many underlying concepts in the pursuit of class discrimination; consequentially, accessing lower-level concepts and recombining them in new ways to represent novel classes is difficult with these <e2>features</e2>."
sameAs(e1, e2)
Comment:

4856	"these higher-level features collapse many underlying <e1>concepts</e1> in the pursuit of class discrimination; consequentially, accessing lower-level <e2>concepts</e2> and recombining them in new ways to represent novel classes is difficult with these features."
sameAs(e1, e2)
Comment:

4857	"mapping class descriptions to lower-level <e1>activations</e1> directly on the other hand is complicated by the high intra-class variance of <e2>activations</e2> due to both spatial and visual differences within instances of a class."
sameAs(e1, e2)
Comment:

4858	"in our approach, which <e1>we</e1> call neuron importance-based weight transfer (niwt), <e2>we</e2> learn a mapping between class-specific domain knowledge and the importances of individual neurons within a deep network."
sameAs(e1, e2)
Comment:

4859	"in <e1>this</e1> work we introduce a simple, efficient zero-shot learning approach based on <e2>this</e2> observation."
sameAs(e1, e2)
Comment:

4860	"in <e1>this</e1> way, we connect the description of a previous unseen category to weights of a classifier that can predict <e2>this</e2> category at test time -all without having seen a single image from this category."
sameAs(e1, e2)
Comment:

4861	"in <e1>this</e1> way, we connect the description of a previous unseen category to weights of a classifier that can predict this category at test time -all without having seen a single image from <e2>this</e2> category."
sameAs(e1, e2)
Comment:

4862	"in this way, we connect the description of a previous unseen category to weights of a classifier that can predict <e1>this</e1> category at test time -all without having seen a single image from <e2>this</e2> category."
sameAs(e1, e2)
Comment:

4863	"unlike standard zsl settings which evaluate performance only on unseen <e1>classes</e1>, gzsl considers both unseen and seen <e2>classes</e2> to measure the performance."
sameAs(e1, e2)
Comment:

4864	"our <e1>approach</e1>, which we call neuron importance-aware weight transfer (niwt), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these concepts -essentially learning <e2>classifiers</e2> by discovering and composing learned semantic concepts in deep networks."
Used-for(e1, e2)
Comment:

4865	"our approach, which we call neuron importance-aware weight transfer (niwt), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned <e1>concepts</e1> and then optimizes for network parameters that can effectively combine these <e2>concepts</e2> -essentially learning classifiers by discovering and composing learned semantic concepts in deep networks."
sameAs(e1, e2)
Comment:

4866	"our approach, which we call neuron importance-aware weight transfer (niwt), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned <e1>concepts</e1> and then optimizes for network parameters that can effectively combine these concepts -essentially learning classifiers by discovering and composing learned semantic <e2>concepts</e2> in deep networks."
sameAs(e1, e2)
Comment:

4867	"our approach, which we call neuron importance-aware weight transfer (niwt), learns to map domain knowledge about novel "unseen" classes onto this dictionary of learned concepts and then optimizes for network parameters that can effectively combine these <e1>concepts</e1> -essentially learning classifiers by discovering and composing learned semantic <e2>concepts</e2> in deep networks."
sameAs(e1, e2)
Comment:

4868	"we validate our <e1>approach</e1> across two standard datasets -caltech-ucsd birds (cub) [30] and animals with attributes 2 (awa2) [32] -showing improved performance over existing <e2>methods</e2>."
Compare(e1, e2)
Comment:

4869	"concretely, <e1>we</e1> make the following contributions in this work: • <e2>we</e2> introduce a zero-short learning approach based on mapping unseen class descriptions to neuron importance within a deep network and then optimizing unseen classifier weights to effectively combine these concepts."
sameAs(e1, e2)
Comment:

4870	"concretely, we make the following contributions in this work: • we introduce a zero-short learning <e1>approach</e1> based on mapping unseen class descriptions to neuron importance within a deep network and then optimizing unseen <e2>classifier</e2> weights to effectively combine these concepts."
Used-for(e1, e2)
Comment:

4871	"our <e1>approach</e1> shows improvements over previous <e2>approaches</e2> on the cubirds and awa2 generalized zero-shot learning benchmarks."
Compare(e1, e2)
Comment:

4872	"we argue <e1>that</e1> there is an alternative approach <e2>that</e2> combines the best of both approaches."
sameAs(e1, e2)
Comment:

4873	"we argue that there is an alternative <e1>approach</e1> that combines the best of both <e2>approaches</e2>."
Compare(e1, e2)
Comment:

4874	"consequently, here we apply machine learning to address the weak points, while keeping the engineered architecture, with the goal of 1) improving performance over existing neural networks and the classical <e1>methods</e1> upon which our work is based; 2) achieving real-time flow estimates with accuracy better than the much slower classical <e2>methods</e2>; and 3) reducing memory requirements to make flow more practical for embedded, robotic, and mobile applications."
sameAs(e1, e2)
Comment:

4875	"consequently, here we apply machine learning to address the weak points, while keeping the engineered architecture, with the goal of 1) improving performance over existing neural networks and the classical methods upon which our work is based; 2) achieving real-time <e1>flow</e1> estimates with accuracy better than the much slower classical methods; and 3) reducing memory requirements to make <e2>flow</e2> more practical for embedded, robotic, and mobile applications."
sameAs(e1, e2)
Comment:

4876	"the previous <e1>neural network</e1> <e2>method</e2>, flownet [17] , attempts to learn both of these at once."
Used-for(e1, e2)
Comment:

4877	"in contrast, we tackle the <e1>latter</e1> using deep learning and rely on existing methods to solve the <e2>former</e2>."
Used-for(e1, e2)
Comment:

4878	"at <e1>that</e1> top level of the pyramid, the assumption is <e2>that</e2> the motions between frames are smaller than a few pixels and that, consequently, the convolutional filters can learn meaningful temporal structure."
sameAs(e1, e2)
Comment:

4879	"at <e1>that</e1> top level of the pyramid, the assumption is that the motions between frames are smaller than a few pixels and <e2>that</e2>, consequently, the convolutional filters can learn meaningful temporal structure."
sameAs(e1, e2)
Comment:

4880	"at that top level of the pyramid, the assumption is <e1>that</e1> the motions between frames are smaller than a few pixels and <e2>that</e2>, consequently, the convolutional filters can learn meaningful temporal structure."
sameAs(e1, e2)
Comment:

4881	"at each level of the pyramid we solve for the <e1>flow</e1> using a convolutional network and up-sample the <e2>flow</e2> to the next pyramid level."
sameAs(e1, e2)
Comment:

4882	"as is standard, with classical formulations [38] , we warp <e1>one</e1> image towards the <e2>other</e2> using the current flow, and repeat this process at each pyramid level."
Conjunction(e1, e2)
Comment:

4883	"we train the <e1>network</e1> from coarse to fine to learn the flow correction at each level and add this to the flow output of the <e2>network</e2> above."
sameAs(e1, e2)
Comment:

4884	"we train the network from coarse to fine to learn the <e1>flow</e1> correction at each level and add this to the <e2>flow</e2> output of the network above."
sameAs(e1, e2)
Comment:

4885	"we do not claim to solve the full optical flow <e1>problem</e1> with spynet; we address the same <e2>problem</e2> as traditional approaches and inherit some of their limitations."
sameAs(e1, e2)
Comment:

4886	"additionally, because our <e1>approach</e1> connects past <e2>methods</e2> with new tools, it provides insights into how to move forward."
Compare(e1, e2)
Comment:

4887	"in particular, we find <e1>that</e1> spynet learns spatio-temporal convolutional filters <e2>that</e2> resemble traditional spatio-temporal derivative or gabor filters [2, 24] ."
sameAs(e1, e2)
Comment:

4888	"in summary our contributions are: 1) the combination of traditional coarse-to-fine pyramid methods with deep learning for optical <e1>flow</e1> estimation; 2) a new spynet model that is 96% smaller and faster than flownet; 3) spynet achieves comparable or lower error than flownet on standard benchmarks -sintel, kitti and middlebury; 4) the learned spatio-temporal filters provide insight about what filters are needed for <e2>flow</e2> estimation; 5) the trained network and related code are publicly available for research 2 ."
sameAs(e1, e2)
Comment:

4889	"in summary our contributions are: 1) the combination of traditional coarse-to-fine pyramid methods with deep learning for optical flow <e1>estimation</e1>; 2) a new spynet model that is 96% smaller and faster than flownet; 3) spynet achieves comparable or lower error than flownet on standard benchmarks -sintel, kitti and middlebury; 4) the learned spatio-temporal filters provide insight about what filters are needed for flow <e2>estimation</e2>; 5) the trained network and related code are publicly available for research 2 ."
sameAs(e1, e2)
Comment:

4890	"ideally such a network would <e1>learn</e1> to solve the correspondence problem (short and long range), <e2>learn</e2> filters relevant to the problem, learn what is constant in the sequence, and learn about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4891	"ideally such a network would <e1>learn</e1> to solve the correspondence problem (short and long range), learn filters relevant to the problem, <e2>learn</e2> what is constant in the sequence, and learn about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4892	"ideally such a network would <e1>learn</e1> to solve the correspondence problem (short and long range), learn filters relevant to the problem, learn what is constant in the sequence, and <e2>learn</e2> about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4893	"ideally such a network would learn to solve the correspondence <e1>problem</e1> (short and long range), learn filters relevant to the <e2>problem</e2>, learn what is constant in the sequence, and learn about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4894	"ideally such a network would learn to solve the correspondence problem (short and long range), <e1>learn</e1> filters relevant to the problem, <e2>learn</e2> what is constant in the sequence, and learn about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4895	"ideally such a network would learn to solve the correspondence problem (short and long range), <e1>learn</e1> filters relevant to the problem, learn what is constant in the sequence, and <e2>learn</e2> about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4896	"ideally such a network would learn to solve the correspondence problem (short and long range), learn filters relevant to the problem, <e1>learn</e1> what is constant in the sequence, and <e2>learn</e2> about the spatial structure of the flow and how it relates to the image structure."
sameAs(e1, e2)
Comment:

4897	"ideally such a network would learn to solve the correspondence problem (short and long range), learn filters relevant to the problem, learn what is constant in the sequence, and learn about the spatial <e1>structure</e1> of the flow and how it relates to the image <e2>structure</e2>."
sameAs(e1, e2)
Comment:

4898	"in addition to various regularizers, example reweighting algorithms are popular solutions to these problems, but they require careful tuning of additional <e1>hyperparameters</e1>, such as example mining schedules and regularization <e2>hyperparameters</e2>."
sameAs(e1, e2)
Comment:

4899	"to determine the example <e1>weights</e1>, our method performs a meta gradient descent step on the current mini-batch example <e2>weights</e2> (which are initialized from zero) to minimize the loss on a clean unbiased validation set."
sameAs(e1, e2)
Comment:

4900	"abstract recently it has been shown that policy-gradient introduction <e1>image</e1> captioning aims at generating a natural language description of an <e2>image</e2>."
sameAs(e1, e2)
Comment:

4901	"however, this approach creates a mismatch between training and testing, since at test-time the <e1>model</e1> uses the previously generated words from the <e2>model</e2> distribution to predict the next word."
sameAs(e1, e2)
Comment:

4902	"another line of work proposes "professor-forcing" [9] , a technique that uses adversarial <e1>training</e1> to encourage the dynamics of the recurrent network to be the same when <e2>training</e2> conditioned on ground truth previous words and when sampling freely from the network."
sameAs(e1, e2)
Comment:

4903	"reinforce as we will describe, allows one to optimize the gradient of the expected <e1>reward</e1> by sampling from the model during training, and treating those samples as ground-truth labels (that are re-weighted by the <e2>reward</e2> they deliver)."
sameAs(e1, e2)
Comment:

4904	"reinforce as we will describe, allows one to optimize the gradient of the expected reward by sampling from the <e1>model</e1> during training, and treating <e2>those</e2> samples as ground-truth labels (that are re-weighted by the reward they deliver)."
Compare(e1, e2)
Comment:

4905	"actor-critic methods [14] , which instead train a second "critic" <e1>network</e1> to provide an estimate of the value of each generated word given the policy of an actor <e2>network</e2>, have also been investigated for sequence problems recently [19] ."
sameAs(e1, e2)
Comment:

4906	"in this paper <e1>we</e1> present a new approach to sequence training which <e2>we</e2> call self-critical sequence training (scst), and demonstrate that scst can improve the performance of image captioning systems dramatically."
sameAs(e1, e2)
Comment:

4907	"in this paper we present a new approach to sequence <e1>training</e1> which we call self-critical sequence <e2>training</e2> (scst), and demonstrate that scst can improve the performance of image captioning systems dramatically."
sameAs(e1, e2)
Comment:

4908	"scst is a reinforce <e1>algorithm</e1> that, rather than estimating the reward signal, or how the reward signal should be normalized, utilizes the output of its own test-time inference <e2>algorithm</e2> to normalize the rewards it experiences."
sameAs(e1, e2)
Comment:

4909	"scst is a reinforce algorithm that, rather than estimating the <e1>reward</e1> signal, or how the <e2>reward</e2> signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences."
sameAs(e1, e2)
Comment:

4910	"scst is a reinforce algorithm that, rather than estimating the <e1>reward</e1> signal, or how the reward signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the <e2>rewards</e2> it experiences."
sameAs(e1, e2)
Comment:

4911	"scst is a reinforce algorithm that, rather than estimating the reward <e1>signal</e1>, or how the reward <e2>signal</e2> should be normalized, utilizes the output of its own test-time inference algorithm to normalize the rewards it experiences."
sameAs(e1, e2)
Comment:

4912	"scst is a reinforce algorithm that, rather than estimating the reward signal, or how the <e1>reward</e1> signal should be normalized, utilizes the output of its own test-time inference algorithm to normalize the <e2>rewards</e2> it experiences."
sameAs(e1, e2)
Comment:

4913	"using scst, attempting to estimate the reward signal, as actor-critic methods must do, and estimating normalization, as reinforce algorithms must do, is avoided, while at the same <e1>time</e1> harmonizing the model with respect to its test-<e2>time</e2> inference procedure."
sameAs(e1, e2)
Comment:

4914	"deep learning approaches to sequence model-ing have yielded impressive results on the <e1>task</e1>, dominating the <e2>task</e2> leaderboard."
sameAs(e1, e2)
Comment:

4915	"inspired by the recently introduced encoder/<e1>decoder</e1> paradigm for machine translation using recurrent neural networks (rnns) [2] , [3] , and [4] use a deep convolutional neural network (cnn) to encode the input image, and a long short term memory (lstm) [5] rnn <e2>decoder</e2> to generate the output caption."
sameAs(e1, e2)
Comment:

4916	"inspired by the recently introduced encoder/decoder paradigm for machine translation using <e1>recurrent neural networks</e1> (<e2>rnns</e2>) [2] , [3] , and [4] use a deep convolutional neural network (cnn) to encode the input image, and a long short term memory (lstm) [5] rnn decoder to generate the output caption."
sameAs(e1, e2)
Comment:

4917	"inspired by the recently introduced encoder/decoder paradigm for machine translation using <e1>recurrent neural networks</e1> (rnns) [2] , [3] , and [4] use a deep convolutional neural network (cnn) to encode the input image, and a long short term memory (lstm) [5] <e2>rnn</e2> decoder to generate the output caption."
sameAs(e1, e2)
Comment:

4918	"inspired by the recently introduced encoder/decoder paradigm for machine translation using recurrent neural networks (<e1>rnns</e1>) [2] , [3] , and [4] use a deep convolutional neural network (cnn) to encode the input image, and a long short term memory (lstm) [5] <e2>rnn</e2> decoder to generate the output caption."
sameAs(e1, e2)
Comment:

4919	"it has been shown and we have qualitatively observed <e1>that</e1> captioning systems <e2>that</e2> utilize attention mechanisms lead to better generalization, as these models can compose novel text descriptions based on the recognition of the global and local entities that comprise images."
sameAs(e1, e2)
Comment:

4920	"it has been shown and we have qualitatively observed <e1>that</e1> captioning systems that utilize attention mechanisms lead to better generalization, as these models can compose novel text descriptions based on the recognition of the global and local entities <e2>that</e2> comprise images."
sameAs(e1, e2)
Comment:

4921	"it has been shown and we have qualitatively observed that captioning systems <e1>that</e1> utilize attention mechanisms lead to better generalization, as these models can compose novel text descriptions based on the recognition of the global and local entities <e2>that</e2> comprise images."
sameAs(e1, e2)
Comment:

4922	"a key reason for this phenomenon is <e1>that</e1> deep networks trained on imagenet [12] learn transferable representations <e2>that</e2> are useful for other related tasks."
sameAs(e1, e2)
Comment:

4923	"a key challenge here is the dimensionality of the structured output, which can be on the order of the <e1>number</e1> of pixels times the <e2>number</e2> of objects."
sameAs(e1, e2)
Comment:

4924	"recent work on instance segmentation [38, 45, 44] proposes <e1>complex</e1> graphical models, which results in a <e2>complex</e2> and time-consuming pipeline."
sameAs(e1, e2)
Comment:

4925	"to tackle these challenges, we propose a new model based on a <e1>recurrent neural network</e1> (<e2>rnn</e2>) that utilizes visual attention, to perform instance segmentation."
sameAs(e1, e2)
Comment:

4926	"techniques that combine large graphical models with low-level vision have been proposed to address this problem; however, we propose an end-to-end <e1>recurrent neural network</e1> (<e2>rnn</e2>) architecture with an attention mechanism to model a human-like counting process, and produce detailed instance segmentations."
sameAs(e1, e2)
Comment:

4927	"the network is jointly trained to sequentially produce <e1>regions</e1> of interest as well as a dominant object segmentation within each <e2>region</e2>."
sameAs(e1, e2)
Comment:

4928	"while the standard semantic segmentation problem entails assigning class labels to each pixel in an <e1>image</e1>, it says nothing about the number of instances of each class in the <e2>image</e2>."
sameAs(e1, e2)
Comment:

4929	"segmenting at the instance level is useful for many tasks, such as highlighting the outline of <e1>objects</e1> for improved recognition and allowing robots to delineate and grasp individual <e2>objects</e2>; it plays a key role in autonomous driving as well."
sameAs(e1, e2)
Comment:

4930	"abstract introduction due to the huge amount of publicly available video <e1>data</e1>, there is an increasing interest in methods to analyze these <e2>data</e2>."
sameAs(e1, e2)
Comment:

4931	"to address this problem, <e1>we</e1> propose a weakly supervised method that can learn temporal action segmentation and labeling from unordered action labels, which <e2>we</e2> refer to as action sets."
sameAs(e1, e2)
Comment:

4932	"to address this problem, we propose a weakly supervised method that can learn temporal <e1>action</e1> segmentation and labeling from unordered <e2>action</e2> labels, which we refer to as action sets."
sameAs(e1, e2)
Comment:

4933	"to address this problem, we propose a weakly supervised method that can learn temporal <e1>action</e1> segmentation and labeling from unordered action labels, which we refer to as <e2>action</e2> sets."
sameAs(e1, e2)
Comment:

4934	"to address this problem, we propose a weakly supervised method that can learn temporal action segmentation and labeling from unordered <e1>action</e1> labels, which we refer to as <e2>action</e2> sets."
sameAs(e1, e2)
Comment:

4935	"in order to deal with such an enormously large <e1>search space</e1>, we propose three model components that aim at decomposing the <e2>search space</e2> on three different levels of granularity."
sameAs(e1, e2)
Comment:

4936	"note that context <e1>models</e1> [15, 25] and length <e2>models</e2> [24] have been used before."
sameAs(e1, e2)
Comment:

4937	"to the best of our knowledge, we are the first to use these <e1>models</e1> without being provided any training data that allows to directly infer such <e2>models</e2> from the video annotation."
sameAs(e1, e2)
Comment:

4938	"to the best of our knowledge, we are the first to use these models without being provided any training <e1>data</e1> that allows to directly infer such <e2>models</e2> from the video annotation."
Used-for(e1, e2)
Comment:

4939	"moreover, temporal segmentation and <e1>action</e1> labeling quality is evaluated on unseen videos alone and on videos with <e2>action</e2> sets given at inference time as additional supervision."
sameAs(e1, e2)
Comment:

4940	"moreover, temporal segmentation and action labeling quality is evaluated on unseen <e1>videos</e1> alone and on <e2>videos</e2> with action sets given at inference time as additional supervision."
sameAs(e1, e2)
Comment:

4941	"with the availability of large scale <e1>datasets</e1> such as thumos [9] , activity net [5] , or breakfast [12] , many new <e2>approaches</e2> to temporally locate and classify actions in untrimmed videos emerged [27, 21, 39, 29, 24, 18] ."
Used-for(e1, e2)
Comment:

4942	"typically, the image representation is trained on some auxiliary <e1>task</e1> such as image classification and then employed in an often ad-hoc geometric alignment <e2>model</e2>."
Evaluate-for(e1, e2)
Comment:

4943	"the outcome is that the <e1>image</e1> representation can be trained from rich appearance variations present in different but semantically related <e2>image</e2> pairs, rather than synthetically deformed imagery [14, 29] ."
sameAs(e1, e2)
Comment:

4944	"we show that our approach allows to significantly improve the performance of the baseline deep cnn <e1>alignment</e1> model, achieving stateof-the-art performance on multiple standard benchmarks for semantic <e2>alignment</e2>."
sameAs(e1, e2)
Comment:

4945	"in this work we study the problem of finding category-level <e1>correspondence</e1>, or semantic alignment [1, 20] , where the goal is to establish dense <e2>correspondence</e2> between different objects belonging to the same category, such as the two different motorcycles illustrated in fig."
sameAs(e1, e2)
Comment:

4946	"our proposed approach, the <e1>stack</e1> neural module network or <e2>stack</e2>-nmn, can be trained without layout supervision, and replaces the layout graph of [16] with a stack-based data structure."
sameAs(e1, e2)
Comment:

4947	"our proposed approach, the <e1>stack</e1> neural module network or stack-nmn, can be trained without layout supervision, and replaces the layout graph of [16] with a <e2>stack</e2>-based data structure."
sameAs(e1, e2)
Comment:

4948	"our proposed approach, the stack neural module network or <e1>stack</e1>-nmn, can be trained without layout supervision, and replaces the layout graph of [16] with a <e2>stack</e2>-based data structure."
sameAs(e1, e2)
Comment:

4949	"instead of making <e1>discrete</e1> choices on module layout, in this work we make the layout soft and <e2>continuous</e2>, so that our model can be optimized in a fully differentiable way using gradient descent."
Conjunction(e1, e2)
Comment:

4950	"we also show that this <e1>model</e1> can be extended to handle both visual question answering (vqa) [6] and referential expression grounding (ref) [30] seamlessly in a single <e2>model</e2> by sharing knowledge across related tasks through common routines as in figure 1 ."
sameAs(e1, e2)
Comment:

4951	"yet the predictive power of generic deep <e1>architectures</e1> comes at a cost of lost interpretability, as these <e2>architectures</e2> are essentially black boxes with respect to human understanding of their predictions."
sameAs(e1, e2)
Comment:

4952	"we assess <e1>these</e1> models below and identify tradeoffs between accuracy and interpretability of <e2>these</e2> existing model classes."
sameAs(e1, e2)
Comment:

4953	"existing methods on vqa can be mainly categorized into holistic <e1>approaches</e1> (e.g., [36, 35, 10, 1, 31, 26, 17] ), and neural module <e2>approaches</e2> [5, 4, 16, 19, 21] ."
sameAs(e1, e2)
Comment:

4954	"the major difference between <e1>these</e1> two lines of work is that neural module approaches explicitly decompose the reasoning procedure into a sequence of sub-<e2>tasks</e2>, and have specialized modules to handle the sub-tasks, while holistic approaches do not have explicit sub-task structure, and different kinds of reasoning routines are all handled homogeneously."
Used-for(e1, e2)
Comment:

4955	"the major difference between <e1>these</e1> two lines of work is that neural module approaches explicitly decompose the reasoning procedure into a sequence of sub-tasks, and have specialized modules to handle the sub-<e2>tasks</e2>, while holistic approaches do not have explicit sub-task structure, and different kinds of reasoning routines are all handled homogeneously."
Used-for(e1, e2)
Comment:

4956	"the major difference between these two lines of work is that neural module <e1>approaches</e1> explicitly decompose the reasoning procedure into a sequence of sub-tasks, and have specialized modules to handle the sub-tasks, while holistic <e2>approaches</e2> do not have explicit sub-task structure, and different kinds of reasoning routines are all handled homogeneously."
sameAs(e1, e2)
Comment:

4957	"the major difference between these two lines of work is that neural module approaches explicitly decompose the reasoning procedure into a sequence of sub-<e1>tasks</e1>, and have specialized modules to handle the sub-<e2>tasks</e2>, while holistic approaches do not have explicit sub-task structure, and different kinds of reasoning routines are all handled homogeneously."
sameAs(e1, e2)
Comment:

4958	"film [26] uses multiple conditional batch normalization layers to fuse the image <e1>representation</e1> and question <e2>representation</e2>."
sameAs(e1, e2)
Comment:

4959	"although <e1>these</e1> models have sequential interactions between the input image and the question, they do not explicitly decompose the reasoning procedure into semanticallytyped sub-<e2>tasks</e2>."
Used-for(e1, e2)
Comment:

4960	"although these <e1>models</e1> have sequential interactions between the input image and the question, they do not explicitly decompose the reasoning procedure into semanticallytyped sub-<e2>tasks</e2>."
Used-for(e1, e2)
Comment:

4961	"in nmn [5] , n2nmn [16] , pg+ee [19] and tbd [21] , the inference <e1>procedure</e1> is performed by analyzing the question and decomposing the reasoning <e2>procedure</e2> into a sequence of sub-tasks."
sameAs(e1, e2)
Comment:

4962	"then the <e1>module</e1> layout is executed with a neural <e2>module</e2> network."
sameAs(e1, e2)
Comment:

4963	"here, given an input question, the layout policy <e1>learns</e1> what sub-tasks to perform, and the neural modules <e2>learn</e2> how to perform each individual sub-tasks."
sameAs(e1, e2)
Comment:

4964	"here, given an input question, the layout policy learns what sub-<e1>tasks</e1> to perform, and the neural modules learn how to perform each individual sub-<e2>tasks</e2>."
sameAs(e1, e2)
Comment:

4965	"in <e1>this</e1> work, we address <e2>this</e2> problem with soft and continuous module layout, making our model fully differentiable and trainable with using gradient descent without resorting to expert layouts."
sameAs(e1, e2)
Comment:

4966	"this includes work aimed at both explaining the decision <e1>rules</e1> implemented by learned models, and the mechanisms by which these <e2>rules</e2> are derived from data [32, 20] ."
sameAs(e1, e2)
Comment:

4967	"different from existing multi-<e1>task</e1> approaches such as sharing common features (e.g., [13] ), our <e2>model</e2> simultaneously handles both visual question answering (vqa) [6] and referential expression grounding (ref) [30] by exploiting the intuition that related tasks should have common sub-routines, and addressing them with a common set of neural modules."
Evaluate-for(e1, e2)
Comment:

4968	"different from existing multi-task approaches such as sharing common <e1>features</e1> (e.g., [13] ), our <e2>model</e2> simultaneously handles both visual question answering (vqa) [6] and referential expression grounding (ref) [30] by exploiting the intuition that related tasks should have common sub-routines, and addressing them with a common set of neural modules."
Used-for(e1, e2)
Comment:

4969	"however, explainable <e1>models</e1> of more complex problems involving multiple sub-<e2>tasks</e2>, such as visual question answering (vqa) [6] and referential expression grounding (ref) [30] , are less studied in comparison."
Used-for(e1, e2)
Comment:

4970	"abstract despite the great advances made by <e1>deep learning</e1> in many machine learning problems, there is a relative dearth of <e2>deep learning</e2> approaches for anomaly detection."
sameAs(e1, e2)
Comment:

4971	"classical ad methods such as the one-class <e1>svm</e1> (oc-<e2>svm</e2>) (schölkopf et al, 2001) or kernel density estimation (kde) (parzen, 1962) , often fail in high-dimensional, datarich scenarios due to bad computational scalability and the curse of dimensionality."
sameAs(e1, e2)
Comment:

4972	"current approaches to deep ad have shown promising <e1>results</e1> (hawkins et al, 2002; sakurada & yairi, 2014; xu et al, 2015; erfani et al, 2016; andrews et al, 2016; chen et al, 2017) , but none of these <e2>methods</e2> are trained by optimizing an ad based objective function and typically rely on reconstruction error based heuristics."
Compare(e1, e2)
Comment:

4973	"our method, deep support vector <e1>data</e1> description (deep svdd), trains a neural network while minimizing the volume of a hypersphere that encloses the network representations of the <e2>data</e2> (see figure 1 )."
sameAs(e1, e2)
Comment:

4974	"we show the effectiveness of our method on <e1>mnist</e1> and cifar-10 image benchmark <e2>datasets</e2> as well as on the detection of adversarial examples of gt-srb stop signs."
isA(e1, e2)
Comment:

4975	"introduction anomaly detection (ad) (chandola et al, 2009; aggarwal, 2016) is the <e1>task</e1> of discerning unusual samples in <e2>data</e2>."
Conjunction(e1, e2)
Comment:

4976	"as real life is more complex, our method relies on <e1>motion</e1> foreground segmentation to localize the salient <e2>motion</e2> and handle non-static video."
sameAs(e1, e2)
Comment:

4977	"to permit non-stationary <e1>video</e1> dynamics, we adopt the wavelet transform for decomposing <e2>video</e2> signals into a time-frequency spectrum."
sameAs(e1, e2)
Comment:

4978	"we derive three <e1>motion</e1> types and three <e2>motion</e2> continuities."
sameAs(e1, e2)
Comment:

4979	"for the 2d perception of 3d intrinsic periodicity, the observer's <e1>viewpoint</e1> can be somewhere in the continuous range between two <e2>viewpoint</e2> extremes."
sameAs(e1, e2)
Comment:

4980	"first and foremost, repetition appears in many forms due to its variety in <e1>motion</e1> pattern and <e2>motion</e2> continuity."
sameAs(e1, e2)
Comment:

4981	"a key challenge is <e1>that</e1> in a realistic video, object sounds are observed not as separate entities, but as a single audio channel <e2>that</e2> mixes all their frequencies together."
sameAs(e1, e2)
Comment:

4982	"our goal is to learn how different <e1>objects</e1> sound by both looking at and listening to unlabeled video containing multiple sounding <e2>objects</e2>."
sameAs(e1, e2)
Comment:

4983	"yet modeling how <e1>objects</e1> look and sound is challenging: most natural scenes and events contain multiple <e2>objects</e2>, and the audio track mixes all the sound sources together."
sameAs(e1, e2)
Comment:

4984	"specifically, <e1>we</e1> use state-of-the-art image recognition tools to infer the objects present in each video clip, and <e2>we</e2> perform non-negative matrix factorization (nmf) on each video's audio channel to recover its set of frequency basis vectors."
sameAs(e1, e2)
Comment:

4985	"specifically, we use state-of-the-art image recognition tools to infer the objects present in each <e1>video</e1> clip, and we perform non-negative matrix factorization (nmf) on each <e2>video</e2>'s audio channel to recover its set of frequency basis vectors."
sameAs(e1, e2)
Comment:

4986	"specifically, we use state-of-the-art image recognition tools to infer the objects present in each video clip, and we perform <e1>non-negative matrix factorization</e1> (<e2>nmf</e2>) on each video's audio channel to recover its set of frequency basis vectors."
sameAs(e1, e2)
Comment:

4987	"from this <e1>audio</e1> basis-object association network, we extract the <e2>audio</e2> bases linked to each visual object, yielding its prototypical spectral patterns."
sameAs(e1, e2)
Comment:

4988	"finally, given a novel video, we use the learned per-object <e1>audio</e1> bases to steer <e2>audio</e2> source separation."
sameAs(e1, e2)
Comment:

4989	"while a resurgence of research on cross-modal learning from images and <e1>audio</e1> also capitalizes on synchronized <e2>audio</e2>-visual data for various tasks [3, 4, 5, 47, 49, 59, 60] , they treat the audio as a single monolithic input, and thus cannot associate different sounds to different objects in the same video."
sameAs(e1, e2)
Comment:

4990	"while a resurgence of research on cross-modal learning from images and <e1>audio</e1> also capitalizes on synchronized audio-visual data for various tasks [3, 4, 5, 47, 49, 59, 60] , they treat the <e2>audio</e2> as a single monolithic input, and thus cannot associate different sounds to different objects in the same video."
sameAs(e1, e2)
Comment:

4991	"while a resurgence of research on cross-modal learning from images and audio also capitalizes on synchronized <e1>audio</e1>-visual data for various tasks [3, 4, 5, 47, 49, 59, 60] , they treat the <e2>audio</e2> as a single monolithic input, and thus cannot associate different sounds to different objects in the same video."
sameAs(e1, e2)
Comment:

4992	"we propose to learn <e1>audio</e1>-visual object models from unlabeled video, then exploit the visual context to perform <e2>audio</e2> source separation in novel videos."
sameAs(e1, e2)
Comment:

4993	"secondly, we propose a novel deep multiinstance multi-label learning <e1>framework</e1> to learn prototypical spectral patterns of different acoustic objects, and inject the learned prior into an nmf source separation <e2>framework</e2>."
sameAs(e1, e2)
Comment:

4994	"we demonstrate stateof-the-art results on visually-aided <e1>audio</e1> source separation and <e2>audio</e2> denoising."
sameAs(e1, e2)
Comment:

4995	"our approach relies on a deep multi-instance multi-label learning framework to disentangle the audio frequency bases that map to individual visual <e1>objects</e1>, even without observing/hearing those <e2>objects</e2> in isolation."
sameAs(e1, e2)
Comment:

4996	"our work is the first to learn <e1>audio</e1> source separation from large-scale "in the wild" videos containing multiple <e2>audio</e2> sources per video."
sameAs(e1, e2)
Comment:

4997	"we obtain state-of-the-art results on visuallyaided <e1>audio</e1> source separation and <e2>audio</e2> denoising."
sameAs(e1, e2)
Comment:

4998	"adversarially learned <e1>one</e1>-class <e2>classifier</e2> for novelty detection"
isA(e1, e2)
Comment:

4999	"the <e1>results</e1> on mnist and caltech-256 image datasets, along with the challenging ucsd ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

5000	"the results on <e1>mnist</e1> and caltech-256 image <e2>datasets</e2>, along with the challenging ucsd ped2 dataset for video anomaly detection illustrate that our proposed method learns the target class effectively and is superior to the baseline and state-of-the-art methods."
isA(e1, e2)
Comment:

5001	"the results on mnist and caltech-256 image datasets, along with the challenging ucsd ped2 dataset for video anomaly detection illustrate that our proposed <e1>method</e1> learns the target class effectively and is superior to the <e2>baseline</e2> and state-of-the-art methods."
Compare(e1, e2)
Comment:

5002	"the results on mnist and caltech-256 image datasets, along with the challenging ucsd ped2 dataset for video anomaly detection illustrate that our proposed <e1>method</e1> learns the target class effectively and is superior to the baseline and state-of-the-art <e2>methods</e2>."
Compare(e1, e2)
Comment:

5003	"r(x) is indeed more separable than only using the original input <e1>image</e1>, x. to build <e2>classification</e2> models when the negative class is absent, poorly sampled or not well defined."
Used-for(e1, e2)
Comment:

5004	"to accurately chart the intrinsic geometry of the positive class, the first step is to efficiently represent the <e1>data</e1> in a way that can entangle more or less the different explanatory factors of variation in the <e2>data</e2>."
sameAs(e1, e2)
Comment:

5005	"some efforts have been made, in recent years, to benefit from deep <e1>features</e1> in learning one-class classifiers [48, 39, 33, 20, 40, 38] , few of which could train an end-to-end feature learning and classification <e2>model</e2>."
Used-for(e1, e2)
Comment:

5006	"inspired by the recent developments in <e1>generative adversarial networks</e1> (<e2>gans</e2>) [14] , we propose an end-to-end model for one-class classification and apply it to different applications including outlier detection, novelty detection in images and anomaly event detection in videos."
sameAs(e1, e2)
Comment:

5007	"the first <e1>module</e1> (denoted as r) refines the input and gradually injects discriminative material into the learning process to make the positive and novelty samples (i.e., inliers, and outliers) more separable for the detector, the second <e2>module</e2> (referred to as d)."
sameAs(e1, e2)
Comment:

5008	"in summary, the main contributions of this paper are as follows: (1) we propose an end-to-end deep network for learning <e1>one</e1>-class <e2>classifier</e2> learning."
isA(e1, e2)
Comment:

5009	"while the accuracy of these <e1>networks</e1> has improved with their increase in depth and width, large <e2>networks</e2> are slow and power hungry."
sameAs(e1, e2)
Comment:

5010	"we introduce an efficient convolutional <e1>module</e1>, esp (efficient spatial pyramid), which is based on the convolutional factorization the large effective receptive field of the esp <e2>module</e2> introduces gridding artifacts, which are removed using hierarchical feature fusion (hff)."
sameAs(e1, e2)
Comment:

5011	"esp is based on a <e1>convolution</e1> factorization principle that decomposes a standard <e2>convolution</e2> into two steps: (1) point-wise convolutions and (2) spatial pyramid of dilated convolutions, as shown in fig."
sameAs(e1, e2)
Comment:

5012	"esp is based on a <e1>convolution</e1> factorization principle that decomposes a standard convolution into two steps: (1) point-wise <e2>convolutions</e2> and (2) spatial pyramid of dilated convolutions, as shown in fig."
sameAs(e1, e2)
Comment:

5013	"esp is based on a <e1>convolution</e1> factorization principle that decomposes a standard convolution into two steps: (1) point-wise convolutions and (2) spatial pyramid of dilated <e2>convolutions</e2>, as shown in fig."
sameAs(e1, e2)
Comment:

5014	"esp is based on a convolution factorization principle that decomposes a standard <e1>convolution</e1> into two steps: (1) point-wise <e2>convolutions</e2> and (2) spatial pyramid of dilated convolutions, as shown in fig."
sameAs(e1, e2)
Comment:

5015	"esp is based on a convolution factorization principle that decomposes a standard <e1>convolution</e1> into two steps: (1) point-wise convolutions and (2) spatial pyramid of dilated <e2>convolutions</e2>, as shown in fig."
sameAs(e1, e2)
Comment:

5016	"esp is based on a convolution factorization principle that decomposes a standard convolution into two steps: (1) point-wise <e1>convolutions</e1> and (2) spatial pyramid of dilated <e2>convolutions</e2>, as shown in fig."
sameAs(e1, e2)
Comment:

5017	"the point-wise <e1>convolutions</e1> help in reducing the computation, while the spatial pyramid of dilated <e2>convolutions</e2> re-samples the feature maps to learn the representations from large effective receptive field."
sameAs(e1, e2)
Comment:

5018	"existing models based on dilated <e1>convolutions</e1> [1, 3, 18, 19] are large and inefficient, but our esp module generalizes the use of dilated <e2>convolutions</e2> in a novel and efficient way."
sameAs(e1, e2)
Comment:

5019	"espnet is 22 <e1>times</e1> faster (on a standard gpu) and 180 <e2>times</e2> smaller than the state-of-the-art semantic segmentation network pspnet, while its category-wise accuracy is only 8% less."
sameAs(e1, e2)
Comment:

5020	"under the same constraints on memory and computation, espnet outperforms all the current efficient cnn networks such as mobilenet, shufflenet, and enet on both standard <e1>metrics</e1> and our newly introduced performance <e2>metrics</e2> that measure efficiency on edge devices."
sameAs(e1, e2)
Comment:

5021	"introduction deep convolutional neural network (cnn) <e1>models</e1> have achieved high accuracy in visual scene understanding <e2>tasks</e2> [1] [2] [3] ."
Used-for(e1, e2)
Comment:

5022	" introduction the development of powerful<e1> learning algorithm</e1>s such as convolutional neural networks (cnns) has provided an effective pipeline for solving many<e2> classificatio</e2>n problems [30] ."
Used-for(e1, e2)
Comment:

5023	"we achieve this by using unsupervised <e1>data</e1> sampled from the target distribution to guide the supervised learning procedure that uses <e2>data</e2> sampled from the source distribution."
sameAs(e1, e2)
Comment:

5024	"it should be noted <e1>that</e1> while there have been a few approaches <e2>that</e2> use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed approach is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure that is modeled using a variant of generative adversarial networks (gans) [7] ."
sameAs(e1, e2)
Comment:

5025	"it should be noted <e1>that</e1> while there have been a few approaches that use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed approach is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure <e2>that</e2> is modeled using a variant of generative adversarial networks (gans) [7] ."
sameAs(e1, e2)
Comment:

5026	"it should be noted that while there have been a few approaches <e1>that</e1> use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed approach is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure <e2>that</e2> is modeled using a variant of generative adversarial networks (gans) [7] ."
sameAs(e1, e2)
Comment:

5027	"it should be noted that while there have been a few approaches that use an adversarial <e1>framework</e1> for solving the domain adaptation problem, the novelty of the proposed <e2>approach</e2> is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure that is modeled using a variant of generative adversarial networks (gans) [7] ."
Used-for(e1, e2)
Comment:

5028	"it should be noted that while there have been a few approaches that use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed <e1>approach</e1> is in using a joint generative discriminative <e2>method</e2>: the embeddings are learned using a combination of classification loss and an image generation procedure that is modeled using a variant of generative adversarial networks (gans) [7] ."
Compare(e1, e2)
Comment:

5029	"it should be noted that while there have been a few approaches that use an adversarial framework for solving the domain adaptation problem, the novelty of the proposed approach is in using a joint generative discriminative method: the embeddings are learned using a combination of classification loss and an image generation procedure that is modeled using a variant of <e1>generative adversarial networks</e1> (<e2>gans</e2>) [7] ."
sameAs(e1, e2)
Comment:

5030	"during training, the source images are passed through the feature extraction <e1>network</e1> (encoder) to obtain an embedding which is then used by the label prediction <e2>network</e2> (classifier) for predicting the source label and also used by the generator to generate a realistic source image."
sameAs(e1, e2)
Comment:

5031	"the encoder is updated based on the discriminative <e1>gradients</e1> from the classifier and generative <e2>gradients</e2> from the adversarial framework."
sameAs(e1, e2)
Comment:

5032	"in the training phase, our pipeline consists of two parallel streams -(1) stream 1: <e1>classification</e1> branch where f-c networks are updated using supervised <e2>classification</e2> loss and (2) stream 2: adversarial branch which is a auxiliary classifier gan (acgan) framework (g-d pair)."
sameAs(e1, e2)
Comment:

5033	"note: the auxiliary classifier in acgan uses only the source domain labels, and is needed to ensure <e1>that</e1> class-consistent images are generated (e.g) embedding of digit 3 generates an image <e2>that</e2> looks like 3. in the test phase, we remove stream 2, and classification is performed using the f-c pair after training, we show that the network has indeed learned to bring the source and target distributions closer."
sameAs(e1, e2)
Comment:

5034	"note: the auxiliary classifier in acgan uses only the source domain labels, and is needed to ensure <e1>that</e1> class-consistent images are generated (e.g) embedding of digit 3 generates an image that looks like 3. in the test phase, we remove stream 2, and classification is performed using the f-c pair after training, we show <e2>that</e2> the network has indeed learned to bring the source and target distributions closer."
sameAs(e1, e2)
Comment:

5035	"note: the auxiliary classifier in acgan uses only the source domain labels, and is needed to ensure that class-consistent images are generated (e.g) embedding of digit 3 generates an <e1>image</e1> that looks like 3. in the test phase, we remove stream 2, and <e2>classification</e2> is performed using the f-c pair after training, we show that the network has indeed learned to bring the source and target distributions closer."
Used-for(e1, e2)
Comment:

5036	"note: the auxiliary classifier in acgan uses only the source domain labels, and is needed to ensure that class-consistent images are generated (e.g) embedding of digit 3 generates an image <e1>that</e1> looks like 3. in the test phase, we remove stream 2, and classification is performed using the f-c pair after training, we show <e2>that</e2> the network has indeed learned to bring the source and target distributions closer."
sameAs(e1, e2)
Comment:

5037	"note: the auxiliary classifier in acgan uses only the source domain labels, and is needed to ensure that class-consistent images are generated (e.g) embedding of digit 3 generates an image that looks like 3. in the test phase, <e1>we</e1> remove stream 2, and classification is performed using the f-c pair after training, <e2>we</e2> show that the network has indeed learned to bring the source and target distributions closer."
sameAs(e1, e2)
Comment:

5038	"our experiments show that the proposed <e1>approach</e1> yields superior results compared to similar <e2>approaches</e2> which update the embedding based on auto-encoders [5] or disentangling the domain information from the embedding by learning a separate domain classifier [4] ."
Compare(e1, e2)
Comment:

5039	"our experiments show that the proposed <e1>approach</e1> yields superior results compared to similar approaches which update the embedding based on auto-encoders [5] or disentangling the domain information from the embedding by learning a separate domain <e2>classifier</e2> [4] ."
Used-for(e1, e2)
Comment:

5040	"our experiments show that the proposed approach yields superior results compared to similar approaches which update the embedding based on auto-encoders [5] or disentangling the <e1>domain</e1> information from the embedding by learning a separate <e2>domain</e2> classifier [4] ."
sameAs(e1, e2)
Comment:

5041	"while labeled <e1>data</e1> is available and getting labeled <e2>data</e2> has been easier over the years, the lack of uniformity of label distributions across different domains results in suboptimal performance of even the most powerful cnn-based algorithms on realistic unseen test data."
sameAs(e1, e2)
Comment:

5042	"while labeled <e1>data</e1> is available and getting labeled data has been easier over the years, the lack of uniformity of label distributions across different domains results in suboptimal performance of even the most powerful cnn-based algorithms on realistic unseen test <e2>data</e2>."
sameAs(e1, e2)
Comment:

5043	"while labeled data is available and getting labeled <e1>data</e1> has been easier over the years, the lack of uniformity of label distributions across different domains results in suboptimal performance of even the most powerful cnn-based algorithms on realistic unseen test <e2>data</e2>."
sameAs(e1, e2)
Comment:

5044	"for example, labeled <e1>synthetic data</e1> is available in plenty but al- * first two authors contributed equally gorithms trained only on <e2>synthetic data</e2> perform poorly on real data."
sameAs(e1, e2)
Comment:

5045	"the use of such unlabeled <e1>target</e1> data to mitigate the shift between source and <e2>target</e2> distributions is the most useful direction among domain adaptation approaches."
sameAs(e1, e2)
Comment:

5046	"among the recent <e1>techniques</e1> that address this problem, fcn in the wild [14] is the only <e2>approach</e2> that uses an adversarial framework."
Compare(e1, e2)
Comment:

5047	"among the recent techniques <e1>that</e1> address this problem, fcn in the wild [14] is the only approach <e2>that</e2> uses an adversarial framework."
sameAs(e1, e2)
Comment:

5048	"however, unlike [14] where a discriminator operates directly on the feature space, we project the features to the <e1>image</e1> space using a generator and the discriminator operates on this projected <e2>image</e2> space."
sameAs(e1, e2)
Comment:

5049	"the main contribution of this work is <e1>that</e1> we propose a technique <e2>that</e2> employs generative models to align the source and target distributions in the feature space."
sameAs(e1, e2)
Comment:

5050	"we then impose the domain alignment constraint by forcing the network to learn <e1>features</e1> such that source <e2>features</e2> produce target-like images when passed to the reconstruction module and vice versa."
sameAs(e1, e2)
Comment:

5051	"the other challenge lies in collecting the <e1>data</e1>: while natural images are easier to obtain, there are certain domains like medical imaging where collecting <e2>data</e2> and finding experts to precisely label them can also be very expensive."
sameAs(e1, e2)
Comment:

5052	"specifically, it requires an agent to answer a sequence of questions about an <e1>image</e1>, requiring it to reason about both the <e2>image</e2> and the past dialog history."
sameAs(e1, e2)
Comment:

5053	"visual <e1>dialog</e1> entails answering a series of questions grounded in an image, using <e2>dialog</e2> history as context."
sameAs(e1, e2)
Comment:

5054	"in addition to the challenges found in visual question answering (vqa), which can be seen as oneround <e1>dialog</e1>, visual <e2>dialog</e2> encompasses several more."
sameAs(e1, e2)
Comment:

5055	"furthermore, we qualitatively show that our <e1>model</e1> is (a) more interpretable (a user can inspect which entities were detected and tracked as the dialog progresses, and which ones were referred to for answering a specific question), (b) more grounded (where the <e2>model</e2> looked to answer a question in the dialog), (c) more consistent (same entities are considered across rounds of dialog)."
sameAs(e1, e2)
Comment:

5056	"furthermore, we qualitatively show that our model is (a) more interpretable (a user can inspect which entities were detected and tracked as the <e1>dialog</e1> progresses, and which ones were referred to for answering a specific question), (b) more grounded (where the model looked to answer a question in the <e2>dialog</e2>), (c) more consistent (same entities are considered across rounds of dialog)."
sameAs(e1, e2)
Comment:

5057	"furthermore, we qualitatively show that our model is (a) more interpretable (a user can inspect which entities were detected and tracked as the <e1>dialog</e1> progresses, and which ones were referred to for answering a specific question), (b) more grounded (where the model looked to answer a question in the dialog), (c) more consistent (same entities are considered across rounds of <e2>dialog</e2>)."
sameAs(e1, e2)
Comment:

5058	"furthermore, we qualitatively show that our model is (a) more interpretable (a user can inspect which entities were detected and tracked as the dialog progresses, and which ones were referred to for answering a specific question), (b) more grounded (where the model looked to answer a question in the <e1>dialog</e1>), (c) more consistent (same entities are considered across rounds of <e2>dialog</e2>)."
sameAs(e1, e2)
Comment:

5059	"we demonstrate the effectiveness of our <e1>model</e1> on mnist dialog, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on visdial, a large and challenging visual dialog dataset on real images, where our <e2>model</e2> outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively."
sameAs(e1, e2)
Comment:

5060	"we demonstrate the effectiveness of our model on mnist <e1>dialog</e1>, a visually simple yet coreference-wise complex dataset, by achieving near perfect accuracy, and on visdial, a large and challenging visual <e2>dialog</e2> dataset on real images, where our model outperforms other approaches, and is more interpretable, grounded, and consistent qualitatively."
sameAs(e1, e2)
Comment:

5061	"in essence, this work trades off <e1>model</e1> complexity and performance for more transparency into the working of the <e2>model</e2>."
sameAs(e1, e2)
Comment:

5062	"our <e1>approach</e1> is a generalization of cam [47] and is applicable to a significantly broader range of cnn model families: (1) cnns with fully-connected <e2>layers</e2> (e.g."
Used-for(e1, e2)
Comment:

5063	"in contrast, localization approaches like cam or our proposed method gradient-weighted class activation mapping (grad-cam), are highly class-discriminative (the 'cat' explanation exclusively highlights the 'cat' <e1>regions</e1> but not 'dog' <e2>regions</e2> in fig."
sameAs(e1, e2)
Comment:

5064	"in order to combine the best of both worlds, we show <e1>that</e1> it is possible to fuse existing pixel-space gradient visualizations with grad-cam to create guided grad-cam visualizations <e2>that</e2> are both high-resolution and class-discriminative."
sameAs(e1, e2)
Comment:

5065	"as a result, important regions of the <e1>image</e1> which correspond to any decision of interest are visualized in high-resolution detail even if the <e2>image</e2> contains evidence for multiple possible concepts, as shown in figures 1d and 1j ."
sameAs(e1, e2)
Comment:

5066	"for captioning and vqa, our visualizations expose the somewhat surprising insight that common cnn + lstm models are often good at localizing discriminative <e1>image</e1> regions despite not being trained on grounded <e2>image</e2>-text pairs."
sameAs(e1, e2)
Comment:

5067	"going from deep to shallow <e1>layers</e1>, the discriminative ability of grad-cam significantly reduces as we encounter <e2>layers</e2> with different output dimensionality."
sameAs(e1, e2)
Comment:

5068	"in order to build trust in intelligent systems and move towards their meaningful integration into our everyday lives, it is clear <e1>that</e1> we must build 'transparent' models <e2>that</e2> explain why they predict what they predict."
sameAs(e1, e2)
Comment:

5069	"in order to build trust in intelligent systems and move towards their meaningful integration into our everyday lives, it is clear that we must build 'transparent' models that explain why <e1>they</e1> predict what <e2>they</e2> predict."
sameAs(e1, e2)
Comment:

5070	"the recent advances in parallel processing and <e1>deep learning</e1> (dl) have contributed to improve many computer vision tasks, such as <e2>object detection</e2>/recognition and optical character recognition (ocr), which clearly benefit alpr systems."
Used-for(e1, e2)
Comment:

5071	"an additional contribution is the massive use of synthetically warped versions of real <e1>images</e1> for augmenting the training dataset, allowing the network to be trained from scratch using less than 200 manually labeled <e2>images</e2>."
sameAs(e1, e2)
Comment:

5072	"the proposed network and data augmentation scheme also led to a flexible alpr <e1>system</e1> that was able to successfully detect and recognize lps in independent test datasets using the same <e2>system</e2> parametrization."
sameAs(e1, e2)
Comment:

5073	"in order to determine how close the generated and target distributions are, a class of <e1>divergences</e1>, the so-called adversarial <e2>divergences</e2> was defined and explored by (liu et al, 2017 )."
sameAs(e1, e2)
Comment:

5074	"this class is broad enough to encompass most popular <e1>gan</e1> methods such as the original <e2>gan</e2> (goodfellow et al, 2014) , f -gans (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein gans  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5075	"this class is broad enough to encompass most popular <e1>gan</e1> methods such as the original gan (goodfellow et al, 2014) , f -<e2>gans</e2> (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein gans  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5076	"this class is broad enough to encompass most popular <e1>gan</e1> methods such as the original gan (goodfellow et al, 2014) , f -gans (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein <e2>gans</e2>  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5077	"this class is broad enough to encompass most popular gan methods such as the original <e1>gan</e1> (goodfellow et al, 2014) , f -<e2>gans</e2> (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein gans  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5078	"this class is broad enough to encompass most popular gan methods such as the original <e1>gan</e1> (goodfellow et al, 2014) , f -gans (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein <e2>gans</e2>  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5079	"this class is broad enough to encompass most popular gan methods such as the original gan (goodfellow et al, 2014) , f -<e1>gans</e1> (nowozin et al, 2016) , moment matching networks (li et al, 2015) , wasserstein <e2>gans</e2>  and the tractable version thereof, the wgan-gp (gulrajani et al, 2017) ."
sameAs(e1, e2)
Comment:

5080	"gans learn a generative model with distribution q by minimizing an <e1>objective function</e1> τ (p q) measuring the similarity between target and generated distributions p and q. in most gan settings, the <e2>objective function</e2> to be minimized is an adversarial divergence (liu et al, 2017) , where a critic function is learned that distinguishes between target and generated data."
sameAs(e1, e2)
Comment:

5081	"gans learn a generative model with distribution q by minimizing an objective function τ (p q) measuring the similarity between <e1>target</e1> and generated distributions p and q. in most gan settings, the objective function to be minimized is an adversarial divergence (liu et al, 2017) , where a critic function is learned that distinguishes between <e2>target</e2> and generated data."
sameAs(e1, e2)
Comment:

5082	"in these situations, updates to the generator don't significantly reduce the <e1>divergence</e1> between generated and target distributions; if there always was a significant reduction in the <e2>divergence</e2> then the generated distribution would converge to the target."
sameAs(e1, e2)
Comment:

5083	"in these situations, updates to the generator don't significantly reduce the divergence between generated and <e1>target</e1> distributions; if there always was a significant reduction in the divergence then the generated distribution would converge to the <e2>target</e2>."
sameAs(e1, e2)
Comment:

5084	"after stating these conditions, we devote section 5 to defining a <e1>divergence</e1>, the penalized wasserstein <e2>divergence</e2> that fulfills the first two basic requirements."
sameAs(e1, e2)
Comment:

5085	"this divergence enforces not just correct values for the critic, but also ensures <e1>that</e1> the critic's gradient, its first order information, assumes values <e2>that</e2> allow for an easy formulation of an update rule."
sameAs(e1, e2)
Comment:

5086	"we hope <e1>that</e1> this gradient penalty trick will be applied to other popular gan methods and ensure <e2>that</e2> they too return better generator updates."
sameAs(e1, e2)
Comment:

5087	"we verify our method, the first order gan, with image <e1>generation</e1> on celeba, lsun and cifar-10 and set a new state of the art on the one billion word language <e2>generation</e2> task."
sameAs(e1, e2)
Comment:

5088	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2>) (goodfellow et al, 2014) excel at learning generative models of complex distributions, such as images ledig et al, 2017) , textures (jetchev et al, 2016; bergmann et al, 2017; jetchev et al, 2017) , and even texts (gulrajani et al, 2017; heusel et al, 2017) ."
sameAs(e1, e2)
Comment:

5089	"if the ldr images are perfectly aligned, in other words no camera <e1>motion</e1> or object <e2>motion</e2> is observed, the merging problem is considered almost solved [17, 1] ."
sameAs(e1, e2)
Comment:

5090	"our goal is to produce an hdr image from a stack of ldr <e1>images</e1> that can be corrupted by large foreground motions, such as <e2>images</e2> shown on the left."
sameAs(e1, e2)
Comment:

5091	"the last three columns compare the <e1>results</e1> produced by other state-of-theart <e2>methods</e2> and ours where no optical flow alignment is used."
Compare(e1, e2)
Comment:

5092	"in contrast, we regard merging multiple exposure shots into an hdr <e1>image</e1> as an <e2>image</e2> translation problem, which have been actively studied in recent years."
sameAs(e1, e2)
Comment:

5093	"first, unlike [14] , our network is trained end-to-end without optical <e1>flow</e1> alignment, thus intrinsically avoiding artifacts and distortions caused by erroneous <e2>flows</e2>."
sameAs(e1, e2)
Comment:

5094	"in stark contrast to prevailing flow-based hdr <e1>imaging</e1> approaches [14] , this provides a novel perspective and significant insights for hdr <e2>imaging</e2>, and is much faster and more practical."
sameAs(e1, e2)
Comment:

5095	"in stark contrast to <e1>flow</e1>-based methods, we formulate hdr imaging as an image translation problem without optical <e2>flows</e2>."
sameAs(e1, e2)
Comment:

5096	"we also show that our network is robust across various kinds of input ldrs, including <e1>images</e1> with different exposure separations and <e2>images</e2> without correct radiometric calibration."
sameAs(e1, e2)
Comment:

5097	"we performed extensive qualitative and quantitative comparisons to show that our <e1>approach</e1> produces excellent results where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art <e2>methods</e2>, and is robust across various inputs, including images without radiometric calibration."
Compare(e1, e2)
Comment:

5098	"we performed extensive qualitative and quantitative comparisons to show that our approach produces excellent <e1>results</e1> where color artifacts and geometric distortions are significantly reduced compared to existing state-of-the-art <e2>methods</e2>, and is robust across various inputs, including images without radiometric calibration."
Compare(e1, e2)
Comment:

5099	"in this paper, <e1>we</e1> address the task of multi-view novel view synthesis, where <e2>we</e2> are interested in synthesizing a target image with an arbitrary camera pose from given source images."
sameAs(e1, e2)
Comment:

5100	"given a <e1>target</e1> camera pose and an arbitrary number of source images and their camera poses, our goal is to develop a model that can synthesize a <e2>target</e2> image and progressively improve its predictions."
sameAs(e1, e2)
Comment:

5101	"given a target camera <e1>pose</e1> and an arbitrary number of source images and their camera <e2>poses</e2>, our goal is to develop a model that can synthesize a target image and progressively improve its predictions."
sameAs(e1, e2)
Comment:

5102	"while directly regressing pixels can generate structurally consistent <e1>results</e1>, it is susceptible to generating blurry <e2>results</e2> largely in part of the inherent multi-modality of this task."
sameAs(e1, e2)
Comment:

5103	"to step towards developing a <e1>framework</e1> that is able to address the task of multi-view novel view synthesis, we propose an end-to-end trainable <e2>framework</e2> (shown in fig."
sameAs(e1, e2)
Comment:

5104	"the flow predictor estimates flow fields to move the pixels from a source view to a <e1>target</e1> view; the recurrent pixel generator, augmented with an internal memory, iteratively synthesizes and refines a <e2>target</e2> view when a new source view is given."
sameAs(e1, e2)
Comment:

5105	"we compare our <e1>model</e1> against state-of-the-art <e2>methods</e2> on a variety of datasets such as 3d-object models as well as real and synthetic scenes."
Compare(e1, e2)
Comment:

5106	"specifically, our model consists of a flow prediction <e1>module</e1> and a pixel generation <e2>module</e2> to directly leverage information presented in source views as well as hallucinate missing pixels from statistical priors."
sameAs(e1, e2)
Comment:

5107	"introduction with countless encounters of <e1>scenes</e1> and objects , humans learn to build a mental understanding of 3d objects and <e2>scenes</e2> just from 2d cross-sections, which in turn, allows us to imagine an unseen view with little effort."
sameAs(e1, e2)
Comment:

5108	"introduction with countless encounters of scenes and <e1>objects</e1> , humans learn to build a mental understanding of 3d <e2>objects</e2> and scenes just from 2d cross-sections, which in turn, allows us to imagine an unseen view with little effort."
sameAs(e1, e2)
Comment:

5109	"it is thus desirable to derive a framework for obtaining actionable insights into the <e1>model</e1>'s behavior allowing us to automatically improve a <e2>model</e2>'s performance."
sameAs(e1, e2)
Comment:

5110	"assuming a smooth parametric model family (e.g., linear models or <e1>neural networks</e1>), the authors employ the influence functions framework from classical statistics (cook & weisberg (1980) ; also see koh & liang (2017) for a literature review on the topic) to show that this quantity can be estimated much faster than via straightforward model retraining, which makes their <e2>method</e2> tractable in a real-world scenario."
Used-for(e1, e2)
Comment:

5111	"a natural use-case of such a framework is to consider individual test <e1>objects</e1> (or groups of them) on which the model performs poorly and either remove the most "harmful" training <e2>objects</e2> or prioritize a batch of new objects for labeling based on which ones are expected to be the most "helpful," akin to active learning."
sameAs(e1, e2)
Comment:

5112	"a natural use-case of such a framework is to consider individual test <e1>objects</e1> (or groups of them) on which the model performs poorly and either remove the most "harmful" training objects or prioritize a batch of new <e2>objects</e2> for labeling based on which ones are expected to be the most "helpful," akin to active learning."
sameAs(e1, e2)
Comment:

5113	"a natural use-case of such a framework is to consider individual test objects (or groups of them) on which the model performs poorly and either remove the most "harmful" training <e1>objects</e1> or prioritize a batch of new <e2>objects</e2> for labeling based on which ones are expected to be the most "helpful," akin to active learning."
sameAs(e1, e2)
Comment:

5114	"unfortunately, the <e1>method</e1> suggested by koh & liang (2017) heavily relies on the smooth parametric nature of the <e2>model</e2> family."
Used-for(e1, e2)
Comment:

5115	"in particular, <e1>decision tree</e1> ensembles such as random forests (ho, 1995, rf) and gradient boosted <e2>decision trees</e2> (friedman, 2001, gbdt) are probably the most widely used model family in industry, largely due to their state-of-the-art performance on struc-tured and/or multimodal data."
sameAs(e1, e2)
Comment:

5116	"for the first <e1>one</e1>, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the trees' structures) to derive leafrefit and fastleafrefit, a well-founded family of approximations to leave-<e2>one</e2>-out retraining that trade off approximation accuracy for computational complexity."
sameAs(e1, e2)
Comment:

5117	"for the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming <e1>that</e1> a small training sample perturbation does not change the trees' structures) to derive leafrefit and fastleafrefit, a well-founded family of approximations to leave-one-out retraining <e2>that</e2> trade off approximation accuracy for computational complexity."
sameAs(e1, e2)
Comment:

5118	"for the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the <e1>trees</e1>' <e2>structures</e2>) to derive leafrefit and fastleafrefit, a well-founded family of approximations to leave-one-out retraining that trade off approximation accuracy for computational complexity."
isA(e1, e2)
Comment:

5119	"for the first one, leaveone-out retraining, we utilize the inner mechanics of fitting decision trees (in particular, assuming that a small training sample perturbation does not change the trees' structures) to derive leafrefit and fastleafrefit, a well-founded family of <e1>approximations</e1> to leave-one-out retraining that trade off <e2>approximation</e2> accuracy for computational complexity."
sameAs(e1, e2)
Comment:

5120	"for the second, analogously to the influence functions framework, we consider infinitesimal <e1>training</e1> sample weight perturbations and derive leafinfluence and fastleafinfluence, methods for estimating gradients of the model's predictions with respect to <e2>training</e2> objects' weights."
sameAs(e1, e2)
Comment:

5121	"in our experiments we (1) study the conditions under which <e1>our methods</e1>, fastleafrefit and fastleafinfluence, successfully approximate their proxy metrics, (2) demonstrate <e2>our methods</e2>' ability to target training objects which are influential for specific test objects, and (3) show that our algorithms run much faster than straightforward retraining, which makes them applicable in practical scenarios."
sameAs(e1, e2)
Comment:

5122	"in our experiments we (1) study the conditions under which our methods, fastleafrefit and fastleafinfluence, successfully approximate their proxy metrics, (2) demonstrate our methods' ability to target training <e1>objects</e1> which are influential for specific test <e2>objects</e2>, and (3) show that our algorithms run much faster than straightforward retraining, which makes them applicable in practical scenarios."
sameAs(e1, e2)
Comment:

5123	"various problem setups (palczewska et al, 2013; tolomei et al, 2017; fong & vedaldi, 2017) and interpretation methods, both <e1>model</e1>-agnostic (ribeiro et al, 2016; lundberg & lee, 2017) and <e2>model</e2>-specific (shrikumar et al, 2017; tolomei et al, 2017; sundararajan et al, 2017) , have recently been proposed in the literature."
sameAs(e1, e2)
Comment:

5124	"a common trait shared by the majority of these methods is that they do not provide a way of automatically improving the <e1>model</e1>, since the <e2>model</e2> is fixed; the main use-case thus becomes manual analytics by the user or the developer, which is both time and resource-consuming."
sameAs(e1, e2)
Comment:

5125	"it is thus straightforward to perform multi-<e1>scale</e1> zero-shot style transfer by 1) extracting the content and style features via its encoding module, 2) decorating the content features by the patch-based style decorator, and 3) progressively decoding the stylized features with multi-<e2>scale</e2> holistic style adaptation."
sameAs(e1, e2)
Comment:

5126	"it is thus straightforward to perform multi-scale zero-shot style transfer by 1) extracting the content and style <e1>features</e1> via its encoding module, 2) decorating the content <e2>features</e2> by the patch-based style decorator, and 3) progressively decoding the stylized features with multi-scale holistic style adaptation."
sameAs(e1, e2)
Comment:

5127	"it is thus straightforward to perform multi-scale zero-shot style transfer by 1) extracting the content and style <e1>features</e1> via its encoding module, 2) decorating the content features by the patch-based style decorator, and 3) progressively decoding the stylized <e2>features</e2> with multi-scale holistic style adaptation."
sameAs(e1, e2)
Comment:

5128	"it is thus straightforward to perform multi-scale zero-shot style transfer by 1) extracting the content and style features via its encoding module, 2) decorating the content <e1>features</e1> by the patch-based style decorator, and 3) progressively decoding the stylized <e2>features</e2> with multi-scale holistic style adaptation."
sameAs(e1, e2)
Comment:

5129	"especially inspired by the remarkable representative power of convolutional neural networks (cnns), the seminal work by gatys et al [8, 9] discovered that multilevel feature <e1>statistics</e1> extracted from a trained cnn notably represent the characteristics of visual styles, which boosts the development of style transfer <e2>approaches</e2>, either by iterative optimizations [9, 16, 30, 20] or feed-forward networks [5, 13, 18, 20, 31, 28, 29, 25, 3] ."
isA(e1, e2)
Comment:

5130	"our contributions in this paper are threefold: (1) a novel patch-based <e1>feature</e1> manipulation module named as style decorator, transfers the content features to semantically nearest style features and simultaneously minimizes the discrepancy between their holistic <e2>feature</e2> distributions."
sameAs(e1, e2)
Comment:

5131	"our contributions in this paper are threefold: (1) a novel patch-based feature manipulation module named as style decorator, transfers the content <e1>features</e1> to semantically nearest style <e2>features</e2> and simultaneously minimizes the discrepancy between their holistic feature distributions."
sameAs(e1, e2)
Comment:

5132	"(2) a new hourglass network equipped with multi-<e1>scale</e1> style adaptation enables visually plausible multi-<e2>scale</e2> transfer for arbitrary style in one feed-forward pass."
sameAs(e1, e2)
Comment:

5133	"(3) <e1>theoretical analysis</e1> proves that the style decorator module owns superior transferring ability, and the <e2>experimental results</e2> demonstrate the effectiveness of the proposed method with superior visual quality and economical processing cost."
Conjunction(e1, e2)
Comment:

5134	"in contrast, another category [19, 4] manipulates the content <e1>features</e1> under the guidance of the style <e2>features</e2> in a shared high-level feature space."
sameAs(e1, e2)
Comment:

5135	"by decoding the manipulated features back into the <e1>image</e1> space with a style-agnostic <e2>image</e2> decoder, the reconstructed images will be stylized with seamless integration of the style patterns."
sameAs(e1, e2)
Comment:

5136	"existing object detectors directly adopt the pre-trained networks, and as a result there is little flexibility to control/adjust the <e1>network</e1> structures (even for small changes of <e2>network</e2> structure)."
sameAs(e1, e2)
Comment:

5137	"thanks to <e1>these</e1> excellent network structures, the accuracy of many vision <e2>tasks</e2> has been greatly improved."
Used-for(e1, e2)
Comment:

5138	"first, most existing datasets have multiple <e1>video</e1>-level sentence annotations, which usu- ally describe very diverse aspects (regions/segments) of the <e2>video</e2> clip."
sameAs(e1, e2)
Comment:

5139	"figure 2 illustrates the architecture of the proposed approach, which consists of three major components: visual sub-<e1>model</e1>, region-sequence sub-<e2>model</e2> and language sub-model."
sameAs(e1, e2)
Comment:

5140	"figure 2 illustrates the architecture of the proposed approach, which consists of three major components: visual sub-<e1>model</e1>, region-sequence sub-model and language sub-<e2>model</e2>."
sameAs(e1, e2)
Comment:

5141	"figure 2 illustrates the architecture of the proposed approach, which consists of three major components: visual sub-model, region-sequence sub-<e1>model</e1> and language sub-<e2>model</e2>."
sameAs(e1, e2)
Comment:

5142	"the main contributions are summarized as follows: (1) to the best of our knowledge, this is the first work for dense <e1>video</e1> captioning with only <e2>video</e2>-level sentence annotations."
sameAs(e1, e2)
Comment:

5143	"(2) we propose a novel dense video captioning <e1>approach</e1>, which <e2>models</e2> visual cues with lexical-fcn, discovers region-sequence with submodular maximization, and decodes language outputs with sequence-tosequence learning."
Used-for(e1, e2)
Comment:

5144	"(3) we evaluate dense captioning <e1>results</e1> by measuring the performance gap to oracle <e2>results</e2>, and diversity of the dense captions."
sameAs(e1, e2)
Comment:

5145	"though the stein <e1>gradient</e1> estimator has been shown to be a fast and easy way to obtain <e2>gradient</e2> estimates for implicit models."
sameAs(e1, e2)
Comment:

5146	"it is still limited by its simple formulation, i.e., the unjustified choice of both the test function (the kernel feature mapping) and the <e1>regularization</e1> scheme (the frobeniusnorm <e2>regularization</e2> used in ridge regression)."
sameAs(e1, e2)
Comment:

5147	"in this paper, we develop a novel <e1>gradient</e1> estimator for implicit distributions, which is called the spectral stein <e2>gradient</e2> estimator (ssge)."
sameAs(e1, e2)
Comment:

5148	"unlike the previous works that only provide <e1>estimates</e1> at the sample points, our approach directly <e2>estimates</e2> the gradient function, thus allows for a simple and principled out-ofsample extension."
sameAs(e1, e2)
Comment:

5149	"compared to the explicit likelihoods (e.g., gaussian) in <e1>other</e1> deep generative models such as variational autoencoders (vae) (kingma & welling, 2013) , implicit distributions are shown able to capture the complex data manifold that lies in a high dimensional space, leading to more realistic samples generated by gan than <e2>other</e2> models."
sameAs(e1, e2)
Comment:

5150	"compared to the explicit likelihoods (e.g., gaussian) in other deep generative models such as variational autoencoders (vae) (kingma & welling, 2013) , implicit distributions are shown able to capture the complex <e1>data</e1> manifold that lies in a high dimensional space, leading to more realistic samples generated by gan than other <e2>models</e2>."
Used-for(e1, e2)
Comment:

5151	"our <e1>approach</e1> is inspired by recent gan-based image translation <e2>methods</e2>, like pix2pix [9] or cyclegan [10] , that can transform an image from one domain to another."
Compare(e1, e2)
Comment:

5152	"our approach is inspired by recent gan-based <e1>image</e1> translation methods, like pix2pix [9] or cyclegan [10] , that can transform an <e2>image</e2> from one domain to another."
sameAs(e1, e2)
Comment:

5153	"inspired by the recent works of simulating <e1>training</e1> data by physically-based rendering [11] [12] [13] [14] and domain adaptation [15] [16] [17] [18] , we present a fine-tuned process for generating <e2>training</e2> data, then adapting it to real world data."
sameAs(e1, e2)
Comment:

5154	"inspired by the recent works of simulating training <e1>data</e1> by physically-based rendering [11] [12] [13] [14] and domain adaptation [15] [16] [17] [18] , we present a fine-tuned process for generating training <e2>data</e2>, then adapting it to real world data."
sameAs(e1, e2)
Comment:

5155	"inspired by the recent works of simulating training <e1>data</e1> by physically-based rendering [11] [12] [13] [14] and domain adaptation [15] [16] [17] [18] , we present a fine-tuned process for generating training data, then adapting it to real world <e2>data</e2>."
sameAs(e1, e2)
Comment:

5156	"inspired by the recent works of simulating training data by physically-based rendering [11] [12] [13] [14] and domain adaptation [15] [16] [17] [18] , we present a fine-tuned process for generating training <e1>data</e1>, then adapting it to real world <e2>data</e2>."
sameAs(e1, e2)
Comment:

5157	"however, most photometric stereo methods are based on the assumption <e1>that</e1> the object surface is diffuse, <e2>that</e2> is, the appearance of the object is view independent."
sameAs(e1, e2)
Comment:

5158	"it is well known <e1>that</e1> modeling the specularity is difficult as the specular effects are largely caused by the complicated global illumination <e2>that</e2> is usually unknown."
sameAs(e1, e2)
Comment:

5159	"gan-based <e1>image</e1>-to-<e2>image</e2> translation."
sameAs(e1, e2)
Comment:

5160	"we are inspired by the latest success of learning based <e1>image</e1>-to-<e2>image</e2> translation methods, such as conditionalgan [9] , cyclegan [10] , [24] dualgan, and discogan [17] ."
sameAs(e1, e2)
Comment:

5161	"the remarkable capacity of <e1>generative adversarial networks</e1> (<e2>gans</e2>) [25] in modeling data distributions allows these methods to transform images from one domain to another with relatively small amounts of training data, while preserving the intrinsic structure of original images faithfully."
sameAs(e1, e2)
Comment:

5162	"the remarkable capacity of generative adversarial networks (gans) [25] in modeling <e1>data</e1> distributions allows these methods to transform images from one domain to another with relatively small amounts of training <e2>data</e2>, while preserving the intrinsic structure of original images faithfully."
sameAs(e1, e2)
Comment:

5163	"the remarkable capacity of generative adversarial networks (gans) [25] in modeling data distributions allows these methods to transform <e1>images</e1> from one domain to another with relatively small amounts of training data, while preserving the intrinsic structure of original <e2>images</e2> faithfully."
sameAs(e1, e2)
Comment:

5164	"with improved multi-scale training techniques, such as progressive gan [26] and pix2pixhd [27] , <e1>image</e1>-to-<e2>image</e2> translation can be performed at mega pixel resolutions and achieve results of stunning visual quality."
sameAs(e1, e2)
Comment:

5165	"recently, modified <e1>image</e1>-to-<e2>image</e2> translation architectures have been successfully applied to ill-posed or underconstrained vision tasks, including face frontal view synthesis [28] , facial geometry reconstruction [29] [30] [31] [32] , raindrop removal [33] , or shadow removal [34] ."
sameAs(e1, e2)
Comment:

5166	"nonetheless, in general, the reconstruction quality of these methods cannot really surpass <e1>that</e1> of traditional approaches <e2>that</e2> exploit multiple-view geometry and heavily engineered photometric stereo pipelines."
sameAs(e1, e2)
Comment:

5167	"to take the local <e1>image</e1> feature coherence into account, we focus on removing the specular effect on the <e2>image</e2> level and resort to the power of multi-view reconstruction as a post-processing and also a production step."
sameAs(e1, e2)
Comment:

5168	"our <e1>method</e1> shares certain similarity with these <e2>methods</e2>."
Compare(e1, e2)
Comment:

5169	"3 multi-view specular-to-diffuse gan in this section, we introduce s2dnet, a conditional gan that translates multi-view <e1>images</e1> of highly specular scenes into corresponding diffuse <e2>images</e2>."
sameAs(e1, e2)
Comment:

5170	"the <e1>input</e1> to our model is a multi-view sequence of a glossy scene without any additional <e2>input</e2> such as segmentation masks, camera parameters, or light probes."
sameAs(e1, e2)
Comment:

5171	"training <e1>data</e1> to train our model to translate multi-view glossy images to diffuse correspondents, we need appropriate <e2>data</e2> for both domains, i.e., glossy source domain images as inputs, and diffuse images as the target domain."
sameAs(e1, e2)
Comment:

5172	"training data to train our model to translate multi-view glossy <e1>images</e1> to diffuse correspondents, we need appropriate data for both domains, i.e., glossy source domain <e2>images</e2> as inputs, and diffuse images as the target domain."
sameAs(e1, e2)
Comment:

5173	"training data to train our model to translate multi-view glossy <e1>images</e1> to diffuse correspondents, we need appropriate data for both domains, i.e., glossy source domain images as inputs, and diffuse <e2>images</e2> as the target domain."
sameAs(e1, e2)
Comment:

5174	"training data to train our model to translate multi-view glossy images to diffuse correspondents, we need appropriate data for both domains, i.e., glossy source domain <e1>images</e1> as inputs, and diffuse <e2>images</e2> as the target domain."
sameAs(e1, e2)
Comment:

5175	"we exclude three <e1>models</e1> for testing and use the remaining 88 <e2>models</e2> for training."
sameAs(e1, e2)
Comment:

5176	"on one hand, making the two <e1>domains</e1> more similar by choosing similar materials for both <e2>domains</e2> improves the translation quality on synthetic data."
sameAs(e1, e2)
Comment:

5177	"in addition, we orient a directional light source pointing from the camera approximately towards the center of the <e1>scene</e1> and position two additional light sources above the <e2>scene</e2>."
sameAs(e1, e2)
Comment:

5178	"to render the source domain images, we applied the various metal materials defined in pbrt, including copper, <e1>silver</e1>, and <e2>gold</e2>."
Feature-of(e1, e2)
Comment:

5179	"we randomly sample camera positions on the upper hemisphere around the <e1>scene</e1> pointing towards the center of the <e2>scene</e2>."
sameAs(e1, e2)
Comment:

5180	"since <e1>we</e1> collect 5 images of the same scene and the input to our network consists of 3 views, <e2>we</e2> obtain 3 training samples per scene."
sameAs(e1, e2)
Comment:

5181	"since we collect 5 images of the same <e1>scene</e1> and the input to our network consists of 3 views, we obtain 3 training samples per <e2>scene</e2>."
sameAs(e1, e2)
Comment:

5182	"the proposed procedure results in a training dataset of more than 647k <e1>images</e1>, i.e., more than 320k <e2>images</e2> per domain."
sameAs(e1, e2)
Comment:

5183	"for testing, we rendered 2k sequences of <e1>images</e1>, each consisting of 50 <e2>images</e2>."
sameAs(e1, e2)
Comment:

5184	"all qualitative results on synthetic data shown in <e1>this</e1> paper belong to <e2>this</e2> test set."
sameAs(e1, e2)
Comment:

5185	"however, generating a complete shape from 2.5d <e1>surfaces</e1> creates an additional challenge, since the <e2>surfaces</e2> need to be aligned and fused into a single 3d object surface."
sameAs(e1, e2)
Comment:

5186	"nearly all recent 3d <e1>shape</e1> generation methods use object-centered coordinates, where the object's <e2>shape</e2> is represented in a canonical view."
sameAs(e1, e2)
Comment:

5187	"in viewcentered prediction, the <e1>shape</e1> is predicted relative to the viewpoint of the input image, which requires encoding both <e2>shape</e2> and pose."
sameAs(e1, e2)
Comment:

5188	"in object-centered <e1>prediction</e1>, the shape is predicted in a canonical pose, which is standardized across training and <e2>prediction</e2> evaluation."
sameAs(e1, e2)
Comment:

5189	"covered; 3d models used for training must be aligned to a canonical <e1>pose</e1>; and prediction on novel object categories is difficult due to lack of predefined canonical <e2>pose</e2>."
sameAs(e1, e2)
Comment:

5190	"in viewercentered coordinates, the shape is represented in a coordinate system aligned to the viewing perspective of the input image, so a front-view of a car should yield a front-facing <e1>3d model</e1>, while a side-view of a car should generate a sidefacing <e2>3d model</e2>."
sameAs(e1, e2)
Comment:

5191	"this increases the variation of predicted <e1>models</e1>, but also does not require aligned training <e2>models</e2> and generalizes naturally to novel categories."
sameAs(e1, e2)
Comment:

5192	"we examine effects of familiarity by measuring accuracy for novel views of known <e1>objects</e1>, novel instances of known categories, and <e2>objects</e2> from novel categories."
sameAs(e1, e2)
Comment:

5193	"we examine effects of familiarity by measuring accuracy for novel views of known objects, novel instances of known <e1>categories</e1>, and objects from novel <e2>categories</e2>."
sameAs(e1, e2)
Comment:

5194	"our experiments indicate a clear advantage for surface-based <e1>representations</e1> in novel object categories, which likely benefit from the more compact output <e2>representations</e2> relative to voxels."
sameAs(e1, e2)
Comment:

5195	"further, <e1>models</e1> that learn to predict in object-centered coordinates seem to learn and rely on object categorization to a greater degree than <e2>models</e2> trained to predict viewer-centered coordinates."
sameAs(e1, e2)
Comment:

5196	"further, models that <e1>learn</e1> to predict in object-centered coordinates seem to <e2>learn</e2> and rely on object categorization to a greater degree than models trained to predict viewer-centered coordinates."
sameAs(e1, e2)
Comment:

5197	"• we compare the efficacy of volumetric and surfacebased <e1>representations</e1> for predicting 3d shape, showing an advantage for surface-based <e2>representations</e2> on unfamiliar object categories regardless of whether final evaluation is volumetric or surface-based."
sameAs(e1, e2)
Comment:

5198	"• we compare the efficacy of volumetric and surfacebased representations for predicting 3d shape, showing an advantage for <e1>surface</e1>-based representations on unfamiliar object categories regardless of whether final evaluation is volumetric or <e2>surface</e2>-based."
sameAs(e1, e2)
Comment:

5199	"• we examine the impact of prediction in viewercentered and object-centered coordinates and showing <e1>that</e1> networks generalize better to novel shapes if they learn to predict in viewer-centered coordinates (which is not currently common practice), and <e2>that</e2> the coordinate choice significantly changes the embedding learned by the network encoder."
sameAs(e1, e2)
Comment:

5200	"we also find that using viewer-centered coordinates is advantageous for novel <e1>objects</e1>, while object-centered representations are better for more familiar <e2>objects</e2>."
sameAs(e1, e2)
Comment:

5201	"interestingly, the coordinate frame significantly affects the <e1>shape representation</e1> learned, with object-centered placing more importance on implicitly recognizing the object category and viewer-centered producing <e2>shape representations</e2> with less dependence on category recognition."
sameAs(e1, e2)
Comment:

5202	" introduction the recovery of a high<e1> resolutio</e1>n (hr) image or video from its low<e2> resolutio</e2>n (lr) counter part is topic of great interest in digital image processing."
sameAs(e1, e2)
Comment:

5203	"this usually arises in the form of local spatial <e1>correlations</e1> for images and additional temporal <e2>correlations</e2> in videos."
sameAs(e1, e2)
Comment:

5204	"the proposed efficient sub-pixel convolutional neural network (espcn), with two <e1>convolution</e1> layers for feature maps extraction, and a sub-pixel <e2>convolution</e2> layer that aggregates the feature maps from lr space and builds the sr image in a single step."
sameAs(e1, e2)
Comment:

5205	"the proposed efficient sub-pixel convolutional neural network (espcn), with two convolution layers for <e1>feature</e1> maps extraction, and a sub-pixel convolution layer that aggregates the <e2>feature</e2> maps from lr space and builds the sr image in a single step."
sameAs(e1, e2)
Comment:

5206	"the proposed efficient sub-pixel convolutional neural network (espcn), with two convolution layers for feature <e1>maps</e1> extraction, and a sub-pixel convolution layer that aggregates the feature <e2>maps</e2> from lr space and builds the sr image in a single step."
sameAs(e1, e2)
Comment:

5207	"the global sr problem assumes lr <e1>data</e1> to be a low-pass filtered (blurred), downsampled and noisy version of hr <e2>data</e2>."
sameAs(e1, e2)
Comment:

5208	"a key assumption <e1>that</e1> underlies many sr techniques is <e2>that</e2> much of the high-frequency data is redundant and thus can be accurately reconstructed from low frequency components."
sameAs(e1, e2)
Comment:

5209	"while this framework is very successful on the new <e1>classes</e1>, its performance on the old ones suffers dramatically, if the network is not trained on all the <e2>classes</e2> jointly."
sameAs(e1, e2)
Comment:

5210	"it has been known for over a couple of decades in the context of feedforward fully connected <e1>networks</e1> [25, 30] , and needs to be addressed in the current state-of-the-art object detector <e2>networks</e2>, if we want to do incremental learning."
sameAs(e1, e2)
Comment:

5211	"in <e1>this</e1> paper, we present a method to alleviate <e2>this</e2> issue."
sameAs(e1, e2)
Comment:

5212	"using only the training samples for the new <e1>classes</e1>, we propose a method for not only adapting the old network to the new <e2>classes</e2>, but also ensuring performance on the old classes does not degrade."
sameAs(e1, e2)
Comment:

5213	"using only the training samples for the new <e1>classes</e1>, we propose a method for not only adapting the old network to the new classes, but also ensuring performance on the old <e2>classes</e2> does not degrade."
sameAs(e1, e2)
Comment:

5214	"using only the training samples for the new classes, we propose a method for not only adapting the old network to the new <e1>classes</e1>, but also ensuring performance on the old <e2>classes</e2> does not degrade."
sameAs(e1, e2)
Comment:

5215	"the core of our approach is a loss function balancing the interplay between predictions on the new <e1>classes</e1>, i.e., cross-entropy loss, and a new distillation loss which minimizes the discrepancy between responses for old <e2>classes</e2> from the original and the new networks."
sameAs(e1, e2)
Comment:

5216	"our results show that we can add new <e1>classes</e1> incrementally to an existing network without forgetting the original <e2>classes</e2>, and with no access to the original training data."
sameAs(e1, e2)
Comment:

5217	"first, they do not <e1>learn</e1> the distribution of the input x. we argue that in the case of high-dimensional input x where there might exist a low-dimensional representation (such as a low-dimensional manifold) of the data, recovering this structure is important, even if the task at hand is to <e2>learn</e2> the conditional density p(y|x)."
sameAs(e1, e2)
Comment:

5218	"first, they do not learn the distribution of the <e1>input</e1> x. we argue that in the case of high-dimensional <e2>input</e2> x where there might exist a low-dimensional representation (such as a low-dimensional manifold) of the data, recovering this structure is important, even if the task at hand is to learn the conditional density p(y|x)."
sameAs(e1, e2)
Comment:

5219	"motivated by lasserre et al (2006) , we propose a hybrid training framework that regularizes the conditionally-trained bcde <e1>parameters</e1> toward the jointly-trained bjde <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

5220	"our experiments show that 1) hybrid <e1>training</e1> is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid <e2>training</e2> helps to avoid overfitting, performs significantly better than conditional training with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid training encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5221	"our experiments show that 1) hybrid <e1>training</e1> is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid training helps to avoid overfitting, performs significantly better than conditional <e2>training</e2> with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid training encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5222	"our experiments show that 1) hybrid <e1>training</e1> is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid training helps to avoid overfitting, performs significantly better than conditional training with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid <e2>training</e2> encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5223	"our experiments show that 1) hybrid training is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid <e1>training</e1> helps to avoid overfitting, performs significantly better than conditional <e2>training</e2> with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid training encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5224	"our experiments show that 1) hybrid training is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid <e1>training</e1> helps to avoid overfitting, performs significantly better than conditional training with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid <e2>training</e2> encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5225	"our experiments show that 1) hybrid training is competitive for fully-supervised cde, 2) in semi-supervised cde, hybrid training helps to avoid overfitting, performs significantly better than conditional <e1>training</e1> with unlabeled data pre-training, and achieves state-of-the-art results, and 3) hybrid <e2>training</e2> encourages the model to learn better and more robust representations."
sameAs(e1, e2)
Comment:

5226	"crucially, we propose a new hybrid training method that blends the conditional <e1>generative model</e1> with a joint <e2>generative model</e2>."
sameAs(e1, e2)
Comment:

5227	"we show that our hybrid training procedure enables models to achieve competitive results in the <e1>mnist</e1> quadrant prediction task in the fullysupervised setting, and sets new benchmarks in the semi-supervised regime for <e2>mnist</e2>, svhn, and celeba."
sameAs(e1, e2)
Comment:

5228	"introduction conditional density estimation (cde) refers to the problem of estimating a conditional density p(y|x) for the input x and <e1>target</e1> y. in contrast to classification where the <e2>target</e2> y is simply a discrete class label, y is typically continuous or high-dimensional in cde."
sameAs(e1, e2)
Comment:

5229	"introduction conditional density estimation (cde) refers to the problem of estimating a conditional density p(y|x) for the input x and target y. in contrast to classification where the target y is simply a <e1>discrete</e1> class label, y is typically <e2>continuous</e2> or high-dimensional in cde."
Conjunction(e1, e2)
Comment:

5230	"cde problems in which both x and y are high-dimensional have a wide range of important applications, including video <e1>prediction</e1>, cross-modality <e2>prediction</e2> (e.g."
sameAs(e1, e2)
Comment:

5231	"while <e1>we</e1> can clearly see a street scene with objects such as cars and trees, <e2>we</e2> can also reason about the shape and appearance of aspects of the scene hidden from view, such as the continuation of the buildings behind the trees, or the ground underneath the car."
sameAs(e1, e2)
Comment:

5232	"while we can clearly see a street <e1>scene</e1> with objects such as cars and trees, we can also reason about the shape and appearance of aspects of the <e2>scene</e2> hidden from view, such as the continuation of the buildings behind the trees, or the ground underneath the car."
sameAs(e1, e2)
Comment:

5233	"while we can clearly see a street scene with objects such as cars and <e1>trees</e1>, we can also reason about the shape and appearance of aspects of the scene hidden from view, such as the continuation of the buildings behind the <e2>trees</e2>, or the ground underneath the car."
sameAs(e1, e2)
Comment:

5234	"2.5d representations such as depth maps are straightforward to use and learn because there is a <e1>one</e1>-to-<e2>one</e2> mapping between the pixels of an input image and the output representation."
sameAs(e1, e2)
Comment:

5235	"in contrast, a robot or other agent might wish to predict the appearance of a <e1>scene</e1> from a different viewpoint, or reason about which parts of the <e2>scene</e2> are navigable."
sameAs(e1, e2)
Comment:

5236	"unlike a depth map, which stores a single depth value <e1>per</e1> pixel, an ldi represents multiple ordered depths <e2>per</e2> pixel, along with an on the left is an image of a street scene."
sameAs(e1, e2)
Comment:

5237	"on the right, we show our method's predicted 2-layer <e1>texture</e1> and shape for the highlighted area: a,b) show the predicted <e2>textures</e2> for the foreground and background layers respectively, and c,d) show the corresponding predicted inverse depth."
sameAs(e1, e2)
Comment:

5238	"associated color for each depth, representing the multiple intersections of a ray with scene geometry (foreground <e1>objects</e1>, background behind those <e2>objects</e2>, etc.)"
sameAs(e1, e2)
Comment:

5239	"for our purposes, <e1>they</e1> are also appealing as a 3d scene representation as <e2>they</e2> maintain the direct relationship between input pixels and output layers, while allowing for much more flexible and general modeling of scenes."
sameAs(e1, e2)
Comment:

5240	"in particular, given two views of a <e1>scene</e1>, there will often be parts of the <e2>scene</e2> that are hidden from one view but visible from the second."
sameAs(e1, e2)
Comment:

5241	"in section 3, we present our learning setup <e1>that</e1> builds on this insight, and describe a training objective <e2>that</e2> enforces the desired prediction structure."
sameAs(e1, e2)
Comment:

5242	"our insight is to use view synthesis as a proxy task: we enforce that our representation (inferred from a single <e1>image</e1>), when rendered from a novel perspective, matches the true observed <e2>image</e2>."
sameAs(e1, e2)
Comment:

5243	"we find <e1>that</e1> arcs <e2>that</e2> do not use any convolutions show comparable performance to deep convolutional neural networks on challenging datasets like casia webface and omniglot."
sameAs(e1, e2)
Comment:

5244	"in the task of estimating the similarity of two characters from the omniglot dataset, arcs and deep convnets both achieve about 93.4% <e1>accuracy</e1>, whereas convarcs achieve 96.10% <e2>accuracy</e2>."
sameAs(e1, e2)
Comment:

5245	"in the task of face verification on the casia webface dataset, convarcs achieved 81.73% <e1>accuracy</e1> surpassing the 79.48% <e2>accuracy</e2> achieved by a cnn baseline considered."
sameAs(e1, e2)
Comment:

5246	"on the challenging omniglot <e1>one</e1>-shot classification task, our model achieved an accuracy of 98.5%, significantly surpassing the current state-of-the-art set by all <e2>other</e2> methods."
Conjunction(e1, e2)
Comment:

5247	"on the challenging omniglot one-shot classification task, our <e1>model</e1> achieved an accuracy of 98.5%, significantly surpassing the current state-of-the-art set by all other <e2>methods</e2>."
Compare(e1, e2)
Comment:

5248	"this is also the first super-human result achieved for this <e1>task</e1> with a generic <e2>model</e2> that uses only pixel information."
Evaluate-for(e1, e2)
Comment:

5249	"this represents the first super-human result achieved for this <e1>task</e1> with a generic <e2>model</e2> that uses only pixel information."
Evaluate-for(e1, e2)
Comment:

5250	"for models to embody such rich learning capabilities, we believe <e1>that</e1> a crucial characteristic will be the employment of dynamic representations -representations <e2>that</e2> are formed by observing a growing and continually evolving set of features."
sameAs(e1, e2)
Comment:

5251	"we use this data to learn a joint representation of <e1>first</e1> and third-person videos, with only weak supervision, and show its effectiveness for transferring knowledge from the third-person to the <e2>first</e2>-person domain."
sameAs(e1, e2)
Comment:

5252	"our <e1>model</e1> learns a representation from the relationship between these two <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

5253	"abstract several theories in cognitive neuroscience suggest that when people interact with the world, or simulate interactions, they do so from a <e1>first</e1>-person egocentric perspective, and seamlessly transfer knowledge between third-person (observer) and <e2>first</e2>-person (actor)."
sameAs(e1, e2)
Comment:

5254	"this paper takes a step in this direction, with the introduction of charades-ego, a large-scale dataset of paired first-person and third-person <e1>videos</e1>, involving 112 people, with 4000 paired <e2>videos</e2>."
sameAs(e1, e2)
Comment:

5255	"introduction the central idea of model-based <e1>reinforcement learning</e1> is to decompose the <e2>rl</e2> problem into two subproblems: learning a model of the environment, and then planning with this model."
sameAs(e1, e2)
Comment:

5256	"introduction the central idea of model-based reinforcement learning is to decompose the rl problem into two subproblems: learning a <e1>model</e1> of the environment, and then planning with this <e2>model</e2>."
sameAs(e1, e2)
Comment:

5257	"finally, zhou et al [61] replace max <e1>pooling</e1> with global average <e2>pooling</e2> after the final convolution layer of an image classification network."
sameAs(e1, e2)
Comment:

5258	"since average <e1>pooling</e1> aggregates activations across an entire feature map, it encourages the network to look beyond the most discriminative part (which would suffice for max <e2>pooling</e2>)."
sameAs(e1, e2)
Comment:

5259	"in <e1>this</e1> paper, we take a radically different approach to <e2>this</e2> problem."
sameAs(e1, e2)
Comment:

5260	"for the temporal <e1>action</e1> localization task (in which the start and end times of an <e2>action</e2> need to be found), random frame sequences are hidden while training a network on action classification, which forces the network to learn the relevant frames corresponding to an action."
sameAs(e1, e2)
Comment:

5261	"for the temporal <e1>action</e1> localization task (in which the start and end times of an action need to be found), random frame sequences are hidden while training a network on action classification, which forces the network to learn the relevant frames corresponding to an <e2>action</e2>."
sameAs(e1, e2)
Comment:

5262	"for the temporal action localization task (in which the start and end times of an <e1>action</e1> need to be found), random frame sequences are hidden while training a network on action classification, which forces the network to learn the relevant frames corresponding to an <e2>action</e2>."
sameAs(e1, e2)
Comment:

5263	"for the temporal action localization task (in which the start and end times of an action need to be found), random frame sequences are hidden while training a <e1>network</e1> on action classification, which forces the <e2>network</e2> to learn the relevant frames corresponding to an action."
sameAs(e1, e2)
Comment:

5264	"our work has three main contributions: 1) <e1>we</e1> introduce the idea of hide-and-seek for weaklysupervised localization and produce state-of-the-art object localization results on the ilsvrc dataset [36] ; 2) <e2>we</e2> demonstrate the generalizability of the approach on different networks and layers; 3) we extend the idea to the relatively unexplored task of weakly-supervised temporal action localization."
sameAs(e1, e2)
Comment:

5265	"our work has three main contributions: 1) <e1>we</e1> introduce the idea of hide-and-seek for weaklysupervised localization and produce state-of-the-art object localization results on the ilsvrc dataset [36] ; 2) we demonstrate the generalizability of the approach on different networks and layers; 3) <e2>we</e2> extend the idea to the relatively unexplored task of weakly-supervised temporal action localization."
sameAs(e1, e2)
Comment:

5266	"our work has three main contributions: 1) we introduce the idea of hide-and-seek for weaklysupervised localization and produce <e1>state</e1>-of-the-art object localization results on the ilsvrc dataset [36] ; 2) we demonstrate the generalizability of the approach on different networks and layers; 3) we extend the idea to the relatively unexplored task of weakly-supervised temporal <e2>action</e2> localization."
Conjunction(e1, e2)
Comment:

5267	"our work has three main contributions: 1) we introduce the idea of hide-and-seek for weaklysupervised localization and produce state-of-the-art object localization results on the ilsvrc dataset [36] ; 2) <e1>we</e1> demonstrate the generalizability of the approach on different networks and layers; 3) <e2>we</e2> extend the idea to the relatively unexplored task of weakly-supervised temporal action localization."
sameAs(e1, e2)
Comment:

5268	"our work has three main contributions: 1) we introduce the idea of hide-and-seek for weaklysupervised localization and produce state-of-the-art object localization results on the ilsvrc dataset [36] ; 2) we demonstrate the generalizability of the <e1>approach</e1> on different networks and <e2>layers</e2>; 3) we extend the idea to the relatively unexplored task of weakly-supervised temporal action localization."
Used-for(e1, e2)
Comment:

5269	"broadly, optical flow methods describe procedures to relate pixels in one <e1>image</e1> to pixels in another <e2>image</e2> of the same object."
sameAs(e1, e2)
Comment:

5270	"they establish a displacement field that can be thought of as a sampling, or warping, of the input <e1>image</e1> back onto the reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

5271	"the most straight-forward way to estimate a multi-frame optical <e1>flow</e1> is to apply an algorithm that solves the traditional two-frame optical <e2>flow</e2> problem between every frame and the template independently."
sameAs(e1, e2)
Comment:

5272	"an alternative <e1>solution</e1> to the multi-frame optical flow <e2>problem</e2>, also based on two-frame optical flow, is to estimate flow between consecutive frames and then combine the various solutions."
Used-for(e1, e2)
Comment:

5273	"an alternative solution to the multi-frame optical <e1>flow</e1> problem, also based on two-frame optical <e2>flow</e2>, is to estimate flow between consecutive frames and then combine the various solutions."
sameAs(e1, e2)
Comment:

5274	"an alternative solution to the multi-frame optical <e1>flow</e1> problem, also based on two-frame optical flow, is to estimate <e2>flow</e2> between consecutive frames and then combine the various solutions."
sameAs(e1, e2)
Comment:

5275	"an alternative solution to the multi-frame optical flow problem, also based on two-frame optical <e1>flow</e1>, is to estimate <e2>flow</e2> between consecutive frames and then combine the various solutions."
sameAs(e1, e2)
Comment:

5276	"we focus on human <e1>faces</e1> which are a very commonly considered object in computer vision, and dense <e2>face</e2> correspondences are required in many research areas and applications."
sameAs(e1, e2)
Comment:

5277	"that is, the establishment of dense correspondences of deformable <e1>faces</e1> is the first step towards high-performance facial expression recognition [18] , facial motion capture [8] and 3d <e2>face</e2> reconstruction [13] ."
sameAs(e1, e2)
Comment:

5278	"this is attributed to the difficulty of developing a statistical model for dense facial <e1>flow</e1> due to the in-ability of humans to densely annotate sequences and the limited robustness of the optical <e2>flow</e2> techniques [12] ."
sameAs(e1, e2)
Comment:

5279	"in this paper, we build the first, to the best of our knowledge, statistical models of dense facial <e1>flow</e1> by capitalising on the success of recent optical <e2>flow</e2> techniques applied to densely tracking image sequences [14] ."
sameAs(e1, e2)
Comment:

5280	"experimentally, we demonstrate that invariance leads to faster optimization and our techniques improve on traditional natural <e1>gradient</e1> in deep neural network training and natural policy <e2>gradient</e2> for reinforcement learning."
sameAs(e1, e2)
Comment:

5281	"current state-of-the-art training methods are usually variants of <e1>stochastic gradient descent</e1> (<e2>sgd</e2>), such as adagrad (duchi et al, 2011) , rmsprop (hinton et al, 2012) and adam (kingma & ba, 2015) ."
sameAs(e1, e2)
Comment:

5282	"in this paper, <e1>we</e1> propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a clustering quality metric (<e2>we</e2> use the normalized mutual information or nmi metric [21] to measure clustering quality, but other metrics could be used instead.)"
sameAs(e1, e2)
Comment:

5283	"in this paper, we propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a <e1>clustering quality</e1> metric (we use the normalized mutual information or nmi metric [21] to measure <e2>clustering quality</e2>, but other metrics could be used instead.)"
sameAs(e1, e2)
Comment:

5284	"in this paper, we propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a clustering quality <e1>metric</e1> (we use the normalized mutual information or nmi <e2>metric</e2> [21] to measure clustering quality, but other metrics could be used instead.)"
sameAs(e1, e2)
Comment:

5285	"in this paper, we propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a clustering quality <e1>metric</e1> (we use the normalized mutual information or nmi metric [21] to measure clustering quality, but other <e2>metrics</e2> could be used instead.)"
sameAs(e1, e2)
Comment:

5286	"in this paper, we propose a novel learning framework which encourages the network to learn an embedding function that directly optimizes a clustering quality metric (we use the normalized mutual information or nmi <e1>metric</e1> [21] to measure clustering quality, but other <e2>metrics</e2> could be used instead.)"
sameAs(e1, e2)
Comment:

5287	"furthermore, metric learning can be used for challenging extreme classification settings [23, 5, 40] , where the <e1>number</e1> of classes is very large and the <e2>number</e2> of examples per class becomes scarce."
sameAs(e1, e2)
Comment:

5288	"for example, [2] uses this approach to perform product search with 10m <e1>images</e1>, and [25] shows superhuman performance on face verification with 260m <e2>images</e2> of 8m distinct identities."
sameAs(e1, e2)
Comment:

5289	"furthermore, most of the current methods [25, 2, 30, 29] in deep metric learning require a separate <e1>data</e1> preparation stage where the training <e2>data</e2> has to be first prepared in pairs [8, 2] , triplets [39, 25] , or n-pair tuples [29] format."
sameAs(e1, e2)
Comment:

5290	"abstract the tracking-by-detection framework consists of two stages, i.e., drawing samples around the <e1>target</e1> object in the first stage and classifying each sample as the <e2>target</e2> object or as background in the second stage."
sameAs(e1, e2)
Comment:

5291	"the first stage draws a sparse set of samples around the <e1>target</e1> object and the second stage classifies each sample as either the <e2>target</e2> object or as the background using a deep neural network."
sameAs(e1, e2)
Comment:

5292	"extensive experiments on benchmark <e1>datasets</e1> demonstrate that the proposed tracker performs favorably against stateof-the-art <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

5293	"this idea has been recently revisited in the context of deep-learning, where wan et al [34] attempt to learn a manifold of hand poses via a combination of variational autoencoders (vaes) and <e1>generative adversarial networks</e1> (<e2>gans</e2>) for hand pose estimation from depth images."
sameAs(e1, e2)
Comment:

5294	"however, their approach is based on <e1>two</e1> separate manifolds, one for 3d hand joints (vae) and one for depth-maps (gan) and requires a mapping function between the <e2>two</e2>."
sameAs(e1, e2)
Comment:

5295	"however, their approach is based on two separate manifolds, <e1>one</e1> for 3d hand joints (vae) and <e2>one</e2> for depth-maps (gan) and requires a mapping function between the two."
sameAs(e1, e2)
Comment:

5296	"we provide a derivation of the variational lower bound that permits training of a single latent space using multiple modalities, where similar <e1>input</e1> poses are embedded close to each other independent of the <e2>input</e2> modality."
sameAs(e1, e2)
Comment:

5297	"we experimentally show that the proposed <e1>approach</e1> outperforms the state-of-the art <e2>method</e2> [38] in direct rgb to 3d hand pose estimation, as well as in lifting from 2d detections to 3d on a challenging public dataset."
Compare(e1, e2)
Comment:

5298	"furthermore, <e1>we</e1> explore the utility of the same architecture in the case of depth images and show that <e2>we</e2> are comparable to state-of-art depth based methods [15, 16, 34] that employ specialized architectures."
sameAs(e1, e2)
Comment:

5299	"furthermore, we explore the utility of the same <e1>architecture</e1> in the case of depth images and show that we are comparable to state-of-art depth based methods [15, 16, 34] that employ specialized <e2>architectures</e2>."
sameAs(e1, e2)
Comment:

5300	"furthermore, we explore the utility of the same architecture in the case of depth images and show <e1>that</e1> we are comparable to state-of-art depth based methods [15, 16, 34] <e2>that</e2> employ specialized architectures."
sameAs(e1, e2)
Comment:

5301	"much work exists <e1>that</e1> tracks articulated hands in streams of depth images, or <e2>that</e2> estimates hand pose [15, 16, 27, 35] from individual depth frames."
sameAs(e1, e2)
Comment:

5302	"the proposed architecture is specifically designed for the monocular case and splits the <e1>task</e1> into hand and 2d keypoint detection followed by a 2d-3d lifting step but incorporates no explicit hand <e2>model</e2>."
Evaluate-for(e1, e2)
Comment:

5303	"abstract we study the introduction <e1>motion</e1> blur is the result of relative <e2>motion</e2> between the scene and camera, where photons from a single incoming ray of light are spread over multiple sensor pixels during the exposure."
sameAs(e1, e2)
Comment:

5304	"we theoretically study the effects of <e1>motion</e1> blur on a captured light field and present a practical algorithm to deblur light fields of general scenes captured with 3d camera <e2>motion</e2>."
sameAs(e1, e2)
Comment:

5305	"right: absent knowledge of the synthetic <e1>motion</e1> path, our algorithm is able to accurately recover the sharp light field and the <e2>motion</e2> path."
sameAs(e1, e2)
Comment:

5306	"tionally, we show that a light <e1>field</e1> blurred with out-of-plane camera motion is an integral over shears of the sharp light <e2>field</e2> (sec."
sameAs(e1, e2)
Comment:

5307	"in this work, we make both theoretical and practical contributions by studying the effects of camera motion on <e1>light fields</e1> and presenting a method to restore motionblurred <e2>light fields</e2>."
sameAs(e1, e2)
Comment:

5308	"therefore, we can blindly deblur a light <e1>field</e1> of a textured plane captured with out-of-plane camera motion by modulating a slice of the fourier spectrum of the motion-blurred light <e2>field</e2> (figs."
sameAs(e1, e2)
Comment:

5309	"therefore, we can blindly deblur a light field of a textured plane captured with out-of-plane camera <e1>motion</e1> by modulating a slice of the fourier spectrum of the <e2>motion</e2>-blurred light field (figs."
sameAs(e1, e2)
Comment:

5310	"the general light <e1>field</e1> blind motion deblurring problem lacks a simple analytic approach and is severely ill-posed because there is an infinite set of pairs of sharp light fields and motion paths that explain any observed motion-blurred light <e2>field</e2>."
sameAs(e1, e2)
Comment:

5311	"the general light field blind <e1>motion</e1> deblurring problem lacks a simple analytic approach and is severely ill-posed because there is an infinite set of pairs of sharp light fields and <e2>motion</e2> paths that explain any observed motion-blurred light field."
sameAs(e1, e2)
Comment:

5312	"the general light field blind <e1>motion</e1> deblurring problem lacks a simple analytic approach and is severely ill-posed because there is an infinite set of pairs of sharp light fields and motion paths that explain any observed <e2>motion</e2>-blurred light field."
sameAs(e1, e2)
Comment:

5313	"the general light field blind motion deblurring problem lacks a simple analytic approach and is severely ill-posed because there is an infinite set of pairs of sharp light fields and <e1>motion</e1> paths that explain any observed <e2>motion</e2>-blurred light field."
sameAs(e1, e2)
Comment:

5314	"we propose a practical light <e1>field</e1> blind motion deblurring algorithm to correct the complex blurring that occurs in situations where light <e2>field</e2> cameras are useful (sec."
sameAs(e1, e2)
Comment:

5315	"our forward <e1>model</e1> is differentiable with respect to the camera motion path parameterization and the estimated light field, allowing us to simultaneously solve for both using firstorder optimization <e2>methods</e2>."
Compare(e1, e2)
Comment:

5316	"instead of solving for a dense matrix that represents spatially and angularly varying <e1>motion</e1> blurs or separately deblurring each sub-aperture image by solving for a 2d blur kernel and 2d depth map, we directly solve for a parameterization of the continuous camera <e2>motion</e2> curve in r 3 ."
sameAs(e1, e2)
Comment:

5317	"instead of solving for a dense matrix that represents spatially and angularly varying motion <e1>blurs</e1> or separately deblurring each sub-aperture image by solving for a 2d <e2>blur</e2> kernel and 2d depth map, we directly solve for a parameterization of the continuous camera motion curve in r 3 ."
sameAs(e1, e2)
Comment:

5318	"light field cameras are typically used in situations with optically significant scene depth ranges and out-of-plane camera <e1>motion</e1>, so it is important to consider how <e2>motion</e2> blur varies both spatially within each subaperture image and angularly between sub-aperture images."
sameAs(e1, e2)
Comment:

5319	"this is a much lower-dimensional optimization problem, and it allows us to utilize the structure of the light <e1>field</e1> to efficiently recover the motion curve and sharp light <e2>field</e2>."
sameAs(e1, e2)
Comment:

5320	"theory we derive a forward model that describes a <e1>motion</e1>-blurred light field as an integration over transformations of the sharp light field along the camera <e2>motion</e2> path."
sameAs(e1, e2)
Comment:

5321	"theory we derive a forward model that describes a motion-blurred light <e1>field</e1> as an integration over transformations of the sharp light <e2>field</e2> along the camera motion path."
sameAs(e1, e2)
Comment:

5322	"3, 4, 5) , we show <e1>that</e1> capturing a light field enables novel methods of motion deblurring <e2>that</e2> are not possible with just a conventional image."
sameAs(e1, e2)
Comment:

5323	"first, we show that a light <e1>field</e1> blurred with in-plane camera motion is a simple convolution of the sharp light <e2>field</e2> with the camera motion path kernel, regardless of the depth contents of the scene (sec."
sameAs(e1, e2)
Comment:

5324	"first, we show that a light field blurred with in-plane camera <e1>motion</e1> is a simple convolution of the sharp light field with the camera <e2>motion</e2> path kernel, regardless of the depth contents of the scene (sec."
sameAs(e1, e2)
Comment:

5325	"this task is challenging because it demands both distinguishing <e1>objects</e1> from the background and correctly estimating the number of distinct <e2>objects</e2> and their locations."
sameAs(e1, e2)
Comment:

5326	"our experimental results show that this <e1>approach</e1> achieves 27.8 db peak signal to noise ratio (psnr) on the celeba dataset and 33.3 db on the svhn dataset, compared to 22.8 db and 19.0 db provided by the former state-of-the-art <e2>methods</e2>, respectively."
Compare(e1, e2)
Comment:

5327	"in order to resolve or mitigate its ill-posedness, researchers have incorporated image priors such as edge <e1>statistics</e1> (fattal, 2007) , total variation (perrone & favaro, 2014) , and sparse representation (aharon et al, 2006; yang et al, 2010) , which are built on intuition or <e2>statistics</e2> of natural images."
sameAs(e1, e2)
Comment:

5328	"given multiple demonstration videos exhibiting diverse behaviors, our neural <e1>program</e1> synthesizer learn to produce interpretable and executable underlying <e2>programs</e2>."
sameAs(e1, e2)
Comment:

5329	"to empower machines with this ability, we propose a neural <e1>program</e1> synthesizer that is able to explicitly synthesize underlying <e2>programs</e2> from behaviorally diverse and visually complicated demonstration videos."
sameAs(e1, e2)
Comment:

5330	"the formal language is composed of action <e1>blocks</e1>, perception <e2>blocks</e2>, and control flow (e.g."
sameAs(e1, e2)
Comment:

5331	"while <e1>they</e1> can be efficient at mimicking desired behaviors, <e2>they</e2> do not explicitly yield interpretable programs, resulting in inexplicable failure cases."
sameAs(e1, e2)
Comment:

5332	"hence, in this paper, we develop a <e1>model</e1> that synthesizes <e2>programs</e2> from visually complex and sequential inputs that demonstrate more branching conditions and long term effects, increasing the complexity of the underlying programs."
Part-of(e1, e2)
Comment:

5333	"hence, in this paper, we develop a <e1>model</e1> that synthesizes programs from visually complex and sequential inputs that demonstrate more branching conditions and long term effects, increasing the complexity of the underlying <e2>programs</e2>."
Part-of(e1, e2)
Comment:

5334	"hence, in this paper, we develop a model <e1>that</e1> synthesizes programs from visually complex and sequential inputs <e2>that</e2> demonstrate more branching conditions and long term effects, increasing the complexity of the underlying programs."
sameAs(e1, e2)
Comment:

5335	"hence, in this paper, we develop a model that synthesizes <e1>programs</e1> from visually complex and sequential inputs that demonstrate more branching conditions and long term effects, increasing the complexity of the underlying <e2>programs</e2>."
sameAs(e1, e2)
Comment:

5336	"in addition, to enable efficient end-to-end training, we introduce auxiliary tasks to encourage the <e1>model</e1> to learn the knowledge that is essential to infer an underlying <e2>program</e2>."
Part-of(e1, e2)
Comment:

5337	"we extensively evaluate our model in two <e1>environments</e1>: a fully observable, third-person <e2>environment</e2> (karel) and a partially observable, egocentric game (vizdoom)."
sameAs(e1, e2)
Comment:

5338	"we also employ a multi-<e1>task</e1> objective to encourage the <e2>model</e2> to learn meaningful intermediate representations for end-to-end training."
Evaluate-for(e1, e2)
Comment:

5339	"we show that our <e1>model</e1> is able to reliably synthesize underlying <e2>programs</e2> as well as capture diverse behaviors exhibited in demonstrations."
Part-of(e1, e2)
Comment:

5340	"for example, spectral mixture (sm) <e1>kernels</e1> (wilson & adams, 2013 ) approximate all stationary <e2>kernels</e2> using gaussian mixture models in the spectral domain."
sameAs(e1, e2)
Comment:

5341	"the nkn can compactly approximate the kernel <e1>structures</e1> from the automatic statistician grammar, but is fully differentiable, so that the kernel <e2>structures</e2> can be learned with gradient-based optimization."
sameAs(e1, e2)
Comment:

5342	"we show that the nkn can represent nonnegative polynomial functions of its primitive <e1>kernels</e1>, and from this demonstrate universality for the class of stationary <e2>kernels</e2>."
sameAs(e1, e2)
Comment:

5343	"abstract we study the <e1>problem</e1> of attributing the prediction of a deep network to its input features, a <e2>problem</e2> previously studied by several other works."
sameAs(e1, e2)
Comment:

5344	"an attribution of the prediction at <e1>input</e1> x relative to a baseline <e2>input</e2> x is a vector a f (x, x ) = (a 1 , ."
sameAs(e1, e2)
Comment:

5345	"for instance, a deep network <e1>that</e1> predicts a condition based on imaging could help inform the doctor of the part of the image <e2>that</e2> resulted in the recommendation."
sameAs(e1, e2)
Comment:

5346	"we show that they are not satisfied by most known attribution <e1>methods</e1>, which we consider to be a fundamental weakness of those <e2>methods</e2>."
sameAs(e1, e2)
Comment:

5347	"unlike previously proposed methods, integrated <e1>gradients</e1> do not need any instrumentation of the network, and can be computed easily using a few calls to the <e2>gradient</e2> operation, allowing even novice practitioners to easily apply the technique."
sameAs(e1, e2)
Comment:

5348	"in section 6, we demonstrate the ease of applicability over several deep networks, including <e1>two</e1> images networks, <e2>two</e2> text processing networks, and a chemistry network."
sameAs(e1, e2)
Comment:

5349	"in section 6, we demonstrate the ease of applicability over several deep networks, including two images <e1>networks</e1>, two text processing <e2>networks</e2>, and a chemistry network."
sameAs(e1, e2)
Comment:

5350	"these applications demonstrate the use of our technique in either improving our <e1>understanding</e1> of the network, performing debugging, performing rule extraction, or aiding an end user in <e2>understanding</e2> the network's prediction."
sameAs(e1, e2)
Comment:

5351	"these applications demonstrate the use of our technique in either improving our understanding of the <e1>network</e1>, performing debugging, performing rule extraction, or aiding an end user in understanding the <e2>network</e2>'s prediction."
sameAs(e1, e2)
Comment:

5352	"we apply this <e1>method</e1> to a couple of image models, a couple of text models and a chemistry <e2>model</e2>, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better."
Used-for(e1, e2)
Comment:

5353	"we apply this method to a couple of image <e1>models</e1>, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with <e2>models</e2> better."
sameAs(e1, e2)
Comment:

5354	"furthermore, pwc-net is about 17 <e1>times</e1> smaller in size and provides 2 <e2>times</e2> faster inferencing than flownet2."
sameAs(e1, e2)
Comment:

5355	"recently, ilg et al [24] stack several flownetc and flownets networks into a large <e1>model</e1>, called flownet2, which performs on par with state-of-the-art <e2>methods</e2> but runs much faster ( fig."
Compare(e1, e2)
Comment:

5356	"in particular, while a full <e1>resolution</e1> 3d representation contains all of the information about an object, in order to use a voxel-based representation in a deep network that can be trained with available samples and in a reasonable amount of time, it would appear that the <e2>resolution</e2> needs to be significantly reduced."
sameAs(e1, e2)
Comment:

5357	"in particular, while a full resolution 3d <e1>representation</e1> contains all of the information about an object, in order to use a voxel-based <e2>representation</e2> in a deep network that can be trained with available samples and in a reasonable amount of time, it would appear that the resolution needs to be significantly reduced."
sameAs(e1, e2)
Comment:

5358	"in particular, while a full resolution 3d representation contains all of the information about an object, in order to use a voxel-based representation in a deep network <e1>that</e1> can be trained with available samples and in a reasonable amount of time, it would appear <e2>that</e2> the resolution needs to be significantly reduced."
sameAs(e1, e2)
Comment:

5359	"another advantage of using 2d representations is that we can leverage (i) advances in <e1>image</e1> descriptors [22, 26] and (ii) massive <e2>image</e2> databases (such as imagenet [9] ) to pre-train our cnn architectures."
sameAs(e1, e2)
Comment:

5360	"because <e1>images</e1> are ubiquitous and large labeled datasets are abundant, we can learn a good deal about generic features for 2d image categorization and then fine-tune to specifics about <e2>3d model</e2> projections."
Used-for(e1, e2)
Comment:

5361	"since <e1>one</e1> seldom has access to 3d object models, <e2>one</e2> must usually learn to recognize and reason about 3d objects based upon their 2d appearances from various viewpoints."
sameAs(e1, e2)
Comment:

5362	"we present state-of-the-art results on 3d object classification, 3d object <e1>retrieval</e1> using 3d objects, and 3d object <e2>retrieval</e2> using sketches (sect."
sameAs(e1, e2)
Comment:

5363	"for example, on the sketch recognition benchmark [11] a multi-view <e1>cnn</e1> trained on jittered copies performs better than a standard <e2>cnn</e2> trained with the same jittered copies (sect."
sameAs(e1, e2)
Comment:

5364	"while intuitively, it seems logical to build 3d shape <e1>classifiers</e1> directly from 3d models, in this paper we present a seemingly counterintuitive result -that by building <e2>classifiers</e2> of 3d shapes from 2d image renderings of those shapes, we can actually dramatically outperform the classifiers built directly on the 3d representations."
sameAs(e1, e2)
Comment:

5365	"while intuitively, it seems logical to build 3d shape <e1>classifiers</e1> directly from 3d models, in this paper we present a seemingly counterintuitive result -that by building classifiers of 3d shapes from 2d image renderings of those shapes, we can actually dramatically outperform the <e2>classifiers</e2> built directly on the 3d representations."
sameAs(e1, e2)
Comment:

5366	"while intuitively, it seems logical to build 3d shape classifiers directly from 3d models, in this paper <e1>we</e1> present a seemingly counterintuitive result -that by building classifiers of 3d shapes from 2d image renderings of those shapes, <e2>we</e2> can actually dramatically outperform the classifiers built directly on the 3d representations."
sameAs(e1, e2)
Comment:

5367	"while intuitively, it seems logical to build 3d shape classifiers directly from 3d models, in this paper we present a seemingly counterintuitive result -that by building <e1>classifiers</e1> of 3d shapes from 2d image renderings of those shapes, we can actually dramatically outperform the <e2>classifiers</e2> built directly on the 3d representations."
sameAs(e1, e2)
Comment:

5368	"while intuitively, it seems logical to build 3d shape classifiers directly from 3d models, in this paper we present a seemingly counterintuitive result -that by building classifiers of 3d <e1>shapes</e1> from 2d image renderings of those <e2>shapes</e2>, we can actually dramatically outperform the classifiers built directly on the 3d representations."
sameAs(e1, e2)
Comment:

5369	"for example, <e1>colors</e1> of objects, or their parts, can be expressed by combining <e2>color</e2> terms (e.g., "red and white", "green and blue", etc.)."
sameAs(e1, e2)
Comment:

5370	"a collection of <e1>these</e1> phrases constitutes the semantic space of describable attributes and can be used as a basis for communication between a human and computer for various <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

5371	"moreover, given a set of attribute phrases, the score of an <e1>image</e1> with respect to each phrase according to a listener provides a natural embedding of the <e2>image</e2> into the space of semantic attributes."
sameAs(e1, e2)
Comment:

5372	"for example, googlenet employed around 7 million <e1>parameters</e1>, which represented a 9× reduction with respect to its predecessor alexnet, which used 60 million <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

5373	"this has made it feasible to utilize inception networks in big-<e1>data</e1> scenarios [17] , [13] , where huge amount of <e2>data</e2> needed to be processed at reasonable cost or scenarios where memory or computational capacity is inherently limited, for example in mobile vision settings."
sameAs(e1, e2)
Comment:

5374	"for example, if it is deemed necessary to increase the capacity of some inception-style model, the simple transformation of just doubling the <e1>number</e1> of all filter bank sizes will lead to a 4x increase in both computational cost and <e2>number</e2> of parameters."
sameAs(e1, e2)
Comment:

5375	"this means <e1>that</e1> architectural improvements in deep convolutional architecture can be utilized for improving performance for most other computer vision tasks <e2>that</e2> are increasingly reliant on high quality, learned visual features."
sameAs(e1, e2)
Comment:

5376	"we develop a means to encode the location and identity of a single semantic keypoint on an <e1>image</e1> as the extra human guidance, and automatically learn how to integrate it within the part of the network that processes the <e2>image</e2>."
sameAs(e1, e2)
Comment:

5377	"to ground this work, we focus on the specific <e1>problem</e1> of monocular viewpoint estimation-the <e2>problem</e2> of identifying the camera's position with respect to the target object from a single rgb image."
sameAs(e1, e2)
Comment:

5378	"in the first two cases, there is not enough visual information for the <e1>model</e1> to make the correct prediction, whereas in the third case, the <e2>model</e2> cannot identify the visual cues necessary to select among multiple plausible viewpoints."
sameAs(e1, e2)
Comment:

5379	"it computes a distance transform based on the keypoint location, combines it with a one-hot vector <e1>that</e1> indicates the keypoint class label, and then uses these data to generate a weight map <e2>that</e2> is combined with hidden activations from the convolutional layers that operate on the image."
sameAs(e1, e2)
Comment:

5380	"it computes a distance transform based on the keypoint location, combines it with a one-hot vector <e1>that</e1> indicates the keypoint class label, and then uses these data to generate a weight map that is combined with hidden activations from the convolutional layers <e2>that</e2> operate on the image."
sameAs(e1, e2)
Comment:

5381	"it computes a distance transform based on the keypoint location, combines it with a one-hot vector that indicates the keypoint class label, and then uses these data to generate a weight map <e1>that</e1> is combined with hidden activations from the convolutional layers <e2>that</e2> operate on the image."
sameAs(e1, e2)
Comment:

5382	"at a high level, our model learns to extract two types of <e1>information</e1>-global image information and keypoint-conditional <e2>information</e2>-and uses them to obtain the final viewpoint prediction."
sameAs(e1, e2)
Comment:

5383	"to our knowledge, our keypoint annotation dataset is an order of magnitude larger than the next largest keypoint dataset for shapenet cad <e1>models</e1> [14] in terms of number of annotated <e2>models</e2>."
sameAs(e1, e2)
Comment:

5384	"as our thorough experiments show, we are able to use this human guidance to vastly improve viewpoint estimation performance: on human-guidance instances from the pascal 3d+ validation set [29] , a fine-tuned version of the state-ofthe-art model from su et al [22] achieves 85.7% mean class <e1>accuracy</e1>, while our ch-cnn achieves 90.7% mean class <e2>accuracy</e2>."
sameAs(e1, e2)
Comment:

5385	"additionally, our <e1>model</e1> is well-suited for handling challenges that the state-of-the-art <e2>model</e2> often fails to overcome, as shown by our qualitative results."
sameAs(e1, e2)
Comment:

5386	"first, we propose a novel cnn that integrates two types of <e1>information</e1>-an image and <e2>information</e2> about a single keypoint-to output viewpoint predictions; this model is designed to be incorporated into a hybrid-intelligence viewpoint estimation pipeline."
sameAs(e1, e2)
Comment:

5387	"first, we propose a novel cnn that integrates two types of information-an image and information about a single keypoint-to output <e1>viewpoint</e1> predictions; this model is designed to be incorporated into a hybrid-intelligence <e2>viewpoint</e2> estimation pipeline."
sameAs(e1, e2)
Comment:

5388	"although we, as a community, design automatic <e1>systems</e1> that seek to extract information from images automaticallyand have done this quite well, e.g., [9, 17] -there are indeed situations that are beyond the capabilities of current <e2>systems</e2>, such as inferring the extent of damage to two vehicles involved in a car accident from data acquired by a dash-cam."
sameAs(e1, e2)
Comment:

5389	"although we, as a community, design automatic systems <e1>that</e1> seek to extract information from images automaticallyand have done this quite well, e.g., [9, 17] -there are indeed situations <e2>that</e2> are beyond the capabilities of current systems, such as inferring the extent of damage to two vehicles involved in a car accident from data acquired by a dash-cam."
sameAs(e1, e2)
Comment:

5390	"in the black bars, gray indicates confidence, magenta <e1>marks</e1> the final prediction, and the green triangle <e2>marks</e2> the ground truth."
sameAs(e1, e2)
Comment:

5391	"this allows for large or small receptive fields without changing the number of <e1>parameters</e1>, facilitating automatic and efficient allocation of <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

5392	"2 <e1>we</e1> review most closely related works, <e2>we</e2> describe dau in sec."
sameAs(e1, e2)
Comment:

5393	"but in dense prediction problems like  <e1>segmentation</e1> [18, 3] , sufficient resolution is required for accurate localization of <e2>segmentation</e2> boundaries."
sameAs(e1, e2)
Comment:

5394	" introduction single image super-resolution (sisr) is a classic computer vision problem, which aims to recover a highresolution (hr)<e1> imag</e1>e from a low-resolution (lr)<e2> imag</e2>e."
sameAs(e1, e2)
Comment:

5395	"as the pioneer cnn <e1>model</e1> for sr, super-resolution convolutional neural network (srcnn) [2] predicts the nonlinear lr-hr mapping via a fully convolutional network, and significantly outperforms classical non-dl <e2>methods</e2>."
Compare(e1, e2)
Comment:

5396	"to address this issue, the deep joint super resolution (djsr) jointly utilizes both the wealth of external <e1>examples</e1> and the power of self <e2>examples</e2> unique to the input."
sameAs(e1, e2)
Comment:

5397	"one commonality among the above cnn models is that their networks contain fewer than 5 <e1>layers</e1>, e.g., srcnn [2] uses 3 convolutional <e2>layers</e2>."
sameAs(e1, e2)
Comment:

5398	"their deeper structures with 4 or 5 <e1>layers</e1> do not achieve better performance, which was attributed to the difficulty of training deeper networks and led to the observation that "the deeper the better" might not be the case in sr. inspired by the success of very deep networks [8, 27, 28] on imagenet [21] , kim et al [13, 14] propose two very deep convolutional networks for sr, both stacking 20 convolutional <e2>layers</e2>, from the viewpoints of training efficiency and storage, respectively."
sameAs(e1, e2)
Comment:

5399	"their deeper structures with 4 or 5 layers do not achieve better performance, which was attributed to the difficulty of <e1>training</e1> deeper networks and led to the observation that "the deeper the better" might not be the case in sr. inspired by the success of very deep networks [8, 27, 28] on imagenet [21] , kim et al [13, 14] propose two very deep convolutional networks for sr, both stacking 20 convolutional layers, from the viewpoints of <e2>training</e2> efficiency and storage, respectively."
sameAs(e1, e2)
Comment:

5400	"on the one hand, to accelerate the convergence speed of very deep networks, the vdsr [13] is trained with a very high learning rate (10 −1 , instead of 10 −4 in srcnn) and the authors further use residual learning and adjustable <e1>gradient</e1> clipping to solve <e2>gradient</e2> explosion problem."
sameAs(e1, e2)
Comment:

5401	"all of the three models learn the residual <e1>image</e1> between the input interpolated lr (ilr) <e2>image</e2> and the ground truth hr image in the residual branch."
sameAs(e1, e2)
Comment:

5402	"all of the three models learn the residual <e1>image</e1> between the input interpolated lr (ilr) image and the ground truth hr <e2>image</e2> in the residual branch."
sameAs(e1, e2)
Comment:

5403	"all of the three models learn the residual image between the input interpolated lr (ilr) <e1>image</e1> and the ground truth hr <e2>image</e2> in the residual branch."
sameAs(e1, e2)
Comment:

5404	"the residual <e1>image</e1> is then added to the ilr <e2>image</e2> from the identity branch to estimate the hr image."
sameAs(e1, e2)
Comment:

5405	"the residual <e1>image</e1> is then added to the ilr image from the identity branch to estimate the hr <e2>image</e2>."
sameAs(e1, e2)
Comment:

5406	"the residual image is then added to the ilr <e1>image</e1> from the identity branch to estimate the hr <e2>image</e2>."
sameAs(e1, e2)
Comment:

5407	"compared to the compact <e1>models</e1>, large <e2>models</e2> demand more storage space and are less applicable to mobile systems [6] ."
sameAs(e1, e2)
Comment:

5408	"to address this issue, we propose a novel deep recursive residual <e1>network</e1> (drrn) to effectively build a very deep <e2>network</e2> structure, which achieves better performance, but with 2×, 6×, and 14× fewer parameters than vdsr, drcn, and red30, respectively."
sameAs(e1, e2)
Comment:

5409	"grl and lrl mainly differ in that lrl is performed in every few stacked <e1>layers</e1>, while grl is performed between the input and output <e2>images</e2>, i.e., drrn has many lrls and only 1 grl."
Part-of(e1, e2)
Comment:

5410	"1 shows the peak signal-to-noise ratio (psnr) performance of several recent <e1>cnn models</e1> for sr [2, 13, 14, 17, 25, 32] versus the number of parameters, denoted as k. compared to the prior <e2>cnn models</e2>, drrn achieves the best performance with fewer parameters."
sameAs(e1, e2)
Comment:

5411	"1 shows the peak signal-to-noise ratio (psnr) performance of several recent cnn models for sr [2, 13, 14, 17, 25, 32] versus the number of <e1>parameters</e1>, denoted as k. compared to the prior cnn models, drrn achieves the best performance with fewer <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

5412	"△, ✩, and • are models with less than 5 <e1>layers</e1>, 20 <e2>layers</e2>, and more than 30 layers, respectively."
sameAs(e1, e2)
Comment:

5413	"△, ✩, and • are models with less than 5 <e1>layers</e1>, 20 layers, and more than 30 <e2>layers</e2>, respectively."
sameAs(e1, e2)
Comment:

5414	"△, ✩, and • are models with less than 5 layers, 20 <e1>layers</e1>, and more than 30 <e2>layers</e2>, respectively."
sameAs(e1, e2)
Comment:

5415	"in these networks, apart from the short-term <e1>memory</e1>, one state is also influenced by a specific prior state, namely restricted long-term <e2>memory</e2>."
sameAs(e1, e2)
Comment:

5416	"in these networks, apart from the short-term memory, one <e1>state</e1> is also influenced by a specific prior <e2>state</e2>, namely restricted long-term memory."
sameAs(e1, e2)
Comment:

5417	"to address this issue, we propose a very deep persistent <e1>memory</e1> network (memnet), which introduces a <e2>memory</e2> block to explicitly mine persistent memory through an adaptive learning process."
sameAs(e1, e2)
Comment:

5418	"to address this issue, we propose a very deep persistent <e1>memory</e1> network (memnet), which introduces a memory block to explicitly mine persistent <e2>memory</e2> through an adaptive learning process."
sameAs(e1, e2)
Comment:

5419	"to address this issue, we propose a very deep persistent memory network (memnet), which introduces a <e1>memory</e1> block to explicitly mine persistent <e2>memory</e2> through an adaptive learning process."
sameAs(e1, e2)
Comment:

5420	"the short-term <e1>memory</e1> generated from the recursive unit, and the long-term <e2>memory</e2> generated from the previous memory blocks 1 (green arrow in fig."
sameAs(e1, e2)
Comment:

5421	"the short-term <e1>memory</e1> generated from the recursive unit, and the long-term memory generated from the previous <e2>memory</e2> blocks 1 (green arrow in fig."
sameAs(e1, e2)
Comment:

5422	"the short-term memory generated from the recursive unit, and the long-term <e1>memory</e1> generated from the previous <e2>memory</e2> blocks 1 (green arrow in fig."
sameAs(e1, e2)
Comment:

5423	"in each <e1>memory</e1> block, the gate unit adaptively learns different weights for different memories, which controls how much of the longterm <e2>memory</e2> should be reserved, and decides how much of the short-term memory should be stored."
sameAs(e1, e2)
Comment:

5424	"in each <e1>memory</e1> block, the gate unit adaptively learns different weights for different memories, which controls how much of the longterm memory should be reserved, and decides how much of the short-term <e2>memory</e2> should be stored."
sameAs(e1, e2)
Comment:

5425	"in each memory block, the gate unit adaptively learns different weights for different memories, which controls how much of the longterm <e1>memory</e1> should be reserved, and decides how much of the short-term <e2>memory</e2> should be stored."
sameAs(e1, e2)
Comment:

5426	"the green arrow denotes the long-term <e1>memory</e1> from the previous <e2>memory</e2> blocks that is directly passed to the gate unit."
sameAs(e1, e2)
Comment:

5427	"1(a) ), adopt the singlepath feed-forward architecture, where one <e1>state</e1> is mainly influenced by its direct former <e2>state</e2>, namely short-term memory."
sameAs(e1, e2)
Comment:

5428	"abstract of late, weakly supervised <e1>object detection</e1> is with great importance in <e2>object recognition</e2>."
isA(e1, e2)
Comment:

5429	"however, collecting such accurate <e1>annotations</e1> can be very labor-intensive and time-consuming, whereas achieving only image-level <e2>annotations</e2> (i.e., image tags) is much easier, as these annotations are often available at the internet (e.g., image search queries [21] )."
sameAs(e1, e2)
Comment:

5430	"however, collecting such accurate <e1>annotations</e1> can be very labor-intensive and time-consuming, whereas achieving only image-level annotations (i.e., image tags) is much easier, as these <e2>annotations</e2> are often available at the internet (e.g., image search queries [21] )."
sameAs(e1, e2)
Comment:

5431	"however, collecting such accurate annotations can be very labor-intensive and time-consuming, whereas achieving only <e1>image</e1>-level annotations (i.e., <e2>image</e2> tags) is much easier, as these annotations are often available at the internet (e.g., image search queries [21] )."
sameAs(e1, e2)
Comment:

5432	"however, collecting such accurate annotations can be very labor-intensive and time-consuming, whereas achieving only <e1>image</e1>-level annotations (i.e., image tags) is much easier, as these annotations are often available at the internet (e.g., <e2>image</e2> search queries [21] )."
sameAs(e1, e2)
Comment:

5433	"however, collecting such accurate annotations can be very labor-intensive and time-consuming, whereas achieving only image-level <e1>annotations</e1> (i.e., image tags) is much easier, as these <e2>annotations</e2> are often available at the internet (e.g., image search queries [21] )."
sameAs(e1, e2)
Comment:

5434	"however, collecting such accurate annotations can be very labor-intensive and time-consuming, whereas achieving only image-level annotations (i.e., <e1>image</e1> tags) is much easier, as these annotations are often available at the internet (e.g., <e2>image</e2> search queries [21] )."
sameAs(e1, e2)
Comment:

5435	"after instance <e1>classifier</e1> refinement, in the right, the correct proposal d is detected and more discriminative performance of instance <e2>classifier</e2> is shown."
sameAs(e1, e2)
Comment:

5436	"meanwhile, recent efforts tend to combine mil and <e1>cnn</e1> by either using <e2>cnn</e2> as an off-the-shelf feature extractor [3, 7, 28, 30, 31] or training an end-to-end mil network [4, 16] ."
sameAs(e1, e2)
Comment:

5437	"bilen and vedaldi [4] presents an end-to-end deep network for wsod, in which final <e1>image classification</e1> score is the weighted sum of proposal scores, that is, each proposal contributes a percentage to the final <e2>image classification</e2>."
sameAs(e1, e2)
Comment:

5438	"bilen and vedaldi [4] presents an end-to-end deep network for wsod, in which final image classification score is the weighted sum of <e1>proposal</e1> scores, that is, each <e2>proposal</e2> contributes a percentage to the final image classification."
sameAs(e1, e2)
Comment:

5439	"to address <e1>this</e1> problem, we put forward two improvements in <e2>this</e2> paper: 1) instead of estimating instance weights through weighted sum pooling, we propose to add some blocks in the network for learning more discriminative instance classifiers by explicitly assigning binary instance labels; 2) we propose to refine instance classifier online using spatial relation."
sameAs(e1, e2)
Comment:

5440	"to address this problem, <e1>we</e1> put forward two improvements in this paper: 1) instead of estimating instance weights through weighted sum pooling, <e2>we</e2> propose to add some blocks in the network for learning more discriminative instance classifiers by explicitly assigning binary instance labels; 2) we propose to refine instance classifier online using spatial relation."
sameAs(e1, e2)
Comment:

5441	"to address this problem, <e1>we</e1> put forward two improvements in this paper: 1) instead of estimating instance weights through weighted sum pooling, we propose to add some blocks in the network for learning more discriminative instance classifiers by explicitly assigning binary instance labels; 2) <e2>we</e2> propose to refine instance classifier online using spatial relation."
sameAs(e1, e2)
Comment:

5442	"to address this problem, we put forward two improvements in this paper: 1) instead of estimating instance weights through weighted sum pooling, <e1>we</e1> propose to add some blocks in the network for learning more discriminative instance classifiers by explicitly assigning binary instance labels; 2) <e2>we</e2> propose to refine instance classifier online using spatial relation."
sameAs(e1, e2)
Comment:

5443	"to address this problem, we put forward two improvements in this paper: 1) instead of estimating instance weights through weighted sum pooling, we propose to add some blocks in the network for learning more discriminative instance <e1>classifiers</e1> by explicitly assigning binary instance labels; 2) we propose to refine instance <e2>classifier</e2> online using spatial relation."
sameAs(e1, e2)
Comment:

5444	"in [4] , bilen and vedaldi propose a spatial regulariser via forcing features of highest scoring <e1>region</e1> and its adjacent <e2>regions</e2> to be the same, which significantly improves wsod performance."
sameAs(e1, e2)
Comment:

5445	"1 (right) , we except the label <e1>information</e1> of a can propagate to b and c which has large overlap with a, and then the label <e2>information</e2> of b and c can propagate to d to correctly localize object."
sameAs(e1, e2)
Comment:

5446	"we name this new <e1>network</e1> structure multiple instance detection <e2>network</e2> (midn) with instance classifier."
sameAs(e1, e2)
Comment:

5447	"a natural way for <e1>classifier</e1> refinement is the alternative strategy, that is, alternatively relabelling instance and training instance <e2>classifier</e2>, while this procedure is very time-consuming, especially considering training deep networks with a huge number of stochastic gradient descent (sgd) iterations."
sameAs(e1, e2)
Comment:

5448	"a natural way for classifier refinement is the alternative strategy, that is, alternatively relabelling instance and <e1>training</e1> instance classifier, while this procedure is very time-consuming, especially considering <e2>training</e2> deep networks with a huge number of stochastic gradient descent (sgd) iterations."
sameAs(e1, e2)
Comment:

5449	"a natural way for classifier refinement is the alternative strategy, that is, alternatively relabelling instance and training instance classifier, while this procedure is very time-consuming, especially considering training deep networks with a huge number of <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) iterations."
sameAs(e1, e2)
Comment:

5450	"our method has multiple output streams for different stages: the first is the midn to train a basic instance <e1>classifier</e1> and others refine the <e2>classifier</e2>."
sameAs(e1, e2)
Comment:

5451	"according to these scores, for each stage, we can label the top-scoring <e1>proposal</e1> along with its spatially overlapped <e2>proposals</e2> to the image label."
sameAs(e1, e2)
Comment:

5452	"though the top-scoring <e1>proposal</e1> may only contain a part of an object, its adjacent <e2>proposals</e2> will cover larger portion of the object."
sameAs(e1, e2)
Comment:

5453	"we propose a novel online instance <e1>classifier</e1> refinement algorithm to integrate mil and the instance <e2>classifier</e2> refinement procedure into a single deep network, and train the network end-to-end with only image-level supervision, i.e., without object location information."
sameAs(e1, e2)
Comment:

5454	"we propose a novel online instance classifier refinement algorithm to integrate mil and the instance classifier refinement procedure into a single deep <e1>network</e1>, and train the <e2>network</e2> end-to-end with only image-level supervision, i.e., without object location information."
sameAs(e1, e2)
Comment:

5455	"but in the beginning of <e1>training</e1>, all classifiers are almost non-trained, which will result in very noisy supervision of refined classifiers, and then the <e2>training</e2> will deviate from correct solutions a lot."
sameAs(e1, e2)
Comment:

5456	"but in the beginning of training, all <e1>classifiers</e1> are almost non-trained, which will result in very noisy supervision of refined <e2>classifiers</e2>, and then the training will deviate from correct solutions a lot."
sameAs(e1, e2)
Comment:

5457	"our <e1>method</e1> achieves 47.0% map and 64.3% corloc on voc 2007 that outperforms previous best performed <e2>methods</e2> by a large margin."
Compare(e1, e2)
Comment:

5458	"• we further design a novel oicr algorithm that integrates the basic detection <e1>network</e1> and the multi-stage instance-level classifier into a single <e2>network</e2>."
sameAs(e1, e2)
Comment:

5459	"• our method achieves significantly better <e1>results</e1> over previous state-of-the-art <e2>methods</e2> on the challenging pascal voc 2007 and 2012 benchmarks for weakly supervised object detection."
Compare(e1, e2)
Comment:

5460	"firstly, the number of persons is not fixed or biased by the definition of the <e1>problem</e1>, but is estimated in an unbiased fashion from the video sequence and is determined by the solution of the <e2>problem</e2>."
sameAs(e1, e2)
Comment:

5461	"firstly, the number of persons is not fixed or biased by the definition of the problem, but is estimated in an unbiased fashion from the video sequence and is determined by the <e1>solution</e1> of the <e2>problem</e2>."
Used-for(e1, e2)
Comment:

5462	"in order to avoid <e1>that</e1> distinct but similar looking people are assigned to the same track, a distinction must be made between the edges <e2>that</e2> define possible connections (i.e., a feasible set) and the edges that define the costs or rewards for assigning the incident nodes to distinct tracks (i.e., an objective function)."
sameAs(e1, e2)
Comment:

5463	"in order to avoid <e1>that</e1> distinct but similar looking people are assigned to the same track, a distinction must be made between the edges that define possible connections (i.e., a feasible set) and the edges <e2>that</e2> define the costs or rewards for assigning the incident nodes to distinct tracks (i.e., an objective function)."
sameAs(e1, e2)
Comment:

5464	"in order to avoid that distinct but similar looking people are assigned to the same track, a distinction must be made between the <e1>edges</e1> that define possible connections (i.e., a feasible set) and the <e2>edges</e2> that define the costs or rewards for assigning the incident nodes to distinct tracks (i.e., an objective function)."
sameAs(e1, e2)
Comment:

5465	"in order to avoid that distinct but similar looking people are assigned to the same track, a distinction must be made between the edges <e1>that</e1> define possible connections (i.e., a feasible set) and the edges <e2>that</e2> define the costs or rewards for assigning the incident nodes to distinct tracks (i.e., an objective function)."
sameAs(e1, e2)
Comment:

5466	"we achieve this, while maintaining the advantages of [28] , by casting the multi-person tracking <e1>problem</e1> as a minimum cost lifted multicut <e2>problem</e2> [1] ."
sameAs(e1, e2)
Comment:

5467	"specifically, <e1>we</e1> make three contributions: firstly, <e2>we</e2> design and train deep networks for reidentifying persons by fusing human pose information."
sameAs(e1, e2)
Comment:

5468	"we introduce two types of <e1>edges</e1> (regular and lifted <e2>edges</e2>) for the tracking graph."
sameAs(e1, e2)
Comment:

5469	"recent works on multi-person <e1>tracking</e1> focus on the <e2>tracking</e2>-by-detection approach [21, 36, 35, 29, 30] ."
sameAs(e1, e2)
Comment:

5470	"introducing tracklets can reduce the state space; however, such approaches need a separate tracklet <e1>generation</e1> step, and any mistakes introduced by the tracklet <e2>generation</e2> are likely to be propagated to the final solution."
sameAs(e1, e2)
Comment:

5471	"this trend has two advantages: firstly, representations of people appearance can be learned for varying camera positions and <e1>motion</e1>, a goal less easy to achieve with simple motion models, especially for monocular video due to the complexity of <e2>motion</e2> under perspective projection."
sameAs(e1, e2)
Comment:

5472	"[31] extends the work [3] to track interacting objects simultaneously by using intertwined <e1>flow</e1> and imposing linear <e2>flow</e2> constraints."
sameAs(e1, e2)
Comment:

5473	"our work is different from the previous multicut based works; our lifted multicut <e1>formulation</e1> introduces additional edges in the graph to incorporate long-range information into the tracking <e2>formulation</e2>."
sameAs(e1, e2)
Comment:

5474	"more specifically, every detection is represented by a <e1>node</e1> in a <e2>graph</e2>; edges connect detections within and across time frames, and costs assigned to the edges can be positive, to encourage the incident nodes to be in the same track, or negative, to encourage the incident nodes to be in distinct tracks."
Part-of(e1, e2)
Comment:

5475	"more specifically, every detection is represented by a node in a graph; <e1>edges</e1> connect detections within and across time frames, and costs assigned to the <e2>edges</e2> can be positive, to encourage the incident nodes to be in the same track, or negative, to encourage the incident nodes to be in distinct tracks."
sameAs(e1, e2)
Comment:

5476	"more specifically, every detection is represented by a node in a graph; edges connect detections within and across time frames, and costs assigned to the edges can be positive, to encourage the incident <e1>nodes</e1> to be in the same track, or negative, to encourage the incident <e2>nodes</e2> to be in distinct tracks."
sameAs(e1, e2)
Comment:

5477	"we term these <e1>two</e1> phases stage 1 and stage 2 and ask <e2>two</e2> scientifically-motivated statistical inference questions: • stage 1: how do we leverage the available side information in a hts study to increase how many significant outcomes we can detect?"
sameAs(e1, e2)
Comment:

5478	"we term these two phases stage 1 and stage 2 and ask two scientifically-motivated statistical inference questions: • stage 1: how do <e1>we</e1> leverage the available side information in a hts study to increase how many significant outcomes <e2>we</e2> can detect?"
sameAs(e1, e2)
Comment:

5479	"bb-fdr then builds a predictive <e1>model</e1> of each covariate to perform variable selection on the stage 1 <e2>model</e2> while conserving a specified stage 2 fdr threshold."
sameAs(e1, e2)
Comment:

5480	"abstract in single <e1>image deblurring</e1>, the "coarse-to-fine" scheme, introduction <e2>image deblurring</e2> has long been an important task in computer vision and image processing."
sameAs(e1, e2)
Comment:

5481	"this method commences from a very coarse scale of the blurry <e1>image</e1>, and progressively recovers the latent <e2>image</e2> at higher resolutions until the full resolution is reached."
sameAs(e1, e2)
Comment:

5482	"given a <e1>motion</e1>-or focal-blurred image, caused by camera shake, object <e2>motion</e2> or out-of-focus, the goal of deblurring is to recover a sharp latent image with necessary edge structures and details."
sameAs(e1, e2)
Comment:

5483	"given a motion-or focal-blurred <e1>image</e1>, caused by camera shake, object motion or out-of-focus, the goal of deblurring is to recover a sharp latent <e2>image</e2> with necessary edge structures and details."
sameAs(e1, e2)
Comment:

5484	"even with the same training <e1>data</e1>, the recurrent exploitation of shared weights works in a way similar to using <e2>data</e2> multiple times to learn parameters, which actually amounts to data augmentation regarding scales."
sameAs(e1, e2)
Comment:

5485	"besides training <e1>efficiency</e1>, our method can also produce higher <e2>quality</e2> results than existing methods both quantitatively and qualitatively, as shown in fig."
Conjunction(e1, e2)
Comment:

5486	"besides training efficiency, our method can also produce higher quality <e1>results</e1> than existing <e2>methods</e2> both quantitatively and qualitatively, as shown in fig."
Compare(e1, e2)
Comment:

5487	"further, the simplified assumptions on the <e1>blur</e1> model often hinder their performance on real-word examples, where <e2>blur</e2> is far more complex than modeled and is entangled with in-camera image processing pipeline."
sameAs(e1, e2)
Comment:

5488	"classic features <e1>that</e1> are used for the analysis of such data are defined in terms <e2>that</e2> acknowledge the latent surface structure, and do not treat the data as a volume [20, 12, 45] ."
sameAs(e1, e2)
Comment:

5489	"classic features that are used for the analysis of such <e1>data</e1> are defined in terms that acknowledge the latent surface structure, and do not treat the <e2>data</e2> as a volume [20, 12, 45] ."
sameAs(e1, e2)
Comment:

5490	"for single object and multiple object pose estimation on the linemod and occlusion <e1>datasets</e1>, our <e2>approach</e2> substantially outperforms other recent cnn-based approaches [10, 25] when they are all used without postprocessing."
Evaluate-for(e1, e2)
Comment:

5491	"for single object and multiple object pose estimation on the linemod and occlusion <e1>datasets</e1>, our approach substantially outperforms other recent cnn-based <e2>approaches</e2> [10, 25] when they are all used without postprocessing."
Used-for(e1, e2)
Comment:

5492	"for single object and multiple object pose estimation on the linemod and occlusion datasets, our <e1>approach</e1> substantially outperforms other recent cnn-based <e2>approaches</e2> [10, 25] when they are all used without postprocessing."
Compare(e1, e2)
Comment:

5493	"during post-processing, a pose refinement step can be used to boost the <e1>accuracy</e1> of these two methods, but at 10 fps or less, they are much slower than <e2>our method</e2>."
Evaluate-for(e1, e2)
Comment:

5494	"similar to previous work on spatial <e1>textures</e1> [13, 19, 33] , we summarize an input dynamic <e2>texture</e2> in terms of a set of spatiotemporal statistics of filter outputs from each stream."
sameAs(e1, e2)
Comment:

5495	"the appearance stream <e1>models</e1> the per frame appearance of the input texture, while the dynamics stream <e2>models</e2> its temporal dynamics."
sameAs(e1, e2)
Comment:

5496	"the appearance stream models the per frame appearance of the input texture, while the <e1>dynamics</e1> stream models its temporal <e2>dynamics</e2>."
sameAs(e1, e2)
Comment:

5497	"in particular, psychophysical studies [6] show <e1>that</e1> humans are able to perceive the structure of a dynamic texture even in the absence of appearance cues, suggesting <e2>that</e2> the two streams are effectively inde-pendent."
sameAs(e1, e2)
Comment:

5498	"similarly, the <e1>two</e1>-stream hypothesis [16] models the human visual cortex in terms of <e2>two</e2> pathways, the ventral stream (involved with object recognition) and the dorsal stream (involved with motion processing)."
sameAs(e1, e2)
Comment:

5499	"we consider a range of dynamic textures and show <e1>that</e1> our approach generates novel, high quality samples <e2>that</e2> match both the frame-wise appearance and temporal evolution of an input example."
sameAs(e1, e2)
Comment:

5500	"further, the factorization of appearance and <e1>dynamics</e1> enables a novel form of style transfer, where <e2>dynamics</e2> of one texture are combined with the appearance of a different one, cf ."
sameAs(e1, e2)
Comment:

5501	"further, the factorization of appearance and dynamics enables a novel form of style transfer, where dynamics of <e1>one</e1> texture are combined with the appearance of a different <e2>one</e2>, cf ."
sameAs(e1, e2)
Comment:

5502	"these patterns have been previously studied under a variety of names, including turbulent-flow <e1>motion</e1> [17] , temporal textures [30] , time-varying textures [3] , dynamic textures [8] , textured <e2>motion</e2> [45] and spacetime textures [7] ."
sameAs(e1, e2)
Comment:

5503	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal <e1>textures</e1> [30] , time-varying <e2>textures</e2> [3] , dynamic textures [8] , textured motion [45] and spacetime textures [7] ."
sameAs(e1, e2)
Comment:

5504	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal <e1>textures</e1> [30] , time-varying textures [3] , dynamic <e2>textures</e2> [8] , textured motion [45] and spacetime textures [7] ."
sameAs(e1, e2)
Comment:

5505	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal <e1>textures</e1> [30] , time-varying textures [3] , dynamic textures [8] , textured motion [45] and spacetime <e2>textures</e2> [7] ."
sameAs(e1, e2)
Comment:

5506	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal textures [30] , time-varying <e1>textures</e1> [3] , dynamic <e2>textures</e2> [8] , textured motion [45] and spacetime textures [7] ."
sameAs(e1, e2)
Comment:

5507	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal textures [30] , time-varying <e1>textures</e1> [3] , dynamic textures [8] , textured motion [45] and spacetime <e2>textures</e2> [7] ."
sameAs(e1, e2)
Comment:

5508	"these patterns have been previously studied under a variety of names, including turbulent-flow motion [17] , temporal textures [30] , time-varying textures [3] , dynamic <e1>textures</e1> [8] , textured motion [45] and spacetime <e2>textures</e2> [7] ."
sameAs(e1, e2)
Comment:

5509	"this factorization is then used to enable dynamic texture synthesis which, based on given an input dynamic <e1>texture</e1> as the target, our two-stream model is able to synthesize a novel dynamic <e2>texture</e2> that preserves the target's appearance and dynamics characteristics."
sameAs(e1, e2)
Comment:

5510	"this factorization is then used to enable dynamic texture synthesis which, based on given an input dynamic texture as the <e1>target</e1>, our two-stream model is able to synthesize a novel dynamic texture that preserves the <e2>target</e2>'s appearance and dynamics characteristics."
sameAs(e1, e2)
Comment:

5511	"(right) our <e1>two</e1>-stream approach enables synthesis that combines the texture appearance from one target with the dynamics from another, resulting in a composition of the <e2>two</e2>."
sameAs(e1, e2)
Comment:

5512	"example <e1>texture</e1> inputs, generates a novel dynamic <e2>texture</e2> instance."
sameAs(e1, e2)
Comment:

5513	"an essential <e1>task</e1> is to establish reliable 3d shape correspondences between scans from raw sensor <e2>data</e2> or between scans and a template 3d shape."
Conjunction(e1, e2)
Comment:

5514	"an essential task is to establish reliable 3d <e1>shape</e1> correspondences between scans from raw sensor data or between scans and a template 3d <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5515	"this process is challenging due to low <e1>sensor</e1> resolution and high <e2>sensor</e2> noise, especially for articulated shapes, such as humans or animals, that exhibit significant non-rigid deformations and shape variations."
sameAs(e1, e2)
Comment:

5516	"effective human-specific templates and registration techniques have been developed over the last decade [46] , but these methods require significant effort and domain-specific knowledge to design the parametric deformable template, create an objective function <e1>that</e1> ensures alignment of salient regions and is not prone to being stuck in local minima, and develop an optimization strategy <e2>that</e2> effectively combines a global search for a good heuristic initialization and a local refinement procedure."
sameAs(e1, e2)
Comment:

5517	"in this work, we propose <e1>shape</e1> deformation networks, a comprehensive, all-in-one solution to template-driven <e2>shape</e2> matching."
sameAs(e1, e2)
Comment:

5518	"a <e1>shape</e1> deformation network learns to deform a template <e2>shape</e2> to align with an input observed shape."
sameAs(e1, e2)
Comment:

5519	"a <e1>shape</e1> deformation network learns to deform a template shape to align with an input observed <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5520	"a shape deformation network learns to deform a template <e1>shape</e1> to align with an input observed <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5521	"given two input shapes, we align the template to both <e1>inputs</e1> and obtain the final map between the <e2>inputs</e2> by reading off the correspondences from the template."
sameAs(e1, e2)
Comment:

5522	"we train our <e1>shape</e1> deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target <e2>shape</e2> as input and generates a global feature representation, and a decoder shape deformation network that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5523	"we train our <e1>shape</e1> deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as input and generates a global feature representation, and a decoder <e2>shape</e2> deformation network that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5524	"we train our <e1>shape</e1> deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as input and generates a global feature representation, and a decoder shape deformation network that takes as input the global feature and deform the template into the target <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5525	"we train our shape deformation <e1>network</e1> as part of an encoder-decoder architecture, which jointly learns an encoder <e2>network</e2> that takes a target shape as input and generates a global feature representation, and a decoder shape deformation network that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5526	"we train our shape deformation <e1>network</e1> as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as input and generates a global feature representation, and a decoder shape deformation <e2>network</e2> that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5527	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder <e1>network</e1> that takes a target shape as input and generates a global feature representation, and a decoder shape deformation <e2>network</e2> that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5528	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network <e1>that</e1> takes a target shape as input and generates a global feature representation, and a decoder shape deformation network <e2>that</e2> takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5529	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a <e1>target</e1> shape as input and generates a global feature representation, and a decoder shape deformation network that takes as input the global feature and deform the template into the <e2>target</e2> shape."
sameAs(e1, e2)
Comment:

5530	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target <e1>shape</e1> as input and generates a global feature representation, and a decoder <e2>shape</e2> deformation network that takes as input the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5531	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target <e1>shape</e1> as input and generates a global feature representation, and a decoder shape deformation network that takes as input the global feature and deform the template into the target <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5532	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as <e1>input</e1> and generates a global feature representation, and a decoder shape deformation network that takes as <e2>input</e2> the global feature and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5533	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as input and generates a global <e1>feature</e1> representation, and a decoder shape deformation network that takes as input the global <e2>feature</e2> and deform the template into the target shape."
sameAs(e1, e2)
Comment:

5534	"we train our shape deformation network as part of an encoder-decoder architecture, which jointly learns an encoder network that takes a target shape as input and generates a global feature representation, and a decoder <e1>shape</e1> deformation network that takes as input the global feature and deform the template into the target <e2>shape</e2>."
sameAs(e1, e2)
Comment:

5535	"at test time, we improve our template-<e1>input</e1> shape alignment by optimizing locally the chamfer distance between target and generated shape over the global feature representation which is passed in as <e2>input</e2> to the shape deformation network."
sameAs(e1, e2)
Comment:

5536	"at test time, we improve our template-input shape alignment by optimizing locally the chamfer distance between target and generated <e1>shape</e1> over the global feature representation which is passed in as input to the <e2>shape</e2> deformation network."
sameAs(e1, e2)
Comment:

5537	"we present a new deep learning approach for matching deformable <e1>shapes</e1> by introducing shape deformation networks which jointly encode 3d <e2>shapes</e2> and correspondences."
sameAs(e1, e2)
Comment:

5538	"critical to the success of our <e1>shape</e1> deformation network is the ability to learn to deform a template <e2>shape</e2> to targets with varied appearances and articulation."
sameAs(e1, e2)
Comment:

5539	"furthermore, while our network can take advantage of known <e1>correspondences</e1> between the template and the example shapes, which are typically available when they have been generated using some parametric model [6, 42] , we show it can also be trained without <e2>correspondence</e2> supervision."
sameAs(e1, e2)
Comment:

5540	"we demonstrate <e1>that</e1> with sufficient training data this simple approach achieves state-of-the-art results and outperforms techniques <e2>that</e2> require complex multiterm objective functions instead of the simple reconstruction loss used by our method."
sameAs(e1, e2)
Comment:

5541	"we demonstrate that with sufficient training data this simple <e1>approach</e1> achieves state-of-the-art results and outperforms <e2>techniques</e2> that require complex multiterm objective functions instead of the simple reconstruction loss used by our method."
Compare(e1, e2)
Comment:

5542	"this is achieved by factoring the <e1>surface</e1> representation into (i) a template, that parameterizes the <e2>surface</e2>, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input surface."
sameAs(e1, e2)
Comment:

5543	"this is achieved by factoring the <e1>surface</e1> representation into (i) a template, that parameterizes the surface, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input <e2>surface</e2>."
sameAs(e1, e2)
Comment:

5544	"this is achieved by factoring the surface representation into (i) a template, <e1>that</e1> parameterizes the surface, and (ii) a learnt global feature vector <e2>that</e2> parameterizes the transformation of the template into the input surface."
sameAs(e1, e2)
Comment:

5545	"this is achieved by factoring the surface representation into (i) a template, that parameterizes the <e1>surface</e1>, and (ii) a learnt global feature vector that parameterizes the transformation of the template into the input <e2>surface</e2>."
sameAs(e1, e2)
Comment:

5546	"by predicting <e1>this</e1> feature for a new shape, we implicitly predict correspondences between <e2>this</e2> shape and the template."
sameAs(e1, e2)
Comment:

5547	"by predicting this feature for a new <e1>shape</e1>, we implicitly predict correspondences between this <e2>shape</e2> and the template."
sameAs(e1, e2)
Comment:

5548	"this strategy has been followed by historical deep learning approaches [16] , but also in some promising recent results with modern illustration of hybridnet behavior: the input <e1>image</e1> is processed by two network paths of weights wc and wu; each path produces a partial reconstruction, and both are summed to produce the final reconstruction, while only one path is used to produce a <e2>classification</e2> prediction."
Used-for(e1, e2)
Comment:

5549	"since <e1>those</e1> <e2>features</e2> are expected to extract invariant class-specific patterns, information is lost and exact reconstruction is not possible."
Compare(e1, e2)
Comment:

5550	"high quality <e1>proposals</e1> should come up with two key properties: (1) <e2>proposals</e2> can cover truth action regions with both high recall and high temporal overlap, (2) proposals are retrieved so that high recall and high overlap can be achieved using fewer proposals to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5551	"high quality <e1>proposals</e1> should come up with two key properties: (1) proposals can cover truth action regions with both high recall and high temporal overlap, (2) <e2>proposals</e2> are retrieved so that high recall and high overlap can be achieved using fewer proposals to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5552	"high quality <e1>proposals</e1> should come up with two key properties: (1) proposals can cover truth action regions with both high recall and high temporal overlap, (2) proposals are retrieved so that high recall and high overlap can be achieved using fewer <e2>proposals</e2> to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5553	"high quality proposals should come up with two key properties: (1) <e1>proposals</e1> can cover truth action regions with both high recall and high temporal overlap, (2) <e2>proposals</e2> are retrieved so that high recall and high overlap can be achieved using fewer proposals to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5554	"high quality proposals should come up with two key properties: (1) <e1>proposals</e1> can cover truth action regions with both high recall and high temporal overlap, (2) proposals are retrieved so that high recall and high overlap can be achieved using fewer <e2>proposals</e2> to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5555	"high quality proposals should come up with two key properties: (1) proposals can cover truth action regions with both high <e1>recall</e1> and high temporal overlap, (2) proposals are retrieved so that high <e2>recall</e2> and high overlap can be achieved using fewer proposals to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5556	"high quality proposals should come up with two key properties: (1) proposals can cover truth action regions with both high recall and high temporal overlap, (2) <e1>proposals</e1> are retrieved so that high recall and high overlap can be achieved using fewer <e2>proposals</e2> to reduce the computation cost of succeeding steps."
sameAs(e1, e2)
Comment:

5557	"to achieve high <e1>proposal</e1> quality, a <e2>proposal</e2> generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5558	"to achieve high <e1>proposal</e1> quality, a proposal generation method should generate <e2>proposals</e2> with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5559	"to achieve high <e1>proposal</e1> quality, a proposal generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve <e2>proposals</e2> with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5560	"to achieve high <e1>proposal</e1> quality, a proposal generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a <e2>proposal</e2> containing an action instance."
sameAs(e1, e2)
Comment:

5561	"to achieve high proposal quality, a <e1>proposal</e1> generation method should generate <e2>proposals</e2> with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5562	"to achieve high proposal quality, a <e1>proposal</e1> generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve <e2>proposals</e2> with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5563	"to achieve high proposal quality, a <e1>proposal</e1> generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a <e2>proposal</e2> containing an action instance."
sameAs(e1, e2)
Comment:

5564	"to achieve high proposal quality, a proposal generation method should generate <e1>proposals</e1> with flexible temporal durations and precise temporal boundaries, then retrieve <e2>proposals</e2> with reliable confidence scores, which indicate the probability of a proposal containing an action instance."
sameAs(e1, e2)
Comment:

5565	"to achieve high proposal quality, a proposal generation method should generate <e1>proposals</e1> with flexible temporal durations and precise temporal boundaries, then retrieve proposals with reliable confidence scores, which indicate the probability of a <e2>proposal</e2> containing an action instance."
sameAs(e1, e2)
Comment:

5566	"to achieve high proposal quality, a proposal generation method should generate proposals with flexible temporal durations and precise temporal boundaries, then retrieve <e1>proposals</e1> with reliable confidence scores, which indicate the probability of a <e2>proposal</e2> containing an action instance."
sameAs(e1, e2)
Comment:

5567	"most recently proposal generation methods [4, 5, 9, 32] generate <e1>proposals</e1> via sliding temporal windows of multiple durations in video with regular interval, then train a model to evaluate the confidence scores of generated <e2>proposals</e2> for proposals retrieving, while there is also method [13] making external boundaries regression."
sameAs(e1, e2)
Comment:

5568	"most recently proposal generation methods [4, 5, 9, 32] generate <e1>proposals</e1> via sliding temporal windows of multiple durations in video with regular interval, then train a model to evaluate the confidence scores of generated proposals for <e2>proposals</e2> retrieving, while there is also method [13] making external boundaries regression."
sameAs(e1, e2)
Comment:

5569	"most recently proposal generation methods [4, 5, 9, 32] generate proposals via sliding temporal windows of multiple durations in video with regular interval, then train a model to evaluate the confidence scores of generated <e1>proposals</e1> for <e2>proposals</e2> retrieving, while there is also method [13] making external boundaries regression."
sameAs(e1, e2)
Comment:

5570	"to address these issues and generate high quality <e1>proposals</e1>, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as <e2>proposals</e2> and globally retrieve candidate proposals using proposal-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5571	"to address these issues and generate high quality <e1>proposals</e1>, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate <e2>proposals</e2> using proposal-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5572	"to address these issues and generate high quality <e1>proposals</e1>, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate proposals using <e2>proposal</e2>-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5573	"to address these issues and generate high quality <e1>proposals</e1>, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate proposals using proposal-level feature as shown in fig 1. in detail, bsn generates <e2>proposals</e2> in three steps."
sameAs(e1, e2)
Comment:

5574	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as <e1>proposals</e1> and globally retrieve candidate <e2>proposals</e2> using proposal-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5575	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as <e1>proposals</e1> and globally retrieve candidate proposals using <e2>proposal</e2>-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5576	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as <e1>proposals</e1> and globally retrieve candidate proposals using proposal-level feature as shown in fig 1. in detail, bsn generates <e2>proposals</e2> in three steps."
sameAs(e1, e2)
Comment:

5577	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate <e1>proposals</e1> using <e2>proposal</e2>-level feature as shown in fig 1. in detail, bsn generates proposals in three steps."
sameAs(e1, e2)
Comment:

5578	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate <e1>proposals</e1> using proposal-level feature as shown in fig 1. in detail, bsn generates <e2>proposals</e2> in three steps."
sameAs(e1, e2)
Comment:

5579	"to address these issues and generate high quality proposals, we propose the boundarysensitive network (bsn), which adopts "local to global" fashion to locally combine high probability boundaries as proposals and globally retrieve candidate proposals using <e1>proposal</e1>-level feature as shown in fig 1. in detail, bsn generates <e2>proposals</e2> in three steps."
sameAs(e1, e2)
Comment:

5580	"finally, using features composed by actionness scores within and around <e1>proposal</e1>, bsn retrieves <e2>proposals</e2> by evaluating the confidence of whether a proposal contains an action."
sameAs(e1, e2)
Comment:

5581	"finally, using features composed by actionness scores within and around <e1>proposal</e1>, bsn retrieves proposals by evaluating the confidence of whether a <e2>proposal</e2> contains an action."
sameAs(e1, e2)
Comment:

5582	"finally, using features composed by actionness scores within and around proposal, bsn retrieves <e1>proposals</e1> by evaluating the confidence of whether a <e2>proposal</e2> contains an action."
sameAs(e1, e2)
Comment:

5583	"in summary, the main contributions of our work are three-folds: (1) we introduce a new architecture (bsn) based on "local to global" fashion to generate high quality temporal action proposals, which locally locates high boundary probability locations to achieve precise <e1>proposal</e1> boundaries and globally evaluates <e2>proposal</e2>-level feature to achieve reliable proposal confidence scores for retrieving."
sameAs(e1, e2)
Comment:

5584	"in summary, the main contributions of our work are three-folds: (1) we introduce a new architecture (bsn) based on "local to global" fashion to generate high quality temporal action proposals, which locally locates high boundary probability locations to achieve precise <e1>proposal</e1> boundaries and globally evaluates proposal-level feature to achieve reliable <e2>proposal</e2> confidence scores for retrieving."
sameAs(e1, e2)
Comment:

5585	"in summary, the main contributions of our work are three-folds: (1) we introduce a new architecture (bsn) based on "local to global" fashion to generate high quality temporal action proposals, which locally locates high boundary probability locations to achieve precise proposal boundaries and globally evaluates <e1>proposal</e1>-level feature to achieve reliable <e2>proposal</e2> confidence scores for retrieving."
sameAs(e1, e2)
Comment:

5586	"(2) extensive experiments demonstrate that our method achieves significantly better <e1>proposal</e1> quality than other state-of-the-art proposal generation methods, and can generate <e2>proposals</e2> in unseen action classes with comparative quality."
sameAs(e1, e2)
Comment:

5587	"(2) extensive experiments demonstrate that our method achieves significantly better proposal <e1>quality</e1> than other state-of-the-art proposal generation methods, and can generate proposals in unseen action classes with comparative <e2>quality</e2>."
sameAs(e1, e2)
Comment:

5588	"(2) extensive experiments demonstrate that our method achieves significantly better proposal quality than other <e1>state</e1>-of-the-art proposal generation methods, and can generate proposals in unseen <e2>action</e2> classes with comparative quality."
Conjunction(e1, e2)
Comment:

5589	"(3) integrating our method with existing <e1>action</e1> classifier into detection framework leads to significantly improved performance on temporal <e2>action</e2> detection task."
sameAs(e1, e2)
Comment:

5590	"for <e1>proposal</e1> generation stage, besides sliding windows [11] , earlier works also attempt to generate <e2>proposals</e2> by exploiting low-level cues such as hog and canny edge [37, 50] ."
sameAs(e1, e2)
Comment:

5591	"this problem requires algorithms for another challenging task: temporal <e1>action</e1> detection, which aims to detect <e2>action</e2> instances in untrimmed video including both temporal boundaries and action classes."
sameAs(e1, e2)
Comment:

5592	"this problem requires algorithms for another challenging task: temporal <e1>action</e1> detection, which aims to detect action instances in untrimmed video including both temporal boundaries and <e2>action</e2> classes."
sameAs(e1, e2)
Comment:

5593	"this problem requires algorithms for another challenging task: temporal action detection, which aims to detect <e1>action</e1> instances in untrimmed video including both temporal boundaries and <e2>action</e2> classes."
sameAs(e1, e2)
Comment:

5594	"temporal <e1>action</e1> detection task aims to detect <e2>action</e2> instances in untrimmed videos including temporal boundaries and action classes, and can be divided into proposal and classification stages."
sameAs(e1, e2)
Comment:

5595	"temporal <e1>action</e1> detection task aims to detect action instances in untrimmed videos including temporal boundaries and <e2>action</e2> classes, and can be divided into proposal and classification stages."
sameAs(e1, e2)
Comment:

5596	"temporal action detection task aims to detect <e1>action</e1> instances in untrimmed videos including temporal boundaries and <e2>action</e2> classes, and can be divided into proposal and classification stages."
sameAs(e1, e2)
Comment:

5597	"most detection methods [32, 34, 49] take <e1>these</e1> two stages separately, while there is also method [26, 3] taking <e2>these</e2> two stages jointly."
sameAs(e1, e2)
Comment:

5598	"most detection methods [32, 34, 49] take these <e1>two</e1> stages separately, while there is also method [26, 3] taking these <e2>two</e2> stages jointly."
sameAs(e1, e2)
Comment:

5599	"for <e1>proposal</e1> generation, earlier works [23, 29, 40] directly use sliding windows as <e2>proposals</e2>."
sameAs(e1, e2)
Comment:

5600	"recently some <e1>methods</e1> [4, 5, 9, 13, 32] generate proposals with pre-defined temporal durations and intervals, and use multiple <e2>methods</e2> to evaluate the confidence score of proposals, such as dictionary learning [5] and recurrent neural network [9] ."
sameAs(e1, e2)
Comment:

5601	"recently some methods [4, 5, 9, 13, 32] generate <e1>proposals</e1> with pre-defined temporal durations and intervals, and use multiple methods to evaluate the confidence score of <e2>proposals</e2>, such as dictionary learning [5] and recurrent neural network [9] ."
sameAs(e1, e2)
Comment:

5602	"tag method [49] adopts watershed algorithm to generate <e1>proposals</e1> with flexible boundaries and durations in local fashion, but without global <e2>proposal</e2>-level confidence evaluation for retrieving."
sameAs(e1, e2)
Comment:

5603	"recently temporal <e1>action</e1> detection method [48] detects <e2>action</e2> instances based on class-wise start, middle and end probabilities of each location."
sameAs(e1, e2)
Comment:

5604	"(1) <e1>we</e1> evaluate boundaries and actionness probabilities of each temporal location and generate proposals based on boundary probabilities, and (2) <e2>we</e2> evaluate the confidence scores of proposals with proposal-level feature to get retrieved proposals."
sameAs(e1, e2)
Comment:

5605	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate <e1>proposals</e1> based on boundary probabilities, and (2) we evaluate the confidence scores of <e2>proposals</e2> with proposal-level feature to get retrieved proposals."
sameAs(e1, e2)
Comment:

5606	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate <e1>proposals</e1> based on boundary probabilities, and (2) we evaluate the confidence scores of proposals with <e2>proposal</e2>-level feature to get retrieved proposals."
sameAs(e1, e2)
Comment:

5607	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate <e1>proposals</e1> based on boundary probabilities, and (2) we evaluate the confidence scores of proposals with proposal-level feature to get retrieved <e2>proposals</e2>."
sameAs(e1, e2)
Comment:

5608	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate proposals based on boundary probabilities, and (2) we evaluate the confidence scores of <e1>proposals</e1> with <e2>proposal</e2>-level feature to get retrieved proposals."
sameAs(e1, e2)
Comment:

5609	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate proposals based on boundary probabilities, and (2) we evaluate the confidence scores of <e1>proposals</e1> with proposal-level feature to get retrieved <e2>proposals</e2>."
sameAs(e1, e2)
Comment:

5610	"(1) we evaluate boundaries and actionness probabilities of each temporal location and generate proposals based on boundary probabilities, and (2) we evaluate the confidence scores of proposals with <e1>proposal</e1>-level feature to get retrieved <e2>proposals</e2>."
sameAs(e1, e2)
Comment:

5611	"the other scheme is template <e1>matching</e1>, which adopts either the target patch in the first frame [4, 29] or the previous frame [14] to construct the <e2>matching</e2> model."
sameAs(e1, e2)
Comment:

5612	"the main difference between these two strategies is that tracking-by-detection maintains the <e1>target</e1>'s appearance information in the weights of the deep neural network, thus requiring online fine-tuning with stochastic gradient descent (sgd) to make the model adaptable, while in contrast, template matching stores the <e2>target</e2>'s appearance in the object template, which is generated by a feed forward computation."
sameAs(e1, e2)
Comment:

5613	"the main difference between these two strategies is that tracking-by-detection maintains the target's appearance information in the weights of the deep neural network, thus requiring online fine-tuning with <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) to make the model adaptable, while in contrast, template matching stores the target's appearance in the object template, which is generated by a feed forward computation."
sameAs(e1, e2)
Comment:

5614	"due to the computationally expensive <e1>model</e1> updating required in tracking-by-detection, the speed of such <e2>methods</e2> are usually slow, e.g."
Compare(e1, e2)
Comment:

5615	"in this paper, we propose a dynamic <e1>memory</e1> network, where the target information is stored and recalled from external <e2>memory</e2>, to maintain the variations of object appearance for template-matching."
sameAs(e1, e2)
Comment:

5616	"unlike tracking-by-detection where the target's information is stored in the weights of neural networks and therefore the capacity of the <e1>model</e1> is fixed after offline training, the <e2>model</e2> capacity of our memory networks can be easily enlarged by increasing the size of external memory, which is useful for memorizing long-term appearance variations."
sameAs(e1, e2)
Comment:

5617	"unlike tracking-by-detection where the target's information is stored in the weights of neural networks and therefore the capacity of the model is fixed after offline training, the model capacity of our <e1>memory</e1> networks can be easily enlarged by increasing the size of external <e2>memory</e2>, which is useful for memorizing long-term appearance variations."
sameAs(e1, e2)
Comment:

5618	"in addition, as the <e1>target</e1> position is at first unknown in the search image, we adopt an attention mechanism to locate the object roughly in the search image, thus leading to a soft representation of the <e2>target</e2> for the input to the lstm controller."
sameAs(e1, e2)
Comment:

5619	"in addition, as the target position is at first unknown in the <e1>search</e1> image, we adopt an attention mechanism to locate the object roughly in the <e2>search</e2> image, thus leading to a soft representation of the target for the input to the lstm controller."
sameAs(e1, e2)
Comment:

5620	"in addition, as the target position is at first unknown in the search <e1>image</e1>, we adopt an attention mechanism to locate the object roughly in the search <e2>image</e2>, thus leading to a soft representation of the target for the input to the lstm controller."
sameAs(e1, e2)
Comment:

5621	"-we propose gated residual template learning to generate the final <e1>matching</e1> template, which effectively controls the amount of appearance variations in retrieved memory that is added to each channel of the initial <e2>matching</e2> template."
sameAs(e1, e2)
Comment:

5622	"an lstm is used as a <e1>memory</e1> controller, where the input is the search feature map and the outputs are the control signals for the reading and writing process of the <e2>memory</e2> block."
sameAs(e1, e2)
Comment:

5623	"as the location of the <e1>target</e1> is at first unknown in the search feature map, an attention mechanism is applied to concentrate the lstm input on the potential <e2>target</e2>."
sameAs(e1, e2)
Comment:

5624	"the question arises as to whether this physical commonsense is just an end-toend model with intermediate <e1>representations</e1> being a black-box, or explicit and meaningful intermediate <e2>representations</e2>?"
sameAs(e1, e2)
Comment:

5625	"for example, <e1>we</e1> can predict that if <e2>we</e2> throw the ball in the second sequence with 10x initial speed then the cylinder might rotate."
sameAs(e1, e2)
Comment:

5626	"for example <e1>we</e1> can determine ratios between properties but not the precise values of both (e.g., <e2>we</e2> can determine the relative mass between two objects but not the exact values for both)."
sameAs(e1, e2)
Comment:

5627	"this is precisely why in [25] only <e1>one</e1> factor is inferred from motion and the <e2>other</e2> factor is directly correlated to the appearance."
Conjunction(e1, e2)
Comment:

5628	"not only <e1>we</e1> can predict the future frames of collisions but <e2>we</e2> can also predict the underlying factors that lead to such an inference."
sameAs(e1, e2)
Comment:

5629	" introduction image<e1> compressio</e1>n has traditionally been one of the tasks which neural networks were suspected to be good at, but there was little evidence that it would be possible to train a single neural network that would be competitive across<e2> compressio</e2>n rates and image sizes."
sameAs(e1, e2)
Comment:

5630	" introduction image compression has traditionally been one of the tasks which<e1> neural network</e1>s were suspected to be good at, but there was little evidence that it would be possible to train a single<e2> neural networ</e2>k that would be competitive across compression rates and image sizes."
sameAs(e1, e2)
Comment:

5631	" introduction image compression has traditionally been one of the tasks which neural networks were suspected to be good at, but there was little evidence<e1> tha</e1>t it would be possible to train a single neural network<e2> tha</e2>t would be competitive across compression rates and image sizes."
sameAs(e1, e2)
Comment:

5632	"in order to be able to <e1>measure</e1> such differences, we need to use a human visual systeminspired <e2>measure</e2> which, ideally should correlate with how humans perceive image differences."
sameAs(e1, e2)
Comment:

5633	"the advantage of integrating the output proposed by the 2d landmark <e1>location</e1> predictors -based purely on image appearance -with the 3d pose predicted by a probabilistic model, is that the 2d landmark <e2>location</e2> estimates are improved by guaranteeing that they satisfy the anatomical 3d constraints encapsulated in the human 3d pose model."
sameAs(e1, e2)
Comment:

5634	"the advantage of integrating the output proposed by the 2d landmark location predictors -based purely on image appearance -with the 3d <e1>pose</e1> predicted by a probabilistic model, is that the 2d landmark location estimates are improved by guaranteeing that they satisfy the anatomical 3d constraints encapsulated in the human 3d <e2>pose</e2> model."
sameAs(e1, e2)
Comment:

5635	"the advantage of integrating the output proposed by the 2d landmark location predictors -based purely on image appearance -with the 3d pose predicted by a probabilistic model, is <e1>that</e1> the 2d landmark location estimates are improved by guaranteeing <e2>that</e2> they satisfy the anatomical 3d constraints encapsulated in the human 3d pose model."
sameAs(e1, e2)
Comment:

5636	"the deep architecture only needs that images are annotated with 2d <e1>poses</e1>, not 3d <e2>poses</e2>."
sameAs(e1, e2)
Comment:

5637	"this decoupling between 2d and 3d <e1>training</e1> data presents a huge advantage since we can augment the <e2>training</e2> sets completely independently."
sameAs(e1, e2)
Comment:

5638	"our contribution: in this work, we show how to integrate a prelearned 3d <e1>human pose</e1> model directly within a novel cnn architecture (illustrated in figure 1 ) for joint 2d landmark and 3d <e2>human pose</e2> estimation."
sameAs(e1, e2)
Comment:

5639	"our contribution: in this work, we show how to integrate a prelearned 3d <e1>human pose</e1> model directly within a novel cnn architecture (illustrated in figure 1 ) for joint 2d landmark and 3d human <e2>pose estimation</e2>."
isA(e1, e2)
Comment:

5640	"instead, we show how such a model can be used as part of the cnn <e1>architecture</e1> itself, and how the <e2>architecture</e2> can learn to use physically plausible 3d reconstructions in its search for better 2d landmark locations."
sameAs(e1, e2)
Comment:

5641	"finding the correct 3d <e1>pose</e1> that matches the image requires injecting additional information usually in the form of 3d geometric <e2>pose</e2> priors and temporal or structural constraints."
sameAs(e1, e2)
Comment:

5642	"information captured by the 3d human pose model is embedded in the cnn architecture as an additional layer <e1>that</e1> lifts 2d landmark coordinates into 3d while imposing <e2>that</e2> they lie on the space of physically plausible poses."
sameAs(e1, e2)
Comment:

5643	"abstract introduction the recovery of a high <e1>resolution</e1> (hr) image from a low <e2>resolution</e2> (lr) version is a highly ill-posed problem since the mapping from lr to hr space can have multiple solutions."
sameAs(e1, e2)
Comment:

5644	"in addition, in previous works [2, 11] , only high-level features at top <e1>layers</e1> were used in the reconstruction of hr <e2>images</e2>."
Part-of(e1, e2)
Comment:

5645	"the reuse of <e1>feature</e1> maps from bottom layers is helpful for reducing <e2>feature</e2> redundancy, thus learning more compact cnn models."
sameAs(e1, e2)
Comment:

5646	"the reuse of feature maps from bottom <e1>layers</e1> is helpful for reducing feature redundancy, thus learning more compact <e2>cnn models</e2>."
Part-of(e1, e2)
Comment:

5647	"the proposed method has been evaluated on four publicly available benchmark <e1>datasets</e1> and outperforms the current state-of-the-art <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

5648	"therefore, it is important to collect useful contextual information in large regions from lr <e1>images</e1> so that sufficient knowledge can be captured for recovering the high-frequency details in hr <e2>images</e2>."
sameAs(e1, e2)
Comment:

5649	"one benefit from using deeper networks is that larger receptive field takes more contextual information from lr <e1>images</e1> to predict data in hr <e2>images</e2>."
sameAs(e1, e2)
Comment:

5650	"one good <e1>solution</e1> to this <e2>problem</e2> is the use of skip connections, which create short paths from top layers to bottom layers."
Used-for(e1, e2)
Comment:

5651	"one good solution to this problem is the use of skip connections, which create short paths from top <e1>layers</e1> to bottom <e2>layers</e2>."
sameAs(e1, e2)
Comment:

5652	"the first key observation to our approach is <e1>that</e1> computer vision researchers have pursued for decades the development of general-purpose stereo correspondence algorithms <e2>that</e2> do not require any adaptation to be deployed in different scenarios."
sameAs(e1, e2)
Comment:

5653	"thus, we propose to leverage on traditional <e1>stereo algorithms</e1> and state-of-the-art confidence measures in order to fine-tune a deep stereo model based on disparities provided by standard <e2>stereo algorithms</e2> that are deemed as highly reliable by the confidence measure."
sameAs(e1, e2)
Comment:

5654	"our approach deploys a loss function that, taking as <e1>target</e1> variables the disparity measurements provided by the stereo algorithm, weighs the error contribution associated with each prediction according to the estimated confidence in the corresponding <e2>target</e2> value."
sameAs(e1, e2)
Comment:

5655	"moreover, we introduce a smoothing term in the loss <e1>that</e1> penalize dissimilar predictions at nearby spatial locations, based on the conjecture <e2>that</e2> as high confidence target disparities may turn out sparse, enforcing smoothness helps propagating the predictions from high confidence locations towards low confidence ones."
sameAs(e1, e2)
Comment:

5656	"moreover, we introduce a smoothing term in the loss that penalize dissimilar <e1>predictions</e1> at nearby spatial locations, based on the conjecture that as high confidence target disparities may turn out sparse, enforcing smoothness helps propagating the <e2>predictions</e2> from high confidence locations towards low confidence ones."
sameAs(e1, e2)
Comment:

5657	"moreover, we introduce a smoothing term in the loss that penalize dissimilar predictions at nearby spatial <e1>locations</e1>, based on the conjecture that as high confidence target disparities may turn out sparse, enforcing smoothness helps propagating the predictions from high confidence <e2>locations</e2> towards low confidence ones."
sameAs(e1, e2)
Comment:

5658	"the effectiveness of our unsupervised technique is demonstrated by experimental evaluation on <e1>kitti</e1> <e2>datasets</e2> [4, 12] and middlebury 2014 [19] , assessing both adaptation ability and generalization to new data."
isA(e1, e2)
Comment:

5659	"we rely on off-the-shelf stereo algorithms together with state-of-the-art confidence measures, the <e1>latter</e1> able to ascertain upon correctness of the measurements yielded by <e2>former</e2>."
Used-for(e1, e2)
Comment:

5660	"experiments on popular datasets (<e1>kitti</e1>  2012, <e2>kitti</e2> 2015 and middlebury 2014 and other challenging test images demonstrate the effectiveness of our proposal."
sameAs(e1, e2)
Comment:

5661	"inspired by these results, we introduce two new forms of spatiotemporal <e1>convolution</e1> that can be viewed as middle grounds between the extremes of 2d (spatial <e2>convolution</e2>) and full 3d."
sameAs(e1, e2)
Comment:

5662	"the first formulation is named mixed <e1>convolution</e1> (mc) and consists in employing 3d <e2>convolutions</e2> only in the early layers of the network, with 2d convolutions in the top layers."
sameAs(e1, e2)
Comment:

5663	"the first formulation is named mixed <e1>convolution</e1> (mc) and consists in employing 3d convolutions only in the early layers of the network, with 2d <e2>convolutions</e2> in the top layers."
sameAs(e1, e2)
Comment:

5664	"the first formulation is named mixed convolution (mc) and consists in employing 3d <e1>convolutions</e1> only in the early layers of the network, with 2d <e2>convolutions</e2> in the top layers."
sameAs(e1, e2)
Comment:

5665	"the first formulation is named mixed convolution (mc) and consists in employing 3d convolutions only in the early <e1>layers</e1> of the network, with 2d convolutions in the top <e2>layers</e2>."
sameAs(e1, e2)
Comment:

5666	"the rationale behind this design is <e1>that</e1> motion modeling is a low/mid-level operation <e2>that</e2> can be implemented via 3d convolutions in the early layers of a network, and spatial reasoning over these mid-level motion features (implemented by 2d convolutions in the top layers) leads to accurate action recognition."
sameAs(e1, e2)
Comment:

5667	"the rationale behind this design is that <e1>motion</e1> modeling is a low/mid-level operation that can be implemented via 3d convolutions in the early layers of a network, and spatial reasoning over these mid-level <e2>motion</e2> features (implemented by 2d convolutions in the top layers) leads to accurate action recognition."
sameAs(e1, e2)
Comment:

5668	"the rationale behind this design is that motion modeling is a low/mid-level operation that can be implemented via 3d <e1>convolutions</e1> in the early layers of a network, and spatial reasoning over these mid-level motion features (implemented by 2d <e2>convolutions</e2> in the top layers) leads to accurate action recognition."
sameAs(e1, e2)
Comment:

5669	"the rationale behind this design is that motion modeling is a low/mid-level operation that can be implemented via 3d convolutions in the early <e1>layers</e1> of a network, and spatial reasoning over these mid-level motion features (implemented by 2d convolutions in the top <e2>layers</e2>) leads to accurate action recognition."
sameAs(e1, e2)
Comment:

5670	"we show that mc <e1>resnets</e1> yield roughly a 3-4% gain in clip-level accuracy over 2d <e2>resnets</e2> of comparable capacity and they match the performance of 3d resnets, which have 3 times as many parameters."
sameAs(e1, e2)
Comment:

5671	"we show that mc <e1>resnets</e1> yield roughly a 3-4% gain in clip-level accuracy over 2d resnets of comparable capacity and they match the performance of 3d <e2>resnets</e2>, which have 3 times as many parameters."
sameAs(e1, e2)
Comment:

5672	"we show that mc resnets yield roughly a 3-4% gain in clip-level accuracy over 2d <e1>resnets</e1> of comparable capacity and they match the performance of 3d <e2>resnets</e2>, which have 3 times as many parameters."
sameAs(e1, e2)
Comment:

5673	"the second spatiotemporal variant is a "(2+1)d" convolutional block, which explicitly factorizes 3d <e1>convolution</e1> into two separate and successive operations, a 2d spatial <e2>convolution</e2> and a 1d temporal convolution."
sameAs(e1, e2)
Comment:

5674	"the second spatiotemporal variant is a "(2+1)d" convolutional block, which explicitly factorizes 3d <e1>convolution</e1> into two separate and successive operations, a 2d spatial convolution and a 1d temporal <e2>convolution</e2>."
sameAs(e1, e2)
Comment:

5675	"the second spatiotemporal variant is a "(2+1)d" convolutional block, which explicitly factorizes 3d convolution into two separate and successive operations, a 2d spatial <e1>convolution</e1> and a 1d temporal <e2>convolution</e2>."
sameAs(e1, e2)
Comment:

5676	"this effectively doubles the <e1>number</e1> of nonlinearities compared to a network using full 3d convolutions for the same <e2>number</e2> of parameters, thus rendering the model capable of representing more complex functions."
sameAs(e1, e2)
Comment:

5677	"in <e1>this</e1> work, we challenge <e2>this</e2> view and revisit the role of temporal reasoning in action recognition by means of 3d cnns, i.e., networks that perform 3d convolutions over the spatiotemporal video volume."
sameAs(e1, e2)
Comment:

5678	"we demonstrate that 3d <e1>resnets</e1> significantly outperform 2d <e2>resnets</e2> for the same depth when trained and evaluated on large-scale, challenging action recognition benchmarks such as sports-1m [16] and kinetics [17] ."
sameAs(e1, e2)
Comment:

5679	"now, large-scale 3dmms of neutral <e1>faces</e1> are available in lsfm [8] and expressive 3dmms can be constructed by combining the statistical model of neutral <e2>faces</e2> with blendshapes [26, 9] ."
sameAs(e1, e2)
Comment:

5680	"sfs [24] , is the process of recovering <e1>surface</e1> by assuming that shading (i.e., the intensity of a pixel in the image) is generated as a function of the <e2>surface</e2> geometry and its interaction with light which is reflected/absorbed by the surface and captured by an imaging device."
sameAs(e1, e2)
Comment:

5681	"sfs [24] , is the process of recovering <e1>surface</e1> by assuming that shading (i.e., the intensity of a pixel in the image) is generated as a function of the surface geometry and its interaction with light which is reflected/absorbed by the <e2>surface</e2> and captured by an imaging device."
sameAs(e1, e2)
Comment:

5682	"sfs [24] , is the process of recovering surface by assuming that shading (i.e., the intensity of a pixel in the image) is generated as a function of the <e1>surface</e1> geometry and its interaction with light which is reflected/absorbed by the <e2>surface</e2> and captured by an imaging device."
sameAs(e1, e2)
Comment:

5683	"this function is generally modelled by the <e1>image</e1> irradiance equation: i(x, y) ∝ r(s x (x, y), s y (x, y)), (1) which states that the measured brightness of the <e2>image</e2> i(x, y) is proportional to the radiance r at the corresponding point on the surface s x (x, y), s y (x, y)."
sameAs(e1, e2)
Comment:

5684	"the most commonly employed radiance <e1>function</e1> is the lambertian <e2>function</e2>, which describes the measured brightness as being proportional to the cosine of the angle between the direction of the incident light and the surface normal."
sameAs(e1, e2)
Comment:

5685	"however, both of these <e1>methods</e1> required pre-built <e2>models</e2> in order to constrain their solutions."
Compare(e1, e2)
Comment:

5686	"the current state-of-the-art sfs <e1>methods</e1> that do not require <e2>models</e2> [57, 28] combine ideas from uncalibrated photometric stereo [4] and low-rank tensor decompositions to robustly recover a combined model of shape and identity."
Compare(e1, e2)
Comment:

5687	"other methods have also explored the adaptation of fitted 3d templates with <e1>surface</e1> normals for more plausible <e2>surface</e2> recovery [45, 46, 30, 29] ."
sameAs(e1, e2)
Comment:

5688	"human <e1>faces</e1> have a number of qualities that are desirable for shape recovery: they are extremely homogeneous in configuration (all healthy human <e2>faces</e2> have two eyes, a nose and mouth in the same approximate location), convex, exhibit approximately lambertian reflectance [54, 17, 61, 43] , are largely captured from a single direction (frontal) and are deformable and mostly not self occluding."
sameAs(e1, e2)
Comment:

5689	"furthermore, there exists a large amount of publicly available imagery of <e1>faces</e1> and human <e2>faces</e2> are of significant interest to a number of fields including entertainment, medicine, and psychology."
sameAs(e1, e2)
Comment:

5690	"a convnet normals <e1>face</e1> shape figure 1 : depiction of our pipeline for 3d <e2>face</e2> shape estimation."
sameAs(e1, e2)
Comment:

5691	"a convnet normals face <e1>shape</e1> figure 1 : depiction of our pipeline for 3d face <e2>shape</e2> estimation."
sameAs(e1, e2)
Comment:

5692	"abstract despite recent advances in training <e1>recurrent neural networks</e1> (<e2>rnns</e2>), capturing long-term dependencies in sequences remains a fundamental challenge."
sameAs(e1, e2)
Comment:

5693	"first, instead of using the vanilla <e1>recurrent network</e1>, one can use long short-term memory (lstm) (hochreiter & schmidhuber, 1997) , which is designed to improve gradient flow in <e2>recurrent networks</e2>."
sameAs(e1, e2)
Comment:

5694	"convolutional neural networks also mitigate the problem of <e1>long-term</e1> dependencies since large kernel sizes and deep networks such as resnets (he et al, 2016) allow <e2>long-term</e2> dependencies to be learnt across distant parts of an image."
sameAs(e1, e2)
Comment:

5695	"furthermore, inference in <e1>rnns</e1> also requires o(1) storage since <e2>rnns</e2> do not need to 'look back'."
sameAs(e1, e2)
Comment:

5696	"in our experiments where sequences of up to 16 000 elements is processed, <e1>lstms</e1> with auxiliary losses can train much faster and with less memory usage, while training <e2>lstms</e2> with full backprop becomes very difficult."
sameAs(e1, e2)
Comment:

5697	"this auxiliary loss forces rnns to either reconstruct previous <e1>events</e1> or predict next <e2>events</e2> in a sequence, making truncated backpropagation feasible for long sequences and also improving full bptt."
sameAs(e1, e2)
Comment:

5698	"our results highlight good performance and resource efficiency of this <e1>approach</e1> over competitive baselines, including other recurrent <e2>models</e2> and a comparable sized transformer."
Used-for(e1, e2)
Comment:

5699	"however, this trained model may not generalize well to unseen <e1>images</e1>, especially when there is a domain gap between the training (source) and test (target) <e2>images</e2>."
sameAs(e1, e2)
Comment:

5700	"to address this issue, knowledge transfer or domain adaptation techniques have been proposed to close the gap between source and <e1>target domains</e1>, where annotations are not available in the <e2>target domain</e2>."
sameAs(e1, e2)
Comment:

5701	"for image classification, one effective approach is to align <e1>features</e1> across two domains [8, 25] such that the adapted <e2>features</e2> can generalize to both domains."
sameAs(e1, e2)
Comment:

5702	"for image classification, one effective approach is to align features across two <e1>domains</e1> [8, 25] such that the adapted features can generalize to both <e2>domains</e2>."
sameAs(e1, e2)
Comment:

5703	"this motivates us to develop an effective <e1>method</e1> for adapting pixel-level prediction <e2>tasks</e2> rather than using feature adaptation."
Used-for(e1, e2)
Comment:

5704	"based on the generative adversarial network (gan) [10, 30, 22] , the proposed <e1>model</e1> consists of two parts: 1) a segmentation <e2>model</e2> to predict output results, and 2) a discriminator to distinguish whether the input is from the source or target segmentation output."
sameAs(e1, e2)
Comment:

5705	"based on the generative adversarial network (gan) [10, 30, 22] , the proposed model consists of two parts: 1) a <e1>segmentation</e1> model to predict output results, and 2) a discriminator to distinguish whether the input is from the source or target <e2>segmentation</e2> output."
sameAs(e1, e2)
Comment:

5706	"third, a multi-level adversarial learning scheme is developed to adapt <e1>features</e1> at different levels of the segmentation <e2>model</e2>, which leads to improved performance."
Used-for(e1, e2)
Comment:

5707	"we show that the proposed <e1>method</e1> performs favorably against the stateof-the-art <e2>methods</e2> in terms of accuracy and visual quality."
Compare(e1, e2)
Comment:

5708	"abstract policy <e1>gradient</e1> methods are a widely used class of model-free reinforcement learning algorithms where a state-dependent baseline is used to reduce <e2>gradient</e2> estimator variance."
sameAs(e1, e2)
Comment:

5709	"on-<e1>policy</e1> monte-carlo <e2>policy</e2> gradient estimates suffer from high variance, and therefore require large batch sizes to reliably estimate the gradient for stable iterative optimization (schulman et al, 2015a) ."
sameAs(e1, e2)
Comment:

5710	"on-policy monte-carlo policy <e1>gradient</e1> estimates suffer from high variance, and therefore require large batch sizes to reliably estimate the <e2>gradient</e2> for stable iterative optimization (schulman et al, 2015a) ."
sameAs(e1, e2)
Comment:

5711	"gu et al (2017a) ; grathwohl et al (2018) ; liu et al (2018) ; wu et al (2018) present promising results extending the classic <e1>state</e1>-dependent baselines to <e2>state</e2>-action-dependent baselines."
sameAs(e1, e2)
Comment:

5712	"gu et al (2017a) ; grathwohl et al (2018) ; liu et al (2018) ; wu et al (2018) present promising results extending the classic <e1>state</e1>-dependent baselines to state-<e2>action</e2>-dependent baselines."
Conjunction(e1, e2)
Comment:

5713	"gu et al (2017a) ; grathwohl et al (2018) ; liu et al (2018) ; wu et al (2018) present promising results extending the classic state-dependent <e1>baselines</e1> to state-action-dependent <e2>baselines</e2>."
sameAs(e1, e2)
Comment:

5714	"gu et al (2017a) ; grathwohl et al (2018) ; liu et al (2018) ; wu et al (2018) present promising results extending the classic state-dependent baselines to <e1>state</e1>-<e2>action</e2>-dependent baselines."
Conjunction(e1, e2)
Comment:

5715	"the standard explanation for the benefits of such approaches is <e1>that</e1> they achieve large reductions in variance (grathwohl et al, 2018; liu et al, 2018) , which translates to improvements over methods <e2>that</e2> only condition the baseline on the state."
sameAs(e1, e2)
Comment:

5716	"several recent papers extend the baseline to depend on both the <e1>state</e1> and <e2>action</e2> and suggest that this significantly reduces variance and improves sample efficiency without introducing bias into the gradient estimates."
Conjunction(e1, e2)
Comment:

5717	"toward this goal, we present a decomposition of the <e1>variance</e1> of the policy gradient esti-mator which isolates the potential <e2>variance</e2> reduction due to state-action-dependent baselines."
sameAs(e1, e2)
Comment:

5718	"toward this goal, we present a decomposition of the variance of the policy gradient esti-mator which isolates the potential variance reduction due to <e1>state</e1>-<e2>action</e2>-dependent baselines."
Conjunction(e1, e2)
Comment:

5719	"we numerically evaluate the <e1>variance</e1> components on a synthetic linear-quadraticgaussian (lqg) task, where the <e2>variances</e2> are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5720	"we numerically evaluate the <e1>variance</e1> components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce <e2>variance</e2> over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5721	"we numerically evaluate the <e1>variance</e1> components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the <e2>variance</e2> caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5722	"we numerically evaluate the <e1>variance</e1> components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the <e2>variance</e2> reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5723	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the <e1>variances</e1> are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce <e2>variance</e2> over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5724	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the <e1>variances</e1> are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the <e2>variance</e2> caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5725	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the <e1>variances</e1> are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the <e2>variance</e2> reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5726	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control <e1>tasks</e1> and draw two conclusions: (1) on these <e2>tasks</e2>, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5727	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on <e1>these</e1> <e2>tasks</e2>, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
Used-for(e1, e2)
Comment:

5728	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned <e1>state</e1>-<e2>action</e2>-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
Conjunction(e1, e2)
Comment:

5729	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned <e1>state</e1>-action-dependent baseline does not significantly reduce variance over a learned <e2>state</e2>-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5730	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned <e1>state</e1>-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or <e2>state</e2>-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5731	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned <e1>state</e1>-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding <e2>action</e2> dependence to the baseline."
Conjunction(e1, e2)
Comment:

5732	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-<e1>action</e1>-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding <e2>action</e2> dependence to the baseline."
sameAs(e1, e2)
Comment:

5733	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent <e1>baseline</e1> does not significantly reduce variance over a learned state-dependent <e2>baseline</e2>, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5734	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent <e1>baseline</e1> does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent <e2>baseline</e2> is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5735	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent <e1>baseline</e1> does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the <e2>baseline</e2>."
sameAs(e1, e2)
Comment:

5736	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce <e1>variance</e1> over a learned state-dependent baseline, and (2) the <e2>variance</e2> caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5737	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce <e1>variance</e1> over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the <e2>variance</e2> reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5738	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned <e1>state</e1>-dependent baseline, and (2) the variance caused by using a function approximator for the value function or <e2>state</e2>-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5739	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned <e1>state</e1>-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding <e2>action</e2> dependence to the baseline."
Conjunction(e1, e2)
Comment:

5740	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent <e1>baseline</e1>, and (2) the variance caused by using a function approximator for the value function or state-dependent <e2>baseline</e2> is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5741	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent <e1>baseline</e1>, and (2) the variance caused by using a function approximator for the value function or state-dependent baseline is much larger than the variance reduction from adding action dependence to the <e2>baseline</e2>."
sameAs(e1, e2)
Comment:

5742	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the <e1>variance</e1> caused by using a function approximator for the value function or state-dependent baseline is much larger than the <e2>variance</e2> reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5743	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a <e1>function</e1> approximator for the value <e2>function</e2> or state-dependent baseline is much larger than the variance reduction from adding action dependence to the baseline."
sameAs(e1, e2)
Comment:

5744	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or <e1>state</e1>-dependent baseline is much larger than the variance reduction from adding <e2>action</e2> dependence to the baseline."
Conjunction(e1, e2)
Comment:

5745	"we numerically evaluate the variance components on a synthetic linear-quadraticgaussian (lqg) task, where the variances are nearly analytically tractable, and on benchmark continuous control tasks and draw two conclusions: (1) on these tasks, a learned state-action-dependent baseline does not significantly reduce variance over a learned state-dependent baseline, and (2) the variance caused by using a function approximator for the value function or state-dependent <e1>baseline</e1> is much larger than the variance reduction from adding action dependence to the <e2>baseline</e2>."
sameAs(e1, e2)
Comment:

5746	"we explain and empirically evaluate variants of <e1>these</e1> prior methods to demonstrate that <e2>these</e2> subtle implementation details, which trade variance for bias, are in fact crucial for their empirical success."
sameAs(e1, e2)
Comment:

5747	"the second observation (2), <e1>that</e1> function approximators poorly estimate the value function, suggests <e2>that</e2> there is room for improvement."
sameAs(e1, e2)
Comment:

5748	"the second observation (2), that <e1>function</e1> approximators poorly estimate the value <e2>function</e2>, suggests that there is room for improvement."
sameAs(e1, e2)
Comment:

5749	"we propose a horizonaware value function parameterization, and this improves performance compared with the <e1>state</e1>-<e2>action</e2>-dependent baseline without biasing the underlying method."
Conjunction(e1, e2)
Comment:

5750	"to better understand this development, we decompose the <e1>variance</e1> of the policy gradient estimator and numerically show that learned state-actiondependent baselines do not in fact reduce <e2>variance</e2> over a state-dependent baseline in commonly tested benchmark domains."
sameAs(e1, e2)
Comment:

5751	"to better understand this development, we decompose the variance of the policy gradient estimator and numerically show that learned <e1>state</e1>-actiondependent baselines do not in fact reduce variance over a <e2>state</e2>-dependent baseline in commonly tested benchmark domains."
sameAs(e1, e2)
Comment:

5752	"we demonstrate that using our method allows predicting shape representations which can be leveraged for obtaining a consistent parsing across the instances of a <e1>shape</e1> collection and constructing an interpretable <e2>shape</e2> similarity measure."
sameAs(e1, e2)
Comment:

5753	" introduction deep generative models have recently received an increasing amount of attention, not only because<e1> the</e1>y provide a means to learn deep feature representations in an unsupervised manner that can potentially leverage all the unlabeled images on the internet for training, but also because<e2> the</e2>y can be used to generate novel images necessary for various vision applications."
sameAs(e1, e2)
Comment:

5754	" introduction deep generative models have recently received an increasing amount of attention, not only because they provide a means to learn deep feature representations in an unsupervised manner that can potentially leverage all the unlabeled<e1> image</e1>s on the internet for training, but also because they can be used to generate novel<e2> image</e2>s necessary for various vision applications."
sameAs(e1, e2)
Comment:

5755	"third, as human beings have evolved to be sensitive to <e1>motion</e1>, <e2>motion</e2> artifacts are particularly perceptible."
sameAs(e1, e2)
Comment:

5756	"recently, a few attempts to approach the video generation problem were made through <e1>generative adversarial networks</e1> (<e2>gans</e2>) [12] ."
sameAs(e1, e2)
Comment:

5757	"as steady progress toward better image <e1>generation</e1> is made, it is also important to study the video <e2>generation</e2> problem."
sameAs(e1, e2)
Comment:

5758	"in addition, as videos are about objects (content) performing actions (<e1>motion</e1>), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the <e2>motion</e2> subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5759	"in addition, as videos are about objects (content) performing actions (<e1>motion</e1>), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the motion subspace) results in temporal <e2>motions</e2>."
sameAs(e1, e2)
Comment:

5760	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two <e1>subspaces</e1>, where the deviation of a point in the first <e2>subspace</e2> (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5761	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two <e1>subspaces</e1>, where the deviation of a point in the first subspace (the content <e2>subspace</e2>) leads content changes in a video clip and the deviation in the second subspace (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5762	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two <e1>subspaces</e1>, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second <e2>subspace</e2> (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5763	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two <e1>subspaces</e1>, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the motion <e2>subspace</e2>) results in temporal motions."
sameAs(e1, e2)
Comment:

5764	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first <e1>subspace</e1> (the content <e2>subspace</e2>) leads content changes in a video clip and the deviation in the second subspace (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5765	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first <e1>subspace</e1> (the content subspace) leads content changes in a video clip and the deviation in the second <e2>subspace</e2> (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5766	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first <e1>subspace</e1> (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the motion <e2>subspace</e2>) results in temporal motions."
sameAs(e1, e2)
Comment:

5767	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content <e1>subspace</e1>) leads content changes in a video clip and the deviation in the second <e2>subspace</e2> (the motion subspace) results in temporal motions."
sameAs(e1, e2)
Comment:

5768	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content <e1>subspace</e1>) leads content changes in a video clip and the deviation in the second subspace (the motion <e2>subspace</e2>) results in temporal motions."
sameAs(e1, e2)
Comment:

5769	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second <e1>subspace</e1> (the motion <e2>subspace</e2>) results in temporal motions."
sameAs(e1, e2)
Comment:

5770	"in addition, as videos are about objects (content) performing actions (motion), the latent space of images should be further decomposed into two subspaces, where the deviation of a point in the first subspace (the content subspace) leads content changes in a video clip and the deviation in the second subspace (the <e1>motion</e1> subspace) results in temporal <e2>motions</e2>."
sameAs(e1, e2)
Comment:

5771	"by changing the content representation while fixing the <e1>motion</e1> trajectory, we have videos of different objects performing the same <e2>motion</e2>."
sameAs(e1, e2)
Comment:

5772	"it generates a <e1>video</e1> clip by sequentially generating <e2>video</e2> frames."
sameAs(e1, e2)
Comment:

5773	"at each time step, an <e1>image</e1> generative network maps a random vector to an <e2>image</e2>."
sameAs(e1, e2)
Comment:

5774	"this vector consists of two parts where the first is sampled from a content <e1>subspace</e1> and the second is sampled from a motion <e2>subspace</e2>."
sameAs(e1, e2)
Comment:

5775	"however, the extension from generating images to generating videos turns out to be a highly challenging <e1>task</e1>, although the generated <e2>data</e2> has just one more dimension -the time dimension."
Conjunction(e1, e2)
Comment:

5776	"since content in a short <e1>video</e1> clip usually remains the same, we model the content space using a gaussian distribution and use the same realization to generate each frame in a <e2>video</e2> clip."
sameAs(e1, e2)
Comment:

5777	"first, since a video is a spatio-temporal recording of visual information of <e1>objects</e1> performing various actions, a generative model needs to learn the plausible physical motion models of <e2>objects</e2> in addition to learn- by sampling a point in the content subspace and sampling different trajectories in the motion subspace, it generates videos of the same object performing different motion."
sameAs(e1, e2)
Comment:

5778	"first, since a video is a spatio-temporal recording of visual information of objects performing various actions, a generative model needs to <e1>learn</e1> the plausible physical motion models of objects in addition to <e2>learn</e2>- by sampling a point in the content subspace and sampling different trajectories in the motion subspace, it generates videos of the same object performing different motion."
sameAs(e1, e2)
Comment:

5779	"first, since a video is a spatio-temporal recording of visual information of objects performing various actions, a generative model needs to learn the plausible physical motion models of objects in addition to learn- by <e1>sampling</e1> a point in the content subspace and <e2>sampling</e2> different trajectories in the motion subspace, it generates videos of the same object performing different motion."
sameAs(e1, e2)
Comment:

5780	"first, since a video is a spatio-temporal recording of visual information of objects performing various actions, a generative model needs to learn the plausible physical motion models of objects in addition to learn- by sampling a point in the content <e1>subspace</e1> and sampling different trajectories in the motion <e2>subspace</e2>, it generates videos of the same object performing different motion."
sameAs(e1, e2)
Comment:

5781	"first, since a video is a spatio-temporal recording of visual information of objects performing various actions, a generative model needs to learn the plausible physical motion models of objects in addition to learn- by sampling a point in the content subspace and sampling different trajectories in the <e1>motion</e1> subspace, it generates videos of the same object performing different <e2>motion</e2>."
sameAs(e1, e2)
Comment:

5782	"by sampling different points in the content <e1>subspace</e1> and the same motion trajectory in the motion <e2>subspace</e2>, it generates videos of different objects performing the same motion."
sameAs(e1, e2)
Comment:

5783	"by sampling different points in the content subspace and the same <e1>motion</e1> trajectory in the <e2>motion</e2> subspace, it generates videos of different objects performing the same motion."
sameAs(e1, e2)
Comment:

5784	"by sampling different points in the content subspace and the same <e1>motion</e1> trajectory in the motion subspace, it generates videos of different objects performing the same <e2>motion</e2>."
sameAs(e1, e2)
Comment:

5785	"by sampling different points in the content subspace and the same motion trajectory in the <e1>motion</e1> subspace, it generates videos of different objects performing the same <e2>motion</e2>."
sameAs(e1, e2)
Comment:

5786	"introduction superpixels are the <e1>image</e1> regions generated by grouping <e2>image</e2> pixels."
sameAs(e1, e2)
Comment:

5787	"the process of extracting superpixels is known as superpixel <e1>segmentation</e1> or over-<e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

5788	"superpixels are widely used in both conventional energyminimization [32, 30] and recent <e1>deep learning</e1> [23, 13, 9] frameworks with applications to a wide range of problems such as salient <e2>object detection</e2> [29, 32, 13] , and semantic segmentation [23, 9] , to name a few."
Used-for(e1, e2)
Comment:

5789	"our experiments show that simply replacing the hand-crafted <e1>features</e1> with pre-trained deep <e2>features</e2> for computing pixel affinities does not result in good superpixel segmentation."
sameAs(e1, e2)
Comment:

5790	"however, our experiments show <e1>that</e1> this approach results in inferior performance compared to <e2>that</e2> of using hand-crafted pixel affinities."
sameAs(e1, e2)
Comment:

5791	"we therefore propose a method for learning <e1>segmentation</e1>-aware affinities for graph-based superpixel <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

5792	"specifically, we propose a deep network, termed pixel affinity net (pan), for learning pixel affinities and exploiting existing <e1>segmentation</e1> datasets [3, 6] as supervisory signals where pixel affinities should be low at <e2>segmentation</e2> boundaries and high elsewhere."
sameAs(e1, e2)
Comment:

5793	"to ensure the predicted affinities result in good <e1>segmentation</e1> quality, we propose a new <e2>segmentation</e2>-aware loss (seal) function."
sameAs(e1, e2)
Comment:

5794	"in addition, we show <e1>that</e1> improvements in superpixel accuracy, obtained with our approach, also translates to performance improvements in vision tasks <e2>that</e2> rely on superpixels, such as semantic segmentation and salient object detection."
sameAs(e1, e2)
Comment:

5795	"in addition, we show that improvements in superpixel <e1>accuracy</e1>, obtained with our <e2>approach</e2>, also translates to performance improvements in vision tasks that rely on superpixels, such as semantic segmentation and salient object detection."
Evaluate-for(e1, e2)
Comment:

5796	"we show a simple integration of deep <e1>features</e1> with existing superpixel algorithms does not result in better performance as these <e2>features</e2> do not model segmentation."
sameAs(e1, e2)
Comment:

5797	"we show a simple integration of deep <e1>features</e1> with existing superpixel algorithms does not result in better performance as these features do not <e2>model</e2> segmentation."
Used-for(e1, e2)
Comment:

5798	"we show a simple integration of deep features with existing superpixel algorithms does not result in better performance as these <e1>features</e1> do not <e2>model</e2> segmentation."
Used-for(e1, e2)
Comment:

5799	"• we develop a novel <e1>segmentation</e1>-aware loss that makes use of <e2>segmentation</e2> errors to learn affinities for superpixel segmentation."
sameAs(e1, e2)
Comment:

5800	"• we develop a novel <e1>segmentation</e1>-aware loss that makes use of segmentation errors to learn affinities for superpixel <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

5801	"• we develop a novel segmentation-aware loss that makes use of <e1>segmentation</e1> errors to learn affinities for superpixel <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

5802	"• we demonstrate that with the learned pixel affinities, the computed superpixels preserve object boundaries better than <e1>those</e1> with hand-crafted <e2>features</e2>."
Compare(e1, e2)
Comment:

5803	"instead, we propose a <e1>segmentation</e1>-aware affinity learning approach for superpixel <e2>segmentation</e2>."
sameAs(e1, e2)
Comment:

5804	"adversarial adaptation methods have become an increasingly popular incarnation of this type of approach which seeks to minimize an approximate <e1>domain</e1> discrepancy distance through an adversarial objective with respect to a <e2>domain</e2> discriminator."
sameAs(e1, e2)
Comment:

5805	"however, due to a phenomenon known as dataset bias or <e1>domain</e1> shift [3] , recognition models trained along with these representations on one large dataset do not generalize well to source target target*encoder <e2>domain</e2>*discriminator we propose an improved unsupervised domain adaptation method that combines adversarial learning with discriminative feature learning."
sameAs(e1, e2)
Comment:

5806	"we therefore propose a previously unexplored unsupervised <e1>adversarial</e1> adaptation method, <e2>adversarial</e2> discriminative domain adaptation (adda), illustrated in figure 1 ."
sameAs(e1, e2)
Comment:

5807	"our approach is simple yet surprisingly powerful and achieves state-of-the-art visual adaptation results on the <e1>mnist</e1>, usps, and svhn digits <e2>datasets</e2>."
isA(e1, e2)
Comment:

5808	"specifically, we learn a discriminative mapping of <e1>target</e1> images to the source feature space (<e2>target</e2> encoder) by fooling a domain discriminator that tries to distinguish the encoded target images from source examples."
sameAs(e1, e2)
Comment:

5809	"specifically, we learn a discriminative mapping of <e1>target</e1> images to the source feature space (target encoder) by fooling a domain discriminator that tries to distinguish the encoded <e2>target</e2> images from source examples."
sameAs(e1, e2)
Comment:

5810	"specifically, we learn a discriminative mapping of target <e1>images</e1> to the source feature space (target encoder) by fooling a domain discriminator that tries to distinguish the encoded target <e2>images</e2> from source examples."
sameAs(e1, e2)
Comment:

5811	"specifically, we learn a discriminative mapping of target images to the source feature space (<e1>target</e1> encoder) by fooling a domain discriminator that tries to distinguish the encoded <e2>target</e2> images from source examples."
sameAs(e1, e2)
Comment:

5812	"the typical solution is to further fine-tune these <e1>networks</e1> on task-specific datasetshowever, it is often prohibitively difficult and expensive to obtain enough labeled data to properly fine-tune the large number of parameters employed by deep multilayer <e2>networks</e2>."
sameAs(e1, e2)
Comment:

5813	"the typical solution is to further fine-tune these networks on <e1>task</e1>-specific datasetshowever, it is often prohibitively difficult and expensive to obtain enough labeled <e2>data</e2> to properly fine-tune the large number of parameters employed by deep multilayer networks."
Conjunction(e1, e2)
Comment:

5814	"previous work proposed techniques for domain <e1>transfer</e1> with cnn models [12, 24] but did not utilize the learned source semantic structure for task <e2>transfer</e2>."
sameAs(e1, e2)
Comment:

5815	"thus, we also explicitly transfer the similarity <e1>structure</e1> amongst categories from the source to the target and further optimize our representation to produce the same <e2>structure</e2> in the target domain using the few target labeled examples as reference points."
sameAs(e1, e2)
Comment:

5816	"thus, we also explicitly transfer the similarity structure amongst categories from the source to the <e1>target</e1> and further optimize our representation to produce the same structure in the target domain using the few <e2>target</e2> labeled examples as reference points."
sameAs(e1, e2)
Comment:

5817	"our architecture can be used to solve supervised <e1>adaptation</e1>, when a small amount of target labeled data is available from each category, and semi-supervised <e2>adaptation</e2>, when a small amount of target labeled data is available from a subset of the categories."
sameAs(e1, e2)
Comment:

5818	"our architecture can be used to solve supervised adaptation, when a small amount of <e1>target</e1> labeled data is available from each category, and semi-supervised adaptation, when a small amount of <e2>target</e2> labeled data is available from a subset of the categories."
sameAs(e1, e2)
Comment:

5819	"our architecture can be used to solve supervised adaptation, when a small amount of target labeled <e1>data</e1> is available from each category, and semi-supervised adaptation, when a small amount of target labeled <e2>data</e2> is available from a subset of the categories."
sameAs(e1, e2)
Comment:

5820	"even the traditional <e1>approach</e1> for adapting deep models, fine-tuning [14, 29] , may require hundreds or thousands of labeled examples for each <e2>object category</e2> that needs to be adapted."
Used-for(e1, e2)
Comment:

5821	"for instance, the authors of [33] recently showed <e1>that</e1> the same image classification network <e2>that</e2> generalizes well when trained on genuine data can also overfit when presented with random labels."
sameAs(e1, e2)
Comment:

5822	"thus, generalization requires the <e1>structure</e1> of the network to "resonate" with the <e2>structure</e2> of the data."
sameAs(e1, e2)
Comment:

5823	"in this work, we show <e1>that</e1>, contrary to the belief <e2>that</e2> learning is necessary for building good image priors, a great deal of image statistics are captured by the structure of a convolutional image generator independent of learning."
sameAs(e1, e2)
Comment:

5824	"in this work, we show that, contrary to the belief that learning is necessary for building good <e1>image</e1> priors, a great deal of <e2>image</e2> statistics are captured by the structure of a convolutional image generator independent of learning."
sameAs(e1, e2)
Comment:

5825	"in this work, we show that, contrary to the belief that learning is necessary for building good <e1>image</e1> priors, a great deal of image statistics are captured by the structure of a convolutional <e2>image</e2> generator independent of learning."
sameAs(e1, e2)
Comment:

5826	"in this work, we show that, contrary to the belief that learning is necessary for building good image priors, a great deal of <e1>image</e1> statistics are captured by the structure of a convolutional <e2>image</e2> generator independent of learning."
sameAs(e1, e2)
Comment:

5827	"the weights are randomly initialized and fitted to maximize their likelihood given a specific degraded image and a <e1>task</e1>-dependent observation <e2>model</e2>."
Evaluate-for(e1, e2)
Comment:

5828	"stated in a different way, we cast reconstruction as a conditional <e1>image</e1> generation problem and show that the only information required to solve it is contained in the single degraded input <e2>image</e2> and the handcrafted structure of the network used for reconstruction."
sameAs(e1, e2)
Comment:

5829	"we show that this very simple formulation is very competitive for standard <e1>image processing</e1> problems such as denoising, inpainting and <e2>super-resolution</e2>."
Used-for(e1, e2)
Comment:

5830	"this is particularly remarkable because no aspect of the <e1>network</e1> is learned from data; instead, the weights of the <e2>network</e2> are always randomly initialized, so that the only prior information is in the structure of the network itself."
sameAs(e1, e2)
Comment:

5831	"this is particularly remarkable because no aspect of the <e1>network</e1> is learned from data; instead, the weights of the network are always randomly initialized, so that the only prior information is in the structure of the <e2>network</e2> itself."
sameAs(e1, e2)
Comment:

5832	"this is particularly remarkable because no aspect of the network is learned from data; instead, the weights of the <e1>network</e1> are always randomly initialized, so that the only prior information is in the structure of the <e2>network</e2> itself."
sameAs(e1, e2)
Comment:

5833	"since the new <e1>regularizer</e1>, like the tv norm, is not learned from data but is entirely handcrafted, the resulting visualizations avoid potential biases arising form the use of powerful learned <e2>regularizers</e2> [8] ."
sameAs(e1, e2)
Comment:

5834	"state-of-the-art convnets for image restoration and gencode and supplementary material are available at https:// dmitryulyanov.github.io/deep_image_prior (a) ground truth (b) srresnet [19] , trained our method uses a randomly-initialized convnet to upsample an <e1>image</e1>, using its structure as an <e2>image</e2> prior; similar to bicubic upsampling, this method does not require learning, but produces much cleaner results with sharper edges."
sameAs(e1, e2)
Comment:

5835	"in fact, our <e1>results</e1> are quite close to state-of-the-art superresolution <e2>methods</e2> that use convnets learned from large datasets."
Compare(e1, e2)
Comment:

5836	"in fact, previous work has shown <e1>that</e1> neural networks <e2>that</e2> are trained on sufficiently diverse object recognition classes, such as vgg [38] trained on imagenet [22] , learn surprisingly versatile feature spaces and can be used to train linear classifiers for additional image classes."
sameAs(e1, e2)
Comment:

5837	"in fact, previous work has shown that <e1>neural networks</e1> that are trained on sufficiently diverse object recognition classes, such as vgg [38] trained on imagenet [22] , learn surprisingly versatile feature spaces and can be used to train linear <e2>classifiers</e2> for additional image classes."
Used-for(e1, e2)
Comment:

5838	"in fact, previous work has shown that neural networks that are trained on sufficiently diverse object recognition <e1>classes</e1>, such as vgg [38] trained on imagenet [22] , learn surprisingly versatile feature spaces and can be used to train linear classifiers for additional image <e2>classes</e2>."
sameAs(e1, e2)
Comment:

5839	"inspired by this hypothesis, we argue that, in such deep step 1: <e1>map</e1> images to deep feature space w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in feature space target set (men w/facial hair) source set (men w/o facial hair) deep feature space step 4: reverse <e2>map</e2> to color space step 1: mapping details step 4: reverse mapping details feature spaces, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5840	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep <e1>feature space</e1> w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in <e2>feature space</e2> target set (men w/facial hair) source set (men w/o facial hair) deep feature space step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details feature spaces, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5841	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep <e1>feature space</e1> w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in feature space target set (men w/facial hair) source set (men w/o facial hair) deep <e2>feature space</e2> step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details feature spaces, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5842	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep <e1>feature space</e1> w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in feature space target set (men w/facial hair) source set (men w/o facial hair) deep feature space step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details <e2>feature spaces</e2>, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5843	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep feature space w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in <e1>feature space</e1> target set (men w/facial hair) source set (men w/o facial hair) deep <e2>feature space</e2> step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details feature spaces, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5844	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep feature space w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in <e1>feature space</e1> target set (men w/facial hair) source set (men w/o facial hair) deep feature space step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details <e2>feature spaces</e2>, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5845	"inspired by this hypothesis, we argue that, in such deep step 1: map images to deep feature space w step 2: compute attribute vector φ(x) + αw φ(x) step 3: interpolate in feature space target set (men w/facial hair) source set (men w/o facial hair) deep <e1>feature space</e1> step 4: reverse map to color space step 1: mapping details step 4: reverse mapping details <e2>feature spaces</e2>, some image editing tasks may no longer be as challenging as previously believed."
sameAs(e1, e2)
Comment:

5846	"we propose a simple framework <e1>that</e1> leverages the notion <e2>that</e2> in the right feature space, image editing can be performed simply by linearly interpolating between images with a certain attribute and images without it."
sameAs(e1, e2)
Comment:

5847	"we propose a simple framework that leverages the notion that in the right feature space, image editing can be performed simply by linearly interpolating between <e1>images</e1> with a certain attribute and <e2>images</e2> without it."
sameAs(e1, e2)
Comment:

5848	"for instance, consider the task of adding facial hair to the image of a male face, given two sets of images: <e1>one</e1> set with facial hair, and <e2>one</e2> set without."
sameAs(e1, e2)
Comment:

5849	"indeed, <e1>we</e1> will show that even a simple choice of this vector suffices: <e2>we</e2> average convolutional layer features of each set of images and take the difference."
sameAs(e1, e2)
Comment:

5850	"however, these assumptions on the <e1>data</e1> are comparable to what is typically used to train generative models, and in the presence of such <e2>data</e2> dfi works surprisingly well."
sameAs(e1, e2)
Comment:

5851	"despite its simplicity we show <e1>that</e1> on many of these image editing tasks it outperforms state-of-the-art methods <e2>that</e2> are substantially more involved and specialized."
sameAs(e1, e2)
Comment:

5852	"despite its simplicity we show that on many of <e1>these</e1> image editing <e2>tasks</e2> it outperforms state-of-the-art methods that are substantially more involved and specialized."
Used-for(e1, e2)
Comment:

5853	"although <e1>these</e1> approaches can perform a variety of seemingly impressive <e2>tasks</e2>, in this paper we show that a surprisingly large set of them can be solved via linear interpolation in deep feature space and may not require input older specialized deep architectures."
Used-for(e1, e2)
Comment:

5854	"the success of <e1>image</e1> based place recognition is largely attributed to the ability to extract <e2>image</e2> feature descriptors e.g."
sameAs(e1, e2)
Comment:

5855	"it is obvious <e1>that</e1> the lighting (day and night) and seasonal (with and without snow) changes made it difficult even for human eye to tell <e2>that</e2> the pair of images (bottom row) are from the same scene."
sameAs(e1, e2)
Comment:

5856	"in view of the potential that lidar point clouds could be better in the localization task, we propose the pointnetvlad -a deep network for large-scale 3d <e1>point cloud</e1> retrieval to fill in the gap of place recognition in the 3d <e2>point cloud</e2> based localization."
sameAs(e1, e2)
Comment:

5857	"given a query <e1>image</e1> or lidar scan of a local scene, we then search through the database to retrieve the best match that will tell us the exact pose of the query <e2>image</e2>/scan with respect to the reference map."
sameAs(e1, e2)
Comment:

5858	"we create benchmark <e1>datasets</e1> for point cloud based retrieval for place recognition based on the open-source oxford robotcar dataset [24] and three additional <e2>datasets</e2> collected from three different areas with a velodyne-64 lidar mounted on a car."
sameAs(e1, e2)
Comment:

5859	"in place 1 https://github.com/mikacuy/pointnetvlad.git figure 1. <e1>two</e1> pairs of 3d lidar point clouds (top row) and images (bottom row) taken from <e2>two</e2> different times."
sameAs(e1, e2)
Comment:

5860	"it can be seen <e1>that</e1> the pair of 3d lidar point cloud remain largely invariant to the lighting and seasonal changes <e2>that</e2> made it difficult to match the pair of images."
sameAs(e1, e2)
Comment:

5861	"however, the idea is wasteful in terms of <e1>parameters</e1> needed, redundancy of <e2>parameters</e2>, as well as the associated need for training data."
sameAs(e1, e2)
Comment:

5862	"third, a learned masking operation <e1>that</e1>, given the condition, selects the relevant embedding dimensions <e2>that</e2> induce a subspace which encodes the queried visual concept."
sameAs(e1, e2)
Comment:

5863	"in the proposed approach the convolutional network <e1>that</e1> learns the disentangled embedding as well as the masks <e2>that</e2> learn to select relevant dimensions are trained jointly."
sameAs(e1, e2)
Comment:

5864	"in the proposed approach the convolutional network that <e1>learns</e1> the disentangled embedding as well as the masks that <e2>learn</e2> to select relevant dimensions are trained jointly."
sameAs(e1, e2)
Comment:

5865	"we demonstrate that csns clearly outperform single triplet <e1>networks</e1>, and even sets of specialist triplet <e2>networks</e2> where a lot more parameters are available and each network is trained towards one single similarity notion."
sameAs(e1, e2)
Comment:

5866	"our contributions are a) formulating conditional similarity <e1>networks</e1>, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet <e2>networks</e2> and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5867	"our contributions are a) formulating conditional similarity <e1>networks</e1>, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet <e2>networks</e2> in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5868	"our contributions are a) formulating conditional similarity networks, an <e1>approach</e1> that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed <e2>approach</e2> outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5869	"our contributions are a) formulating conditional similarity networks, an <e1>approach</e1> that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our <e2>approach</e2> successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5870	"our contributions are a) formulating conditional similarity networks, an approach <e1>that</e1> allows to to learn nonlinear embeddings <e2>that</e2> incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5871	"our contributions are a) formulating conditional similarity networks, an approach <e1>that</e1> allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating <e2>that</e2> the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5872	"our contributions are a) formulating conditional similarity networks, an approach <e1>that</e1> allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating <e2>that</e2> our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5873	"our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings <e1>that</e1> incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating <e2>that</e2> the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5874	"our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings <e1>that</e1> incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating <e2>that</e2> our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5875	"our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating <e1>that</e1> the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating <e2>that</e2> our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5876	"our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed <e1>approach</e1> outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our <e2>approach</e2> successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5877	"our contributions are a) formulating conditional similarity networks, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet <e1>networks</e1> and even sets of specialist triplet <e2>networks</e2> in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable."
sameAs(e1, e2)
Comment:

5878	"here, we demonstrate three intuitive concepts, which are challenging to combine for a machine vision algorithm that has to embed <e1>objects</e1> in a feature space where distances preserve the relative dissimilarity: shoes are of the same category; red <e2>objects</e2> are more similar in terms of color; sneakers and t-shirts are stylistically closer."
sameAs(e1, e2)
Comment:

5879	"we use long short term memory (lstm) networks [12] , a type of <e1>recurrent neural network</e1> (<e2>rnn</e2>) that has achieved great success on similar sequence-to-sequence tasks such as speech recognition [10] and machine translation [34] ."
sameAs(e1, e2)
Comment:

5880	"we use long short term memory (lstm) networks [12] , a type of recurrent neural network (rnn) that has achieved great success on similar sequence-to-sequence tasks such as <e1>speech recognition</e1> [10] and <e2>machine translation</e2> [34] ."
Conjunction(e1, e2)
Comment:

5881	"due to the inherent sequential nature of <e1>videos</e1> and language, lstms are well-suited for generating descriptions of events in <e2>videos</e2>."
sameAs(e1, e2)
Comment:

5882	"a stacked lstm first encodes the frames <e1>one</e1> by <e2>one</e2>, taking as input the output of a convolutional neural network (cnn) applied to each input frame's intensity values."
sameAs(e1, e2)
Comment:

5883	"a stacked lstm first encodes the frames one by one, taking as <e1>input</e1> the output of a convolutional neural network (cnn) applied to each <e2>input</e2> frame's intensity values."
sameAs(e1, e2)
Comment:

5884	"this allows our model to (a) handle a variable number of input frames, (b) <e1>learn</e1> and use the temporal structure of the video and (c) <e2>learn</e2> a language model to generate natural, grammatical sentences."
sameAs(e1, e2)
Comment:

5885	"our <e1>model</e1> is learned jointly and end-to-end, incorporating both intensity and optical flow inputs, and does not require an explicit attention <e2>model</e2>."
sameAs(e1, e2)
Comment:

5886	"we demonstrate that s2vt achieves state-of-the-art performance on three diverse <e1>datasets</e1>, a standard youtube corpus (msvd) [3] and the m-vad [37] and mpii movie description [28] <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

5887	"related approaches to video description have resolved variable length <e1>input</e1> by holistic video representations [29, 28, 11] , pooling over frames [39] , or sub-sampling on a fixed number of <e2>input</e2> frames [43] ."
sameAs(e1, e2)
Comment:

5888	"to <e1>learn</e1> what is worth describing, our model <e2>learns</e2> from video clips and paired sentences that describe the depicted events in natural language."
sameAs(e1, e2)
Comment:

5889	"abstract we propose a hierarchical approach for making <e1>long-term</e1> <e2>predictions</e2> of future frames."
Feature-of(e1, e2)
Comment:

5890	"in order to make reasonable <e1>long-term</e1> frame <e2>predictions</e2> in natural videos, these approaches need to be highly robust to pixel-level noise."
Feature-of(e1, e2)
Comment:

5891	"it is common that the first few <e1>prediction</e1> steps are of decent quality, but then the <e2>prediction</e2> degrades dramatically until all the video context is lost."
sameAs(e1, e2)
Comment:

5892	"in this work <e1>we</e1> assume that the high-dimensional video data is generated from low-dimensional high-level structures, which <e2>we</e2> hypothesize will be critical for making long-term visual predictions."
sameAs(e1, e2)
Comment:

5893	"in this work we assume that the high-dimensional video data is generated from low-dimensional high-level structures, which we hypothesize will be critical for making <e1>long-term</e1> visual <e2>predictions</e2>."
Feature-of(e1, e2)
Comment:

5894	"to avoid inherent compounding errors in recursive pixellevel prediction, <e1>we</e1> propose to first estimate highlevel structure in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, <e2>we</e2> construct the future frames without having to observe any of the pixel-level predictions."
sameAs(e1, e2)
Comment:

5895	"to avoid inherent compounding errors in recursive pixellevel prediction, we propose to first estimate highlevel <e1>structure</e1> in the input frames, then predict how that <e2>structure</e2> evolves in the future, and finally by observing a single frame from the past and the predicted high-level structure, we construct the future frames without having to observe any of the pixel-level predictions."
sameAs(e1, e2)
Comment:

5896	"to avoid inherent compounding errors in recursive pixellevel prediction, we propose to first estimate highlevel <e1>structure</e1> in the input frames, then predict how that structure evolves in the future, and finally by observing a single frame from the past and the predicted high-level <e2>structure</e2>, we construct the future frames without having to observe any of the pixel-level predictions."
sameAs(e1, e2)
Comment:

5897	"to avoid inherent compounding errors in recursive pixellevel prediction, we propose to first estimate highlevel structure in the input frames, then predict how that <e1>structure</e1> evolves in the future, and finally by observing a single frame from the past and the predicted high-level <e2>structure</e2>, we construct the future frames without having to observe any of the pixel-level predictions."
sameAs(e1, e2)
Comment:

5898	"our main contribution is the hierarchical approach for <e1>video</e1> prediction that involves generative modeling of <e2>video</e2> using high-level structures."
sameAs(e1, e2)
Comment:

5899	"concretely, our algorithm first estimates high-level <e1>structures</e1> of observed frames, and then predicts their future states, and finally generates future frames conditioned on predicted high-level <e2>structures</e2>."
sameAs(e1, e2)
Comment:

5900	"the prediction of future structure is performed by an lstm that observes a sequence of <e1>structures</e1> estimated by a cnn, encodes the observed dynamics, and predicts the future sequence of such <e2>structures</e2>."
sameAs(e1, e2)
Comment:

5901	"we note <e1>that</e1> fragkiadaki et al (2015) developed an lstm architecture <e2>that</e2> can straightforwardly be adapted to our method."
sameAs(e1, e2)
Comment:

5902	"in particular, we propose an <e1>image</e1> generator that learns a shared embedding between <e2>image</e2> and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5903	"in particular, we propose an <e1>image</e1> generator that learns a shared embedding between image and high-level structure information which allows us convert an input <e2>image</e2> into a future image guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5904	"in particular, we propose an <e1>image</e1> generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future <e2>image</e2> guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5905	"in particular, we propose an <e1>image</e1> generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input <e2>image</e2> and the future image."
sameAs(e1, e2)
Comment:

5906	"in particular, we propose an <e1>image</e1> generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input image and the future <e2>image</e2>."
sameAs(e1, e2)
Comment:

5907	"in particular, we propose an image generator that learns a shared embedding between <e1>image</e1> and high-level structure information which allows us convert an input <e2>image</e2> into a future image guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5908	"in particular, we propose an image generator that learns a shared embedding between <e1>image</e1> and high-level structure information which allows us convert an input image into a future <e2>image</e2> guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5909	"in particular, we propose an image generator that learns a shared embedding between <e1>image</e1> and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input <e2>image</e2> and the future image."
sameAs(e1, e2)
Comment:

5910	"in particular, we propose an image generator that learns a shared embedding between <e1>image</e1> and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input image and the future <e2>image</e2>."
sameAs(e1, e2)
Comment:

5911	"in particular, we propose an image generator that learns a shared embedding between image and high-level <e1>structure</e1> information which allows us convert an input image into a future image guided by the <e2>structure</e2> difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5912	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an <e1>input</e1> image into a future image guided by the structure difference between the <e2>input</e2> image and the future image."
sameAs(e1, e2)
Comment:

5913	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input <e1>image</e1> into a future <e2>image</e2> guided by the structure difference between the input image and the future image."
sameAs(e1, e2)
Comment:

5914	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input <e1>image</e1> into a future image guided by the structure difference between the input <e2>image</e2> and the future image."
sameAs(e1, e2)
Comment:

5915	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input <e1>image</e1> into a future image guided by the structure difference between the input image and the future <e2>image</e2>."
sameAs(e1, e2)
Comment:

5916	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future <e1>image</e1> guided by the structure difference between the input <e2>image</e2> and the future image."
sameAs(e1, e2)
Comment:

5917	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future <e1>image</e1> guided by the structure difference between the input image and the future <e2>image</e2>."
sameAs(e1, e2)
Comment:

5918	"in particular, we propose an image generator that learns a shared embedding between image and high-level structure information which allows us convert an input image into a future image guided by the structure difference between the input <e1>image</e1> and the future <e2>image</e2>."
sameAs(e1, e2)
Comment:

5919	"thus, our lstm network models the dynamics of human poses while our analogy-based <e1>image</e1> generator network learns a joint <e2>image</e2>-pose embedding that allows the pose difference between an observed frame and a predicted frame to be transferred to image domain for future frame generation."
sameAs(e1, e2)
Comment:

5920	"thus, our lstm network models the dynamics of human poses while our analogy-based <e1>image</e1> generator network learns a joint image-pose embedding that allows the pose difference between an observed frame and a predicted frame to be transferred to <e2>image</e2> domain for future frame generation."
sameAs(e1, e2)
Comment:

5921	"thus, our lstm network models the dynamics of human poses while our analogy-based image generator network learns a joint <e1>image</e1>-pose embedding that allows the pose difference between an observed frame and a predicted frame to be transferred to <e2>image</e2> domain for future frame generation."
sameAs(e1, e2)
Comment:

5922	"thus, our lstm network models the dynamics of human poses while our analogy-based image generator network learns a joint image-<e1>pose</e1> embedding that allows the <e2>pose</e2> difference between an observed frame and a predicted frame to be transferred to image domain for future frame generation."
sameAs(e1, e2)
Comment:

5923	"as a result, this pose-conditioned <e1>generation</e1> strategy prevents our network from propagating prediction errors through time, which in turn leads to very high quality future frame <e2>generation</e2> for long periods of time."
sameAs(e1, e2)
Comment:

5924	"as a result, this pose-conditioned generation strategy prevents our network from propagating prediction errors through <e1>time</e1>, which in turn leads to very high quality future frame generation for long periods of <e2>time</e2>."
sameAs(e1, e2)
Comment:

5925	"in this paper, we present a generative model based on a deep recurrent architecture <e1>that</e1> combines recent advances in computer vision and machine translation and <e2>that</e2> can be used to generate natural sentences describing an image."
sameAs(e1, e2)
Comment:

5926	"for instance, while the current <e1>state</e1>-of-the-art bleu-1 score (the higher the better) on the pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. we also show bleu-1 score improvements on flickr30k, from 56 to 66, and on sbu, from 19 to 28. lastly, on the newly released coco dataset, we achieve a bleu-4 of 27.7, which is the current <e2>state</e2>-of-the-art."
sameAs(e1, e2)
Comment:

5927	"for instance, while the current state-of-the-art <e1>bleu</e1>-1 score (the higher the better) on the pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. we also show <e2>bleu</e2>-1 score improvements on flickr30k, from 56 to 66, and on sbu, from 19 to 28. lastly, on the newly released coco dataset, we achieve a bleu-4 of 27.7, which is the current state-of-the-art."
sameAs(e1, e2)
Comment:

5928	"for instance, while the current state-of-the-art <e1>bleu</e1>-1 score (the higher the better) on the pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. we also show bleu-1 score improvements on flickr30k, from 56 to 66, and on sbu, from 19 to 28. lastly, on the newly released coco dataset, we achieve a <e2>bleu</e2>-4 of 27.7, which is the current state-of-the-art."
sameAs(e1, e2)
Comment:

5929	"for instance, while the current state-of-the-art bleu-1 score (the higher the better) on the pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. <e1>we</e1> also show bleu-1 score improvements on flickr30k, from 56 to 66, and on sbu, from 19 to 28. lastly, on the newly released coco dataset, <e2>we</e2> achieve a bleu-4 of 27.7, which is the current state-of-the-art."
sameAs(e1, e2)
Comment:

5930	"for instance, while the current state-of-the-art bleu-1 score (the higher the better) on the pascal dataset is 25, our approach yields 59, to be compared to human performance around 69. we also show <e1>bleu</e1>-1 score improvements on flickr30k, from 56 to 66, and on sbu, from 19 to 28. lastly, on the newly released coco dataset, we achieve a <e2>bleu</e2>-4 of 27.7, which is the current state-of-the-art."
sameAs(e1, e2)
Comment:

5931	"indeed, a description must capture not only the <e1>objects</e1> contained in an image, but it also must express how these <e2>objects</e2> relate to each other as well as their attributes and the activities they are involved in."
sameAs(e1, e2)
Comment:

5932	"further, szegedy et al [20] demonstrated that adversarial samples are transferable across multiple <e1>models</e1>, i.e., samples crafted to mislead one model often fool other <e2>models</e2> also."
sameAs(e1, e2)
Comment:

5933	"further, szegedy et al [20] demonstrated that adversarial samples are transferable across multiple models, i.e., samples crafted to mislead <e1>one model</e1> often fool <e2>other</e2> models also."
Compare(e1, e2)
Comment:

5934	"augmenting the <e1>training</e1> data with adversarial samples, known as adversarial <e2>training</e2> (at) [4, 20] has been introduced as a simple defense mechanism against these attacks."
sameAs(e1, e2)
Comment:

5935	"augmenting the training data with <e1>adversarial</e1> samples, known as <e2>adversarial</e2> training (at) [4, 20] has been introduced as a simple defense mechanism against these attacks."
sameAs(e1, e2)
Comment:

5936	"in the <e1>adversarial</e1> training regime, models are trained with mini-batches comprising of both clean and <e2>adversarial</e2> samples, typically obtained from the same model."
sameAs(e1, e2)
Comment:

5937	"it is shown by madry et al [13] that adversarial <e1>training</e1> helps to learn models robust to white-box attacks, provided the perturbations computed during the <e2>training</e2> closely maximize the model's loss."
sameAs(e1, e2)
Comment:

5938	"they revealed that the model's decision surface exhibits sharp curvature near the <e1>data</e1> points which leads to overfitting in adversarially trained <e2>models</e2>."
Used-for(e1, e2)
Comment:

5939	"thus, (i) adversarially trained <e1>models</e1> using single-step attacks remain susceptible to simple attacks, and (ii) perturbations crafted on undefended <e2>models</e2> transfer and form black-box attacks."
sameAs(e1, e2)
Comment:

5940	"their <e1>training</e1> mechanism, called ensemble adversarial <e2>training</e2> (eat), incorporates perturbations from multiple (e.g., n different) pre-trained models."
sameAs(e1, e2)
Comment:

5941	"since they augment the white-box perturbations (from the <e1>model</e1> being learned) with black-box perturbations from an ensemble of different pre-trained models, it is required to train <e2>those</e2> models before we start learning a robust model."
Compare(e1, e2)
Comment:

5942	"since they augment the white-box perturbations (from the <e1>model</e1> being learned) with black-box perturbations from an ensemble of different pre-trained models, it is required to train those models before we start learning a robust <e2>model</e2>."
sameAs(e1, e2)
Comment:

5943	"since they augment the white-box perturbations (from the model being learned) with black-box perturbations from an ensemble of different pre-trained <e1>models</e1>, it is required to train those <e2>models</e2> before we start learning a robust model."
sameAs(e1, e2)
Comment:

5944	"a <e1>training</e1> mechanism, called adversarial <e2>training</e2>, which presents adversarial samples along with clean samples has been introduced to learn robust models."
sameAs(e1, e2)
Comment:

5945	"a training mechanism, called <e1>adversarial</e1> training, which presents <e2>adversarial</e2> samples along with clean samples has been introduced to learn robust models."
sameAs(e1, e2)
Comment:

5946	"for <e1>that</e1>, we present variants of the white-box and black-box attacks, termed "gray-box adversarial attacks" <e2>that</e2> can be launched by temporally evolving intermediate models."
sameAs(e1, e2)
Comment:

5947	"given the efficiency to generate and the ability to examine the <e1>robustness</e1>, we strongly recommend the community to consider <e2>robustness</e2> plots and "worst-case performance" as standard bench-marking for evaluating the models."
sameAs(e1, e2)
Comment:

5948	"for existing evaluation, best model's robustness against <e1>adversarial</e1> attack is tested by obtaining it's prediction on <e2>adversarial</e2> samples generated by itself."
sameAs(e1, e2)
Comment:

5949	"-harnessing the above observations, we propose a novel variant of <e1>adversarial</e1> training, termed "gray-box <e2>adversarial</e2> training" that uses our gray-box perturbations in order to learn robust models."
sameAs(e1, e2)
Comment:

5950	"-harnessing the above observations, we propose a novel variant of adversarial <e1>training</e1>, termed "gray-box adversarial <e2>training</e2>" that uses our gray-box perturbations in order to learn robust models."
sameAs(e1, e2)
Comment:

5951	"in this paper <e1>we</e1>, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box adversarial attacks" based on which <e2>we</e2> propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named "graybox adversarial training" that uses intermediate versions of the models to seed the adversaries."
sameAs(e1, e2)
Comment:

5952	"in this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box <e1>adversarial</e1> attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of <e2>adversarial</e2> training, named "graybox adversarial training" that uses intermediate versions of the models to seed the adversaries."
sameAs(e1, e2)
Comment:

5953	"in this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box <e1>adversarial</e1> attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial training, named "graybox <e2>adversarial</e2> training" that uses intermediate versions of the models to seed the adversaries."
sameAs(e1, e2)
Comment:

5954	"in this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned <e1>models</e1>, and (iii) propose a novel variant of adversarial training, named "graybox adversarial training" that uses intermediate versions of the <e2>models</e2> to seed the adversaries."
sameAs(e1, e2)
Comment:

5955	"in this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of <e1>adversarial</e1> training, named "graybox <e2>adversarial</e2> training" that uses intermediate versions of the models to seed the adversaries."
sameAs(e1, e2)
Comment:

5956	"in this paper we, (i) demonstrate the shortcomings of existing evaluation policy, (ii) introduce novel variants of white-box and black-box attacks, dubbed "gray-box adversarial attacks" based on which we propose novel evaluation method to assess the robustness of the learned models, and (iii) propose a novel variant of adversarial <e1>training</e1>, named "graybox adversarial <e2>training</e2>" that uses intermediate versions of the models to seed the adversaries."
sameAs(e1, e2)
Comment:

5957	"experimental evaluation demonstrates that the <e1>models</e1> trained using our method exhibit better robustness compared to both undefended and adversarially trained <e2>models</e2>."
sameAs(e1, e2)
Comment:

5958	"abstract recent works showed that <e1>generative adversarial networks</e1> (<e2>gans</e2>) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled target dataset, the goal is to train powerful classifiers for the target samples."
sameAs(e1, e2)
Comment:

5959	"abstract recent works showed that generative adversarial networks (gans) can be successfully applied in unsupervised domain adaptation, where, given a labeled source dataset and an unlabeled <e1>target</e1> dataset, the goal is to train powerful classifiers for the <e2>target</e2> samples."
sameAs(e1, e2)
Comment:

5960	"in this framework, adversarial training has been used (i) to learn feature extractors <e1>that</e1> map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop image-to-image translation algorithms [32, 18, 1, 17] aimed at converting source images in a style <e2>that</e2> resembles that of the target image domain."
sameAs(e1, e2)
Comment:

5961	"in this framework, adversarial training has been used (i) to learn feature extractors <e1>that</e1> map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop image-to-image translation algorithms [32, 18, 1, 17] aimed at converting source images in a style that resembles <e2>that</e2> of the target image domain."
sameAs(e1, e2)
Comment:

5962	"in this framework, adversarial training has been used (i) to learn feature extractors that map <e1>target</e1> samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop image-to-image translation algorithms [32, 18, 1, 17] aimed at converting source images in a style that resembles that of the <e2>target</e2> image domain."
sameAs(e1, e2)
Comment:

5963	"in this framework, adversarial training has been used (i) to learn feature extractors that map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop <e1>image</e1>-to-<e2>image</e2> translation algorithms [32, 18, 1, 17] aimed at converting source images in a style that resembles that of the target image domain."
sameAs(e1, e2)
Comment:

5964	"in this framework, adversarial training has been used (i) to learn feature extractors that map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop <e1>image</e1>-to-image translation algorithms [32, 18, 1, 17] aimed at converting source images in a style that resembles that of the target <e2>image</e2> domain."
sameAs(e1, e2)
Comment:

5965	"in this framework, adversarial training has been used (i) to learn feature extractors that map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop image-to-<e1>image</e1> translation algorithms [32, 18, 1, 17] aimed at converting source images in a style that resembles that of the target <e2>image</e2> domain."
sameAs(e1, e2)
Comment:

5966	"in this framework, adversarial training has been used (i) to learn feature extractors that map target samples in a feature space indistinguishable from the one where source samples are mapped [8, 34] , and (ii) to develop image-to-image translation algorithms [32, 18, 1, 17] aimed at converting source images in a style <e1>that</e1> resembles <e2>that</e2> of the target image domain."
sameAs(e1, e2)
Comment:

5967	"in this paper, we build on the work by tzeng et al [34] , which proposes to use a gan objective to learn <e1>target</e1> features that are indistinguishable from the source ones, leading to a pair of feature extractors, one for the source and one for the <e2>target</e2> samples."
sameAs(e1, e2)
Comment:

5968	"in this paper, we build on the work by tzeng et al [34] , which proposes to use a gan objective to learn target features that are indistinguishable from the source ones, leading to a pair of feature extractors, <e1>one</e1> for the source and <e2>one</e2> for the target samples."
sameAs(e1, e2)
Comment:

5969	"we extend this approach in two directions: (a) <e1>we</e1> force domain-invariance in a single feature extractor trained through gans, and (b) <e2>we</e2> perform data augmentation in the feature space (i.e., feature augmentation), by defining a more complex minimax game."
sameAs(e1, e2)
Comment:

5970	"we extend this approach in two directions: (a) we force domain-invariance in a single <e1>feature</e1> extractor trained through gans, and (b) we perform data augmentation in the feature space (i.e., <e2>feature</e2> augmentation), by defining a more complex minimax game."
sameAs(e1, e2)
Comment:

5971	"more specifically, we perform <e1>feature</e1> augmentation by devising a <e2>feature</e2> generator trained with a conditional gan (cgan [21] )."
sameAs(e1, e2)
Comment:

5972	"the minimax game is here played with <e1>features</e1> instead of images, allowing to generate <e2>features</e2> conditioned to the desired classes."
sameAs(e1, e2)
Comment:

5973	"2. proposing a new method for unsupervised domain adaptation, based on <e1>feature</e1> augmentation and (source/target) <e2>feature</e2> domain-invariance."
sameAs(e1, e2)
Comment:

5974	"3. evaluating the proposed <e1>method</e1> on unsupervised domain adaptation benchmarks (cross-dataset digit classification and cross-modal object classification), obtaining results which are superior or comparable to current state-of-the-art in most of the addressed <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

5975	"3. evaluating the proposed method on unsupervised domain adaptation benchmarks (cross-dataset digit <e1>classification</e1> and cross-modal object <e2>classification</e2>), obtaining results which are superior or comparable to current state-of-the-art in most of the addressed tasks."
sameAs(e1, e2)
Comment:

5976	"in <e1>this</e1> work, we extend <e2>this</e2> framework by (i) forcing the learned feature extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing feature augmentation."
sameAs(e1, e2)
Comment:

5977	"in this work, we extend this framework by (i) forcing the learned <e1>feature</e1> extractor to be domain-invariant, and (ii) training it through data augmentation in the feature space, namely performing <e2>feature</e2> augmentation."
sameAs(e1, e2)
Comment:

5978	"introduction <e1>generative adversarial networks</e1> (<e2>gans</e2> [10] ) are models capable of mapping noise vectors into realistic samples from a data distribution."
sameAs(e1, e2)
Comment:

5979	"gans are defined by two neural networks, a generator and a discriminator, and the <e1>training</e1> procedure is a minimax game where the generator is optimized to fool the discriminator, and the discriminator is optimized to correctly classify generated samples from actual <e2>training</e2> samples."
sameAs(e1, e2)
Comment:

5980	"recently, this framework proved to be able to generate <e1>images</e1> with impressive accuracy [24] , to generate videos from static frames [37] , and to translate <e2>images</e2> from one style to another [32, 18, 1, 17] ."
sameAs(e1, e2)
Comment:

5981	"we adopt <e1>these</e1> five levels of granularity from prior work and examine the performance of geolocalization strategies at <e2>these</e2> error thresholds."
sameAs(e1, e2)
Comment:

5982	"one natural approach to the <e1>image</e1> geolocalization task would be to to treat it like an instance retrieval task and match local features from the query <e2>image</e2> (and perhaps their geometric layout) to a reference database of images with known locations [16] ."
sameAs(e1, e2)
Comment:

5983	"one natural approach to the image geolocalization <e1>task</e1> would be to to treat it like an instance retrieval <e2>task</e2> and match local features from the query image (and perhaps their geometric layout) to a reference database of images with known locations [16] ."
sameAs(e1, e2)
Comment:

5984	"such approaches work well if (1) there are images in the reference database with a field of view <e1>that</e1> significantly overlaps with <e2>that</e2> of the query image and (2) if the content of the query image is well suited to local feature matching (i.e., it has distinctive manmade or geological features)."
sameAs(e1, e2)
Comment:

5985	"such approaches work well if (1) there are images in the reference database with a field of view that significantly overlaps with that of the query <e1>image</e1> and (2) if the content of the query <e2>image</e2> is well suited to local feature matching (i.e., it has distinctive manmade or geological features)."
sameAs(e1, e2)
Comment:

5986	"in these cases, <e1>image</e1> geolocalization is similar to scene classification or scene attribute estimation in that a system needs to achieve a higher-level, more qualitative understanding of an <e2>image</e2>, e.g."
sameAs(e1, e2)
Comment:

5987	"one of the most important attributes of an image is geolocation -if <e1>we</e1> know the location of a photo, <e2>we</e2> trivially know hundreds of additional attributes (any attribute for which a map exists, e.g."
sameAs(e1, e2)
Comment:

5988	"interestingly, while <e1>we</e1> approach geolocalization as a retrieval task with learned deep features, <e2>we</e2> don't see a benefit to using embedding formulations (e.g."
sameAs(e1, e2)
Comment:

5989	"we achieve this with as little as 5% of the training <e1>data</e1> used by planet, and increase the gap further while using 28% as much reference <e2>data</e2>."
sameAs(e1, e2)
Comment:

5990	"surprisingly, deep feature learning methods typically used for <e1>retrieval</e1> applications do not outperform training with a <e2>classification</e2> loss."
Conjunction(e1, e2)
Comment:

5991	"• through extensive experimentation, we find that some training procedures lead to higher accuracy at the street <e1>scale</e1> (1km) and others at the country <e2>scale</e2> (750km)."
sameAs(e1, e2)
Comment:

5992	"we observe a trade off between fine-<e1>scale</e1> and coarse-<e2>scale</e2> performance, the regimes traditionally approached with instance-level matching methods and scene classification methods, respectively."
sameAs(e1, e2)
Comment:

5993	"to alleviate the <e1>gradient</e1> propagation difficulties in deep predictive models, we propose a <e2>gradient</e2> highway unit, which provides alternative quick routes for the gradient flows from outputs back to long-range previous inputs."
sameAs(e1, e2)
Comment:

5994	"to alleviate the <e1>gradient</e1> propagation difficulties in deep predictive models, we propose a gradient highway unit, which provides alternative quick routes for the <e2>gradient</e2> flows from outputs back to long-range previous inputs."
sameAs(e1, e2)
Comment:

5995	"to alleviate the gradient propagation difficulties in deep predictive models, we propose a <e1>gradient</e1> highway unit, which provides alternative quick routes for the <e2>gradient</e2> flows from outputs back to long-range previous inputs."
sameAs(e1, e2)
Comment:

5996	" introduction<e1> fac</e1>e aging, also known as aging synthesis of the human<e2> fac</e2>e, is a task of synthesizing faces of a certain person under a given age."
sameAs(e1, e2)
Comment:

5997	" introduction<e1> fac</e1>e aging, also known as aging synthesis of the human face, is a task of synthesizing<e2> face</e2>s of a certain person under a given age."
sameAs(e1, e2)
Comment:

5998	" introduction face aging, also known as aging synthesis of the human<e1> fac</e1>e, is a task of synthesizing<e2> face</e2>s of a certain person under a given age."
sameAs(e1, e2)
Comment:

5999	"recently, <e1>generative adversarial networks</e1>(<e2>gans</e2>) based approaches have been demonstrated their successes in generating high quality images [7] [14] [18] ."
sameAs(e1, e2)
Comment:

6000	"specifically, our ipcgans consists of three modules: a cgans <e1>module</e1>, an identity-preserved <e2>module</e2> and an age classifier."
sameAs(e1, e2)
Comment:

6001	"the generator of cgans takes an <e1>input</e1> image and a target age code as its <e2>input</e2> and generates a face with the target age."
sameAs(e1, e2)
Comment:

6002	"the generator of cgans takes an input image and a <e1>target</e1> age code as its input and generates a face with the <e2>target</e2> age."
sameAs(e1, e2)
Comment:

6003	"the generated <e1>face</e1> is expected to be indistinguishable from real <e2>faces</e2> in that target age group by the discriminator."
sameAs(e1, e2)
Comment:

6004	"finally, to guarantee the synthesized <e1>faces</e1> fall into the target age group, we send the generated aged <e2>faces</e2> to a pre-trained age classifier and add an age classification loss to the objective."
sameAs(e1, e2)
Comment:

6005	"the former lets the aged <e1>faces</e1> keep the same identity with the input <e2>face</e2>."
sameAs(e1, e2)
Comment:

6006	"2. other than quantitatively evaluate the quality of the synthesized <e1>faces</e1>, we also propose to conduct <e2>face</e2> verification and face age classification for the generated aged faces by means of user study."
sameAs(e1, e2)
Comment:

6007	"2. other than quantitatively evaluate the quality of the synthesized <e1>faces</e1>, we also propose to conduct face verification and <e2>face</e2> age classification for the generated aged faces by means of user study."
sameAs(e1, e2)
Comment:

6008	"2. other than quantitatively evaluate the quality of the synthesized <e1>faces</e1>, we also propose to conduct face verification and face age classification for the generated aged <e2>faces</e2> by means of user study."
sameAs(e1, e2)
Comment:

6009	"2. other than quantitatively evaluate the quality of the synthesized faces, we also propose to conduct <e1>face</e1> verification and <e2>face</e2> age classification for the generated aged faces by means of user study."
sameAs(e1, e2)
Comment:

6010	"2. other than quantitatively evaluate the quality of the synthesized faces, we also propose to conduct <e1>face</e1> verification and face age classification for the generated aged <e2>faces</e2> by means of user study."
sameAs(e1, e2)
Comment:

6011	"2. other than quantitatively evaluate the quality of the synthesized faces, we also propose to conduct face verification and <e1>face</e1> age classification for the generated aged <e2>faces</e2> by means of user study."
sameAs(e1, e2)
Comment:

6012	"without any modification, ipcgans can be applied to multi-attribute generation <e1>task</e1>, like hair colors, facial expressions, etc, which can be used for imbalanced <e2>data</e2> classification scenes."
Conjunction(e1, e2)
Comment:

6013	"prototype-based approaches usually compute an average <e1>face</e1> within a <e2>face</e2> age group first, and the difference between different average faces from different groups would be treated as aging pattern which would be used for synthesizing an aged face [11] ."
sameAs(e1, e2)
Comment:

6014	"prototype-based approaches usually compute an average <e1>face</e1> within a face age group first, and the difference between different average <e2>faces</e2> from different groups would be treated as aging pattern which would be used for synthesizing an aged face [11] ."
sameAs(e1, e2)
Comment:

6015	"prototype-based approaches usually compute an average <e1>face</e1> within a face age group first, and the difference between different average faces from different groups would be treated as aging pattern which would be used for synthesizing an aged <e2>face</e2> [11] ."
sameAs(e1, e2)
Comment:

6016	"prototype-based approaches usually compute an average face within a <e1>face</e1> age group first, and the difference between different average <e2>faces</e2> from different groups would be treated as aging pattern which would be used for synthesizing an aged face [11] ."
sameAs(e1, e2)
Comment:

6017	"prototype-based approaches usually compute an average face within a <e1>face</e1> age group first, and the difference between different average faces from different groups would be treated as aging pattern which would be used for synthesizing an aged <e2>face</e2> [11] ."
sameAs(e1, e2)
Comment:

6018	"prototype-based approaches usually compute an average face within a face age group first, and the difference between different average <e1>faces</e1> from different groups would be treated as aging pattern which would be used for synthesizing an aged <e2>face</e2> [11] ."
sameAs(e1, e2)
Comment:

6019	"changing the <e1>color</e1> of a car or the <e2>texture</e2> of a road."
Conjunction(e1, e2)
Comment:

6020	"these methods achieve better performance compared to two types of <e1>baselines</e1>: (i) they outperform their counterparts with hand-crafted features (e.g., sift) by a huge margin, which means that lowlevel cnn features are far more effective than previous hand-crafted ones; (ii) they significantly outperform their <e2>baselines</e2> which finetune the same cnn used for feature extraction."
sameAs(e1, e2)
Comment:

6021	"these methods achieve better performance compared to two types of baselines: (i) <e1>they</e1> outperform their counterparts with hand-crafted features (e.g., sift) by a huge margin, which means that lowlevel cnn features are far more effective than previous hand-crafted ones; (ii) <e2>they</e2> significantly outperform their baselines which finetune the same cnn used for feature extraction."
sameAs(e1, e2)
Comment:

6022	"these methods achieve better performance compared to two types of baselines: (i) they outperform their counterparts with hand-crafted <e1>features</e1> (e.g., <e2>sift</e2>) by a huge margin, which means that lowlevel cnn features are far more effective than previous hand-crafted ones; (ii) they significantly outperform their baselines which finetune the same cnn used for feature extraction."
Compare(e1, e2)
Comment:

6023	"these methods achieve better performance compared to two types of baselines: (i) they outperform their counterparts with hand-crafted <e1>features</e1> (e.g., sift) by a huge margin, which means that lowlevel cnn <e2>features</e2> are far more effective than previous hand-crafted ones; (ii) they significantly outperform their baselines which finetune the same cnn used for feature extraction."
sameAs(e1, e2)
Comment:

6024	"these methods achieve better performance compared to two types of baselines: (i) they outperform their counterparts with hand-crafted features (e.g., sift) by a huge margin, which means that lowlevel <e1>cnn</e1> features are far more effective than previous hand-crafted ones; (ii) they significantly outperform their baselines which finetune the same <e2>cnn</e2> used for feature extraction."
sameAs(e1, e2)
Comment:

6025	"the first category, localization-<e1>classification</e1> subnetworks, consists of a <e2>classification</e2> network assisted by a localization network."
sameAs(e1, e2)
Comment:

6026	"the first category, localization-classification subnetworks, consists of a classification <e1>network</e1> assisted by a localization <e2>network</e2>."
sameAs(e1, e2)
Comment:

6027	"previous approaches achieve this by introducing an auxiliary <e1>network</e1> to infuse localization information into the main classification <e2>network</e2>, or a sophisticated feature encoding method to capture higher order feature statistics."
sameAs(e1, e2)
Comment:

6028	"previous approaches achieve this by introducing an auxiliary network to infuse localization information into the main classification network, or a sophisticated <e1>feature</e1> encoding method to capture higher order <e2>feature</e2> statistics."
sameAs(e1, e2)
Comment:

6029	"head and body of birds) to be shared across object <e1>classes</e1>, encouraging the representations of the parts to be similar; but, in order to be discriminative, the latter encourages the part representations to be different across <e2>classes</e2>."
sameAs(e1, e2)
Comment:

6030	"head and body of birds) to be shared across object classes, encouraging the <e1>representations</e1> of the parts to be similar; but, in order to be discriminative, the latter encourages the part <e2>representations</e2> to be different across classes."
sameAs(e1, e2)
Comment:

6031	"such a trade-off is also reflected in practice, in that <e1>training</e1> usually involves alternating optimization of the two networks or separately <e2>training</e2> the two followed by joint tuning."
sameAs(e1, e2)
Comment:

6032	"such a trade-off is also reflected in practice, in that training usually involves alternating optimization of the <e1>two</e1> networks or separately training the <e2>two</e2> followed by joint tuning."
sameAs(e1, e2)
Comment:

6033	"the second category, end-to-end <e1>feature</e1> encoding [30, 10, 23, 8, 5 ], enhances cnn mid-level learning by encoding higher order statistics of convolutional <e2>feature</e2> maps."
sameAs(e1, e2)
Comment:

6034	"a discriminative patch can be discovered by convolving the feature <e1>map</e1> with the 1 × 1 filter and performing global max pooling (gmp) over the response <e2>map</e2>."
sameAs(e1, e2)
Comment:

6035	"we show <e1>that</e1> mid-level representation learning can be enhanced within the cnn framework, by learning a bank of convolutional filters <e2>that</e2> capture class-specific discriminative patches without extra part or bounding box annotations."
sameAs(e1, e2)
Comment:

6036	"while effective, end-to-end encoding <e1>networks</e1> are less human-interpretable and less consistent in their performance across non-rigid and rigid visual domains, compared to localization-classification sub-<e2>networks</e2>."
sameAs(e1, e2)
Comment:

6037	"conceptually, our discriminative patches differ from the parts in localizationrecognition sub-networks, such that <e1>they</e1> are not necessarily shared across classes as long as <e2>they</e2> have discriminative appearance."
sameAs(e1, e2)
Comment:

6038	"early applications of deep learning to this task built traditional multistage frameworks upon convolutional neural network (cnn) features; more recent cnn-based approaches are usually trained <e1>end-to-end</e1> and can be roughly divided into two categories: localization-classification subnetworks and <e2>end-to-end</e2> feature encoding."
sameAs(e1, e2)
Comment:

6039	"recently, wang et al [26] proposed to jointly characterize the semantic label dependency and the image-label relevance by combining <e1>recurrent neural networks</e1> (<e2>rnns</e2>) with cnns."
sameAs(e1, e2)
Comment:

6040	"in contrast to all <e1>these</e1> mentioned methods, we introduce an end-to-end trainable framework that explicitly discovers attentional regions over image scales corresponding to multiple semantic labels and captures the contextual dependencies of <e2>these</e2> regions from a global perspective."
sameAs(e1, e2)
Comment:

6041	"in contrast to all these mentioned methods, we introduce an end-to-end trainable framework that explicitly discovers attentional <e1>regions</e1> over image scales corresponding to multiple semantic labels and captures the contextual dependencies of these <e2>regions</e2> from a global perspective."
sameAs(e1, e2)
Comment:

6042	"specifically, this module consists of two components: i) a spatial transformer layer to locate attentional <e1>regions</e1> on the convolutional maps and ii) an lstm (long-short term memory) sub-network to sequentially predict the labeling scores over the attentional <e2>regions</e2> and output the parameters of the spatial transformer layer."
sameAs(e1, e2)
Comment:

6043	"besides the challenges shared with single-label <e1>image classification</e1> (e.g., large intra-class variation caused by viewpoint, scale, occlusion, illumination), multi-label <e2>image classification</e2> is much more difficult since accurately predicting the presence of multiple object categories usu- figure 1 ."
sameAs(e1, e2)
Comment:

6044	"• we develop a proposal-free pipeline for multi-label <e1>image</e1> recognition, which is capable of automatically discovering semantic-aware regions over <e2>image</e2> scales and simultaneously capturing their long-range contextual dependencies."
sameAs(e1, e2)
Comment:

6045	"recently, convolutional neural networks (cnns) [18, 24] achieve great success in visual recognition/<e1>classification tasks</e1> by learning powerful feature representations from raw images, and they have been also applied to the problem of multi-label <e2>image classification</e2> by combining with some object localization techniques [30, 27] ."
Used-for(e1, e2)
Comment:

6046	"a batch of hypothesis <e1>regions</e1> are first produced by either exploiting bottom-up image cues [25] or casting extra detectors [4] , and these <e2>regions</e2> are assumed to contain all possible foreground objects in the image."
sameAs(e1, e2)
Comment:

6047	"a batch of hypothesis regions are first produced by either exploiting bottom-up <e1>image</e1> cues [25] or casting extra detectors [4] , and these regions are assumed to contain all possible foreground objects in the <e2>image</e2>."
sameAs(e1, e2)
Comment:

6048	"a <e1>classifier</e1> or neural network is then trained to predict the label score on these hypothesis regions, and these predictions are aggregated to achieve the multi-label <e2>classification results</e2>."
Compare(e1, e2)
Comment:

6049	"a classifier or neural network is then trained to predict the label score on <e1>these</e1> hypothesis regions, and <e2>these</e2> predictions are aggregated to achieve the multi-label classification results."
sameAs(e1, e2)
Comment:

6050	"our contribution is fourfold: (1) <e1>we</e1> introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, <e2>we</e2> show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6051	"our contribution is fourfold: (1) we introduce a <e1>hierarchical</e1> network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our <e2>hierarchical</e2> training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6052	"our contribution is fourfold: (1) we introduce a <e1>hierarchical</e1> network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our <e2>hierarchical</e2> style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6053	"our contribution is fourfold: (1) we introduce a hierarchical <e1>network</e1> and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn <e2>network</e2> architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6054	"our contribution is fourfold: (1) we introduce a hierarchical <e1>network</e1> and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one <e2>network</e2> to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6055	"our contribution is fourfold: (1) we introduce a hierarchical <e1>network</e1> and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our <e2>network</e2> utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6056	"our contribution is fourfold: (1) we introduce a hierarchical <e1>network</e1> and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer <e2>network</e2> can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6057	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated <e1>training</e1> scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical <e2>training</e2> scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6058	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training <e1>scheme</e1> that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training <e2>scheme</e2> and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6059	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme <e1>that</e1> is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show <e2>that</e2> our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6060	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-<e1>scale</e1> texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple <e2>scales</e2> of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6061	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale <e1>texture</e1> distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate <e2>texture</e2> patterns."
sameAs(e1, e2)
Comment:

6062	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our <e1>hierarchical</e1> training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our <e2>hierarchical</e2> style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6063	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn <e1>network</e1> architecture allow us to combine multiple models into one <e2>network</e2> to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6064	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn <e1>network</e1> architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our <e2>network</e2> utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6065	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn <e1>network</e1> architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer <e2>network</e2> can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6066	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one <e1>network</e1> to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our <e2>network</e2> utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6067	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one <e1>network</e1> to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer <e2>network</e2> can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6068	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb <e1>color</e1> channels into consideration, our network utilizes representations of both <e2>color</e2> and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6069	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb <e1>color</e1> channels into consideration, our network utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate <e2>texture</e2> patterns."
Conjunction(e1, e2)
Comment:

6070	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our <e1>network</e1> utilizes representations of both color and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer <e2>network</e2> can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6071	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both <e1>color</e1> and luminance channels for style transfer; and (4) through experimentation, we show that our hierarchical style transfer network can better capture both coarse and intricate <e2>texture</e2> patterns."
Conjunction(e1, e2)
Comment:

6072	"our contribution is fourfold: (1) we introduce a hierarchical network and design an associated training scheme that is able to learn both coarse, large-scale texture distortion and fine, exquisite brushwork of an artistic style by utilizing multiple scales of a style image; (2) our hierarchical training scheme and end-to-end cnn network architecture allow us to combine multiple models into one network to handle increasingly larger image sizes; (3) instead of taking only rgb color channels into consideration, our network utilizes representations of both color and luminance channels for style <e1>transfer</e1>; and (4) through experimentation, we show that our hierarchical style <e2>transfer</e2> network can better capture both coarse and intricate texture patterns."
sameAs(e1, e2)
Comment:

6073	"our hierarchical style <e1>transfer</e1> network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal <e2>transfer</e2> from the feed-forward style transfer networks with only one stylization loss [13, 25] , which we call singular transfer."
sameAs(e1, e2)
Comment:

6074	"our hierarchical style <e1>transfer</e1> network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal transfer from the feed-forward style <e2>transfer</e2> networks with only one stylization loss [13, 25] , which we call singular transfer."
sameAs(e1, e2)
Comment:

6075	"our hierarchical style <e1>transfer</e1> network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal transfer from the feed-forward style transfer networks with only one stylization loss [13, 25] , which we call singular <e2>transfer</e2>."
sameAs(e1, e2)
Comment:

6076	"our hierarchical style transfer network is trained with multiple stylization losses at different scales using a mixture of modalities, so <e1>we</e1> distinguish it as multimodal transfer from the feed-forward style transfer networks with only one stylization loss [13, 25] , which <e2>we</e2> call singular transfer."
sameAs(e1, e2)
Comment:

6077	"our hierarchical style transfer network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal <e1>transfer</e1> from the feed-forward style <e2>transfer</e2> networks with only one stylization loss [13, 25] , which we call singular transfer."
sameAs(e1, e2)
Comment:

6078	"our hierarchical style transfer network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal <e1>transfer</e1> from the feed-forward style transfer networks with only one stylization loss [13, 25] , which we call singular <e2>transfer</e2>."
sameAs(e1, e2)
Comment:

6079	"our hierarchical style transfer network is trained with multiple stylization losses at different scales using a mixture of modalities, so we distinguish it as multimodal transfer from the feed-forward style <e1>transfer</e1> networks with only one stylization loss [13, 25] , which we call singular <e2>transfer</e2>."
sameAs(e1, e2)
Comment:

6080	"1 we give an example that compares results from our multimodal <e1>transfer</e1> network with those from the current state-of-the-art singular <e2>transfer</e2> networks."
sameAs(e1, e2)
Comment:

6081	"1 shows the advantages of multimodal transfer on learning different levels of <e1>textures</e1>, including style, color, large <e2>texture</e2> distortion and fine brushwork."
sameAs(e1, e2)
Comment:

6082	"1 shows the advantages of multimodal transfer on learning different levels of textures, including style, <e1>color</e1>, large <e2>texture</e2> distortion and fine brushwork."
Conjunction(e1, e2)
Comment:

6083	"though past work creates visually pleasing <e1>results</e1> for many different types of artworks, two important drawbacks stand out: (1) the current feed-forward networks [13, 25] are trained on a specific resolution of the style image, so deviating from that resolution (bigger or smaller) <e2>results</e2> in a scale mismatch."
sameAs(e1, e2)
Comment:

6084	"though past work creates visually pleasing results for many different types of artworks, two important drawbacks stand out: (1) the current feed-forward networks [13, 25] are trained on a specific <e1>resolution</e1> of the style image, so deviating from that <e2>resolution</e2> (bigger or smaller) results in a scale mismatch."
sameAs(e1, e2)
Comment:

6085	"for example, applying a model trained with a style guide of size 256 on higher-<e1>resolution</e1> images would generate results whose texture scale is smaller than that of the artistic style, and (2) current networks often fail to capture small, intricate textures, like brushwork, of many kinds of artworks on high-<e2>resolution</e2> images."
sameAs(e1, e2)
Comment:

6086	"for example, applying a model trained with a style guide of size 256 on higher-resolution <e1>images</e1> would generate results whose texture scale is smaller than that of the artistic style, and (2) current networks often fail to capture small, intricate textures, like brushwork, of many kinds of artworks on high-resolution <e2>images</e2>."
sameAs(e1, e2)
Comment:

6087	"for example, applying a model trained with a style guide of size 256 on higher-resolution images would generate results whose <e1>texture</e1> scale is smaller than that of the artistic style, and (2) current networks often fail to capture small, intricate <e2>textures</e2>, like brushwork, of many kinds of artworks on high-resolution images."
sameAs(e1, e2)
Comment:

6088	"in general, <e1>occlusion</e1> can be divided into two groups: inter-class <e2>occlusion</e2> and intra-class occlusion."
sameAs(e1, e2)
Comment:

6089	"in general, <e1>occlusion</e1> can be divided into two groups: inter-class occlusion and intra-class <e2>occlusion</e2>."
sameAs(e1, e2)
Comment:

6090	"in general, occlusion can be divided into two groups: inter-class <e1>occlusion</e1> and intra-class <e2>occlusion</e2>."
sameAs(e1, e2)
Comment:

6091	"the <e1>former</e1> one occurs when an object is occluded by stuff or objects of other categories, while the <e2>latter</e2> one, also referred to as crowd occlusion, occurs when an object is occluded by objects of the same category."
Conjunction(e1, e2)
Comment:

6092	"the former <e1>one</e1> occurs when an object is occluded by stuff or objects of <e2>other</e2> categories, while the latter one, also referred to as crowd occlusion, occurs when an object is occluded by objects of the same category."
Conjunction(e1, e2)
Comment:

6093	"the former <e1>one</e1> occurs when an object is occluded by stuff or objects of other categories, while the latter <e2>one</e2>, also referred to as crowd occlusion, occurs when an object is occluded by objects of the same category."
sameAs(e1, e2)
Comment:

6094	"the former one occurs when an object is occluded by stuff or <e1>objects</e1> of other categories, while the latter one, also referred to as crowd occlusion, occurs when an object is occluded by <e2>objects</e2> of the same category."
sameAs(e1, e2)
Comment:

6095	"in pedestrian detection [31, 14, 6, 5, 7, 21] , crowd <e1>occlusion</e1> constitutes the majority of <e2>occlusion</e2> cases."
sameAs(e1, e2)
Comment:

6096	"while saliency <e1>benchmarks</e1> (e.g., mit300 [32] and lsun [68] ) have been very instrumental in progressing the static saliency field, such standard widespread <e2>benchmarks</e2> are missing for video saliency modeling."
sameAs(e1, e2)
Comment:

6097	"results show that our <e1>model</e1> significantly outperforms previous <e2>methods</e2>."
Compare(e1, e2)
Comment:

6098	"this task, referred to as dynamic fixation prediction or <e1>video</e1> saliency detection is very useful for understanding human attentional behaviors and has several practical real-word applications (e.g., <e2>video</e2> captioning, compression, question answering, object segmentation, etc)."
sameAs(e1, e2)
Comment:

6099	"in summary, sgpn has three output branches for instance segmentation on point clouds: a similarity matrix yielding point-wise group <e1>proposals</e1>, a confidence map for pruning these <e2>proposals</e2>, and a semantic segmentation map to give the class label for each group."
sameAs(e1, e2)
Comment:

6100	"in summary, sgpn has three output branches for instance segmentation on point clouds: a similarity matrix yielding point-wise group proposals, a confidence <e1>map</e1> for pruning these proposals, and a semantic segmentation <e2>map</e2> to give the class label for each group."
sameAs(e1, e2)
Comment:

6101	"although a minimalistic framework with no bells and whistles already gives visually pleasing results (figure 1 ), <e1>we</e1> also demonstrate the flexibility of sgpn as <e2>we</e2> boost performance even more by seamlessly integrating cnn features from rgbd images."
sameAs(e1, e2)
Comment:

6102	"with the rise of autonomous driving and robotics applications, the demand for 3d <e1>scene</e1> understanding and the availability of 3d <e2>scene</e2> data has rapidly increased in recently."
sameAs(e1, e2)
Comment:

6103	"following the pioneering works in 2d <e1>scene</e1> understanding, our goal is to develop a novel deep learning framework trained end-to-end for 3d instance-aware semantic segmentation on point clouds that, like established baseline systems for 2d <e2>scene</e2> understanding tasks, is intuitive, simple, flexi- an important consideration for instance segmentation on a point cloud is how to represent output results."
sameAs(e1, e2)
Comment:

6104	"following the pioneering works in 2d scene <e1>understanding</e1>, our goal is to develop a novel deep learning framework trained end-to-end for 3d instance-aware semantic segmentation on point clouds that, like established baseline systems for 2d scene <e2>understanding</e2> tasks, is intuitive, simple, flexi- an important consideration for instance segmentation on a point cloud is how to represent output results."
sameAs(e1, e2)
Comment:

6105	"inspired by the trend of predicting <e1>proposals</e1> for tasks with a variable number of outputs, we introduce a similarity group <e2>proposal</e2> network (sgpn), which formulates group proposals of object instances by learning a novel 3d instance segmentation representation in the form of a similarity matrix ."
sameAs(e1, e2)
Comment:

6106	"inspired by the trend of predicting <e1>proposals</e1> for tasks with a variable number of outputs, we introduce a similarity group proposal network (sgpn), which formulates group <e2>proposals</e2> of object instances by learning a novel 3d instance segmentation representation in the form of a similarity matrix ."
sameAs(e1, e2)
Comment:

6107	"inspired by the trend of predicting proposals for tasks with a variable number of outputs, we introduce a similarity group <e1>proposal</e1> network (sgpn), which formulates group <e2>proposals</e2> of object instances by learning a novel 3d instance segmentation representation in the form of a similarity matrix ."
sameAs(e1, e2)
Comment:

6108	"as a form of similarity metric learning, <e1>we</e1> enforce the idea that points belonging to the same object instance should have very similar features; hence <e2>we</e2> measure the distance between the features of each pair of points in order to form a similarity matrix that indicates whether any given pair of points belong to the same object instance."
sameAs(e1, e2)
Comment:

6109	"as a form of similarity metric learning, we enforce the idea <e1>that</e1> points belonging to the same object instance should have very similar features; hence we measure the distance between the features of each pair of points in order to form a similarity matrix <e2>that</e2> indicates whether any given pair of points belong to the same object instance."
sameAs(e1, e2)
Comment:

6110	"as a form of similarity metric learning, we enforce the idea that points belonging to the same object instance should have very similar <e1>features</e1>; hence we measure the distance between the <e2>features</e2> of each pair of points in order to form a similarity matrix that indicates whether any given pair of points belong to the same object instance."
sameAs(e1, e2)
Comment:

6111	"therefore, a natural encoder is a convolutional neural network (cnn) instead of a <e1>recurrent neural network</e1> (<e2>rnn</e2>)."
sameAs(e1, e2)
Comment:

6112	"starting from the basic form of a <e1>cnn</e1> encoder-rnn <e2>decoder</e2>, there have been many attempts to improve the system."
Used-for(e1, e2)
Comment:

6113	"despite the variation in approaches, most of the existing lstm-based methods suffer from two problems: 1) <e1>they</e1> tend to parrot back sentences from the training corpus, and lack variation in the generated captions [10] ; 2) due to the word-by-word prediction process in sentence generation, attributes are generated before the object <e2>they</e2> refer to."
sameAs(e1, e2)
Comment:

6114	"mixtures of attributes, subjects, and relations in a complete sentence create large variations across <e1>training</e1> samples, which can affect <e2>training</e2> effectiveness."
sameAs(e1, e2)
Comment:

6115	"in order to overcome these problems, in this paper, we propose a coarse-to-fine algorithm to generate the <e1>image description</e1> in a two stage manner: first, the skeleton sentence of the <e2>image description</e2> is generated, containing the main objects involved in the image, and their relationships."
sameAs(e1, e2)
Comment:

6116	"during visual processing such as object recognition, two types of mechanisms play important roles: first, a fast subcortical pathway <e1>that</e1> projects to the frontal lobe does a coarse analysis of the image, categorizing the objects [5, 15, 18] , and this provides top-down feedback to a slower, cortical pathway in the ventral temporal lobe [40, 6] <e2>that</e2> proceeds from low level to high level regions to recognize an object."
sameAs(e1, e2)
Comment:

6117	"the exact way <e1>that</e1> the top-down mechanism is involved is not fully understood, but bar [4] proposed a hypothesis <e2>that</e2> low spatial frequency features trigger the quick "initial guesses" of the objects, and then the "initial guesses" are back-projected to low level visual cortex to integrate with the bottom-up process."
sameAs(e1, e2)
Comment:

6118	"analogous to this object recognition procedure, our <e1>image</e1> captioning process also comprises two stages: 1) a quick global prediction of the main objects and their relationship in the <e2>image</e2>, and 2) an object-wise attribute description."
sameAs(e1, e2)
Comment:

6119	"the main contributions of this paper are as follows: <e1>first</e1>, we are the <e2>first</e2> to divide the image caption task such that the skeleton and attributes are predicted separately."
sameAs(e1, e2)
Comment:

6120	"the coarse-tofine <e1>system</e1> naturally benefits from <e2>this</e2> mechanism, with the ability to vary the skeleton/attribute part of the captions separately."
Used-for(e1, e2)
Comment:

6121	"the first category tackles this problem based on retrieval: given a query <e1>image</e1>, the system searches for visually similar images in a database, finds and transfers the best descriptions from the nearest neighbor captions for the description of the query <e2>image</e2> [11, 20, 26, 34] ."
sameAs(e1, e2)
Comment:

6122	"in other words, for an extracted <e1>feature</e1>, its likelihood to the training <e2>feature</e2> distribution is not well formulated."
sameAs(e1, e2)
Comment:

6123	"in this paper we propose a <e1>gaussian mixture</e1> loss (gm loss) under the intuition that it is reasonable as well as tractable to assume the learned features of the training set to follow a <e2>gaussian mixture</e2> (gm) distribution, with each component representing a class."
sameAs(e1, e2)
Comment:

6124	"in <e1>these</e1> <e2>tasks</e2>, the softmax cross-entropy loss, or the softmax loss for short, has been widely adopted as the classification loss function for various deep neural networks [31, 10, 35, 19, 12] ."
Used-for(e1, e2)
Comment:

6125	" introduction the problem that we address in this paper is bidirectional<e1> retrieva</e1>l, also regarded as multimodal content<e2> retrieva</e2>l or image-text alignment."
sameAs(e1, e2)
Comment:

6126	"bearing in mind that bidirectional retrieval can be seen as a special case of a single visual-<e1>semantic</e1> hierarchy over words, sentences, and images, we employ a loss function based on order embeddings [29] , which are designed to model the partial order structure of the visual-<e2>semantic</e2> hierarchy existing in image descriptions."
sameAs(e1, e2)
Comment:

6127	"while typical <e1>strategies</e1> for bidirectional retrieval rely on distance-preserving <e2>strategies</e2>, our approach performs order-preserving optimization, making the process of relating the naturallyhierarchical concepts within descriptions much easier."
sameAs(e1, e2)
Comment:

6128	"moreover, we <e1>measure</e1> the robustness of our <e2>approach</e2> to input noise."
Used-for(e1, e2)
Comment:

6129	"we compare our proposed <e1>architecture</e1> with the current state-of-the-art approaches in the wellknown ms coco [17] retrieval dataset, and we show that it outperforms all other approaches while presenting a much lighter and simpler retrieval <e2>architecture</e2>."
sameAs(e1, e2)
Comment:

6130	"we compare our proposed architecture with the current state-of-the-art <e1>approaches</e1> in the wellknown ms coco [17] retrieval dataset, and we show that it outperforms all other <e2>approaches</e2> while presenting a much lighter and simpler retrieval architecture."
sameAs(e1, e2)
Comment:

6131	"we compare our proposed architecture with the current state-of-the-art approaches in the wellknown ms coco [17] <e1>retrieval</e1> dataset, and we show that it outperforms all other approaches while presenting a much lighter and simpler <e2>retrieval</e2> architecture."
sameAs(e1, e2)
Comment:

6132	"in this scenario, the main target is to retrieve content from a <e1>modality</e1> (e.g., image) given some input content from another <e2>modality</e2> (e.g., textual description)."
sameAs(e1, e2)
Comment:

6133	"several important applications benefit from successful <e1>retrieval</e1> strategies, such as image and video <e2>retrieval</e2>, captioning [23, 30] , and navigation for the blind."
sameAs(e1, e2)
Comment:

6134	"our architecture learns from scratch, in a character basis, how to retrieve <e1>descriptions</e1> from images and images from <e2>descriptions</e2>, and for that it relies exclusively on convolutional layers."
sameAs(e1, e2)
Comment:

6135	"our architecture learns from scratch, in a character basis, how to retrieve descriptions from <e1>images</e1> and <e2>images</e2> from descriptions, and for that it relies exclusively on convolutional layers."
sameAs(e1, e2)
Comment:

6136	"abstract in <e1>transfer learning</e1>, what and how to transfer are two primary issues to be addressed, as different <e2>transfer learning</e2> algorithms applied between a source and a target domain result in different knowledge transferred and thereby the performance improvement in the target domain."
sameAs(e1, e2)
Comment:

6137	"abstract in transfer learning, what and how to transfer are two primary issues to be addressed, as different transfer learning algorithms applied between a source and a <e1>target domain</e1> result in different knowledge transferred and thereby the performance improvement in the <e2>target domain</e2>."
sameAs(e1, e2)
Comment:

6138	"meanwhile, it is widely accepted in educational psychology that human beings improve <e1>transfer learning</e1> skills of deciding what to transfer through meta-cognitive reflection on inductive <e2>transfer learning</e2> practices."
sameAs(e1, e2)
Comment:

6139	"motivated by this, we propose a novel <e1>transfer learning</e1> framework known as learning to transfer (l2t) to automatically determine what and how to transfer are the best by leveraging previous <e2>transfer learning</e2> experiences."
sameAs(e1, e2)
Comment:

6140	"motivated by this, we propose a novel transfer learning framework known as learning to <e1>transfer</e1> (l2t) to automatically determine what and how to <e2>transfer</e2> are the best by leveraging previous transfer learning experiences."
sameAs(e1, e2)
Comment:

6141	"we establish the l2t framework in two stages: 1) <e1>we</e1> learn a reflection function encrypting transfer learning skills from experiences; and 2) <e2>we</e2> infer what and how to transfer are the best for a future pair of domains by optimizing the reflection function."
sameAs(e1, e2)
Comment:

6142	"we establish the l2t framework in two stages: 1) we learn a <e1>reflection</e1> function encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the <e2>reflection</e2> function."
sameAs(e1, e2)
Comment:

6143	"we establish the l2t framework in two stages: 1) we learn a reflection <e1>function</e1> encrypting transfer learning skills from experiences; and 2) we infer what and how to transfer are the best for a future pair of domains by optimizing the reflection <e2>function</e2>."
sameAs(e1, e2)
Comment:

6144	"introduction inspired by human beings' capabilities to transfer <e1>knowledge</e1> across tasks, transfer learning aims to leverage <e2>knowledge</e2> from a source domain to improve the learning performance or minimize the number of labeled examples required in a target domain."
sameAs(e1, e2)
Comment:

6145	"to address the aforementioned problems, in this paper, we present an end-toend network, depth-aware <e1>cnn</e1> (d-<e2>cnn</e2>), for rgb-d segmentation."
sameAs(e1, e2)
Comment:

6146	"depth-aware <e1>convolution</e1> augments the standard <e2>convolution</e2> with a depth similarity term."
sameAs(e1, e2)
Comment:

6147	"this simple depth similarity term efficiently incorporates geometry in a <e1>convolution</e1> kernel and helps build a depth-aware receptive field, where <e2>convolution</e2> is not constrained to the fixed grid geometric structure."
sameAs(e1, e2)
Comment:

6148	"similarly, when a filter is applied on a local <e1>region</e1> of the feature map, the pairwise relations in depth between neighboring pixels are considered in computing mean of the local <e2>region</e2>."
sameAs(e1, e2)
Comment:

6149	"as illustrated in figure 1 , pixel a and pixel c should be more correlated with each other than pixel a and pixel b. this correlation difference is obvious in depth <e1>image</e1> while it is not captured in rgb <e2>image</e2>."
sameAs(e1, e2)
Comment:

6150	"the main advantages of depth-aware <e1>cnn</e1> are summarized as follows: -by exploiting the nature of <e2>cnn</e2> kernel handling spatial information, geometry in depth image is able to be integrated into cnn seamlessly."
sameAs(e1, e2)
Comment:

6151	"the main advantages of depth-aware <e1>cnn</e1> are summarized as follows: -by exploiting the nature of cnn kernel handling spatial information, geometry in depth image is able to be integrated into <e2>cnn</e2> seamlessly."
sameAs(e1, e2)
Comment:

6152	"the main advantages of depth-aware cnn are summarized as follows: -by exploiting the nature of <e1>cnn</e1> kernel handling spatial information, geometry in depth image is able to be integrated into <e2>cnn</e2> seamlessly."
sameAs(e1, e2)
Comment:

6153	"-depth-aware <e1>cnn</e1> does not introduce any parameters and computation complexity to the conventional <e2>cnn</e2>."
sameAs(e1, e2)
Comment:

6154	"depth-aware <e1>cnn</e1> is a general framework that bonds 2d <e2>cnn</e2> and 3d geometry."
sameAs(e1, e2)
Comment:

6155	"in this work, we show how for a structured prediction problem such as pose estimation, cpms naturally suggest a systematic framework that replenishes gradients and guides the <e1>network</e1> to produce increasingly accurate belief maps by enforcing intermediate supervision periodically through the <e2>network</e2>."
sameAs(e1, e2)
Comment:

6156	"our main contributions are (a) learning implicit spatial <e1>models</e1> via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial <e2>models</e2> for structured prediction tasks, without the need for any graphical model style inference."
sameAs(e1, e2)
Comment:

6157	"our main contributions are (a) learning implicit spatial <e1>models</e1> via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial models for structured prediction <e2>tasks</e2>, without the need for any graphical model style inference."
Used-for(e1, e2)
Comment:

6158	"our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional <e1>architectures</e1> and (b) a systematic approach to designing and training such an <e2>architecture</e2> to learn both image features and image-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference."
sameAs(e1, e2)
Comment:

6159	"our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic <e1>approach</e1> to designing and training such an architecture to learn both image features and image-dependent spatial <e2>models</e2> for structured prediction tasks, without the need for any graphical model style inference."
Used-for(e1, e2)
Comment:

6160	"our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture to learn both <e1>image</e1> features and <e2>image</e2>-dependent spatial models for structured prediction tasks, without the need for any graphical model style inference."
sameAs(e1, e2)
Comment:

6161	"our main contributions are (a) learning implicit spatial models via a sequential composition of convolutional architectures and (b) a systematic approach to designing and training such an architecture to learn both image features and image-dependent spatial <e1>models</e1> for structured prediction <e2>tasks</e2>, without the need for any graphical model style inference."
Used-for(e1, e2)
Comment:

6162	"cpms inherit the benefits of the pose machine [29] <e1>architecture</e1>-the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional <e2>architectures</e2>: the ability to learn feature representations for both image and spatial context directly from data; a differentiable architecture that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets."
sameAs(e1, e2)
Comment:

6163	"cpms inherit the benefits of the pose machine [29] <e1>architecture</e1>-the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both image and spatial context directly from data; a differentiable <e2>architecture</e2> that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets."
sameAs(e1, e2)
Comment:

6164	"cpms inherit the benefits of the pose machine [29] architecture-the implicit learning of long-range dependencies between <e1>image</e1> and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both <e2>image</e2> and spatial context directly from data; a differentiable architecture that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets."
sameAs(e1, e2)
Comment:

6165	"cpms inherit the benefits of the pose machine [29] architecture-the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional <e1>architectures</e1>: the ability to learn feature representations for both image and spatial context directly from data; a differentiable <e2>architecture</e2> that allows for globally joint training with backpropagation; and the ability to efficiently handle large training datasets."
sameAs(e1, e2)
Comment:

6166	"cpms inherit the benefits of the pose machine [29] architecture-the implicit learning of long-range dependencies between image and multi-part cues, tight integration between learning and inference, a modular sequential design-and combine them with the advantages afforded by convolutional architectures: the ability to learn feature representations for both image and spatial context directly from data; a differentiable architecture that allows for globally joint <e1>training</e1> with backpropagation; and the ability to efficiently handle large <e2>training</e2> datasets."
sameAs(e1, e2)
Comment:

6167	"instead of explicitly parsing such belief <e1>maps</e1> either using graphical models [28, 38, 39] or specialized post-processing steps [38, 40] , we learn convolutional networks that directly operate on intermediate belief <e2>maps</e2> and learn implicit image-dependent spatial models of the relationships between parts."
sameAs(e1, e2)
Comment:

6168	"instead of explicitly parsing such belief maps either using graphical models [28, 38, 39] or specialized post-processing steps [38, 40] , we <e1>learn</e1> convolutional networks that directly operate on intermediate belief maps and <e2>learn</e2> implicit image-dependent spatial models of the relationships between parts."
sameAs(e1, e2)
Comment:

6169	"our method requires only partial geometric information in the form of <e1>two</e1> depth maps or partial reconstructed surfaces, works for humans in arbitrary poses and wearing any clothing, does not require the <e2>two</e2> people to be scanned from similar viewpoints, and runs in real time."
sameAs(e1, e2)
Comment:

6170	"we use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence <e1>problem</e1> directly, we train it to solve a body region classification <e2>problem</e2>, modified to increase the smoothness of the learned descriptors near region boundaries."
sameAs(e1, e2)
Comment:

6171	"we use a deep convolutional neural network to train a feature descriptor on depth map pixels, but crucially, rather than training the network to solve the shape correspondence problem directly, we train it to solve a body <e1>region</e1> classification problem, modified to increase the smoothness of the learned descriptors near <e2>region</e2> boundaries."
sameAs(e1, e2)
Comment:

6172	"we validate our method on real and synthetic data for both clothed and unclothed humans, and show <e1>that</e1> our correspondences are more robust than is possible with state-of-the-art unsupervised methods, and more accurate than those found using methods <e2>that</e2> require full watertight 3d geometry."
sameAs(e1, e2)
Comment:

6173	"following rcnn, faster-rcnn [32] proposed region <e1>proposal</e1> network (rpn) to generate <e2>proposals</e2> in a unified framework."
sameAs(e1, e2)
Comment:

6174	"however, when the <e1>processing</e1> speed is considered, faster-rcnn is still unsatisfactory because it requires two-stage <e2>processing</e2>, namely proposal generation and classification of roipooling features."
sameAs(e1, e2)
Comment:

6175	"since both ssd and faster r-cnn have default anchor boxes, we guess that the key is the two-step <e1>prediction</e1> of the default anchor boxes, with rpn one step, and <e2>prediction</e2> of rois another step, but not the roi-pooling module."
sameAs(e1, e2)
Comment:

6176	"recently, cascade r-<e1>cnn</e1> [6] has proved that faster r-<e2>cnn</e2> can be further improved by applying multi-step roi-pooling and prediction after rpn."
sameAs(e1, e2)
Comment:

6177	"for example, as depicted in fig.1 (a) , the augmented training data [42] on caltech has 42782 <e1>images</e1>, among which about 80% <e2>images</e2> have no pedestrian instances, while the remains have only 1.4 pedestrian instances per image."
sameAs(e1, e2)
Comment:

6178	"for example, as depicted in fig.1 (a) , the augmented training data [42] on caltech has 42782 images, among which about 80% images have no pedestrian <e1>instances</e1>, while the remains have only 1.4 pedestrian <e2>instances</e2> per image."
sameAs(e1, e2)
Comment:

6179	"in order to make reasonable <e1>long-term</e1> frame <e2>predictions</e2> in natural videos, these approaches need to automatically identify the dynamics of the main factors of variation changing through time, while also being highly robust to pixel-level noise."
Feature-of(e1, e2)
Comment:

6180	"the advantage of predicting the future in high-level space first is that the <e1>predictions</e1> degrade less quickly compared to <e2>predictions</e2> made solely in pixel space."
sameAs(e1, e2)
Comment:

6181	"the <e1>method</e1> by villegas et al (2017b) is an example of a hierarchical <e2>model</e2>; however, it requires ground truth human landmark annotations during training time."
Used-for(e1, e2)
Comment:

6182	"• a joint training strategy for generating high-level <e1>features</e1> from low-level features and low-level features from high-level <e2>features</e2> simultaneously."
sameAs(e1, e2)
Comment:

6183	"• a joint training strategy for generating high-level features from <e1>low-level features</e1> and <e2>low-level features</e2> from high-level features simultaneously."
sameAs(e1, e2)
Comment:

6184	"the hierarchical <e1>video</e1> prediction method by villegas et al (2017b) is an example of a state-of-the-art method for long-term <e2>video</e2> prediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time."
sameAs(e1, e2)
Comment:

6185	"the hierarchical video <e1>prediction</e1> method by villegas et al (2017b) is an example of a state-of-the-art method for long-term video <e2>prediction</e2>, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time."
sameAs(e1, e2)
Comment:

6186	"the hierarchical video prediction <e1>method</e1> by villegas et al (2017b) is an example of a state-of-the-art <e2>method</e2> for long-term video prediction, but their method is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time."
sameAs(e1, e2)
Comment:

6187	"the hierarchical video prediction <e1>method</e1> by villegas et al (2017b) is an example of a state-of-the-art method for long-term video prediction, but their <e2>method</e2> is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time."
sameAs(e1, e2)
Comment:

6188	"the hierarchical video prediction <e1>method</e1> by villegas et al (2017b) is an example of a state-of-the-art method for long-term video prediction, but their method is limited because it requires ground truth <e2>annotation</e2> of high-level structures (e.g., human joint landmarks) at training time."
Used-for(e1, e2)
Comment:

6189	"the hierarchical video prediction method by villegas et al (2017b) is an example of a state-of-the-art <e1>method</e1> for long-term video prediction, but their <e2>method</e2> is limited because it requires ground truth annotation of high-level structures (e.g., human joint landmarks) at training time."
sameAs(e1, e2)
Comment:

6190	"the hierarchical video prediction method by villegas et al (2017b) is an example of a state-of-the-art <e1>method</e1> for long-term video prediction, but their method is limited because it requires ground truth <e2>annotation</e2> of high-level structures (e.g., human joint landmarks) at training time."
Used-for(e1, e2)
Comment:

6191	"the hierarchical video prediction method by villegas et al (2017b) is an example of a state-of-the-art method for long-term video prediction, but their <e1>method</e1> is limited because it requires ground truth <e2>annotation</e2> of high-level structures (e.g., human joint landmarks) at training time."
Used-for(e1, e2)
Comment:

6192	"unlike villegas et al (2017b) , <e1>we</e1> develop a novel training method that jointly trains the encoder, the predictor, and the decoder together without highlevel supervision; <e2>we</e2> further improve upon this by using an adversarial loss in the feature space to train the predictor."
sameAs(e1, e2)
Comment:

6193	"models <e1>that</e1> are able to accurately predict the future can play a vital role in developing intelligent agents <e2>that</e2> interact with their environment (jayaraman and grauman, 2015; finn et al, 2016) ."
sameAs(e1, e2)
Comment:

6194	"abstract off-<e1>policy</e1> learning, the task of evaluating and improving policies using historic data collected from a logging <e2>policy</e2>, is important because onpolicy evaluation is usually expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6195	"abstract off-policy learning, the <e1>task</e1> of evaluating and improving policies using historic <e2>data</e2> collected from a logging policy, is important because onpolicy evaluation is usually expensive and has adverse impacts."
Conjunction(e1, e2)
Comment:

6196	"however, in this setting, <e1>we</e1> can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward <e2>we</e2> could have obtained had we taken another action, the best action we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6197	"however, in this setting, <e1>we</e1> can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had <e2>we</e2> taken another action, the best action we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6198	"however, in this setting, <e1>we</e1> can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another action, the best action <e2>we</e2> should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6199	"however, in this setting, we can only observe limited feedback, often in the form of a scalar <e1>reward</e1> or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what <e2>reward</e2> we could have obtained had we taken another action, the best action we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6200	"however, in this setting, we can only observe limited feedback, often in the form of a scalar <e1>reward</e1> or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another action, the best action we should have take, and the relationship between the change in policy and the change in <e2>reward</e2>."
sameAs(e1, e2)
Comment:

6201	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every <e1>action</e1>; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another <e2>action</e2>, the best action we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6202	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every <e1>action</e1>; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another action, the best <e2>action</e2> we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6203	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what <e1>reward</e1> we could have obtained had we taken another action, the best action we should have take, and the relationship between the change in policy and the change in <e2>reward</e2>."
sameAs(e1, e2)
Comment:

6204	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward <e1>we</e1> could have obtained had <e2>we</e2> taken another action, the best action we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6205	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward <e1>we</e1> could have obtained had we taken another action, the best action <e2>we</e2> should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6206	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had <e1>we</e1> taken another action, the best action <e2>we</e2> should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6207	"however, in this setting, we can only observe limited feedback, often in the form of a scalar reward or loss, for every action; a larger amount of information about other possibilities is never revealed, such as what reward we could have obtained had we taken another <e1>action</e1>, the best <e2>action</e2> we should have take, and the relationship between the change in policy and the change in reward."
sameAs(e1, e2)
Comment:

6208	"for example, after an item is suggested to a user by an online recommendation <e1>system</e1>, although we can observe the user's subsequent interactions with <e2>this</e2> particular item, we cannot anticipate the user's reaction to other items that could have been the better options."
Used-for(e1, e2)
Comment:

6209	"for example, after an item is suggested to a user by an online recommendation system, although <e1>we</e1> can observe the user's subsequent interactions with this particular item, <e2>we</e2> cannot anticipate the user's reaction to other items that could have been the better options."
sameAs(e1, e2)
Comment:

6210	"using historic data to perform off-<e1>policy</e1> learning in bandit feedback case faces a common challenge in counterfactual inference: how do we handle the distribution mismatch between the logging <e2>policy</e2> and a new policy and the induced generalization error?"
sameAs(e1, e2)
Comment:

6211	"using historic data to perform off-<e1>policy</e1> learning in bandit feedback case faces a common challenge in counterfactual inference: how do we handle the distribution mismatch between the logging policy and a new <e2>policy</e2> and the induced generalization error?"
sameAs(e1, e2)
Comment:

6212	"using historic data to perform off-policy learning in bandit feedback case faces a common challenge in counterfactual inference: how do we handle the distribution mismatch between the logging <e1>policy</e1> and a new <e2>policy</e2> and the induced generalization error?"
sameAs(e1, e2)
Comment:

6213	"to answer this question, (swaminathan & joachims, 2015a) derived the new counterfactual risk <e1>minimization</e1> framework, that added the sample variance as a regularization term into conventional empirical risk <e2>minimization</e2> objective."
sameAs(e1, e2)
Comment:

6214	"we explicitly regularize the generalization error of the new <e1>policy</e1> by minimizing the distribution divergence between it and the logging <e2>policy</e2>."
sameAs(e1, e2)
Comment:

6215	"2. to enable end-to-end training, we propose to parametrize the policy as a neural network, and solves the <e1>divergence</e1> minimization problem using recent work on variational <e2>divergence</e2> minimization (nowozin et al, 2016) and gumbel soft-max (jang et al, 2016) sampling."
sameAs(e1, e2)
Comment:

6216	"2. to enable end-to-end training, we propose to parametrize the policy as a neural network, and solves the divergence <e1>minimization</e1> problem using recent work on variational divergence <e2>minimization</e2> (nowozin et al, 2016) and gumbel soft-max (jang et al, 2016) sampling."
sameAs(e1, e2)
Comment:

6217	"our method regularizes the generalization error by minimizing the distribution divergence between the logging <e1>policy</e1> and the new <e2>policy</e2>, and removes the need for iterating through all training samples to compute sample variance regularization in prior work."
sameAs(e1, e2)
Comment:

6218	"introduction off-<e1>policy</e1> learning refers to evaluating and improving a deterministic <e2>policy</e2> using historic data collected from a stationary policy, which is important because in real-world scenarios on-policy evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6219	"introduction off-<e1>policy</e1> learning refers to evaluating and improving a deterministic policy using historic data collected from a stationary <e2>policy</e2>, which is important because in real-world scenarios on-policy evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6220	"introduction off-<e1>policy</e1> learning refers to evaluating and improving a deterministic policy using historic data collected from a stationary policy, which is important because in real-world scenarios on-<e2>policy</e2> evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6221	"introduction off-policy learning refers to evaluating and improving a deterministic <e1>policy</e1> using historic data collected from a stationary <e2>policy</e2>, which is important because in real-world scenarios on-policy evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6222	"introduction off-policy learning refers to evaluating and improving a deterministic <e1>policy</e1> using historic data collected from a stationary policy, which is important because in real-world scenarios on-<e2>policy</e2> evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6223	"introduction off-policy learning refers to evaluating and improving a deterministic policy using historic data collected from a stationary <e1>policy</e1>, which is important because in real-world scenarios on-<e2>policy</e2> evaluation is oftentimes expensive and has adverse impacts."
sameAs(e1, e2)
Comment:

6224	"for instance, evaluating a new treatment option, a <e1>clinical</e1> policy, by administering it to patients requires rigorous human <e2>clinical</e2> trials, in which patients are exposed to risks of serious side effects."
sameAs(e1, e2)
Comment:

6225	"abstract the current trend of pushing <e1>cnns</e1> deeper with convolutions has created a pressing demand to achieve higher compression gains on <e2>cnns</e2> where convolutions dominate the computation and parameter amount (e.g., googlenet, resnet and wide resnet)."
sameAs(e1, e2)
Comment:

6226	"abstract the current trend of pushing cnns deeper with <e1>convolutions</e1> has created a pressing demand to achieve higher compression gains on cnns where <e2>convolutions</e2> dominate the computation and parameter amount (e.g., googlenet, resnet and wide resnet)."
sameAs(e1, e2)
Comment:

6227	"abstract the current trend of pushing cnns deeper with convolutions has created a pressing demand to achieve higher compression gains on cnns where convolutions dominate the computation and parameter amount (e.g., googlenet, <e1>resnet</e1> and wide <e2>resnet</e2>)."
sameAs(e1, e2)
Comment:

6228	"we additionally propose an improved set of metrics to estimate energy consumption of cnn <e1>hardware</e1> implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual <e2>hardware</e2> measurements."
sameAs(e1, e2)
Comment:

6229	"we additionally propose an improved set of metrics to estimate energy consumption of cnn hardware implementations, whose <e1>estimation</e1> results are verified to be consistent with previously proposed energy <e2>estimation</e2> tool extrapolated from actual hardware measurements."
sameAs(e1, e2)
Comment:

6230	"we additionally propose an improved set of metrics to estimate energy consumption of cnn <e1>hardware</e1> implementations, whose estimation results are verified to be consistent with previously proposed energy estimation tool extrapolated from actual <e2>hardware</e2> measurements."
sameAs(e1, e2)
Comment:

6231	"we additionally propose an improved set of metrics to estimate energy consumption of cnn hardware implementations, whose <e1>estimation</e1> results are verified to be consistent with previously proposed energy <e2>estimation</e2> tool extrapolated from actual hardware measurements."
sameAs(e1, e2)
Comment:

6232	"the current trend of pushing <e1>cnns</e1> deeper with convolutions has created a pressing demand to achieve higher compression gains on <e2>cnns</e2> where convolutions dominate the computation and parameter amount (e.g., googlenet, resnet and wide resnet)."
sameAs(e1, e2)
Comment:

6233	"the current trend of pushing cnns deeper with <e1>convolutions</e1> has created a pressing demand to achieve higher compression gains on cnns where <e2>convolutions</e2> dominate the computation and parameter amount (e.g., googlenet, resnet and wide resnet)."
sameAs(e1, e2)
Comment:

6234	"the current trend of pushing cnns deeper with convolutions has created a pressing demand to achieve higher compression gains on cnns where convolutions dominate the computation and parameter amount (e.g., googlenet, <e1>resnet</e1> and wide <e2>resnet</e2>)."
sameAs(e1, e2)
Comment:

6235	"abstract we focus on the one-shot learning for video-based <e1>person re-identification</e1> (re-id) introduction <e2>person re-identification</e2> (re-id) aims at spotting the person-of-interest from different cameras."
sameAs(e1, e2)
Comment:

6236	"this work mainly focuses on the <e1>one</e1>-shot setting, in which only <e2>one</e2> tracklet is labeled for each identity."
sameAs(e1, e2)
Comment:

6237	"the initial labeled <e1>data</e1> and some selected pseudo-labeled <e2>data</e2> are considered as an enlarged training set."
sameAs(e1, e2)
Comment:

6238	"updating the <e1>model</e1> with excessive not-yet-reliable data would hinder the subsequent improvement of the <e2>model</e2>."
sameAs(e1, e2)
Comment:

6239	"to characterize the proposed progressive approach in one-shot person re-id, we intensively investigate two significant aspects, i.e., how the progressive <e1>sampling</e1> strategy benefits the label estimation and which <e2>sampling</e2> criterion is effective for the confidence estimation in person re-id."
sameAs(e1, e2)
Comment:

6240	"to characterize the proposed progressive approach in one-shot person re-id, we intensively investigate two significant aspects, i.e., how the progressive sampling strategy benefits the label <e1>estimation</e1> and which sampling criterion is effective for the confidence <e2>estimation</e2> in person re-id."
sameAs(e1, e2)
Comment:

6241	"for the first aspect, <e1>we</e1> find that if <e2>we</e2> enlarge the sampled subset of pseudo-labeled data in a more conservative way (at a slower speed), the model achieves a better performance."
sameAs(e1, e2)
Comment:

6242	"• we apply a distance-based sampling criterion for label <e1>estimation</e1> and candidates selection to remarkably improve the performance of label <e2>estimation</e2>."
sameAs(e1, e2)
Comment:

6243	"due to the intrinsical difference between them, existing works usually focus on improving the efficiency for either convolutional <e1>layers</e1> or fully-connected <e2>layers</e2>."
sameAs(e1, e2)
Comment:

6244	"in this paper, we propose a unified framework for con-volutional networks, namely quantized <e1>cnn</e1> (q-<e2>cnn</e2>), to simultaneously accelerate and compress cnn models with only minor performance degradation."
sameAs(e1, e2)
Comment:

6245	"for most cnns, convolutional <e1>layers</e1> are the most timeconsuming part, while fully-connected <e2>layers</e2> involve massive network parameters."
sameAs(e1, e2)
Comment:

6246	"consequently, there are three common <e1>strategies</e1>: 1) bottom-up [1, 24, 5, 38, 29, 19] : first detect text at the component-or character-level, and convert initial detection results to a desired word-or line-level; 2) top-down [42] : first detect text on the block level, and break a block to a word-or line-level if necessary; and 3) hybrid [34] : take advantage of both bottom-up and top-down <e2>strategies</e2>."
sameAs(e1, e2)
Comment:

6247	"consequently, there are three common strategies: 1) bottom-up [1, 24, 5, 38, 29, 19] : <e1>first</e1> detect text at the component-or character-level, and convert initial detection results to a desired word-or line-level; 2) top-down [42] : <e2>first</e2> detect text on the block level, and break a block to a word-or line-level if necessary; and 3) hybrid [34] : take advantage of both bottom-up and top-down strategies."
sameAs(e1, e2)
Comment:

6248	"consequently, there are three common strategies: 1) bottom-up [1, 24, 5, 38, 29, 19] : first detect <e1>text</e1> at the component-or character-level, and convert initial detection results to a desired word-or line-level; 2) top-down [42] : first detect <e2>text</e2> on the block level, and break a block to a word-or line-level if necessary; and 3) hybrid [34] : take advantage of both bottom-up and top-down strategies."
sameAs(e1, e2)
Comment:

6249	"actually, these are the mainstream frameworks used in text detection until now, although people may use different component/character detectors, <e1>rules</e1> to connect character bounding boxes to a word/line, <e2>rules</e2> to divide a region into words/lines, etc."
sameAs(e1, e2)
Comment:

6250	"swt assumes that <e1>text</e1> components have comparable stroke width, and thus finding components with comparable stroke width finds <e2>text</e2>."
sameAs(e1, e2)
Comment:

6251	"swt assumes that text <e1>components</e1> have comparable stroke width, and thus finding <e2>components</e2> with comparable stroke width finds text."
sameAs(e1, e2)
Comment:

6252	"although the <e1>text</e1> detection problem seems to be selfexplanatory, namely detecting <e2>texts</e2> in a given image, it can be interpreted in many different ways according to the used detection unit, e.g., component detection [2, 33, 17, 22, 39] , character detection [19, 40, 5, 38, 24, 28, 29] , word detection [10] , line detection [41] , and region/block detection [42, 34] ."
sameAs(e1, e2)
Comment:

6253	"from the perspective of detection speed, the component detection is the most handy because components can be efficiently extracted from low-level features like edges, and some heuristic rules are also effective to distinguish <e1>text</e1> from non-<e2>text</e2>."
sameAs(e1, e2)
Comment:

6254	"from the perspective of reducing false alarms, the <e1>region</e1>/block-level detection is preferred because it contains a larger image context <e2>region</e2> to differentiate text from non-text."
sameAs(e1, e2)
Comment:

6255	"from the perspective of reducing false alarms, the region/block-level detection is preferred because it contains a larger image context region to differentiate <e1>text</e1> from non-<e2>text</e2>."
sameAs(e1, e2)
Comment:

6256	"from the perspective of a down-streaming <e1>recognition system</e1>, the most preferred is the line-or word-level detection depending on whether a <e2>recognition system</e2> is trained by text lines or isolated words."
sameAs(e1, e2)
Comment:

6257	"we take the class-wise <e1>supervision</e1> to the extreme of instance-wise <e2>supervision</e2>, and ask: can we learn a meaningful metric that reflects apparent similarity among instances via pure discriminative learning?"
sameAs(e1, e2)
Comment:

6258	"if <e1>we</e1> learn to discriminate between individual instances, without any notion of semantic categories, <e2>we</e2> may end up with a representation that captures apparent similarity among instances, just like how class-wise supervised learning still retains apparent similarity among classes."
sameAs(e1, e2)
Comment:

6259	"if we learn to discriminate between individual <e1>instances</e1>, without any notion of semantic categories, we may end up with a representation that captures apparent similarity among <e2>instances</e2>, just like how class-wise supervised learning still retains apparent similarity among classes."
sameAs(e1, e2)
Comment:

6260	"this formulation of <e1>unsupervised learning</e1> as an instance-level discrimination is also technically appealing, as it could benefit from latest advances in discriminative <e2>supervised learning</e2>, e.g."
Conjunction(e1, e2)
Comment:

6261	"however, it is unclear why features learned via a training <e1>task</e1> could be linearly separable for an unknown testing <e2>task</e2>."
sameAs(e1, e2)
Comment:

6262	"at the test time, we perform classification using <e1>k-nearest neighbors</e1> (<e2>knn</e2>) based on the learned metric."
sameAs(e1, e2)
Comment:

6263	"our novel approach to <e1>unsupervised learning</e1> stems from a few observations on the results of <e2>supervised learning</e2> for object recognition."
Conjunction(e1, e2)
Comment:

6264	"on imagenet, the top-5 classification error is significantly lower than the top-1 error [18] , and the second highest responding class in the softmax output to an leopard jaguar cheetah lifeboat shopcart bookcase figure 1 : <e1>supervised learning</e1> results that motivate our unsupervised <e2>approach</e2>."
Used-for(e1, e2)
Comment:

6265	"we apply our approach on two <e1>retrieval</e1> tasks, face <e2>retrieval</e2> on the celeba dataset [12] and product retrieval on the ut-zappos50k dataset [34, 35] , and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency."
sameAs(e1, e2)
Comment:

6266	"we apply our approach on two <e1>retrieval</e1> tasks, face retrieval on the celeba dataset [12] and product <e2>retrieval</e2> on the ut-zappos50k dataset [34, 35] , and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency."
sameAs(e1, e2)
Comment:

6267	"we apply our approach on two retrieval tasks, face <e1>retrieval</e1> on the celeba dataset [12] and product <e2>retrieval</e2> on the ut-zappos50k dataset [34, 35] , and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency."
sameAs(e1, e2)
Comment:

6268	"introduction multi-task learning aims to improve learning efficiency and boost the performance of individual <e1>tasks</e1> by jointly learning multiple <e2>tasks</e2> at the same time."
sameAs(e1, e2)
Comment:

6269	"multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual <e1>tasks</e1>, under the assumption that those <e2>tasks</e2> are correlated and complementary to each other."
sameAs(e1, e2)
Comment:

6270	"however, the relationships between the <e1>tasks</e1> are complicated in practice, especially when the number of involved <e2>tasks</e2> scales up."
sameAs(e1, e2)
Comment:

6271	"when two <e1>tasks</e1> are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the <e2>tasks</e2>."
sameAs(e1, e2)
Comment:

6272	"this will raise destructive interference which decreases learning <e1>efficiency</e1> of shared parameters and lead to low <e2>quality</e2> loss local optimum w.r.t."
Conjunction(e1, e2)
Comment:

6273	"to address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant <e1>tasks</e1> while disentangling the learning of irrelevant <e2>tasks</e2> with minor parameters addition."
sameAs(e1, e2)
Comment:

6274	"the module is end-to-end learnable without ad-hoc design for specific <e1>tasks</e1>, and can naturally handle many <e2>tasks</e2> at the same time."
sameAs(e1, e2)
Comment:

6275	"in particular, if the generator learns discriminative visual <e1>data</e1> with enough variation, the generated <e2>data</e2> should be useful for supervised learning."
sameAs(e1, e2)
Comment:

6276	"in particular, we look into both zero-shot learning (zsl) where the test <e1>time</e1> search space is restricted to unseen class labels and generalized zero-shot learning (gzsl) for being a more realistic scenario as at test <e2>time</e2> the classifier has to decide between both seen and unseen class labels."
sameAs(e1, e2)
Comment:

6277	"figure 1 : cnn features can be extracted from: 1) real <e1>images</e1>, however in zero-shot learning we do not have access to any real <e2>images</e2> of unseen classes, 2) synthetic images, however they are not accurate enough to improve image classification performance."
sameAs(e1, e2)
Comment:

6278	"figure 1 : cnn features can be extracted from: 1) real <e1>images</e1>, however in zero-shot learning we do not have access to any real images of unseen classes, 2) synthetic <e2>images</e2>, however they are not accurate enough to improve image classification performance."
sameAs(e1, e2)
Comment:

6279	"figure 1 : cnn features can be extracted from: 1) real images, however in zero-shot learning we do not have access to any real <e1>images</e1> of unseen classes, 2) synthetic <e2>images</e2>, however they are not accurate enough to improve image classification performance."
sameAs(e1, e2)
Comment:

6280	"these include 357 on road <e1>images</e1> and 628 off road <e2>images</e2>."
sameAs(e1, e2)
Comment:

6281	"we show that <e1>fcn</e1>-8s with raus improves significantly precision and recall metrics as compared to <e2>fcn</e2>-8s, deeplab v2 and gaussian mixture model (gmm)."
sameAs(e1, e2)
Comment:

6282	"we show that fcn-8s with raus improves significantly <e1>precision</e1> and <e2>recall</e2> metrics as compared to fcn-8s, deeplab v2 and gaussian mixture model (gmm)."
Conjunction(e1, e2)
Comment:

6283	"we show that fcn-8s with raus improves significantly precision and recall metrics as compared to fcn-8s, deeplab v2 and <e1>gaussian mixture model</e1> (<e2>gmm</e2>)."
sameAs(e1, e2)
Comment:

6284	"they generate a likelihood heat <e1>map</e1> for each joint and locate the joint as the point with the maximum likelihood in the <e2>map</e2>."
sameAs(e1, e2)
Comment:

6285	"using image and heat <e1>map</e1> with higher resolution helps to increase accuracy but is computational and storage demanding, especially for 3d heat <e2>maps</e2>."
sameAs(e1, e2)
Comment:

6286	"however, regression <e1>methods</e1> are not as effective as well as detection based <e2>methods</e2> for 2d human pose estimation."
sameAs(e1, e2)
Comment:

6287	"among the best-performing <e1>methods</e1> in the 2d pose benchmark [2] , only one <e2>method</e2> [7] is regression based."
Compare(e1, e2)
Comment:

6288	"because the integral regression is parameter free and only transforms the pose representation from a heat <e1>map</e1> to a joint, it does not affect other algorithm design choices and can be combined with any of them, including different tasks, heat <e2>map</e2> and joint losses, network architectures, image and heat map resolutions."
sameAs(e1, e2)
Comment:

6289	"because the integral regression is parameter free and only transforms the pose representation from a heat <e1>map</e1> to a joint, it does not affect other algorithm design choices and can be combined with any of them, including different tasks, heat map and joint losses, network architectures, image and heat <e2>map</e2> resolutions."
sameAs(e1, e2)
Comment:

6290	"because the integral regression is parameter free and only transforms the pose representation from a heat map to a joint, it does not affect other algorithm design choices and can be combined with any of them, including different tasks, heat <e1>map</e1> and joint losses, network architectures, image and heat <e2>map</e2> resolutions."
sameAs(e1, e2)
Comment:

6291	"existing methods can be divided into two categories, including video based <e1>approaches</e1> and single image based <e2>approaches</e2>."
sameAs(e1, e2)
Comment:

6292	"however, single image deraining is more challenging, and we focus on <e1>this</e1> task in <e2>this</e2> paper."
sameAs(e1, e2)
Comment:

6293	"to better utilize the useful information for rain removal in previous stages, we further incorporate the <e1>recurrent neural network</e1> (<e2>rnn</e2>) architecture with three kinds of recurrent units to guide the deraining in later stages."
sameAs(e1, e2)
Comment:

6294	"main contributions of this paper are listed as follows: 1. <e1>we</e1> propose a novel unified deep network for single image deraining, by which <e2>we</e2> remove the rain stage by stage."
sameAs(e1, e2)
Comment:

6295	"prior domain adaptation <e1>methods</e1> address this problem through aligning the global distribution statistics between source domain and target domain, but a drawback of prior <e2>methods</e2> is that they ignore the semantic information contained in samples, e.g., features of backpacks in target domain might be mapped near features of cars in source domain."
sameAs(e1, e2)
Comment:

6296	"prior domain adaptation methods address this problem through aligning the global distribution statistics between <e1>source domain</e1> and target domain, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., features of backpacks in target domain might be mapped near features of cars in <e2>source domain</e2>."
sameAs(e1, e2)
Comment:

6297	"prior domain adaptation methods address this problem through aligning the global distribution statistics between source domain and <e1>target domain</e1>, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., features of backpacks in <e2>target domain</e2> might be mapped near features of cars in source domain."
sameAs(e1, e2)
Comment:

6298	"prior domain adaptation methods address this problem through aligning the global distribution statistics between source domain and target domain, but a drawback of prior methods is that they ignore the semantic information contained in samples, e.g., <e1>features</e1> of backpacks in target domain might be mapped near <e2>features</e2> of cars in source domain."
sameAs(e1, e2)
Comment:

6299	"even with perfect confusion alignment, there is no guarantee that samples from different domains but with the same class label will map nearby in the feature space, e.g, <e1>features</e1> of backpacks in the target domain may be mapped near <e2>features</e2> of cars in the source domain."
sameAs(e1, e2)
Comment:

6300	"we firstly assign pseudo labels to <e1>target</e1> samples to fix the problem of lacking <e2>target</e2> label information."
sameAs(e1, e2)
Comment:

6301	"in this paper, we present moving semantic transfer network, which learn semantic representations for unlabeled <e1>target</e1> samples by aligning labeled source centroid and pseudo-labeled <e2>target</e2> centroid."
sameAs(e1, e2)
Comment:

6302	"it can be shown that the <e1>solution</e1> space of this architecture is a strict subspace of the <e2>solution</e2> space of a single large layer (e.g., 5×5) operating on a high-dimensional embedding."
sameAs(e1, e2)
Comment:

6303	"in contrast to traditional handdesigned <e1>features</e1> (e.g., <e2>sift</e2> [29] and hog [5] ), features learned by neural networks from large-scale data [33] require minimal human involvement during training, and can be transferred to a variety of recognition tasks [7, 10, 28] ."
Compare(e1, e2)
Comment:

6304	"in contrast to traditional handdesigned <e1>features</e1> (e.g., sift [29] and hog [5] ), <e2>features</e2> learned by neural networks from large-scale data [33] require minimal human involvement during training, and can be transferred to a variety of recognition tasks [7, 10, 28] ."
sameAs(e1, e2)
Comment:

6305	"the split-transform-merge behavior of inception modules is expected to <e1>approach</e1> the representational power of large and dense <e2>layers</e2>, but at a considerably lower computational complexity."
Used-for(e1, e2)
Comment:

6306	"although careful combinations of <e1>these</e1> components yield excellent neural network recipes, it is in general unclear how to adapt the inception architectures to new datasets/<e2>tasks</e2>, especially when there are many factors and hyper-parameters to be designed."
Used-for(e1, e2)
Comment:

6307	"in this paper, we present a simple architecture which adopts vgg/resnets' <e1>strategy</e1> of repeating layers, while exploiting the split-transform-merge <e2>strategy</e2> in an easy, extensible way."
sameAs(e1, e2)
Comment:

6308	"3 (b) appears similar to the inceptionresnet <e1>module</e1> [37] in that it concatenates multiple paths; but our <e2>module</e2> differs from all existing inception modules in that all our paths share the same topology and thus the number of paths can be easily isolated as a factor to be investigated."
sameAs(e1, e2)
Comment:

6309	"3 (b) appears similar to the inceptionresnet module [37] in <e1>that</e1> it concatenates multiple paths; but our module differs from all existing inception modules in <e2>that</e2> all our paths share the same topology and thus the number of paths can be easily isolated as a factor to be investigated."
sameAs(e1, e2)
Comment:

6310	"we emphasize <e1>that</e1> while it is relatively easy to increase accuracy by increasing capacity (going deeper or wider), methods <e2>that</e2> increase accuracy while maintaining (or reducing) complexity are rare in the literature."
sameAs(e1, e2)
Comment:

6311	"we emphasize that while it is relatively easy to increase <e1>accuracy</e1> by increasing capacity (going deeper or wider), methods that increase <e2>accuracy</e2> while maintaining (or reducing) complexity are rare in the literature."
sameAs(e1, e2)
Comment:

6312	"our <e1>method</e1> indicates that cardinality (the size of the set of <e2>transformations</e2>) is a concrete, measurable dimension that is of central importance, in addition to the dimensions of width and depth."
Used-for(e1, e2)
Comment:

6313	"our method indicates <e1>that</e1> cardinality (the size of the set of transformations) is a concrete, measurable dimension <e2>that</e2> is of central importance, in addition to the dimensions of width and depth."
sameAs(e1, e2)
Comment:

6314	"our neural networks, named resnext (suggesting the next dimension), outperform <e1>resnet</e1>-101/152 [14] , <e2>resnet</e2>-200 [15] , inception-v3 [39] , and inception-resnet-v2 [37] on the imagenet classification dataset."
sameAs(e1, e2)
Comment:

6315	"our neural networks, named resnext (suggesting the next dimension), outperform <e1>resnet</e1>-101/152 [14] , resnet-200 [15] , inception-v3 [39] , and inception-<e2>resnet</e2>-v2 [37] on the imagenet classification dataset."
sameAs(e1, e2)
Comment:

6316	"our neural networks, named resnext (suggesting the next dimension), outperform resnet-101/152 [14] , <e1>resnet</e1>-200 [15] , inception-v3 [39] , and inception-<e2>resnet</e2>-v2 [37] on the imagenet classification dataset."
sameAs(e1, e2)
Comment:

6317	"our neural networks, named resnext (suggesting the next dimension), outperform resnet-101/152 [14] , resnet-200 [15] , <e1>inception</e1>-v3 [39] , and <e2>inception</e2>-resnet-v2 [37] on the imagenet classification dataset."
sameAs(e1, e2)
Comment:

6318	"this paper further evaluates resnext on a larger imagenet-5k set and the coco object detection dataset [27] , showing consistently better <e1>accuracy</e1> than its resnet <e2>counterparts</e2>."
Evaluate-for(e1, e2)
Comment:

6319	"one key module of cnn model is the <e1>convolution</e1> layer, which extracts features from high-dimensional structural data efficiently by a set of <e2>convolution</e2> kernels."
sameAs(e1, e2)
Comment:

6320	"when dealing with multi-channel inputs (e.g., color images), the <e1>convolution</e1> kernels merges these channels by summing up the <e2>convolution</e2> results and output one single channel per kernel accordingly, as fig."
sameAs(e1, e2)
Comment:

6321	"accordingly, we may lose important structural information of <e1>color</e1> and obtain non-optimal representation of <e2>color</e2> image [36] ."
sameAs(e1, e2)
Comment:

6322	"in particular, each <e1>color</e1> pixel in a <e2>color</e2> image (i.e., the yellow dot in fig."
sameAs(e1, e2)
Comment:

6323	"while the traditional real-valued <e1>convolution</e1> is only capable to enforce scaling transformation on the input, specifically, the quaternion <e2>convolution</e2> achieves the scaling and the rotation of input in the color space, which provides us with more structural representation of color information."
sameAs(e1, e2)
Comment:

6324	"while the traditional real-valued convolution is only capable to enforce scaling transformation on the <e1>input</e1>, specifically, the quaternion convolution achieves the scaling and the rotation of <e2>input</e2> in the color space, which provides us with more structural representation of color information."
sameAs(e1, e2)
Comment:

6325	"while the traditional real-valued convolution is only capable to enforce scaling transformation on the input, specifically, the quaternion convolution achieves the scaling and the rotation of input in the <e1>color</e1> space, which provides us with more structural representation of <e2>color</e2> information."
sameAs(e1, e2)
Comment:

6326	"as a result, using quaternion <e1>cnns</e1>, we can achieve better learning results with fewer parameters compared with real-valued <e2>cnns</e2>."
sameAs(e1, e2)
Comment:

6327	"additionally, a <e1>color</e1> image is represented as a quaternion matrix in our qcnn, so that we can transform a <e2>color</e2> pixel throughout the color space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the color cone shown in fig."
sameAs(e1, e2)
Comment:

6328	"additionally, a <e1>color</e1> image is represented as a quaternion matrix in our qcnn, so that we can transform a color pixel throughout the <e2>color</e2> space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the color cone shown in fig."
sameAs(e1, e2)
Comment:

6329	"additionally, a <e1>color</e1> image is represented as a quaternion matrix in our qcnn, so that we can transform a color pixel throughout the color space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the <e2>color</e2> cone shown in fig."
sameAs(e1, e2)
Comment:

6330	"additionally, a color image is represented as a quaternion matrix in our qcnn, so that we can transform a <e1>color</e1> pixel throughout the <e2>color</e2> space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the color cone shown in fig."
sameAs(e1, e2)
Comment:

6331	"additionally, a color image is represented as a quaternion matrix in our qcnn, so that we can transform a <e1>color</e1> pixel throughout the color space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the <e2>color</e2> cone shown in fig."
sameAs(e1, e2)
Comment:

6332	"additionally, a color image is represented as a quaternion matrix in our qcnn, so that we can transform a color pixel throughout the <e1>color</e1> space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the <e2>color</e2> cone shown in fig."
sameAs(e1, e2)
Comment:

6333	"1 shows, our qcnn preserves more <e1>color</e1> information than real-valued cnn, which is suitable for <e2>color</e2> image processing, especially for lowlevel color feature extraction."
sameAs(e1, e2)
Comment:

6334	"1 shows, our qcnn preserves more <e1>color</e1> information than real-valued cnn, which is suitable for color image processing, especially for lowlevel <e2>color</e2> feature extraction."
sameAs(e1, e2)
Comment:

6335	"1 shows, our qcnn preserves more color information than real-valued cnn, which is suitable for <e1>color</e1> image processing, especially for lowlevel <e2>color</e2> feature extraction."
sameAs(e1, e2)
Comment:

6336	"experimental results show that our qcnn model provides benefits for both high-level <e1>vision</e1> task (i.e., color image classification) and low-level <e2>vision</e2> task (i.e., color image denoising), which outperforms its competitors."
sameAs(e1, e2)
Comment:

6337	"experimental results show that our qcnn model provides benefits for both high-level vision <e1>task</e1> (i.e., color image classification) and low-level vision <e2>task</e2> (i.e., color image denoising), which outperforms its competitors."
sameAs(e1, e2)
Comment:

6338	"experimental results show that our qcnn model provides benefits for both high-level vision task (i.e., <e1>color</e1> image classification) and low-level vision task (i.e., <e2>color</e2> image denoising), which outperforms its competitors."
sameAs(e1, e2)
Comment:

6339	"we test our qcnn <e1>models</e1> in both color image classification and denoising <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

6340	"although analyzing human body from different perspectives, <e1>these</e1> two <e2>tasks</e2> are highly correlated and could provide beneficial clues for each other."
Used-for(e1, e2)
Comment:

6341	"however, existing <e1>methods</e1> usually train the task-specific <e2>models</e2> separately and leverage the guidance information for post-processing, suffering several drawbacks."
Compare(e1, e2)
Comment:

6342	"first, they heavily rely on hand-crafted features extracted from outputs of <e1>one</e1> task to assist the <e2>other</e2>, in an ad hoc manner."
Conjunction(e1, e2)
Comment:

6343	"moreover, instead of simply fusing learned features from two tasks as in existing works, mula introduces a learning to adapt mechanism where the guidance information from <e1>one</e1> task can be effectively transferred to modify model parameters for the <e2>other</e2> parallel task, leading to augmented representation and better performance."
Conjunction(e1, e2)
Comment:

6344	"moreover, instead of simply fusing learned features from two tasks as in existing works, mula introduces a learning to adapt mechanism where the guidance information from one <e1>task</e1> can be effectively transferred to modify model parameters for the other parallel <e2>task</e2>, leading to augmented representation and better performance."
sameAs(e1, e2)
Comment:

6345	"specifically, the mula model includes a representation encoding <e1>module</e1>, a mutual adaptation <e2>module</e2> and a classification module."
sameAs(e1, e2)
Comment:

6346	"specifically, the mula model includes a representation encoding <e1>module</e1>, a mutual adaptation module and a classification <e2>module</e2>."
sameAs(e1, e2)
Comment:

6347	"specifically, the mula model includes a representation encoding module, a mutual adaptation <e1>module</e1> and a classification <e2>module</e2>."
sameAs(e1, e2)
Comment:

6348	"with such guidance information, the mutual adaptation module learns to dynamically predict model parameters for augmenting representations by incorporating useful prior learned from the other <e1>task</e1>, enabling effective between-<e2>task</e2> interaction and cooperation in model training."
sameAs(e1, e2)
Comment:

6349	"introduc-ing such a mutual adaptation module improves the learning process of <e1>one</e1> task towards benefiting the <e2>other</e2>, providing easily transferable information between tasks."
Conjunction(e1, e2)
Comment:

6350	"mula fuses mutually-tailored <e1>representations</e1> with the preliminary ones in a residual manner to produce augmented <e2>representations</e2> for making final prediction, through the classification modules."
sameAs(e1, e2)
Comment:

6351	"the experiment <e1>results</e1> well demonstrate its superiority over existing <e2>methods</e2> in exploiting mutual guidance information for joint human parsing and pose estimation."
Compare(e1, e2)
Comment:

6352	"introduction human <e1>parsing</e1> and pose estimation are two crucial yet challenging <e2>tasks</e2> for human body configuration analysis in 2d monocular images, which aim at segmenting human body into semantic parts and allocating body joints for human instances respectively."
isA(e1, e2)
Comment:

6353	"the top-down <e1>strategy</e1> [7, 8, 13, 20, 23] first detects the bottom-up <e2>strategy</e2> [3, 12, 11, 15, 16, 22] , in contrast, generates all joint candidates at first, and then tries to partition them to corresponding person instances."
sameAs(e1, e2)
Comment:

6354	"the top-down strategy [7, 8, 13, 20, 23] <e1>first</e1> detects the bottom-up strategy [3, 12, 11, 15, 16, 22] , in contrast, generates all joint candidates at <e2>first</e2>, and then tries to partition them to corresponding person instances."
sameAs(e1, e2)
Comment:

6355	"moreover, <e1>they</e1> suffer from high joint detection complexity, which linearly increases with the number of persons in the image, because <e2>they</e2> need to run the single-person joint detector for each person detection sequentially."
sameAs(e1, e2)
Comment:

6356	"however, they suffer from very high complexity of partitioning joints to corresponding persons, which usually involves solving np-hard <e1>graph</e1> partition problems [11, 22] on densely connected <e2>graphs</e2> covering the whole image."
sameAs(e1, e2)
Comment:

6357	"in this paper, we propose a novel solution, termed the <e1>pose</e1> partition network (ppn), to overcome essential limitations of the above two types of approaches and meanwhile inherit their strengths within a unified model for efficiently and effectively estimating <e2>poses</e2> of multiple persons in a given image."
sameAs(e1, e2)
Comment:

6358	"this local optimization <e1>strategy</e1> reduces the search space of the graph partition problem for finding optimal poses, avoiding high joint partition complexity challenging the bottom-up <e2>strategy</e2>."
sameAs(e1, e2)
Comment:

6359	"abstract we present a texture <e1>network</e1> called deep encoding  pooling <e2>network</e2> (dep) introduction ground terrain recognition is an important area of research in computer vision for potential applications in autonomous driving and robot navigation."
sameAs(e1, e2)
Comment:

6360	"outputs from convolutional <e1>layers</e1> are fed into two feature representation <e2>layers</e2> jointly; the encoding layer [38] and the global average pooling layer."
sameAs(e1, e2)
Comment:

6361	"recognition with cnns have achieved success in object recognition and the cnn architecture balances preservation of relative <e1>spatial information</e1> (with convolutional layers) and aggregation of <e2>spatial information</e2> (pooling layers)."
sameAs(e1, e2)
Comment:

6362	"recognition with cnns have achieved success in object recognition and the cnn architecture balances preservation of relative spatial information (with convolutional <e1>layers</e1>) and aggregation of spatial information (pooling <e2>layers</e2>)."
sameAs(e1, e2)
Comment:

6363	"we introduce a new texture manifold method, dep-manifold, to find the relationship between newly captured <e1>images</e1> and <e2>images</e2> in dataset."
sameAs(e1, e2)
Comment:

6364	"inspired by this work, we introduce a method for <e1>texture</e1> manifolds that treats the embedded distribution from non-parametric embedding algorithms as an output, and use a deep neural network to predict the manifold coordinates of a <e2>texture</e2> image directly."
sameAs(e1, e2)
Comment:

6365	"the training set is a ground terrain database (gtos) [36] with 31 classes of ground terrain <e1>images</e1> (over 30,000 <e2>images</e2> in the dataset)."
sameAs(e1, e2)
Comment:

6366	"recently, zhang et al [38] introduce deep <e1>texture</e1> encoding network (deep-ten) that ports the dictionary learning and feature pooling approaches into the cnn pipeline for an end-to-end material/<e2>texture</e2> recognition network."
sameAs(e1, e2)
Comment:

6367	"the first component is an attentional generative network, in which an attention mechanism is developed for the generator to draw different sub-<e1>regions</e1> of the image by focusing on words that are most relevant to the sub-<e2>region</e2> being drawn (see figure 1 )."
sameAs(e1, e2)
Comment:

6368	"it then combines the regional <e1>image</e1> vector and the corresponding word-context vector to form a multimodal context vector, based on which the model generates new <e2>image</e2> features in the surrounding sub-regions."
sameAs(e1, e2)
Comment:

6369	"with an attention mechanism, the damsm is able to compute the similarity between the generated image and the sentence using both the global sentence level <e1>information</e1> and the fine-grained word level <e2>information</e2>."
sameAs(e1, e2)
Comment:

6370	"it also drives <e1>research</e1> progress in multimodal learning and inference across vision and language, which is one of the most active <e2>research</e2> areas in recent years [20, 18, 36, 19, 41, 4, 30, 5, 1, 31, 33, 32] most recently proposed text-to-image synthesis methods are based on generative adversarial networks (gans) [6] ."
sameAs(e1, e2)
Comment:

6371	"it also drives research progress in multimodal learning and inference across vision and language, which is one of the most active research areas in recent years [20, 18, 36, 19, 41, 4, 30, 5, 1, 31, 33, 32] most recently proposed text-to-image synthesis methods are based on <e1>generative adversarial networks</e1> (<e2>gans</e2>) [6] ."
sameAs(e1, e2)
Comment:

6372	"this equation formulates the matting <e1>problem</e1> as a linear combination of two colors, and consequently most current algorithms approach this largely as a color <e2>problem</e2>."
sameAs(e1, e2)
Comment:

6373	"this equation formulates the matting problem as a linear combination of two <e1>colors</e1>, and consequently most current algorithms approach this largely as a <e2>color</e2> problem."
sameAs(e1, e2)
Comment:

6374	"such approaches rely largely on <e1>color</e1> as the distinguishing feature (often along with the spatial position of the pixels), making them incredibly sensitive to situations where the foreground and background <e2>color</e2> distributions overlap, which unfortunately for these methods is the common case for natural images, often leading to lowfrequency "smearing" or high-frequency "chunky" artifacts depending on the method (see fig 1 top row) ."
sameAs(e1, e2)
Comment:

6375	"such approaches rely largely on color as the distinguishing feature (often along with the spatial position of the pixels), making them incredibly sensitive to situations where the foreground and background color distributions overlap, which unfortunately for these <e1>methods</e1> is the common case for natural images, often leading to lowfrequency "smearing" or high-frequency "chunky" artifacts depending on the <e2>method</e2> (see fig 1 top row) ."
Compare(e1, e2)
Comment:

6376	"unfortunately, it contains only 27 training <e1>images</e1> and 8 test <e2>images</e2>, most of which are objects in front of an image on a monitor."
sameAs(e1, e2)
Comment:

6377	"a recent video matting dataset is available [10] with 3 training <e1>videos</e1> and 10 test <e2>videos</e2>, 5 of which were extracted from green screen footage and the"
sameAs(e1, e2)
Comment:

6378	"this is partially due to the difficulty of the <e1>problem</e1>: as formulated the matting <e2>problem</e2> is underconstrained with 7 unknown values per pixel but only 3 known values: i i = α i f i + (1 − α i )b i α i ∈ [0, 1]."
sameAs(e1, e2)
Comment:

6379	"(1) where the rgb <e1>color</e1> at pixel i, i i , is known and the foreground <e2>color</e2> f i , background color b i and matte estimation α i are unknown."
sameAs(e1, e2)
Comment:

6380	"(1) where the rgb <e1>color</e1> at pixel i, i i , is known and the foreground color f i , background <e2>color</e2> b i and matte estimation α i are unknown."
sameAs(e1, e2)
Comment:

6381	"(1) where the rgb color at pixel i, i i , is known and the foreground <e1>color</e1> f i , background <e2>color</e2> b i and matte estimation α i are unknown."
sameAs(e1, e2)
Comment:

6382	"we propose a <e1>scene</e1> graph generation model that takes an image as input, and generates a visually-grounded <e2>scene</e2> graph (second row, right) that captures the objects in the image (blue nodes) and their pairwise relationships (red nodes)."
sameAs(e1, e2)
Comment:

6383	"we propose a scene <e1>graph</e1> generation model that takes an image as input, and generates a visually-grounded scene <e2>graph</e2> (second row, right) that captures the objects in the image (blue nodes) and their pairwise relationships (red nodes)."
sameAs(e1, e2)
Comment:

6384	"we propose a scene graph generation model <e1>that</e1> takes an image as input, and generates a visually-grounded scene graph (second row, right) <e2>that</e2> captures the objects in the image (blue nodes) and their pairwise relationships (red nodes)."
sameAs(e1, e2)
Comment:

6385	"we propose a scene graph generation model that takes an <e1>image</e1> as input, and generates a visually-grounded scene graph (second row, right) that captures the objects in the <e2>image</e2> (blue nodes) and their pairwise relationships (red nodes)."
sameAs(e1, e2)
Comment:

6386	"we propose a scene graph generation model that takes an image as input, and generates a visually-grounded scene graph (second row, right) that captures the objects in the image (blue <e1>nodes</e1>) and their pairwise relationships (red <e2>nodes</e2>)."
sameAs(e1, e2)
Comment:

6387	"in short, a scene <e1>graph</e1> is a visually-grounded <e2>graph</e2> over the object instances in an image, where the edges depict their pairwise relationships (see example in fig."
sameAs(e1, e2)
Comment:

6388	"the value of <e1>scene</e1> graph representation has been proven in a wide range of visual tasks, such as semantic image retrieval [18] , 3d <e2>scene</e2> synthesis [4] , and visual question answering [37] ."
sameAs(e1, e2)
Comment:

6389	"however, these models that use <e1>scene</e1> graphs either rely on groundtruth annotations [18] , synthetic images [37] , or extract a <e2>scene</e2> graph from text domain [1, 4] ."
sameAs(e1, e2)
Comment:

6390	"however, these models that use <e1>scene</e1> graphs either rely on groundtruth annotations [18] , synthetic images [37] , or extract a scene graph from <e2>text</e2> domain [1, 4] ."
Conjunction(e1, e2)
Comment:

6391	"however, these models that use scene <e1>graphs</e1> either rely on groundtruth annotations [18] , synthetic images [37] , or extract a scene <e2>graph</e2> from text domain [1, 4] ."
sameAs(e1, e2)
Comment:

6392	"however, these models that use scene graphs either rely on groundtruth annotations [18] , synthetic images [37] , or extract a <e1>scene</e1> graph from <e2>text</e2> domain [1, 4] ."
Conjunction(e1, e2)
Comment:

6393	"in this work, we address the problem of <e1>scene</e1> graph generation, where the goal is to generate a visually-grounded <e2>scene</e2> graph from an image."
sameAs(e1, e2)
Comment:

6394	"in this work, we address the problem of scene <e1>graph</e1> generation, where the goal is to generate a visually-grounded scene <e2>graph</e2> from an image."
sameAs(e1, e2)
Comment:

6395	"however, by doing local <e1>predictions</e1> these models ignore surrounding context, whereas joint reasoning with contextual information can often resolve ambiguity due to local <e2>predictions</e2> in isolation."
sameAs(e1, e2)
Comment:

6396	"our major contribution is that instead of inferring each component of a <e1>scene</e1> graph in isolation, the model passes messages containing contextual information between a pair of bipartite sub-graphs of the <e2>scene</e2> graph, and iteratively refines its predictions using rnns."
sameAs(e1, e2)
Comment:

6397	"our major contribution is that instead of inferring each component of a scene <e1>graph</e1> in isolation, the model passes messages containing contextual information between a pair of bipartite sub-<e2>graphs</e2> of the scene graph, and iteratively refines its predictions using rnns."
sameAs(e1, e2)
Comment:

6398	"our major contribution is that instead of inferring each component of a scene <e1>graph</e1> in isolation, the model passes messages containing contextual information between a pair of bipartite sub-graphs of the scene <e2>graph</e2>, and iteratively refines its predictions using rnns."
sameAs(e1, e2)
Comment:

6399	"our major contribution is that instead of inferring each component of a scene graph in isolation, the model passes messages containing contextual information between a pair of bipartite sub-<e1>graphs</e1> of the scene <e2>graph</e2>, and iteratively refines its predictions using rnns."
sameAs(e1, e2)
Comment:

6400	"we evaluate our model on a new <e1>scene</e1> graph dataset based on visual genome [20] , which contains human-annotated <e2>scene</e2> graphs on 108,077 images."
sameAs(e1, e2)
Comment:

6401	"we evaluate our model on a new scene <e1>graph</e1> dataset based on visual genome [20] , which contains human-annotated scene <e2>graphs</e2> on 108,077 images."
sameAs(e1, e2)
Comment:

6402	"we demonstrate its use for generating semantic <e1>scene</e1> graphs from a new <e2>scene</e2> graph dataset as well as predicting support relations using the nyu depth v2 dataset [28] ."
sameAs(e1, e2)
Comment:

6403	"we demonstrate its use for generating semantic scene <e1>graphs</e1> from a new scene <e2>graph</e2> dataset as well as predicting support relations using the nyu depth v2 dataset [28] ."
sameAs(e1, e2)
Comment:

6404	"the rich semantic relationships between <e1>these</e1> objects have been largely untapped by <e2>these</e2> models."
sameAs(e1, e2)
Comment:

6405	"the recent success of deep learning-based recognition models [15, 21, 36] has surged interest in examining the detailed structures of a visual <e1>scene</e1>, especially in the form of  object detectors perceive a <e2>scene</e2> by attending to individual objects."
sameAs(e1, e2)
Comment:

6406	"abstract recent work on generative text modeling has found that variational autoencoders (vae) with <e1>lstm</e1> decoders perform worse than simpler <e2>lstm</e2> language models (bowman et al, 2015) ."
sameAs(e1, e2)
Comment:

6407	"by parameterzing generative models using neural nets, recent work has proposed <e1>model</e1> classes that are particularly expressive and can pontentially <e2>model</e2> a wide range of phenomena in language and other modalities."
sameAs(e1, e2)
Comment:

6408	"by parameterzing generative models using neural nets, recent work has proposed <e1>model</e1> classes that are particularly expressive and can pontentially model a wide range of phenomena in language and other <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

6409	"by parameterzing generative models using neural nets, recent work has proposed model classes that are particularly expressive and can pontentially <e1>model</e1> a wide range of phenomena in language and other <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

6410	"introduction generative models play an important role in <e1>nlp</e1>, both in their use as <e2>language models</e2> and because of their ability to effectively learn from unlabeled data."
Used-for(e1, e2)
Comment:

6411	"yet, in practice, many cases do require strategic interactions among a large number of <e1>agents</e1>, such as the gaming bots in massively multiplayer online role-playing game (jeong et al, 2015) , the trading <e2>agents</e2> in stock markets (troy, 1997) , or the online advertising bidding agents ."
sameAs(e1, e2)
Comment:

6412	"yet, in practice, many cases do require strategic interactions among a large number of <e1>agents</e1>, such as the gaming bots in massively multiplayer online role-playing game (jeong et al, 2015) , the trading agents in stock markets (troy, 1997) , or the online advertising bidding <e2>agents</e2> ."
sameAs(e1, e2)
Comment:

6413	"yet, in practice, many cases do require strategic interactions among a large number of agents, such as the gaming bots in massively multiplayer online role-playing game (jeong et al, 2015) , the trading <e1>agents</e1> in stock markets (troy, 1997) , or the online advertising bidding <e2>agents</e2> ."
sameAs(e1, e2)
Comment:

6414	"we consider a setting where each agent is directly interacting with a finite set of other <e1>agents</e1>; through a chain of direct interactions, any pair of <e2>agents</e2> is interconnected globally (blume, 1993) ."
sameAs(e1, e2)
Comment:

6415	"the learning is mutually reinforced between two entities rather than many entities: the learning of the individual agent's optimal policy is based on the <e1>dynamics</e1> of the agent population, meanwhile, the <e2>dynamics</e2> of the population is updated according to the individual policies."
sameAs(e1, e2)
Comment:

6416	"based on such formulation, we develop practical mean <e1>field</e1> q-learning and mean <e2>field</e2> actor-critic algorithms, and discuss the convergence of our solution under certain assumptions."
sameAs(e1, e2)
Comment:

6417	"in this paper, we present mean field reinforcement learning where the interactions within the population of <e1>agents</e1> are approximated by those between a single agent and the average effect from the overall population or neighboring <e2>agents</e2>; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the dynamics of the population, while the dynamics of the population change according to the collective patterns of the individual policies."
sameAs(e1, e2)
Comment:

6418	"in this paper, we present mean field reinforcement learning where the interactions within the population of agents are approximated by those between a single agent and the average effect from the overall population or neighboring agents; the interplay between the two entities is mutually reinforced: the learning of the individual agent's optimal policy depends on the <e1>dynamics</e1> of the population, while the <e2>dynamics</e2> of the population change according to the collective patterns of the individual policies."
sameAs(e1, e2)
Comment:

6419	"we develop practical mean <e1>field</e1> q-learning and mean <e2>field</e2> actor-critic algorithms and analyze the convergence of the solution to nash equilibrium."
sameAs(e1, e2)
Comment:

6420	"independent qlearning (tan, 1993 ) <e1>that</e1> considers other agents as a part of the environment often fails as the multi-agent setting breaks the theoretical convergence guarantee and makes the learning unstable: changes in the policy of one agent will affect <e2>that</e2> of the others, and vice versa (matignon et al, 2012) ."
sameAs(e1, e2)
Comment:

6421	"independent qlearning (tan, 1993 ) that considers <e1>other</e1> agents as a part of the environment often fails as the multi-agent setting breaks the theoretical convergence guarantee and makes the learning unstable: changes in the policy of one agent will affect that of the <e2>others</e2>, and vice versa (matignon et al, 2012) ."
sameAs(e1, e2)
Comment:

6422	"independent qlearning (tan, 1993 ) that considers other agents as a part of the environment often fails as the multi-agent setting breaks the theoretical convergence guarantee and makes the learning unstable: changes in the policy of <e1>one</e1> agent will affect that of the <e2>others</e2>, and vice versa (matignon et al, 2012) ."
Conjunction(e1, e2)
Comment:

6423	"compared to the traditional <e1>convolution</e1> operator, atrous <e2>convolution</e2> is able to achieve a larger receptive field size without increasing the numbers of kernel parameters."
sameAs(e1, e2)
Comment:

6424	"although atrous convolution solves the contradiction between <e1>feature</e1> map resolution and receptive field size, a method that simply generates a semantic mask from atrous-convolved <e2>feature</e2> map still suffers from a limitation."
sameAs(e1, e2)
Comment:

6425	"although atrous convolution solves the contradiction between feature <e1>map</e1> resolution and receptive field size, a method that simply generates a semantic mask from atrous-convolved feature <e2>map</e2> still suffers from a limitation."
sameAs(e1, e2)
Comment:

6426	"to this end, aspp [2, 3] proposed to concatenate <e1>feature</e1> maps generated by atrous convolution with different dilation rates, so that the neurons in the output <e2>feature</e2> map contain multiple receptive field sizes, which encode multiscale info and eventually boost performance."
sameAs(e1, e2)
Comment:

6427	"to this end, aspp [2, 3] proposed to concatenate feature <e1>maps</e1> generated by atrous convolution with different dilation rates, so that the neurons in the output feature <e2>map</e2> contain multiple receptive field sizes, which encode multiscale info and eventually boost performance."
sameAs(e1, e2)
Comment:

6428	"it uses dense connections to feed the output of each atrous <e1>convolution</e1> layer to all unvisited atrous <e2>convolution</e2> layers ahead, see fig."
sameAs(e1, e2)
Comment:

6429	"and by the series of <e1>feature</e1> concatenations, neurons at each intermediate <e2>feature</e2> map encode semantic information from multiple scales, and different intermediate feature maps encode multi-scale information from different scale ranges."
sameAs(e1, e2)
Comment:

6430	"and by the series of <e1>feature</e1> concatenations, neurons at each intermediate feature map encode semantic information from multiple scales, and different intermediate <e2>feature</e2> maps encode multi-scale information from different scale ranges."
sameAs(e1, e2)
Comment:

6431	"and by the series of feature concatenations, neurons at each intermediate <e1>feature</e1> map encode semantic information from multiple scales, and different intermediate <e2>feature</e2> maps encode multi-scale information from different scale ranges."
sameAs(e1, e2)
Comment:

6432	"and by the series of feature concatenations, neurons at each intermediate feature <e1>map</e1> encode semantic information from multiple scales, and different intermediate feature <e2>maps</e2> encode multi-scale information from different scale ranges."
sameAs(e1, e2)
Comment:

6433	"and by the series of feature concatenations, neurons at each intermediate feature map encode semantic information from multiple <e1>scales</e1>, and different intermediate feature maps encode multi-<e2>scale</e2> information from different scale ranges."
sameAs(e1, e2)
Comment:

6434	"and by the series of feature concatenations, neurons at each intermediate feature map encode semantic information from multiple <e1>scales</e1>, and different intermediate feature maps encode multi-scale information from different <e2>scale</e2> ranges."
sameAs(e1, e2)
Comment:

6435	"and by the series of feature concatenations, neurons at each intermediate feature map encode semantic information from multiple scales, and different intermediate feature maps encode multi-<e1>scale</e1> information from different <e2>scale</e2> ranges."
sameAs(e1, e2)
Comment:

6436	"on the other hand, if <e1>we</e1> output the segmentation from an early layer with larger resolution, <e2>we</e2> were not able to make use of higher figure 1 ."
sameAs(e1, e2)
Comment:

6437	"we first exploit and reorganize the <e1>videos</e1> in activitynet to form a new dataset for video re-localization research, which consists of about 10,000 <e2>videos</e2> of diverse visual appearances associated with the localized boundary information."
sameAs(e1, e2)
Comment:

6438	"subsequently, we propose an innovative cross gated bilinear matching model such that every time-step in the reference <e1>video</e1> is matched against the attentively weighted query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6439	"the tags of a <e1>video</e1> are user specified and it is unlikely for a user to tag all the content in a complex <e2>video</e2>."
sameAs(e1, e2)
Comment:

6440	"given the top query <e1>video</e1>, <e2>video</e2> re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom video, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6441	"given the top query <e1>video</e1>, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle <e2>video</e2> and the bottom video, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6442	"given the top query <e1>video</e1>, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom <e2>video</e2>, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6443	"given the top query <e1>video</e1>, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom video, which semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6444	"given the top query video, <e1>video</e1> re-localization aims to accurately detect the starting and ending points of the green segment in the middle <e2>video</e2> and the bottom video, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6445	"given the top query video, <e1>video</e1> re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom <e2>video</e2>, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6446	"given the top query video, <e1>video</e1> re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom video, which semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6447	"given the top query video, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle <e1>video</e1> and the bottom <e2>video</e2>, which semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6448	"given the top query video, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle <e1>video</e1> and the bottom video, which semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6449	"given the top query video, video re-localization aims to accurately detect the starting and ending points of the green segment in the middle video and the bottom <e1>video</e1>, which semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6450	"after retrieving <e1>videos</e1>, the user will have many <e2>videos</e2> in hand."
sameAs(e1, e2)
Comment:

6451	"thus <e1>video</e1> summarization methods [30, 21] are proposed to create a brief synopsis of a long <e2>video</e2>."
sameAs(e1, e2)
Comment:

6452	"users are able to get the general idea of a long <e1>video</e1> quickly with the help of <e2>video</e2> summarization."
sameAs(e1, e2)
Comment:

6453	"similar to <e1>video</e1> summarization, <e2>video</e2> captioning aims to summarize a video using one or more sentences."
sameAs(e1, e2)
Comment:

6454	"similar to <e1>video</e1> summarization, video captioning aims to summarize a <e2>video</e2> using one or more sentences."
sameAs(e1, e2)
Comment:

6455	"similar to video summarization, <e1>video</e1> captioning aims to summarize a <e2>video</e2> using one or more sentences."
sameAs(e1, e2)
Comment:

6456	"for example, given a query <e1>video</e1> and a reference <e2>video</e2>, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query video?"
sameAs(e1, e2)
Comment:

6457	"for example, given a query <e1>video</e1> and a reference video, how to accurately localize a segment in the reference <e2>video</e2> such that the segment semantically corresponds to the query video?"
sameAs(e1, e2)
Comment:

6458	"for example, given a query <e1>video</e1> and a reference video, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query <e2>video</e2>?"
sameAs(e1, e2)
Comment:

6459	"for example, given a query video and a reference <e1>video</e1>, how to accurately localize a segment in the reference <e2>video</e2> such that the segment semantically corresponds to the query video?"
sameAs(e1, e2)
Comment:

6460	"for example, given a query video and a reference <e1>video</e1>, how to accurately localize a segment in the reference video such that the segment semantically corresponds to the query <e2>video</e2>?"
sameAs(e1, e2)
Comment:

6461	"for example, given a query video and a reference video, how to accurately localize a segment in the reference <e1>video</e1> such that the segment semantically corresponds to the query <e2>video</e2>?"
sameAs(e1, e2)
Comment:

6462	"an alternative <e1>approach</e1> is relying on the action localization <e2>methods</e2>."
Compare(e1, e2)
Comment:

6463	"motivated by this example, we define a distinctively new task called <e1>video</e1> re-localization, which aims at localizing a segment in a reference <e2>video</e2> such that the segment semantically corresponds to a query video."
sameAs(e1, e2)
Comment:

6464	"motivated by this example, we define a distinctively new task called <e1>video</e1> re-localization, which aims at localizing a segment in a reference video such that the segment semantically corresponds to a query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6465	"motivated by this example, we define a distinctively new task called video re-localization, which aims at localizing a segment in a reference <e1>video</e1> such that the segment semantically corresponds to a query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6466	"specifically, the inputs to the task are <e1>one</e1> query video and <e2>one</e2> reference video."
sameAs(e1, e2)
Comment:

6467	"specifically, the inputs to the task are one query <e1>video</e1> and one reference <e2>video</e2>."
sameAs(e1, e2)
Comment:

6468	"the reference <e1>video</e1> contains at least one segment semantically corresponding to the content in the query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6469	"with a query clip, a user can quickly find the content he is interested in by <e1>video</e1> re-localization, thus avoiding seeking in a long <e2>video</e2> manually."
sameAs(e1, e2)
Comment:

6470	"another key obstacle to <e1>video</e1> re-localization is the lack of <e2>video</e2> datasets that contain pairs of query and reference videos as well as the associated localization information."
sameAs(e1, e2)
Comment:

6471	"160 <e1>action</e1> classes are used for training and 20 <e2>action</e2> classes are used for validation."
sameAs(e1, e2)
Comment:

6472	"160 action <e1>classes</e1> are used for training and 20 action <e2>classes</e2> are used for validation."
sameAs(e1, e2)
Comment:

6473	"the feature of every reference <e1>video</e1> is matched with the attentively weighted query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6474	"in each <e1>matching</e1> step, the reference video feature and the query video feature are processed by factorized bilinear <e2>matching</e2> to generate their interaction results."
sameAs(e1, e2)
Comment:

6475	"in each matching step, the reference <e1>video</e1> feature and the query <e2>video</e2> feature are processed by factorized bilinear matching to generate their interaction results."
sameAs(e1, e2)
Comment:

6476	"in each matching step, the reference video <e1>feature</e1> and the query video <e2>feature</e2> are processed by factorized bilinear matching to generate their interaction results."
sameAs(e1, e2)
Comment:

6477	"since not all the parts in the reference <e1>video</e1> are equally relevant to the query <e2>video</e2>, a cross gating strategy is stacked before bilinear matching to preserve the most relevant information while gating out the irrelevant information."
sameAs(e1, e2)
Comment:

6478	"for each <e1>time</e1> step, the recurrent unit outputs the probability that the <e2>time</e2> step belongs to one of the four classes: starting point, ending point, inside the segment, and outside the segment."
sameAs(e1, e2)
Comment:

6479	"in summary, our contributions lie in four-fold: 1. we introduce a novel task, namely <e1>video</e1> re-localization, which aims at localizing a segment in the reference <e2>video</e2> such that the segment semantically corresponds to the given query video."
sameAs(e1, e2)
Comment:

6480	"in summary, our contributions lie in four-fold: 1. we introduce a novel task, namely <e1>video</e1> re-localization, which aims at localizing a segment in the reference video such that the segment semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6481	"in summary, our contributions lie in four-fold: 1. we introduce a novel task, namely video re-localization, which aims at localizing a segment in the reference <e1>video</e1> such that the segment semantically corresponds to the given query <e2>video</e2>."
sameAs(e1, e2)
Comment:

6482	"data-driven approaches followed, where <e1>face</e1> age progression was primarily carried out by applying the prototype of aging details to test <e2>faces</e2> [13] [29] , or by modeling the dependency between longitudinal facial changes and corresponding ages [28] [34] [20] ."
sameAs(e1, e2)
Comment:

6483	"to be specific, the proposed <e1>approach</e1> uses a convolutional neural networks (cnn) based generator to learn age transformation, and it separately <e2>models</e2> different face attributes depending upon their changes over time."
Used-for(e1, e2)
Comment:

6484	"the <e1>training</e1> critic thus incorporates the squared euclidean loss in the image space, the gan loss that encourages generated faces to be indistinguishable from the elderly faces in the <e2>training</e2> set in terms of age, and the identity loss which minimizes the input-output distance by a high-level feature representation embedding personalized characteristics."
sameAs(e1, e2)
Comment:

6485	"the training critic thus incorporates the squared euclidean loss in the image space, the gan loss that encourages generated <e1>faces</e1> to be indistinguishable from the elderly <e2>faces</e2> in the training set in terms of age, and the identity loss which minimizes the input-output distance by a high-level feature representation embedding personalized characteristics."
sameAs(e1, e2)
Comment:

6486	"additionally, in contrast to the previous techniques <e1>that</e1> primarily operate on cropped facial areas (usually excluding foreheads), we emphasize <e2>that</e2> synthesis of the entire face is important since the parts of forehead and hair also significantly impact the perceived age."
sameAs(e1, e2)
Comment:

6487	"the main contributions of this study include: (1) <e1>we</e1> propose a novel gan based method for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) <e2>we</e2> highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) we set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6488	"the main contributions of this study include: (1) <e1>we</e1> propose a novel gan based method for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) we highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) <e2>we</e2> set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6489	"the main contributions of this study include: (1) we propose a novel <e1>gan</e1> based <e2>method</e2> for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) we highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) we set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
Part-of(e1, e2)
Comment:

6490	"the main contributions of this study include: (1) we propose a novel gan based method for age progression, which incorporates <e1>face</e1> verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) we highlight the importance of the forehead and hair components of a <e2>face</e2> that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) we set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6491	"the main contributions of this study include: (1) we propose a novel gan based method for age progression, which incorporates <e1>face</e1> verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) we highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) we set up new validating experiments in addition to existent ones, including commercial <e2>face</e2> analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6492	"the main contributions of this study include: (1) we propose a novel gan based method for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) <e1>we</e1> highlight the importance of the forehead and hair components of a face that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) <e2>we</e2> set up new validating experiments in addition to existent ones, including commercial face analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6493	"the main contributions of this study include: (1) we propose a novel gan based method for age progression, which incorporates face verification and age estimation techniques, thereby addressing the issues of aging effect generation and identity cue preservation in a coupled manner; (2) we highlight the importance of the forehead and hair components of a <e1>face</e1> that are closely related to the perceived age but ignored in other studies; it indeed enhances the synthesized age accuracy; (3) we set up new validating experiments in addition to existent ones, including commercial <e2>face</e2> analysis tool based evaluation and insensitivity assessment to the changes in expression, pose, and makeup."
sameAs(e1, e2)
Comment:

6494	"one naive approach would be representing a video <e1>face</e1> as a set of frame-level <e2>face</e2> features such as those extracted from deep neural networks [35, 31] , which have dominated face recognition recently [35, 28, 33, 31, 23, 41] ."
sameAs(e1, e2)
Comment:

6495	"however, to compare <e1>two</e1> video faces, one needs to fuse the matching results across all pairs of frames between the <e2>two</e2> face videos."
sameAs(e1, e2)
Comment:

6496	"however, to compare two video <e1>faces</e1>, one needs to fuse the matching results across all pairs of frames between the two <e2>face</e2> videos."
sameAs(e1, e2)
Comment:

6497	"the most commonly adopted <e1>pooling</e1> strategies may be average and max <e2>pooling</e2> [28, 21, 7, 9] ."
sameAs(e1, e2)
Comment:

6498	"while these naive <e1>pooling</e1> strategies were shown to be effective in the previous works, we believe that a good <e2>pooling</e2> or aggregation strategy should adaptively weigh and combine the framelevel features across all frames."
sameAs(e1, e2)
Comment:

6499	"the intuition is simple: a video (especially a long video sequence) or an image set may contain <e1>face images</e1> captured at various conditions of lighting, resolution, head pose etc., and a smart algorithm should favor <e2>face images</e2> that are more discriminative (or more "memorizable") and prevent poor face images from jeopardizing the recognition."
sameAs(e1, e2)
Comment:

6500	"the intuition is simple: a video (especially a long video sequence) or an image set may contain <e1>face images</e1> captured at various conditions of lighting, resolution, head pose etc., and a smart algorithm should favor face images that are more discriminative (or more "memorizable") and prevent poor <e2>face images</e2> from jeopardizing the recognition."
sameAs(e1, e2)
Comment:

6501	"the intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of <e1>lighting</e1>, resolution, head <e2>pose</e2> etc., and a smart algorithm should favor face images that are more discriminative (or more "memorizable") and prevent poor face images from jeopardizing the recognition."
Conjunction(e1, e2)
Comment:

6502	"the intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of lighting, <e1>resolution</e1>, head pose etc., and a smart algorithm should favor <e2>face images</e2> that are more discriminative (or more "memorizable") and prevent poor face images from jeopardizing the recognition."
Feature-of(e1, e2)
Comment:

6503	"the intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of lighting, <e1>resolution</e1>, head pose etc., and a smart algorithm should favor face images that are more discriminative (or more "memorizable") and prevent poor <e2>face images</e2> from jeopardizing the recognition."
Feature-of(e1, e2)
Comment:

6504	"the intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of lighting, resolution, head pose etc., and a smart algorithm should favor <e1>face images</e1> that are more discriminative (or more "memorizable") and prevent poor <e2>face images</e2> from jeopardizing the recognition."
sameAs(e1, e2)
Comment:

6505	"we named our <e1>network</e1> the neural aggregation <e2>network</e2> (nan), whose coefficients can be trained through supervised learning in a normal face recognition training task without the need for extra supervision signals."
sameAs(e1, e2)
Comment:

6506	"the proposed nan is composed of two major modules that could be trained end-to-end or <e1>one</e1> by <e2>one</e2> separately."
sameAs(e1, e2)
Comment:

6507	"the first one is a <e1>feature</e1> embedding module which serves as a frame-level <e2>feature</e2> extractor using a deep cnn model."
sameAs(e1, e2)
Comment:

6508	"the key issue in <e1>video</e1> face recognition is to build an appropriate representation of the <e2>video</e2> face, such that it can effectively integrate the information across different frames together, maintaining beneficial while discarding noisy information."
sameAs(e1, e2)
Comment:

6509	"the key issue in video face recognition is to build an appropriate representation of the video face, such that it can effectively integrate the <e1>information</e1> across different frames together, maintaining beneficial while discarding noisy <e2>information</e2>."
sameAs(e1, e2)
Comment:

6510	"this mechanism can take an <e1>input</e1> of arbitrary size and work as a tailor emphasizing or suppressing each <e2>input</e2> element just via a weighted averaging, and very importantly it is order independent and has trainable parameters."
sameAs(e1, e2)
Comment:

6511	"in <e1>this</e1> work, we design a simple network structure of two cascaded attention blocks associated with <e2>this</e2> attention mechanism for face feature aggregation."
sameAs(e1, e2)
Comment:

6512	"for example, one can feed it with all available images and <e1>videos</e1>, or the aggregated video-level features of multiple <e2>videos</e2> from the same subject, to obtain a single feature representation with fixed size."
sameAs(e1, e2)
Comment:

6513	"with the advance of <e1>texture</e1> descriptors [37] , martin et al [39] combined color, brightness and <e2>texture</e2> gradients in their probabilistic boundary detector."
sameAs(e1, e2)
Comment:

6514	"with the advance of texture descriptors [37] , martin et al [39] combined <e1>color</e1>, brightness and <e2>texture</e2> gradients in their probabilistic boundary detector."
Conjunction(e1, e2)
Comment:

6515	"given <e1>image</e1>-contour pairs, we formulate object contour detection as an <e2>image</e2> labeling problem."
sameAs(e1, e2)
Comment:

6516	"being fully convolutional, our cedn <e1>network</e1> can operate on arbitrary image size and the encoder-decoder <e2>network</e2> emphasizes its asymmetric structure that differs from deconvolutional network [40] ."
sameAs(e1, e2)
Comment:

6517	"being fully convolutional, our cedn <e1>network</e1> can operate on arbitrary image size and the encoder-decoder network emphasizes its asymmetric structure that differs from deconvolutional <e2>network</e2> [40] ."
sameAs(e1, e2)
Comment:

6518	"being fully convolutional, our cedn network can operate on arbitrary image size and the encoder-decoder <e1>network</e1> emphasizes its asymmetric structure that differs from deconvolutional <e2>network</e2> [40] ."
sameAs(e1, e2)
Comment:

6519	"we initialize our <e1>encoder</e1> with vgg-16 net [48] (up to the "fc6" layer) and to achieve dense prediction of image size our decoder is constructed by alternating unpooling and convolution layers where unpooling layers re-use the switches from max-pooling layers of <e2>encoder</e2> to upscale the feature maps."
sameAs(e1, e2)
Comment:

6520	"we initialize our encoder with vgg-16 net [48] (up to the "fc6" layer) and to achieve dense prediction of image size our decoder is constructed by alternating unpooling and convolution <e1>layers</e1> where unpooling <e2>layers</e2> re-use the switches from max-pooling layers of encoder to upscale the feature maps."
sameAs(e1, e2)
Comment:

6521	"we initialize our encoder with vgg-16 net [48] (up to the "fc6" layer) and to achieve dense prediction of image size our decoder is constructed by alternating unpooling and convolution <e1>layers</e1> where unpooling layers re-use the switches from max-pooling <e2>layers</e2> of encoder to upscale the feature maps."
sameAs(e1, e2)
Comment:

6522	"we initialize our encoder with vgg-16 net [48] (up to the "fc6" layer) and to achieve dense prediction of image size our decoder is constructed by alternating unpooling and convolution layers where unpooling <e1>layers</e1> re-use the switches from max-pooling <e2>layers</e2> of encoder to upscale the feature maps."
sameAs(e1, e2)
Comment:

6523	"during training, we fix the encoder <e1>parameters</e1> (vgg-16) and only optimize decoder <e2>parameters</e2>."
sameAs(e1, e2)
Comment:

6524	"we evaluate the trained <e1>network</e1> on unseen object categories from bsds500 and ms coco datasets [33] , and find the <e2>network</e2> generalizes well to objects in similar "super-categories" to those in the training set, e.g."
sameAs(e1, e2)
Comment:

6525	"we evaluate the trained network on unseen <e1>object categories</e1> from bsds500 and ms coco datasets [33] , and find the network generalizes well to objects in similar "super-<e2>categories</e2>" to those in the training set, e.g."
isA(e1, e2)
Comment:

6526	"as a result, our method significantly improves the quality of segmented object proposals on the pascal voc 2012 validation set, achieving 0.67 average <e1>recall</e1> from overlap 0.5 to 1.0 with only about 1660 candidates per image, compared to the 0.62 average <e2>recall</e2> by original mcg algorithm with near 5140 candidates per image."
sameAs(e1, e2)
Comment:

6527	"as a result, our method significantly improves the quality of segmented object proposals on the pascal voc 2012 validation set, achieving 0.67 average recall from overlap 0.5 to 1.0 with only about 1660 <e1>candidates</e1> per image, compared to the 0.62 average recall by original mcg algorithm with near 5140 <e2>candidates</e2> per image."
sameAs(e1, e2)
Comment:

6528	"as a result, our method significantly improves the quality of segmented object proposals on the pascal voc 2012 validation set, achieving 0.67 average recall from overlap 0.5 to 1.0 with only about 1660 candidates <e1>per</e1> image, compared to the 0.62 average recall by original mcg algorithm with near 5140 candidates <e2>per</e2> image."
sameAs(e1, e2)
Comment:

6529	"as a result, our method significantly improves the quality of segmented object proposals on the pascal voc 2012 validation set, achieving 0.67 average recall from overlap 0.5 to 1.0 with only about 1660 candidates per <e1>image</e1>, compared to the 0.62 average recall by original mcg algorithm with near 5140 candidates per <e2>image</e2>."
sameAs(e1, e2)
Comment:

6530	"we also evaluate object proposals on the ms coco dataset with 80 object <e1>classes</e1> and analyze the average recalls from different object <e2>classes</e2> and their supercategories."
sameAs(e1, e2)
Comment:

6531	"our key contributions are summarized below: • we develop a simple yet effective fully convolutional encoder-decoder network for object contour detection and the trained <e1>model</e1> generalizes well to unseen object classes from the same super-categories, yielding significantly higher precision than previous <e2>methods</e2>."
Compare(e1, e2)
Comment:

6532	"• <e1>we</e1> show <e2>we</e2> can fine tune our network for edge detection and match the state-of-the-art in terms of precision and recall."
sameAs(e1, e2)
Comment:

6533	"• we show we can fine tune our network for edge detection and match the state-of-the-art in terms of <e1>precision</e1> and <e2>recall</e2>."
Conjunction(e1, e2)
Comment:

6534	"the main problem with filter based methods is that they only look at the <e1>color</e1> or brightness differences between adjacent pixels but cannot tell the <e2>texture</e2> differences in a larger receptive field."
Conjunction(e1, e2)
Comment:

6535	"one example is the existence of adversarial samples [37] , when <e1>we</e1> add small noises or make some small changes to the initial samples, cnn will give different predictions for these samples with high confidence, al- though visually <e2>we</e2> can hardly find any significant changes in the images."
sameAs(e1, e2)
Comment:

6536	"the main reasons for <e1>these</e1> problems include two aspects: first, cnn is a purely discriminative model, it essentially learns a partition of the "whole" feature space, therefore, the samples from unseen classes are still predicted to some specific regions under the partition, and cnn still views <e2>these</e2> samples as some known classes with high confidence."
sameAs(e1, e2)
Comment:

6537	"the main reasons for these problems include two aspects: first, <e1>cnn</e1> is a purely discriminative model, it essentially learns a partition of the "whole" feature space, therefore, the samples from unseen classes are still predicted to some specific regions under the partition, and <e2>cnn</e2> still views these samples as some known classes with high confidence."
sameAs(e1, e2)
Comment:

6538	"the main reasons for these problems include two aspects: first, cnn is a purely discriminative model, it essentially learns a partition of the "whole" feature space, therefore, the samples from unseen <e1>classes</e1> are still predicted to some specific regions under the partition, and cnn still views these samples as some known <e2>classes</e2> with high confidence."
sameAs(e1, e2)
Comment:

6539	"this explains why the rejection ability of <e1>cnn</e1> is poor; second, from the perspective of representation learning, the learned representation of <e2>cnn</e2> is linear separable, see fig."
sameAs(e1, e2)
Comment:

6540	"1 for an illustration, and under <e1>this</e1> kind of representation, the interclass distance is sometimes even smaller than the intra-class distance, <e2>this</e2> significantly reduces the robustness of cnn in real and complicated environments."
sameAs(e1, e2)
Comment:

6541	"1 for an illustration, and under this kind of representation, the interclass <e1>distance</e1> is sometimes even smaller than the intra-class <e2>distance</e2>, this significantly reduces the robustness of cnn in real and complicated environments."
sameAs(e1, e2)
Comment:

6542	"despite its high accuracies, <e1>cnn</e1> has been shown to be easily fooled by some adversarial examples, indicating that <e2>cnn</e2> is not robust enough for pattern classification."
sameAs(e1, e2)
Comment:

6543	"[38] proposed the center loss to improve the performance of softmax-based <e1>cnn</e1>, however, the centers can not be learned jointly with the <e2>cnn</e2> and are only updated according to some pre-defined rules rather than learned directly from data."
sameAs(e1, e2)
Comment:

6544	"from the perspective of probability, our cpl and pl <e1>framework</e1> essentially extract, transform, and model the data of each class as a gaussian mixture distribution and the prototypes act as the means of gaussian components for each class, this enables integrating probabilistic methods such as bayesian models into our <e2>framework</e2>."
sameAs(e1, e2)
Comment:

6545	"from the perspective of probability, our cpl and pl framework essentially extract, transform, and <e1>model</e1> the data of each class as a gaussian mixture distribution and the prototypes act as the means of gaussian components for each class, this enables integrating probabilistic <e2>methods</e2> such as bayesian models into our framework."
Compare(e1, e2)
Comment:

6546	"from the perspective of probability, our cpl and pl framework essentially extract, transform, and model the <e1>data</e1> of each class as a gaussian mixture distribution and the prototypes act as the means of gaussian components for each class, this enables integrating probabilistic methods such as bayesian <e2>models</e2> into our framework."
Used-for(e1, e2)
Comment:

6547	"from the perspective of probability, our cpl and pl framework essentially extract, transform, and model the data of each class as a gaussian mixture distribution and the prototypes act as the means of gaussian components for each class, this enables integrating probabilistic <e1>methods</e1> such as bayesian <e2>models</e2> into our framework."
Compare(e1, e2)
Comment:

6548	"compared with the traditional cnn framework, we do not make partition for the "whole" <e1>feature space</e1>, but project the samples to some specific regions of the <e2>feature space</e2> (near the prototypes), thus our model is more robust to samples from unseen classes and more suitable for rejection."
sameAs(e1, e2)
Comment:

6549	"introduction in recent years, convolutional neural networks [16] (cnns) have achieved great success for <e1>pattern recognition</e1> and computer vision, leading to important progress in a variety of tasks, like image classification [15, 8, 7, 34] , <e2>object detection</e2> [1, 29, 9] , instance segmentation [28, 6] and so on."
Used-for(e1, e2)
Comment:

6550	"as taylor series expansion of the gaussian <e1>function</e1> can be used to approximate the bilateral filters [38] , [51] learns a <e2>function</e2> that maps feature vector comprising of the exponentiation of the pixel intensity, the corresponding gaussian filtered response, and their products to the corresponding exact bilateral filtered values from the training image."
sameAs(e1, e2)
Comment:

6551	"it is an <e1>image</e1> smoothing technique that removes low-contrast details/textures while maintaining sharp edges/<e2>image</e2> structures."
sameAs(e1, e2)
Comment:

6552	"unlike previous filters, it can sufficiently suppress image variance/textures inside large <e1>objects</e1> while maintaining the structures of small <e2>objects</e2> as shown in (g)."
sameAs(e1, e2)
Comment:

6553	"unfortunately, many visually salient <e1>edges</e1> like texture <e2>edges</e2> do not correspond to image gradients."
sameAs(e1, e2)
Comment:

6554	"as a result, most of the state-of-the-art <e1>edge</e1>-preserving filters ignore the potential contribution from an <e2>edge</e2> detector."
sameAs(e1, e2)
Comment:

6555	"this paper proposes a simple seamless combination of the recursive filtering <e1>technique</e1> and the learning-based edge classification <e2>technique</e2> for fast scale-aware edgepreserving filtering."
sameAs(e1, e2)
Comment:

6556	"the distance measurement is then converted to the confidence of an <e1>edge</e1> between the two pixels for <e2>edge</e2>-aware filtering."
sameAs(e1, e2)
Comment:

6557	"1 (a) contains two images with both large <e1>scale</e1> objects (e.g., sky and meadow) and small <e2>scale</e2> objects (e.g., animals)."
sameAs(e1, e2)
Comment:

6558	"1 (a) contains two images with both large scale <e1>objects</e1> (e.g., sky and meadow) and small scale <e2>objects</e2> (e.g., animals)."
sameAs(e1, e2)
Comment:

6559	"compared with the other edge-preserving filters, the proposed filtering technique has the following advantages: • it is robust to natural scenes containing <e1>objects</e1> of different sizes and structures of various scales, and thus can successfully extract subjectively-meaningful structures from images containing multiple-scale <e2>objects</e2>."
sameAs(e1, e2)
Comment:

6560	"compared with the other edge-preserving filters, the proposed filtering technique has the following advantages: • it is robust to natural scenes containing objects of different sizes and <e1>structures</e1> of various scales, and thus can successfully extract subjectively-meaningful <e2>structures</e2> from images containing multiple-scale objects."
sameAs(e1, e2)
Comment:

6561	"compared with the other edge-preserving filters, the proposed filtering technique has the following advantages: • it is robust to natural scenes containing objects of different sizes and structures of various <e1>scales</e1>, and thus can successfully extract subjectively-meaningful structures from images containing multiple-<e2>scale</e2> objects."
sameAs(e1, e2)
Comment:

6562	"in the beginning, <e1>face</e1> alignment that aims at detecting a special 2d fiducial points [66, 64, 38, 46] is commonly used as a prerequisite for other facial tasks such as face recognition [59] and assists 3d <e2>face</e2> reconstruction [68, 27] to a great extent."
sameAs(e1, e2)
Comment:

6563	"with the development of <e1>deep learning</e1>, many computer vision problems have been well solved by utilizing convolution <e2>neural networks</e2> (cnns)."
isA(e1, e2)
Comment:

6564	"thus, some works start to use cnns to estimate the 3d morphable model (3dmm) coefficients [32, 67, 47, 39, 48, 40] or 3d model warping functions [4, 53] to restore the corresponding 3d information from a single 2d facial image, which provides both dense <e1>face</e1> alignment and 3d <e2>face</e2> reconstruction results."
sameAs(e1, e2)
Comment:

6565	"[9] trains a complex <e1>network</e1> to regress 68 facial landmarks with 2d coordinates from a single image, but needs an extra <e2>network</e2> to estimate the depth value."
sameAs(e1, e2)
Comment:

6566	"to sum up, model-based <e1>methods</e1> keep semantic meaning of points well but are restricted in model space, recent model-free <e2>methods</e2> are unrestricted and achieve state-of-the-art performance but discard the semantic meaning, which motivate us to find a new approach to reconstruct 3d face with alignment information in a model-free manner."
sameAs(e1, e2)
Comment:

6567	"to sum up, model-based methods keep <e1>semantic</e1> meaning of points well but are restricted in model space, recent model-free methods are unrestricted and achieve state-of-the-art performance but discard the <e2>semantic</e2> meaning, which motivate us to find a new approach to reconstruct 3d face with alignment information in a model-free manner."
sameAs(e1, e2)
Comment:

6568	"to sum up, model-based methods keep semantic meaning of points well but are restricted in model space, recent <e1>model-free</e1> methods are unrestricted and achieve state-of-the-art performance but discard the semantic meaning, which motivate us to find a new approach to reconstruct 3d face with alignment information in a <e2>model-free</e2> manner."
sameAs(e1, e2)
Comment:

6569	"in summary, our main contributions are: -for the first time, we solve the problems of <e1>face</e1> alignment and 3d <e2>face</e2> reconstruction together in an end-to-end fashion without the restriction of low-dimensional solution space."
sameAs(e1, e2)
Comment:

6570	"-to directly regress the 3d facial structure and dense alignment, we develop a novel representation called uv <e1>position</e1> map, which records the <e2>position</e2> information of 3d face and provides dense correspondence to the semantic meaning of each point on uv space."
sameAs(e1, e2)
Comment:

6571	"-comparison on the aflw2000-3d and florence datasets shows that our method achieves more than 25% relative improvements over other stateof-the-art methods on both tasks of 3d <e1>face</e1> reconstruction and dense <e2>face</e2> alignment."
sameAs(e1, e2)
Comment:

6572	"our <e1>method</e1> does not rely on any prior face <e2>model</e2>, and can reconstruct full facial geometry along with semantic meaning."
Used-for(e1, e2)
Comment:

6573	"introduction 3d <e1>face</e1> reconstruction and <e2>face</e2> alignment are two fundamental and highly related topics in computer vision."
sameAs(e1, e2)
Comment:

6574	"they have shown promising <e1>results</e1> and gradually surpassed traditional <e2>methods</e2> in stereo benchmarks [9, 25] ."
Compare(e1, e2)
Comment:

6575	"to this end, we propose an end-to-end deep learning architecture for depth <e1>map</e1> inference, which computes one depth <e2>map</e2> at each time, rather than the whole 3d scene at once."
sameAs(e1, e2)
Comment:

6576	"similar to other depth <e1>map</e1> based mvs methods [35, 3, 8, 32] , the proposed network, mvsnet, takes one reference image and several source images as input, and infers the depth <e2>map</e2> for the reference image."
sameAs(e1, e2)
Comment:

6577	"similar to other depth map based mvs methods [35, 3, 8, 32] , the proposed network, mvsnet, takes one reference <e1>image</e1> and several source images as input, and infers the depth map for the reference <e2>image</e2>."
sameAs(e1, e2)
Comment:

6578	"for example, a sound <e1>network</e1> is learned in [6] by a visual teacher <e2>network</e2> with a large amount of unlabeled videos, which shows better performance than learning in a single modality."
sameAs(e1, e2)
Comment:

6579	"however, <e1>they</e1> have all assumed that the audio and visual contents in a video are matched (which is often not the case as we will show) and <e2>they</e2> are yet to explore whether the joint audio-visual representations can facilitate understanding unconstrained videos."
sameAs(e1, e2)
Comment:

6580	"however, they have all assumed that the <e1>audio</e1> and visual contents in a video are matched (which is often not the case as we will show) and they are yet to explore whether the joint <e2>audio</e2>-visual representations can facilitate understanding unconstrained videos."
sameAs(e1, e2)
Comment:

6581	"(q3) how does knowing <e1>one</e1> modality help model the <e2>other</e2> modality?"
Conjunction(e1, e2)
Comment:

6582	"(q3) how does knowing one <e1>modality</e1> help model the other <e2>modality</e2>?"
sameAs(e1, e2)
Comment:

6583	"(q5) can we locate the content in <e1>one</e1> modality given its observation in the <e2>other</e2> modality?"
Conjunction(e1, e2)
Comment:

6584	"(q5) can we locate the content in one <e1>modality</e1> given its observation in the other <e2>modality</e2>?"
sameAs(e1, e2)
Comment:

6585	"notice <e1>that</e1> the individual questions might be studied in the literature, but we are not aware of any work <e2>that</e2> conducts a systematic study to answer these collective questions as a whole."
sameAs(e1, e2)
Comment:

6586	"in particular, <e1>we</e1> define an audio-visual event as an event that is both visible and audible in a video segment, and <e2>we</e2> establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6587	"in particular, we define an <e1>audio</e1>-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised <e2>audio</e2>-visual event localization, 2) weakly-supervised audio-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6588	"in particular, we define an <e1>audio</e1>-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised <e2>audio</e2>-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6589	"in particular, we define an audio-visual <e1>event</e1> as an <e2>event</e2> that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6590	"in particular, we define an audio-visual <e1>event</e1> as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual <e2>event</e2> localization, 2) weakly-supervised audio-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6591	"in particular, we define an audio-visual <e1>event</e1> as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual <e2>event</e2> localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6592	"in particular, we define an audio-visual <e1>event</e1> as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual event localization, and 3) <e2>event</e2>-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6593	"in particular, we define an audio-visual event as an <e1>event</e1> that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual <e2>event</e2> localization, 2) weakly-supervised audio-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6594	"in particular, we define an audio-visual event as an <e1>event</e1> that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual <e2>event</e2> localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6595	"in particular, we define an audio-visual event as an <e1>event</e1> that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual event localization, and 3) <e2>event</e2>-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6596	"in particular, we define an audio-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised <e1>audio</e1>-visual event localization, 2) weakly-supervised <e2>audio</e2>-visual event localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6597	"in particular, we define an audio-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual <e1>event</e1> localization, 2) weakly-supervised audio-visual <e2>event</e2> localization, and 3) event-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6598	"in particular, we define an audio-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual <e1>event</e1> localization, 2) weakly-supervised audio-visual event localization, and 3) <e2>event</e2>-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6599	"in particular, we define an audio-visual event as an event that is both visible and audible in a video segment, and we establish three tasks to explore aforementioned research questions: 1) supervised audio-visual event localization, 2) weakly-supervised audio-visual <e1>event</e1> localization, and 3) <e2>event</e2>-agnostic crossmodality localization."
sameAs(e1, e2)
Comment:

6600	"the first two tasks aim to predict which temporal segment of an input video has an audio-visual <e1>event</e1> and what category the <e2>event</e2> belongs to."
sameAs(e1, e2)
Comment:

6601	"the weakly-supervised setting assumes that we have no access to the temporal <e1>event</e1> boundary but an <e2>event</e2> tag at video-level for training."
sameAs(e1, e2)
Comment:

6602	"q1-q4 will be explored within <e1>these</e1> two <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

6603	"upon this baseline model, we introduce an <e1>audio</e1>-guided visual attention mechanism to verify whether <e2>audio</e2> can help attend visual features; it also implies spatial locations for sounding objects as a side output."
sameAs(e1, e2)
Comment:

6604	"we define an audio-visual <e1>event</e1> as an <e2>event</e2> that is both visible and audible in a video segment."
sameAs(e1, e2)
Comment:

6605	"to address the harder cross-modality localization task, we propose an <e1>audio</e1>-visual distance learning network that measures the relativeness of any given pair of <e2>audio</e2> and visual content."
sameAs(e1, e2)
Comment:

6606	"observing <e1>that</e1> there is no publicly available dataset directly suitable for our tasks, we collect a large video dataset <e2>that</e2> consists of 4143 10-second videos with both audio and video tracks for 28 audio-visual events and annotate their temporal boundaries."
sameAs(e1, e2)
Comment:

6607	"observing that there is no publicly available dataset directly suitable for our tasks, we collect a large <e1>video</e1> dataset that consists of 4143 10-second videos with both audio and <e2>video</e2> tracks for 28 audio-visual events and annotate their temporal boundaries."
sameAs(e1, e2)
Comment:

6608	"observing that there is no publicly available dataset directly suitable for our tasks, we collect a large video dataset that consists of 4143 10-second videos with both <e1>audio</e1> and video tracks for 28 <e2>audio</e2>-visual events and annotate their temporal boundaries."
sameAs(e1, e2)
Comment:

6609	"our extensive experiments support the following findings: <e1>modeling</e1> jointly over auditory and visual modalities outperforms <e2>modeling</e2> independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6610	"our extensive experiments support the following findings: modeling jointly over auditory and visual <e1>modalities</e1> outperforms modeling independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two <e2>modalities</e2> enable crossmodality localization."
sameAs(e1, e2)
Comment:

6611	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, <e1>audio</e1>-visual event localization in a noisy condition can still achieve promising results, the <e2>audio</e2>-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6612	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, <e1>audio</e1>-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish <e2>audio</e2>-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6613	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, <e1>audio</e1>-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for <e2>audio</e2>-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6614	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the <e1>audio</e1>-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish <e2>audio</e2>-visual unrelated videos, temporal alignment is important for audio-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6615	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the <e1>audio</e1>-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish audio-visual unrelated videos, temporal alignment is important for <e2>audio</e2>-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6616	"our extensive experiments support the following findings: modeling jointly over auditory and visual modalities outperforms modeling independently over them, audio-visual event localization in a noisy condition can still achieve promising results, the audio-guided visual attention can well capture semantic regions covering sounding objects and can even distinguish <e1>audio</e1>-visual unrelated videos, temporal alignment is important for <e2>audio</e2>-visual fusion, the proposed dual multimodal residual network is effective in addressing the fusion task, and strong correlations between the two modalities enable crossmodality localization."
sameAs(e1, e2)
Comment:

6617	"these findings have paved a way for our community to solve harder, high-level <e1>understanding</e1> problems in the future, such as video captioning [56] and movieqa [53] , where the auditory modality plays an important role in <e2>understanding</e2> video but lacks effective modeling."
sameAs(e1, e2)
Comment:

6618	"these findings have paved a way for our community to solve harder, high-level understanding problems in the future, such as <e1>video</e1> captioning [56] and movieqa [53] , where the auditory modality plays an important role in understanding <e2>video</e2> but lacks effective modeling."
sameAs(e1, e2)
Comment:

6619	"our work makes the following contributions: (1) a family of three <e1>audio</e1>-visual event localization tasks; (2) an <e2>audio</e2>-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6620	"our work makes the following contributions: (1) a family of three <e1>audio</e1>-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the <e2>audio</e2>-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6621	"our work makes the following contributions: (1) a family of three <e1>audio</e1>-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse <e2>audio</e2>-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6622	"our work makes the following contributions: (1) a family of three <e1>audio</e1>-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective <e2>audio</e2>-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6623	"our work makes the following contributions: (1) a family of three <e1>audio</e1>-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large <e2>audio</e2>-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6624	"our work makes the following contributions: (1) a family of three audio-visual <e1>event</e1> localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual <e2>event</e2> dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6625	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an <e1>audio</e1>-guided visual attention model to adaptively explore the <e2>audio</e2>-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6626	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an <e1>audio</e1>-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse <e2>audio</e2>-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6627	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an <e1>audio</e1>-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective <e2>audio</e2>-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6628	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an <e1>audio</e1>-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large <e2>audio</e2>-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6629	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the <e1>audio</e1>-visual correlations; (3) a novel dual multimodal residual network to fuse <e2>audio</e2>-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6630	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the <e1>audio</e1>-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective <e2>audio</e2>-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6631	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the <e1>audio</e1>-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large <e2>audio</e2>-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6632	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual <e1>network</e1> to fuse audio-visual features; (4) an effective audio-visual distance learning <e2>network</e2> to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6633	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse <e1>audio</e1>-visual features; (4) an effective <e2>audio</e2>-visual distance learning network to address cross-modality localization; (5) a large audio-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6634	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse <e1>audio</e1>-visual features; (4) an effective audio-visual distance learning network to address cross-modality localization; (5) a large <e2>audio</e2>-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6635	"our work makes the following contributions: (1) a family of three audio-visual event localization tasks; (2) an audio-guided visual attention model to adaptively explore the audio-visual correlations; (3) a novel dual multimodal residual network to fuse audio-visual features; (4) an effective <e1>audio</e1>-visual distance learning network to address cross-modality localization; (5) a large <e2>audio</e2>-visual event dataset containing more than 4k unconstrained and annotated videos, which to the best of our knowledge, is the largest dataset for sound event detection."
sameAs(e1, e2)
Comment:

6636	"we collect an <e1>audio</e1>-visual event (ave) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised <e2>audio</e2>-visual event localization, and cross-modality localization."
sameAs(e1, e2)
Comment:

6637	"we collect an audio-visual <e1>event</e1> (ave) dataset to systemically investigate three temporal localization tasks: supervised and weakly-supervised audio-visual <e2>event</e2> localization, and cross-modality localization."
sameAs(e1, e2)
Comment:

6638	"we develop an <e1>audio</e1>-guided visual attention mechanism to explore <e2>audio</e2>-visual correlations, propose a dual multimodal residual network (dmrn) to fuse information over the two modalities, and introduce an audio-visual distance learning network to handle the cross-modality localization."
sameAs(e1, e2)
Comment:

6639	"we develop an <e1>audio</e1>-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual network (dmrn) to fuse information over the two modalities, and introduce an <e2>audio</e2>-visual distance learning network to handle the cross-modality localization."
sameAs(e1, e2)
Comment:

6640	"we develop an audio-guided visual attention mechanism to explore <e1>audio</e1>-visual correlations, propose a dual multimodal residual network (dmrn) to fuse information over the two modalities, and introduce an <e2>audio</e2>-visual distance learning network to handle the cross-modality localization."
sameAs(e1, e2)
Comment:

6641	"we develop an audio-guided visual attention mechanism to explore audio-visual correlations, propose a dual multimodal residual <e1>network</e1> (dmrn) to fuse information over the two modalities, and introduce an audio-visual distance learning <e2>network</e2> to handle the cross-modality localization."
sameAs(e1, e2)
Comment:

6642	"our experiments support the following findings: joint <e1>modeling</e1> of auditory and visual modalities outperforms independent <e2>modeling</e2>, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed dmrn is effective in fusing audio-visual features, and strong correlations between the two modalities enable cross-modality localization."
sameAs(e1, e2)
Comment:

6643	"our experiments support the following findings: joint modeling of auditory and visual <e1>modalities</e1> outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for audio-visual fusion, the proposed dmrn is effective in fusing audio-visual features, and strong correlations between the two <e2>modalities</e2> enable cross-modality localization."
sameAs(e1, e2)
Comment:

6644	"our experiments support the following findings: joint modeling of auditory and visual modalities outperforms independent modeling, the learned attention can capture semantics of sounding objects, temporal alignment is important for <e1>audio</e1>-visual fusion, the proposed dmrn is effective in fusing <e2>audio</e2>-visual features, and strong correlations between the two modalities enable cross-modality localization."
sameAs(e1, e2)
Comment:

6645	"albeit <e1>these</e1> advances, <e2>these</e2> models are limited in their constrained domains."
sameAs(e1, e2)
Comment:

6646	"based on cnn architecture, the pixel-wise <e1>classification</e1> loss is usually used [19, 34, 10] which punishes the <e2>classification</e2> error for each pixel."
sameAs(e1, e2)
Comment:

6647	"in the attempt to address the inconsistency problems, the <e1>conditional random fields</e1> (<e2>crfs</e2>) [17] can be employed as a post processing method."
sameAs(e1, e2)
Comment:

6648	"(a) good <e1>convergence</e1>, where lossd(real) and lossd(f ake) converge to 0.5 and lossg converges to 0. it indicates a successful adversarial network training, where g is able to fool d. (b) poor <e2>convergence</e2>, where lossd(real) and lossd(f ake) converge to 0 and lossg converges to 1. it stands for an unbalanced adversarial network training, where d can easily distinguish generated images from real images."
sameAs(e1, e2)
Comment:

6649	"(a) good convergence, where lossd(real) and lossd(f ake) converge to 0.5 and lossg converges to 0. it indicates a successful <e1>adversarial</e1> network training, where g is able to fool d. (b) poor convergence, where lossd(real) and lossd(f ake) converge to 0 and lossg converges to 1. it stands for an unbalanced <e2>adversarial</e2> network training, where d can easily distinguish generated images from real images."
sameAs(e1, e2)
Comment:

6650	"(a) good convergence, where lossd(real) and lossd(f ake) converge to 0.5 and lossg converges to 0. it indicates a successful adversarial <e1>network</e1> training, where g is able to fool d. (b) poor convergence, where lossd(real) and lossd(f ake) converge to 0 and lossg converges to 1. it stands for an unbalanced adversarial <e2>network</e2> training, where d can easily distinguish generated images from real images."
sameAs(e1, e2)
Comment:

6651	"(a) good convergence, where lossd(real) and lossd(f ake) converge to 0.5 and lossg converges to 0. it indicates a successful adversarial network <e1>training</e1>, where g is able to fool d. (b) poor convergence, where lossd(real) and lossd(f ake) converge to 0 and lossg converges to 1. it stands for an unbalanced adversarial network <e2>training</e2>, where d can easily distinguish generated images from real images."
sameAs(e1, e2)
Comment:

6652	"(a) good convergence, where lossd(real) and lossd(f ake) converge to 0.5 and lossg converges to 0. it indicates a successful adversarial network training, where g is able to fool d. (b) poor convergence, where lossd(real) and lossd(f ake) converge to 0 and lossg converges to 1. it stands for an unbalanced adversarial network training, where d can easily distinguish generated <e1>images</e1> from real <e2>images</e2>."
sameAs(e1, e2)
Comment:

6653	"however, the local inconsistency is generated from top <e1>layers</e1> and the semantic inconsistency is generated from deep <e2>layers</e2>."
sameAs(e1, e2)
Comment:

6654	"we adopt the idea of <e1>adversarial</e1> training and at the same time aim to addresses its limitations, i.e., the inferior ability in improving parsing consistency with a single <e2>adversarial</e2> loss and the poor convergence problem."
sameAs(e1, e2)
Comment:

6655	"mman consists of a dual-output generator (g) and <e1>two</e1> discriminators (d), named macro d and micro d. the three modules constitute <e2>two</e2> adversarial networks (macro an , micro an ), addressing the semantic consistency and the local consistency, respectively."
sameAs(e1, e2)
Comment:

6656	"given an input human image, the cnn-based generator outputs two segmentation maps with different <e1>resolution</e1> levels, i.e., low <e2>resolution</e2> and high resolution."
sameAs(e1, e2)
Comment:

6657	"given an input human image, the cnn-based generator outputs two segmentation maps with different <e1>resolution</e1> levels, i.e., low resolution and high <e2>resolution</e2>."
sameAs(e1, e2)
Comment:

6658	"given an input human image, the cnn-based generator outputs two segmentation maps with different resolution levels, i.e., low <e1>resolution</e1> and high <e2>resolution</e2>."
sameAs(e1, e2)
Comment:

6659	"more detailed description of the merits of the proposed <e1>network</e1> is provided in section 3.5. our contributions are summarized as follows: image cnn (baseline) + micro d + macro d + macro d + micro d (mman) gt -we propose a new framework called macro-micro adversarial <e2>network</e2> (mman) for human parsing."
sameAs(e1, e2)
Comment:

6660	"compared with traditional <e1>adversarial</e1> networks, mman not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of <e2>adversarial</e2> networks when handling high resolution images."
sameAs(e1, e2)
Comment:

6661	"compared with traditional adversarial <e1>networks</e1>, mman not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of adversarial <e2>networks</e2> when handling high resolution images."
sameAs(e1, e2)
Comment:

6662	"with the recent proliferation of applications employing 3d depth sensors [23] such as autonomous navigation, <e1>robotics</e1> and virtual reality, there is an increasing demand for algorithms to efficiently analyze <e2>point clouds</e2>."
Used-for(e1, e2)
Comment:

6663	"various studies are devoted to making <e1>convolution</e1> neural networks applicable for learning on non-euclidean domains such as graphs or manifolds by trying to generalize the definition of <e2>convolution</e2> to functions on manifolds or graphs, enriching the emerging field of geometric deep learning [3] ."
sameAs(e1, e2)
Comment:

6664	"various studies are devoted to making convolution neural networks applicable for learning on non-euclidean domains such as <e1>graphs</e1> or manifolds by trying to generalize the definition of convolution to functions on manifolds or <e2>graphs</e2>, enriching the emerging field of geometric deep learning [3] ."
sameAs(e1, e2)
Comment:

6665	"we design the filter as a product of a simple step function <e1>that</e1> captures local geodesic information and a taylor polynomial <e2>that</e2> ensures the expressiveness."
sameAs(e1, e2)
Comment:

6666	"we train an embedding network (c-d) to predict coplanarity for a pair of <e1>planar patches</e1> across different views, based on the co-<e2>planar patches</e2> (b) sampled from training sequences with ground-truth camera poses (a)."
sameAs(e1, e2)
Comment:

6667	"in order to evaluate our descriptor, <e1>we</e1> introduce a new coplanarity matching datasets, where <e2>we</e2> can see in series of thorough experiments that our new descriptor outperforms existing baseline alternatives by significant margins."
sameAs(e1, e2)
Comment:

6668	"the core of our method is a deep convolutional neural network <e1>that</e1> takes in rgb, depth, and normal information of a planar patch in an image and outputs a descriptor <e2>that</e2> can be used to find coplanar patches from other images."
sameAs(e1, e2)
Comment:

6669	"furthermore, <e1>we</e1> demonstrate that by using our new descriptor, <e2>we</e2> are able to compute strong coplanarity constraints that improve the performance of current global rgb-d registration algorithms."
sameAs(e1, e2)
Comment:

6670	"furthermore, we demonstrate <e1>that</e1> by using our new descriptor, we are able to compute strong coplanarity constraints <e2>that</e2> improve the performance of current global rgb-d registration algorithms."
sameAs(e1, e2)
Comment:

6671	"overall, the research contributions of this paper are: -a new task: predicting coplanarity of <e1>image</e1> patches for the purpose of rgb-d <e2>image</e2> registration."
sameAs(e1, e2)
Comment:

6672	"-reconstruction <e1>results</e1> demonstrating that coplanarity can be used to align scans where keypoint-based <e2>methods</e2> fail to find loop closures."
Compare(e1, e2)
Comment:

6673	"we find that coplanarity constraints detected with our method are sufficient to get reconstruction <e1>results</e1> comparable to state-of-the-art frameworks on most scenes, but outperform other <e2>methods</e2> on established benchmarks when combined with traditional keypoint matching."
Compare(e1, e2)
Comment:

6674	"because of the structured description and enlarged semantic space of scene graphs, efficient relationships (subject-predicate-object triplets) are represented by phrase <e1>features</e1> in previous works [6, 28, 34, 35, 37, 58, 64] ; right: replacing the phrases with a concise subgraph representation, where relationships can be restored with subgraph <e2>features</e2> (green) and corresponding subject and object."
sameAs(e1, e2)
Comment:

6675	"the <e1>first</e1> approach adopts the two-stage pipeline, which detects the objects <e2>first</e2> and then recognizes their pair-wise relationships [6, 36, 37, 58, 62] ."
sameAs(e1, e2)
Comment:

6676	"thus, the number of phrase <e1>features</e1> determines how fast the <e2>model</e2> performs."
Used-for(e1, e2)
Comment:

6677	"however, due to the <e1>number</e1> of combinations growing quadratically with that of objects, the problem will quickly get intractable as the <e2>number</e2> of objects grows."
sameAs(e1, e2)
Comment:

6678	"however, due to the number of combinations growing quadratically with that of <e1>objects</e1>, the problem will quickly get intractable as the number of <e2>objects</e2> grows."
sameAs(e1, e2)
Comment:

6679	"then the shared <e1>representation</e1> is refined to learn a general <e2>representation</e2> of the are by passing the message from the connected objects."
sameAs(e1, e2)
Comment:

6680	"this design significantly reduces the number of the phrase <e1>features</e1> in the intermediate stage and speed up the <e2>model</e2> both in training and inference."
Used-for(e1, e2)
Comment:

6681	"as different objects correspond to different parts of the shared subgraph <e1>regions</e1>, maintaining the spatial structure of the subgraph feature explicitly retains such connections and helps the subgraph features integrate more spatial information into the representations of the <e2>region</e2>."
sameAs(e1, e2)
Comment:

6682	"different from the previous works, which use object coordinates or the mask to extract the spatial features, our sri could learn to extract the embedded spatial <e1>feature</e1> directly from the subgraph <e2>feature</e2> maps."
sameAs(e1, e2)
Comment:

6683	"to summarize, we propose an efficient sub-<e1>graph</e1> based scene <e2>graph</e2> generation approach with following novelties: first, a bottom-up clustering method is proposed to factorize the image into subgraphs."
sameAs(e1, e2)
Comment:

6684	"to improve the efficiency of <e1>scene</e1> graph generation, we propose a subgraph-based connection graph to concisely represent the <e2>scene</e2> graph during the inference."
sameAs(e1, e2)
Comment:

6685	"to improve the efficiency of scene <e1>graph</e1> generation, we propose a subgraph-based connection <e2>graph</e2> to concisely represent the scene graph during the inference."
sameAs(e1, e2)
Comment:

6686	"to improve the efficiency of scene <e1>graph</e1> generation, we propose a subgraph-based connection graph to concisely represent the scene <e2>graph</e2> during the inference."
sameAs(e1, e2)
Comment:

6687	"to improve the efficiency of scene graph generation, we propose a subgraph-based connection <e1>graph</e1> to concisely represent the scene <e2>graph</e2> during the inference."
sameAs(e1, e2)
Comment:

6688	"they do not explore the inherent redundancy among <e1>these</e1> <e2>tasks</e2>, which can be formulated by geometry regularities via the nature of 3d scene construction."
Used-for(e1, e2)
Comment:

6689	"for example, they require large quantities of laser scanned depth <e1>data</e1> for supervision [48] , demand stereo cameras as additional equipment for <e2>data</e2> acquisition [15] , or cannot explicitly handle non-rigidity and occlusions [50, 56] ."
sameAs(e1, e2)
Comment:

6690	"their projected 2d image <e1>motion</e1> between video frames can be fully determined by the depth structure and camera <e2>motion</e2>."
sameAs(e1, e2)
Comment:

6691	"prediction coherence is enforced between different views in non-occluded <e1>regions</e1>, while erroneous predictions get smoothed out especially in occluded <e2>regions</e2>."
sameAs(e1, e2)
Comment:

6692	"traditional <e1>structure</e1> from motion (sfm) methods [34, 42] tackle them in an integrated way, which aim to simultaneously reconstruct the scene <e2>structure</e2> and camera motion."
sameAs(e1, e2)
Comment:

6693	"traditional structure from <e1>motion</e1> (sfm) methods [34, 42] tackle them in an integrated way, which aim to simultaneously reconstruct the scene structure and camera <e2>motion</e2>."
sameAs(e1, e2)
Comment:

6694	"to resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the <e1>features</e1> to enforce the network to be sparse, and at the same time remove any redundancies among the <e2>features</e2> to fully utilize the capacity of the network."
sameAs(e1, e2)
Comment:

6695	"to resolve this issue, we propose a sparsity regularization method that exploits both positive and negative correlations among the features to enforce the <e1>network</e1> to be sparse, and at the same time remove any redundancies among the features to fully utilize the capacity of the <e2>network</e2>."
sameAs(e1, e2)
Comment:

6696	"specifically, we propose to use an exclusive sparsity regularization based on (1, 2)-norm, which promotes competition for <e1>features</e1> between different weights, thus enforcing them to fit to disjoint sets of <e2>features</e2>."
sameAs(e1, e2)
Comment:

6697	"we further combine the exclusive <e1>sparsity</e1> with the group <e2>sparsity</e2> based on (2, 1)-norm, to promote both sharing and competition for features in training of a deep neural network."
sameAs(e1, e2)
Comment:

6698	"we validate <e1>our method</e1> on multiple public datasets, and the results show that <e2>our method</e2> can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy."
sameAs(e1, e2)
Comment:

6699	"we validate our method on multiple public datasets, and the results show <e1>that</e1> our method can obtain more compact and efficient networks while also improving the performance over the base networks with full weights, as opposed to existing sparsity regularizations <e2>that</e2> often obtain efficiency at the expense of prediction accuracy."
sameAs(e1, e2)
Comment:

6700	"we validate our method on multiple public datasets, and the results show that our method can obtain more compact and efficient <e1>networks</e1> while also improving the performance over the base <e2>networks</e2> with full weights, as opposed to existing sparsity regularizations that often obtain efficiency at the expense of prediction accuracy."
sameAs(e1, e2)
Comment:

6701	"developing generative models that can learn directly from data is an important step towards improving the fidelity of generated <e1>graphs</e1>, and paves a way for new kinds of applications, such as discovering new graph structures and completing evolving <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

6702	"in contrast, traditional <e1>generative models</e1> for graphs (e.g., barabási-albert model, kronecker graphs, exponential random graphs, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of graphs, and thus do not have the capacity to directly learn the <e2>generative model</e2> from observed data."
sameAs(e1, e2)
Comment:

6703	"in contrast, traditional generative models for <e1>graphs</e1> (e.g., barabási-albert model, kronecker <e2>graphs</e2>, exponential random graphs, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of graphs, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6704	"in contrast, traditional generative models for <e1>graphs</e1> (e.g., barabási-albert model, kronecker graphs, exponential random <e2>graphs</e2>, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of graphs, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6705	"in contrast, traditional generative models for <e1>graphs</e1> (e.g., barabási-albert model, kronecker graphs, exponential random graphs, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of <e2>graphs</e2>, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6706	"in contrast, traditional generative models for graphs (e.g., barabási-albert <e1>model</e1>, kronecker graphs, exponential random graphs, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to <e2>model</e2> a particular family of graphs, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6707	"in contrast, traditional generative models for graphs (e.g., barabási-albert model, kronecker <e1>graphs</e1>, exponential random <e2>graphs</e2>, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of graphs, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6708	"in contrast, traditional generative models for graphs (e.g., barabási-albert model, kronecker <e1>graphs</e1>, exponential random graphs, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of <e2>graphs</e2>, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6709	"in contrast, traditional generative models for graphs (e.g., barabási-albert model, kronecker graphs, exponential random <e1>graphs</e1>, and stochastic block models) (erdős & rényi, 1959; leskovec et al, 2010; albert & barabási, 2002; airoldi et al, 2008; leskovec et al, 2007; robins et al, 2007) are hand-engineered to model a particular family of <e2>graphs</e2>, and thus do not have the capacity to directly learn the generative model from observed data."
sameAs(e1, e2)
Comment:

6710	"however, these recently proposed deep models are either limited to learning from a single <e1>graph</e1> (kipf & welling, 2016; grover et al, 2017) or generating small <e2>graphs</e2> with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the graph generation problem: • large and variable output spaces: to generate a graph with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6711	"however, these recently proposed deep models are either limited to learning from a single <e1>graph</e1> (kipf & welling, 2016; grover et al, 2017) or generating small graphs with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the <e2>graph</e2> generation problem: • large and variable output spaces: to generate a graph with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6712	"however, these recently proposed deep models are either limited to learning from a single <e1>graph</e1> (kipf & welling, 2016; grover et al, 2017) or generating small graphs with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the graph generation problem: • large and variable output spaces: to generate a <e2>graph</e2> with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6713	"however, these recently proposed deep models are either limited to learning from a single graph (kipf & welling, 2016; grover et al, 2017) or generating small <e1>graphs</e1> with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the <e2>graph</e2> generation problem: • large and variable output spaces: to generate a graph with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6714	"however, these recently proposed deep models are either limited to learning from a single graph (kipf & welling, 2016; grover et al, 2017) or generating small <e1>graphs</e1> with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the graph generation problem: • large and variable output spaces: to generate a <e2>graph</e2> with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6715	"however, these recently proposed deep models are either limited to learning from a single graph (kipf & welling, 2016; grover et al, 2017) or generating small graphs with 40 or fewer <e1>nodes</e1> (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the graph generation problem: • large and variable output spaces: to generate a graph with n <e2>nodes</e2> the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6716	"however, these recently proposed deep models are either limited to learning from a single graph (kipf & welling, 2016; grover et al, 2017) or generating small graphs with 40 or fewer nodes (li et al, 2018; simonovsky & komodakis, 2018) -limitations that stem from three fundamental challenges in the <e1>graph</e1> generation problem: • large and variable output spaces: to generate a <e2>graph</e2> with n nodes the generative model has to output n 2 values to fully specify its structure."
sameAs(e1, e2)
Comment:

6717	"however, modeling <e1>complex</e1> distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the <e2>complex</e2>, non-local dependencies that exist between edges in a given graph."
sameAs(e1, e2)
Comment:

6718	"however, modeling complex distributions over <e1>graphs</e1> and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of <e2>graphs</e2> and the complex, non-local dependencies that exist between edges in a given graph."
sameAs(e1, e2)
Comment:

6719	"however, modeling complex distributions over <e1>graphs</e1> and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given <e2>graph</e2>."
sameAs(e1, e2)
Comment:

6720	"however, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of <e1>graphs</e1> and the complex, non-local dependencies that exist between edges in a given <e2>graph</e2>."
sameAs(e1, e2)
Comment:

6721	"• <e1>complex</e1> dependencies: edge formation in graphs involves <e2>complex</e2> structural dependencies."
sameAs(e1, e2)
Comment:

6722	"therefore, <e1>edges</e1> cannot be modeled as a sequence of independent events, but rather need to be generated jointly, where each next edge depends on the previously generated <e2>edges</e2>."
sameAs(e1, e2)
Comment:

6723	"li et al 2018 address <e1>this</e1> problem using graph neural networks to perform a form of "message passing"; however, while expressive, <e2>this</e2> approach takes o(mn 2 diam(g)) operations to generate a graph with m edges, n nodes and diameter diam(g)."
sameAs(e1, e2)
Comment:

6724	"li et al 2018 address this problem using <e1>graph</e1> neural networks to perform a form of "message passing"; however, while expressive, this approach takes o(mn 2 diam(g)) operations to generate a <e2>graph</e2> with m edges, n nodes and diameter diam(g)."
sameAs(e1, e2)
Comment:

6725	"here we address the above challenges and present <e1>graph</e1> recurrent neural networks (graphrnn), a scalable framework for learning generative models of <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

6726	"graphrnn models a <e1>graph</e1> in an autoregressive (or recurrent) manner-as a sequence of additions of new nodes and edges-to capture the complex joint probability of all nodes and edges in the <e2>graph</e2>."
sameAs(e1, e2)
Comment:

6727	"graphrnn models a graph in an autoregressive (or recurrent) manner-as a sequence of additions of new <e1>nodes</e1> and edges-to capture the complex joint probability of all <e2>nodes</e2> and edges in the graph."
sameAs(e1, e2)
Comment:

6728	"graphrnn models a graph in an autoregressive (or recurrent) manner-as a sequence of additions of new nodes and <e1>edges</e1>-to capture the complex joint probability of all nodes and <e2>edges</e2> in the graph."
sameAs(e1, e2)
Comment:

6729	"in particular, graphrnn can be viewed as a hierarchical model, where a <e1>graph</e1>-level rnn maintains the state of the <e2>graph</e2> and generates new nodes, while an edge-level rnn generates the edges for each newly generated node."
sameAs(e1, e2)
Comment:

6730	"in particular, graphrnn can be viewed as a hierarchical model, where a graph-level <e1>rnn</e1> maintains the state of the graph and generates new nodes, while an edge-level <e2>rnn</e2> generates the edges for each newly generated node."
sameAs(e1, e2)
Comment:

6731	"this bfs approach alleviates the fact that graphs have non-unique <e1>representations</e1>-by collapsing distinct <e2>representations</e2> to unique bfs trees-and the tree-structure induced by bfs allows us to limit the number of edge predictions made for each node during training."
sameAs(e1, e2)
Comment:

6732	"a key challenge for the <e1>graph</e1> generation problem is quantitative evaluation of the quality of generated <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

6733	"whereas prior studies have mainly relied on visual inspection or first-order moment <e1>statistics</e1> for evaluation, we provide a comprehensive evaluation setup by comparing graph <e2>statistics</e2> such as the degree distribution, clustering coefficient distribution and motif counts for two sets of graphs based on variants of the maximum mean discrepancy (mmd) (gretton et al, 2012) ."
sameAs(e1, e2)
Comment:

6734	"whereas prior studies have mainly relied on visual inspection or first-order moment statistics for evaluation, we provide a comprehensive evaluation setup by comparing <e1>graph</e1> statistics such as the degree distribution, clustering coefficient distribution and motif counts for two sets of <e2>graphs</e2> based on variants of the maximum mean discrepancy (mmd) (gretton et al, 2012) ."
sameAs(e1, e2)
Comment:

6735	"extensive experiments on synthetic and real-world <e1>graphs</e1> of varying size demonstrate the significant improvement graphrnn provides over baseline approaches, including the most recent deep <e2>graph</e2> generative models as well as traditional models."
sameAs(e1, e2)
Comment:

6736	"compared to traditional baselines (e.g., stochastic block <e1>models</e1>), graphrnn is able to generate high-quality graphs on all benchmark datasets, while the traditional <e2>models</e2> are only able to achieve good performance on specific datasets that exhibit special structures."
sameAs(e1, e2)
Comment:

6737	"compared to traditional baselines (e.g., stochastic block models), graphrnn is able to generate high-quality graphs on all benchmark <e1>datasets</e1>, while the traditional models are only able to achieve good performance on specific <e2>datasets</e2> that exhibit special structures."
sameAs(e1, e2)
Comment:

6738	"graphrnn learns to generate <e1>graphs</e1> by training on a representative set of <e2>graphs</e2> and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far."
sameAs(e1, e2)
Comment:

6739	"graphrnn learns to generate <e1>graphs</e1> by training on a representative set of graphs and decomposes the <e2>graph</e2> generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far."
sameAs(e1, e2)
Comment:

6740	"graphrnn learns to generate graphs by training on a representative set of <e1>graphs</e1> and decomposes the <e2>graph</e2> generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far."
sameAs(e1, e2)
Comment:

6741	"compared to other state-of-the-art deep <e1>graph</e1> generative models, graphrnn is able to achieve superior quantitative performance-in terms of the mmd distance between the generated and test set <e2>graphs</e2>-while also scaling to graphs that are 50× larger than what these previous approaches can handle."
sameAs(e1, e2)
Comment:

6742	"compared to other state-of-the-art deep <e1>graph</e1> generative models, graphrnn is able to achieve superior quantitative performance-in terms of the mmd distance between the generated and test set graphs-while also scaling to <e2>graphs</e2> that are 50× larger than what these previous approaches can handle."
sameAs(e1, e2)
Comment:

6743	"compared to other state-of-the-art deep graph generative models, graphrnn is able to achieve superior quantitative performance-in terms of the mmd distance between the generated and test set <e1>graphs</e1>-while also scaling to <e2>graphs</e2> that are 50× larger than what these previous approaches can handle."
sameAs(e1, e2)
Comment:

6744	"our experiments show <e1>that</e1> graphrnn significantly outperforms all baselines, learning to generate diverse graphs <e2>that</e2> match the structural characteristics of a target set, while also scaling to graphs 50× larger than previous deep models."
sameAs(e1, e2)
Comment:

6745	"our experiments show that graphrnn significantly outperforms all baselines, learning to generate diverse <e1>graphs</e1> that match the structural characteristics of a target set, while also scaling to <e2>graphs</e2> 50× larger than previous deep models."
sameAs(e1, e2)
Comment:

6746	"introduction and related work generative models for real-world <e1>graphs</e1> have important applications in many domains, including modeling physical and social interactions, discovering new chemical and molecular structures, and constructing knowledge <e2>graphs</e2>."
sameAs(e1, e2)
Comment:

6747	"development of generative <e1>graph</e1> models has a rich history, and many methods have been proposed that can generate <e2>graphs</e2> based on a priori structural assumptions (newman, 2010) ."
sameAs(e1, e2)
Comment:

6748	"consequently, existing cnn-based <e1>methods</e1> rely on synthetic datasets for training the <e2>models</e2> [5, 12, 16, 24] ."
Compare(e1, e2)
Comment:

6749	"while the two tasks aim to recover highly correlated information from the <e1>scene</e1> (i.e., the <e2>scene</e2> structure and the dense motion field between consecutive frames), existing efforts typically study each problem in isolation."
sameAs(e1, e2)
Comment:

6750	"with the estimated depth and camera pose, these methods can produce dense optical <e1>flow</e1> by backprojecting the 3d scene <e2>flow</e2> induced from camera ego-motion."
sameAs(e1, e2)
Comment:

6751	"(1) we propose an unsupervised learning framework to simultaneously train a depth prediction <e1>network</e1> and an optical flow <e2>network</e2>."
sameAs(e1, e2)
Comment:

6752	"while promising <e1>results</e1> have been shown, these <e2>methods</e2> rely on the absolute ground truth depth maps."
Compare(e1, e2)
Comment:

6753	"recent work also explores gathering training datasets from web videos [7] or internet photos [36] using structure-from-<e1>motion</e1> and <e2>multi-view stereo</e2> algorithms."
Conjunction(e1, e2)
Comment:

6754	"compared to ground truth depth <e1>datasets</e1>, constructing optical flow <e2>datasets</e2> of diverse scenes in real-world is even more challenging."
sameAs(e1, e2)
Comment:

6755	"in contrast, our <e1>approach</e1> leverages the readily available real-world videos to jointly train the depth and flow <e2>models</e2>."
Used-for(e1, e2)
Comment:

6756	"the core idea is to treat the estimated depth and flow as latent layers and use them to differentiably warp the source frame to the <e1>target</e1> frame, where the source and <e2>target</e2> frames can either be the stereo pair or two consecutive frames in a video sequence."
sameAs(e1, e2)
Comment:

6757	"recently, a number of work exploits the geometric relationship between depth, camera pose, and <e1>flow</e1> for learning depth or <e2>flow</e2> models [60, 65, 68, 73] ."
sameAs(e1, e2)
Comment:

6758	"the synthesized <e1>flow</e1> from depth and pose can either be used for <e2>flow</e2> prediction in rigid regions [60, 65, 68, 48] as is or used for view synthesis to train depth model using monocular videos [73] ."
sameAs(e1, e2)
Comment:

6759	"our key insight is that for rigid regions the estimated <e1>flow</e1> (from <e2>flow</e2> prediction network) and the synthesized rigid flow (from depth and camera pose networks) should be consistent."
sameAs(e1, e2)
Comment:

6760	"our key insight is that for rigid regions the estimated <e1>flow</e1> (from flow prediction network) and the synthesized rigid <e2>flow</e2> (from depth and camera pose networks) should be consistent."
sameAs(e1, e2)
Comment:

6761	"our key insight is that for rigid regions the estimated flow (from <e1>flow</e1> prediction network) and the synthesized rigid <e2>flow</e2> (from depth and camera pose networks) should be consistent."
sameAs(e1, e2)
Comment:

6762	"our <e1>approach</e1> tackles the problems of learning both depth and flow <e2>models</e2>."
Used-for(e1, e2)
Comment:

6763	"unlike existing multi-task learning methods that often require direct <e1>supervision</e1> using ground truth training data for each task, our approach instead leverage meta-<e2>supervision</e2> to couple the training of depth and flow models."
sameAs(e1, e2)
Comment:

6764	"unlike existing multi-task learning methods that often require direct supervision using ground truth <e1>training</e1> data for each task, our approach instead leverage meta-supervision to couple the <e2>training</e2> of depth and flow models."
sameAs(e1, e2)
Comment:

6765	"unlike existing multi-task learning methods that often require direct supervision using ground truth training <e1>data</e1> for each task, our approach instead leverage meta-supervision to couple the training of depth and flow <e2>models</e2>."
Used-for(e1, e2)
Comment:

6766	"unlike existing multi-task learning methods that often require direct supervision using ground truth training data for each task, our <e1>approach</e1> instead leverage meta-supervision to couple the training of depth and flow <e2>models</e2>."
Used-for(e1, e2)
Comment:

6767	"(2) a <e1>pose</e1> net that takes two stacked input frames and estimates the relative camera <e2>pose</e2> between the two input frames; and (3) a flow net that estimates dense optical flow field between the two input frames."
sameAs(e1, e2)
Comment:

6768	"(2) a pose net <e1>that</e1> takes two stacked input frames and estimates the relative camera pose between the two input frames; and (3) a flow net <e2>that</e2> estimates dense optical flow field between the two input frames."
sameAs(e1, e2)
Comment:

6769	"(2) a pose net that takes <e1>two</e1> stacked input frames and estimates the relative camera pose between the <e2>two</e2> input frames; and (3) a flow net that estimates dense optical flow field between the two input frames."
sameAs(e1, e2)
Comment:

6770	"(2) a pose net that takes <e1>two</e1> stacked input frames and estimates the relative camera pose between the two input frames; and (3) a flow net that estimates dense optical flow field between the <e2>two</e2> input frames."
sameAs(e1, e2)
Comment:

6771	"(2) a pose net that takes two stacked <e1>input</e1> frames and estimates the relative camera pose between the two <e2>input</e2> frames; and (3) a flow net that estimates dense optical flow field between the two input frames."
sameAs(e1, e2)
Comment:

6772	"(2) a pose net that takes two stacked <e1>input</e1> frames and estimates the relative camera pose between the two input frames; and (3) a flow net that estimates dense optical flow field between the two <e2>input</e2> frames."
sameAs(e1, e2)
Comment:

6773	"(2) a pose net that takes two stacked input frames and <e1>estimates</e1> the relative camera pose between the two input frames; and (3) a flow net that <e2>estimates</e2> dense optical flow field between the two input frames."
sameAs(e1, e2)
Comment:

6774	"(2) a pose net that takes two stacked input frames and estimates the relative camera pose between the <e1>two</e1> input frames; and (3) a flow net that estimates dense optical flow field between the <e2>two</e2> input frames."
sameAs(e1, e2)
Comment:

6775	"(2) a pose net that takes two stacked input frames and estimates the relative camera pose between the two <e1>input</e1> frames; and (3) a flow net that estimates dense optical flow field between the two <e2>input</e2> frames."
sameAs(e1, e2)
Comment:

6776	"using the predicted <e1>scene</e1> depth and the estimated camera pose, we can synthesize 2d forward and backward optical flows (referred as rigid flow ) by backprojecting the induced 3d forward and backward <e2>scene</e2> flows (section 3.2)."
sameAs(e1, e2)
Comment:

6777	"using the predicted scene depth and the estimated camera pose, we can synthesize 2d forward and backward optical <e1>flows</e1> (referred as rigid <e2>flow</e2> ) by backprojecting the induced 3d forward and backward scene flows (section 3.2)."
sameAs(e1, e2)
Comment:

6778	"using the predicted scene depth and the estimated camera pose, we can synthesize 2d forward and backward optical <e1>flows</e1> (referred as rigid flow ) by backprojecting the induced 3d forward and backward scene <e2>flows</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

6779	"using the predicted scene depth and the estimated camera pose, we can synthesize 2d forward and backward optical flows (referred as rigid <e1>flow</e1> ) by backprojecting the induced 3d forward and backward scene <e2>flows</e2> (section 3.2)."
sameAs(e1, e2)
Comment:

6780	"as <e1>we</e1> do not have ground truth depth and flow maps for supervision, <e2>we</e2> leverage standard photometric and spatial smoothness costs to regularize the network training (section 3.3, not shown in this figure for clarity)."
sameAs(e1, e2)
Comment:

6781	"relationship between depth, camera motion, and <e1>flow</e1> for unsupervised learning of depth and <e2>flow</e2> estimation models."
sameAs(e1, e2)
Comment:

6782	"to enforce the consistency of flow and depth prediction in both directions, we exploit the forward-backward consistency (section 3.4), and adopt the valid masks derived from it to filter out invalid regions (e.g., <e1>occlusion</e1>/dis-<e2>occlusion</e2>) for the photometric loss."
sameAs(e1, e2)
Comment:

6783	"finally, we propose a novel cross-network consistency loss (section 3.5) -encouraging the optical <e1>flow</e1> estimation (from the <e2>flow</e2> net) and the rigid flow (from the depth and pose net) to be consistent to each other within in valid regions."
sameAs(e1, e2)
Comment:

6784	"finally, we propose a novel cross-network consistency loss (section 3.5) -encouraging the optical <e1>flow</e1> estimation (from the flow net) and the rigid <e2>flow</e2> (from the depth and pose net) to be consistent to each other within in valid regions."
sameAs(e1, e2)
Comment:

6785	"finally, we propose a novel cross-network consistency loss (section 3.5) -encouraging the optical flow estimation (from the <e1>flow</e1> net) and the rigid <e2>flow</e2> (from the depth and pose net) to be consistent to each other within in valid regions."
sameAs(e1, e2)
Comment:

6786	"introduction we address the problem of reconstructing an accurate high-<e1>resolution</e1> (hr) image given its low-<e2>resolution</e2> (lr) counterpart, usually referred as single image super-resolution (sr) [8] ."
sameAs(e1, e2)
Comment:

6787	"overall, our contributions are three-fold: (1) <e1>we</e1> propose the very deep residual channel attention networks (rcan) for highly accurate image sr. (2) <e2>we</e2> propose residual in residual (rir) structure to construct very deep trainable networks."
sameAs(e1, e2)
Comment:

6788	"overall, our contributions are three-fold: (1) we propose the very deep residual channel attention <e1>networks</e1> (rcan) for highly accurate image sr. (2) we propose residual in residual (rir) structure to construct very deep trainable <e2>networks</e2>."
sameAs(e1, e2)
Comment:

6789	"ledig et al [21] introduced resnet [11] to construct a deeper <e1>network</e1> with perceptual losses [15] and generative adversarial <e2>network</e2> (gan) [9] for photo-realistic sr. however, most of these methods have limited network depth, which has demonstrated to be very important in visual recognition tasks [11] ."
sameAs(e1, e2)
Comment:

6790	"ledig et al [21] introduced resnet [11] to construct a deeper <e1>network</e1> with perceptual losses [15] and generative adversarial network (gan) [9] for photo-realistic sr. however, most of these methods have limited <e2>network</e2> depth, which has demonstrated to be very important in visual recognition tasks [11] ."
sameAs(e1, e2)
Comment:

6791	"ledig et al [21] introduced resnet [11] to construct a deeper network with perceptual losses [15] and generative adversarial <e1>network</e1> (gan) [9] for photo-realistic sr. however, most of these methods have limited <e2>network</e2> depth, which has demonstrated to be very important in visual recognition tasks [11] ."
sameAs(e1, e2)
Comment:

6792	"furthermore, most of these methods treat the channel-wise <e1>features</e1> equally, hindering better discriminative ability for different <e2>features</e2>."
sameAs(e1, e2)
Comment:

6793	"to make a further step, we propose channel attention (ca) mechanism to adaptively rescale each channel-wise <e1>feature</e1> by modeling the interdependencies across <e2>feature</e2> channels."
sameAs(e1, e2)
Comment:

6794	"the heavy reliance on bn's effectiveness to train <e1>models</e1> in turn prohibits people from exploring higher-capacity <e2>models</e2> that would be limited by memory."
sameAs(e1, e2)
Comment:

6795	"we notice that many classical <e1>features</e1> like <e2>sift</e2> [14] and hog [15] are group-wise features and involve group-wise normalization."
Compare(e1, e2)
Comment:

6796	"we notice that many classical <e1>features</e1> like sift [14] and hog [15] are group-wise <e2>features</e2> and involve group-wise normalization."
sameAs(e1, e2)
Comment:

6797	"the effectiveness of gn in imagenet, coco, and kinetics demonstrates <e1>that</e1> gn is a competitive alternative to bn <e2>that</e2> has been dominant in these tasks."
sameAs(e1, e2)
Comment:

6798	"the effectiveness of gn in imagenet, coco, and kinetics demonstrates that gn is a competitive alternative to bn that has been dominant in <e1>these</e1> <e2>tasks</e2>."
Used-for(e1, e2)
Comment:

6799	"gn's <e1>computation</e1> is independent of batch sizes, and its <e2>accuracy</e2> is stable in a wide range of batch sizes."
Conjunction(e1, e2)
Comment:

6800	"gn's computation is independent of batch <e1>sizes</e1>, and its accuracy is stable in a wide range of batch <e2>sizes</e2>."
sameAs(e1, e2)
Comment:

6801	"on <e1>resnet-50</e1> trained in imagenet, gn has 10.6% lower error than its bn <e2>counterpart</e2> when using a batch size of 2; when using typical batch sizes, gn is comparably good with bn and outperforms other normalization variants."
Compare(e1, e2)
Comment:

6802	"on resnet-50 trained in imagenet, gn has 10.6% lower error than its <e1>bn</e1> counterpart when using a batch size of 2; when using typical batch sizes, gn is comparably good with <e2>bn</e2> and outperforms other normalization variants."
sameAs(e1, e2)
Comment:

6803	"gn can outperform its <e1>bn</e1>-based counterparts for object detection and segmentation in coco, and for video classification in kinetics, showing that gn can effectively replace the powerful <e2>bn</e2> in a variety of tasks."
sameAs(e1, e2)
Comment:

6804	" introduction deep convolutional neural network (cnn) has achieved immense success, not only in high-level<e1> visio</e1>n tasks, but also low-level<e2> visio</e2>n tasks such as deblurring [31, 35, 42] , denoising [6, 24] , jpeg artifacts reduction [7, 9, 41] and super-resolution [8, 19, 17, 37, 39] ."
sameAs(e1, e2)
Comment:

6805	" introduction deep convolutional neural network (cnn) has achieved immense success, not only in high-level vision<e1> task</e1>s, but also low-level vision<e2> task</e2>s such as deblurring [31, 35, 42] , denoising [6, 24] , jpeg artifacts reduction [7, 9, 41] and super-resolution [8, 19, 17, 37, 39] ."
sameAs(e1, e2)
Comment:

6806	"moreover, the new <e1>approach</e1> may lead to parameter-efficient restoration in comparison to existing cnn-based <e2>models</e2>."
Used-for(e1, e2)
Comment:

6807	"interestingly, our <e1>approach</e1> is more transparent than existing <e2>methods</e2> as it can reveal how complicated distortions could be removed step by step using different tools."
Compare(e1, e2)
Comment:

6808	"figure 1 (b-c) illustrate a learned policy to restore an <e1>image</e1> corrupted by multiple distortions, where <e2>image</e2> quality is refined step-by-step."
sameAs(e1, e2)
Comment:

6809	"as we will further present in the experimental section, rl-restore is superior to <e1>cnn</e1> approaches given similar complexity and it requires 82.2% fewer computations to achieve the same performance as a single large <e2>cnn</e2>."
sameAs(e1, e2)
Comment:

6810	"in <e1>this</e1> paper, we show that <e2>this</e2> is not necessary, or even desirable."
sameAs(e1, e2)
Comment:

6811	"starting with the residual <e1>network</e1> architecture, the current state of the art for image classification [6] , we increase the resolution of the <e2>network</e2>'s output by replacing a subset of interior subsampling layers by dilation [18] ."
sameAs(e1, e2)
Comment:

6812	"specifically, drns yield higher <e1>accuracy</e1> in imagenet classification than their non-dilated <e2>counterparts</e2>, with no increase in depth or model complexity."
Evaluate-for(e1, e2)
Comment:

6813	"while it may not be clear a priori <e1>that</e1> average pooling can properly handle such high-resolution output, we show <e2>that</e2> it can, yielding a notable accuracy gain."
sameAs(e1, e2)
Comment:

6814	"image classification is most often a proxy <e1>task</e1> that is used to pretrain a <e2>model</e2> before it is transferred to other applications that involve more detailed scene understanding [4, 10] ."
Evaluate-for(e1, e2)
Comment:

6815	"image classification is most often a proxy task <e1>that</e1> is used to pretrain a model before it is transferred to other applications <e2>that</e2> involve more detailed scene understanding [4, 10] ."
sameAs(e1, e2)
Comment:

6816	"therefore, in this paper we propose a modular <e1>network</e1> for referring expression comprehension -modular attention <e2>network</e2> (mattnet) -that takes a natural language expression as input and softly decomposes it into three phrase embeddings."
sameAs(e1, e2)
Comment:

6817	"as in [12] , a referring expression could be parsed into 7 attributes: category name, color, size, absolute <e1>location</e1>, relative <e2>location</e2>, relative object and generic attribute."
sameAs(e1, e2)
Comment:

6818	"the subject <e1>module</e1> handles the category name, color and other attributes, the location <e2>module</e2> handles both absolute and (some) relative location, and the relationship module handles subject-object relations."
sameAs(e1, e2)
Comment:

6819	"the subject <e1>module</e1> handles the category name, color and other attributes, the location module handles both absolute and (some) relative location, and the relationship <e2>module</e2> handles subject-object relations."
sameAs(e1, e2)
Comment:

6820	"the subject module handles the category name, color and other attributes, the <e1>location</e1> module handles both absolute and (some) relative <e2>location</e2>, and the relationship module handles subject-object relations."
sameAs(e1, e2)
Comment:

6821	"the subject module handles the category name, color and other attributes, the location <e1>module</e1> handles both absolute and (some) relative location, and the relationship <e2>module</e2> handles subject-object relations."
sameAs(e1, e2)
Comment:

6822	"we show that our learned "<e1>parser</e1>" attends to the relevant words for each module and outperforms an off-the-shelf <e2>parser</e2> by a large margin."
sameAs(e1, e2)
Comment:

6823	"additionally, our model computes <e1>module</e1> weights which are adaptive to the input expression, measuring how much each <e2>module</e2> should contribute to the overall score."
sameAs(e1, e2)
Comment:

6824	"expressions like "red cat" will have larger subject <e1>module</e1> weights and smaller location and relationship <e2>module</e2> weights, while expressions like "woman on left" will have larger subject and location module weights."
sameAs(e1, e2)
Comment:

6825	"expressions like "red cat" will have larger subject <e1>module</e1> weights and smaller location and relationship module weights, while expressions like "woman on left" will have larger subject and location <e2>module</e2> weights."
sameAs(e1, e2)
Comment:

6826	"expressions like "red cat" will have larger subject module <e1>weights</e1> and smaller location and relationship module <e2>weights</e2>, while expressions like "woman on left" will have larger subject and location module weights."
sameAs(e1, e2)
Comment:

6827	"expressions like "red cat" will have larger subject module <e1>weights</e1> and smaller location and relationship module weights, while expressions like "woman on left" will have larger subject and location module <e2>weights</e2>."
sameAs(e1, e2)
Comment:

6828	"expressions like "red cat" will have larger subject module weights and smaller <e1>location</e1> and relationship module weights, while expressions like "woman on left" will have larger subject and <e2>location</e2> module weights."
sameAs(e1, e2)
Comment:

6829	"expressions like "red cat" will have larger subject module weights and smaller location and relationship <e1>module</e1> weights, while expressions like "woman on left" will have larger subject and location <e2>module</e2> weights."
sameAs(e1, e2)
Comment:

6830	"expressions like "red cat" will have larger subject module weights and smaller location and relationship module <e1>weights</e1>, while expressions like "woman on left" will have larger subject and location module <e2>weights</e2>."
sameAs(e1, e2)
Comment:

6831	"however, most of these work uses a simple concatenation of all features (target object <e1>feature</e1>, location <e2>feature</e2> and context feature) as input and a single lstm to encode/decode the whole expression, ignoring the variance among different types of referring expressions."
sameAs(e1, e2)
Comment:

6832	"however, most of these work uses a simple concatenation of all features (target object <e1>feature</e1>, location feature and context <e2>feature</e2>) as input and a single lstm to encode/decode the whole expression, ignoring the variance among different types of referring expressions."
sameAs(e1, e2)
Comment:

6833	"however, most of these work uses a simple concatenation of all features (target object feature, location <e1>feature</e1> and context <e2>feature</e2>) as input and a single lstm to encode/decode the whole expression, ignoring the variance among different types of referring expressions."
sameAs(e1, e2)
Comment:

6834	"it should be noted that unlike previous <e1>network</e1>-based methods designed for 3d point sets [29, 30, 18] , the number of input and output points in our <e2>network</e2> are not the same."
sameAs(e1, e2)
Comment:

6835	"in these works, the deep networks directly extract <e1>features</e1> from the raw 3d point coordinates without using traditional <e2>features</e2>, e.g., normal and curvature."
sameAs(e1, e2)
Comment:

6836	"we restore different level <e1>features</e1> for the original n points with interpolation, and reduce all level <e2>features</e2> to a fixed dimension c with a convolution."
sameAs(e1, e2)
Comment:

6837	"the red <e1>color</e1> in the point feature embedding component shows the original and progressively subsampled points in hierarchical feature learning, while the green <e2>color</e2> indicates the restored features."
sameAs(e1, e2)
Comment:

6838	"the red color in the point <e1>feature</e1> embedding component shows the original and progressively subsampled points in hierarchical <e2>feature</e2> learning, while the green color indicates the restored features."
sameAs(e1, e2)
Comment:

6839	"then, huang et al [15] introduce an edge-aware point set resampling method by first resampling away from <e1>edges</e1> and then progressively approaching <e2>edges</e2> and corners."
sameAs(e1, e2)
Comment:

6840	"points in a <e1>point cloud</e1> do not have any specific order nor follow any regular grid structure, so only a few recent works adopt a deep learning model to directly process <e2>point clouds</e2>."
sameAs(e1, e2)
Comment:

6841	"this upsampling <e1>problem</e1> is similar in spirit to the image super-resolution <e2>problem</e2> [33, 20] ; however, dealing with 3d points rather than a 2d grid of pixels * equal contribution."
sameAs(e1, e2)
Comment:

6842	"first, unlike the image space, which is represented by a <e1>regular</e1> grid, point clouds do not have any spatial order and <e2>regular</e2> structure."
sameAs(e1, e2)
Comment:

6843	"second, the generated points should describe the underlying geometry of a latent <e1>target</e1> object, meaning that they should roughly lie on the <e2>target</e2> object surface."
sameAs(e1, e2)
Comment:

6844	"abstract <e1>we</e1> present an approach that exploits hierarchical recurrent neural networks (rnns) introduction in this paper, <e2>we</e2> consider the problem of video captioning, i.e."
sameAs(e1, e2)
Comment:

6845	"abstract we present an approach that exploits hierarchical <e1>recurrent neural networks</e1> (<e2>rnns</e2>) introduction in this paper, we consider the problem of video captioning, i.e."
sameAs(e1, e2)
Comment:

6846	"as a result, the video captioning task becomes much more challenging, and the generation performance of <e1>these</e1> methods is usually low on <e2>these</e2> large-scale datasets."
sameAs(e1, e2)
Comment:

6847	"since then, inspiring results have been achieved by a recent line of work [11, 48, 47, 32, 54, 56] which benefits from the rapid development of deep neural networks, especially <e1>recurrent neural network</e1> (<e2>rnn</e2>)."
sameAs(e1, e2)
Comment:

6848	"the idea is to treat the image sequence of a video as the "source <e1>text</e1>" and the corresponding caption as the target <e2>text</e2>."
sameAs(e1, e2)
Comment:

6849	"given a sequence of deep convolutional <e1>features</e1> (e.g., vggnet [40] and c3d [45] ) extracted from video frames, a compact representation of the video is obtained by: average pooling [48, 32] , weighted average pooling with an attention <e2>model</e2> [56] , or taking the last output from an rnn en-"
Used-for(e1, e2)
Comment:

6850	"given a sequence of deep convolutional features (e.g., vggnet [40] and c3d [45] ) extracted from <e1>video</e1> frames, a compact representation of the <e2>video</e2> is obtained by: average pooling [48, 32] , weighted average pooling with an attention model [56] , or taking the last output from an rnn en-"
sameAs(e1, e2)
Comment:

6851	"given a sequence of deep convolutional features (e.g., vggnet [40] and c3d [45] ) extracted from video frames, a compact representation of the video is obtained by: average <e1>pooling</e1> [48, 32] , weighted average <e2>pooling</e2> with an attention model [56] , or taking the last output from an rnn en-"
sameAs(e1, e2)
Comment:

6852	"this ability to generate linguistic descriptions for unconstrained <e1>video</e1> is important because not only it is a critical step towards machine intelligence, but also it has many applications in daily scenarios such as <e2>video</e2> retrieval, automatic video subtitling, blind navigation, etc."
sameAs(e1, e2)
Comment:

6853	"this ability to generate linguistic descriptions for unconstrained <e1>video</e1> is important because not only it is a critical step towards machine intelligence, but also it has many applications in daily scenarios such as video retrieval, automatic <e2>video</e2> subtitling, blind navigation, etc."
sameAs(e1, e2)
Comment:

6854	"this ability to generate linguistic descriptions for unconstrained video is important because not only it is a critical step towards machine intelligence, but also it has many applications in daily scenarios such as <e1>video</e1> retrieval, automatic <e2>video</e2> subtitling, blind navigation, etc."
sameAs(e1, e2)
Comment:

6855	"as a preprocessing step, saliency detection is appealing for many practical applications, such as content-ware video compression [37] , <e1>image</e1> resizing [2] , and <e2>image</e2> retrieval [10] ."
sameAs(e1, e2)
Comment:

6856	"conventional saliency detection methods usually utilize low-level features and heuristic priors which are not robust enough to discover salient <e1>objects</e1> in complex scenes, neither are capable of capturing semantic <e2>objects</e2>."
sameAs(e1, e2)
Comment:

6857	"in <e1>this</e1> study, we introduce intelligent synapses that bring some of <e2>this</e2> biological complexity into artificial neural networks."
sameAs(e1, e2)
Comment:

6858	"introduction <e1>artificial neural networks</e1> (anns) have become an indispensable asset for applied <e2>machine learning</e2>, rivaling human performance in a variety of domain-specific tasks (lecun et al, 2015) ."
isA(e1, e2)
Comment:

6859	"in consideration of intrinsic consistency between informativeness of the <e1>regions</e1> and their probability being ground-truth class, we design a novel training paradigm, which enables navigator to detect most informative <e2>regions</e2> under the guidance from teacher."
sameAs(e1, e2)
Comment:

6860	"abstract feature <e1>pooling</e1> layers (e.g., max <e2>pooling</e2>) introduction the use of pooling layers (max pooling, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6861	"abstract feature <e1>pooling</e1> layers (e.g., max pooling) introduction the use of <e2>pooling</e2> layers (max pooling, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6862	"abstract feature <e1>pooling</e1> layers (e.g., max pooling) introduction the use of pooling layers (max <e2>pooling</e2>, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6863	"abstract feature pooling <e1>layers</e1> (e.g., max pooling) introduction the use of pooling <e2>layers</e2> (max pooling, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6864	"abstract feature pooling layers (e.g., max <e1>pooling</e1>) introduction the use of <e2>pooling</e2> layers (max pooling, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6865	"abstract feature pooling layers (e.g., max <e1>pooling</e1>) introduction the use of pooling layers (max <e2>pooling</e2>, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6866	"abstract feature pooling layers (e.g., max pooling) introduction the use of <e1>pooling</e1> layers (max <e2>pooling</e2>, in particular) in deep convolutional neural networks (cnns) is critical for their success in modern object recognition systems."
sameAs(e1, e2)
Comment:

6867	"we have also empirically verified that s3pool only introduces marginal computational overheads during <e1>training time</e1> (evaluated by <e2>time</e2> per epoch)."
Used-for(e1, e2)
Comment:

6868	"during test time, s3pool can either be reduced to standard max <e1>pooling</e1>, or be combined with an additional average <e2>pooling</e2> layer for a slightly better approximation of the stochastic downsampling step."
sameAs(e1, e2)
Comment:

6869	"in our experiments, we show that s3pool yields excellent results on three standard image classification benchmarks, with two state-of-the-art architectures, namely <e1>network</e1> in <e2>network</e2> [17] , and residual networks [9] ."
sameAs(e1, e2)
Comment:

6870	"this not only reduces the amount of computation required by the time consuming convolution operation in subsequent <e1>layers</e1> of the network, it also facilitates the higher <e2>layers</e2> to learn more abstract representations by looking at larger receptive fields."
sameAs(e1, e2)
Comment:

6871	"in the first step, a pooling window slides over the <e1>feature</e1> map with stride size 1 producing the pooled output; in the second step, spatial downsampling is performed by extracting the top-left corner element of each disjoint sˆs window, resulting in a <e2>feature</e2> map with s times smaller spatial dimensions."
sameAs(e1, e2)
Comment:

6872	"in the first step, a pooling window slides over the feature <e1>map</e1> with stride size 1 producing the pooled output; in the second step, spatial downsampling is performed by extracting the top-left corner element of each disjoint sˆs window, resulting in a feature <e2>map</e2> with s times smaller spatial dimensions."
sameAs(e1, e2)
Comment:

6873	"our starting point in <e1>this</e1> work is the observation that although <e2>this</e2> uniformly spaced spatial downsampling is reasonable from a signal processing perspective which aims for signal reconstruction [19] and is also computationally friendly, it is not necessarily the optimal design for the purpose of learning which aims for generalization to unseen examples 1 ."
sameAs(e1, e2)
Comment:

6874	"this <e1>task</e1> is important because it enables generation of novel sentences that preserve the semantic and syntactic properties of real-world sentences, while being potentially different from any of the examples used to estimate the <e2>model</e2>."
Evaluate-for(e1, e2)
Comment:

6875	"one simple approach consists of first learning a latent space to represent (fixed-length) sentences using an encoderdecoder (autoencoder) framework based on <e1>recurrent neural networks</e1> (<e2>rnns</e2>) (cho et al, 2014; sutskever et al, 2014) , then generate synthetic sentences by decoding ran-1 duke university, durham, nc, 27708. correspondence to: yizhe zhang <yizhe.zhang@duke.edu>."
sameAs(e1, e2)
Comment:

6876	"we employ a long shortterm memory <e1>network</e1> as generator, and a convolutional <e2>network</e2> as discriminator."
sameAs(e1, e2)
Comment:

6877	"abstract we describe the class of convexified <e1>convolutional neural networks</e1> (ccnns), which capture the parameter sharing of <e2>convolutional neural networks</e2> in a convex manner."
sameAs(e1, e2)
Comment:

6878	"we obtain ccnns by convexifying <e1>two</e1>-layer cnns; doing so requires overcoming <e2>two</e2> challenges."
sameAs(e1, e2)
Comment:

6879	"on the theoretical front, <e1>we</e1> prove an oracle inequality on the generalization error achieved by our class of ccnns, showing that it is upper bounded by the best possible performance achievable by a two-layer cnn given infinite data-a quantity to which <e2>we</e2> refer as the oracle risk-plus a model complexity term that decays to zero polynomially in the sample size."
sameAs(e1, e2)
Comment:

6880	"on the theoretical front, we prove an oracle inequality on the generalization error achieved by our class of ccnns, showing <e1>that</e1> it is upper bounded by the best possible performance achievable by a two-layer cnn given infinite data-a quantity to which we refer as the oracle risk-plus a model complexity term <e2>that</e2> decays to zero polynomially in the sample size."
sameAs(e1, e2)
Comment:

6881	"our results suggest <e1>that</e1> the sample complexity for ccnns is significantly lower than <e2>that</e2> of the convexified fully-connected neural network (zhang et al, 2016a) , highlighting the importance of parameter sharing."
sameAs(e1, e2)
Comment:

6882	"finally, we apply ccnns to the <e1>mnist</e1> handwritten digit dataset as well as four variation <e2>datasets</e2> (variationsmnist), and find that it achieves state-of-the-art accuracy."
isA(e1, e2)
Comment:

6883	"for learning two-layer convolutional neural networks, we prove <e1>that</e1> the generalization error obtained by a convexified cnn converges to <e2>that</e2> of the best possible cnn."
sameAs(e1, e2)
Comment:

6884	"for learning two-layer convolutional neural networks, we prove that the generalization error obtained by a convexified <e1>cnn</e1> converges to that of the best possible <e2>cnn</e2>."
sameAs(e1, e2)
Comment:

6885	"however, <e1>this</e1> method is not equipped with the optimality guarantees that we provide for ccnns in <e2>this</e2> paper, even for learning one convolutional layer."
sameAs(e1, e2)
Comment:

6886	"daniely et al (2016) show that a randomly initialized cnn can extract <e1>features</e1> as powerful as kernel methods, but it is not clear how to provably improve the <e2>model</e2> from random initialization."
Used-for(e1, e2)
Comment:

6887	"empirically, we find that ccnns achieve competitive or better performance than cnns trained by <e1>backpropagation</e1>, svms, fully-connected <e2>neural networks</e2>, stacked denoising auto-encoders, and other baseline methods."
isA(e1, e2)
Comment:

6888	"in practice, researchers use some flavor of stochastic <e1>gradient</e1> method, in which <e2>gradients</e2> are computed via backpropagation (bottou, 1998) ."
sameAs(e1, e2)
Comment:

6889	"in particular, several approximate natural <e1>gradient</e1> optimizers have been proposed which fit tractable approximations to the fisher matrix to <e2>gradients</e2> sampled during training (kingma & ba, 2014; martens & grosse, 2015) ."
sameAs(e1, e2)
Comment:

6890	"while <e1>these</e1> procedures were described as natural gradient descent on the weights using an approximate fisher matrix, we reinterpret <e2>these</e2> algorithms as natural gradient on a variational posterior using the exact fisher matrix."
sameAs(e1, e2)
Comment:

6891	"while these procedures were described as natural gradient descent on the weights using an approximate fisher <e1>matrix</e1>, we reinterpret these algorithms as natural gradient on a variational posterior using the exact fisher <e2>matrix</e2>."
sameAs(e1, e2)
Comment:

6892	"since manually labeling sufficient training <e1>data</e1> for various applications is often prohibitive, for problems short of labeled <e2>data</e2>, there is strong incentive to designing versatile algorithms to reduce the labeling consumption."
sameAs(e1, e2)
Comment:

6893	"at the core of <e1>these</e1> methods is the shift in data distributions across different domains, which hinders the generalization of predictive models to new target <e2>tasks</e2> [2] ."
Used-for(e1, e2)
Comment:

6894	"another technical difficulty is that it is nontrivial to identify which <e1>classes</e1> are outlier source <e2>classes</e2> since the target classes are unknown during training."
sameAs(e1, e2)
Comment:

6895	"another technical difficulty is that it is nontrivial to identify which <e1>classes</e1> are outlier source classes since the target <e2>classes</e2> are unknown during training."
sameAs(e1, e2)
Comment:

6896	"another technical difficulty is that it is nontrivial to identify which classes are outlier source <e1>classes</e1> since the target <e2>classes</e2> are unknown during training."
sameAs(e1, e2)
Comment:

6897	"since the large dataset is required to be big enough, it is reasonable to assume <e1>that</e1> its label space subsumes <e2>that</e2> of our target dataset."
sameAs(e1, e2)
Comment:

6898	"1 , this novel scenario is more general and challenging than standard domain adaptation, since the outlier source <e1>classes</e1> ('tv') will trigger negative transfer when discriminating the target <e2>classes</e2> ('chairs' and 'mug')."
sameAs(e1, e2)
Comment:

6899	"thus, matching the whole source and target domains as previous methods is not an effective <e1>solution</e1> to this new <e2>problem</e2>."
Used-for(e1, e2)
Comment:

6900	"in this paper, we present partial <e1>adversarial</e1> domain adaptation (pada), an end-to-end framework that largely extends the ability of domain <e2>adversarial</e2> adaptation approaches [10, 11, 12] to address the new partial domain adaptation scenario."
sameAs(e1, e2)
Comment:

6901	"in this paper, we present partial adversarial <e1>domain adaptation</e1> (pada), an end-to-end framework that largely extends the ability of domain adversarial adaptation approaches [10, 11, 12] to address the new partial <e2>domain adaptation</e2> scenario."
sameAs(e1, e2)
Comment:

6902	"in the presence of big data, there is strong motivation of transferring deep models from existing big <e1>domains</e1> to unknown small <e2>domains</e2>."
sameAs(e1, e2)
Comment:

6903	"pada aligns the feature distributions of the source and target <e1>data</e1> in the shared label space and more importantly, identifies the irrelevant source <e2>data</e2> belonging to the outlier source classes and down-weighs their importance automatically."
sameAs(e1, e2)
Comment:

6904	"the key improvement over previous methods is the capability to simultaneously promote positive <e1>transfer</e1> of relevant source data and alleviate negative <e2>transfer</e2> of irrelevant source data."
sameAs(e1, e2)
Comment:

6905	"the key improvement over previous methods is the capability to simultaneously promote positive transfer of relevant source <e1>data</e1> and alleviate negative transfer of irrelevant source <e2>data</e2>."
sameAs(e1, e2)
Comment:

6906	"this paper introduces partial <e1>domain adaptation</e1> as a new <e2>domain adaptation</e2> scenario, which relaxes the fully shared label space assumption to that the source label space subsumes the target label space."
sameAs(e1, e2)
Comment:

6907	"we present partial adversarial domain adaptation (pada), which simultaneously alleviates negative <e1>transfer</e1> by down-weighing the data of outlier source classes for training both source classifier and domain adversary, and promotes positive <e2>transfer</e2> by matching the feature distributions in the shared label space."
sameAs(e1, e2)
Comment:

6908	"introduction weakly supervised object localization (wsol) refers to learning object locations in a given <e1>image</e1> using the <e2>image</e2>-level labels."
sameAs(e1, e2)
Comment:

6909	"based on this, we design the parallel <e1>adversarial</e1> classifier architecture, where complementary regions (the head and hind legs vs. forelegs) are discovered by two classifiers (a and b) via <e2>adversarial</e2> erasing feature maps."
sameAs(e1, e2)
Comment:

6910	"we first mathematically prove that class localization <e1>maps</e1> can be obtained by directly selecting the class-specific feature <e2>maps</e2> of the last convolutional layer, which paves a simple way to identify object regions."
sameAs(e1, e2)
Comment:

6911	"to tackle such issues, wei et al [ 39] proposed an ad-versarial erasing (ae) approach to discover integral <e1>object regions</e1> by training additional classification networks on images whose discriminative <e2>object regions</e2> have partially been erased."
sameAs(e1, e2)
Comment:

6912	"in particular, <e1>one</e1> <e2>classifier</e2> is firstly leveraged to identify the most discriminative regions and guide the erasing operation on the intermediate feature maps."
isA(e1, e2)
Comment:

6913	"to easily conduct end-to-end training for acol, we mathematically prove that object localization <e1>maps</e1> can be obtained by directly selecting from the classspecific feature <e2>maps</e2> of the last convolutional layer, rather than using a post-inference manner in [48] ."
sameAs(e1, e2)
Comment:

6914	"differently, our method generates localization <e1>maps</e1> in one step, by selecting the feature <e2>map</e2> which best matches the groundtruth as the localization map."
sameAs(e1, e2)
Comment:

6915	"differently, our method generates localization <e1>maps</e1> in one step, by selecting the feature map which best matches the groundtruth as the localization <e2>map</e2>."
sameAs(e1, e2)
Comment:

6916	"differently, our method generates localization maps in one step, by selecting the feature <e1>map</e1> which best matches the groundtruth as the localization <e2>map</e2>."
sameAs(e1, e2)
Comment:

6917	"• we propose a novel acol <e1>approach</e1> to efficiently mine different discriminative regions by two adversary <e2>classifiers</e2> in a weakly supervised manner, which discover integral target regions of objects for localization."
Used-for(e1, e2)
Comment:

6918	"• we propose a novel acol approach to efficiently mine different discriminative <e1>regions</e1> by two adversary classifiers in a weakly supervised manner, which discover integral target <e2>regions</e2> of objects for localization."
sameAs(e1, e2)
Comment:

6919	"although it is usually responsive to sparse parts of the target objects, this <e1>classifier</e1> can drive the counterpart <e2>classifier</e2> to discover new and complementary object regions by erasing its discovered regions from the feature maps."
sameAs(e1, e2)
Comment:

6920	"this, however, introduces a new problem, that of <e1>domain</e1> mismatch between the source (simulated) <e2>domain</e2> and the target (real) domain."
sameAs(e1, e2)
Comment:

6921	"this, however, introduces a new problem, that of <e1>domain</e1> mismatch between the source (simulated) domain and the target (real) <e2>domain</e2>."
sameAs(e1, e2)
Comment:

6922	"this, however, introduces a new problem, that of domain mismatch between the source (simulated) <e1>domain</e1> and the target (real) <e2>domain</e2>."
sameAs(e1, e2)
Comment:

6923	"furthermore, the prediction function p (y |z) from <e1>that</e1> space is assumed to be the same across the domains so <e2>that</e2> one can leverage the rich labeled data in the source domain to train classifiers that generalize well to the target."
sameAs(e1, e2)
Comment:

6924	"furthermore, the prediction function p (y |z) from <e1>that</e1> space is assumed to be the same across the domains so that one can leverage the rich labeled data in the source domain to train classifiers <e2>that</e2> generalize well to the target."
sameAs(e1, e2)
Comment:

6925	"furthermore, the prediction function p (y |z) from that space is assumed to be the same across the domains so <e1>that</e1> one can leverage the rich labeled data in the source domain to train classifiers <e2>that</e2> generalize well to the target."
sameAs(e1, e2)
Comment:

6926	"furthermore, the prediction function p (y |z) from that space is assumed to be the same across the domains so that <e1>one</e1> can leverage the rich labeled data in the source domain to train <e2>classifiers</e2> that generalize well to the target."
isA(e1, e2)
Comment:

6927	"second, the structured output in semantic segmentation enables convenient posterior <e1>regularization</e1> [16] , as opposed to the popular (e.g., ℓ 2 ) <e2>regularization</e2> over model parameters."
sameAs(e1, e2)
Comment:

6928	"to rectify this, the image-level label distribution informs the segmentation <e1>network</e1> how to update the predictions while the label distributions of the landmark superpixels tell the <e2>network</e2> where to update."
sameAs(e1, e2)
Comment:

6929	"built upon these, we learn a pixel-wise discriminative segmentation <e1>network</e1> from the labeled source data and, meanwhile, conduct a "sanity check" to ensure the <e2>network</e2> behavior is consistent with the previously learned knowledge about the target domain."
sameAs(e1, e2)
Comment:

6930	"this is related to very recent work on the robustness of high posterior <e1>entropy</e1> solutions (network parameter settings) in deep learning [4, 17] , but with a more informed choice of alternatives than blind <e2>entropy</e2> regularisation."
sameAs(e1, e2)
Comment:

6931	"furthermore we observe that: (i) it applies to a variety of network architectures, and to heterogeneous cohorts consisting of mixed big and small <e1>networks</e1>; (ii) the efficacy increases with the number of <e2>networks</e2> in the cohort -a nice property to have because by training on small networks only, more of them can fit on given gpu resources for more effective mutual learning; (iii) it also benefits semisupervised learning with the mimicry loss activated both on labelled and unlabelled data."
sameAs(e1, e2)
Comment:

6932	"furthermore we observe that: (i) it applies to a variety of network architectures, and to heterogeneous cohorts consisting of mixed big and small <e1>networks</e1>; (ii) the efficacy increases with the number of networks in the cohort -a nice property to have because by training on small <e2>networks</e2> only, more of them can fit on given gpu resources for more effective mutual learning; (iii) it also benefits semisupervised learning with the mimicry loss activated both on labelled and unlabelled data."
sameAs(e1, e2)
Comment:

6933	"furthermore we observe that: (i) it applies to a variety of network architectures, and to heterogeneous cohorts consisting of mixed big and small networks; (ii) the efficacy increases with the number of <e1>networks</e1> in the cohort -a nice property to have because by training on small <e2>networks</e2> only, more of them can fit on given gpu resources for more effective mutual learning; (iii) it also benefits semisupervised learning with the mimicry loss activated both on labelled and unlabelled data."
sameAs(e1, e2)
Comment:

6934	"achieving compact yet accurate models has been approached in a variety of ways including explicit frugal architecture design [9] , <e1>model</e1> compression [22] , pruning [14] , binarisation [18] and most interestingly <e2>model</e2> distillation [8] ."
sameAs(e1, e2)
Comment:

6935	"distillation-based model compression relates to the observation [3, 1] <e1>that</e1> small networks often have the same representation capacity as large networks; but compared to large networks they are simply harder to train and find the right parameters <e2>that</e2> realise the desired function."
sameAs(e1, e2)
Comment:

6936	"distillation-based model compression relates to the observation [3, 1] that small <e1>networks</e1> often have the same representation capacity as large <e2>networks</e2>; but compared to large networks they are simply harder to train and find the right parameters that realise the desired function."
sameAs(e1, e2)
Comment:

6937	"distillation-based model compression relates to the observation [3, 1] that small <e1>networks</e1> often have the same representation capacity as large networks; but compared to large <e2>networks</e2> they are simply harder to train and find the right parameters that realise the desired function."
sameAs(e1, e2)
Comment:

6938	"distillation-based model compression relates to the observation [3, 1] that small networks often have the same representation capacity as large <e1>networks</e1>; but compared to large <e2>networks</e2> they are simply harder to train and find the right parameters that realise the desired function."
sameAs(e1, e2)
Comment:

6939	"to better learn a small <e1>network</e1>, the distillation approach starts with a powerful (deep and/or wide) teacher <e2>network</e2> (or network ensemble), and then trains a smaller student network to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6940	"to better learn a small <e1>network</e1>, the distillation approach starts with a powerful (deep and/or wide) teacher network (or <e2>network</e2> ensemble), and then trains a smaller student network to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6941	"to better learn a small <e1>network</e1>, the distillation approach starts with a powerful (deep and/or wide) teacher network (or network ensemble), and then trains a smaller student <e2>network</e2> to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6942	"to better learn a small network, the distillation approach starts with a powerful (deep and/or wide) teacher <e1>network</e1> (or <e2>network</e2> ensemble), and then trains a smaller student network to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6943	"to better learn a small network, the distillation approach starts with a powerful (deep and/or wide) teacher <e1>network</e1> (or network ensemble), and then trains a smaller student <e2>network</e2> to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6944	"to better learn a small network, the distillation approach starts with a powerful (deep and/or wide) teacher network (or <e1>network</e1> ensemble), and then trains a smaller student <e2>network</e2> to mimic the teacher [8, 1, 16, 3] ."
sameAs(e1, e2)
Comment:

6945	"these <e1>methods</e1> can be broadly divided into two main groups: priorbased <e2>methods</e2> and learning-based methods."
sameAs(e1, e2)
Comment:

6946	"these <e1>methods</e1> can be broadly divided into two main groups: priorbased methods and learning-based <e2>methods</e2>."
sameAs(e1, e2)
Comment:

6947	"these methods can be broadly divided into two main groups: priorbased <e1>methods</e1> and learning-based <e2>methods</e2>."
sameAs(e1, e2)
Comment:

6948	"prior-based <e1>methods</e1> often leverage different priors in characterizing the transmission map such as dark-channel prior [13] , contrast color-lines [10] and haze-line prior [3] , while learningbased <e2>methods</e2>, such as those based on convolutional neural networks (cnns), attempt to learn the transmission map di- we first estimate the transmission map using the proposed pyramid densely-connected transmission estimation net, followed by prediction of atmospheric light using the u-net structure."
sameAs(e1, e2)
Comment:

6949	"prior-based methods often leverage different priors in characterizing the <e1>transmission map</e1> such as dark-channel prior [13] , contrast color-lines [10] and haze-line prior [3] , while learningbased methods, such as those based on convolutional neural networks (cnns), attempt to learn the <e2>transmission map</e2> di- we first estimate the transmission map using the proposed pyramid densely-connected transmission estimation net, followed by prediction of atmospheric light using the u-net structure."
sameAs(e1, e2)
Comment:

6950	"prior-based methods often leverage different priors in characterizing the <e1>transmission map</e1> such as dark-channel prior [13] , contrast color-lines [10] and haze-line prior [3] , while learningbased methods, such as those based on convolutional neural networks (cnns), attempt to learn the transmission map di- we first estimate the <e2>transmission map</e2> using the proposed pyramid densely-connected transmission estimation net, followed by prediction of atmospheric light using the u-net structure."
sameAs(e1, e2)
Comment:

6951	"prior-based methods often leverage different priors in characterizing the transmission map such as dark-channel <e1>prior</e1> [13] , contrast color-lines [10] and haze-line <e2>prior</e2> [3] , while learningbased methods, such as those based on convolutional neural networks (cnns), attempt to learn the transmission map di- we first estimate the transmission map using the proposed pyramid densely-connected transmission estimation net, followed by prediction of atmospheric light using the u-net structure."
sameAs(e1, e2)
Comment:

6952	"prior-based methods often leverage different priors in characterizing the transmission map such as dark-channel prior [13] , contrast color-lines [10] and haze-line prior [3] , while learningbased methods, such as those based on convolutional neural networks (cnns), attempt to learn the <e1>transmission map</e1> di- we first estimate the <e2>transmission map</e2> using the proposed pyramid densely-connected transmission estimation net, followed by prediction of atmospheric light using the u-net structure."
sameAs(e1, e2)
Comment:

6953	"(2) though tremendous improvements have been made by the learning-based <e1>methods</e1>, several factors hinder the performance of these <e2>methods</e2> and the results are far from optimal."
sameAs(e1, e2)
Comment:

6954	"in particular, we propose a new <e1>image</e1> dehazing architecture, called densely connected pyramid dehazing network (dcpdn), that can be jointly optimized to estimate transmission map, atmospheric light and also <e2>image</e2> dehazing simultaneously by following the image degradation model eq."
sameAs(e1, e2)
Comment:

6955	"in particular, we propose a new <e1>image</e1> dehazing architecture, called densely connected pyramid dehazing network (dcpdn), that can be jointly optimized to estimate transmission map, atmospheric light and also image dehazing simultaneously by following the <e2>image</e2> degradation model eq."
sameAs(e1, e2)
Comment:

6956	"in particular, we propose a new image dehazing architecture, called densely connected pyramid dehazing network (dcpdn), that can be jointly optimized to estimate transmission map, atmospheric light and also <e1>image</e1> dehazing simultaneously by following the <e2>image</e2> degradation model eq."
sameAs(e1, e2)
Comment:

6957	"to ease the training process and accelerate the <e1>network</e1> convergence, we leverage a stage-wise learning technique in which we first progressively optimize each part of the <e2>network</e2> and then jointly optimize the entire network."
sameAs(e1, e2)
Comment:

6958	"to ease the training process and accelerate the <e1>network</e1> convergence, we leverage a stage-wise learning technique in which we first progressively optimize each part of the network and then jointly optimize the entire <e2>network</e2>."
sameAs(e1, e2)
Comment:

6959	"to ease the training process and accelerate the network convergence, <e1>we</e1> leverage a stage-wise learning technique in which <e2>we</e2> first progressively optimize each part of the network and then jointly optimize the entire network."
sameAs(e1, e2)
Comment:

6960	"to ease the training process and accelerate the network convergence, we leverage a stage-wise learning technique in which we first progressively optimize each part of the <e1>network</e1> and then jointly optimize the entire <e2>network</e2>."
sameAs(e1, e2)
Comment:

6961	"to make sure <e1>that</e1> the estimated transmission map preserves sharp edges and avoids halo artifacts when dehazing, a new edge-preserving loss function is proposed in this paper based on the observation <e2>that</e2> gradient operators and first several layers of a cnn structure can function as edge extractors."
sameAs(e1, e2)
Comment:

6962	"to make sure that the estimated transmission map preserves sharp edges and avoids halo artifacts when dehazing, a new <e1>edge</e1>-preserving loss function is proposed in this paper based on the observation that gradient operators and first several layers of a cnn structure can function as <e2>edge</e2> extractors."
sameAs(e1, e2)
Comment:

6963	"the <e1>image</e1> degradation (atmospheric scattering model) due to the presence of haze is mathematically formulated as i(z) = j(z)t(z) + a(z)(1 − t(z)), where i is the observed hazy <e2>image</e2>, j is the true scene radiance, a is the global atmospheric light, indicating the intensity of the ambient light, t is the transmission map and z is the pixel location."
sameAs(e1, e2)
Comment:

6964	"transmission map is the distance-dependent factor <e1>that</e1> affects the fraction of light <e2>that</e2> reaches the camera sensor."
sameAs(e1, e2)
Comment:

6965	"1 that there exists two important aspects in the dehazing process: (1) accurate <e1>estimation</e1> of transmission map, and (2) accurate <e2>estimation</e2> of atmospheric light."
sameAs(e1, e2)
Comment:

6966	" introduction in many applications such as drone-based video surveillance and self driving cars,<e1> on</e1>e has to process images and videos containing undesirable artifacts such as rain, snow, and fog [8, 7] or<e2> othe</e2>r distortion such as blur and light [25] ."
Conjunction(e1, e2)
Comment:

6967	"state-of-the-art de-raining algorithms such as [36, 7] often tend to over de-rain or under de-rain the <e1>image</e1> if the rain condition present in the test <e2>image</e2> is not properly considered during training."
sameAs(e1, e2)
Comment:

6968	"1(d) , it tends to under de-rain the <e1>image</e1> and leaves some rain streaks in the output de-rained <e2>image</e2>."
sameAs(e1, e2)
Comment:

6969	"one possible <e1>solution</e1> to this <e2>problem</e2> is to build a very large training dataset with sufficient rain conditions containing various rain-density levels with different orientations and scales."
Used-for(e1, e2)
Comment:

6970	"this has been achieved by fu et al [7] and yang et al [36] , where <e1>they</e1> synthesize a novel large-scale dataset consisting of rainy images with various conditions and <e2>they</e2> train a single network based on this dataset for image de-raining."
sameAs(e1, e2)
Comment:

6971	"alternative <e1>solution</e1> to this <e2>problem</e2> is to learn a density-specific model for deraining."
Used-for(e1, e2)
Comment:

6972	"to accurately estimate the rain-density level, a new residual-aware classifier that makes use of the residual component in the rainy <e1>image</e1> for density <e2>classification</e2> is proposed in this paper."
Used-for(e1, e2)
Comment:

6973	"1 (c) & (d) present sample <e1>results</e1> from our network, where one can clearly see that did-mdn does not over de-rain or under de-rain the image and is able to provide better <e2>results</e2> as compared to [7] and [36] ."
sameAs(e1, e2)
Comment:

6974	"3. a new synthetic dataset consisting of 12,000 training <e1>images</e1> with rain-density labels and 1,200 test <e2>images</e2> is synthesized."
sameAs(e1, e2)
Comment:

6975	"4. extensive experiments are conducted on three highly challenging <e1>datasets</e1> (two synthetic and one realworld) and comparisons are performed against several recent state-of-the-art <e2>approaches</e2>."
Used-for(e1, e2)
Comment:

6976	"note that [7] tends to over de-rain the <e1>image</e1> while [36] tends to under de-rain the <e2>image</e2>."
sameAs(e1, e2)
Comment:

6977	"age de-raining methods is that <e1>they</e1> are designed to deal with certain types of rainy images and <e2>they</e2> do not effectively consider various shapes, scales and density of rain drops into their algorithms."
sameAs(e1, e2)
Comment:

6978	"abstract we focus on <e1>grounding</e1> (i.e., localizing or linking) introduction <e2>grounding</e2> natural language in visual data is a hallmark of ai, since it establishes a communication channel between humans, machines, and the physical world, underpinning a variety of multimodal ai tasks such as robotic navigation [38] , visual q&a [1, 16, 49] , and visual chatbot [6] ."
sameAs(e1, e2)
Comment:

6979	"here, we refer to context as the visual <e1>objects</e1> (e.g., "elephant"), attributes (e.g., "largest" and "baby"), and relationships (e.g., "behind") mentioned in the expression that help to distinguish the referent from other <e2>objects</e2>."
sameAs(e1, e2)
Comment:

6980	"one straightforward way of modeling the <e1>relations</e1> between the referent and context is to: 1) use external syntactic parsers to parse the expression into entities, modifiers, and <e2>relations</e2> [34] , and then 2) apply visual relation detectors to localize them [47] ."
sameAs(e1, e2)
Comment:

6981	"specifically, the model consists of three multimodal modules: context <e1>posterior</e1> q(z|x, l), referent <e2>posterior</e2> p(x|z, l), and context prior p z (z|l), each of which performs a grounding task (cf."
sameAs(e1, e2)
Comment:

6982	"section 4.3) <e1>that</e1> aligns image regions with a cue-specific language feature; each cue dynamically encodes different subsets of words in the expression l <e2>that</e2> help the corresponding localization (cf."
sameAs(e1, e2)
Comment:

6983	"thanks to the reciprocity between referent and context, our model can not only be used in the conventional supervised setting, where there is <e1>annotation</e1> for referent , but also in the challenging unsupervised setting, where there is no instance-level <e2>annotation</e2> (e.g., bounding boxes) of both referent and context."
sameAs(e1, e2)
Comment:

6984	"our <e1>model</e1> consistently outperforms previous <e2>methods</e2> in both supervised and unsupervised settings."
Compare(e1, e2)
Comment:

6985	"but also short <e1>phrases</e1> (e.g., noun <e2>phrases</e2> [28] and relations [47, 36] )."
sameAs(e1, e2)
Comment:

6986	" introduction from much fewer acquired measurements than determined by nyquist sampling<e1> theor</e1>y, compressive sensing (cs)<e2> theor</e2>y demonstrates that a signal can be reconstructed with high probability when it exhibits sparsity in some transform domain [6, 11] ."
sameAs(e1, e2)
Comment:

6987	"most traditional methods exploit some structured <e1>sparsity</e1> as an image prior and then solve a <e2>sparsity</e2>-regularized optimization problem in an iterative fashion [18, 22, 46, 45, 28] ."
sameAs(e1, e2)
Comment:

6988	"however, <e1>they</e1> usually suffer from high computational complexity, and <e2>they</e2> are also faced with the challenges of choosing optimal transforms and tuning parameters in their solvers."
sameAs(e1, e2)
Comment:

6989	"construction algorithms have been recently proposed to directly learn the inverse mapping from the cs measurement <e1>domain</e1> to the original signal <e2>domain</e2> [30, 15] ."
sameAs(e1, e2)
Comment:

6990	"compared to optimization-based <e1>algorithms</e1>, these non-iterative <e2>algorithms</e2> dramatically reduce time complexity, while achieving impressive reconstruction performance."
sameAs(e1, e2)
Comment:

6991	"cs reconstruction results produced by our proposed ista-net <e1>method</e1> and a recent network-based cs reconstruction <e2>method</e2> (reconnet [21] ), when the cs ratio is 25%."
sameAs(e1, e2)
Comment:

6992	"cs has been applied in many practical applications, including but not limited to singlepixel <e1>imaging</e1> [11, 33] , accelerating magnetic resonance <e2>imaging</e2> (mri) [26] , wireless tele-monitoring [50] and cognitive radio communication [36] ."
sameAs(e1, e2)
Comment:

6993	"the <e1>semantic</e1> spaces used by most early works are based on <e2>semantic</e2> attributes [8, 9, 32] ."
sameAs(e1, e2)
Comment:

6994	"with the <e1>former</e1>, the class names are projected into a word vector space so that different classes can be compared, whilst with the <e2>latter</e2>, a neural language model is required to provide a vector representation of the description."
Conjunction(e1, e2)
Comment:

6995	"with the <e1>semantic</e1> space and a visual feature representation of image content, zsl is typically solved in two steps: (1) a joint embedding space is learned where both the <e2>semantic</e2> vectors (prototypes) and the visual feature vectors can be projected to; and (2) nearest neighbour (nn) search is performed in this embedding space to match the projection of an image feature vector against that of an unseen class prototype."
sameAs(e1, e2)
Comment:

6996	"with the semantic space and a visual <e1>feature</e1> representation of image content, zsl is typically solved in two steps: (1) a joint embedding space is learned where both the semantic vectors (prototypes) and the visual feature vectors can be projected to; and (2) nearest neighbour (nn) search is performed in this embedding space to match the projection of an image <e2>feature</e2> vector against that of an unseen class prototype."
sameAs(e1, e2)
Comment:

6997	"most state-of-the-arts zsl models [11, 13, 2, 3, 37, 47, 22] use deep cnn <e1>features</e1> for visual feature representation; the <e2>features</e2> are extracted with pretrained cnn models."
sameAs(e1, e2)
Comment:

6998	"for example, if sentence descriptions are used as the input to a neural <e1>language model</e1> such as recurrent neural networks (rnns) for computing a semantic space, both the neural <e2>language model</e2> and the cnn visual feature representation learning model can be jointly optimised in an end-to-end fashion."
sameAs(e1, e2)
Comment:

6999	"for example, if sentence descriptions are used as the input to a neural <e1>language model</e1> such as recurrent neural networks (rnns) for computing a semantic space, both the neural language model and the cnn visual feature representation learning <e2>model</e2> can be jointly optimised in an end-to-end fashion."
isA(e1, e2)
Comment:

7000	"for example, if sentence descriptions are used as the input to a neural language model such as <e1>recurrent neural networks</e1> (<e2>rnns</e2>) for computing a semantic space, both the neural language model and the cnn visual feature representation learning model can be jointly optimised in an end-to-end fashion."
sameAs(e1, e2)
Comment:

7001	"for example, if sentence descriptions are used as the input to a neural language model such as recurrent neural networks (rnns) for computing a semantic space, both the neural <e1>language model</e1> and the cnn visual feature representation learning <e2>model</e2> can be jointly optimised in an end-to-end fashion."
isA(e1, e2)
Comment:

7002	"third, when multiple semantic spaces are available, this <e1>model</e1> can provide a natural mechanism for fusing the multiple <e2>modalities</e2>."
Used-for(e1, e2)
Comment:

7003	"however, despite all <e1>these</e1> intrinsic advantages, in practice, the few existing end-to-end deep models for zsl in the literature [24, 10, 43, 46, 34] fail to demonstrate <e2>these</e2> advantages and yield only weaker or merely comparable performances on benchmarks when compared to non-deep learning alternatives."
sameAs(e1, e2)
Comment:

7004	"using the <e1>semantic</e1> space as the embedding space means that the visual feature vectors need to be projected into the <e2>semantic</e2> space which will shrink the variance of the projected data points and thus aggravate the hubness problem [33, 7] ."
sameAs(e1, e2)
Comment:

7005	"in this work, <e1>we</e1> propose a novel deep neural network based embedding model for zsl which differs from existing models in that: (1) to alleviate the hubness problem, <e2>we</e2> use the output visual feature space of a cnn subnet as the embedding space."
sameAs(e1, e2)
Comment:

7006	"a zero-shot learning method relies on the existence of a labelled training set of seen <e1>classes</e1> and the knowledge about how an unseen class is semantically related to the seen <e2>classes</e2>."
sameAs(e1, e2)
Comment:

7007	"seen and unseen <e1>classes</e1> are usually related in a high dimensional vector space, called semantic space, where the knowledge from seen <e2>classes</e2> can be transferred to unseen classes."
sameAs(e1, e2)
Comment:

7008	"seen and unseen <e1>classes</e1> are usually related in a high dimensional vector space, called semantic space, where the knowledge from seen classes can be transferred to unseen <e2>classes</e2>."
sameAs(e1, e2)
Comment:

7009	"seen and unseen classes are usually related in a high dimensional vector space, called semantic space, where the knowledge from seen <e1>classes</e1> can be transferred to unseen <e2>classes</e2>."
sameAs(e1, e2)
Comment:

7010	" introduction computing<e1> imag</e1>e patch correspondences based on local descriptor matching is important in many computer vision problems such as<e2> imag</e2>e retrieval, wide baseline stereo matching and panorama building."
sameAs(e1, e2)
Comment:

7011	"in addition to the model itself, the most important aspect of learning-based method is the loss function which defines the goal of descriptor learning: <e1>matching</e1> patches should be close in the descriptor space, while the non-<e2>matching</e2> patches should be far-away 1 ."
sameAs(e1, e2)
Comment:

7012	"in addition to the model itself, the most important aspect of learning-based method is the loss function which defines the goal of descriptor learning: matching <e1>patches</e1> should be close in the descriptor space, while the non-matching <e2>patches</e2> should be far-away 1 ."
sameAs(e1, e2)
Comment:

7013	"the success of global loss motivates us to further explore the desired <e1>properties</e1> of the descriptor space and design a robust regularization term based on these <e2>properties</e2>."
sameAs(e1, e2)
Comment:

7014	"kim et al increased the <e1>network</e1> depth in vdsr [10] and drcn [11] by using gradient clipping, skip connection, or recursive-supervision to ease the difficulty of training deep <e2>network</e2>."
sameAs(e1, e2)
Comment:

7015	"although the gate unit in <e1>memory</e1> block was proposed to control short-term <e2>memory</e2> [26] , the local convolutional layers don't have direct access to the subsequent layers."
sameAs(e1, e2)
Comment:

7016	"although the gate unit in memory block was proposed to control short-term memory [26] , the local convolutional <e1>layers</e1> don't have direct access to the subsequent <e2>layers</e2>."
sameAs(e1, e2)
Comment:

7017	"although <e1>memory</e1> block [26] also takes information from preceding <e2>memory</e2> blocks as input, the multi-level features are not extracted from the original lr image."
sameAs(e1, e2)
Comment:

7018	"in <e1>this</e1> paper, we propose a novel residual dense network (rdn) to address <e2>this</e2> problem in image sr. we fully exploit the hierarchical features from all the convolutional layers."
sameAs(e1, e2)
Comment:

7019	"in this paper, <e1>we</e1> propose a novel residual dense network (rdn) to address this problem in image sr. <e2>we</e2> fully exploit the hierarchical features from all the convolutional layers."
sameAs(e1, e2)
Comment:

7020	"after extracting multi-level local dense <e1>features</e1>, we further conduct global feature fusion (gff) to adaptively preserve the hierarchical <e2>features</e2> in a global way."
sameAs(e1, e2)
Comment:

7021	"with global residual learning, we combine the shallow <e1>features</e1> and deep <e2>features</e2> together, resulting in global dense features from the original lr image."
sameAs(e1, e2)
Comment:

7022	"with global residual learning, we combine the shallow <e1>features</e1> and deep features together, resulting in global dense <e2>features</e2> from the original lr image."
sameAs(e1, e2)
Comment:

7023	"with global residual learning, we combine the shallow features and deep <e1>features</e1> together, resulting in global dense <e2>features</e2> from the original lr image."
sameAs(e1, e2)
Comment:

7024	"introduction single image super-resolution (sisr) aims to generate a visually pleasing high-<e1>resolution</e1> (hr) image from its degraded low-<e2>resolution</e2> (lr) measurement."
sameAs(e1, e2)
Comment:

7025	"the main steps of this kind of method are: 1) segmenting the foreground; 2) extracting various <e1>features</e1> from the foreground, such as area of crowd mask [4, 7, 27, 23] , edge count [4, 7, 27, 25] , or texture <e2>features</e2> [22, 7] ; 3) utilizing a regression function to estimate the crowd count."
sameAs(e1, e2)
Comment:

7026	"[28] has utilized the <e1>features</e1> extracted from a pre-trained cnn to train a <e2>support vector machine</e2> (svm) that subsequently generates counts for still images."
Used-for(e1, e2)
Comment:

7027	"[28] has utilized the <e1>features</e1> extracted from a pre-trained cnn to train a support vector machine (<e2>svm</e2>) that subsequently generates counts for still images."
Used-for(e1, e2)
Comment:

7028	"[28] has utilized the features extracted from a pre-trained cnn to train a <e1>support vector machine</e1> (<e2>svm</e2>) that subsequently generates counts for still images."
sameAs(e1, e2)
Comment:

7029	"but their method requires perspective maps both on training <e1>scenes</e1> and the test <e2>scene</e2>."
sameAs(e1, e2)
Comment:

7030	"however foreground <e1>segmentation</e1> is a challenging task all by itself and inaccurate <e2>segmentation</e2> will have irreversible bad effect on the final count."
sameAs(e1, e2)
Comment:

7031	"3. as there might be significant variation in the <e1>scale</e1> of the people in the images, we need to utilize features at different <e2>scales</e2> all together in order to accurately estimate crowd counts for different images."
sameAs(e1, e2)
Comment:

7032	"3. as there might be significant variation in the scale of the people in the <e1>images</e1>, we need to utilize features at different scales all together in order to accurately estimate crowd counts for different <e2>images</e2>."
sameAs(e1, e2)
Comment:

7033	"since <e1>we</e1> do not have tracked features and it is difficult to handcraft features for all different scales, <e2>we</e2> have to resort to methods that can automatically learn effective features."
sameAs(e1, e2)
Comment:

7034	"since we do not have tracked <e1>features</e1> and it is difficult to handcraft <e2>features</e2> for all different scales, we have to resort to methods that can automatically learn effective features."
sameAs(e1, e2)
Comment:

7035	"since we do not have tracked <e1>features</e1> and it is difficult to handcraft features for all different scales, we have to resort to methods that can automatically learn effective <e2>features</e2>."
sameAs(e1, e2)
Comment:

7036	"since we do not have tracked features and it is difficult to handcraft <e1>features</e1> for all different scales, we have to resort to methods that can automatically learn effective <e2>features</e2>."
sameAs(e1, e2)
Comment:

7037	"then final <e1>predictions</e1> are obtained by averaging individual <e2>predictions</e2> of all deep neural networks."
sameAs(e1, e2)
Comment:

7038	"however, constructing a perceptual <e1>metric</e1> is challenging, because human judgments of similarity (1) depend on high-order image structure [57] , (2) are context-dependent [ 19, 36, 35] , and (3) may not actually constitute a distance <e2>metric</e2> [55] ."
sameAs(e1, e2)
Comment:

7039	"the crux of (2) is <e1>that</e1> there are many different "senses of similarity" <e2>that</e2> we can simultaneously hold in mind: is a red circle more similar to a red square or to a blue circle?"
sameAs(e1, e2)
Comment:

7040	"indeed, we show in <e1>this</e1> paper a negative result where <e2>this</e2> approach fails to generalize, even when trained on a large-scale dataset containing many distortion types."
sameAs(e1, e2)
Comment:

7041	"for example, features from the vgg architecture [51] have been used on tasks such as neural style transfer [17] , <e1>image</e1> superresolution [23] , and conditional <e2>image</e2> synthesis [14, 8] ."
sameAs(e1, e2)
Comment:

7042	"furthermore, the best performing self-supervised networks, including bigans [13] , crosschannel prediction [63] , and puzzle solving [40] perform just as well at this <e1>task</e1>, even without the benefit of humanlabeled training <e2>data</e2>."
Conjunction(e1, e2)
Comment:

7043	"previous databases summarize their judgments into a <e1>mean opinion score</e1> (<e2>mos</e2>); we simply report pairwise judgments (two alternative force choice)."
sameAs(e1, e2)
Comment:

7044	"for instance, in <e1>image</e1> compression, the goal is for the compressed <e2>image</e2> to be indistinguishable from the original by a human observer, irrespective of the fact that their pixel representations might be very different."
sameAs(e1, e2)
Comment:

7045	"prior work on <e1>datasets</e1> in order to evaluate existing similarity measures, a number of <e2>datasets</e2> have been proposed."
sameAs(e1, e2)
Comment:

7046	"these <e1>datasets</e1> are referred to full-reference image quality assessment (fr-iqa) <e2>datasets</e2> and have served as the de-facto baselines for development and evaluation of similarity metrics."
sameAs(e1, e2)
Comment:

7047	"similarly, we not only assess image patch similarity on parameterized distortions, but also test <e1>generalization</e1> to real algorithms, as well as <e2>generalization</e2> to a separate perceptual task -just noticeable differences."
sameAs(e1, e2)
Comment:

7048	"with a different way to calculate landmark coordinates, the <e1>image</e1> decoding module can make the landmark configuration informative regarding <e2>image</e2> reconstruction."
sameAs(e1, e2)
Comment:

7049	"4. our landmark-based <e1>image</e1> decoder is useful for controllable <e2>image</e2> decoding, such as object shape manipulation and structure-conditioned image generation."
sameAs(e1, e2)
Comment:

7050	"4. our landmark-based <e1>image</e1> decoder is useful for controllable image decoding, such as object shape manipulation and structure-conditioned <e2>image</e2> generation."
sameAs(e1, e2)
Comment:

7051	"4. our landmark-based image decoder is useful for controllable <e1>image</e1> decoding, such as object shape manipulation and structure-conditioned <e2>image</e2> generation."
sameAs(e1, e2)
Comment:

7052	"modern <e1>neural networks</e1> can learn latent representations to effectively solve various vision problems, including <e2>image classification</e2> [26, 53, 56, 20] , segmentation [32, 40, 21] , object detection [17, 80, 49] , human pose estimation [39] , 3d reconstruction [13, 67, 14] , and image generation [25, 18, 43] ."
Used-for(e1, e2)
Comment:

7053	"thewlis et al [59] proposed an unsupervised <e1>method</e1> to locate landmarks at the places where a convolutional neural network can detect stable visual patterns with high spatial equivariance to image <e2>transformations</e2>."
Used-for(e1, e2)
Comment:

7054	"however, most existing efforts to the latter vision-language tasks attempt to directly bridge the visual model (e.g., cnn) and the language model (e.g., <e1>rnn</e1>), but fall short in <e2>modeling</e2> and understanding the relationships between objects."
Used-for(e1, e2)
Comment:

7055	"as a result, poor generalization ability was observed as those <e1>models</e1> are often optimized on specialized datasets for specific <e2>tasks</e2> such as image captioning or image qa."
Used-for(e1, e2)
Comment:

7056	"as a result, poor generalization ability was observed as those models are often optimized on specialized datasets for specific tasks such as <e1>image</e1> captioning or <e2>image</e2> qa."
sameAs(e1, e2)
Comment:

7057	"we propose a novel feature extraction layer <e1>that</e1> enables object-relation knowledge transfer in a fully-convolutional fashion <e2>that</e2> supports training and inference in a single forward/backward pass."
sameAs(e1, e2)
Comment:

7058	"note that even though vtranse is a purely visual <e1>model</e1>, it is still competitive to the lu's multi-modal <e2>model</e2> with language priors [27] ."
sameAs(e1, e2)
Comment:

7059	"introduction image inpainting is the process of filling in missing <e1>regions</e1> with plausible hypothesis, and can be used in many real world applications such as removing distracting objects, repairing corrupted or damaged parts, and completing occluded <e2>regions</e2>."
sameAs(e1, e2)
Comment:

7060	"in this paper, we present a novel <e1>cnn</e1>, namely shift-net, to take into account the advantages of both exemplar-based and <e2>cnn</e2>-based methods for image inpainting."
sameAs(e1, e2)
Comment:

7061	"we note that cnn is effective in predicting the image <e1>structure</e1> and <e2>semantics</e2> of the missing parts."
Conjunction(e1, e2)
Comment:

7062	"guided by the salient structure produced by <e1>cnn</e1>, the filling process in our shift-net can be finished concurrently by introducing a shift-connection layer to connect the encoder feature of known region and the <e2>decoder</e2> feature of missing parts."
Used-for(e1, e2)
Comment:

7063	"guided by the salient structure produced by cnn, the filling process in our shift-net can be finished concurrently by introducing a shift-connection layer to connect the encoder <e1>feature</e1> of known region and the decoder <e2>feature</e2> of missing parts."
sameAs(e1, e2)
Comment:

7064	"to ensure that the <e1>decoder</e1> feature can serve as a good guidance, a guidance loss is introduced to enforce the <e2>decoder</e2> feature be close to the ground-truth encoder feature."
sameAs(e1, e2)
Comment:

7065	"to ensure that the decoder <e1>feature</e1> can serve as a good guidance, a guidance loss is introduced to enforce the decoder <e2>feature</e2> be close to the ground-truth encoder feature."
sameAs(e1, e2)
Comment:

7066	"to ensure that the decoder <e1>feature</e1> can serve as a good guidance, a guidance loss is introduced to enforce the decoder feature be close to the ground-truth encoder <e2>feature</e2>."
sameAs(e1, e2)
Comment:

7067	"to ensure that the decoder feature can serve as a good guidance, a guidance loss is introduced to enforce the decoder <e1>feature</e1> be close to the ground-truth encoder <e2>feature</e2>."
sameAs(e1, e2)
Comment:

7068	"the <e1>results</e1> show that our shift-net can handle missing regions with any shape, and is effective in producing sharper, fine-detailed, and visually plausible <e2>results</e2> (see fig."
sameAs(e1, e2)
Comment:

7069	"besides, yang et al [41] also suggest a multi-scale neural patch synthesis (mnps) <e1>approach</e1> to incorporating cnn-based with exemplar-based <e2>methods</e2>."
Compare(e1, e2)
Comment:

7070	"3. our shift-net achieves state-of-the-art <e1>results</e1> in comparison with [1, 28, 41] and performs favorably in generating fine-detailed textures and visually plausible <e2>results</e2>."
sameAs(e1, e2)
Comment:

7071	"related work in <e1>this</e1> section, we briefly review the work on each of the three sub-fields, i.e., exemplar-based inpainting, cnn-based inpainting, and style transfer, and specially focus on those relevant to <e2>this</e2> work."
sameAs(e1, e2)
Comment:

7072	"in <e1>these</e1> cases, image inpainting can serve as a remedy to remove <e2>these</e2> elements and fill in with plausible content."
sameAs(e1, e2)
Comment:

7073	"martens (2010) introduced <e1>hessian</e1>-free optimization, a variant of truncated-newton methods that relies on using the linear conjugate gradient to avoid computing the <e2>hessian</e2>."
sameAs(e1, e2)
Comment:

7074	"recent advances in deep learning optimization focus mainly on <e1>stochastic gradient descent</e1> (<e2>sgd</e2>) (bottou, 1998) and its variants (sutskever et al, 2013) ."
sameAs(e1, e2)
Comment:

7075	"another problem with the ftrl family of algorithms is <e1>that</e1> in each round, the learner has to solve an optimization problem <e2>that</e2> considers the sum of all previous gradients."
sameAs(e1, e2)
Comment:

7076	"well-known examples include the convolutional neural network (lecun et al, 1998) , long short term <e1>memory</e1> (hochreiter & schmidhuber, 1997) , <e2>memory</e2> network (weston et al, 2014) , and deep q-network (mnih et al, 2015) ."
sameAs(e1, e2)
Comment:

7077	"well-known examples include the convolutional neural network (lecun et al, 1998) , long short term memory (hochreiter & schmidhuber, 1997) , memory <e1>network</e1> (weston et al, 2014) , and deep q-<e2>network</e2> (mnih et al, 2015) ."
sameAs(e1, e2)
Comment:

7078	"these <e1>models</e1> have achieved remarkable performance on various difficult <e2>tasks</e2> such as image classification (he et al, 2016) , speech recognition (graves et al, 2013) , natural language understanding (bahdanau et al, 2015; sukhbaatar et al, 2015) , and game playing ."
Used-for(e1, e2)
Comment:

7079	"in particular, markov random fields (mrfs) and its variant <e1>conditional random fields</e1> (<e2>crfs</e2>) have observed widespread success in this area [30, 27] and have become one of the most successful graphical models used in computer vision."
sameAs(e1, e2)
Comment:

7080	"one way to utilize crfs to improve the semantic labelling results produced by a <e1>cnn</e1> is to apply crf inference as a post-processing step disconnected from the training of the <e2>cnn</e2> [9] ."
sameAs(e1, e2)
Comment:

7081	"arguably, this does not fully harness the strength of crfs since it is not integrated with the deep <e1>network</e1> -the deep <e2>network</e2> cannot adapt its weights to the crf behaviour during the training phase."
sameAs(e1, e2)
Comment:

7082	"in this paper, we propose an end-to-end deep learning <e1>solution</e1> for the pixel-level semantic image segmentation <e2>problem</e2>."
Used-for(e1, e2)
Comment:

7083	"more specifically, we formulate mean-field inference of dense crf with gaussian pairwise potentials as a <e1>recurrent neural network</e1> (<e2>rnn</e2>) which can refine coarse outputs from a traditional cnn in the forward pass, while passing error differentials back to the cnn during training."
sameAs(e1, e2)
Comment:

7084	"more specifically, we formulate mean-field inference of dense crf with gaussian pairwise potentials as a recurrent neural network (rnn) which can refine coarse outputs from a traditional <e1>cnn</e1> in the forward pass, while passing error differentials back to the <e2>cnn</e2> during training."
sameAs(e1, e2)
Comment:

7085	"arguably, when properly trained, the proposed network should outperform a <e1>system</e1> where crf inference is applied as a post-processing <e2>method</e2> on independent pixel-level predictions produced by a pre-trained cnn."
Used-for(e1, e2)
Comment:

7086	"recently, supervised <e1>deep learning</e1> approaches such as large-scale deep convolutional neural networks (cnns) have been immensely successful in many high-level computer vision tasks such as image recognition [29] and <e2>object detection</e2> [19] ."
Used-for(e1, e2)
Comment:

7087	"we provide an analytical justification illustrating the benefits of <e1>feature</e1> normalization and thereby cosine <e2>feature</e2> embeddings."
sameAs(e1, e2)
Comment:

7088	"we find that ring loss provides consistent improvements over a large range of its hyperparameter when compared to <e1>other</e1> baselines in normalization and indeed <e2>other</e2> losses proposed for face recognition in general."
sameAs(e1, e2)
Comment:

7089	"a good <e1>understanding</e1> of the challenges in this task results  in a better <e2>understanding</e2> of the core problems in supervised classification, and in general representation learning."
sameAs(e1, e2)
Comment:

7090	"it is challenging in 1) how to obtain more <e1>training</e1> data only from the <e2>training</e2> set and 2) how to use the newly generated data."
sameAs(e1, e2)
Comment:

7091	"it is challenging in 1) how to obtain more training <e1>data</e1> only from the training set and 2) how to use the newly generated <e2>data</e2>."
sameAs(e1, e2)
Comment:

7092	"this <e1>method</e1> assigns a uniform label distribution to the unlabeled images, which regularizes the supervised <e2>model</e2> and improves the baseline."
Used-for(e1, e2)
Comment:

7093	"this <e1>method</e1> assigns a uniform label distribution to the unlabeled images, which regularizes the supervised model and improves the <e2>baseline</e2>."
Compare(e1, e2)
Comment:

7094	"the proposed tracker can perform at 160 fps on short-term <e1>benchmarks</e1> and 110 fps on long-term <e2>benchmarks</e2>."
sameAs(e1, e2)
Comment:

7095	"recently, siamese networks, which follow a <e1>tracking</e1> by similarity comparison strategy, have drawn great attention in visual <e2>tracking</e2> community because of favorable performance [31, 8, 2, 36, 33, 7, 37, 16] ."
sameAs(e1, e2)
Comment:

7096	"siamrpn [16] introduces a region proposal <e1>network</e1> after the siamese <e2>network</e2>, thus formulating the tracking as a one-shot local detection task."
sameAs(e1, e2)
Comment:

7097	"although these <e1>tracking</e1> approaches obtain balanced accuracy and speed, there are 3 problems that should be addressed: firstly, features used in most siamese <e2>tracking</e2> approaches can only discriminate foreground from the nonsemantic background."
sameAs(e1, e2)
Comment:

7098	"although these tracking <e1>approaches</e1> obtain balanced accuracy and speed, there are 3 problems that should be addressed: firstly, features used in most siamese tracking <e2>approaches</e2> can only discriminate foreground from the nonsemantic background."
sameAs(e1, e2)
Comment:

7099	"although their simplicity and fixed-<e1>model</e1> nature lead to high speed, these <e2>methods</e2> lose the ability to update the appearance model online which is often critical to account for drastic appearance changes in tracking scenarios."
Compare(e1, e2)
Comment:

7100	"although their simplicity and fixed-<e1>model</e1> nature lead to high speed, these methods lose the ability to update the appearance <e2>model</e2> online which is often critical to account for drastic appearance changes in tracking scenarios."
sameAs(e1, e2)
Comment:

7101	"in this paper, we identify that the imbalance of the non-<e1>semantic</e1> background and <e2>semantic</e2> distractor in the training data is the main obstacle for the representation learning."
sameAs(e1, e2)
Comment:

7102	"except the challenging situations in short-term <e1>tracking</e1>, severe out-ofview and full occlusion introduce extra challenges in long-term <e2>tracking</e2>."
sameAs(e1, e2)
Comment:

7103	"besides the favorable performance, our tracker can perform at far beyond real-time speed: 160 fps on short-term <e1>datasets</e1> and 110 fps on long-term <e2>datasets</e2>."
sameAs(e1, e2)
Comment:

7104	"due to the open-set nature, <e1>one</e1> may want to generalize the feature embedding instead of learning another parametric <e2>classifier</e2>."
isA(e1, e2)
Comment:

7105	"so far, none of these non-parametric methods have become competitive in the state-of-the-art <e1>image</e1> recognition benchmarks such as imagenet <e2>classification</e2> [31] and mscoco object detection [21] ."
Used-for(e1, e2)
Comment:

7106	"1) we <e1>scale</e1> up nca to handle large-<e2>scale</e2> datasets and deep neural networks by using an augmented memory to store non-parametric embeddings."
sameAs(e1, e2)
Comment:

7107	"we adopt a non-parametric <e1>approach</e1> for visual recognition by optimizing feature embeddings instead of parametric <e2>classifiers</e2>."
Used-for(e1, e2)
Comment:

7108	"we use a deep neural network to learn the visual feature that preserves the <e1>neighborhood</e1> structure in the semantic space, based on the <e2>neighborhood</e2> component analysis (nca) criterion."
sameAs(e1, e2)
Comment:

7109	"east: an efficient and accurate <e1>scene</e1> <e2>text</e2> detector"
Conjunction(e1, e2)
Comment:

7110	"in this paper, we propose a fast and accurate <e1>scene</e1> <e2>text</e2> detection pipeline that has only two stages."
Conjunction(e1, e2)
Comment:

7111	"the core of <e1>text</e1> detection is the design of features to distinguish <e2>text</e2> from backgrounds."
sameAs(e1, e2)
Comment:

7112	"6. <e1>features</e1> are manually designed [5, 25, 40, 10, 26, 45] to capture the properties of scene text, while in deep learning based methods [3, 13, 11, 12, 7, 48] effective <e2>features</e2> are directly learned from training data."
sameAs(e1, e2)
Comment:

7113	"6. features are manually designed [5, 25, 40, 10, 26, 45] to capture the properties of <e1>scene</e1> <e2>text</e2>, while in deep learning based methods [3, 13, 11, 12, 7, 48] effective features are directly learned from training data."
Conjunction(e1, e2)
Comment:

7114	"despite having <e1>this</e1> remarkable ability to localize objects in the convolutional layers, <e2>this</e2> ability is lost when fully-connected layers are used for classification."
sameAs(e1, e2)
Comment:

7115	"despite having this remarkable ability to localize objects in the convolutional <e1>layers</e1>, this ability is lost when fully-connected <e2>layers</e2> are used for classification."
sameAs(e1, e2)
Comment:

7116	"recently some popular fully-convolutional neural networks such as the <e1>network</e1> in <e2>network</e2> (nin) [13] and googlenet [25] have been proposed to avoid the use of fully-connected layers to minimize the number of parameters while maintaining high performance."
sameAs(e1, e2)
Comment:

7117	" introduction with the advances of<e1> imag</e1>e editing techniques and userfriendly editing software, low-cost tampered or manipulated<e2> imag</e2>e generation processes have become widely available."
sameAs(e1, e2)
Comment:

7118	"image splicing copies <e1>regions</e1> from an authentic image and pastes them to other images, copy-move copies and pastes <e2>regions</e2> within the same image, and removal eliminates regions from an authentic image followed by inpainting."
sameAs(e1, e2)
Comment:

7119	"image splicing copies <e1>regions</e1> from an authentic image and pastes them to other images, copy-move copies and pastes regions within the same image, and removal eliminates <e2>regions</e2> from an authentic image followed by inpainting."
sameAs(e1, e2)
Comment:

7120	"image splicing copies regions from an authentic <e1>image</e1> and pastes them to other images, copy-move copies and pastes regions within the same <e2>image</e2>, and removal eliminates regions from an authentic image followed by inpainting."
sameAs(e1, e2)
Comment:

7121	"image splicing copies regions from an authentic <e1>image</e1> and pastes them to other images, copy-move copies and pastes regions within the same image, and removal eliminates regions from an authentic <e2>image</e2> followed by inpainting."
sameAs(e1, e2)
Comment:

7122	"image splicing copies regions from an authentic image and pastes them to other images, copy-move copies and pastes <e1>regions</e1> within the same image, and removal eliminates <e2>regions</e2> from an authentic image followed by inpainting."
sameAs(e1, e2)
Comment:

7123	"image splicing copies regions from an authentic image and pastes them to other images, copy-move copies and pastes regions within the same <e1>image</e1>, and removal eliminates regions from an authentic <e2>image</e2> followed by inpainting."
sameAs(e1, e2)
Comment:

7124	"therefore, existing weakly supervised <e1>semantic segmentation</e1> methods cannot be simply generalized to instance-level <e2>semantic segmentation</e2> [16, 12] , which aims to detect all objects in an image as well as predicting precise masks for each instance."
sameAs(e1, e2)
Comment:

7125	"the above <e1>maps</e1> generated from class peak responses are referred to as peak response <e2>maps</e2> (prms)."
sameAs(e1, e2)
Comment:

7126	"in contrast, <e1>image</e1>-level annotations, i.e., presence or absence of object categories in an <e2>image</e2>, are much cheaper and easier to define."
sameAs(e1, e2)
Comment:

7127	"compared with many fully supervised approaches that typically use complex <e1>frameworks</e1> including conditional random fields (crf) [46, 45] , recurrent neural networks (rnn) [30, 32] , or template matching [37] , to handle instance extraction; our <e2>approach</e2> is simple yet effective."
Used-for(e1, e2)
Comment:

7128	"compared with many fully supervised approaches that typically use complex frameworks including conditional random fields (crf) [46, 45] , <e1>recurrent neural networks</e1> (<e2>rnn</e2>) [30, 32] , or template matching [37] , to handle instance extraction; our approach is simple yet effective."
sameAs(e1, e2)
Comment:

7129	"we first stimulate peaks to emerge from a class response <e1>map</e1> and then back-propagate them to <e2>map</e2> to highly informative regions of each object instance, such as instance boundaries."
sameAs(e1, e2)
Comment:

7130	"such class response maps indicate essential <e1>image</e1> regions used by the network to identify an <e2>image</e2> class; however, cannot distinguish different object instances from the same category."
sameAs(e1, e2)
Comment:

7131	"all these methods suffer from huge <e1>training</e1> complexity, because they directly train the cnns using the triplets, the number of which scales cubically with the number of images in the <e2>training</e2> set."
sameAs(e1, e2)
Comment:

7132	"all these methods suffer from huge training complexity, because they directly train the cnns using the triplets, the <e1>number</e1> of which scales cubically with the <e2>number</e2> of images in the training set."
sameAs(e1, e2)
Comment:

7133	"this two-step <e1>approach</e1> enables us to convert tripletbased hashing into an efficient combination of solving binary quadratic programs and learning conventional cnn <e2>classifiers</e2>."
Used-for(e1, e2)
Comment:

7134	"the two-step <e1>approach</e1> to hashing advocated by [17, 18] uses decision trees as hash functions in combination with the design of efficient binary code inference <e2>methods</e2>."
Compare(e1, e2)
Comment:

7135	"hashing methods aim to map the original <e1>features</e1> to compact binary codes that are able to preserve the semantic structure of the original <e2>features</e2> in the hamming space."
sameAs(e1, e2)
Comment:

7136	"we also demonstrate our hashing <e1>method</e1> in the context of a face <e2>search</e2>/retrieval system."
Used-for(e1, e2)
Comment:

7137	"after obtaining a sufficiently good solution of the first stage, the activation of the <e1>network</e1> is further required to be in low-precision and the <e2>network</e2> will be trained again."
sameAs(e1, e2)
Comment:

7138	"essentially, this progressive approach first solves a related subproblem, i.e., <e1>training</e1> a network with only low-bit weights and the solution of the sub-problem provides a good initial point for <e2>training</e2> our target problem."
sameAs(e1, e2)
Comment:

7139	"essentially, this progressive approach first solves a related subproblem, i.e., training a network with only low-bit weights and the <e1>solution</e1> of the sub-<e2>problem</e2> provides a good initial point for training our target problem."
Used-for(e1, e2)
Comment:

7140	"essentially, this progressive approach first solves a related subproblem, i.e., training a network with only low-bit weights and the <e1>solution</e1> of the sub-problem provides a good initial point for training our target <e2>problem</e2>."
Used-for(e1, e2)
Comment:

7141	"essentially, this progressive approach first solves a related subproblem, i.e., training a network with only low-bit weights and the solution of the sub-<e1>problem</e1> provides a good initial point for training our target <e2>problem</e2>."
sameAs(e1, e2)
Comment:

7142	"specifically, we incrementally train a serial of networks with the quantization bit-width (<e1>precision</e1>) gradually decreased from full-<e2>precision</e2> to the target precision."
sameAs(e1, e2)
Comment:

7143	"specifically, we incrementally train a serial of networks with the quantization bit-width (<e1>precision</e1>) gradually decreased from full-precision to the target <e2>precision</e2>."
sameAs(e1, e2)
Comment:

7144	"specifically, we incrementally train a serial of networks with the quantization bit-width (precision) gradually decreased from full-<e1>precision</e1> to the target <e2>precision</e2>."
sameAs(e1, e2)
Comment:

7145	"the basic idea of those works is to train a target <e1>network</e1> alongside another guidance <e2>network</e2>."
sameAs(e1, e2)
Comment:

7146	"for example, the works in [1, 11, 22, 24, 32] propose to train a small student <e1>network</e1> to mimic the deeper or wider teacher <e2>network</e2>."
sameAs(e1, e2)
Comment:

7147	"it is observed that by using the guidance of the teacher <e1>model</e1>, better performance can be obtained with the student <e2>model</e2> than directly training the student model on the target problem."
sameAs(e1, e2)
Comment:

7148	"it is observed that by using the guidance of the teacher <e1>model</e1>, better performance can be obtained with the student model than directly training the student <e2>model</e2> on the target problem."
sameAs(e1, e2)
Comment:

7149	"it is observed that by using the guidance of the teacher model, better performance can be obtained with the student <e1>model</e1> than directly training the student <e2>model</e2> on the target problem."
sameAs(e1, e2)
Comment:

7150	"motivated by these observations, we propose to train a full-<e1>precision</e1> network alongside the target low-<e2>precision</e2> network."
sameAs(e1, e2)
Comment:

7151	"motivated by these observations, we propose to train a full-precision <e1>network</e1> alongside the target low-precision <e2>network</e2>."
sameAs(e1, e2)
Comment:

7152	"rather, <e1>we</e1> allow the two models to be trained jointly from scratch since <e2>we</e2> discover that this treatment enables the two nets adjust better to each other."
sameAs(e1, e2)
Comment:

7153	"rather, we allow the <e1>two</e1> models to be trained jointly from scratch since we discover that this treatment enables the <e2>two</e2> nets adjust better to each other."
sameAs(e1, e2)
Comment:

7154	"for example, some methods adopt a layer-wise <e1>training</e1> procedure [30] , thus their <e2>training</e2> cost will be significantly increased if the number of layers becomes larger."
sameAs(e1, e2)
Comment:

7155	"work <e1>weights</e1> [7, 8] , low rank approximation of <e2>weights</e2> [16, 34] , and training a low-bit-precision network [4, [36] [37] [38] ."
sameAs(e1, e2)
Comment:

7156	"in this work, we follow the idea of training a low-precision <e1>network</e1> and our focus is to improve the training process of such a <e2>network</e2>."
sameAs(e1, e2)
Comment:

7157	"although this treatment leads to lower performance decrease comparing to its full-<e1>precision</e1> counterpart, it still needs substantial amount of computational resource requirement to handle the full-<e2>precision</e2> activations."
sameAs(e1, e2)
Comment:

7158	"learning spatial regularization with <e1>image</e1>-level supervisions for multi-label <e2>image</e2> classification"
sameAs(e1, e2)
Comment:

7159	"deep convolution <e1>neural networks</e1> (cnns) [21, 33, 32, 13] have achieved great success on single-label <e2>image classification</e2> in recent years."
Used-for(e1, e2)
Comment:

7160	"inspired by recent success of attention mechanism in many vision tasks [40, 43, 15] , we propose a deep neural network for multi-label classification, which consists of a sub-network, spatial <e1>regularization</e1> net (srn), to learn spatial <e2>regularizations</e2> between labels with only image-level supervisions."
sameAs(e1, e2)
Comment:

7161	"binary relevance method [34] is an easy way to extend single-label algorithms to solve multi-label classification, which simply trains <e1>one</e1> binary <e2>classifier</e2> for each label."
isA(e1, e2)
Comment:

7162	"to cope with the <e1>problem</e1> that labels may relate to different visual regions over the whole image, proposal-based approaches [38] are proposed to transform multi-label classification <e2>problem</e2> into multiple single-label classification tasks."
sameAs(e1, e2)
Comment:

7163	"to cope with the problem that labels may relate to different visual regions over the whole <e1>image</e1>, proposal-based approaches [38] are proposed to transform multi-label <e2>classification</e2> problem into multiple single-label classification tasks."
Used-for(e1, e2)
Comment:

7164	"such relations or dependency can be modeled by probabilistic graphical models [23, 22] , structured inference neural network [16] , or <e1>recurrent neural networks</e1> (<e2>rnns</e2>) [36] ."
sameAs(e1, e2)
Comment:

7165	"in <e1>this</e1> paper, we propose to mimic <e2>this</e2> process by exploring the saliency detection problem on 360 • videos."
sameAs(e1, e2)
Comment:

7166	"despite significant progresses in convolutional neural networks (cnn) [4] for saliency detection in images/videos [5] [6] , there are very little, if few, studies on • <e1>image</e1> on sphere; right: 360 • <e2>image</e2> on equirectangular panorama."
sameAs(e1, e2)
Comment:

7167	"directly applying perspective based saliency detection onto the panorama <e1>images</e1> is also problematic: panoramic <e2>images</e2> exhibit geometric distortion where many useful saliency cues are not valid."
sameAs(e1, e2)
Comment:

7168	"this implies that the <e1>gaze</e1> in the previous frame affects the <e2>gaze</e2> in the subsequent frames."
sameAs(e1, e2)
Comment:

7169	"specifically, in our spherical <e1>convolution</e1> neural network definition, kernel is defined on a spherical crown, and the <e2>convolution</e2> involves the rotation of the kernel along the sphere."
sameAs(e1, e2)
Comment:

7170	"by far, nearly all saliency detection <e1>datasets</e1> are based on narrow fov perspective images while only a few <e2>datasets</e2> on 360 • images."
sameAs(e1, e2)
Comment:

7171	"by far, nearly all saliency detection datasets are based on narrow fov perspective <e1>images</e1> while only a few datasets on 360 • <e2>images</e2>."
sameAs(e1, e2)
Comment:

7172	"to validate our approach, we construct a large-scale 360 • <e1>videos</e1> saliency detection benchmark that consists of 104 360 • <e2>videos</e2> viewed by 20+ human subjects."
sameAs(e1, e2)
Comment:

7173	"we further extend it to panorama case; ii) <e1>we</e1> propose a sequential saliency detection scheme and instantiate the spherical convolutional neural networks with a spherical u-net architecture for frame-wise saliency detection; iii) <e2>we</e2> build a largescale 360 • video saliency detection dataset which would facilitate the evaluation of saliency detection in 360 • videos."
sameAs(e1, e2)
Comment:

7174	"to validate our approach, we construct a large-scale 360 • <e1>videos</e1> saliency detection benchmark that consists of 104 360 • <e2>videos</e2> viewed by 20+ human subjects."
sameAs(e1, e2)
Comment:

7175	"due to their sequential nature, <e1>recurrent neural networks</e1> (<e2>rnns</e2>; robinson & fallside, 1987; werbos, 1988; williams, 1989) have long credit assignment paths and so are deep in time."
sameAs(e1, e2)
Comment:

7176	"introduction network depth is of central importance in the resurgence of <e1>neural networks</e1> as a powerful <e2>machine learning</e2> paradigm (schmidhuber, 2015) ."
isA(e1, e2)
Comment:

7177	"meanwhile, lift [34] and [19] seek to enhance <e1>data</e1> diversity and generate training <e2>data</e2> from reconstructions of internet tourism data."
sameAs(e1, e2)
Comment:

7178	"meanwhile, lift [34] and [19] seek to enhance <e1>data</e1> diversity and generate training data from reconstructions of internet tourism <e2>data</e2>."
sameAs(e1, e2)
Comment:

7179	"meanwhile, lift [34] and [19] seek to enhance data diversity and generate training <e1>data</e1> from reconstructions of internet tourism <e2>data</e2>."
sameAs(e1, e2)
Comment:

7180	"due to weak semantics and efficiency requirements, existing descriptor learning often relies on shallow and thin <e1>networks</e1>, e.g., threelayer <e2>networks</e2> in ddesc [27] with 128-dimensional output features."
sameAs(e1, e2)
Comment:

7181	"moreover, although widely-used in high-level computer vision tasks, max <e1>pooling</e1> is found to be unsuitable for descriptor learning, which is then replaced by l2 <e2>pooling</e2> in ddesc [27] or even removed in l2-net [29] ."
sameAs(e1, e2)
Comment:

7182	"loss <e1>formulation</e1> various of loss <e2>formulations</e2> have been explored for effective descriptor learning."
sameAs(e1, e2)
Comment:

7183	"both loss formulations encourage <e1>matching</e1> patches to be close whereas non-<e2>matching</e2> patches to be far-away in some measure space."
sameAs(e1, e2)
Comment:

7184	"both loss formulations encourage matching <e1>patches</e1> to be close whereas non-matching <e2>patches</e2> to be far-away in some measure space."
sameAs(e1, e2)
Comment:

7185	" introduction developing<e1> neural networ</e1>k<e2> image classificatio</e2>n models often requires significant architecture engineering."
Used-for(e1, e2)
Comment:

7186	"in our experiments, this approach significantly accelerates the search for the best <e1>architectures</e1> using cifar-10 by a factor of 7× and learns <e2>architectures</e2> that successfully transfer to imagenet."
sameAs(e1, e2)
Comment:

7187	"additionally, by simply varying the <e1>number</e1> of the convolutional cells and <e2>number</e2> of filters in the convolutional cells, we can create different versions of nasnets with different computational demands."
sameAs(e1, e2)
Comment:

7188	"thanks to this property of the cells, we can generate a family of <e1>models</e1> that achieve accuracies superior to all human-invented <e2>models</e2> at equivalent or smaller computational budgets [60, 29] ."
sameAs(e1, e2)
Comment:

7189	"starting from the seminal work of [32] on using convolutional <e1>architectures</e1> [17, 34] for imagenet [11] classification, successive advancements through <e2>architecture</e2> engineering have achieved impressive results [53, 59, 20, 60, 58, 68] ."
sameAs(e1, e2)
Comment:

7190	"in this paper, we study a new paradigm of designing convolutional <e1>architectures</e1> and describe a scalable method to optimize convolutional <e2>architectures</e2> on a dataset of interest, for instance the imagenet classification dataset."
sameAs(e1, e2)
Comment:

7191	"our <e1>approach</e1> is inspired by the recently proposed neural architecture search (nas) framework [71] , which uses a reinforcement learning search <e2>method</e2> to optimize architecture configurations."
Compare(e1, e2)
Comment:

7192	"our approach is inspired by the recently proposed <e1>neural architecture search</e1> (nas) framework [71] , which uses a reinforcement learning <e2>search</e2> method to optimize architecture configurations."
isA(e1, e2)
Comment:

7193	"we therefore propose to search for a good <e1>architecture</e1> on a proxy dataset, for example the smaller cifar-10 dataset, and then transfer the learned <e2>architecture</e2> to imagenet."
sameAs(e1, e2)
Comment:

7194	"we achieve this transferrability by designing a <e1>search space</e1> (which we call "the nasnet <e2>search space</e2>") so that the complexity of the architecture is independent of the depth of the network and the size of input images."
sameAs(e1, e2)
Comment:

7195	"existing works apply to special cases of the problem, such as predicting cuboid-shaped layouts from perspective <e1>images</e1> or from panoramic <e2>images</e2>."
sameAs(e1, e2)
Comment:

