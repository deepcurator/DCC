<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juyong</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yookoon</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><forename type="middle">Ju</forename><surname>Hwang</surname></persName>
						</author>
						<title level="a" type="main">SplitNet: Learning to Semantically Split Deep Networks for Parameter Reduction and Model Parallelization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a novel deep neural network that is both lightweight and effectively structured for model parallelization. Our network, which we name as SplitNet, automatically learns to split the network weights into either a set or a hierarchy of multiple groups that use disjoint sets of features, by learning both the class-to-group and feature-to-group assignment matrices along with the network weights. This produces a treestructured network that involves no connection between branched subtrees of semantically disparate class groups. SplitNet thus greatly reduces the number of parameters and required computations, and is also embarrassingly modelparallelizable at test time, since the evaluation for each subnetwork is completely independent except for the shared lower layer weights that can be duplicated over multiple processors, or assigned to a separate processor. We validate our method with two different deep network models (ResNet and AlexNet) on two datasets (CIFAR-100 and ILSVRC 2012) for image classification, on which our method obtains networks with significantly reduced number of parameters while achieving comparable or superior accuracies over original full deep networks, and accelerated test speed with multiple GPUs.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, deep neural networks have shown impressive performances on a multitude of tasks, including visual recognition <ref type="bibr" target="#b15">(Krizhevsky et al., 2012;</ref><ref type="bibr" target="#b21">Szegedy et al., 2015;</ref><ref type="bibr" target="#b10">He et al., 2016)</ref>, speech recognition , and natural language processing <ref type="bibr" target="#b3">(Bengio et al., 2003;</ref><ref type="bibr" target="#b20">Sutskever et al., 2014)</ref>. However, such remarkable performances are achieved at the cost of increased computational complexity at both training and test time compared to traditional machine learning models including shallow neural networks. This increased complexity of deep networks can be problematic if the model and the task size becomes very large (e.g. classifying tens of thousands of object classes), or the application is time-critical (e.g. real-time object detection).</p><p>There exist various solutions to tackle this complexity issue. One way is to reduce the number of model parameters; this could be achieved either by training a new smaller network while maintaining similar behavior as the original network <ref type="bibr" target="#b2">(Ba &amp; Caruana, 2014;</ref><ref type="bibr" target="#b12">Hinton et al., 2014)</ref>, or by disconnecting unnecessary weights through pruning or sparsity regularization <ref type="bibr" target="#b18">(Reed, 1993;</ref><ref type="bibr" target="#b9">Han et al., 2015;</ref><ref type="bibr" target="#b5">Collins &amp; Kohli, 2014;</ref><ref type="bibr" target="#b23">Wen et al., 2016;</ref><ref type="bibr" target="#b1">Alvarez &amp; Salzmann, 2016)</ref>. Another approach to speed up deep networks is distributed machine learning <ref type="bibr" target="#b6">(Dean et al., 2012;</ref><ref type="bibr" target="#b4">Chilimbi et al., 2014;</ref><ref type="bibr" target="#b26">Zhang et al., 2015)</ref>; however most research effort has been made on the systems and optimization sides, without consideration of the ways to obtain network structure that is intrinsically scalable.</p><p>In this work, we aim to learn a deep neural network that not only reduces the number of model parameters, but also is effectively structured for model parallelization. How can we then achieve these seemingly disjoint goals in a single, unified learning framework? We focus on the observation that as the number of classes increases, semantically disparate classes (or tasks) can be represented by largely disjoint sets of features. For example, features describing animals may be quite different from those describing vehicles, whereas low-level features such as stripes, dots, and colors are likely be shared across all classes. It implicates that we can cluster classes into mutually exclusive groups based on the features they use. Such grouping of concepts based on semantic proximity also agrees with the way that our brain stores semantic concepts, where semantically related concepts activate similar part of the brain <ref type="bibr" target="#b13">(Huth et al., 2012)</ref>, in a highly localized manner.</p><p>Based on this intuition, we propose a novel deep network architecture named SplitNet that automatically performs  <ref type="figure">Figure 1</ref>. Concept. Our network automatically learns to split the classes and associated features into multiple groups at multiple network layers, obtaining a tree-structured network. Given a base network in (a), (b) our algorithm optimizes network weights as well as class-to-group and feature-to-group assignments. Colors indicate the group assignments of classes and features. (c) After learning to split the network, the model can be distributed to multiple GPUs to accelerate training and inference.</p><p>deep splitting of the network into a set of subnetworks or a hierarchy of subnetworks sharing common lower layers, such that classes in each group share a common subset of features, that is completely disjoint from the features for other class groups. We train such a network by additionally learning classes-to-group assignments and feature-togroup assignments along with the network weights, while regularizing them to be disjoint across groups. <ref type="figure">Figure 1</ref> illustrates the key idea of our proposed approach.</p><p>Our splitting algorithm is fairly generic, and is applicable to any general deep neural networks including feedforward and convolutional networks. We test our method with ResNet <ref type="bibr" target="#b10">(He et al., 2016;</ref><ref type="bibr" target="#b25">Zagoruyko &amp; Komodakis, 2016)</ref> and AlexNet <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref> on CIFAR-100 and ILSVRC 2012 datasets, on which our networks respectively achieve 32% and 56% parameter reduction, while obtaining comparable or even superior performance to the full base network. We further perform test-time parallelization of our network with multiple GPUs, where it achieved 1.73× speedup to naive parallelization method with 2 GPUs. Our contributions in this work are threefold.</p><p>• We propose a novel deep neural network named SplitNet, which is organized as a tree of disjoint subnetworks with greatly reduced the number of parameters and computations in terms of FLOPs, that obtains comparable accuracies to fully-connected networks.</p><p>• We propose an efficient algorithm for automatically training such a tree-structured network, which learns class-to-group and feature-to-group assignments along with the network weights.</p><p>• The networks trained by our approach are embarrassingly model-parallelizable; in distributed learning setting, we show that our networks scale well to an increased number of processors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Parameter reduction for deep neural networks. Achieving test-time efficiency is an active research topic in deep learning. One straightforward approach is to remove weak connections during the training, usually implemented using the 1 -norm <ref type="bibr" target="#b5">(Collins &amp; Kohli, 2014)</ref>. However, the 1 -norm often results in a model that trades-off the accuracy with the efficiency. <ref type="bibr" target="#b9">Han et al. (2015)</ref> presented an iterative weight pruning technique that repeatedly retrains the network while removing of weak connections, which achieves a superior performance over 1 -regularization. Recently, the group sparsity using 2,1 -norm has been explored for learning a compact model. <ref type="bibr" target="#b1">Alvarez &amp; Salzmann (2016)</ref> applied (2,1)-norm regularization at each layer to eliminate the hidden units that are not shared across upper-level units, thus automatically deciding how many neurons to use at each layer. <ref type="bibr" target="#b23">Wen et al. (2016)</ref> used the same group sparsity to select unimportant channels and spatial features in a CNN, and let the network to automatically decide how many layers to use. However, they assume that all classes share the same set of features, which is restrictive when the number of classes is very large. On the contrary, our proposed SplitNet enforces feature sharing only within a group of related classes, and thus semantically reduce the number of parameters and computations for large-scale problems. Recently, <ref type="bibr" target="#b19">Shankar et al. (2016)</ref> also addressed the use of symmetrical split at mid-level convolutional layers in CNNs for architecture refinement. However, they did not learn the splits but predefine them, as opposed to SplitNet which learns semantic splits along with network weights.</p><p>Parallel and distributed deep learning. As deep networks and training data become increasingly larger, researchers are exploring parallelization and distribution techniques to speed up the training process. Most parallelization techniques exploit either 1) data parallelism, where the training data is distributed across multiple computational nodes, or 2) model parallelism, where the model parameters are distributed. <ref type="bibr" target="#b6">Dean et al. (2012)</ref> used both data and model parallelism to train a large-scale deep neural network on a computing cluster with thousands of machines. For CNNs, <ref type="bibr" target="#b15">Krizhevsky et al. (2012)</ref> used both data and model parallelism to train separate convolutional filters from disjoint datasets. <ref type="bibr" target="#b14">Krizhevsky (2014)</ref> later proposed to vertically split the network, exploiting the different time/memory characteristics of the convolutional and fully connected layers. <ref type="bibr" target="#b4">Chilimbi et al. (2014)</ref> proposed a server-client architecture where each client computes partial gradients of parameters, that are stored and communicated from the global model parameter server. <ref type="bibr" target="#b26">Zhang et al. (2015)</ref> proposed a GPU-based distributed deep learning, with greatly reduced the communication overheads. However, all these are systems-based approaches that work under the assumption that the model structure is given and fixed. Our approach, on the other hand, leverages semantic knowledge of class relatedness to learn a network structure that is well-fitted to a distributed machine learning setting.</p><p>Tree-structured deep networks. There have been some efforts to exploit hierarchical class structures for improving the performance of deep networks. To list a few recent work, <ref type="bibr" target="#b22">Warde-Farley et al. (2014)</ref> proposed to group classes based on their weight similarity, and augmented the original deep network with the softmax loss for fine-grained classification for classifying classes within each group. <ref type="bibr" target="#b24">Yan et al. (2015)</ref> proposed a convolutional network that combines predictions from separate sub-networks for coarseand fine-grained category predictions, which share common lower-layers. <ref type="bibr" target="#b8">Goo et al. (2016)</ref> exploited the hierarchical structure among the classes to learn common and discriminative features across classes, by adding in simple feature pooling layers. <ref type="bibr" target="#b16">Murdock et al. (2016)</ref> generalized dropout to stochastically assign nodes to clusters which results in obtaining a hierarchical structure. However, all of these methods focus on improving the model accuracy, at the expense of increased computational complexity. On the contrary, SplitNet is focused on improving memory/time efficiency and parallelization performance of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Given a base network, our goal is to obtain a tree-structured network that contains either a set or a hierarchy of subnetworks, where the leaf-level subnetworks are associated with a specific group of classes, as in <ref type="figure">Figure 1</ref>. Then what is the optimal way to split the classes? A key insight to our approach is that classes within each group should share features as much as possible, since grouping of disparate classes may result in learning redundant features over multiple groups, and may waste network capacity as a result. Thus, to maximize the utility of this splitting process, we need to cluster classes together into groups so that each group uses a subset of features that are completely disjoint from the ones used by other groups. One straightforward way to obtain such mutually exclusive groupings of classes is to leverage a semantic taxonomy, since semantically similar classes are likely to share features, whereas dissimilar classes are unlikely to do so. However, in practice, such semantic taxonomy may not be available, or may not agree with actual hierarchical grouping based on what features each class uses. Another simple approach is to perform (hierarchical) clustering on the weights learned in the original network, which is also based on actual feature uses. Yet, this grouping may be still suboptimal since the groups are highly likely to overlap, and is inefficient since it requires training the network twice.</p><p>In the following sections, we will describe how to learn such groups with disjoint set of classes and features, along with the network weights in a deep learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Statement</head><formula xml:id="formula_0">Given a dataset D = {x i , y i } N i=1</formula><p>, where x i ∈ R d is an input data instance and y i ∈ {1, . . . , K} is a class label for K classes, our goal is to learn a network whose weight at each layer l, W (l) is a block-diagonal matrix, where each</p><formula xml:id="formula_1">block W (l)</formula><p>g is associated with a class group g ∈ G, where G is the set of all groups. Such a block-diagonal W (l) ensures that each disjoint group of classes has exclusive features associated with it, such that no other groups use those features; this allows the network to be split across multiple class groups, for faster computation and parallelization.</p><p>In order to obtain such a block diagonal weight matrix W (l) , we propose a novel splitting algorithm that learns a feature-group and class-group assignment along with the network weights. We first illustrate our splitting method on the parameters for the softmax classifier in Section 3.2, and then describe how to extend this to other layers of DNNs in Section 3.3.</p><p>We assume that the number of groups G, is given. Let p gi be a binary variable indicating whether feature i is assigned to group g (1 ≤ g ≤ G), and q gj be a binary variable indicating whether class j is assigned to group g. We define</p><formula xml:id="formula_2">p g ∈ Z D</formula><p>2 as a feature group assignment vector for group g, where Z 2 = {0, 1} and D is the dimension of features. Similarly, q g ∈ Z K 2 denotes a class group assignment vector for group g. That is, p g and q g define a group g together, where p g represents features associated with the group and q g indicates a set of classes assigned to the group.</p><p>We assume that there is no overlap between groups, either in features or classes, i.e. g p g = 1 D and g q g = 1 K , where 1 D and 1 K are the vectors with all-one elements. While this assumption imposes hard regularizations on group assignments, it enables the weight matrix W ∈ R D×K to be sorted into a block-diagonal matrix, since each class is assigned to a group and each group depends on a disjoint subset of features. This greatly reduces the number of parameters, and at the same time, the multiplication W T x can be decomposed into smaller and faster block matrix multiplications.</p><p>The objective for training our SplitNet is then defined as:</p><formula xml:id="formula_3">min ω,P ,Q L(ω, X, y) + L l=1 λ||W (l) || 2 2 + L l=S Ω(W (l) , P (l) , Q (l) ),<label>(1)</label></formula><p>where L(W , X, y) is the cross entropy loss on the training data, ω = {W (1) , . . . , W (L) } is the set of network weights at all layers, W 2 2 is the weight decay regularizer with a hyperparameter λ, S is the layer where splitting starts, Ω(W , P , Q) is the regularizer for splitting the network, and P (l) and Q (l) are the set of feature-to-group and class- to-group assignment vectors respectively, for each layer l.</p><formula xml:id="formula_4">P 2 R G⇥D Q 2 R G⇥K W 2 R D⇥K K D p g q g W ||((I P g )W Q g ) i⇤ || 2 i D K Figure 2. Group</formula><p>In the next section, we propose a novel regularization Ω that automatically finds appropriate disjoint group assignments with no external semantic information. The objective of Eq.</p><p>(1) are jointly optimized using (stochastic) gradient descent, starting with a full weight matrix, and an unknown group assignment. As we jointly optimize the cross entropy loss and the group regularization, our method automatically obtains appropriate grouping and prune out inter-group connections. Once the grouping is learned, the weight matrix can be explicitly split into block diagonal matrices to reduce number of parameters, which in turn allows for much faster inference at test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning to Split Network Weights into Disjoint Groups</head><p>Our regularization assigns features and classes into disjoint groups; it consists of three objectives as follows:</p><formula xml:id="formula_5">Ω(W , P , Q) = γ 1 R W (W , P , Q) + γ 2 R D (P , Q) + γ 3 R E (P , Q)<label>(2)</label></formula><p>where γ 1 , γ 2 , γ 3 controls the strength of each regularization, which will be discussed in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">GROUP WEIGHT REGULARIZATION</head><p>To make the numerical optimization tractable, we first relax the binary variables p gi and q gj to have real values in the interval of [0, 1] with the constraints g p g = 1 D and g q g = 1 K . These sum-to-one constraints can be directly optimized using reduced gradient algorithm <ref type="bibr" target="#b17">(Rakotomamonjy et al., 2008)</ref>, which yields sparse solutions. Or, we can perform soft assignments, by reparamterizing p gi and q gj with unconstrained variables α gi and β gj in the softmax form:</p><formula xml:id="formula_6">p gi = e αgi / g e αgi , q gj = e βgj / g e βgj .<label>(3)</label></formula><p>We empirically observe that the softmax form results in more semantically meaningful grouping. However, the direct optimization of sum-to-one constraint often leads to faster convergence than the softmax formulation.</p><p>Let P g = diag(p g ) and Q g = diag(q g ) be the feature and class group assignment matrix for group g respectively. Then P g W Q g represents the weight parameters associated with group g, i.e. intra-group connections between features and classes. Since our goal is to prune out inter-group connections to obtain block-diagonal weight matrices, we minimize off block-diagonal entries as follows:</p><formula xml:id="formula_7">R W (W , P , Q) = g i ||((I − P g ) W Q g )) i * || 2 + g j ||(P g W (I − Q g )) * j || 2<label>(4)</label></formula><p>where (M ) i * and (M ) * j denote i-th row and j-th column of M . Eq.(4) imposes row/column-wise 2,1 -norm on the inter-group connections. <ref type="figure">Figure 2</ref> illustrates this regularization, where the portions of the weights to which the regularization is applied are colored differently.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>We observe that this regularization yields groups that are fairly similar to semantic groups. One caution is to avoid uniform initialization on group assignments, i.e. p i = 1/G, in which case the objective reduces to row/columnwise 2,1 -norm and some row/column weight vectors may die out before appropriate group assignments are obtained.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">DISJOINT GROUP ASSIGNMENT</head><p>For the group assignment vectors to be completely mutually exclusive, they should be orthogonal; i.e. they should satisfy the condition p i · p j = 0 and q i · q j = 0, ∀ i = j. We introduce an additional orthogonal regularization term:</p><formula xml:id="formula_8">R D (P , Q) = i&lt;j p i · p j + i&lt;j q i · q j .<label>(5)</label></formula><p>where the inequalities avoid the duplicative dot products.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">BALANCED GROUP ASSIGNMENT</head><p>The disjoint group assignment objective in Eq.(5) alone may drive one group to dominate over all other groups; that is, one group includes all features and classes, while other groups do not. Therefore, we also constrain the group assignments to be balanced, by regularizing the squared sum of elements in each group assignment vector.</p><formula xml:id="formula_9">R E (P , Q) = g ( i p gi ) 2 + ( j q gj ) 2 .<label>(6)</label></formula><p>1 When using group weight regularization followed by batch normalization, the weights tend to decrease their magnitudes while the scale parameters of BN layers increase. To prevent this effect, we use 2-normalized weights W /||W ||2, instead of W in RW , or simply deactivate the scale parameters of BN layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Splitting Deep Neural Networks</head><p>Input: Number of groups G, layers to split S ≤ L and hyperparamaters γ1, γ2, γ3 Initialize weights and group assignments while groupings have not converged do Optimize the objective using SGD with a learning rate η</p><formula xml:id="formula_10">L(ω, X, Y ) + λ L l=1 ||W (l) || 2 2 + γ1 L l=S RW (W (l) , P (l) , Q (l) ) +γ2 L l=S RD(P (l) , Q (l) ) + γ3 L l=S RE(P (l) , Q (l) )</formula><p>end while Split the network using the obtained group assignments and weight matrices while validation accuracy improves do</p><formula xml:id="formula_11">Optimize L(ω, X, Y ) + λ L l=1 ||W (l) || 2</formula><p>2 using SGD end while Due to the constraints g p g = 1 D and g q g = 1 K , the objective of Eq. <ref type="formula" target="#formula_9">(6)</ref> is minimized when sums of elements in each group assignment vector are even; i.e. each group has identical number of elements. Since the dimensions of feature and class group assignment vectors may differ, we scale the two terms with appropriate weights. See <ref type="figure" target="#fig_2">Figure 4</ref> to see the effect of balanced group regularization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Splitting Deep Neural Networks</head><p>Our weight-splitting method in section 3.2 can be applied to deep neural networks (DNN), which has two types of layers: 1) the input and hidden layers that produce a feature vector for a given input, and 2) the output fully-connected (FC) layer on which the softmax classifier produces class probabilities. The output FC layer can be split by directly applying our method in section 3.2 on the output FC weight matrix W (L) . Our splitting framework can be further extended into deep splits, involving either multiple consecutive layers or recursive hierarchical group assignments. Algorithm 1 describes the deep splitting process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.1.">DEEP SPLIT</head><p>The lower layers of a DNN learn low-level, generic representations, which are likely to be shared across all classes. The higher level representations, on the contrary, are more likely to be specific to the classes in a particular group. Therefore we do not split all layers but split layers down to S-th layer (S ≤ L), while maintaining lower layers (l &lt; S) to be shared across class groups.</p><p>Each layer consists of input and output nodes with the weights W (l) that connects between them, and P</p><formula xml:id="formula_12">(l) g , Q (l) g</formula><p>for input-to-group and output-to-group assignments. Since the output nodes of each layer correspond to the input nodes of the next layer, the grouping assignment are shared as q</p><formula xml:id="formula_13">(l) g = p (l+1) g</formula><p>. This enforces that no signal is passed across different groups of layers, so that forward and backward propagation in each group is independent from the processes in other groups. This allows the computations for each group to be parallelized, except for the softmax layer. The softmax layer includes a normalizing operation over all classes which requires aggregating logits over groups; however, during inference it suffices to identify the class with the maximum logit, which can be simply obtained by first identifying the class with maximum logit in each group, and then selecting the class with maximum logit among the identified group-specific maximums. This requires minimal communication overhead.</p><p>The objective function for deep split is the same with the weight-splitting method in section 3.2 (i.e. Eq. <ref type="formula" target="#formula_3">(1)</ref>- <ref type="formula" target="#formula_5">(2)</ref>), except for the previously explained alignment constraints.</p><p>When applied to CNNs, the proposed group splitting process can be performed in the same manner on the convolutional filters. Suppose that a weight of a convolutional layer is a 4-D tensor W c ∈ R M ×N ×D×K , where M, N denote height and width of receptive fields and D, K denote the number of input and output convolutional filters. We reduce the 4-D weight tensor W c into a 2-D matrix W c ∈ R D×K by taking the root sum squared of elements over height and width dimensions of W c , i.e. W c = {w dk } = { m,n w 2 mndk }. Then the weight regularization objective for the convolutional weight is obtained by Eq.(4), using W c instead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.2.">HIERARCHICAL GROUPING</head><p>There often exist natural semantic hierarchies of classes: for example, The group dog and the group cat are subgroups of mammal. We can easily extend our deep split method to obtain multi-level hierarchies of categories. <ref type="figure">Figure 1</ref> shows an example of such a hierachical split.</p><p>Assume that the grouping branches at the l-th layer and the output nodes of the l-th layer are grouped by G supergroup assignment vectors q , we can map subgroup assignments into corresponding supergroup assignments. This allows us to impose the constraint q</p><formula xml:id="formula_14">(l) g = p (l+1) g</formula><p>as in Deep Split.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Parallelization of SplitNet</head><p>Our learning algorithm produces a tree-structured network whose subnetworks have no inter-group connections. This results in an embarrassingly model-parallel network where we can simply assign the obtained subnetworks to each processor, or a machine. In our implementation, we consider two approaches for model parallelization: 1) Assigning both the lower-layers and group-specific upper layers to each node. At the test time the lower layers are not changed; thus this approach is acceptable, although it causes unnecessary redundant computations across the processors. 2) Assigning the lower layer to a separate processor. This eliminates redundancies in the lower layer but incurs communication overhead between the lower layer and the upper layers.</p><p>Training-time parallelization is currently done only at the finetuning step, after group assignments have been decided. We leave the parallelization from the initial network training stage as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets. We validate our method for image classification tasks on two benchmark datasets.</p><p>1) CIFAR-100. The CIFAR-100 dataset contains 32 × 32 pixel images from 100 generic object classes. For each class, there are 500 images for training and 100 images for test. We set aside 50 images for each class from the training dataset as a validation set for cross-validation. Note that the test errors are not directly comparable to <ref type="bibr" target="#b25">(Zagoruyko &amp; Komodakis, 2016)</ref>, as we use only 45,000 training images.</p><p>2) ImageNet-1K. The ImageNet 1K dataset <ref type="bibr" target="#b7">(Deng et al., 2009</ref>) that consists of 1.2 million images from 1, 000 generic object classes. For each class, there are 1K − 1.3K images for training and 50 images for validation, which we use for test, following the standard procedure.</p><p>Baselines. To compare different ways to obtain grouping, we test multiple variants of our SplitNet and baselines.</p><p>1) Base Network. Base networks with full network weights. For experiments on the CIFAR-100, we use Wide Residual Network (WRN) <ref type="bibr" target="#b25">(Zagoruyko &amp; Komodakis, 2016)</ref>, which is one of the state-of-the-art networks of the dataset. We use AlexNet <ref type="bibr" target="#b15">(Krizhevsky et al., 2012)</ref> and <ref type="bibr">ResNet-18 (He et al., 2016)</ref> variants as the base network for the ILSVRC2012.</p><p>2) SplitNet-Semantic. A variant of our SplitNet that obtains class grouping from a provided semantic taxonomy. Before training, we split the networks according to the taxonomy, evenly splitting layers and assigning subnetworks to each group, and train it from scratch. We use the same approach for SplitNet-Clustering and SplitNet-Random.</p><p>3) SplitNet-Clustering. A variant of our SplitNet, where classes are split (hierarchical) performing spectral clustering of the pre-trained base network weights.</p><p>4) SplitNet-Random. SplitNet using random class splits.</p><p>5) SplitNet. Our proposed SplitNet, that is trained using the proposed automatic splitting of weight matrices.</p><p>All SplitNet variants and other baselines are implemented using TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parameter Reduction and Accuracies</head><p>Experimental results below validate the two key benefits of our SplitNet: 1) Reducing the number of parameters without losing the prediction accuracy, and 2) obtaining a better structure for model-parallelization.</p><p>CIFAR-100. <ref type="table">Table 1</ref> summarizes split structures and test errors of different SplitNet variants and baselines. See the supplementary material for more details of used models. SplitNet variants using semantic taxonomy provided by the dataset (-S) and spectral clustering (-C) are better than random grouping (-R), showing that the appropriate grouping is critical for splitting DNNs. The SplitNet with learned splits outperforms all other variants, although it does not require any additional semantic information or pretrained network weights as with semantic or clustering split. Shallow Split shows even better performance than the baseline with significantly fewer parameters. We attribute it to the fact that SplitNet starts from a full network and learns and cuts unnecessary connections between different groups for inner layers, imposing regularization effect on the layers. In addition, splitting layers can be regarded as a form of variable selection: each group in the layer parsimoniously selects only a needed group of input nodes.</p><p>ImageNet-1K.  WRN-16-8 on CIFAR-100. With all split schemes, SplitNet outperforms SplitNet-Random. Further, with 1-2-2 split that splits the last FC layer and two residual blocks, the network parameters are significantly reduced by 37.86% while the test error is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Test-time Model Parallelization</head><p>As illustrated in <ref type="figure">Figure 1</ref>, splitting DNNs yields speedup not only by reducing parameters, but also by utilizing the split structure for model parallelization. Thus we further validate parallelization performance of SplitNet at evaluation time, with multiple GPUs. <ref type="table" target="#tab_4">Table 5</ref> summarizes the run-time performance of SplitNet with model parallelization. We test two approaches: 1) Redundant assignment of lower layers (Deep and Hier. Split), and 2) assignment of lower layers to a separate GPU (Deep Split 3-way). With redundant assignment, the speedup becomes larger with <ref type="figure">Figure 3</ref>. Learned Groups and Block-diagonal Weight Matrices. Visualization of the weight matrices along with corresponding group assignments learned in in AlexNet 2-4-8 split. Obtained block-diagonal weights are split for faster multiplication. Note the hierarchical grouping structure. for different values of γ3 on ImageNet-1K with G = 3</p><formula xml:id="formula_15">W (fc8) W (fc7) W (fc6) P (fc7) P (fc8) P (fc6) Q (fc7) Q (fc8) Q (fc6)</formula><p>deeper splits, up to 1.73×. Assigning lower layers to a third GPU eliminates redundant computation and achieves 2.22× speedup. We compare against model-parallel approaches with both horizontal split (Baseline-Horz.) that splits the network horizontally as our split method but without pruning connections between two subnetworks, and vertical split (Baseline-Vert.) that splits the network into blocks of sequential layers. With vertical split, computations on GPUs are done asynchronously using queues to maintain intermediate activations. Horizontal split obtains much worse performance to original network due to excessive communication overhead. Vertical splitting scheme makes more sense with full networks, but the communication overhead is still large and it yields only 1.20× and 1.08× speedup with two and three GPUs respectively. <ref type="figure">Figure 3</ref> visualizes the weight matrices and corresponding group assignments obtained in AlexNet 2-4-8 split on ImageNet-1K. Both group assignments of classes and features, P and Q, are sparse, and the weight matrices W of all layers are block diagonal. It indicates that grouping has converged with inter-group connections zeroed out. <ref type="figure" target="#fig_2">Figure 4</ref> shows the effect of balanced group regularization on the group size. With large regularization, groups becomes almost uniform in size, which is desirable for parameter reduction and model parallelization. Relaxing this regularization grants some flexibility on the individual group sizes. Setting γ 3 to be too small causes all classes and features to collapse into a single group, which may more closely resemble semantic taxonomies. In experiments, we enforced the models to be balanced, as our focus is more on efficiency and load balancing. <ref type="figure" target="#fig_3">Figure 5</ref> compares the learned group assignments in FC SplitNet (G = 4) with supercategories provided by the CIFAR-100. Each supercategory (column) includes five classes. For example, supercategory people includes baby, boy, girl, man and woman, which are grouped together by our algorithm into Group 2. Note that we have at least three classes from the same supercategory in each group. This shows that groupings learned by our method bear some resemblance to semantic categories even when no external semantic information is given.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Qualitative Analysis</head><p>The supplementary file presents the experimental results with varying G, the number of groups. Interestingly, with Shallow SplitNet on CIFAR-100, a higher G=4 achieves a better test accuracy with a parameter reduction of 48.66%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We proposed a novel solution to split deep network into a tree of subnetworks, to not only reduce the number of parameters and computations, but to also enable straightforward model-parallelization. Specifically, we proposed an algorithm to cluster the classes into groups that fit to exclusive sets of features, which results in obtaining blockdiagonal weight matrices. Our splitting algorithm is seamlessly integrated into the network training procedure as a regularization term, and thus allows the network weights and splitting to be trained at the same time. We validated our method on two classification tasks with two different CNN architectures, on which it greatly reduced the number of parameters over the base networks, while obtaining superior performance over networks with semantic or clustering-based groups. Moreover, our network obtained superior parallelization performance against the base networks at test time. As future work, we plan to explore ways to efficiently train SplitNet on multi-GPU or multiprocessor environments from the initial training stage.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>D . Suppose that in the next layer, for each supergroup g ∈ {1, ..., G} there are corresponding S subgroup assignment vectors p (l+1) gs with s,g p (l+1) gs = 1 D . As aforementioned, the input nodes to the l+1-th layer corresponds to the output nodes of l-th layer. By defining p</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Effect of Balanced Group Regularization RE(P , Q). The above figures show group-to-class assignment matrix Q (f c8)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Learned Groups. Visualization of grouping learned in FC SplitNet on CIFAR-100. Rows denote learned groups, while columns denote semantic groups provided by CIFAR-100 dataset, each of which includes 5 classes. The brightness of a cell shows the agreement between learned groups and semantic groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Assignment and Group Weight Regulariza- tion. (Left) An example of group assignment with G = 3. Col- ors indicate groups. Each row of matrix P , Q is group assign- ment vectors for group g: pg, qg. (Right) Visualization of matrix (I − Pg)W Qg. The group assignment vectors work as soft in- dicators for inter-group connections. As the groupings converge, 2,1-norm is concentrated on inter-group connections.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .Table 2 .</head><label>12</label><figDesc>Comparison of Test Errors According to Depths of Splitting (row) and Splitting Methods (column) on CIFAR-100. Postfix S, C and R denote SplitNet variants -Semantic, Clustering and Random, respectively.Comparison of Parameter/Computation Reduction and Test Errors on CIFAR-100.</figDesc><table>WRN-16-8 (BASELINE) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Comparison of Parameter/Computation Reduction and Test Errors of AlexNet variants on ILSVRC2012. The number of splits indicates the split in fc6, fc7 and fc8 layer, respectively. In 2×5 split, we split from conv4 to fc8 with G = 2.</figDesc><table>NETWORK 
SPLITS PARAMS(10 
6 ) % REDUCED FLOPS(10 
9 ) % REDUCED TEST ERROR(%) 
ALEXNET(BASELINE) 
0 
62.37 
0 
2.278 
0 
41.72 

SPLITNET 

1-1-3 
59.64 
4.38 
2.273 
0.21 
42.07 
1-2-5 
50.69 
18.72 
2.256 
1.00 
42.21 
2-4-8 
27.34 
56.17 
2.209 
3.05 
43.02 
2×5 
31.96 
48.76 
1.847 
18.95 
44.60 

SPLITNET-R 

1-1-3 
59.64 
4.38 
2.273 
0.21 
42.20 
1-2-5 
50.70 
18.70 
2.256 
0.99 
43.20 
2-4-8 
27.34 
56.17 
2.209 
3.05 
43.35 
2×5 
31.94 
48.79 
1.846 
18.93 
44.99 

Table 4. Comparison of Parameter/Computation Reduction and Test Errors of ResNet-18 variants(ResNet-18x2) on 
ILSVRC2012. The number of splits indicates the split in conv4-1&amp;2, conv5-1&amp;2 and the last fc layer, respectively. 

NETWORK 
SPLITS SPLIT DEPTH PARAMS(10 
6 ) % REDUCED FLOPS(10 
9 ) % REDUCED TEST ERROR(%) 
RESNET-18X2 
0 
0 
45.67 
0 
14.04 
0 
25.58 

SPLITNET 

1-1-3 
1 
44.99 
1.49 
14.04 
0.01 
24.90 
1-2-2 
6 
28.39 
37.84 
12.39 
11.72 
25.48 
2-2-2 
11 
24.21 
47.00 
10.75 
23.42 
26.45 

SPLITNET-R 

1-1-3 
1 
44.99 
1.49 
14.03 
0.01 
25.86 
1-2-2 
6 
28.38 
37.86 
12.39 
11.72 
26.41 
2-2-2 
11 
24.14 
47.14 
10.75 
23.46 
28.61 

Table 2 compares test errors, parameter reduction, and 
computation (FLOPs) reduction of SplitNets against the 
base network WRN-16-8: a 16 layer residual network with 
widening factor k = 8. Most of parameters in WRNs exist 
in convolutional layers, especially in higher layers due to 
the large number of filters. Thus, FC Split yields minimal 
parameter reduction. On the other hand, Shallow Split of 
the last 5 convolutional layers significantly reduces the net-
work parameters by 32.44% while even slightly improving 
the accuracy. Deep and Hierarchical Split further reduce 
parameters and FLOPs at the cost of minor accuracy drop. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3</head><label>3</label><figDesc></figDesc><table>and 4 summarize the results of Im-
ageNet experiments with two base networks: AlexNet and 
ResNet-18x2, a variant of ResNet-18 where the numbers 
of filters are doubled, respectively. Refer to supplementary 
materials for model description. Using AlexNet as a base 
model, SplitNet greatly reduces the number of parameters 
concentrated in fc layers. However, most of the FLOPs 
come from lower conv layers, yielding only minor FLOPs 
reduction. We observe that SplitNet on AlexNet shows mi-
nor test accuracy drop with significant parameter reduction. 

SplitNet based on ResNet-18x2 shows similar results as 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Model Parallelization Benchmark of SplitNet on Mul- tiple GPUs. We measure evaluation time performance of our SplitNet over 50,000 CIFAR-100 images with batch size 100 on TITAN X Pascal. Baseline implements layer-wise parallelization where sequential blocks of layers are distributed on GPUs.</figDesc><table>NETWORK 
GPUS 
TIME(S) SPEEDUP(×) 
BASELINE 
1 24.27 ± 0.35 
1.00 
BASELINE-HORZ. 
2 46.70 ± 0.48 
0.52 
BASELINE-VERT. 
2 20.15 ± 0.67 
1.20 
BASELINE-VERT. 
3 22.45 ± 0.77 
1.08 
SHALLOW SPLIT 
2 17.78 ± 0.23 
1.37 
DEEP SPLIT 
2 14.03 ± 0.13 
1.73 
HIER. SPLIT 
2 14.22 ± 0.05 
1.71 
DEEP SPLIT 3-WAY 
3 10.92 ± 0.44 
2.22 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported by Samsung Research Funding Center of Samsung Electronics under project number SRFC-IT150203.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Eugene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhifeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Craig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devin</forename><surname>Jeffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Tensorflow: Large-scale Machine Learning on Heterogeneous Distributed Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning the Number of Neurons in Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathieu</forename><surname>Salzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Do Deep Nets Really Need to Be Deep</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rich</forename><surname>Caruana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rejean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jauvin</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Christian. A Neural Probabilistic Language Model. JMLR</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Project Adam: Building an Efficient and Scalable Deep Learning Training System</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trishul</forename><surname>Chilimbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Suzue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yutaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johnson</forename><surname>Apacible</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karthik</forename><surname>Kalyanaraman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">OSDI</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Memory Bounded Deep Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.1442</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Large Scale Distributed Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Matthieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><forename type="middle">Z</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marcaurelio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fei-F˙Imagenet: A Large-Scale Hierarchical Image Database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ei</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Taxonomy-Regularized Semantic Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wonjoon</forename><surname>Goo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Juyong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gunhee</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sung</forename><surname>Hwang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ju</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning Both Weights and Connections for Efficient Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jeff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep Neural Networks for Acoustic Modeling in Speech Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Dahl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abdel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Navdeep</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Kingsbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tara</forename><surname>Sainath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="82" to="97" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distilling the Knowledge in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A Continuous Semantic Space Describes the Representation of Thousands of Object and Action Categories across the Human Brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">H</forename><surname>Huth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nishimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shinji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><forename type="middle">T</forename><surname>Vu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gallant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neuron</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1210" to="1224" />
			<date type="published" when="2012-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1404.5997</idno>
		<title level="m">One Weird Trick for Parallelizing Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Blockout: Dynamic Model Selection for Hierarchical Deep Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Calvin</forename><surname>Murdock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Howard</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Duerig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alain</forename><surname>Rakotomamonjy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stéphane</forename><surname>Canu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grandvalet</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Yves. SimpleMKL. JMLR</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2491" to="2521" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Pruning Algorithms -A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="740" to="747" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Refining Architectures of Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sukrit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Robertson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duncan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ioannou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Criminisi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sequence to Sequence Learning with Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Going Deeper with Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anguelov</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6563</idno>
		<title level="m">Self-informed Neural Network Structure Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Learning Structured Sparsity in Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chunpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yandan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiran</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hai</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhicheng</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jagadeesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vignesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Decoste</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dennis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Di</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yizhou</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hd-Cnn</surname></persName>
		</author>
		<title level="m">Hierarchical Deep Convolutional Neural Network for Image Classification. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Wide Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhiting</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jinliang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pengtao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gunhee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qirong</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.06216</idno>
		<title level="m">Poseidon: A System Architecture for Efficient GPU-based Deep Learning on Multiple Machines</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
