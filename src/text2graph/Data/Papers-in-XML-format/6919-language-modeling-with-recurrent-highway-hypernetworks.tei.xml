<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Character-Level Language Modeling with Recurrent Highway Hypernetworks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph</forename><surname>Suarez</surname></persName>
							<email>joseph15@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Character-Level Language Modeling with Recurrent Highway Hypernetworks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present extensive experimental and theoretical support for the efficacy of recurrent highway networks (RHNs) and recurrent hypernetworks complimentary to the original works. Where the original RHN work primarily provides theoretical treatment of the subject, we demonstrate experimentally that RHNs benefit from far better gradient flow than LSTMs in addition to their improved task accuracy. The original hypernetworks work presents detailed experimental results but leaves several theoretical issues unresolved-we consider these in depth and frame several feasible solutions that we believe will yield further gains in the future. We demonstrate that these approaches are complementary: by combining RHNs and hypernetworks, we make a significant improvement over current state-of-the-art character-level language modeling performance on Penn Treebank while relying on much simpler regularization. Finally, we argue for RHNs as a drop-in replacement for LSTMs (analogous to LSTMs for vanilla RNNs) and for hypernetworks as a de-facto augmentation (analogous to attention) for recurrent architectures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction and related works</head><p>Recurrent architectures have seen much improvement since their inception in the 1990s, but they still suffer significantly from the problem of vanishing gradients <ref type="bibr" target="#b0">[1]</ref>. Though many consider LSTMs <ref type="bibr" target="#b1">[2]</ref> the de-facto solution to vanishing gradients, in practice, the problem is far from solved (see Discussion). Several LSTM variants have been developed, most notably GRUs <ref type="bibr" target="#b2">[3]</ref>, which are simpler than LSTM cells but benefit from only marginally better gradient flow. <ref type="bibr">Greff et al. and Britz et al.</ref> conducted exhaustive (for all practical purposes) architecture searches over simple LSTM variants and demonstrated that none achieved significant improvement <ref type="bibr" target="#b3">[4]</ref> [5]-in particular, the latter work discovered that LSTMs consistently outperform comparable GRUs on machine translation, and no proposed cell architecture to date has been proven significantly better than the LSTM. This result necessitated novel approaches to the problem.</p><p>One approach is to upscale by simply stacking recurrent cells and increasing the number of hidden units. While there is certainly some optimal trade off between depth and cell size, with enough data, simply upscaling both has yielded remarkable results in neural machine translation (NMT) <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b0">1</ref> However, massive upscaling is impractical in all but the least hardware constrained settings and fails to remedy fundamental architecture issues, such as poor gradient flow inherent in recurrent cells <ref type="bibr" target="#b7">[8]</ref>. We later demonstrate that gradient issues persist in LSTMs (see Results) and that the grid-like architecture of stacked LSTMs is suboptimal.</p><p>The problem of gradient flow can be somewhat mitigated by the adaptation of Batch Normalization <ref type="bibr" target="#b8">[9]</ref> to the recurrent case <ref type="bibr" target="#b9">[10]</ref>  <ref type="bibr" target="#b10">[11]</ref>. While effective, it does not solve the problem entirely and also imposes significant overhead in memory and thus in performance, given the efficiency of parallelization over minibatches. This is often offset by a reduction in total epochs over the data required, but recurrent architectures with better gradient flow could ideally provide comparable or better convergence without reliance upon explicit normalization.</p><p>Zilly et al. recently proposed recurrent highway networks (RHNs) and offered copious theoretical support for the architecture's improved gradient flow <ref type="bibr" target="#b11">[12]</ref>. However, while the authors provided mathematical rigor, we believe that experimental confirmation of the authors' claims could further demonstrate the model's simplicity and widespread applicability. Furthermore, we find that the discussion of gradient flow is more nuanced than presented in the original work (see Discussion).</p><p>Ha et al. recently questioned the weight-sharing paradigm common among recurrent architectures, proposing hypernetworks as a mechanism for allowing weight drift between timesteps <ref type="bibr" target="#b12">[13]</ref>. This consideration is highly desirable, given the successes of recent convolutional architectures on language modeling tasks <ref type="bibr" target="#b13">[14]</ref> [15], which were previously dominated by recurrent architectures.</p><p>Both RHNs and hypernetworks achieved state-of-the-art (SOTA) on multiple natural language processing (NLP) tasks at the time. As these approaches address unrelated architectural issues, it should not be surprising that combining them yields SOTA on Penn Treebank <ref type="bibr" target="#b15">[16]</ref> (PTB), improving significantly over either model evaluated individually. We consider both RHNs and hypernetworks to be largely overlooked in recent literature on account of apparent rather than extant complexity. Furthermore, the original RHN work lacks sufficient experimental demonstration of improved gradient flow; the original hypernetworks work lacks theoretical generalization of their weight-drift scheme. We present experimental results for RHNs complementary to the original work's theoretical results and theoretical results for hypernetworks complementary to the original work's experimental results.</p><p>Founded on these results, our most important contribution is a strong argument for the utility of RHNs and hypernetworks, both individually and jointly, in constructing improved recurrent architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Recurrent highway networks</head><p>We make a few notational simplifications to the original RHN equations that will later facilitate extensibility. We find it clearest and most succinct to be programmatic in our notation <ref type="bibr" target="#b1">2</ref> . First, consider the GRU:</p><formula xml:id="formula_0">[h, t] =x i U + s i−1 W r = tanh(x iÛ + (s i−1 • h)Ŵ ) h, t =σ(h), σ(t) s i =(1 − t) • r + t • s i−1<label>(1)</label></formula><p>where x ∈ R d , h, t, r, s t ∈ R n , and U,Û ∈ R d×2n , W,Ŵ ∈ R n×2n are weight matrices where d, n are the input and hidden dimensions. σ is the sigmoid nonlinearity, and • is the Hadamard (elementwise) product. A one layer RHN cell is a simplified GRU variant:</p><formula xml:id="formula_1">[h, t] =x i U + s i−1 W s i =(1 − t) • s i−1 + t • h h, t = tanh(h), σ(t)<label>(2)</label></formula><p>where the definitions from above hold. The RHN is extended to arbitrary depth by simply stacking this cell with new hidden weight matrices, with the caveat that x i U is omitted except at the input layer:</p><formula xml:id="formula_2">RHN Cell(x i , s i−1 , l) : [h, t] =1 [l = 0] x i U + s i−1 W c, t =1 − t, dropout(t) h, t = tanh(h), σ(t) s i =c • s i−1 + t • h<label>(3)</label></formula><p>where l is the layer index, which is used as an indicator. We can introduce recurrent dropout <ref type="bibr" target="#b16">[17]</ref> on t across all layers with a single hyperparameter. We later demonstrate strong results without the need for more complex regularization or layer normalization. Finally, unlike stacked LSTMs, RHNs are structurally linear. That is, a depth L RHN applied to a sequence of length M can be unrolled to a simple depth M L network. We restate this fact from the original work only because it is important to our analysis, which we defer to Results and Discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Hypernetworks</head><p>We slightly alter the original notation of recurrent hypernetworks for ease of combination with RHNs. We define a hypervector z as a linear upscaling projection applied to the outputs of a small recurrent network:</p><formula xml:id="formula_3">z(a) = W p a<label>(4)</label></formula><p>where a ∈ R h is the activation vector output by an arbitrary recurrent architecture, W p ∈ R n×h is an upscaling projection from dimension h to n, and h n. The hypervector is then used to scale the weights of the main recurrent network by:</p><formula xml:id="formula_4">W (z(a)) = z(a) • W<label>(5)</label></formula><p>where we overload • as the element-wise product across columns. That is, each element of z scales one column (or row, depending on notation) of W . As this constitutes a direct modification of the weights, hypernetworks have the interpretation of relaxing the weight sharing constraint implicit in RNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Recurrent highway hypernetworks</head><p>We adapt hypernetworks to RHNs by directly modifying the RHN cell using <ref type="formula" target="#formula_4">(5)</ref>:</p><formula xml:id="formula_5">RHN CellHyper(x i , s i−1 , l, z) : [h, t] =1 [l = 0] x i U (z) + s i−1 W (z) c, t =1 − t, dropout(t) h, t = tanh(h), σ(t) s i =c • s i−1 + t • h<label>(6)</label></formula><p>If RHN Cell and RHN CellHyper had the same state sizes, we could simply stack them. However, as the hypernetwork is much smaller than the main network by design, we instead must upscale between the networks. Our final architecture at each timestep for layer l can thus be written:</p><formula xml:id="formula_6">s h =RHN Cell(s h , l) z =[M pl s h , M pl s h ] s n =RHN CellHyper(s n , l, z)<label>(7)</label></formula><p>where M pl ∈ R h×n is the upscale projection matrix for layer l and z is the concatenation of M pl s h with itself. Notice the simplicity of this extension-it is at least as straightforward to extend RHNs as GRUs and LSTMs. Again, we use only simple recurrent dropout for regularization.</p><p>A few notes, for clarity and ease of reproduction: as the internal weight matrices of the main network have different dimensionality (U l ∈ R d×2n , W l ∈ R n×2n ), we require the concatenation operation to form z in (7). We find this works much better than simply using larger projection matrices. Also, s h , s n in <ref type="formula" target="#formula_6">(7)</ref> are the hypernetwork and main network states, respectively. This may seem backwards from the notation above, but note that the hypernetwork is a standard, unmodified RHN Cell; its outputs are then used in the main network, which is the modified RHN CellHyper.</p><p>3 Results (experimental)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Penn Treebank</head><p>Penn Treebank (PTB) contains approximately 5.10M/0.40M/0.45M characters in the train/val/test sets respectively and has a small vocabulary of 50 characters. There has recently been some controversy surrounding results on PTB: Jozefowicz et al. went as far to say that performance on such small datasets is dominated by regularization <ref type="bibr" target="#b17">[18]</ref>. Radford et al. chose to evaluate language modeling performance only upon the (38GB) Amazon Product Review dataset for this reason <ref type="bibr" target="#b18">[19]</ref>.</p><p>Performance on large, realistic datasets is inarguably a better metric of architecture quality than performance on smaller datasets such as PTB. However, such metrics make comparison among models nearly impossible: performance on large datasets is non-standard because evaluation at this scale is infeasible in many research settings simply because of limited hardware access. While most models can be trained on 1-4 GPUs within a few weeks, this statement is misleading, as significantly more hardware is required for efficient development and hyperparameter search. We therefore emphasize the importance of small datasets for standardized comparison among models. Hutter is a medium sized task (approximately 20 times larger than PTB) that should be feasible in most settings (e.g. the original RHN and Hypernetwork works). We are only reasonably able to First, we address the critiques of Jozefoqicz et al. by avoiding complex regularization. We use only simple recurrent dropout with a uniform probability across layers. Second, we minimally tune hyperparameters as discussed below. Finally, we are careful with the validation data and run the test set only once on our best model. We believe these precautions prevent overfitting the domain and corroborate the integrity of our result. Furthermore, SOTA performance with suboptimal hyperparameters demonstrates the robustness of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Architecture and training details</head><p>In addition to our HyperRHN, we consider our implementations of a 2-Layer LSTM and a plain RHN below. All models, including hypernetworks and their strong baselines, are compared in <ref type="table" target="#tab_0">Table 1</ref>.</p><p>Other known published results are included in the original hypernetworks work, but have test bpc ≥ 1.27. We train all models using Adam <ref type="bibr" target="#b19">[20]</ref> with the default learning rate 0.001 and sequence length 100, batch size 256 (the largest that fits in memory for our main model) on a single GTX 1080 Ti until overfitting becomes obvious. We evaluate test performance only once and only on our main model, using the validation set for early stopping.</p><p>Our data batcher loads the dataset into main memory as a single contiguous block and reshapes it to column size 100. We do not zero pad for efficiency and no distinction is made between sentences for simplicity. Data is embedded into a 27 dimensional vector. We do not cross validate any hyperparameters except for dropout.</p><p>We first consider our implementation of a 2-Layer LSTM with hidden dimension 1125, which yields approximately as many learnable parameters as our main model. We train for 350 epochs with recurrent dropout probability 0.9. As expected, our model performs slightly better than the slightly smaller baseline in the original hypernetworks work. We use this model in gradient flow comparisons (see Discussion)</p><p>As the original RHN work presents only word-level results for PTB, we trained a RHN baseline by simply disabling the Hypernetwork augmentation. Convergence was achieved in 618 epochs.</p><p>Our model consists of a recurrent highway hypernetwork with 7 layers per cell. The main network has 1000 neurons per layer and the hypernetwork has 128 neurons per layer, for a total of approximately 15.2M parameters. Both subnetworks use a recurrent dropout keep probability of 0.65 and no other regularizer/normalizer. We attribute our model's ability to perform without layer normalization to the improved gradient flow of RHNs (see Discussion).</p><p>The model converges in 660 epochs, obtaining test perplexity 2.29 (where cross entropy corresponds to log e of perplexity) and 1.19 bits per character (BPC, log 2 of perplexity), 74.6 percent accuracy. By epoch count, our model is comparable to a plain RHN but performs better. Training takes 2-3 days (fairly long for PTB) compared to 1-2 days for a plain RHN and a few hours for an LSTM. However, this comparison is unfair: all models require a similar number of floating point operations and differ primarily in backend implementation optimization. We consider possible modifications to our model that take advantage of existing optimization in Results (theoretical), below.</p><p>Finally, we note that reporting of accuracy is nonstandard. Accuracy is a standard metric in vision; we encourage its adoption in language modeling, as BPC is effectively a change of base applied to standard cross entropy and is exponential in scale. This downplays the significance of gains where the error ceiling is likely small. Accuracy is more immediately comparable to maximum task performance, which we estimate to be well below 80 percent given the recent trend of diminishing returns coupled with genuine ambiguity in the task. Human performance is roughly 55 percent, as measured by our own performance on the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Results (theoretical)</head><p>Our final model is a direct adaptation of the original hypervector scaling factor to RHNs. However, we did attempt a generalization of hypernetworks and encountered extreme memory considerations that have important implications for future work. Notice that the original hypernetwork scaling factor is equivalent to element-wise multiplication by a rank-1 matrix (e.g. the outer product of z with a ones vector, which does not include all rank-1 matrices). Ideally, we should be able to scale by any matrix at all. As mentioned by the authors, naively generating different scaling vectors for each column of the weight matrix is prohibitively expensive in both memory and computation time. We propose a low rank-d update inspired by the thin singular value decomposition as follows:</p><formula xml:id="formula_7">W = W • d i=1 u i v i<label>(8)</label></formula><p>Compared to the original scaling update, our variation has memory and performance cost linear in the rank of the update. As with the SVD, we would expect most of the information relevant to the weight drift scale to be contained in a relatively low-rank update. However, we were unable to verify this hypothesis due to a current framework limitation. All deep learning platforms currently assemble computation graphs, and this low rank approximation is added as a node in the graph. This requires memory equal to the dimensionality of the scaling matrix per training example!</p><p>The original hypernetworks update is only feasible because of a small mathematical trick: row-wise scaling of the weight matrix is equal to elementwise multiplication after the matrix-vector multiply. Note that this is a practical rather than theoretical limitation. As variations in the weights of the hypernetwork arise only as a function of variations in u i , v i , W , it is possible to define a custom gradient operation that does not need to store the low rank scaling matrices at each time step for backpropagation.</p><p>Lastly, we note that hypernetworks are a new and largely unexplored area of research. Even without the above addition, hypernetworks have yielded large improvements on a diverse array of tasks while introducing a minimal number of additional parameters. The only reason we cannot currently recommend hypernetworks as a drop-in network augmentation for most tasks (compare to e.g. attention) is another framework limitation. Despite requiring far fewer floating point operations than the larger main network, adding a hypernetwork still incurs nearly a factor of two in performance. This is due to the extreme efficiency of parallelization over large matrix multiplies; the overhead is largely time spent copying data. We propose rolling the hypernetwork into the main network. This could be accomplished by simply increasing the hidden dimension by the desired hypernetwork dimension h. The first h elements of the activation can then be treated as the hypervector. Note that this may require experimentation with matrix blocking and/or weight masking schemes in order to avoid linear interactions between the hypernetwork and main network during matrix multiplication.</p><p>The issues and solutions above are left as thought experiments; we prioritize our limited computational resources towards experimental efforts on recurrent highway networks. The theoretical results above are included to simultaneously raise and assuage concerns surrounding generalization and efficiency of hypernetworks. We see additional development of hypernetworks as crucial to the continued success of our recurrent model in the same manner that attention is a necessary, de-facto network augmentation in machine translation (and we further expect the gains to stack). Our model's strong language modeling result using a single graphics card was facilitated by the small size of PTB, which allowed us to afford the 2X computational cost of recurrent hypernetworks. We present methods <ref type="figure">Figure 1</ref>: Visualization of hyper recurrent highway network training convergence for optimizing the representational power and computational cost of hypernetworks; additional engineering will still be required in order to fully enable efficient training on large datasets.</p><p>5 Discussion (experimental)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Training time</head><p>We visualize training progress in <ref type="figure">Fig. 1</ref>. Notice that validation perplexity seems to remain below training perplexity for nearly 500 epochs. While the validation and test sets in PTB appear slightly easier than the training set, the cause of this artifact is that the validation loss is masked by a minimum 50-character context whereas the training loss is not (we further increase minimum context to 95 after training and observe a small performance gain), therefore the training loss suffers from the first few impossible predictions at the start of each example. The validation data is properly overlapped such that performance is being evaluated over the entire set.</p><p>It may also seem surprising that the model takes over 600 epochs to converge, and that training progress appears incredibly slow towards the end. We make three observations: first, we did not experiment with different optimizers, annealing the learning rate, or even the fixed learning rate itself. Second, as the maximum task accuracy is unknown, it is likely that gains small on an absolute scale are large on a relative scale. We base this conjecture on the diminishing gains of recent work on an absolute scale: we find that the difference between 1.31 (1 layer LSTM) and 1.19 bpc (our model) is approximately 71.1-74.6 percent accuracy. For reference, our improvement over the original hypernetworks work is approximately 1.0 percent (this figure is obtained from interpolation on the BPC scale). Third and finally, regardless of whether our second observation is true, our architecture exhibits similar convergence to a RHN and begins outperforming the 2-layer LSTM baseline before the latter converges.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Overview of visualizations</head><p>Our motivation in the visualizations that follow is to compare desirable and undesirable properties of our RHN-based model and standard recurrent models, namely stacked LSTMs. There are two natural gradient visualizations: within-cell gradients, which are averaged over time but not over all of the weight layers within the recurrent cell, and outside-cell gradients, which are averaged over internal weight layers but not over time. Time-averaged gradients are less useful to our discussion than the norms of raw weight layers; we therefore present these along with outside-cell gradient visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Cell visualizations</head><p>We visualize the 2-norms of the learned weight layers of our RHN-based model in <ref type="figure" target="#fig_0">Fig. 2</ref> and of an LSTM baseline (2 layers, 1150 hidden units, recurrent dropout keep p=0.90, 15.6M parameters) in <ref type="figure" target="#fig_1">Fig. 3</ref>.</p><p>Notice that in the middle six layers (the first/last layers have different dimensionality and are incomparable) of the RHN block <ref type="figure" target="#fig_0">(Fig. 2)</ref>, weight magnitude decreases with increasing layer depth. We view this as evidence for the iterative-refinement view of deep learning, as smaller updates are  applied in deeper layers. This is first evidence of this paradigm that we are aware of in the recurrent case, as similar statistics in stacked LSTMs are less conclusive because of horizontal grid connections. This also explains why performance gains diminish as RHN depth increases, as was noted in the original work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Gradient visualizations over time</head><p>We consider the mean L2-norms of the gradients of the activations with respect to the loss at the final timestep. But first, an important digression: when should we visualize gradient flow: at initialization, during training, or after convergence? To our knowledge, this matter has not yet received direct treatment. <ref type="figure" target="#fig_2">Fig. 4</ref> is computed at initialization and seems to suggest that RHNs are far inferior to LSTMs in the multilayer case, as the network cannot possibly learn in the presence of extreme vanishing gradients. This line of reasoning lacks the required nuance, which we discuss below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion (theoretical)</head><p>We address the seemingly inconsistent experimental results surrounding gradient flow in RHN.</p><p>First, we note that the LSTM/RHN comparison is unfair: multilayer LSTM/GRU cells are laid out in a grid. The length of the gradient path is equal to the sum of the sequence length and the number of layers (minus one); in an RHN, it is equal to the product. In the fair one layer case, we found that the RHN actually possesses far greater initial gradient flow. Second, these intuitions regarding vanishing gradients at initialization are incorrect. As shown in <ref type="figure" target="#fig_3">Fig. 5</ref>, gradient flow improves dramatically after training for just one epoch. By convergence, as shown in <ref type="figure" target="#fig_4">Fig. 6</ref>, results shift in the favor of RHNs, confirming experimentally the theoretical gradient flow benefits of RHNs over LSTMs.</p><p>Third, we address a potential objection. One might argue that while the gradient curves of our RHN based model and the LSTM baseline are similar in shape, the magnitude difference is misleading. For example, if LSTMs naturally have a million times smaller weights, then the factor of a hundred magnitude difference in <ref type="figure" target="#fig_4">Fig. 6</ref> would actually demonstrate superiority of the LSTM. This is the reason for our consideration of weight norms in <ref type="figure" target="#fig_0">Fig. 2-3</ref>, which show that LSTMs have only 100 times smaller weights. Thus the gradient curves in <ref type="figure" target="#fig_4">Fig. 6</ref> are effectively comparable in magnitude. However, RHNs maintain gradient flow equal to that of stacked LSTMs while having far greater   gradient path length, thus the initial comparison is unfair. We believe that this is the basis for the RHN's performance increase over the LSTM: RHNs allow much greater effective network depth without incurring additional gradient vanishing.</p><p>Fourth, we experimented with adding the corresponding horizontal grid connections to our RHN, obtaining significantly better gradient flow. With the same parameter budget as our HyperRHN model, this variant obtains 1.40 bpc-far inferior to our HyperRHN, though it could likely be optimized somewhat. It appears that long gradient paths are precisely the advantage in RHNs. We therefore suggest that gradient flow specifically along the deepest gradient path is an important consideration in architecture design: it provides an upper limit on effective network depth. It appears that greater effective depth is precisely the advantage in modeling potential of the RHN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>We present a cohesive set of contributions to recurrent architectures. First, we provide strong experimental evidence for RHNs as a simple drop-in replacement for stacked LSTMs and a detailed discussion of several engineering optimizations that could further performance. Second, we visualize and discuss the problem of vanishing gradients in recurrent architectures, revealing that gradient flow significantly shifts during training, which can lead to misleading comparisons among models. This demonstrates that gradient flow should be evaluated at or near convergence; using this metric, we confirm that RHNs benefit from far greater effective depth than stacked LSTMs while maintaining equal gradient flow. Third, we suggest multiple expansions upon hypernetworks for future work that have the potential to significantly improve efficiency and generalize the weight-drift paradigm. This could lead to further improvement upon our architecture and, we hope, facilitate general adoption of hypernetworks as a network augmentation. Finally, we demonstrate effectiveness by presenting and open sourcing (code 3 ) a combined architecture that obtains SOTA on PTB with minimal regularization and tuning which normally compromise results on small datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: L2 norms of learned weights in our recurrent highway hypernetwork model. Increasing depth is shown from left to right in each block of layers. As dimensionality differs between blocks, the middle layers of each block are incomparable to the first/last layers, hence the disparity in norm.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: L2 norms of learned weights in our 2-layer LSTM baseline, with layer 1 left of layer 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Layer-averaged gradient comparison between our model and an LSTM baseline. Gradients are computed at initialization at the input layer of each timestep with respect to the final timestep's loss. Weights are initialized orthogonally.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Identical to Fig. 4, but gradients are computed from models trained for one epoch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Identical to Fig. 4, but gradients are computed after convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Comparison of bits per character (BPC) test errors on PTB. We achieve SOTA without layer normalization, improving over vanilla hypernetworks, which require layer normalizationevaluate on PTB due to a strict hardware limitation of two personally owned GPUs. We therefore take additional precautions to ensure fair comparison:</figDesc><table>Model 
Test Val 
Params (M) 

LSTM 
1.31 1.35 4.3 
2-Layer LSTM 
1.28 1.31 12.2 
2-Layer LSTM (1125 hidden, ours) 
-
1.29 15.6 
HyperLSTM 
1.26 1.30 4.9 
Layer Norm LSTM 
1.27 1.30 4.3 
Layer Norm HyperLSTM 
1.25 1.28 4.9 
Layer Norm HyperLSTM (large embed) 1.23 1.26 5.1 
2-Layer Norm HyperLSTM, 1000 units 1.22 1.24 14.4 
Recurrent Highway Network (ours) 
-
1.24 14.0 
HyperRHN (ours) 
1.19 1.21 15.5 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">For fair comparison, Google's NMT system does far more than upscaling and includes an explicit attentional mechanism [7]. We do not experiment with attention and/or residual schemes, but we expect the gains made by such techniques to stack with our work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that for purpose of clean alignment, equations are presented top to bottom, then left to right.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">github.com/jsuarez5341/Recurrent-Highway-Hypernetworks-NIPS</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Special thanks to Ziang Xie, Jeremy Irvin, Dillon Laird, and Hao Sheng for helpful commentary and suggestion during the revision process.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rupesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Massive exploration of neural machine translation architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denny</forename><surname>Britz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><surname>Goldie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Luong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03906</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Gradient flow in recurrent nets: the difficulty of learning long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Frasconi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<title level="m">Recurrent batch normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Georg Zilly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rupesh Kumar</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutník</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.03474</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Recurrent highway networks. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Language modeling with gated convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Yann N Dauphin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Auli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grangier</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08083</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lasse</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.10099</idno>
		<title level="m">Neural machine translation in linear time</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05118</idno>
		<title level="m">Recurrent dropout without memory loss</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Exploring the limits of language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.02410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to generate reviews and discovering sentiment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01444</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
