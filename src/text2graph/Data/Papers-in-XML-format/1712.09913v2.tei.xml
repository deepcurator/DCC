<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">VISUALIZING THE LOSS LANDSCAPE OF NEURAL NETS</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Li</surname></persName>
							<email>haoli@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zheng</forename><surname>Xu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gavin</forename><surname>Taylor</surname></persName>
							<email>taylor@usna.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">United States Naval Academy</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Studer</surname></persName>
							<email>studer@cornell.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Goldstein</surname></persName>
							<email>tomg@cs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">VISUALIZING THE LOSS LANDSCAPE OF NEURAL NETS</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>Neural network training relies on our ability to find "good" minimizers of highly non-convex loss functions. It is well known that certain network architecture designs (e.g., skip connections) produce loss functions that train easier, and wellchosen training parameters (batch size, learning rate, optimizer) produce minimizers that generalize better. However, the reasons for these differences, and their effect on the underlying loss landscape, is not well understood. In this paper, we explore the structure of neural loss functions, and the effect of loss landscapes on generalization, using a range of visualization methods. First, we introduce a simple "filter normalization" method that helps us visualize loss function curvature, and make meaningful side-by-side comparisons between loss functions. Then, using a variety of visualizations, we explore how network architecture affects the loss landscape, and how training parameters affect the shape of minimizers.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Training neural networks requires minimizing a high-dimensional non-convex loss function -a task that is hard in theory, but sometimes easy in practice. Despite the NP-hardness of training general neural loss functions <ref type="bibr">(Blum &amp; Rivest, 1989)</ref>, simple gradient methods often find global minimizers (parameter configurations with zero or near-zero training loss), even when data and labels are randomized before training <ref type="bibr" target="#b22">(Zhang et al., 2017)</ref>. However, this good behavior is not universal; the trainability of neural nets is highly dependent on network architecture design choices, the choice of optimizer, variable initialization, and a variety of other considerations. Unfortunately, the effect of each of these choices on the structure of the underlying loss surface is unclear. Because of the prohibitive cost of loss function evaluations (which requires looping over all the data points in the training set), studies in this field have remained predominantly theoretical.</p><p>We use high-resolution visualizations to provide an empirical characterization of neural loss functions, and explore how different network architecture choices affect the loss landscape. Furthermore, we explore how the non-convex structure of neural loss functions relates to their trainability, and how the geometry of neural minimizers (i.e., their sharpness/flatness, and their surrounding landscape), affects their generalization properties.</p><p>To do this in a meaningful way, we propose a simple "filter normalization" scheme that enables us to do side-by-side comparisons of different minima found in neural networks training. We then use visualizations to explore sharpness/flatness of minimizers found by different methods, as well as the effect of network architecture choices (use of skip connections, number of filters, network depth) on the loss landscape. Out goal is to understand how differences in loss function geometry affect the generalization of neural nets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">CONTRIBUTIONS</head><p>In this article, we study methods for producing meaningful loss function visualizations. Then, using these visualization methods, we explore how loss landscape geometry effects generalization error and trainability. More specifically, we address the following issues:</p><p>• We reveal faults in a number of visualization methods for loss functions, and show that simple visualization strategies fail to accurately capture the local geometry (sharpness or flatness) of loss function minimizers.</p><p>• We present a simple visualization method based on "filter normalization" that enables sideby-side comparisons of different minimizers. The sharpness of minimizers correlates well with generalization error when this visualization is used, even when making comparisons across disparate network architectures and training methods.</p><p>• We observe that, when networks become sufficiently deep, neural loss landscapes quickly transition from being nearly convex to being highly chaotic. This transition from convex to chaotic behavior coincides with a dramatic drop in generalization error, and ultimately to a lack of trainability.</p><p>• We show that skip connections promote flat minimizers and prevent the transition to chaotic behavior, which helps explain why skip connections are necessary for training extremely deep networks.</p><p>• We study the visualization of SGD optimization trajectories. We explain the difficulties that arise when visualizing these trajectories, and show that optimization trajectories lie in an extremely low dimensional space. This low dimensionality can be explained by the presence of large nearly convex regions in the loss landscape, such as those observed in our 2-dimensional visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">THEORETICAL BACKGROUND &amp; RELATED WORK</head><p>Visualizations have the potential to help us answer several important questions about why neural networks work. In particular, why are we able to minimize highly non-convex neural loss functions? And why do the resulting minima generalize?</p><p>Because of the difficulty of visualizing loss functions, most studies of loss landscapes are largely theoretical in nature. A number of authors have studied our ability to minimize neural loss functions. Using random matrix theory and spin glass theory, several authors have shown that local minima are of low objective value <ref type="bibr">(Dauphin et al., 2014;</ref><ref type="bibr">Choromanska et al., 2015)</ref>. It can also be shown that local minima are global minima, provided one assumes linear neurons <ref type="bibr">(Hardt &amp; Ma, 2017)</ref>, very wide layers <ref type="bibr" target="#b11">(Nguyen &amp; Hein, 2017)</ref>, or full rank weight matrices <ref type="bibr" target="#b20">(Yun et al., 2017)</ref>. These assumptions have been relaxed by <ref type="bibr" target="#b2">Kawaguchi (2016)</ref> and <ref type="bibr" target="#b10">Lu &amp; Kawaguchi (2017)</ref>, although some assumptions (e.g., of the loss functions) are still required. <ref type="bibr" target="#b16">Soudry &amp; Hoffer (2017)</ref>; <ref type="bibr">Freeman &amp; Bruna (2017)</ref>; <ref type="bibr" target="#b19">Xie et al. (2017)</ref> also analyzed shallow networks with one or two hidden layers under mild conditions.</p><p>Another approach is to show that we can expect good minimizers, not simply because of the endogenous properties of neural networks, but because of the optimizers. For restricted network classes such as those with one hidden layer, with some extra assumptions on the sample distribution, globally optimal or near-optimal solutions can be found by common optimization methods <ref type="bibr" target="#b15">(Soltanolkotabi et al., 2017;</ref><ref type="bibr" target="#b6">Li &amp; Yuan, 2017;</ref><ref type="bibr" target="#b18">Tian, 2017)</ref>. For networks with specific structures, <ref type="bibr" target="#b12">Safran &amp; Shamir (2016)</ref> and <ref type="bibr">Haeffele &amp; Vidal (2017)</ref> show there likely exists a monotonically decreasing path from an initialization to a global minimum. <ref type="bibr" target="#b17">Swirszcz et al. (2016)</ref> show counterexamples that achieve "bad" local minima for toy problems.</p><p>Also of interest is work on assessing the sharpness/flatness of local minima. Hochreiter &amp; Schmidhuber (1997) defined "flatness" as the size of the connected region around the minimum where the training loss remains low. <ref type="bibr" target="#b4">Keskar et al. (2017)</ref> suggested that flatness can be characterized by the magnitude of the eigenvalues of the Hessian, and proposed -sharpness as approximation, which looks at the maximum loss in a bounded neighborhood of a minimum. However, <ref type="bibr">Dinh et al. (2017)</ref> show that these quantitative measure of sharpness are problematic because they are not invariant to symmetries in the network, and are thus not sufficient to determine the generalization ability. <ref type="bibr">Chaudhari et al. (2017)</ref> used local entropy as a measure of sharpness, which is invariant to the simple transformation in <ref type="bibr">Dinh et al. (2017)</ref>, but difficult to accurately compute. <ref type="bibr">Dziugaite &amp; Roy (2017)</ref> connects sharpness to PAC-Bayes bounds for generalization.</p><p>Theoretical results make some restrictive assumptions such as the independence of the input samples, or restrictions on non-linearities and loss functions. For this reason, visualizations play a key role in verifying the validity of theoretical assumptions, and understanding loss function behavior in real-world systems. In the next section, we briefly review methods that have been used for this purpose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">THE BASICS OF LOSS FUNCTION VISUALIZATION</head><p>Neural networks are trained on a corpus of feature vectors (e.g., images) {x i } and accompanying labels {y i } by minimizing a loss of the form</p><formula xml:id="formula_0">L(θ) = 1 m m i=1 (x i , y i ; θ)</formula><p>where θ denotes the parameters (weights) of the neural network, the function (x i , y i ; θ) measures how well the neural network with parameters θ predicts the label of a data sample, and m is the number of data samples.</p><p>Neural nets contain many parameters, and so their loss functions live in a very high-dimensional space. Unfortunately, visualizations are only possible using low-dimensional 1D (line) or 2D (surface) plots. Several methods exist for closing this dimensionality gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1-Dimensional Linear Interpolation</head><p>One simple and lightweight way to plot loss functions is to choose two sets of parameters θ and θ , and plot the values of the loss function along the line connecting these two points. We can parameterize this line by choosing a scalar parameter α, and defining the weighted average θ(α) = (1 − α)θ + αθ . Finally, we plot the function f (α) = L(θ(α)). This strategy was taken by <ref type="bibr">Goodfellow et al. (2015)</ref>, who studied the loss surface along the line between a (random) initial guess, and a nearby minimizer obtained by stochastic gradient descent. This method has been widely used to study the "sharpness" and "flatness" of different minima, and the dependence of sharpness on batch-size <ref type="bibr" target="#b4">(Keskar et al., 2017;</ref><ref type="bibr">Dinh et al., 2017)</ref>. <ref type="bibr" target="#b14">Smith &amp; Topin (2017)</ref> use the same 1D interpolation technique to show different minima and the "peaks" between them, while <ref type="bibr">Im et al. (2016)</ref> plot the line between minima obtained via different optimizers.</p><p>The 1D linear interpolation method suffers from several weaknesses. First, it is difficult to visualize non-convexities using 1D plots. Indeed, <ref type="bibr">Goodfellow et al. (2015)</ref> found that loss functions appear to lack local minima along the minimization trajectory. We will see later, using 2D methods, that some loss functions have extreme non-convexities, and that these non-convexities correlate with the difference in generalization between different network architectures. Second, this method does not consider batch normalization or invariance symmetries in the network. For this reason, the visual sharpness comparisons produced by 1D interpolation plots may be misleading; this issue will be explored in depth in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Contour Plots</head><p>To use this approach, one chooses a center point θ * in the graph, and chooses two direction vectors, δ and η. One then plots a function of the form f (α) = L(θ * + αδ) in the 1D (line) case, or</p><formula xml:id="formula_1">f (α, β) = L(θ * + αδ + βη)<label>(1)</label></formula><p>in the 2D (surface) case 1 . This approach was used in <ref type="bibr">(Goodfellow et al., 2015)</ref> to explore the trajectories of different minimization methods. It was also used in <ref type="bibr">(Im et al., 2016)</ref> to show that different optimization algorithms find different local minima within the 2D projected space.</p><p>Because of the computational burden of 2D plotting, these methods generally result in low-resolution plots of small regions that have not captured the complex non-convexity of loss surfaces. Below, we use high-resolution visualizations over large slices of weight space to visualize how network design affects non-convex structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">PROPOSED VISUALIZATION: FILTER-WISE NORMALIZATION</head><p>This study relies heavily on plots of the form (1) produced using random direction vectors, δ and η, each sampled from a random Gaussian distribution with appropriate scaling (described below).</p><p>While the "random directions" approach to plotting is simple, it cannot be used to compare the geometry of two different minimizers or two different networks. This is because of the scale invariance in network weights. When ReLU non-linearities are used, the network remains unchanged if we (for example) multiply the weights in one layer of a network by 10, and divide the next layer by 10. This invariance is even more prominent when batch normalization is used. In this case, the size (i.e., norm) of a filter is irrelevant because the output of each layer is re-scaled during batch normalization. For this reason, a network's behavior remains unchanged if we re-scale the weights.</p><p>Scale invariance prevents us from making meaningful comparisons between plots, unless special precautions are taken. A neural network with large weights may appear to have a smooth and slowly varying loss function; perturbing the weights by one unit will have very little effect on network performance if the weights live on a scale much larger than one. However, if the weights are much smaller than one, then that same one unit perturbation may have a catastrophic effect, making the loss function appear quite sensitive to weight perturbations. Keep in mind that neural nets are scale invariant; if the small-parameter and large-parameter networks in this example are equivalent (because one is simply a re-scaling of the other), then any apparent differences in the loss function are merely an artifact of scale invariance. This scale invariance was exploited by <ref type="bibr">Dinh et al. (2017)</ref> to build pairs of equivalent networks that have different apparent sharpness.</p><p>To remove this scaling effect, we plot loss functions using filter-wise normalized directions. To obtain such directions for a network with parameters θ, we begin by producing a random Gaussian direction vector d with dimensions compatible with θ. Then we normalize each filter in d to have the same norm of the corresponding filter in θ. In other words, we make the replacement</p><formula xml:id="formula_2">d i,j ← d i,j d i,j θ i,j ,</formula><p>where d i,j represents the jth filter (not the jth weight) of the ith layer of d, and · denotes the Frobenius norm. Note that the filter-wise normalization is different from that of <ref type="bibr">(Im et al., 2016)</ref>, which normalize the direction without considering the norm of individual filters.</p><p>The proposed scaling is an important factor when making meaningful plots of loss function geometry. We will explore the importance of proper scaling below as we explore the sharpness/flatness of different minimizers. In this context, we show that the sharpness of filter-normalized plots correlates with generalization error, while plots without filter normalization can be very misleading.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">THE SHARP VS FLAT DILEMMA</head><p>Section 4 introduces the concept of filter normalization, and provides an intuitive justification for its use. In this section, we address the issue of whether sharp minimizers generalize better than flat minimizers. In doing so, we will see that the sharpness of minimizers correlates well with generalization error when filter normalization is used. This enables side-by-side comparisons between plots. In contrast, the sharpness of non-normalized plots may appear distorted and unpredictable.</p><p>It is widely thought that small-batch SGD produces "flat" minimizers that generalize better, while large batch sizes produce "sharp" minima with poor generalization <ref type="bibr">(Chaudhari et al., 2017;</ref><ref type="bibr" target="#b4">Keskar et al., 2017;</ref><ref type="bibr">Hochreiter &amp; Schmidhuber, 1997)</ref>. This claim is disputed though, with Dinh et al. <ref type="formula" target="#formula_1">(2017)</ref>;  arguing that generalization is not directly related to the curvature of loss surfaces, and some authors proposing specialized training methods that achieve good performance with large batch sizes <ref type="bibr" target="#b16">(Hoffer et al., 2017;</ref><ref type="bibr">Goyal et al., 2017;</ref><ref type="bibr">De et al., 2017)</ref>.</p><p>Here, we explore the difference between sharp and flat minimizers. We begin by discussing difficulties that arise when performing such a visualization, and how proper normalization can prevent such plots from producing distorted results.</p><p>We train a CIFAR-10 classifier using a 9-layer VGG network <ref type="bibr" target="#b13">(Simonyan &amp; Zisserman, 2015)</ref> with Batch Normalization <ref type="bibr" target="#b1">(Ioffe &amp; Szegedy, 2015)</ref>. We use two batch sizes: a large batch size of 8192 (16.4% of the training data of CIFAR-10), and a small batch size of 128. Let θ s and θ l indicate the solutions obtained by running SGD using small and large batch sizes, respectively 2 . Using the linear interpolation approach <ref type="bibr">(Goodfellow et al., 2015)</ref>, we plot the loss values on both training and testing data sets of CIFAR-10, along a direction containing the two solutions, i.</p><formula xml:id="formula_3">e., f (α) = L(θ s +α(θ l −θ s )).</formula><p>2 In this section, we consider the "running mean" and "running variance" as trainable parameters and include them in θ. Note that the original study by <ref type="bibr">Goodfellow et al. (2015)</ref> does not consider batch normalization. These parameters are not included in θ in future sections, as they are only needed when interpolating between two minimizers.  Similar to <ref type="bibr" target="#b4">Keskar et al. (2017)</ref>, we also superimpose the classification accuracy in red. This plot is shown in <ref type="figure" target="#fig_1">Figure 2</ref>.</p><p>Figures 2(a) and 2(b) show linear interpolation plots with θ s at x-axis location 0, and θ l at location 1 3 . As observed by <ref type="bibr" target="#b4">Keskar et al. (2017)</ref>, we can clearly see that the small-batch solution is quite wide, while the large-batch solution is sharp. However, this sharpness balance can be flipped simply by turning on weight decay <ref type="bibr" target="#b5">(Krogh &amp; Hertz, 1992)</ref>. Figures 2(c) and 2(d) show results of the same experiment, except this time with a non-zero weight decay parameter. This time, the large batch minimizer is considerably flatter than the sharp small batch minimizer. However, we see from <ref type="table" target="#tab_0">Table 1</ref> that small batches generalize better in all 4 experiments; there is no apparent correlation between sharpness and generalization. We will see that these side-by-side sharpness comparisons are extremely misleading, and fail to capture the endogenous properties of the minima.</p><p>The apparent differences in sharpness in <ref type="figure" target="#fig_1">Figure 2</ref> can be explained by examining the weights of each minimizer. Histograms of the networks weights are shown for each experiment in <ref type="figure" target="#fig_2">Figure 3</ref>. We see that, when a large batch is used with zero weight decay, the resulting weights tends to be smaller than in the small batch case. We reverse this effect by adding weight decay; in this case the large batch minimizer has much larger weights than the small batch minimizer. This difference in scale occurs for a simple reason: A smaller batch size results in more weight updates per epoch than a large batch size, and so the shrinking effect of weight decay (which imposes a penalty on the norm of the weights) is more pronounced. The evolution of the filter norms during training is depicted in <ref type="figure" target="#fig_0">Figure 11</ref> of the Appendix. <ref type="figure" target="#fig_1">Figure 2</ref> in not visualizing the endogenous sharpness of minimizers, but rather just the (irrelevant) weight scaling. The scaling of weights in these networks is irrelevant because batch normalization re-scales the outputs to have unit variance. However, small weights still appear more sensitive to perturbations, and produce sharper looking minimizers.</p><p>Filter Normalized Plots We repeat the experiment in <ref type="figure" target="#fig_1">Figure 2</ref>, but this time we plot the loss function near each minimizer separately using random filter-normalized directions. This removes the apparent differences in geometry caused by the scaling depicted in <ref type="figure" target="#fig_2">Figure 3</ref>. The results, presented in <ref type="figure">Figure 4</ref>, still show differences in sharpness between small batch and large batch minima, however these differences are much more subtle than it would appear in the un-normalized plots. For comparison, sample un-normalized plots are shown in Section 9.2 of the Appendix.</p><p>We also visualize these results using two random directions and contour plots. As shown in <ref type="figure">Figure 5</ref>, the weights obtained with small batch size and non-zero weight decay have wider contours than the sharper large batch minimizers. Results for ResNet-56 appear in <ref type="figure" target="#fig_0">Figure 17</ref> of the Appendix. <ref type="bibr">3</ref> The 1D interpolation method for plotting is described in detail in Section 3.   <ref type="figure">Figure 4</ref>, the first row uses zero weight decay and the second row sets weight decay to 5e-4.</p><p>Generalization and Flatness Using the filter-normalized plots in Figures 4 and 5, we can make side-by-side comparisons between minimizers, and we see that now sharpness correlates well with generalization error. Large batches produced visually sharper minima (although not dramatically so) with higher test error. Interestingly, the Adam optimizer attained larger test error than SGD, and, as predicted, the corresponding minima are visually sharper. Results of a similar experiment using ResNet-56 are presented in the Appendix <ref type="figure" target="#fig_0">(Figure 17</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">WHAT MAKES NEURAL NETWORKS TRAINABLE? INSIGHTS ON THE (NON)CONVEXITY STRUCTURE OF LOSS SURFACES</head><p>Our ability to find global minimizers to neural loss functions is not universal; it seems that some neural architectures are easier to minimize than others. For example, using skip connections, <ref type="bibr">He et al. (2016)</ref> were able to train extremely deep architectures, while comparable architectures without skip connections are not trainable. Furthermore, our ability to train seems to depend strongly on the initial parameters from which training starts.</p><p>Using visualization methods, we do an empirical study of neural architectures to explore why the non-convexity of loss functions seems to be problematic in some situations, but not in others. We aim to provide insight into the following questions: Do loss functions have significant non-convexity at all? If prominent non-convexities exist, why are they not problematic in all situations? Why are some architectures easy to train, and why are results so sensitive to the initialization? We will see that different architectures have extreme differences in non-convexity structure that answer these questions, and that these differences correlate with generalization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">EXPERIMENTAL SETUP</head><p>To understand the effects of network architecture on non-convexity, we trained a number of networks, and plotted the landscape around the obtained minimizers using the filter-normalized random direction method described in Section 4. We consider three classes of neural networks:</p><p>• Residual networks that are optimized for performance on CIFAR <ref type="bibr">(He et al., 2016)</ref>. We consider , where each name is labeled with the number of convolutional layers it has.</p><p>• "VGG-like" networks that do not contain shortcut/skip connections. We produced these networks simply by removing the skip connections from the CIFAR-optimized ResNets. We call these networks ResNet-20-noshort, ResNet-56-noshort, and ResNet-110-noshort. Note that these networks do not all perform well on the CIFAR-10 task. We use them purely for experimental purposes to explore the effect of shortcut connections.</p><p>• "Wide" ResNets that have been optimized for ImageNet rather than CIFAR. These networks have more filters per layer than the CIFAR optimized networks, and also have different numbers of layers. These models include ResNet-18, ResNet-34, and ResNet-50.</p><p>All models are trained on the CIFAR-10 dataset using SGD with Nesterov momentum, batch-size 128, and 0.0005 weight decay for 300 epochs. The learning rate was initialized at 0.1, and decreased by a factor of 10 at epochs 150, 225 and 275. Deeper experimental VGG-like networks (e.g., ResNet-56-noshort, as described below) required a smaller initial learning rate of 0.01.</p><p>High resolution 2D plots of the minimizers for different neural networks are shown in <ref type="figure" target="#fig_4">Figure 7</ref>.</p><p>Results are shown as contour plots rather than surface plots because this makes it extremely easy to see non-convex structures and evaluate sharpness. For surface plots of ResNet-56, see <ref type="figure" target="#fig_0">Figure 1</ref>. Note that the center of each plot corresponds to the minimizer, and the two axes parameterize two random directions with filter-wise normalization as in (1). We make several observations below about how architecture effects the loss landscape. We also provide loss and error values for these networks in <ref type="table" target="#tab_3">Table 3</ref>, and convergence curves in <ref type="figure" target="#fig_1">Figure 22</ref> of the Appendix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">THE EFFECT OF NETWORK DEPTH</head><p>From <ref type="figure" target="#fig_4">Figure 7</ref>, we see that network depth has a dramatic effect on the loss surfaces of neural networks when skip connections are not used. The network ResNet-20-noshort has a fairly benign landscape dominated by a region with convex contours in the center, and no dramatic non-convexity. This isn't too surprising: the original VGG networks for ImageNet had 19 layers and could be trained effectively <ref type="bibr" target="#b13">(Simonyan &amp; Zisserman, 2015)</ref>.</p><p>However, as network depth increases, the loss surface of the VGG-like nets spontaneously transitions from (nearly) convex to chaotic. ResNet-56-noshort has dramatic non-convexities and large regions where the gradient directions (which are normal to the contours depicted in the plots) do not point towards the minimizer at the center. Also, the loss function becomes extremely large as we move in some directions. ResNet-110-noshort displays even more dramatic non-convexities, and becomes extremely steep as we move in all directions shown in the plot. Furthermore, note that the minimizers at the center of the deep VGG-like nets seem to be fairly sharp. In the case of ResNet-56-noshort, the minimizer is also fairly ill-conditioned, as the contours near the minimizer have significant eccentricity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">SHORTCUT CONNECTIONS TO THE RESCUE</head><p>Shortcut connections have a dramatic effect of the geometry of the loss functions. In <ref type="figure" target="#fig_4">Figure 7</ref>, we see that residual connections prevent the transition to chaotic behavior as depth increases. In fact, the width and shape of the 0.1-level contour is almost identical for the 20-and 110-layer networks.</p><p>Interestingly, the effect of skip connections seems to be most important for deep networks. For the more shallow networks , the effect of skip connections is fairly unnoticeable. However residual connections prevent the explosion of non-convexity that occurs when networks get deep. This effect seems to apply to other kinds of skip connections as well; <ref type="figure" target="#fig_1">Figure 20</ref> of the Appendix shows the loss landscape of DenseNet <ref type="bibr">(Huang et al., 2017)</ref>, which shows no noticeable non-convexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">WIDE MODELS VS THIN MODELS</head><p>To see the effect of the number of convolutional filters per layer, we compare the narrow CIFARoptimized ResNets (ResNet-20/56/110) with wider ResNets (ResNet-18/34/50) that have more filters and were optimized for ImageNet. From <ref type="figure" target="#fig_4">Figure 7</ref>, we see that the wider models have loss landscapes with no noticeable chaotic behavior. Increased network width resulted in flat minima and wide regions of apparent convexity.</p><p>This effect is also validated by <ref type="figure" target="#fig_5">Figure 8</ref>, in which we plot the landscape of ResNet-56, but we multiple the number of filter per layer by k = 2, 4, and 8 as in <ref type="bibr" target="#b21">Zagoruyko &amp; Komodakis (2016)</ref>. We see that increased width prevents chaotic behavior, and skip connections dramatically widen minimizers. Finally, note that sharpness correlates extremely well with test error. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">IMPLICATIONS FOR NETWORK INITIALIZATION</head><p>One of the most interesting observations seen in <ref type="figure" target="#fig_4">Figure 7</ref> is that loss landscapes for all the networks considered seem to be partitioned into a well-defined region of low loss value and convex contours, surrounded by a well-defined region of high loss value and non-convex contours.</p><p>This partitioning of chaotic and convex regions may explain the importance of good initialization strategies, and also the easy training behavior of "good" architectures. When using normalized random initialization strategies such as those proposed by Glorot &amp; Bengio (2010), typical neural networks attain an initial loss value less than 2.5. The well behaved loss landscapes in <ref type="figure" target="#fig_4">Figure 7</ref> (ResNets, and shallow VGG-like nets) are dominated by large, flat, nearly convex attractors that rise to a loss value of 4 or greater. For such landscapes, a random initialization will likely lie in the "well-behaved" loss region, and the optimization algorithm might never "see" the pathological non-convexities that occur on the high loss chaotic plateaus.</p><p>Chaotic loss landscapes (ResNet-56-noshort and ResNet-110-noshort) have shallower regions of convexity that rise to lower loss values. For sufficiently deep networks with shallow enough attractors, the initial iterate will likely lie in the chaotic region where the gradients are uninformative. In this case, the gradients "shatter" <ref type="bibr" target="#b0">Balduzzi et al. (2017)</ref>, and training is impossible. SGD was unable to train a 156 layer network without skip connections (even with very low learning rates), which adds weight to this hypothesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6">LANDSCAPE GEOMETRY AFFECTS GENERALIZATION</head><p>Both <ref type="figure" target="#fig_4">Figures 7 and 8</ref> show that landscape geometry has a dramatic effect on generalization. First, note that visually flatter minimizers consistently correspond to lower test error, which further strengthens our assertion that filter normalization is a natural way to visualize loss function geometry.</p><p>Second, we notice that chaotic landscapes (deep networks without skip connections) result in worse training and test error, while more convex landscapes have lower error values. In fact, the most convex landscapes, wide ResNets in the bottom row of <ref type="figure" target="#fig_4">Figure 7)</ref>, generalize the best of all, and show no noticeable chaotic behavior.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">VISUALIZING OPTIMIZATION PATHS</head><p>Finally, we explore methods for visualizing the trajectories of different optimizers. For this application, random directions are ineffective. We will provide a theoretical explanation for why random directions fail, and explore methods for effectively plotting trajectories on top of loss function contours.</p><p>Several authors have observed that random direction fail to capture the variation in optimization trajectories, including Gallagher &amp; Downs <ref type="formula">(2003)</ref>; <ref type="bibr" target="#b9">Lorch (2016)</ref>; Lipton (2016); <ref type="bibr" target="#b7">Liao &amp; Poggio (2017)</ref>. Several failed visualizations are depicted in <ref type="figure" target="#fig_6">Figure 9</ref>. In <ref type="figure" target="#fig_6">Figure 9</ref>(a), we see the iterates of SGD projected onto the plane defined by two random directions. Almost none of the motion is captured (notice the super-zoomed-in axes and the seemingly random walk). This problem was noticed by <ref type="bibr">Goodfellow et al. (2015)</ref>, who then visualized trajectories using one direction that points from initialization to solution, and one random direction. This approach is shown in <ref type="figure" target="#fig_6">Figure 9</ref>(b). As seen in <ref type="figure" target="#fig_6">Figure 9</ref>(c), the random axis captures almost no variation, leading to the (misleading) appearance of a straight line path. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1">WHY RANDOM DIRECTIONS FAIL: LOW DIMENSIONAL OPTIMIZATION TRAJECTORIES</head><p>It is well known that two random vectors in a high dimensional space will be nearly orthogonal with high probability. In fact, the expected cosine similarity between Gaussian random vectors in n dimensions is roughly 2/(πn) <ref type="bibr">(Goldstein &amp; Studer (2016)</ref>, Lemma 5). This is problematic when optimization trajectories lie in extremely low dimensional spaces. In this case, a randomly chosen vector will lie orthogonal to the low-rank space containing the optimization path, and a projection onto a random direction will capture almost no variation. <ref type="figure" target="#fig_6">Figure 9</ref>(b) suggests that optimization trajectories are low dimensional because the random direction captures orders of magnitude less variation than the vector that points along the optimization path. Below, we use PCA directions to directly validate this low dimensionality, and also to produce effective visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2">EFFECTIVE TRAJECTORY PLOTTING USING PCA DIRECTIONS</head><p>To capture variation in trajectories, we need to use non-random (and carefully chosen) directions. Here, we suggest an approach based on PCA that allows us to measure how much variation we've captured; we also provide plots of these trajectories along the contours of the loss surface.</p><p>Let θ i denote model parameters at epoch i and the final estimate as θ n . Given n training epochs, we can apply PCA to the matrix M = [θ 0 − θ n ; · · · ; θ n−1 − θ n ], and then select the two most explanatory directions. Optimizer trajectories (blue dots) and loss surfaces along PCA directions are shown in <ref type="figure" target="#fig_0">Figure 10</ref>. Epochs where the learning rate was decreased are shown as red dots. On each axis, we measure the amount of variation in the descent path captured by that PCA direction.</p><p>We see some interesting behavior in these plots. At early stages of training, the paths tend to move perpendicular to the contours of the loss surface, i.e., along the gradient directions as one would expect from non-stochastic gradient descent. The stochasticity becomes fairly pronounced in several plots during the later stages of training. This is particularly true of the plots that use weight decay and small batches (which leads to more gradient noise, and a more radical departure from deterministic gradient directions). When weight decay and small batches are used, we see the path turn nearly parallel to the contours and "orbit" the solution when the stepsize is large. When the stepsize is dropped (at the red dot), the effective noise in the system decreases, and we see a kink in the path as the trajectory falls into the nearest local minimizer.</p><p>Finally, we can directly observe that the descent path is very low dimensional: between 40% and 90% of the variation in the descent paths lies in a space of only 2 dimensions. The optimization trajectories in <ref type="figure" target="#fig_0">Figure 10</ref> appear to be dominated by movement in the direction of a nearby attractor. This low dimensionality is compatible with the observations in Section 6.5, where we observed that non-chaotic landscapes are dominated by wide, flat minimizers. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">CONCLUSION</head><p>In this paper, we presented a new, more accurate visualization technique that provided insights into the consequences of a variety of choices facing the neural network practitioner, including network architecture, optimizer selection, and batch size.</p><p>Neural networks have advanced dramatically in recent years, largely on the back of anecdotal knowledge and theoretical results with complex assumptions. For progress to continue to be made, a more general understanding of the structure of neural networks is needed. Our hope is that effective visualization, when coupled with continued advances in theory, can result in faster training, simpler models, and better generalization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9">APPENDIX</head><p>9.1 THE CHANGE OF WEIGHTS NORM DURING TRAINING <ref type="figure" target="#fig_0">Figure 11</ref> shows the change of weights norm during training in terms of epochs and iterations. (h) Adam, WD=5e-4, iter <ref type="figure" target="#fig_0">Figure 11</ref>: The change of weights norm during training for VGG-9. When weight decay is disabled, the weight norm grows steadily during training without constraints. While when nonzero weight decay is adopted, the weight norm decreases rapidly at the beginning and becomes stable until the learning rate is decayed. Since we use a fixed number of epochs for different batch sizes, the difference in weight norm change between large-batch and small-batch training is mainly caused by the larger number of updates when a small batch is used. As shown in the second row, the changes of weight norm are at the same pace for both small and large batch training in terms of iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.2">COMPARISION OF NORMALIZATION METHODS</head><p>Here we compare several normalization methods for a given random normal direction d. Let d i denote the weights of layer i and d i,j represent the j-th filter in the i-th layer.</p><p>• No Normalization In this case, the direction d is added to the weights directly without processing.</p><p>• Filter Normalization The direction d is normalized in the filter level so that the direction for each filter has the same norm as the corresponding filter in θ,</p><formula xml:id="formula_4">d i,j ← di,j di,j θ i,j</formula><p>. This is the approach advocated in this article, and is used extensively for plotting loss surfaces. Note that filter normalization is not limited to convolutional (Conv) layers but also applies to fully connected (FC) layers. The FC layer is equivalent to a Conv layer with a 1 × 1 output feature map and the filter corresponds to the weights that generate one neuron.</p><p>• Layer Normalization The direction d is normalized in the layer level so that the direction for each layer has the same norm as the corresponding layer of θ, <ref type="figure" target="#fig_0">Figure 12</ref> shows the 1D randomalized plots without normalization. One issue with the non-normalized plots is that the x-axis range must be chosen carefully. <ref type="figure" target="#fig_0">Figure 13</ref> shows an enlarged plot with [−0.2, 0.2] as the x-axis. Without normalization, the plots fail to show consistency between flatness and generalization error. Here we compare filter normalization with layer normalization. We find filter normalization is more accurate than layer normalization. One failing case for layer normalization is shown in <ref type="figure" target="#fig_0">Figure 14</ref>, where <ref type="figure" target="#fig_0">Figure 14</ref>(g) is flatter than <ref type="figure" target="#fig_0">Figure 14</ref>(c), but with worse generalization error. Figure 14: 1D loss surface for VGG-9 with layer normalization. The first row has no weight decay and the second row uses weight decay 5e-4.</p><formula xml:id="formula_5">d i ← di di θ i .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.3">LARGE-BATCH AND SMALL-BATCH RESULTS FOR RESNET-56</head><p>Similar to the observations made in Section 5, the "sharp vs sharp dilemma" also applies on ResNet-56 as shown in <ref type="figure" target="#fig_0">Figure 15</ref>. The generalization error for each solution is shown in <ref type="table" target="#tab_1">Table 2</ref>. The 1D and 2D visualization with filter normalized directions are shown in <ref type="figure" target="#fig_0">Figure 16</ref> and <ref type="figure" target="#fig_0">Figure 17</ref>.    <ref type="figure" target="#fig_0">Figure 16</ref>, the first row uses zero weight decay and the second row sets weight decay to 5e-4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.4">REPEATABILITY OF THE LOSS CURVES</head><p>Does different direction produce dramatically different surface? We plot the 1D loss surface of VGG-9 with 10 random filter-normalized directions. As shown in <ref type="figure" target="#fig_0">Figure 18</ref>, the shape of different plots are very close, which indicates good generalization ability of the minima. We also repeat the 2D loss surface plots multiple times for ResNet-56-noshort, which has worse generalization error. As shown in <ref type="figure" target="#fig_0">Figure 19</ref>, there are apparent changes in the loss surface for different plots, however, the choatic behaviour is consistent across plots. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.5">IMPLEMENTATION DETAILS</head><p>Computing resources for generating the figures Our PyTorch code can be executed in a multiple GPU workstation as well as a HPC with hundreds of GPUs using mpi4py. The computation time depends on the model's inference speed on the training set, the resolution of the plots, and the number of GPUs. The resolution for the 1D plots in <ref type="figure">Figure 4</ref> is 401. The default resolutions used for the 2D contours in <ref type="figure">Figure 5</ref> and <ref type="figure" target="#fig_4">Figure 7</ref> is 51 × 51. We use higher resolutions (251 × 251) for the ResNet-56-noshort used in <ref type="figure" target="#fig_0">Figure 1</ref> to show more details. For example, a 2D contour of ResNet-56 model with a (relatively low) resolution of 51 × 51 will take about 1 hour on a workstation with 4 GPUs (Titan X Pascal or 1080 Ti).</p><p>Batch normalization parameters In the 1D linear interpolation methods, the BN parameters including the running mean and running variance need to be considered as part of θ. If these parameters are not considered, then it is not possible to reproduce the loss accurately for both minimizers. In the filter-normalized visualization, the random direction applies to all weights but not the weights in BN. Note that the filter normalization process removes the effect of weight scaling, and so the batch normalization can be ignored.</p><p>The VGG-9 architecture details and parameters for Adam VGG-9 is a cropped version of VGG-16, which keeps the first 7 Conv layers in VGG-16 with 2 FC layers. A Batch Normalization layer is added after each Conv layer and the first FC layer. We find VGG-9 is an efficient network with better performance comparing to VGG-16 on CIFAR-10. We use the default values for β 1 , β 2 and in Adam with the same learning rate schedule as used in SGD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="9.6">LOSS SURFACE FOR DENSENET-121</head><p>The 2D visualization of the solution of DeseNet-121 trained on CIFAR-10 is shown in <ref type="figure" target="#fig_1">Figure 20</ref>. The loss curves for training VGG-9 used in Section 5 is shown in <ref type="figure" target="#fig_0">Figure 21</ref>. <ref type="figure" target="#fig_1">Figure 22</ref> shows the loss curves and error curves of architectures used in Section 6 and <ref type="table" target="#tab_3">Table 3</ref> shows the final error and loss values. The default setting for training is using SGD with Nesterov momentum, batch-size 128, and 0.0005 weight decay for 300 epochs. The default learning rate was initialized at 0.1, and decreased by a factor of 10 at epochs 150, 225 and 275.   </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The loss surfaces of ResNet-56 with/without skip connections. The vertical axis is logarithmic to show dynamic range. The proposed filter normalization scheme is used to enable comparisons of sharpness/flatness between the two figures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: 1D linear interpolation of solutions obtained by small-batch and large-batch methods for VGG9. The blue lines are loss values and the red lines are accuracies. The solid lines are training curves and the dashed lines are for testing. Small batch is at abscissa 0, and large batch is at abscissa 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Histogram of weights. With zero weight decay, small-batch methods produce large weights. With non-zero weight decay, small-batch methods produce smaller weights.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: The shape of minima obtained using different optimization algorithms, batch size and weight decay. The title of each subfigure contains the optimizer, batch size, and test error. The first row has no weight decay and the second row uses weight decay 5e-4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 6: (left) The loss surfaces of ResNet-110-noshort, without skip connections. (right) DenseNet, the current state-of-the-art network for CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Wide-ResNet-56 (WRN-56) on CIFAR-10 both with shortcut connections (top) and without (bottom). The label k = 2 means twice as many filters per layer, k = 4 means 4 times, etc. Test error is reported below each figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Ineffective visualizations of optimizer trajectories. These visualizations suffer from the orthogonality of random directions in high dimensions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Projected learning trajectories use normalized PCA directions for VGG-9. The left plot in each subfigure uses batch size 128, and the right one uses batch size 8192.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :Figure 13 :</head><label>1213</label><figDesc>Figure 12: 1D loss surface for VGG-9 without normalization. The first row has no weight decay and the second row uses weight decay 0.0005.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 15 :</head><label>15</label><figDesc>Figure 15: 1D linear interpolation of solutions obtained by small-batch and large-batch methods for ResNet56. The blue lines are loss values and the red lines are error. The solid lines are training curves and the dashed lines are for testing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 16 :Figure 17 :</head><label>1617</label><figDesc>Figure 16: The shape of minima obtained via different optimization algorithms for ResNet-56, with varying batch size and weight decay. Similar to Figure 4, the first row uses zero weight decay and the second row uses 5e-4 weight decay.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 18 :Figure 19 :</head><label>1819</label><figDesc>Figure 18: Repeatability of the surface plots for VGG-9 with filter normalization. The shape of minima obtained using 10 different random filter-normalized directions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 20 :</head><label>20</label><figDesc>Figure 20: The loss landscape for DenseNet-121 trained on CIFAR-10. The final training error is 0.002 and the testing error is 4.37</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 21 :Figure 22 :</head><label>2122</label><figDesc>Figure 21: Training loss/error curves for VGG-9 with different optimization methods. The first row shows loss curves and the second is about the error curves. Dashed lines are for testing, solid for training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Test errors of VGG-9 on CIFAR-10 with different optimization algorithms and hyper- parameters.</figDesc><table>SGD 
Adam 
bs=128 bs=8192 bs=128 bs=8192 
WD = 0 
7.37 
11.07 
7.44 
10.91 
WD = 5e-4 
6.00 
10.19 
7.80 
9.52 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Test error for ResNet-56 with different optimization algorithms and batch-size/weight-decay parameters.</figDesc><table>SGD 
Adam 
bs=128 bs=4096 bs=128 bs=4096 
WD = 0 
8.26 
13.93 
9.55 
14.30 
WD = 5e-4 
5.89 
10.59 
7.67 
12.36 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Loss values and errors for different architectures trained on CIFAR-10. init LR Training Loss Training Error Test Error</figDesc><table>ResNet-20 
0.1 
0.017 
0.286 
7.37 
ResNet-20-noshort 
0.1 
0.025 
0.560 
8.18 
ResNet-56 
0.1 
0.004 
0.052 
5.89 
ResNet-56-noshort 
0.1 
0.192 
6.494 
13.31 
ResNet-56-noshort 
0.01 
0.024 
0.704 
10.83 
ResNet-110 
0.1 
0.002 
0.042 
5.79 
ResNet-110-noshort 
0.01 
0.258 
8.732 
16.44 
ResNet-18 
0.1 
0.002 
0.002 
4.84 
ResNet-34 
0.1 
0.001 
0.014 
4.73 
ResNet-50 
0.1 
0.001 
0.006 
4.55 </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When making 2D plots in this paper, batch normalization parameters are held constant, i.e., random directions are not applied to batch normalization parameters.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The shattered gradients problem: If resnets are the answer, then what is the question? In ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Frean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lennox</forename><surname>Leary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt Wan-Duo</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brian</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcwilliams</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Deep learning without poor local minima. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leslie</forename><forename type="middle">Pack</forename><surname>Kaelbling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.05468</idno>
		<title level="m">Generalization in deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">On large-batch training for deep learning: Generalization gap and sharp minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dheevatsa</forename><surname>Nitish Shirish Keskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jorge</forename><surname>Mudigere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Nocedal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping Tak Peter</forename><surname>Smelyanskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A simple weight decay can improve generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Krogh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">A</forename><surname>Hertz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanzhi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09886</idno>
		<title level="m">Convergence analysis of two-layer neural networks with relu activation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Theory of deep learning ii: Landscape of the empirical risk in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qianli</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomaso</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09833</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zachary C Lipton</surname></persName>
		</author>
		<title level="m">Stuck in a what? adventures in weight space. ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eliana</forename><surname>Lorch</surname></persName>
		</author>
		<title level="m">Visualizing deep network training trajectories with pca. ICML Workshop on Visualization for Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Depth creates no bad local minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haihao</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Kawaguchi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08580</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The loss surface of deep and wide neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quynh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Hein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On the quality of the initial basin in overspecified neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Safran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ohad</forename><surname>Shamir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Exploring loss function topology with cyclical learning rates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Leslie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholay</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Topin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04283</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Theoretical insights into the optimization landscape of over-parameterized shallow neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahdi</forename><surname>Soltanolkotabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adel</forename><surname>Javanmard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.04926</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Exponentially vanishing sub-optimal local minima in multilayer neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hoffer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05777</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Local minima in training of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Swirszcz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><forename type="middle">Marian</forename><surname>Czarnecki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06310</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">An analytical formula of population gradient for two-layered relu network and its applications in convergence and critical point analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Diverse neural network learns true target functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chulhee</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Suvrit</forename><surname>Sra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Jadbabaie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.02444</idno>
		<title level="m">Global optimality conditions for deep neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Understanding deep learning requires rethinking generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chiyuan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moritz</forename><surname>Hardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Recht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
