<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Tome</surname></persName>
							<email>tome@cs.ucl.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Turing Institute</orgName>
								<orgName type="institution" key="instit1">University College London D</orgName>
								<orgName type="institution" key="instit2">The University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Lourdes Agapito University College London</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
							<email>crussell@turing.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">The Turing Institute</orgName>
								<orgName type="institution" key="instit1">University College London D</orgName>
								<orgName type="institution" key="instit2">The University of Edinburgh</orgName>
								<orgName type="institution" key="instit3">Lourdes Agapito University College London</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Lifting from the Deep: Convolutional 3D Pose Estimation from a Single Image</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"> 2D and 3D errors.   </div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Estimating the full 3D pose of a human from a single RGB image is one of the most challenging problems in computer vision. It involves tackling two inherently ambiguous tasks. First, the 2D location of the human joints, or landmarks, must be found in the image, a problem plagued with ambiguities due to the large variations in visual appearance caused by different camera viewpoints, external and self occlusions or changes in clothing, body shape or illumination. Next, lifting the coordinates of the 2D landmarks into 3D from a single image is still an ill-posed problem -the space of possible 3D poses consistent with the 2D landmark locations of a human, is infinite. Finding the correct 3D pose that matches the image requires injecting additional information usually in the form of 3D geometric pose priors and temporal or structural constraints.</p><p>We propose a new joint approach to 2D landmark detection and full 3D pose estimation from a single RGB image that takes advantage of reasoning jointly about the estimation of 2D and 3D landmark locations to improve both tasks. We propose a novel CNN architecture that learns to combine the image appearance based predictions provided by convolutional-pose-machine style 2D landmark detectors <ref type="bibr" target="#b43">[44]</ref>, with the geometric 3D skeletal information encoded in a novel pretrained model of 3D human pose.</p><p>Information captured by the 3D human pose model is embedded in the CNN architecture as an additional layer that lifts 2D landmark coordinates into 3D while imposing that they lie on the space of physically plausible poses. The advantage of integrating the output proposed by the 2D landmark location predictors -based purely on image appearance -with the 3D pose predicted by a probabilistic model, is that the 2D landmark location estimates are improved by guaranteeing that they satisfy the anatomical 3D constraints encapsulated in the human 3D pose model. In this way, both tasks clearly benefit from each other.</p><p>A further advantage of our approach is that the 2D and 3D training data sources may be completely independent. The deep architecture only needs that images are annotated with 2D poses, not 3D poses. The human pose model is trained independently and exclusively from 3D mocap data. This decoupling between 2D and 3D training data presents a huge advantage since we can augment the training sets completely independently. For instance we can take advantage of extra 2D pose annotations without the need for 3D ground truth or extend the 3D training data to further mocap datasets without the need for synchronized 2D images. Our contribution: In this work, we show how to integrate a prelearned 3D human pose model directly within a novel CNN architecture (illustrated in <ref type="figure" target="#fig_0">figure 1</ref>) for joint 2D landmark and 3D human pose estimation. In contrast to preexisting methods, we do not take a pipeline approach that takes 2D landmarks as given. Instead, we show how such a model can be used as part of the CNN architecture itself, and how the architecture can learn to use physically plausible 3D reconstructions in its search for better 2D landmark locations. Our method achieves state-of-the-art results on the Human3.6M dataset both in terms of 2D and 3D errors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We first describe methods that assume that 2D joint locations are provided as input and focus on solving the 3D lifting problem and follow with methods that learn to estimate the 3D pose directly from images.</p><p>3D pose from known 2D joint positions: A large body of work has focused on recovering the 3D pose of people given perfect 2D joint positions as input. Early approaches <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b5">6</ref>] took advantage of anatomical knowledge of the human skeleton or joint angle limits to recover pose from a single image. More recent methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref> have focused on learning a prior statistical model of the human body directly from 3D mocap data.</p><p>Non-rigid structure from motion approaches (NRSfM) also recover 3D articulated motion <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b19">20]</ref> given known 2D correspondences for the joints in every frame of a monocular video. Their huge advantage, as unsupervised methods, is they do not need 3D training data, instead they can learn a linear basis for the 3D poses purely from 2D data. Their main drawback is their need for significant camera movement throughout the sequence to guarantee accurate 3D reconstruction. Recent work on NRSfM applied to human pose estimation has focused on escaping these limitations by the use of a linear model to represent shape variations of the human body. For instance, <ref type="bibr" target="#b9">[10]</ref> defined a generative model based on the assumption that complex shape variations can be decomposed into a mixture of primitive shape variations and achieve competitive results.</p><p>Representing human 3D pose as a linear combination of a sparse set of 3D bases, pretrained using 3D mocap data, has also proved a popular approach for articulated human motion <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b48">49]</ref>, while <ref type="bibr" target="#b48">[49]</ref> propose a convex relaxation to jointly estimate the coefficients of the sparse representation and the camera viewpoint <ref type="bibr" target="#b27">[28]</ref> and <ref type="bibr" target="#b42">[43]</ref> enforce limb length constraints. Although these approaches can reconstruct 3D pose from a single image, their best results come from imposing temporal smoothness on the reconstructions of a video sequence.</p><p>Recently, Zhao et al. <ref type="bibr" target="#b46">[47]</ref> achieved state-of-the-art results by training a simple neural network to recover 3D pose from known 2D joint positions. Although the results on perfect 2D input data are impressive, the inaccuracies in 2D joint estimation are not modeled and the performance of this approach combined with joint detectors is unknown. 3D pose from images: Most approaches to 3D pose inference directly from images fall into one of two categories: (i) models that learn to regress the 3D pose directly from image features and (ii) pipeline approaches where the 2D pose is first estimated, typically using discriminatively trained part models or joint predictors, and then lifted into 3D. While regression based methods suffer from the need to annotate all images with ground truth 3D poses -a technically complex and elaborate process -for pipeline approaches the challenge is how to account for uncertainty in the measurements. Crucial to both types of approaches is the question of how to incorporate the 3D dependencies between the different body joints or to leverage other useful 3D geometric information in the inference process.</p><p>Many earlier works on human pose estimation from a single image relied on discriminatively trained models to learn a direct mapping from image features such as silhouettes, HOG or SIFT, to 3D human poses without passing through 2D landmark estimation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>Recent direct approaches make use of deep learning <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref>. Regression-based approaches train an end-toend network to predict 3D joint locations directly from the image <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b47">48]</ref>. Li et al. <ref type="bibr" target="#b21">[22]</ref> incorporate model joint dependencies in the CNN via a max-margin formalism, others <ref type="bibr" target="#b47">[48]</ref> impose kinematic constraints by embedding a dif-ferentiable kinematic model into the deep learning architecture. Tekin et al. <ref type="bibr" target="#b34">[35]</ref> propose a deep regression architecture for structured prediction that combines traditional CNNs for supervised learning with an auto-encoder that implicitly encodes 3D dependencies between body parts.</p><p>As CNNs have become more prevalent, 2D joint estimation <ref type="bibr" target="#b43">[44]</ref> has become increasingly reliable and many recent works have looked to exploit this using a pipeline approach. Papers such as <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b25">26]</ref> first estimate 2D landmarks and later 3D spatial relationships are imposed between them using structured learning or graphical models.</p><p>Simo-Serra et al. <ref type="bibr" target="#b32">[33]</ref> were one of the first to propose an approach that naturally copes with the noisy detections inherent to off-the-shelf body part detectors by modeling their uncertainty and propagating it through 3D shape space while satisfying geometric and kinematic 3D constraints. The work <ref type="bibr" target="#b30">[31]</ref> also estimates the location of 2D joints before predicting 3D pose using appearance and the probable 3D pose of discovered parts using a non-parametric model. Another recent example is Bogo et al. <ref type="bibr" target="#b6">[7]</ref>, who fit a detailed statistical 3D body model <ref type="bibr" target="#b22">[23]</ref> to 2D joint proposals.</p><p>Zhou et al. <ref type="bibr" target="#b49">[50]</ref> tackles the problem of 3D pose estimation for a monocular image sequence integrating 2D, 3D and temporal information to account for uncertainties in the model and the measurements. Similar to our proposed approach, Zhou et al.'s method <ref type="bibr" target="#b49">[50]</ref> does not need synchronized 2D-3D training data, i.e. it only needs 2D pose annotations to train the CNN joint regressor and a separate 3D mocap dataset to learn the 3D sparse basis. Unlike our approach, it relies on temporal smoothness for its best performance, and performs poorly on a single image.</p><p>Finally, Wu et al. <ref type="bibr" target="#b44">[45]</ref>'s 3D Interpreter Network, a recent approach to estimate the skeletal structure of common objects (chairs, sofas, ...) bears similarities with our method. Although our approaches share common ground in the decoupling of 3D and 2D training data and the use of projection from 3D to improve 2D predictions the network architectures are very different and, unlike us, they do not carry out a quantitative evaluation on 3D human pose estimation. <ref type="figure" target="#fig_0">Figure 1</ref> illustrates the main contribution of our approach, a new multi-stage CNN architecture that can be trained end-to-end to estimate jointly 2D and 3D joint locations. Crucially it includes a novel layer, based on a probabilistic 3D model of human pose, responsible for lifting 2D poses into 3D and propagating 3D information about the skeletal structure to the 2D convolutional layers. In this way, the prediction of 2D pose benefits from the 3D information encoded. Section 4 describes the new probabilistic 3D model of human pose, trained on a dataset of 3D mocap data. Section 5 describes all the new components and layers of the CNN architecture. Finally, Section 6 describes experimental evaluation on the Human3.6M dataset where we obtain state-of-the-art results. In addition we show qualitative results on images from the MPII and Leeds datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Probabilistic 3D Model of Human Pose</head><p>One fundamental challenge in creating models of human poses lies in the lack of access to 3D data of sufficient variety to characterize the space of human poses. To compensate for this lack of data we identify and eliminate confounding factors such as rotation in the ground plane, limb length, and left-right symmetry that lead to conceptually similar poses being unrecognized in the training data.</p><p>Simple preprocessing eliminates some factors. Size variance is addressed by normalizing the data such that the sum of squared limb lengths on the human skeleton is one; while left-right symmetry is exploited by flipping each pose in the x-axis and re-annotating left as right and vice-versa.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Aligning 3D Human Poses in the Training Set</head><p>Allowing for rotational invariance in the ground-plane is more challenging and requires integration with the data model. We seek the optimal rotations for each pose such that after rotating the poses they are closely approximated by a low-rank compact Gaussian distribution.</p><p>We formulate this as a problem of optimization over a set of variables. Given a set of N training 3D poses, each represented as a (3 × L) matrix P i of 3D landmark locations, where i ∈ {1, 2, .., N } and L is the number of human joints/landmarks; we seek global estimates of an average 3D pose µ, a set of J orthonormal basis matrices 1 e and noise variance σ, alongside per sample rotations R i and basis coefficients a i to minimize the following estimate arg min</p><formula xml:id="formula_0">R,µ,a,e,σ N i=1 ||P i − R i (µ + a i · e) || 2 2 (1) + J j=1 (a i,j · σ j ) 2 + ln J j=1 σ 2 j</formula><p>Where a i · e = j a i,j e j is the tensor analog of a multiplication between a vector and a matrix, and || · || 2 2 is the squared Frobenius norm of the matrix. Here the y-axis is assumed to point up, and the rotation matrices R i considered are ground plane rotations. With the large number of 3D pose samples considered (of the order of 1 million when training on the Human3.6M dataset <ref type="bibr" target="#b14">[15]</ref>), and the complex inter-dependencies between samples for e and σ, the memory requirements mean that it is not possible to solve directly as a joint optimization over all variables using a nonlinear solver such as Ceres. Instead, we carefully initialize and alternate between performing closed-form PPCA <ref type="bibr" target="#b37">[38]</ref> to update µ, a, e, σ; and updating R i using Ceres <ref type="bibr" target="#b1">[2]</ref> to minimize the above error. As we do this, we steadily increase the size of the basis from 1 through to its target size J. This stops apparent deformations that could be resolved through rotations from becoming locked into the basis at an early stage, and empirically leads to lower cost solutions.</p><formula xml:id="formula_1">d) c) b) a) g) h) f) e)</formula><p>To initialize we use a variant of the Tomasi-Kanade <ref type="bibr" target="#b38">[39]</ref> algorithm to estimate the mean 3D pose µ. As the y component is not altered by planar rotations, we take as our estimate of the y component of µ, the mean of each point in the y direction. For the x and z components, we interleave the x and z components of each sample and concatenate them into a large 2 N × L matrix M, and find the rank two approximation of this such that M ≈ A · B. We then calculateÂ by replacing each adjacent pair of rows of A with the closest orthonormal matrix of rank two, and takeÂ † M as our estimate <ref type="bibr" target="#b1">2</ref> of the x and z components of µ. The end result of this optimization is a compact lowrank approximation of the data in which all reconstructed poses appear to have the same orientation (see <ref type="figure" target="#fig_1">Figure 2)</ref>. In the next section we extend the model to be described as a multi-modal distribution to better capture the variations in the space of 3D human poses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">A Multi-Modal Model of 3D Human Pose</head><p>Although the learned Gaussian model of section 4.1 can be directly used to estimate the 3D (see <ref type="table" target="#tab_2">Table 1</ref>), inspection of <ref type="figure" target="#fig_1">figure 2</ref> shows that the data is not Gaussian distributed and is better described using a multiple modal distribution. In doing this, we are heavily inspired both by approaches such as <ref type="bibr" target="#b26">[27]</ref> which characterize the space of human poses as a mixture of PCA bases, and by related works such as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b7">8]</ref> that represent poses as an interpolation between exemplars. These approaches are extremely good at modeling tightly distributed poses (e.g. walking) where samples in the testing data are likely to be close to poses seen in training. This is emphatically not the case in much of the Human3.6M dataset, which we use for evaluation. Zooming in on the edges of <ref type="figure" target="#fig_1">Figure 2</ref> reveals many isolated paths where motions occur once and are never revisited again.</p><p>Nonetheless, it is precisely these regions of low-density that we are interested in modeling. As such, we seek a coarse representation of the pose space that says something about the regions of low density but also characterizes the multi-modal nature of the pose space. We represent the data as a mixture of probabilistic PCA models using few clusters, and trained using the EM-algorithm <ref type="bibr" target="#b37">[38]</ref>. When using a small number of clusters, it is important to initialize the algorithm correctly, as accidentally initializing with multiple clusters about a single mode, can lead to poor density estimates. To initialize we make use of a simple heuristic.</p><p>We first subsample the aligned poses (which we refer to as P ), and then compute the Euclidean distance d among pairs. We seek a set of k samples S such that the distance between points and their nearest sample is minimized arg min</p><formula xml:id="formula_2">S p∈P min s∈S d(s, p)<label>(2)</label></formula><p>We find S using greedy selection, holding our previous estimate of S constant, and iteratively selecting the next candidate s such that {s} ∪ S minimizes the above cost. A selection of 3D pose samples found using this procedure can be seen in the rendered poses of <ref type="figure" target="#fig_1">Figure 2</ref>. In practice, we stop proposing candidates when they occur too close to the existing candidates, as shown by samples (a-d), and only choose one candidate from the dominant mode. Given these candidates for cluster centers, we assign each aligned point to a cluster representing its nearest candidate and then run the EM algorithm of <ref type="bibr" target="#b37">[38]</ref>, building a mixture of probabilistic PCA bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">A New Convolutional Architecture for 2D and 3D Pose Inference</head><p>Our 3D pose inference from a single RGB image makes use of a multistage deep convolutional architecture, trained end-to-end, that repeatedly fuses and refines 2D and 3D poses, and a second module which takes the final predicted 2D landmarks and lifts them one last time into 3D space for the final estimate (see <ref type="figure" target="#fig_0">Figure 1)</ref>.</p><p>At its heart, the architecture is a novel refinement of the Convolutional Pose Machine of Wei et al. <ref type="bibr" target="#b43">[44]</ref>, who reasoned exclusively in 2D, and proposed an architecture that  iteratively refined 2D pose estimations of landmarks using a mixture of knowledge of the image and of the estimates of landmark locations of the previous stage. We modify this architecture by generating, at each stage, projected 3D pose belief maps which are fused in a learned manner with the standard maps. From an implementation point of view this is done by introducing two distinct layers, the probabilistic 3D pose layer and the fusion layer (see <ref type="figure" target="#fig_0">Figure 1</ref>). <ref type="figure" target="#fig_2">Figure 3</ref> shows how the 2D uncertainty in the belief maps is reduced at each stage of the architecture and how the accuracy of the 3D poses increases with each stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Architecture of each stage</head><p>The sequential architecture consists of 6 stages. Each stage consists of 4 distinct components (see <ref type="figure" target="#fig_0">Figure 1</ref>):</p><p>Predicting CNN-based belief-maps: we use a set of convolutional and pooling layers, equivalent to those used in the original CPM architecture <ref type="bibr" target="#b43">[44]</ref>, that combine evidence obtained from image learned features with the belief maps obtained from the previous stage (t − 1) to predict an updated set of belief maps for the 2D human joint positions.</p><p>Lifting 2D belief-maps into 3D: the output of the CNNbased belief maps is taken as input to a new layer that uses new pretrained probabilistic 3D human pose model to lift the proposed 2D poses into 3D.</p><p>Projected 2D pose belief maps: The 3D pose estimated by the previous layer is projected back onto the image plane to produce a new set of projected pose belief maps. These maps encapsulate 3D dependencies between the body parts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2D Fusion layer:</head><p>The final layer in each stage (described in section 5.5) learns the weights to fuse the two sets of belief maps into a single estimate passed to the next stage.</p><p>Final lifting: The belief maps produced as the output of the final stage (t = 6) are then lifted into 3D to give the final estimate for the pose (see <ref type="figure" target="#fig_0">Figure 1</ref>) using our algorithm to lift 2D poses into 3D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Predicting CNN-based belief-maps</head><p>Convolutional Pose Machines <ref type="bibr" target="#b43">[44]</ref> can be understood as an updating of the earlier work of Ramakrishna et al. <ref type="bibr" target="#b28">[29]</ref> to use a deep convolutional architecture. In both approaches, at each stage t and for each landmark p, the algorithm returns dense per pixel belief maps b p t <ref type="bibr">[u, v]</ref>, which show how confident it is that a joint center or landmark occurs in any given pixel (u, v). For stages t ∈ {2, . . . , T } the belief maps are a function of not just the information contained in the image but also the information computed by the previous stage.</p><p>In the case of convolutional pose machines, and in our work which uses the same architecture, a summary of the convolution widths and architecture design is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, with more details of training given in <ref type="bibr" target="#b43">[44]</ref>.</p><p>Both <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b43">44]</ref> predict the locations of different landmarks to those captured in the Human3.6M dataset. As such the input and output layers in each stage of the architecture are replaced with a larger set to account for the greater number of landmarks. The new architecture is then initialized by using the weights with those found in CPM's model for all preexisting layers, with the new layers randomly initialized.</p><p>After retraining, CPMs return per-pixel estimates of landmark locations, while the techniques for 3D estimation (described in the next section) make use of 2D locations. To transform these belief maps into locations, we select the most confident pixel as the location of each landmark</p><formula xml:id="formula_3">Y p = arg max (u,v) b p [u, v]<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Lifting 2D belief-maps into 3D</head><p>We follow <ref type="bibr" target="#b49">[50]</ref> in assuming a weak perspective model, and first describe the simplest case of estimating the 3D pose of a single frame using a unimodal Gaussian 3D pose model as described in section 4. This model is composed of a mean shape µ, a set of basis matrices e and variances σ 2 , and from this we can compute the most probable sample from the model that could give rise to a projected image.</p><p>arg min</p><formula xml:id="formula_4">R,a ||Y − sΠER(µ + a · e)|| 2 2 + ||σ · a|| 2 2<label>(4)</label></formula><p>Where Π is the orthographic projection matrix, E a known external camera calibration matrix, and s the estimated perframe scale. Although, given R this problem is convex in a and s together <ref type="bibr" target="#b2">3</ref> , for an unknown rotation matrix R the problem is extremely non-convex -even if a is known -and prone to sticking in local minima using gradient descent. Local optima often lie far apart in pose space and a poor optima leads to a significantly worse 3D reconstructions.</p><p>We take advantage of the matrix R's restricted form that allows it to be parameterized in terms of a single angle θ. Rather than attempting to solve this optimization problem <ref type="bibr" target="#b2">3</ref> To see this consider the trivial reparameterization where we solve for sµ + b · e and then let a = b/s. using local methods we quantize over the space of possible rotations, and for each choice of rotation, we hold this fixed and solve for s and a, before picking the minimum cost solution of any choice of R. With fixed choices of rotation the terms ΠERµ and ΠERe can be precomputed and finding the optimal a becomes a simple linear least square problem.</p><p>This process is highly efficient and by oversampling the rotations and exhaustively checking in 10, 000 locations we can guarantee that a solution extremely close to the global optima is found. In practice, using 20 samples and refining the rotations and basis coefficients of the best found solution using a non-linear least squares solver obtains the same reconstruction, and we make use of the faster option of checking 80 locations and using the best found solution as our 3D estimate. This puts us close to the global optima and has the same average accuracy as finding the global optima. Moreover, it allows us to upgrade from sparse landmark locations to 3D using a single Gaussian at around 3,000 frames a second using python code on a standard laptop.</p><p>To handle models consisting of a mixture of Gaussians, we follow <ref type="bibr" target="#b26">[27]</ref> and simply solve for each Gaussian independently and select the most probably solution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Projecting 3D poses onto 2D belief maps</head><p>The projected pose model is interleaved throughout the architecture (see <ref type="figure" target="#fig_0">Figure 1)</ref>. The goal is to correct the beliefs regarding landmark locations at each stage, by fusing extra information about 3D physical plausibility. Given the solution R, s, and a from the previous component, we estimate a physically plausible projected 3D pose aŝ   estimation with previous works, where we take care to evaluate using the appropriate protocol. Protocol #1, the most standard evaluation protocol on Human3.6M, was followed by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b30">31]</ref>. The training set consists of 5 subjects (S1, S5, S6, S7, S8), while the test set includes 2 subjects (S9, S11). The original frame rate of 50 FPS is down-sampled to 10 FPS and the evaluation is on sequences coming from all 4 cameras and all trials. The reported error metric is the 3D error i.e. the Euclidean distance from the estimated 3D joints to the ground truth, averaged over all 17 joints of the Human3.6M skeletal model. <ref type="table" target="#tab_2">Table 1</ref> shows a comparison between our approach and competing approaches using Protocol #1. Our baseline method using a single unimodal probabilistic PCA model outperforms almost every method in most action types, with the exception of Sanzari et al. <ref type="bibr" target="#b30">[31]</ref>, which it still outperforms on average across the entire dataset. The mixture model improves on this again, offering a 4.76mm improvement over Sanzari et al., our closest competitor.</p><formula xml:id="formula_5">Y = sΠER(µ + a · e)<label>(5)</label></formula><p>Protocol #2, followed by <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b29">30]</ref>, selects 6 subjects (S1, S5, S6, S7, S8 and S9) for training and subject S11 for testing. The original video is down-sampled to every 64th frame and evaluation is performed on sequences from all 4 cameras and all trials. The error metric reported in this case is the 3D pose error equivalent to the per-joint 3D error up to a similarity transformation (i.e. each estimated 3D pose is aligned with the ground truth pose, on a per-frame basis, using Procrustes analysis). The error is averaged over 14 joints. <ref type="table" target="#tab_3">Table 2</ref> shows a comparison between our approach and other approaches that use Protocol #2. Although, our model was trained using only the 5 subjects used for training in Protocol #1 (one fewer subject), it still outperforms the other methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b45">46]</ref>. Protocol #3, followed by <ref type="bibr" target="#b6">[7]</ref>, selects the same subjects for training and testing as Protocol #1. However, evaluation is only on sequences captured from the frontal camera ("cam 3") from trial 1 and the original video is not subsampled. The error metric used in this case is the 3D pose error as described in Protocol #2. The error is averaged over a subset of 14 joints. <ref type="table" target="#tab_3">Table 2</ref> shows a comparison between our approach and <ref type="bibr" target="#b6">[7]</ref>. Our method outperforms Bogo et al. <ref type="bibr" target="#b6">[7]</ref> by almost 3mm on average, even though Bogo et al. exploits a high-quality detailed statistical 3D body model <ref type="bibr" target="#b22">[23]</ref> trained on thousands of 3D body scans, that captures both the variation of human body shape and its deformation through pose.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPII and Leeds datasets:</head><p>The proposed approach trained exclusively on the Human3.6M dataset can be used to identify 2D and 3D landmarks of images contained in different datasets. <ref type="figure">Figure 4</ref> shows some qualitative results on the MPII dataset <ref type="bibr" target="#b4">[5]</ref> and on the Leeds dataset <ref type="bibr" target="#b17">[18]</ref>, including failure cases. Notice how the probabilistic 3D pose model generates anatomically plausible poses even though the 2D landmark estimations are not all correct. However, as shown in bottom row, even small errors in 2D pose can lead to drastically different 3D poses. These inaccuracies could be mitigated without further 3D data by annotating additional RGB images for training from different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We have presented a novel approach to human 3D pose estimation from a single image that outperforms previous solutions. We approach this as a problem of iterative refinement in which 3D proposals help refine and improve upon the 2D estimates. Our approach shows the importance of thinking in 3D even for 2D pose estimation within a single image, with our method demonstrating better 2D accuracy than <ref type="bibr" target="#b43">[44]</ref>, the 2D approach it is based upon. Our novel approach for upgrading from 2D to 3D is extremely efficient. When using 3 models, as in <ref type="table" target="#tab_2">Tables 1 and 2</ref>, the upgrade for each stage in CPU-based Python code runs at approximately 1,000 frames a second, while a GPU-based realtime approach for Convolutional Pose Machines has been announced. Integrating these systems to provide a reliable real-time 3D pose estimator is a natural future direction, as is integrating this work with a simpler 2D approach for realtime pose estimation on lower power devices.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The multistage deep architecture for 2D/3D human pose estimation. Each stage produces as output a set of belief maps for the location of the 2D landmarks (one per landmark). The belief maps from each stage, as well as the image, are used as input to the next stage. Internally, each stage learns to combine: (a) belief maps provided by convolutional 2D joint predictors, with (b) projected pose belief maps, proposed by the probabilistic 3D pose model. The 3D pose layer is responsible for lifting 2D landmark coordinates into 3D and projecting them onto the space of valid 3D poses. These two belief maps are then fused into a single set of output proposals for the 2D landmark locations per stage. The accuracy of the 2D and 3D landmark locations increases progressively through the stages. The loss used at each stage requires only 2D pose annotations, not 3D. The overall architecture is fully differentiable -including the new projected-pose belief maps and 2D-fusion layers -and can be trained end-to-end using back-propagation. [Best viewed in color.]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualization of the 3D training data after alignment (see section 4.1) using 2D PCA. Notice how all poses have the same orientation. Standing-up poses a), b), c) and d) are all close to each other and far from sitting-down poses f) and h) which form another clear cluster.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results returned by different stages of the architecture. Top Left: Evolution of the 2D skeleton after projecting the 3D points back into the 2D space; Bottom Left: Evolution of the beliefs for the landmark Left hand through the stages. Right: 3D skeleton with the relative mean error per landmark in millimeters. Even with incorrect landmark locations, the model returns a physically plausible solution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 : 6M Figure 5 :</head><label>46M5</label><figDesc>Figure 4: Left: Results from the Human3.6M dataset. The identified 2D landmark positions and 3D skeleton is shown for each pose taken from different actions: Walking, Phoning, Greeting, Discussion, Sitting Down. Right: Results on images from the MPII [5] (columns 1 to 3) and Leeds [18] datasets (last column). The model was not trained on images as diverse as those contained in these datasets, however it often retrieves correct 2D and 3D joint positions. The last row shows example cases where the method fails either in the identification of 2D or 3D landmarks. 2D Pose refinement in Human3.6M</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>A comparison of the 3D pose estimation results of our approach on the Human3.6M dataset against competitors that follow Protocol #1 for evaluation (3D errors are given in mm). We substantially outperform all other methods in terms of average error showing a 4.7mm average improvement over our closest competitor. Note that some approaches [37, 50] use video as input instead of a single frame. which is then embedded in a belief map aŝ</figDesc><table>b 

p 

i,j = </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Further evaluation on the Human3.6M dataset. Top two tables compare our 3D pose estimation errors against competitors on Protocols #2 or #3. Bottom table compares our 2D pose esti- mation error against competitors. Our approach, which lifts the 2D landmark predictions into a plausible 3D model and then projects them back into the image, substantially reduces the error. Note that [50] use video as input and knowledge of the action label.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">When we say e is a set of orthonormal basis matrices we mean that each matrix, if unwrapped into a vector, is of unit norm and orthogonal to all other unwrapped matrices.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A † being the pseudo-inverse of A.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">if(i, j) =Ŷ p</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>0 otherwise. <ref type="bibr" target="#b5">(6)</ref> and then convolved using Gaussian filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">2D Fusion of belief maps</head><p>The 2D belief maps predicted by the probabilistic 3D pose model are fused with the CNN-based belief maps b p according to the following equation</p><p>where w t ∈ [0, 1] is a weight trained as part of the end-toend learning. This set of fused belief maps f t is then passed to the next stage and used as an input to guide the 2D reestimation of joint locations, instead of the belief maps b t used by convolutional pose machines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">The Objective and Training</head><p>Following <ref type="bibr" target="#b43">[44]</ref>, the objective or cost function c t minimized at each stage is the the squared distance between the generated fusion maps of the layer f p t , and ground-truth belief maps b p * generated by Gaussian blurring the sparse ground-truth locations of each landmark p</p><p>For end-to-end training the total loss is the sum over all layers t≤6 c t . The novel layers were implemented as an extension of the published code of Convolutional Pose Machines <ref type="bibr" target="#b43">[44]</ref> inside the Caffe framework <ref type="bibr" target="#b16">[17]</ref> as Python layers, with weights updated using Stochastic Gradient Descent with momentum. Details of the novel gradient updates used lifting estimates through 3d pose space are given in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental evaluation</head><p>Human3.6M dataset: The model was trained and tested on the Human3.6M dataset consisting of 3.6 million accurate 3D human poses <ref type="bibr" target="#b14">[15]</ref>. This is a video and mocap dataset of 5 female and 6 male subjects, captured from 4 different viewpoints, that show them performing typical activities (talking on the phone, walking, greeting, eating, etc.). 2D Evaluation: <ref type="figure">Figure 5</ref> shows how the 2D predictions are improved by the projected pose model, reducing the overall mean error per landmark. The 2D error reduction using our full approach over the estimates of <ref type="bibr" target="#b43">[44]</ref> is comparable in magnitude to the improvement due to the change of architecture moving from the work Zhou et al. <ref type="bibr" target="#b49">[50]</ref> to the state-of-the-art 2d architecture <ref type="bibr" target="#b43">[44]</ref> (i.e. a reduction of 0.59 pixels vs. 0.81 pixels). See <ref type="table">Table 2</ref> for details. 3D Evaluation: Several evaluation protocols have been followed by different authors to measure the performance of their 3D pose estimation methods on the Human3.6M dataset. <ref type="table">Tables 1 and 2</ref> show comparisons of the 3D pose</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Recovering 3d human pose from monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="44" to="58" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mierle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename><surname>Ceres</surname></persName>
		</author>
		<ptr target="http://ceres-solver.org.4" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1446" to="1455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Trajectory space: A dual representation for nonrigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Estimating anthropometry and pose from a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrón</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Keep it smpl: Automatic estimation of 3d human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="561" to="578" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Recovering non-rigid 3d shape from image streams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Articulated pose estimation by a graphical model with image dependent pairwise relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1736" to="1744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Complex non-rigid 3d shape recovery using a procrustean normal distribution mixture model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="226" to="246" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Gaussian process latent variable models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Ek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Computer Science</title>
		<editor>A. Popescu-Belis, S. Renals, and H. Bourlard</editor>
		<imprint>
			<biblScope unit="volume">4892</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="132" to="143" />
			<date type="published" when="2007" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Inferring 3d body pose from silhouettes using activity manifold learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pose locality constrained representation for 3d human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="174" to="188" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Computing smooth time trajectories for camera and deformable shape in structure from motion with occlusion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gotardo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Human3. 6m: Large scale datasets and predictive methods for 3d human sensing in natural environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1325" to="1339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.7302</idno>
		<title level="m">Learning human pose estimation features with convolutional networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<idno type="doi">10.5244/C.24.12</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Determination of 3d human body postures from a single view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
	<note>Computer Vision, Graphics, and Image Processing</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Procrustean normal distribution for non-rigid structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Oh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">3d human pose estimation from monocular images with deep convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="332" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Maximum-margin structured learning with deep networks for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Chan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2848" to="2856" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Smpl: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">248</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Recovering 3d human body configurations using shape contexts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PAMI</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">View independent human body pose estimation from a single perspective image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2004 IEEE Computer Society Conference on</title>
		<meeting>the 2004 IEEE Computer Society Conference on</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">16</biblScope>
		</imprint>
	</monogr>
	<note>Computer Vision and Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Flowing convnets for human pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1913" to="1921" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning a manifold as an atlas</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Pitelis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reconstructing 3d human pose from 2d image landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="573" to="586" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3108" to="3116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="566" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Shared kernel information embedding for discriminative inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fleet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Single image 3d human pose estimation from noisy observations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramisa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alenyà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Torras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2673" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in a single uncalibrated image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="677" to="684" />
		</imprint>
	</monogr>
	<note>Proceedings. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Structured prediction of 3d human pose with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Katircioglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Márquez-Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.05708</idno>
		<title level="m">Fusing 2d uncertainty and 3d cues for monocular body pose estimation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Predicting people&apos;s 3d poses from short sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.08200</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Probabilistic principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Tipping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society, Series B</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="611" to="622" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Shape and motion from image streams under orthography: a factorization method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="137" to="154" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Representing data by a mixture of activated simplices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Flynn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.4102</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Robust estimation of human poses from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.00134</idno>
		<title level="m">Convolutional pose machines</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="365" to="382" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A dual-source approach for 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Yasin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kruger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">A simple, fast and highly-accurate algorithm to recover 3d shape from 2d landmarks on a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09058</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05317</idno>
		<title level="m">Deep kinematic pose regression</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.04309</idno>
		<title level="m">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.09439</idno>
		<title level="m">Sparseness meets deepness: 3d human pose estimation from monocular video</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
