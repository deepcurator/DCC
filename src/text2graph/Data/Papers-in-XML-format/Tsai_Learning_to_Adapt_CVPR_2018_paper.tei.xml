<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Adapt Structured Output Space for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Hsuan</forename><surname>Tsai</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Chih</forename><surname>Hung</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><surname>Schulter</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Hsuan</forename><surname>Yang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Merced</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">NEC Laboratories America</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>San Diego</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Adapt Structured Output Space for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Convolutional neural network-based approaches for semantic segmentation rely on supervision with pixel-level ground truth, but may not generalize well to unseen image domains. As the labeling process is tedious and labor intensive, developing algorithms that can adapt source ground truth labels to the target domain is of great interest. In this paper, we propose an adversarial learning method for domain adaptation in the context of semantic segmentation. Considering semantic segmentations as structured outputs that contain spatial similarities between the source and target domains, we adopt adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Extensive experiments and ablation study are conducted under various domain adaptation settings, including synthetic-to-real and cross-city scenarios. We show that the proposed method performs favorably against the stateof-the-art methods in terms of accuracy and visual quality.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Semantic segmentation aims to assign each pixel a semantic label, e.g., person, car, road or tree, in an image. Recently, methods based on convolutional neural networks (CNNs) have achieved significant progress in semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b40">41]</ref> with applications for autonomous driving <ref type="bibr" target="#b8">[9]</ref> and image editing <ref type="bibr" target="#b34">[35]</ref>. The crux of CNN-based approaches is to annotate a large number of images that cover possible scene variations. However, this trained model may not generalize well to unseen images, especially when there is a domain gap between the training (source) and test (target) images. For instance, the distribution of appearance for objects and scenes may vary in different cities, and even weather and lighting conditions can change significantly in the same city. In such cases, rely- * Both authors contribute equally to this work. ing only on the supervised model that requires re-annotating per-pixel ground truths in different scenarios would entail prohibitively high labor cost.</p><p>To address this issue, knowledge transfer or domain adaptation techniques have been proposed to close the gap between source and target domains, where annotations are not available in the target domain. For image classification, one effective approach is to align features across two domains <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> such that the adapted features can generalize to both domains. Similar efforts have been made for semantic segmentation via adversarial learning in the feature space <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13]</ref>. However, different from the image classification task, feature adaptation for semantic segmentation may suffer from the complexity of high-dimensional features that needs to encode diverse visual cues, including appearance, shape and context. This motivates us to develop an effective method for adapting pixel-level prediction tasks rather than using feature adaptation. In semantic segmentation, we note that the output space contains rich information, both spatially and locally. For instance, even if images from two domains are very different in appearance, their segmentation outputs share a significant amount of similarities, e.g., spatial layout and local context (see <ref type="bibr">Figure 1)</ref>. Based on this observation, we address the pixellevel domain adaptation problem in the output (segmentation) space.</p><p>In this paper, we propose an end-to-end CNN-based domain adaptation algorithm for semantic segmentation. Our formulation is based on adversarial learning in the output space, where the intuition is to directly make the predicted label distributions close to each other across source and target domains. Based on the generative adversarial network (GAN) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b21">22]</ref>, the proposed model consists of two parts: 1) a segmentation model to predict output results, and 2) a discriminator to distinguish whether the input is from the source or target segmentation output. With an adversarial loss, the proposed segmentation model aims to fool the discriminator, with the goal of generating similar distributions in the output space for either source or target images.</p><p>The proposed method also adapts features as the errors are back-propagated to the feature level from the output labels. However, one concern is that lower-level features may not be adapted well as they are far away from the high-level output labels. To address this issue, we develop a multilevel strategy by incorporating adversarial learning at different feature levels of the segmentation model. For instance, we can use both conv5 and conv4 features to predict segmentation results in the output space. Then two discriminators can be connected to each of the predicted output for multi-level adversarial learning. We perform one-stage endto-end training for the segmentation model and discriminators jointly, without using any prior knowledge of the data in the target domain. In the testing phase, we can simply discard discriminators and use the adapted segmentation model on target images, with no extra computational requirements.</p><p>Due to the high labor cost of annotating segmentation ground truth, there has been great interest in large-scale synthetic datasets with annotations, e.g., GTA5 <ref type="bibr" target="#b30">[31]</ref> and SYN-THIA <ref type="bibr" target="#b31">[32]</ref>. As a result, one critical setting is to adapt the model trained on synthetic data to real-world datasets, such as Cityscapes <ref type="bibr" target="#b3">[4]</ref>. We follow this setting and conduct extensive experiments to validate the proposed domain adaptation method. First, we use a strong baseline model that is able to generalize to different domains. We note that a strong baseline facilitates real-world applications and can evaluate the limitation of the proposed adaptation approach. Based on this baseline model, we show comparisons using adversarial adaptation in the feature and output spaces. Furthermore, we show that the multi-level adversarial learning improves the results over single-level adaptation. In addition to the synthetic-to-real setting, we show experimental results on the Cross-City dataset <ref type="bibr" target="#b2">[3]</ref>, where annotations are provided in one city (source), while testing the model on another unseen city (target). Overall, our method performs favorably against state-of-the-art algorithms on numerous benchmark datasets under different settings.</p><p>The contributions of this work are as follows. First, we propose a domain adaptation method for pixel-level semantic segmentation via adversarial learning. Second, we demonstrate that adaptation in the output (segmentation) space can effectively align scene layout and local context between source and target images. Third, a multi-level adversarial learning scheme is developed to adapt features at different levels of the segmentation model, which leads to improved performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic Segmentation. State-of-the-art semantic segmentation methods are mainly based on the recent advances of deep neural networks. As proposed by Long et al. <ref type="bibr" target="#b23">[24]</ref>, one can transform a classification CNN (e.g., AlexNet <ref type="bibr" target="#b18">[19]</ref>, VGG <ref type="bibr" target="#b32">[33]</ref>, or ResNet <ref type="bibr" target="#b10">[11]</ref>) to a fully-convolutional network (FCN) for semantic segmentation. Numerous methods have since been developed to improve this model by utilizing context information <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b39">40]</ref> or enlarging receptive fields <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>. To train these advanced networks, a substantial amount of dense pixel annotations must be collected in order to match the model capacity of deep CNNs. As a result, weakly and semi-supervised approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29]</ref> are proposed in recent years to reduce the heavy labeling cost of collecting segmentation ground truths. However, in most real-world applications, it is difficult to obtain weak annotations and the trained model may not generalize well to unseen image domains.</p><p>Another approach to tackle the annotation problem is to construct synthetic datasets based on rendering, e.g., GTA5 <ref type="bibr" target="#b30">[31]</ref> and SYNTHIA <ref type="bibr" target="#b31">[32]</ref>. While the data collection is less costly since the pixel-level annotation can be done with a partially automated process, these datasets are usually used in conjunction with real-world datasets for joint learning to improve the performance. However, when training solely on the synthetic dataset, the model does not generalize well to real-world data, mainly due to the large domain shift between synthetic images and real-world images, i.e., appearance differences are still significant with current rendering techniques. Although synthesizing more realistic images can decrease the domain shift, it is necessary to use domain adaptation to narrow the performance gap. Domain Adaptation. Domain adaptation methods for image classification have been developed to address the domain-shift problem between the source and target domains. Numerous methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> are developed based on CNN classifiers due to performance gain. The main insight behind these approaches is to tackle the problem by aligning the feature distribution between source and target images. Ganin et al. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref> propose the DomainAdversarial Neural Network (DANN) to transfer the feature distribution. A number of variants have since been proposed with different loss functions <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> or classifiers <ref type="bibr" target="#b25">[26]</ref>. Recently, the PixelDA method <ref type="bibr" target="#b0">[1]</ref> addresses domain adaptation for image classification by transferring the source im- <ref type="figure">Figure 2</ref>. Algorithmic overview. Given images with the size H by W in source and target domains, we pass them through the segmentation network to obtain output predictions. For source predictions with C categories, a segmentation loss is computed based on the source ground truth. To make target predictions closer to the source ones, we utilize a discriminator to distinguish whether the input is from the source or target domain. Then an adversarial loss is calculated on the target prediction and is back-propagated to the segmentation network. We call this process as one adaptation module, and we illustrate our proposed multi-level adversarial learning by adopting two adaptation modules at two different levels here.</p><p>ages to target domain, thereby obtaining a simulated training set for target images.</p><p>We note that domain adaptation for pixel-level prediction tasks have not been explored widely. Hoffman et al. <ref type="bibr" target="#b12">[13]</ref> introduce the task of domain adaptation on semantic segmentation by applying adversarial learning (i.e., DANN) in a fully-convolutional way on feature representations and additional category constraints similar to the constrained CNN <ref type="bibr" target="#b28">[29]</ref>. Other methods focus on adapting synthetic-toreal or cross-city images by adopting class-wise adversarial learning <ref type="bibr" target="#b2">[3]</ref> or label transfer <ref type="bibr" target="#b2">[3]</ref>. Similar to the PixelDA method <ref type="bibr" target="#b0">[1]</ref>, one concurrent work, CyCADA <ref type="bibr" target="#b11">[12]</ref> uses the CycleGAN <ref type="bibr" target="#b41">[42]</ref> and transfers source domain images to the target domain with pixel alignment, thus generating extra training data combined with feature space adversarial learning <ref type="bibr" target="#b12">[13]</ref>.</p><p>Although feature space adaptation has been successfully applied to image classification, pixel-level tasks such as semantic segmentation remains challenging based on feature adaptation-based approaches. In this paper, we use the property that pixel-level predictions are structured outputs that contain information spatially and locally, to propose an efficient domain adaptation algorithm through adversarial learning in the output space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Algorithmic Overview</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of the Proposed Model</head><p>Our domain adaptation algorithm consists of two modules: a segmentation network G and the discriminator D i , where i indicates the level of a discriminator in the multilevel adversarial learning. Two sets of images ∈ R H×W ×3 from source and target domains are denoted as {I S } and {I T }. We first forward the source image I s (with annotations) to the segmentation network for optimizing G. Then we predict the segmentation softmax output P t for the target image I t (without annotations). Since our goal is to make segmentation predictions P of source and target images (i.e., P s and P t ) close to each other, we use these two predictions as the input to the discriminator D i to distinguish whether the input is from the source or target domain. With an adversarial loss on the target prediction, the network propagates gradients from D i to G, which would encourage G to generate similar segmentation distributions in the target domain to the source prediction. <ref type="figure">Figure 2</ref> shows the overview of the proposed algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Objective Function for Domain Adaptation</head><p>With the proposed network, we formulate the adaptation task containing two loss functions from both modules:</p><formula xml:id="formula_0">L(I s , I t ) = L seg (I s ) + λ adv L adv (I t ),<label>(1)</label></formula><p>where L seg is the cross-entropy loss using ground truth annotations in the source domain, and L adv is the adversarial loss that adapts predicted segmentations of target images to the distribution of source predictions (see Section 4). In (1), λ adv is the weight used to balance the two losses.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Output Space Adaptation</head><p>Different from image classification based on features <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b24">25]</ref> that describe the global visual information of the image, high-dimensional features learned for semantic segmentation encodes complex representations. As a result, adaptation in the feature space may not be the best choice for semantic segmentation. On the other hand, although segmentation outputs are in the low-dimensional space, they contain rich information, e.g., scene layout and context. Our intuition is that no matter images are from the source or target domain, their segmentations should share strong similarities, spatially and locally. Thus, we utilize this property to adapt low-dimensional softmax outputs of segmentation predictions via an adversarial learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Single-level Adversarial Learning</head><p>Discriminator Training. Before introducing how to adapt the segmentation network via adversarial learning, we first describe the training objective for the discriminator. Given the segmentation softmax output P = G(I) ∈ R H×W ×C , where C is the number of categories, we forward P to a fully-convolutional discriminator D using a cross-entropy loss L d for the two classes (i.e., source and target). The loss can be written as:</p><formula xml:id="formula_1">L d (P ) = − h,w (1 − z) log(D(P ) (h,w,0) )<label>(2)</label></formula><p>+z log(D(P ) (h,w,1) ),</p><p>where z = 0 if the sample is drawn from the target domain, and z = 1 for the sample from the source domain. Segmentation Network Training. First, we define the segmentation loss in (1) as the cross-entropy loss for images from the source domain:</p><formula xml:id="formula_2">L seg (I s ) = − h,w c∈C Y (h,w,c) s log(P (h,w,c) s ),<label>(3)</label></formula><p>where Y s is the ground truth annotations for source images and P s = G(I s ) is the segmentation output. Second, for images in the target domain, we forward them to G and obtain the prediction P t = G(I t ). To make the distribution of P t closer to P s , we use an adversarial loss L adv in (1) as:</p><formula xml:id="formula_3">L adv (I t ) = − h,w log(D(P t ) (h,w,1) ).<label>(4)</label></formula><p>This loss is designed to train the segmentation network and fool the discriminator by maximizing the probability of the target prediction being considered as the source prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Multi-level Adversarial Learning</head><p>Although performing adversarial learning in the output space directly adapts predictions, low-level features may not be adapted well as they are far away from the output. Similar to the deep supervision method <ref type="bibr" target="#b19">[20]</ref> that uses auxiliary loss for semantic segmentation <ref type="bibr" target="#b39">[40]</ref>, we incorporate additional adversarial module in the low-level feature space to enhance the adaptation. The training objective for the segmentation network can be extended from (1) as: <ref type="formula">(5)</ref> where i indicates the level used for predicting the segmentation output. We note that, the segmentation output is still predicted in each feature space, before passing through individual discriminators for adversarial learning. Hence, L i seg (I s ) and L i adv (I t ) remain in the same form as in <ref type="formula" target="#formula_2">(3)</ref> and <ref type="formula" target="#formula_3">(4)</ref>, respectively. Based on (5), we optimize the following min-max criterion:</p><formula xml:id="formula_4">L(I s , I t ) = i λ i seg L i seg (I s ) + i λ i adv L i adv (I t ),</formula><formula xml:id="formula_5">max D min G L(I s , I t ).<label>(6)</label></formula><p>The ultimate goal is to minimize the segmentation loss in G for source images, while maximizing the probability of target predictions being considered as source predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Network Architecture and Training</head><p>Discriminator. For the discriminator, we use an architecture similar to <ref type="bibr" target="#b29">[30]</ref> but utilize all fully-convolutional layers to retain the spatial information. The network consists of 5 convolution layers with kernel 4 × 4 and stride of 2, where the channel number is {64, 128, 256, 512, 1}, respectively. Except for the last layer, each convolution layer is followed by a leaky ReLU <ref type="bibr" target="#b26">[27]</ref> parameterized by 0.2. An up-sampling layer is added to the last convolution layer for re-scaling the output to the size of the input. We do not use any batch-normalization layers <ref type="bibr" target="#b15">[16]</ref> as we jointly train the discriminator with the segmentation network using a small batch size. Segmentation Network. It is essential to build upon a good baseline model to achieve high-quality segmentation results <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b39">40]</ref>. We adopt the DeepLab-v2 <ref type="bibr" target="#b1">[2]</ref> framework with ResNet-101 <ref type="bibr" target="#b10">[11]</ref> model pre-trained on ImageNet <ref type="bibr" target="#b5">[6]</ref> as our segmentation baseline network. However, we do not use the multi-scale fusion strategy <ref type="bibr" target="#b1">[2]</ref> due to the memory issue. Similar to the recent work on semantic segmentation <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b37">38]</ref>, we remove the last classification layer and modify the stride of the last two convolution layers from 2 to 1, making the resolution of the output feature maps effectively 1/8 times the input image size. To enlarge the receptive field, we apply dilated convolution layers <ref type="bibr" target="#b37">[38]</ref> in conv4 and conv5 layers with a stride of 2 and 4, respectively. After the last layer, we use the Atrous Spatial Pyramid Pooling (ASPP) <ref type="bibr" target="#b1">[2]</ref> as the final classifier. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image. Based on this architecture, our segmentation model Multi-level Adaptation Model. We construct the abovementioned discriminator and segmentation network as our single-level adaptation model. For the multi-level structure, we extract feature maps from the conv4 layer and add an ASPP module as the auxiliary classifier. Similarly, a discriminator with the same architecture is added for adversarial learning. <ref type="figure">Figure 2</ref> shows the proposed multi-level adaptation model. In this paper, we use two levels due to the balance of its efficiency and accuracy. Network Training. To train the proposed single/multi-level adaptation model, we find that jointly training the segmentation network and discriminators in one stage is effective.</p><p>In each training batch, we first forward the source image I s to optimize the segmentation network for L seg in (3) and generate the output P s . For the target image I t , we obtain the segmentation output P t , and pass it along with P s to the discriminator for optimizing L d in <ref type="bibr" target="#b1">(2)</ref>. In addition, we compute the adversarial loss L adv in (4) for the target prediction P t . For the multi-level training objective in (5), we simply repeat the same procedure for each adaptation module. We implement our network using the PyTorch toolbox on a single Titan X GPU with 12 GB memory. To train the segmentation network, we use the Stochastic Gradient Descent (SGD) optimizer with Nesterov acceleration where the momentum is 0.9 and the weight decay is 10 −4 . The initial learning rate is set as 2.5 × 10 −4 and is decreased using the polynomial decay with power of 0.9 as mentioned in <ref type="bibr" target="#b1">[2]</ref>. For training the discriminator, we use the Adam optimizer <ref type="bibr" target="#b17">[18]</ref> with the learning rate as 10 −4 and the same polynomial decay as the segmentation network. The momentum is set as 0.9 and 0.99. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experimental Results</head><p>In this section, we present experimental results to validate the proposed domain adaptation method for semantic segmentation under different settings. First, we show evaluations of the model trained on synthetic datasets (i.e., GTA5 <ref type="bibr" target="#b30">[31]</ref> and SYNTHIA <ref type="bibr" target="#b31">[32]</ref>) and test the adapted model on real-world images from the Cityscapes <ref type="bibr" target="#b3">[4]</ref> dataset. Extensive experiments including comparisons to the state-ofthe-art methods and ablation study are also conducted, e.g., adaptation in the feature/output spaces and single/multilevel adversarial learning. Second, we carry out experiments on the Cross-City dataset <ref type="bibr" target="#b2">[3]</ref>, where the model is trained on one city and adapted to another city without using annotations. In all the experiments, the IoU metric is used. The code and model are available at https: //github.com/wasidennis/AdaptSegNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">GTA5</head><p>The GTA5 dataset <ref type="bibr" target="#b30">[31]</ref> consists of 24966 images with the resolution of 1914 × 1052 synthesized from the video game based on the city of Los Angeles. The ground truth annotations are compatible with the Cityscapes dataset <ref type="bibr" target="#b3">[4]</ref> that contains 19 categories. Following <ref type="bibr" target="#b12">[13]</ref>, we use the full set of GTA5 and adapt the model to the Cityscapes training set with 2975 images. During testing, we evaluate on the Cityscapes validation set with 500 images.</p><p>Overall Results. We present adaptation results in <ref type="table" target="#tab_0">Table 1</ref> with comparisons to the state-of-the-art domain adaptation methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref>. For these approaches, the baseline model is trained using VGG-based architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b37">38]</ref>. To fairly evaluate our method, we first use the same baseline architecture (VGG-16) and train our model with the proposed single-level adaptation module. <ref type="table" target="#tab_0">Table 1</ref> shows that our method performs favorably against the other algorithms. While these methods all have feature adaptation modules, our results show that adapting the model in the output space achieves better performance. We note that CyCADA <ref type="bibr" target="#b11">[12]</ref> has a pixel adaptation module by transforming source domain images to the target domain and hence obtains additional training samples. Although this strategy achieves a similar performance as ours, one can always apply pixel transformation combined with our output space adaptation to improve the results.</p><p>On the other hand, we argue that utilizing a stronger baseline model is critical for understanding the importance of different adaptation components as well as for enhancing the performance to enable real-world applications. Thus, we use the ResNet-101 based network introduced in Section 5 and train the proposed adaptation model. <ref type="table" target="#tab_0">Table 1</ref> shows the baseline results only trained on source images without adaptation, with comparisons to our adapted models under different settings, including feature adaptation and single/multi-level adversarial learning in the output space. <ref type="figure">Figure 3</ref> presents some example results for adapted segmentation. We note that for small objects such as poles and traffic signs, they are harder to adapt since they easily get merged with background classes.</p><p>In addition, another factor to evaluate the adaptation performance is to measure how much gap is narrowed between the adaptation model and the fully-supervised model. Hence, we train the model using annotated ground truths in the Cityscapes dataset as the oracle results. <ref type="table" target="#tab_1">Table 2</ref> shows the gap under different baseline models. We observe that, although the oracle result does not differ a lot between VGG-16 and ResNet-101 based models, the gap is larger for the VGG one. It suggests us that to narrow the gap, using a deeper model with larger capacity is more practical. Parameter Analysis. During optimizing the segmentation network G, it is essential to balance the weight between segmentation and adversarial losses. We first consider the single-level case in <ref type="bibr" target="#b0">(1)</ref> and conduct experiments to observe the impact of changing λ adv . <ref type="table">Table 3</ref> shows that a smaller λ adv may not facilitate the training process significantly, <ref type="table">Table 3</ref>. Sensitivity analysis of λ adv for feature/output space domain adaptation in the proposed method. We show that output space adaptation can tolerate a wide range of λ adv , while it is sensitive to change λ adv for feature adaptation. while a larger λ adv may propagate incorrect gradients to the network. We empirically choose λ adv as 0.001 in the single-level setting. Feature Level v.s. Output Space Adaptation. In the single-level setting in (1), we compare results by using feature-level or output space adaptation via adversarial learning. For feature-level adaptation, we adopt a similar strategy as used in <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b2">3]</ref> and train our model accordingly. <ref type="table" target="#tab_0">Table 1</ref> shows that the proposed adaptation method in the output space performs better than the one in the feature level.</p><p>In addition, <ref type="table">Table 3</ref> shows that adaptation in the feature space is more sensitive to λ adv , which causes the training process more difficult, while output space adaptation allows for a wider range of λ adv . One reason is that as feature adaptation is performed in the high-dimensional space, the problem for the discriminator becomes easier. Thus, such an adapted model cannot effectively match distributions between source and target domains via adversarial learning.</p><p>Single-level v.s. Multi-level Adversarial Learning. We have shown the merits of adopting adversarial learning in the output space. In addition, we present the results of using multi-level adversarial learning in <ref type="table" target="#tab_0">Table 1</ref>. Here, we utilize an additional adversarial module (see <ref type="figure">Figure 2</ref>) and jointly optimize <ref type="formula">(5)</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">SYNTHIA</head><p>To adapt from the SYNTHIA to Cityscapes datasets, we use the SYNTHIA-RAND-CITYSCAPES <ref type="bibr" target="#b31">[32]</ref> set as the source domain which contains 9400 images compatible with the cityscapes annotated classes. Similar to <ref type="bibr" target="#b2">[3]</ref>, we evaluate images on the Cityscapes validation set with 13 classes. For the weight in (1) and <ref type="formula">(5)</ref>, we use the same ones as in the case of GTA5 dataset. <ref type="table" target="#tab_2">Table 4</ref> shows evaluation results of the proposed algorithm against the state-of-the-art methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b38">39]</ref> that use feature adaptation. Similar to the experiments with the GTA5 dataset, we first utilize the same VGG-based model and train our single-level adaptation model for fair comparisons. The experimental results suggest that adapting the model in the output space performs better. Second, we compare results using different components of the proposed method with the ResNet based model. We show that the multi-level adaptation module improves the results over the baseline, feature space adaptation and single-level adaptation models. In addition, we present comparisons of mean IoU gap between adapted and oracle results in <ref type="table">Table 5</ref>. Our method achieves the smallest gap and is the only one that can minimize the gap below 30%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Cross-City Dataset</head><p>In addition to the synthetic-to-real adaptation for a larger domain gap, we conduct experiment on the Cross-City dataset <ref type="bibr" target="#b2">[3]</ref> with smaller domain gaps between cities. The dataset contains four different cities: Rio, Rome, Tokyo and Taipei, in which each city has 3200 images without annotations and 100 images with pixel-level ground truths for 13 classes. Similar to <ref type="bibr" target="#b2">[3]</ref>, we use the Cityscapes training set as the source domain and adapt it to each target city using 3200 images, while 100 annotated images are used for evaluation. Since a smaller domain gap results in smaller output differences, we use smaller weights for the adversarial loss (i.e., λ i adv = 0.0005) when training our models, while the weights for segmentation remain the same as previous experiments.</p><p>We show our results in <ref type="table" target="#tab_3">Table 6</ref> with comparisons to <ref type="bibr" target="#b2">[3]</ref> and our baseline models under different settings. Again, our final multi-level model achieves consistent improvement for different cities, which demonstrates the advantages of the proposed adaptation method in the output space. Note that the state-of-the-art method <ref type="bibr" target="#b2">[3]</ref> uses a different baseline model, and we present it as a reference to analyze how much the proposed algorithm can improve.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>In this paper, we exploit the fact that segmentations are structured outputs and share many similarities between source and target domains. We tackle the domain adaptation problem for semantic segmentation via adversarial learning in the output space. To further enhance the adapted model, we construct a multi-level adversarial network to effectively perform output space domain adaptation at different feature levels. Experimental results show that the proposed method performs favorably against numerous baseline models and the state-of-the-art algorithms. We hope that our proposed method can be a generic adaptation model for a wide range of pixel-level prediction tasks.  <ref type="figure">Figure 3</ref>. Example results of adapted segmentation for GTA5-to-Cityscapes. For each target image, we show results before adaptation, with feature adaptation and our adapted segmentations in the output space.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Our motivation of learning adaptation in the output space. While images may be very different in appearance, their outputs are structured and share many similarities, such as spatial layout and local context.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we use the same weight as in the single-level set- ting for the high-level output space (i.e.,001). Since the low-level output carries less informa- tion to predict the segmentation, we use smaller weights for both the segmentation and adversarial loss (i.e.,0.0002). Evaluation results show that our multi- level adversarial adaptation further improves the segmenta- tion accuracy. More results and analysis are presented in the supplementary material.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Results of adapting GTA5 to Cityscapes. We first compare our results using single-level adversarial learning in the output space with other state-of-the-art algorithms with the VGG-16 based model. Then we adopt the ResNet-101 based model and present ablation study on different components of our proposed method.</figDesc><table>GTA5 → Cityscapes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Performance gap between the adapted model and the fully-supervised (oracle) model. We first compare results with state-of-the-art methods using the VGG based model, and then show our result using the ResNet one.</figDesc><table>GTA5 → Cityscapes 

method 
Baseline Adapt Oracle mIoU Gap 

FCNs in the Wild [13] 

VGG-16 

27.1 64.6 
-37.5 
CDA [39] 
28.9 60.3 
-31.4 
CyCADA (feature) [12] 
29.2 60.3 
-30.5 
CyCADA (pixel) [12] 
34.8 60.3 
-24.9 
Ours (single-level) 
35.0 61.8 
-25.2 

Ours (multi-level) 
ResNet-101 42.4 65.1 
-22.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Results of adapting SYNTHIA to Cityscapes. We first compare our results using single-level adversarial learning in the output space with other state-of-the-art algorithms with the VGG-16 based model. Then we adopt the ResNet-101 based model and present ablation study on different components of our proposed method.Table 5. Performance gap between the adapted model and the fully-supervised (oracle) model. We first compare results with state-of-the-art methods using the VGG based model, and then show our result using the ResNet one. SYNTHIA → Cityscapes Method Baseline Adapt Oracle mIoU Gap FCNs in the Wild [13]</figDesc><table>SYNTHIA → Cityscapes 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Results of adapting Cityscapes to the Cross-City dataset. We construct our baseline model using the ResNet-101 architecture, and compare results between feature adaptation and our multi-level adaptation method in the output space. Cityscapes → Cross-CityOurs (output space) 83.9 34.2 88.3 18.8 40.2 86.2 93.1 47.8 21.7 80.9 47.8 48.3 8.6 53.8Ours (output space) 76.2 44.7 84.6 9.3 25.5 81.8 87.3 55.3 32.7 74.3 28.9 43.0 27.6 51.6Ours (output space) 81.5 26.0 77.8 17.8 26.8 82.7 90.9 55.8 38.0 72.1 4.2 24.5 50.8 49.933.4 86.6 12.7 16.4 77.0 92.1 17.6 13.7 70.7 37.7 44.4 18.5 46.5 Ours (feature) 82.1 31.9 84.1 25.7 13.2 77.2 81.2 28.1 12.0 67.0 35.8 43.5 20.9 46.6 Ours (output space) 81.7 29.5 85.2 26.4 15.6 76.7 91.7 31.0 12.5 71.5 41.1 47.3 27.7 49.1</figDesc><table>City 
Method 
road 
sidewalk 
building 
light 
sign 
veg 
sky 
person 
rider 
car 
bus 
mbike 
bike 
mIoU 

Rome 

Cross-City [3] 
79.5 29.3 84.5 0.0 22.2 80.6 82.8 29.5 13.0 71.7 37.5 25.9 1.0 
42.9 
Our Baseline 
83.9 34.3 87.7 13.0 41.9 84.6 92.5 37.7 22.4 80.8 38.1 39.1 5.3 
50.9 
Ours (feature) 
78.8 28.6 85.5 16.6 40.1 85.3 79.6 42.4 20.7 79.6 58.8 45.5 6.1 
51.4 
Rio 

Cross-City [3] 
74.2 43.9 79.0 2.4 
7.5 77.8 69.5 39.3 10.3 67.9 41.2 27.9 10.9 
42.5 
Our Baseline 
76.6 47.3 82.5 12.6 22.5 77.9 86.5 43.0 19.8 74.5 36.8 29.4 16.7 
48.2 
Ours (feature) 
73.7 44.2 83.0 6.1 18.1 79.6 86.9 51.0 22.1 73.7 31.4 48.3 28.4 
49.7 
Tokyo 

Cross-City [3] 
83.4 35.4 72.8 12.3 12.7 77.4 64.3 42.7 21.5 64.1 20.8 8.9 40.3 
42.8 
Our Baseline 
82.9 31.3 78.7 14.2 24.5 81.6 89.2 48.6 33.3 70.5 7.7 11.5 45.9 
47.7 
Ours (feature) 
81.5 30.8 76.6 15.3 20.2 82.0 84.0 49.4 33.3 70.5 4.5 24.3 51.6 
48.0 
Taipei 

Cross-City [3] 
78.6 28.6 80.0 13.1 7.6 68.2 82.1 16.8 9.4 60.4 34.0 26.5 9.9 
39.6 
Our Baseline 
83.5 </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. W.-C. Hung is supported in part by the NSF CAREER Grant #1149783, gifts from Adobe and NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs. CoRR, abs/1606.00915</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">No more discrimination: Cross city adaptation of road scene segmenters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-C</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><forename type="middle">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">JMLR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1711.03213</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1612.02649</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Decoupled deep neural network for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Scene parsing with global context embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Efficient piecewise training of deep structured models for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Dan Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a dcnn for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Constrained convolutional neural networks for weakly supervised segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The SYNTHIA Dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation for face recognition in unlabeled videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Deep image harmonization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
