<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatially Adaptive Computation Time for Residual Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Figurnov</surname></persName>
							<email>michael@figurnov.rumaxwellcollins</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research University Higher School of Economics</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yukun</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
							<email>jonathanhuang@google.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Google Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Vetrov</surname></persName>
							<email>vetrovd@yandex.rursalakhu@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">National Research University Higher School of Economics</orgName>
							</affiliation>
							<affiliation key="aff2">
								<address>
									<settlement>Yandex</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Spatially Adaptive Computation Time for Residual Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional networks gained a wide adoption in the image classification problem <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> due to their exceptional accuracy. In recent years deep convolutional networks have become an integral part of state-of-the-art systems for a diverse set of computer vision problems such as object detection <ref type="bibr" target="#b33">[34]</ref>, image segmentation <ref type="bibr" target="#b31">[32]</ref>, imageto-text <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b41">42]</ref>, visual question answering <ref type="bibr" target="#b10">[11]</ref> and image generation <ref type="bibr" target="#b8">[9]</ref>. They have also been shown to be surprisingly effective in non-vision domains, e.g. natural language processing <ref type="bibr" target="#b43">[44]</ref> and analyzing the board in the game of Go <ref type="bibr" target="#b36">[37]</ref>.</p><p>A major drawback of deep convolutional networks is their huge computational cost. A natural way to tackle this issue is by using attention to guide the computation, which is similar to how biological vision systems operate <ref type="bibr" target="#b34">[35]</ref>. Glimpse-based attention models <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b19">20]</ref> assume that the problem at hand can be solved by carefully processing a small number of typically rectangular sub-regions of the * This work was done while M. Figurnov was an intern at Google. image. This makes such models unsuitable for multi-output problems (generating box proposals in object detection) and per-pixel prediction problems (image segmentation, image generation). Additionally, choosing the glimpse positions requires designing a separate prediction network or a heuristic procedure <ref type="bibr" target="#b0">[1]</ref>. On the other hand, soft spatial attention models <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b35">36]</ref> do not allow to save computation since they require evaluating the model at all spatial positions to choose per-position attention weights.</p><p>We build upon the Adaptive Computation Time (ACT) <ref type="bibr" target="#b11">[12]</ref> mechanism which was recently proposed for Recurrent Neural Networks (RNNs). We show that ACT can be applied to dynamically choose the number of evaluated layers in Residual Network <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> (the similarity between Residual Networks and RNNs was explored in <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b12">13]</ref>). Next, we propose Spatially Adaptive Computation Time (SACT) which adapts the amount of computation between spatial positions. While we use SACT mechanism for Residual Networks, it can potentially be used for convolutional LSTM <ref type="bibr" target="#b40">[41]</ref> models for video processing <ref type="bibr" target="#b27">[28]</ref>.</p><p>SACT is an end-to-end trainable architecture that incorporates attention into Residual Networks. It learns a deterministic policy that stops computation in a spatial position as soon as the features become "good enough". Since SACT maintains the alignment between the image and the feature maps, it is well-suited for a wide range of computer vision problems, including multi-output and per-pixel prediction problems.</p><p>We evaluate the proposed models on the ImageNet classification problem <ref type="bibr" target="#b7">[8]</ref> and find that SACT outperforms both ACT and non-adaptive baselines. Then, we use SACT as a feature extractor in the Faster R-CNN object detection pipeline <ref type="bibr" target="#b33">[34]</ref> and demonstrate results on the challenging COCO dataset <ref type="bibr" target="#b30">[31]</ref>. Example detections and a ponder cost (computation time) map are presented in <ref type="figure" target="#fig_0">fig. 1</ref>. SACT achieves significantly superior FLOPs-quality trade-off to the non-adaptive ResNet model. Finally, we demonstrate that the obtained computation time maps are well-correlated with human eye fixations positions, suggesting that a reasonable attention model arises in the model automatically without any explicit supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Method</head><p>We begin by outlining the recently proposed deep convolutional model Residual Network (ResNet) <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>. Then, we present Adaptive Computation Time, a model which adaptively chooses the number of residual units in ResNet. Finally, we show how this idea can be applied at the spatial position level to obtain Spatially Adaptive Computation Time model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Residual Network</head><p>We first describe the ResNet-101 ImageNet classification architecture ( <ref type="figure" target="#fig_1">fig. 2</ref>). It has been extended for object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b6">7]</ref> and image segmentation <ref type="bibr" target="#b5">[6]</ref> problems. The models we propose are general and can be applied to any ResNet architecture. The first two layers of ResNet-101 are a convolution and a max-pooling layer which together have a total stride of four. Then, a sequence of four blocks is stacked together, each block consisting of multiple stacked residual units. ResNet-101 contains four blocks with 3, 4, 23 and 3 units, respectively. A residual unit has a form F (x) = x + f (x), where the first term is called a shortcut connection and the second term is a residual function. A residual function consists of three convolutional layers: 1 × 1 layer that reduces the number of channels, 3×3 layer that has equal number of input and output channels and 1 × 1 layer that restores the number of channels. We use pre-activation ResNet <ref type="bibr" target="#b15">[16]</ref> in which each convolutional layer is preceded by batch normalization <ref type="bibr" target="#b18">[19]</ref> and ReLU non-linearity. The first units in blocks 2-4 have a stride of 2 and increases the number of output channels by a factor of 2. All other units have equal input and output dimensions. This design choice follows Very Deep Networks <ref type="bibr" target="#b37">[38]</ref> and ensures that all units in the network have an equal computational cost (except for the first units of blocks 2-4 having a slightly higher cost).</p><p>Finally, the obtained feature map is passed through a global average pooling layer <ref type="bibr" target="#b29">[30]</ref> and a fully-connected layer that outputs the logits of class probabilities. The global average pooling ensures that the network is fully convolutional meaning that it can be applied to images of varying resolutions without changing the network's parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Adaptive Computation Time</head><p>Let us first informally explain Adaptive Computation Time (ACT) before describing it in more detail and providing an algorithm. We add a branch to the outputs of each residual unit which predicts a halting score, a scalar value in the range [0, 1]. The residual units and the halting scores are evaluated sequentially, as shown in <ref type="figure" target="#fig_2">fig. 3</ref>. As soon as the cumulative sum of the halting score reaches one, all following residual units in this block will be skipped. We set the halting distribution to be the evaluated halting scores with the last value replaced by a remainder. This ensures that the distribution over the values of the halting scores sums to one. The output of the block is then re-defined as a weighted sum of the outputs of residual units, where the weight of each unit is given by the corresponding probability value. Finally, a ponder cost is introduced that is the number of evaluated residual units plus the remainder value. Minimizing the ponder cost increases the halting scores of the non-last residual units making it more likely that the computation would stop earlier. The ponder cost is then multiplied by a constant τ and added to the original loss function. ACT is applied to each block of ResNet independently with the ponder costs summed.</p><p>Formally, we consider a block of L residual units (boldface denotes tensors of shape Height × Width × Channels):</p><formula xml:id="formula_0">x 0 = input,<label>(1)</label></formula><formula xml:id="formula_1">x l = F l (x l−1 ) = x l−1 + f l (x l−1 ), l = 1 . . . L,<label>(2)</label></formula><formula xml:id="formula_2">output = x L .<label>(3)</label></formula><p>We introduce a halting score h l ∈ [0, 1] for each residual unit. We define h L = 1 to enforce stopping after the last unit.</p><formula xml:id="formula_3">h l = H l (x l ), l = 1 . . . (L − 1),<label>(4)</label></formula><formula xml:id="formula_4">h L = 1.<label>(5)</label></formula><p>We choose the halting score function to be a simple linear model on top of the pooled features:</p><formula xml:id="formula_5">h l = H l (x l ) = σ(W l pool(x l ) + b l ),<label>(6)</label></formula><p>where pool is a global average pooling and σ(t) = 1 1+exp(−t) . Next, we determine N , the number of residual units to evaluate, as the index of the first unit where the cumulative halting score exceeds 1 − ε:</p><formula xml:id="formula_6">N = min n ∈ {1 . . . L} : n l=1 h l ≥ 1 − ε ,<label>(7)</label></formula><p>where ε is a small constant (e.g., 0.01) that ensures that N can be equal to 1 (the computation stops after the first unit) even though h 1 is an output of a sigmoid function meaning that h 1 &lt; 1. Additionally, we define the remainder R:</p><formula xml:id="formula_7">R = 1 − N −1 l=1 h l .<label>(8)</label></formula><p>Due to the definition of N in eqn. <ref type="formula" target="#formula_6">(7)</ref>, we have 0 ≤ R ≤ 1.</p><p>We next transform the halting scores into a halting distribution, which is a discrete distribution over the residual units. Its property is that all the units starting from (N + 1)-st have zero probability:</p><formula xml:id="formula_8">p l =      h l if l &lt; N, R if l = N, 0 if l &gt; N. (9)</formula><p>The output of the block is now defined as the outputs of residual units weighted by the halting distribution. Since representations of residual units are compatible with </p><formula xml:id="formula_9">x = F l (x) 8: if l &lt; L then h = H l (x) 9:</formula><p>else h = 1 10:</p><p>end if</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>c += h</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>ρ += 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>if c &lt; 1 − ε then</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>output += h · x</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>15:</head><p>R −= h 16:</p><formula xml:id="formula_10">else 17: output += R · x 18:</formula><p>ρ += R 19:</p><formula xml:id="formula_11">break 20:</formula><p>end if 21: end for 22: return output, ρ each other <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b12">13]</ref>, the weighted average also produces a feature representation of the same type. The values of</p><formula xml:id="formula_12">x N +1 , . . . , x</formula><p>L have zero weight and therefore their evaluation can be skipped:</p><formula xml:id="formula_13">output = L l=1 p l x l = N l=1 p l x l .<label>(10)</label></formula><p>Ideally, we would like to directly minimize the number of evaluated units N . However, N is a piecewise constant function of the halting scores that cannot be optimized with gradient descent. Instead, we introduce the ponder cost ρ, an almost everywhere differentiable upper bound on the number of evaluated units N (recall that R ≥ 0):</p><formula xml:id="formula_14">ρ = N + R.<label>(11)</label></formula><p>When differentiating ρ, we ignore the gradient of N . Also, note that R is not a continuous function of the halting scores <ref type="bibr" target="#b24">[25]</ref>. The discontinuities happen in the configurations of halting scores where N changes value. Following <ref type="bibr" target="#b11">[12]</ref>, we ignore these discontinuities and find that they do not impede training. Algorithm 1 shows the description of ACT. The partial derivative of the ponder cost w.r.t. a halting score h l is ∂ρ</p><formula xml:id="formula_15">∂h l = −1 if l &lt; N, 0 if l ≥ N.<label>(12)</label></formula><p>Therefore, minimizing the ponder cost increases h 1 , . . . , h N −1 , making the computation stop earlier. This effect is balanced by the original loss function L which also depends on the halting scores via the block output, eqn. <ref type="bibr" target="#b9">(10)</ref>. Intuitively, the more residual units are used, the better the output, so minimizing L usually increases the weight R of the last used unit's output x N , which in turn decreases h 1 , . . . , h N −1 . ACT has several important advantages. First, it adds very few parameters and computation to the base model. Second, it allows to calculate the output of the block "on the fly" without storing all the intermediate residual unit outputs and halting scores in memory. For example, this would not be possible if the halting distribution were a softmax of halting scores, as done in soft attention <ref type="bibr" target="#b41">[42]</ref>. Third, we can recover a block with any constant number of units l ≤ L by setting</p><formula xml:id="formula_16">h 1 = · · · = h l−1 = 0, h l = 1.</formula><p>Therefore, ACT is a strict generalization of standard ResNet.</p><p>We apply ACT to each block independently and then stack the obtained blocks as in the original ResNet. The input of the next block becomes the weighted average of the residual units from the previous block, eqn. <ref type="bibr" target="#b9">(10)</ref>. A similar connectivity pattern has been explored in <ref type="bibr" target="#b16">[17]</ref>. We add the sum of the ponder costs ρ k , k = 1 . . . K from the K blocks to the original loss function L:</p><formula xml:id="formula_17">L ′ = L + τ K k=1 ρ k .<label>(13)</label></formula><p>The resulting loss function L ′ is differentiable and can be optimized using conventional backpropagation. τ ≥ 0 is a regularization coefficient which controls the trade-off between optimizing the original loss function and the ponder cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Spatially Adaptive Computation Time</head><p>In this section, we present Spatially Adaptive Computation Time (SACT). We adjust the per-position amount of computation by applying ACT to each spatial position of the block, as shown in <ref type="figure">fig. 4</ref>. As we show in the experiments, SACT can learn to focus the computation on the regions of interest.</p><p>We define the active positions as the spatial locations where the cumulative halting score is less than one. Because an active position might have inactive neighbors, the values for the the inactive positions need to be imputed to evaluate the residual unit in the active positions. We simply copy the previous value for the inactive spatial positions, which is equivalent to setting the residual function f (x) value to  This transformation can be implemented efficiently using the perforated convolutional layer <ref type="bibr" target="#b9">[10]</ref>. zero, as displayed in <ref type="figure">fig. 5</ref>. The evaluation of a block can be stopped completely as soon as all the positions become inactive. Also, the ponder cost is averaged across the spatial positions to make it comparable with the ACT ponder cost. The full algorithm is described in alg. 2.</p><p>We define the halting scores for SACT as</p><formula xml:id="formula_18">H l (x) = σ( W l * x + W l pool(x) + b l ),<label>(14)</label></formula><p>where * denotes a 3 × 3 convolution with a single output channel and pool is a global average-pooling (see <ref type="figure" target="#fig_6">fig. 6</ref>). SACT is fully convolutional and can be applied to images of any size. Note that SACT is a more general model than ACT, and, consequently, than standard ResNet. If we choose W l = 0, then the halting scores for all spatial positions coincide. In this case the computation for all the positions halts simultaneously and we recover the ACT model.</p><p>SACT requires evaluation of the residual function f (x) in just the active spatial positions. This can be performed if not aij ∀(i, j) ∈ X then break <ref type="bibr">12:</ref> end if <ref type="bibr">13:</ref> for all (i, j) ∈ X do <ref type="bibr">14:</ref> if aij then xij = F l (x)ij <ref type="bibr">15:</ref> else xij =xij for all (i, j) ∈ X do <ref type="bibr">19:</ref> if not aij then continue <ref type="bibr">20:</ref> end if <ref type="bibr">21:</ref> if l &lt; L then hij = H l (x)ij <ref type="bibr">22:</ref> else hij = 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>23:</head><p>end if <ref type="bibr">24:</ref> cij += hij <ref type="bibr">25:</ref> ρij += 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>26:</head><p>if cij &lt; 1 − ε then <ref type="bibr">27:</ref> output ij += hij · xij</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>28:</head><p>Rij −= hij end for 35:x = x 36: end for 37: ρ = (i,j)∈X ρ ij/(HW ) 38: return output, ρ efficiently using the perforated convolutional layer proposed in <ref type="bibr" target="#b9">[10]</ref> (with skipped values replaced by zeros instead of the nearest neighbor's values). Recall that the residual function consists of a stack of 1 × 1, 3 × 3 and 1 × 1 convolutional layers. The first convolutional layer has to be evaluated in the positions obtained by dilating the active positions set with a 3 × 3 kernel. The second and third layers need to be evaluated just in the active positions.</p><p>An alternative approach to using the perforated convolutional layer is to tile the halting scores map. Suppose that we share the values of the halting scores h l within k × k tiles. For example, we can perform pooling of h l with a kernel size k ×k and stride k and then upscale the results by a factor of k. Then, all positions in a tile have the same active flag, and we can apply the residual unit densely to just the active tiles, reusing the commonly available convolution routines. k should be sufficiently high to mitigate the overhead of the additional kernel calls and the overlapping computations of the first 1 × 1 convolution. Therefore, tiling is advisable when the SACT is applied to high-resolution images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Related work</head><p>The majority of the work on increasing the computational efficiency of deep convolutional networks focuses on static techniques. These include decompositions of convolutional kernels <ref type="bibr" target="#b20">[21]</ref> and pruning of connections <ref type="bibr" target="#b13">[14]</ref>. Many of these techniques made their way into the design of the standard deep architectures. For example, Inception <ref type="bibr" target="#b38">[39]</ref> and ResNet <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref> use factorized convolutional kernels.</p><p>Recently, several works have considered the problem of varying the amount of computation in computer vision. Cascaded classifiers <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b42">43]</ref> are used in object detection to quickly reject "easy" negative proposals. Dynamic Capacity Networks <ref type="bibr" target="#b0">[1]</ref> use the same amount of computation for all images and use image classification-specific heuristic. PerforatedCNNs <ref type="bibr" target="#b9">[10]</ref> vary the amount of computation spatially but not between images. <ref type="bibr" target="#b2">[3]</ref> proposes to tune the amount of computation in a fully-connected network using a REINFORCE-trained policy which makes the optimization problem significantly more challenging.</p><p>BranchyNet <ref type="bibr" target="#b39">[40]</ref> is the most similar approach to ours although only applicable to classification problems. It adds classification branches to the intermediate layers of the network. As soon as the entropy of the intermediate classifications is below some threshold, the network's evaluation halts. Our preliminary experiments with a similar procedure based on ACT (using ACT to choose the number of blocks to evaluate) show that it is inferior to using less units per block.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We first apply ACT and SACT models to the image classification task for the ImageNet dataset <ref type="bibr" target="#b7">[8]</ref>. We show that SACT achieves a better FLOPs-accuracy trade-off than ACT by directing computation to the regions of interest. Additionally, SACT improves the accuracy on high-resolution images compared to the ResNet model. Next, we use the obtained SACT model as a feature extractor in the Faster R-CNN object detection pipeline <ref type="bibr" target="#b33">[34]</ref> on the COCO dataset <ref type="bibr" target="#b30">[31]</ref>. Again we show that we obtain significantly improved FLOPs-mAP trade-off compared to basic ResNet models. Finally, we demonstrate that SACT ponder cost maps correlate well with the position of human eye fixations by evaluating them as a visual saliency model on the cat2000 dataset <ref type="bibr" target="#b3">[4]</ref> without any training on this dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image classification (ImageNet dataset)</head><p>First, we train the basic ResNet-50 and ResNet-101 models from scratch using asynchronous SGD with momentum (see the supplementary text for the hyperparameters). Our models achieve similar performance to the reference implementation <ref type="bibr" target="#b0">1</ref> . For a single center 224 × 224 resolution crop, the reference ResNet-101 model achieves 76.4% accuracy, 92.9% recall@5, while our implementation achieves 76% and 93.1%, respectively. Note that our model is the newer pre-activation ResNet <ref type="bibr" target="#b15">[16]</ref> and the reference implementation is the post-activation ResNet <ref type="bibr" target="#b14">[15]</ref>.</p><p>We use ResNet-101 as the basic architecture for ACT and SACT models. Thanks to the end-to-end differentiability and deterministic behaviour, we find the same optimization hyperparameters are applicable for training of ACT and SACT as for the ResNet models. However, special care needs to be taken to address the dead residual unit problem in ACT and SACT models. Since ACT and SACT are deterministic, the last units in the blocks do not get enough training signal and their parameters become obsolete. As a result, the ponder cost saved by not using these units overwhelms the possible initial gains in the original loss function and the units are never used. We observe that while the dead residual units can be recovered during training, this process is very slow. Note that ACT-RNN <ref type="bibr" target="#b11">[12]</ref> is not affected by this problem since the parameters for all timesteps are shared.</p><p>We find two techniques helpful for alleviating the dead residual unit problem. First, we initialize the bias of the halting scores units to a negative value to force the model to use the last units during the initial stages of learning. We use b l = −3 in the experiments which corresponds to initially using 1/σ(b l ) ≈ 21 units. Second, we use a two-stage training procedure by initializing the ACT/SACT network's weights from the pretrained ResNet-101 model. The halting score weights are still initialized randomly. This greatly simplifies learning of a reasonable halting policy in the beginning of training.</p><p>As a baseline for ACT and SACT, we consider a nonadaptive ResNet model with a similar number of floating point operations. We take the average numbers of units used in each block in the ACT or SACT model (for SACT we also average over the spatial dimensions) and round them to the nearest integers. Then, we train a ResNet model with such number of units per block. We follow the two-stage training procedure by initializing the network's parameters with the the first residual units of the full ResNet-101 in each block. This slightly improves the performance compared to using the random initialization.   We compare ACT and SACT to ResNet-50, ResNet-101 and the baselines in <ref type="figure" target="#fig_10">fig. 7</ref>. We measure the average per-image number of floating point operations (FLOPs) required for evaluation of the validation set. We treat multiply-add as two floating point operations. The FLOPs are calculated just for the convolution operations (perforated convolution for SACT) since all other operations (nonlinearities, pooling and output averaging in ACT/SACT) have minimal impact on this metric. The ACT models use τ ∈ {0.0005, 0.001, 0.005, 0.01} and SACT models use τ ∈ {0.001, 0.005, 0.01}. If we increase the image resolution at the test time, as suggested in <ref type="bibr" target="#b15">[16]</ref>, we observe that SACT outperforms ACT and the baselines. Surprisingly, in this setting SACT has higher accuracy than the ResNet-101 model while being computationally cheaper. Such accuracy improvement does not happen for the baseline models or ACT models. We attribute this to the improved scale tolerance provided by the SACT mechanism. The extended results of <ref type="figure" target="#fig_10">fig. 7(a,b)</ref>, including the average number of residual units per block, are presented in the supplementary.</p><p>We visualize the ponder cost for each block of SACT as heat maps (which we call ponder cost maps henceforth) in <ref type="figure" target="#fig_11">fig. 8</ref>. More examples of the total SACT ponder cost maps are shown in <ref type="figure" target="#fig_12">fig. 9</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Object detection (COCO dataset)</head><p>Motivated by the success of SACT in classification of high-resolution images and ignoring uninformative background, we now turn to a harder problem of object detection. Object detection is typically performed for high-resolution images (such as 1000 × 600, compared to 224 × 224 for ImageNet classification) to allow detection of small objects. Computational redundancy becomes a big issue in this setting since a large image area is often occupied by the background.</p><p>We use the Faster R-CNN object detection pipeline <ref type="bibr" target="#b33">[34]</ref> which consists of three stages. First, the image is processed with a feature extractor. This is the most computationally expensive part. Second, a Region Proposal Network predicts a number of class-agnostic rectangular proposals (typically 300). Third, each proposal box's features are cropped from the feature map and passed through a box classifier which predicts whether the proposal corresponds to an object, the class of this object and refines the boundaries. We train the model end-to-end using asynchronous SGD with momentum, employing Tensorflow's crop_and_resize operation, which is similar to the Spatial Transformer Network <ref type="bibr" target="#b19">[20]</ref>, to perform cropping of the region proposals. The training hyperparameters are provided in the supplementary.</p><p>We use ResNet blocks 1-3 as a feature extractor and block 4 as a box classifier, as suggested in <ref type="bibr" target="#b14">[15]</ref>. We reuse the models pretrained on the ImageNet classification task and fine-tune them for COCO detection. For SACT, the ponder cost penalty τ is only applied to the feature extractor (we use the same value as for ImageNet classification). We use COCO train for training and COCO val for evaluation (instead of the combined train+val set which is sometimes used in the literature). We do not employ multiscale inference, iterative box refinement or global context. We find that SACT achieves superior speed-mAP tradeoff compared to the baseline of using non-adaptive ResNet as a feature extractor (see <ref type="table">table 1</ref>). SACT τ = 0.005 model has slightly higher FLOPs count than ResNet-50 and 2.1 points better mAP. Note that this SACT model outperforms the originally reported result for ResNet-101, 27.2 mAP <ref type="bibr" target="#b14">[15]</ref>. Several examples are presented in <ref type="figure" target="#fig_0">fig. 10.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Visual saliency (cat2000 dataset)</head><p>We now show that SACT ponder cost maps correlate well with human attention. To do that, we use a large dataset of visual saliency: the cat2000 dataset <ref type="bibr" target="#b3">[4]</ref>. The dataset is obtained by showing 4,000 images of 20 scene categories to 24 human subjects and recording their eye fixation positions. The ground-truth saliency map is a heat map of the eye fixation positions. We do not train the SACT models on this dataset and simply reuse the ImageNet-and COCO-trained models. Cat2000 saliency maps exhibit a strong center bias. Most images contain a blob of saliency in the center even when there is no object of interest located there. Since our </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>AUC-Judd (%) Center baseline <ref type="bibr" target="#b3">[4]</ref> 83.4 DeepFix <ref type="bibr" target="#b23">[24]</ref> 87 † "Infinite humans" <ref type="bibr" target="#b3">[4]</ref> 90 † ImageNet SACT τ = 0.005 84.6 COCO SACT τ = 0.005 84.7 model is fully convolutional, we cannot learn such bias even if we trained on the saliency data. Therefore, we combine our ponder cost maps with a constant center-biased map.</p><p>We resize the 1920 × 1080 cat2000 images to 320 × 180 for ImageNet model and to 640 × 360 for COCO model and pass them through the SACT model. Following <ref type="bibr" target="#b3">[4]</ref>, we consider a linear combination of the Gaussian blurred ponder cost map normalized to [0, 1] range and a "center baseline," a Gaussian centered at the middle of the image. Full description of the combination scheme is provided in the supplementary. The first half of the training set images for every scene category is used for determining the optimal values of the Gaussian blur kernel size and the center baseline multiplier, while the second half is used for validation. <ref type="table" target="#tab_2">Table 2</ref> presents the AUC-Judd <ref type="bibr" target="#b4">[5]</ref> metric, the area under the ROC-curve for the saliency map as a predictor for eye fixation positions. SACT outperforms the naïve center baseline. Compared to the state-of-the-art deep model DeepFix <ref type="bibr" target="#b23">[24]</ref> method, SACT does competitively. Examples are shown in <ref type="figure" target="#fig_0">fig. 11</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We present a Residual Network based model with a spatially varying computation time. This model is end- to-end trainable, deterministic and can be viewed as a black-box feature extractor. We show its effectiveness in image classification and object detection problems. The amount of per-position computation in this model correlates well with the human eye fixation positions, suggesting that this model captures the important parts of the image. We hope that this paper will lead to a wider adoption of attention and adaptive computation time in large-scale computer vision systems. The source code is available at https://github.com/mfigurnov/sact.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Left: object detections. Right: feature extractor SACT ponder cost (computation time) map for a COCO validation image. The proposed method learns to allocate more computation for the object-like regions of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Residual Network (ResNet) with 101 convolutional layers. Each residual unit contains three convolutional layers. We apply Adaptive Computation Time to each block of ResNet to learn an image-dependent policy of stopping the computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Adaptive Computation Time (ACT) for one block of residual units. The computation halts as soon as the cumulative sum of the halting score reaches 1. The remainder is R = 1 − h 1 − h 2 − h 3 = 0.6, the number of evaluated units N = 4, and the ponder cost is ρ = N + R = 4.6. See alg. 1. ACT provides a deterministic and end-to-end learnable policy of choosing the amount of computation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 1</head><label>1</label><figDesc>Adaptive Computation Time for one block of residual units. ACT does not require storing the intermediate residual units outputs. Input: 3D tensor input Input: number of residual units in the block L Input: 0 &lt; ε &lt; 1 Output: 3D tensor output Output: ponder cost ρ 1: x = input 2:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Spatially Adaptive Computation Time (SACT) for one block of residual units. We apply ACT to each spatial position of the block. As soon as position's cumulative halting score reaches 1, we mark it as inactive. See alg. 2. SACT learns to choose the appropriate amount of computation for each spatial position in the block. copy</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: SACT halting scores. Halting scores are evaluated fully convolutionally making SACT applicable to images of arbitrary resolution. SACT becomes ACT if the 3 × 3 conv weights are set to zero.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Algorithm 2 ⊲</head><label>2</label><figDesc>Spatially Adaptive Computation Time for one block of residual units Input: 3D tensor input Input: number of residual units in the block L Input: 0 &lt; ε &lt; 1 ⊲ input and output have different shapes Output: 3D tensor output of shape H × W × C Output: ponder cost ρ 1:x = input 2: X = {1 . . . H} × {1 . . . W } 3: for all (i, j)Per-position ponder cost 9: end for 10: for l = 1 . . . L do 11:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: ImageNet validation set. Comparison of ResNet, ACT, SACT and the respective baselines. Error bars denote one standard deviation across images. All models are trained with 224 × 224 resolution images. SACT outperforms ACT and baselines when applied to images whose resolutions are higher than the training images. The advantage margin grows as resolution difference increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Ponder cost maps for each block (SACT τ = 0.005, ImageNet validation image). Note that the first block reacts to the low-level features while the last two blocks attempt to localize the object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: ImageNet validation set. SACT (τ = 0.005) ponder cost maps. Top: low ponder cost (19.8-20.55), middle: average ponder cost (23.4-23.6), bottom: high ponder cost (24.9-26.0). SACT typically focuses the computation on the region of interest.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_13"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: COCO testdev set. Detections and feature extractor ponder cost maps (τ = 0.005). SACT allocates much more computation to the object-like regions of the image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: cat2000 saliency dataset. Left to right: image, human saliency, SACT ponder cost map (COCO model, τ = 0.005) with postprocessing (see text) and softmax with temperature 1/5. Note the center bias of the dataset. SACT model performs surprisingly well on out-of-domain images such as art and fractals.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 1: COCO val set. Faster R-CNN with SACT results. FLOPs are average (± one standard deviation) feature extractor floating point operations relative to ResNet-101 (that does 1.42E+11 oper- ations). SACT improves the FLOPs-mAP trade-off compared to using ResNet without adaptive computation.</figDesc><table>Feature extractor 
FLOPs (%) mAP @ [.5, .95] (%) 
ResNet-101 [15] 
100 
27.2 
ResNet-50 (our impl.) 
46.6 
25.56 
SACT τ = 0.005 
56.0 ± 8.5 
27.61 
SACT τ = 0.001 
72.4 ± 8.4 
29.04 
ResNet-101 (our impl.) 
100 
29.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>cat2000 validation set. † -results for the test set. SACT ponder cost maps work as a visual saliency model even without explicit supervision.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. D. Vetrov is supported by Russian Academic Excellence Project '5-100'. R. Salakhutdinov is supported in part by ONR grants N00014-13-1-0721, N00014-14-1-0232, and the ADeLAIDE grant FA8750-16C-0130-001.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<title level="m">Dynamic capacity networks. ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-L</forename><surname>Bacon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pineau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Precup</surname></persName>
		</author>
		<title level="m">Conditional computation in neural networks for faster models. ICLR Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<ptr target="http://saliency.mit.edu/.6" />
		<title level="m">Mit saliency benchmark</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03605</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet: A large-scale hierarchical image database. CVPR</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning to generate chairs with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Tobias</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">PerforatedCNNs: Acceleration through elimination of redundant convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ibraimova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01847</idno>
		<title level="m">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Highway and residual networks learn unrolled iterative estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep compression: Compressing deep neural network with pruning, trained quantization and huffman coding. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Deep networks with stochastic depth. ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<title level="m">Spatial transformer networks. NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">BMVC</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deepfix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02927</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">My notes on adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<ptr target="https://goo.gl/QxBucH,2016.3" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to combine foveal glimpses with a third-order boltzmann machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A convolutional neural network cascade for face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Videolstm convolves, attends and flows for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Bridging the gaps between residual learning, recurrent neural networks and visual cortex</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03640</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<title level="m">Network in network. ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>Microsoft coco: Common objects in context. ECCV</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">The dynamic representation of scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Rensink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Visual cognition</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1-3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">ICLR Workshop</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
		<title level="m">Branchynet: Fast inference via early exiting from deep neural networks. ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Convolutional lstm network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xingjian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers. CVPR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Character-level convolutional networks for text classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
