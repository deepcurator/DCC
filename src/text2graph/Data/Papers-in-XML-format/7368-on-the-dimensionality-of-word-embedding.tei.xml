<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">On the Dimensionality of Word Embedding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Corp. &amp; Stanford University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanyuan</forename><surname>Shen</surname></persName>
							<email>yuanyuan.shen@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Stanford University</orgName>
								<orgName type="institution" key="instit2">Microsoft Corp. &amp; Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">On the Dimensionality of Word Embedding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we provide a theoretical understanding of word embedding and its dimensionality. Motivated by the unitary-invariance of word embedding, we propose the Pairwise Inner Product (PIP) loss, a novel metric on the dissimilarity between word embeddings. Using techniques from matrix perturbation theory, we reveal a fundamental bias-variance trade-off in dimensionality selection for word embeddings. This bias-variance trade-off sheds light on many empirical observations which were previously unexplained, for example the existence of an optimal dimensionality. Moreover, new insights and discoveries, like when and how word embeddings are robust to over-fitting, are revealed. By optimizing over the biasvariance trade-off of the PIP loss, we can explicitly answer the open question of dimensionality selection for word embedding.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings are very useful and versatile tools, serving as keys to many fundamental problems in numerous NLP research <ref type="bibr" target="#b43">[Turney and Pantel, 2010]</ref>. To name a few, word embeddings are widely applied in information retrieval <ref type="bibr" target="#b33">[Salton, 1971</ref><ref type="bibr" target="#b34">, Salton and Buckley, 1988</ref><ref type="bibr" target="#b38">, Sparck Jones, 1972</ref>, recommendation systems <ref type="bibr" target="#b7">[Breese et al., 1998</ref><ref type="bibr" target="#b50">, Yin et al., 2017</ref>, image description <ref type="bibr" target="#b16">[Frome et al., 2013]</ref>, relation discovery <ref type="bibr" target="#b28">[Mikolov et al., 2013c]</ref> and word level translation <ref type="bibr" target="#b27">[Mikolov et al., 2013b]</ref>. Furthermore, numerous important applications are built on top of word embeddings. Some prominent examples are long short-term memory (LSTM) networks <ref type="bibr" target="#b19">[Hochreiter and Schmidhuber, 1997]</ref> that are used for language modeling <ref type="bibr" target="#b5">[Bengio et al., 2003</ref>], machine translation <ref type="bibr" target="#b41">[Sutskever et al., 2014</ref><ref type="bibr" target="#b2">, Bahdanau et al., 2014</ref>], text summarization <ref type="bibr" target="#b30">[Nallapati et al., 2016]</ref> and image caption generation <ref type="bibr" target="#b48">[Xu et al., 2015</ref><ref type="bibr" target="#b44">, Vinyals et al., 2015</ref>. Other important applications include named entity recognition <ref type="bibr" target="#b22">[Lample et al., 2016]</ref>, sentiment analysis <ref type="bibr" target="#b37">[Socher et al., 2013]</ref> and so on.</p><p>However, the impact of dimensionality on word embedding has not yet been fully understood. As a critical hyper-parameter, the choice of dimensionality for word vectors has huge influence on the performance of a word embedding. First, it directly impacts the quality of word vectors -a word embedding with a small dimensionality is typically not expressive enough to capture all possible word relations, whereas one with a very large dimensionality suffers from over-fitting. Second, the number of parameters for a word embedding or a model that builds on word embeddings (e.g. recurrent neural networks) is usually a linear or quadratic function of dimensionality, which directly affects training time and computational costs. Therefore, large dimensionalities tend to increase model complexity, slow down training speed, and add inferential latency, all of which are constraints that can potentially limit model applicability and deployment <ref type="bibr" target="#b47">[Wu et al., 2016]</ref>.</p><p>Dimensionality selection for embedding is a well-known open problem. In most NLP research, dimensionality is either selected ad hoc or by grid search, either of which can lead to sub-optimal model performances. For example, 300 is perhaps the most commonly used dimensionality in various studies <ref type="bibr" target="#b26">[Mikolov et al., 2013a</ref><ref type="bibr" target="#b32">, Pennington et al., 2014</ref><ref type="bibr" target="#b6">, Bojanowski et al., 2017</ref>. This is possibly due to the influence of the groundbreaking paper, which introduced the skip-gram Word2Vec model and chose a dimensionality of 300 <ref type="bibr" target="#b26">[Mikolov et al., 2013a]</ref>. A better empirical approach used by some researchers is to first train many embeddings of different dimensionalities, evaluate them on a functionality test (like word relatedness or word analogy), and then pick the one with the best empirical performance. However, this method suffers from 1) greatly increased time complexity and computational burden, 2) inability to exhaust all possible dimensionalities and 3) lack of consensus between different functionality tests as their results can differ. Thus, we need a universal criterion that can reflect the relationship between the dimensionality and quality of word embeddings in order to establish a dimensionality selection procedure for embedding methods.</p><p>In this regard, we outline a few major contributions of our paper:</p><p>1. We introduce the PIP loss, a novel metric on the dissimilarity between word embeddings; 2. We develop a mathematical framework that reveals a fundamental bias-variance trade-off in dimensionality selection. We explain the existence of an optimal dimensionality, a phenomenon commonly observed but lacked explanations;</p><p>3. We quantify the robustness of embedding algorithms using the exponent parameter α, and establish that many widely used embedding algorithms, including skip-gram and GloVe, are robust to over-fitting;</p><p>4. We propose a mathematically rigorous answer to the open problem of dimensionality selection by minimizing the PIP loss. We perform this procedure and cross-validate the results with grid search for LSA, skip-gram Word2Vec and GloVe on an English corpus.</p><p>For the rest of the paper, we consider the problem of learning an embedding for a vocabulary of size n, which is canonically defined as V = {1, 2, · · · , n}. Specifically, we want to learn a vector representation v i ∈ R d for each token i. The main object is the embedding matrix E ∈ R n×d , consisting of the stacked vectors v i , where E i,· = v i . All matrix norms in the paper are Frobenius norms unless otherwise stated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries and Background Knowledge</head><p>Our framework is built on the following preliminaries:</p><p>1. Word embeddings are unitary-invariant; 2. Most existing word embedding algorithms can be formulated as low rank matrix approximations, either explicitly or implicitly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Unitary Invariance of Word Embeddings</head><p>The unitary-invariance of word embeddings has been discovered in recent research <ref type="bibr" target="#b18">[Hamilton et al., 2016</ref><ref type="bibr" target="#b1">, Artetxe et al., 2016</ref><ref type="bibr" target="#b36">, Smith et al., 2017</ref><ref type="bibr" target="#b49">, Yin, 2018</ref>. It states that two embeddings are essentially identical if one can be obtained from the other by performing a unitary operation, e.g., a rotation. A unitary operation on a vector corresponds to multiplying the vector by a unitary matrix, i.e. v = vU , where</p><formula xml:id="formula_0">U T U = U U T = Id.</formula><p>Note that a unitary transformation preserves the relative geometry of the vectors, and hence defines an equivalence class of embeddings. In Section 3, we introduce the Pairwise Inner Product loss, a unitary-invariant metric on embedding similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Word Embeddings from Explicit Matrix Factorization</head><p>A wide range of embedding algorithms use explicit matrix factorization, including the popular Latent Semantics Analysis (LSA). In LSA, word embeddings are obtained by truncated SVD of a signal matrix M which is usually based on co-occurrence statistics, for example the Pointwise Mutual Information (PMI) matrix, positive PMI (PPMI) matrix and Shifted PPMI (SPPMI) matrix <ref type="bibr" target="#b23">[Levy and Goldberg, 2014]</ref>. Eigen-words <ref type="bibr" target="#b14">[Dhillon et al., 2015]</ref> is another example of this type. <ref type="bibr" target="#b11">Caron [2001]</ref>, Bullinaria and <ref type="bibr" target="#b8">Levy [2012]</ref>, <ref type="bibr" target="#b42">Turney [2012]</ref>, <ref type="bibr" target="#b23">Levy and Goldberg [2014]</ref> described a generic approach of obtaining embeddings from matrix factorization. Let M be the signal matrix (e.g. the PMI matrix) and M = U DV T be its SVD. A k-dimensional embedding is obtained by truncating the left singular matrix U at dimension k, and multiplying it by a power of the truncated diagonal matrix D, i.e. E = U 1:k D α 1:k,1:k for some α ∈ [0, 1]. <ref type="bibr" target="#b11">Caron [2001]</ref>, Bullinaria and <ref type="bibr" target="#b8">Levy [2012]</ref> discovered through empirical studies that different α works for different language tasks. In <ref type="bibr" target="#b23">Levy and Goldberg [2014]</ref> where the authors explained the connection between skip-gram Word2Vec and matrix factorization, α is set to 0.5 to enforce symmetry. We discover that α controls the robustness of embeddings against over-fitting, as will be discussed in Section 5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Word Embeddings from Implicit Matrix Factorization</head><p>In NLP, two most widely used embedding models are skip-gram Word2Vec <ref type="bibr" target="#b28">[Mikolov et al., 2013c]</ref> and GloVe <ref type="bibr" target="#b32">[Pennington et al., 2014]</ref>. Although they learn word embeddings by optimizing over some objective functions using stochastic gradient methods, they have both been shown to be implicitly performing matrix factorizations.  <ref type="bibr" target="#b23">Levy and Goldberg [2014]</ref> showed that skip-gram Word2Vec's objective is an implicit symmetric factorization of the Pointwise Mutual Information (PMI) matrix:</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Skip-gram</head><formula xml:id="formula_1">PMIij = log p(vi, vj) p(vi)p(vj)</formula><p>Skip-gram is sometimes enhanced with techniques like negative sampling <ref type="bibr" target="#b27">[Mikolov et al., 2013b]</ref>, where the signal matrix becomes the Shifted PMI matrix <ref type="bibr" target="#b23">[Levy and Goldberg, 2014]</ref>.</p><p>GloVe <ref type="bibr" target="#b24">Levy et al. [2015]</ref> pointed out that the objective of GloVe is implicitly a symmetric factorization of the log-count matrix. The factorization is sometimes augmented with bias vectors and the log-count matrix is sometimes raised to an exponent γ ∈ [0, 1] <ref type="bibr" target="#b32">[Pennington et al., 2014]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PIP Loss: a Novel Unitary-invariant Loss Function for Embeddings</head><p>How do we know whether a trained word embedding is good enough? Questions of this kind cannot be answered without a properly defined loss function. For example, in statistical estimation (e.g. linear regression), the quality of an estimatorθ can often be measured using the l 2 loss E[ θ − θ * 2 2 ] where θ * is the unobserved ground-truth parameter. Similarly, for word embedding, a proper metric is needed in order to evaluate the quality of a trained embedding.</p><p>As discussed in Section 2.1, a reasonable loss function between embeddings should respect the unitary-invariance. This rules out choices like direct comparisons, for example using E 1 − E 2 as the loss function. We propose the Pairwise Inner Product (PIP) loss, which naturally arises from the unitary-invariance, as the dissimilarity metric between two word embeddings: Definition 1 (PIP matrix). Given an embedding matrix E ∈ R n×d , define its associated Pairwise Inner Product (PIP) matrix to be PIP(E) = EE T It can be seen that the (i, j)-th entry of the PIP matrix corresponds to the inner product between the embeddings for word i and word j, i.e. PIP i,j = v i , v j . To compare E 1 and E 2 , two embedding matrices on a common vocabulary, we propose the PIP loss: Definition 2 (PIP loss). The PIP loss between E 1 and E 2 is defined as the norm of the difference between their PIP matrices</p><formula xml:id="formula_2">PIP(E1) − PIP(E2) = E1E T 1 − E2E T 2 = i,j ( v (1) i , v (1) j − v (2) i , v (2) j ) 2</formula><p>Note that the i-th row of the PIP matrix,</p><formula xml:id="formula_3">v i E T = ( v i , v 1 , · · · , v i , v n )</formula><p>, can be viewed as the relative position of v i anchored against all other vectors {v 1 , · · · , v n }. In essence, the PIP loss measures the vectors' relative position shifts between E 1 and E 2 , thereby removing their dependencies on any specific coordinate system. The PIP loss respects the unitary-invariance. Specifically, if E 2 = E 1 U where U is a unitary matrix, then the PIP loss between E 1 and E 2 is zero because</p><formula xml:id="formula_4">E 2 E T 2 = E 1 E T 1 .</formula><p>In addition, the PIP loss serves as a metric of functionality dissimilarity. A practitioner may only care about the usability of word embeddings, for example, using them to solve analogy and relatedness tasks <ref type="bibr" target="#b35">[Schnabel et al., 2015</ref><ref type="bibr">, Baroni et al., 2014</ref>, which are the two most important properties of word embeddings. Since both properties are tightly related to vector inner products, a small PIP loss between E 1 and E 2 leads to a small difference in E 1 and E 2 's relatedness and analogy as the PIP loss measures the difference in inner products 1 . As a result, from both theoretical and practical standpoints, the PIP loss is a suitable loss function for embeddings. Furthermore, we show in Section 4 that this formulation opens up a new angle to understanding the effect of embedding dimensionality with matrix perturbation theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">How Does Dimensionality Affect the Quality of Embedding?</head><p>With the PIP loss, we can now study the quality of trained word embeddings for any algorithm that uses matrix factorization. Suppose a d-dimensional embedding is derived from a signal matrix M with the form</p><formula xml:id="formula_5">f α,d (M ) ∆ = U ·,1:d D α 1:d,1:d , where M = U DV</formula><p>T is the SVD. In the ideal scenario, a genie reveals a clean signal matrix M (e.g. PMI matrix) to the algorithm, which yields the oracle embedding E = f α,d (M ). However, in practice, there is no magical oil lamp, and we have to estimateM (e.g. empirical PMI matrix) from the training data, whereM = M + Z is perturbed by the estimation noise Z. The trained embeddingÊ = f α,k (M ) is computed by factorizing this noisy matrix. To ensureÊ is close to E, we want the PIP loss EE T −ÊÊ T to be small. In particular, this PIP loss is affected by k, the dimensionality we select for the trained embedding.</p><p>Arora <ref type="bibr">[2016]</ref> discussed in an article about a mysterious empirical observation of word embeddings: "... A striking finding in empirical work on word embeddings is that there is a sweet spot for the dimensionality of word vectors: neither too small, nor too large" 2 . He proceeded by discussing two possible explanations: low dimensional projection (like the Johnson-Lindenstrauss Lemma) and the standard generalization theory (like the VC dimension), and pointed out why neither is sufficient for explaining this phenomenon. While some may argue that this is caused by underfitting/overfitting, the concept itself is too broad to provide any useful insight. We show that this phenomenon can be explicitly explained by a bias-variance trade-off in Section 4.1, 4.2 and 4.3. Equipped with the PIP loss, we give a mathematical presentation of the bias-variance trade-off using matrix perturbation theory. We first introduce a classical result in Lemma 1. The proof is deferred to the appendix, which can also be found in Stewart and <ref type="bibr" target="#b40">Sun [1990]</ref>.</p><formula xml:id="formula_6">Lemma 1. Let X, Y be two orthogonal matrices of R n×n . Let X = [X 0 , X 1 ] and Y = [Y 0 , Y 1 ] be the first k columns of X and Y respectively, namely X 0 , Y 0 ∈ R n×k and k ≤ n. Then X 0 X T 0 − Y 0 Y T 0 = c X T 0 Y 1</formula><p>where c is a constant depending on the norm only. c = 1 for 2-norm and √ 2 for Frobenius norm.</p><p>As pointed out by several papers <ref type="bibr" target="#b11">[Caron, 2001</ref><ref type="bibr" target="#b8">, Bullinaria and Levy, 2012</ref><ref type="bibr" target="#b42">, Turney, 2012</ref><ref type="bibr" target="#b23">, Levy and Goldberg, 2014</ref>, embedding algorithms can be generically characterized as E = U 1:k,· D The following theorem shows how the PIP loss can be naturally decomposed into a bias term and a variance term when α = 0: Theorem 1. Let E ∈ R n×d andÊ ∈ R n×k be the oracle and trained embeddings, where k ≤ d. Assume both have orthonormal columns. Then the PIP loss has a bias-variance decomposition</p><formula xml:id="formula_7">PIP(E) − PIP(Ê) 2 = d − k + 2 Ê T E ⊥ 2</formula><p>Proof. The proof utilizes techniques from matrix perturbation theory. To simplify notations, denote</p><formula xml:id="formula_8">X 0 = E, Y 0 =Ê, and let X = [X 0 , X 1 ], Y = [Y 0 , Y 1 ]</formula><p>be the complete n by n orthogonal matrices.</p><p>1 A detailed discussion on the PIP loss and analogy/relatedness is deferred to the appendix 2 http://www.offconvex.org/2016/02/14/word-embeddings-2/</p><p>Since k ≤ d, we can further split X 0 into X 0,1 and X 0,2 , where the former has k columns and the latter d − k. Now, the PIP loss equals</p><formula xml:id="formula_9">EE T −ÊÊ T 2 = X0,1X T 0,1 − Y0Y T 0 + X0,2X T 0,2 2 = X0,1X T 0,1 − Y0Y T 0 2 + X0,2X T 0,2 2 + 2 X0,1X T 0,1 − Y0Y T 0 , X0,2X T 0,2 (a) = 2 Y T 0 [X0,2, X1] 2 + d − k − 2 Y0Y T 0 , X0,2X T 0,2 =2 Y T 0 X0,2 2 + 2 Y T 0 X1 2 + d − k − 2 Y0Y T 0 , X0,2X T 0,2 =d − k + 2 Y T 0 X1 2 = d − k + 2 Ê T E ⊥ 2</formula><p>where in equality (a) we used Lemma 1.</p><p>The observation is that the right-hand side now consists of two parts, which we identify as bias and variance. The first part d−k is the amount of lost signal, which is caused by discarding the rest d−k dimensions when selecting k ≤ d. However, Ê T E ⊥ increases as k increases, as the noise perturbs the subspace spanned by E, and the singular vectors corresponding to smaller singular values are more prone to such perturbation. As a result, the optimal dimensionality k * which minimizes the PIP loss lies in between 0 and d, the rank of the matrix M . In this generic case, the columns of E,Ê are no longer orthonormal, which does not satisfy the assumptions in matrix perturbation theory. We develop a novel technique where Lemma 1 is applied in a telescoping fashion. The proof of the theorem is deferred to the appendix. </p><formula xml:id="formula_10">≤ d. Let D = diag(λ i ) andD = diag(λ i ), then PIP(E) − PIP(Ê) ≤ d i=k+1 λ 4α i + k i=1 (λ 2α i −λ 2α i ) 2 + √ 2 k i=1 (λ 2α i − λ 2α i+1 ) Ũ T ·,1:i U·,i:n</formula><p>As before, the three terms in Theorem 2 can be characterized into bias and variances. The first term is the bias as we lose part of the signal by choosing k ≤ d. Notice that the embedding matrix E consists of signal directions (given by U ) and their magnitudes (given by D α ). The second term is the variance on the magnitudes, and the third term is the variance on the directions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">The Bias-Variance Trade-off Captures the Signal-to-Noise Ratio</head><p>We now present the main theorem, which shows that the bias-variance trade-off reflects the "signalto-noise ratio" in dimensionality selection.</p><p>Theorem 3 (Main theorem). SupposeM = M + Z, where M is the signal matrix, symmetric with spectrum</p><formula xml:id="formula_11">{λ i } d i=1</formula><p>. Z is the estimation noise, symmetric with iid, zero mean, variance σ 2 entries. For any 0 ≤ α ≤ 1 and k ≤ d, let the oracle and trained embeddings be</p><formula xml:id="formula_12">E = U ·,1:d D α 1:d,1:d ,Ê =Ũ ·,1:kD α 1:k,1:k where M = U DV T ,M =ŨDṼ</formula><p>T are the SVDs of the clean and estimated signal matrices. Then</p><formula xml:id="formula_13">1. When α = 0, E[ EE T −ÊÊ T ] ≤ d − k + 2σ 2 r≤k,s&gt;d (λr − λs) −2 2. When 0 &lt; α ≤ 1, E[ EE T −ÊÊ T ] ≤ d i=k+1 λ 4α i + 2 √ 2nασ k i=1 λ 4α−2 i + √ 2 k i=1 (λ 2α i − λ 2α i+1 )σ r≤i&lt;s (λr − λs) −2</formula><p>Proof. We sketch the proof for part 2, as the proof of part 1 is simpler and can be done with the same arguments. We start by taking expectation on both sides of Theorem 2:</p><formula xml:id="formula_14">E[ EE T −ÊÊ T ] ≤ d i=k+1 λ 4α i + E k i=1 (λ 2α i −λ 2α i ) 2 + √ 2 k i=1 (λ 2α i − λ 2α i+1 )E[ Ũ T ·,1:i U·,i:n ],</formula><p>The first term involves only the spectrum, which is the same after taking expectation. The second term is upper bounded using Lemma 2 below, derived from Weyl's theorem. We state the lemma, and leave the proof to the appendix.</p><p>Lemma 2. Under the conditions of Theorem 3,</p><formula xml:id="formula_15">E k i=1 (λ 2α i −λ 2α i ) 2 ≤ 2 √ 2nασ k i=1 λ 4α−2 i</formula><p>For the last term, we use the Sylvester operator technique by Stewart and Sun <ref type="bibr">[1990]</ref>. Our result is presented in Lemma 3, and the proof of which is discussed in the appendix. </p><formula xml:id="formula_16">δ k ∆ = min 1≤i≤k,k&lt;j≤n {λi − λj} = λ k − λ k+1 &gt; 0,</formula><p>and Z has iid, zero mean entries with variance σ 2 , then</p><formula xml:id="formula_17">E[ Ũ T 1 U0 ] ≤ σ 1≤i≤k&lt;j≤n (λi − λj) −2</formula><p>Now, collect results in Lemma 2 and Lemma 3, we obtain an upper bound approximation for the PIP loss:</p><formula xml:id="formula_18">E[ EE T −ÊÊ T ] ≤ d i=k+1 λ 4α i + 2 √ 2nασ k i=1 λ 4α−2 i + √ 2 k i=0 (λ 2α i − λ 2α i+1 )σ r≤i&lt;s (λr − λs) −2</formula><p>which completes the proof.</p><p>Theorem 3 shows that when dimensionality is too small, too much signal power (specifically, the spectrum of the signal M ) is discarded, causing the first term to be too large (high bias). On the other hand, when dimensionality is too large, too much noise is included, causing the second and third terms to be too large (high variance). This explicitly answers the question of <ref type="bibr" target="#b0">Arora [2016]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Two New Discoveries</head><p>In this section, we introduce two more discoveries regarding the fundamentals of word embedding. The first is the relationship between the robustness of embedding and the exponent parameter α, with a corollary that both skip-gram and GloVe are robust to over-fitting. The second is a dimensionality selection method by explicitly minimizing the PIP loss between the oracle and trained embeddings 3 . All our experiments use the Text8 corpus <ref type="bibr" target="#b25">[Mahoney, 2011]</ref>, a standard benchmark corpus used for various natural language tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Word Embeddings' Robustness to Over-Fitting Increases with Respect to α</head><p>Theorem 3 provides a good indicator for the sensitivity of the PIP loss with respect to overparametrization. <ref type="bibr" target="#b45">Vu [2011]</ref> showed that the approximations obtained by matrix perturbation theory are minimax tight. As k increases, the bias term</p><formula xml:id="formula_19">d i=k λ 4α i</formula><p>decreases, which can be viewed as a zeroth-order term because the arithmetic means of singular values are dominated by the large ones.</p><p>As a result, when k is already large (say, the singular values retained contain more than half of the total energy of the spectrum), increasing k has only marginal effect on the PIP loss.</p><p>On the other hand, the variance terms demonstrate a first-order effect, which contains the difference of the singular values, or singular gaps. Both variance terms grow at the rate of λ 2α−1 k with respect to the dimensionality k (the analysis is left to the appendix). For small λ k (i.e. λ k &lt; 1), the rate λ 2α−1 k increases as α decreases: when α &lt; 0.5, this rate can be very large; When 0.5 ≤ α ≤ 1, the rate is bounded and sub-linear, in which case the PIP loss will be robust to over-parametrization. In other words, as α becomes larger, the embedding algorithm becomes less sensitive to over-fitting caused by the selection of an excessively large dimensionality k. To illustrate this point, we compute the PIP loss of word embeddings (approximated by Theorem 3) for the PPMI LSA algorithm, and plot them for different α's in <ref type="figure" target="#fig_6">Figure 1a</ref>.  Our discussion that over-fitting hurts algorithms with smaller α more can be empirically verified. <ref type="figure" target="#fig_6">Figure 1b</ref> and 1c display the performances (measured by the correlation between vector cosine similarity and human labels) of word embeddings of various dimensionalities from the PPMI LSA algorithm, evaluated on two word correlation tests: WordSim353 <ref type="bibr" target="#b15">[Finkelstein et al., 2001]</ref> and MTurk771 <ref type="bibr" target="#b17">[Halawi et al., 2012]</ref>. These results validate our theory: performance drop due to overparametrization is more significant for smaller α.</p><p>For the popular skip-gram <ref type="bibr" target="#b27">[Mikolov et al., 2013b]</ref> and GloVe <ref type="bibr" target="#b32">[Pennington et al., 2014]</ref>, α equals 0.5 as they are implicitly doing a symmetric factorization. Our previous discussion suggests that they are robust to over-parametrization. We empirically verify this by training skip-gram and GloVe embeddings. <ref type="figure" target="#fig_2">Figure 2</ref> shows the empirical performance on three word functionality tests. Even with extreme over-parametrization (up to k = 10000), skip-gram still performs within 80% to 90% of optimal performance, for both analogy test <ref type="bibr" target="#b26">[Mikolov et al., 2013a]</ref> and relatedness tests (WordSim353 <ref type="bibr" target="#b15">[Finkelstein et al., 2001]</ref> and MTurk771 <ref type="bibr" target="#b17">[Halawi et al., 2012]</ref>). This observation holds for GloVe as well as shown in <ref type="figure" target="#fig_4">Figure 3</ref>.</p><formula xml:id="formula_20">(a) Google Analogy Test (b) WordSim353 Test (c) MTurk771 Test</formula><p>Figure 2: skip-gram Word2Vec: over-parametrization does not significantly hurt performance</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Optimal Dimensionality Selection: Minimizing the PIP Loss</head><p>The optimal dimensionality can be selected by finding the k * that minimizes the PIP loss between the trained embedding and the oracle embedding. With a proper estimate of the spectrum D = {λ} 1:k,1:k , in which case the PIP loss between them can be directly calculated. We found empirically that the Monte-Carlo procedure is more accurate as the simulated PIP losses concentrate tightly around their means across different runs. In the following experiments, we demonstrate that dimensionalities selected using the Monte-Carlo approach achieve near-optimal performances on various word intrinsic tests. As a first step, we demonstrate how one can obtain good estimates of {λ i } d 1 and σ in 5.2.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Spectrum and Noise Estimation from Corpus</head><p>Noise Estimation We note that for most NLP tasks, the signal matrices are estimated by counting or transformations of counting, including taking log or normalization. This holds for word embeddings that are based on co-occurrence statistics, e.g., LSA, skip-gram and GloVe. We use a count-twice trick to estimate the noise: we randomly split the data into two equally large subsets, and get matricesM 1 = M + Z 1 ,M 2 = M + Z 2 in R m×n , where Z 1 , Z 2 are two independent copies of noise with variance 2σ</p><p>2 . Now,M 1 −M 2 = Z 1 − Z 2 is a random matrix with zero mean and variance 4σ 2 . Our estimator is the sample standard deviation, a consistent estimator:</p><formula xml:id="formula_21">σ = 1 2 √ mn M 1 −M2</formula><p>Spectral Estimation Spectral estimation is a well-studied subject in statistical literature <ref type="bibr" target="#b9">[Cai et al., 2010</ref><ref type="bibr" target="#b10">, Candès and Recht, 2009</ref><ref type="bibr" target="#b21">, Kong and Valiant, 2017</ref>. For our experiments, we use the wellestablished universal singular value thresholding (USVT) proposed by <ref type="bibr" target="#b12">Chatterjee [2015]</ref>.</p><formula xml:id="formula_22">λi = (λi − 2σ √ n)+</formula><p>whereλ i is the i-th empirical singular value and σ is the noise standard deviation. This estimator is shown to be minimax optimal <ref type="bibr" target="#b12">[Chatterjee, 2015]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Dimensionality Selection: LSA, Skip-gram Word2Vec and GloVe</head><p>After estimating the spectrum {λ i } d 1 and the noise σ, we can use the Monte-Carlo procedure described above to estimate the PIP loss. For three popular embedding algorithms: LSA, skip-gram Word2Vec and GloVe, we find their optimal dimensionalities k * that minimize their respective PIP loss. We define the sub-optimality of a particular dimensionality k as the additional PIP loss compared with k</p><formula xml:id="formula_23">* : E k E T k − EE T − E k * E k * T − EE T .</formula><p>In addition, we define the p% sub-optimal interval as the interval of dimensionalities whose sub-optimality are no more than p% of that of a 1-D embedding. In other words, if k is within the p% interval, then the PIP loss of a k-dimensional embedding is at most p% worse than the optimal embedding. We show an example in <ref type="figure" target="#fig_8">Figure 4</ref>.</p><p>LSA with PPMI Matrix For the LSA algorithm, the optimal dimensionalities and sub-optimal intervals around them (5%, 10%, 20% and 50%) for different α values are shown in <ref type="table" target="#tab_0">Table 1</ref>. <ref type="figure" target="#fig_8">Figure  4</ref> shows how PIP losses vary across different dimensionalities. From the shapes of the curves, we can see that models with larger α suffer less from over-parametrization, as predicted in Section 5.1.</p><p>We further cross-validated our theoretical results with intrinsic functionality tests on word relatedness. The empirically optimal dimensionalities that achieve highest correlations with human labels for the two word relatedness tests (WordSim353 and MTurk777) lie close to the theoretically selected k * 's. All of them fall in the 5% interval except when α = 0, in which case they fall in the 20% sub-optimal interval.  Word2Vec with Skip-gram For skip-gram, we use the PMI matrix as its signal matrix <ref type="bibr" target="#b23">[Levy and Goldberg, 2014]</ref>. On the theoretical side, the PIP loss-minimizing dimensionality k * and the sub-optimal intervals (5%, 10%, 20% and 50%) are reported in <ref type="table" target="#tab_1">Table 2</ref>. On the empirical side, the optimal dimensionalities for WordSim353, MTurk771 and Google analogy tests are 56, 102 and 220 respectively for skip-gram. They agree with the theoretical selections: one is within the 5% interval and the other two are within the 10% interval. GloVe For GloVe, we use the log-count matrix as its signal matrix <ref type="bibr" target="#b32">[Pennington et al., 2014]</ref>. On the theoretical side, the PIP loss-minimizing dimensionality k * and sub-optimal intervals (5%, 10%, 20% and 50%) are reported in <ref type="table" target="#tab_2">Table 3</ref>. On the empirical side, the optimal dimensionalities for WordSim353, MTurk771 and Google analogy tests are 220, 860, and 560. Again, they agree with the theoretical selections: two are within the 5% interval and the other is within the 10% interval. The above three experiments show that our method is a powerful tool in practice: the dimensionalities selected according to empirical grid search agree with the PIP-loss minimizing criterion, which can be done simply by knowing the spectrum and noise standard deviation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a theoretical framework for understanding vector embedding dimensionality. We propose the PIP loss, a metric of dissimilarity between word embeddings. We focus on embedding algorithms that can be formulated as explicit or implicit matrix factorizations including the widely-used LSA, skip-gram and GloVe, and reveal a bias-variance trade-off in dimensionality selection using matrix perturbation theory. With this theory, we discover the robustness of word embeddings trained from these algorithms and its relationship to the exponent parameter α. In addition, we propose a dimensionality selection procedure, which consists of estimating and minimizing the PIP loss. This procedure is theoretically justified, accurate and fast. All of our discoveries are concretely validated on real datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Skip-gram Word2Vec maximizes the likelihood of co-occurrence of the center word and context words. The log likelihood is defined as</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>some α ∈ [0, 1]. For illustration purposes, we first consider a special case where α = 0. 4.1 The Bias Variance Trade-off for a Special Case: α = 0</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>4. 2</head><label>2</label><figDesc>The Bias Variance Trade-off for the Generic Case: α ∈ (0, 1]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Theorem 2 .</head><label>2</label><figDesc>Let M = U DV T ,M =ŨDṼ T be the SVDs of the clean and estimated signal matrices. Suppose E = U ·,1:d D α 1:d,1:d is the oracle embedding, andÊ =Ũ ·,1:kD α 1:k,1:k is the trained embedding, for some k</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Lemma 3 .</head><label>3</label><figDesc>For two matrices M andM = M + Z, denote their SVDs as M = U DV T and M =ŨDṼ T . Write the left singular matrices in block form as U = [U 0 , U 1 ],Ũ = [Ũ 0 ,Ũ 1 ], and similarly partition D into diagonal blocks D 0 and D 1 . If the spectrum of D 0 and D 1 has separation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Sensitivity to over-parametrization: theoretical prediction versus empirical results</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: GloVe: over-parametrization does not significantly hurt performance</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PIP loss and its bias-variance trade-off allow for explicit dimensionality selection for LSA</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Optimal dimensionalities for word relatedness tests are close to PIP loss minimizing ones α PIP arg min 5% interval 10% interval 20% interval 50% interval WS353 opt. MT771 opt.</figDesc><table>0 
214 
[164,289] 
[143,322] 
[115,347] 
[62,494] 
127 
116 
0.25 
138 
[95,190] 
[78,214] 
[57,254] 
[23,352] 
146 
116 
0.5 
108 
[61,177] 
[45,214] 
[29,280] 
[9,486] 
146 
116 
0.75 
90 
[39,206] 
[27,290] 
[16,485] 
[5,1544] 
155 
176 
1 
82 
[23,426] 
[16,918] 
[9,2204] 
[3,2204] 
365 
282 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>PIP loss minimizing dimensionalities and intervals for Skip-gram on Text8 corpus Surrogate Matrix arg min +5% interval +10% interval +20% interval +50% interval WS353 MT771 Analogy</figDesc><table>Skip-gram (PMI) 
129 
[67,218] 
[48,269] 
[29,365] 
[9,679] 
56 
102 
220 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>PIP loss minimizing dimensionalities and intervals for GloVe on Text8 corpus Surrogate Matrix arg min +5% interval +10% interval +20% interval +50% interval WS353 MT771 Analogy</figDesc><table>GloVe (log-count) 
719 
[290,1286] 
[160,1663] 
[55,2426] 
[5,2426] 
220 
860 
560 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="32">nd Conference on Neural Information Processing Systems (NeurIPS 2018), Montréal, Canada.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code can be found on GitHub: https://github.com/ziyin-dl/word-embedding-dimensionality-selection</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknoledgements The authors would like to thank Andrea Montanari, John Duchi, Will Hamilton, Dan Jurafsky, Percy Liang, Peng Qi and Greg Valiant for the helpful discussions. We thank Balaji Prabhakar, Pin Pin Tea-mangkornpan and Feiran Wang for proofreading an earlier version and for their suggestions. Finally, we thank the anonymous reviewers for their valuable feedback and suggestions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Word embeddings: Explaining their properties</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<ptr target="http://www.offconvex.org/2016/02/14/word-embeddings-2/" />
		<imprint>
			<date type="published" when="2016-05" />
			<biblScope unit="volume">16</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning principled bilingual mappings of word embeddings while preserving monolingual invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikel</forename><surname>Artetxe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gorka</forename><surname>Labaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eneko</forename><surname>Agirre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2016 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2289" to="2294" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dzmitry</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Don&apos;t count, predict! a systematic comparison of context-counting vs. context-predicting semantic vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Baroni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgiana</forename><surname>Dinu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Germán</forename><surname>Kruszewski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 52nd</title>
		<meeting>the 52nd</meeting>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<title level="m">Annual Meeting of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="238" to="247" />
		</imprint>
	</monogr>
	<note>Long Papers)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Réjean</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Enriching word vectors with subword information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edouard</forename><surname>Grave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<idno>2307-387X</idno>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="135" to="146" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Empirical analysis of predictive algorithms for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>John S Breese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Heckerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kadie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth conference on Uncertainty in artificial intelligence</title>
		<meeting>the Fourteenth conference on Uncertainty in artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1998" />
			<biblScope unit="page" from="43" to="52" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Extracting semantic representations from word co-occurrence statistics: stop-lists, stemming, and svd. Behavior research methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joseph P</forename><surname>Bullinaria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Levy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="890" to="907" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">A singular value thresholding algorithm for matrix completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian-Feng</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuel</forename><forename type="middle">J</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zuowei</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1956" to="1982" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Exact matrix completion via convex optimization. Foundations of Computational mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Emmanuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Candès</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Recht</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">717</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Experiments with LSA scoring: Optimal rank and basis. Computational information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Caron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="157" to="169" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Matrix estimation by universal singular value thresholding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sourav</forename><surname>Chatterjee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Annals of Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="page" from="177" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The rotation of eigenvectors by a perturbation. iii</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chandler</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">Morton</forename><surname>Kahan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Numerical Analysis</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="46" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Eigenwords: spectral word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paramveer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dhillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lyle</forename><forename type="middle">H</forename><surname>Foster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="3035" to="3078" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Placing search in context: The concept revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lev</forename><surname>Finkelstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yossi</forename><surname>Matias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ehud</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zach</forename><surname>Solan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gadi</forename><surname>Wolfman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eytan</forename><surname>Ruppin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 10th international conference on World Wide Web</title>
		<meeting>the 10th international conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="406" to="414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2121" to="2129" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Large-scale learning of word relatedness with constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guy</forename><surname>Halawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gideon</forename><surname>Dror</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evgeniy</forename><surname>Gabrilovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yehuda</forename><surname>Koren</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 18th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1406" to="1414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Diachronic word embeddings reveal statistical laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>William L Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jurafsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09096</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Perturbation theory for linear operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tosio</forename><surname>Kato</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">132</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Spectrum estimation from samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weihao</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Valiant</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2218" to="2247" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Neural architectures for named entity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miguel</forename><surname>Ballesteros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sandeep</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kazuya</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Dyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of NAACL-HLT</title>
		<meeting>NAACL-HLT</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="260" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improving distributional similarity with lessons learned from word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Omer</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Goldberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ido</forename><surname>Dagan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="211" to="225" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Large text compression benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matt</forename><surname>Mahoney</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 26</title>
		<editor>C. J. C. Burges, L. Bottou, M. Welling, Z. Ghahramani, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Symmetric gauge functions and unitarily invariant norms. The quarterly journal of mathematics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leon</forename><surname>Mirsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1960" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="50" to="59" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Abstractive text summarization using sequence-to-sequence rnns and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramesh</forename><surname>Nallapati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bowen</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cicero Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Glar Gulçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">280</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">History and generality of the CS decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Musheng</forename><surname>Paige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Linear Algebra and its Applications</title>
		<imprint>
			<biblScope unit="volume">208</biblScope>
			<biblScope unit="page" from="303" to="326" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">GloVe: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">The SMART retrieval system-experiments in automatic document processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1971" />
			<publisher>Prentice-Hall, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Term-weighting approaches in automatic text retrieval. Information processing &amp; management</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>Salton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Buckley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="513" to="523" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Evaluation methods for unsupervised word embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tobias</forename><surname>Schnabel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Labutov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Mimno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thorsten</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2015 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="298" to="307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Offline bilingual word vectors, orthogonal transformations and the inverted softmax</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steven</forename><surname>Turban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nils</forename><forename type="middle">Y</forename><surname>Hamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hammerla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.03859</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Recursive deep models for semantic compositionality over a sentiment treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Perelygin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Chuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Potts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2013 conference on empirical methods in natural language processing</title>
		<meeting>the 2013 conference on empirical methods in natural language processing</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1631" to="1642" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A statistical interpretation of term specificity and its application in retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Sparck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jones</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of documentation</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="11" to="21" />
			<date type="published" when="1972" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Stochastic perturbation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stewart</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM review</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="579" to="610" />
			<date type="published" when="1990" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Matrix perturbation theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gilbert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji-Guang</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1990" />
			<publisher>Academic press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Domain and function: A dual-space model of semantic relations and compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Artificial Intelligence Research</title>
		<imprint>
			<biblScope unit="volume">44</biblScope>
			<biblScope unit="page" from="533" to="585" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">From frequency to meaning: Vector space models of semantics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Turney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pantel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of artificial intelligence research</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="141" to="188" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Singular vectors under random perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Vu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Random Structures &amp; Algorithms</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="526" to="538" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Das asymptotische verteilungsgesetz der eigenwerte linearer partieller differentialgleichungen (mit einer anwendung auf die theorie der hohlraumstrahlung)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Weyl</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematische Annalen</title>
		<imprint>
			<biblScope unit="volume">71</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="441" to="479" />
			<date type="published" when="1912" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Google&apos;s neural machine translation system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yonghui</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxim</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuan</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qin</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
	</analytic>
	<monogr>
		<title level="m">Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Understand functionality and dimensionality of vector embeddings: the distributional hypothesis, the pairwise inner product loss and its bias-variance trade-off</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.00502</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">DeepProbe: Information directed sequence understanding and chatbot design via recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zi</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Keng-Hao</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruofei</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 23rd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2131" to="2139" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
