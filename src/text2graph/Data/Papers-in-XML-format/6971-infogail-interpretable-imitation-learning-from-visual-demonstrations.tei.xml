<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunzhu</forename><surname>Li</surname></persName>
							<email>liyunzhu@mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
							<email>tsong@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">MIT</orgName>
								<orgName type="institution" key="instit2">Stanford University</orgName>
								<orgName type="institution" key="instit3">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">InfoGAIL: Interpretable Imitation Learning from Visual Demonstrations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>The goal of imitation learning is to mimic expert behavior without access to an explicit reward signal. Expert demonstrations provided by humans, however, often show significant variability due to latent factors that are typically not explicitly modeled. In this paper, we propose a new algorithm that can infer the latent structure of expert demonstrations in an unsupervised way. Our method, built on top of Generative Adversarial Imitation Learning, can not only imitate complex behaviors, but also learn interpretable and meaningful representations of complex behavioral data, including visual demonstrations. In the driving domain, we show that a model learned from human demonstrations is able to both accurately reproduce a variety of behaviors and accurately anticipate human actions using raw visual inputs. Compared with various baselines, our method can better capture the latent structure underlying expert demonstrations, often recovering semantically meaningful factors of variation in the data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A key limitation of reinforcement learning (RL) is that it involves the optimization of a predefined reward function or reinforcement signal <ref type="bibr" target="#b0">[1]</ref><ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref>. Explicitly defining a reward function is straightforward in some cases, e.g., in games such as Go or chess. However, designing an appropriate reward function can be difficult in more complex and less well-specified environments, e.g., for autonomous driving where there is a need to balance safety, comfort, and efficiency.</p><p>Imitation learning methods have the potential to close this gap by learning how to perform tasks directly from expert demonstrations, and has succeeded in a wide range of problems <ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref><ref type="bibr" target="#b9">[10]</ref><ref type="bibr" target="#b10">[11]</ref>. Among them, Generative Adversarial Imitation Learning (GAIL, <ref type="bibr" target="#b11">[12]</ref>) is a model-free imitation learning method that is highly effective and scales to relatively high dimensional environments. The training process of GAIL can be thought of as building a generative model, which is a stochastic policy that when coupled with a fixed simulation environment, produces similar behaviors to the expert demonstrations. Similarity is achieved by jointly training a discriminator to distinguish expert trajectories from ones produced by the learned policy, as in GANs <ref type="bibr" target="#b12">[13]</ref>.</p><p>In imitation learning, example demonstrations are typically provided by human experts. These demonstrations can show significant variability. For example, they might be collected from multiple experts, each employing a different policy. External latent factors of variation that are not explicitly captured by the simulation environment can also significantly affect the observed behavior. For example, expert demonstrations might be collected from users with different skills and habits. The goal of this paper is to develop an imitation learning framework that is able to automatically discover and disentangle the latent factors of variation underlying expert demonstrations. Analogous to the goal of uncovering style, shape, and color in generative modeling of images <ref type="bibr" target="#b13">[14]</ref>, we aim to automatically learn similar interpretable concepts from human demonstrations through an unsupervised manner.</p><p>We propose a new method for learning a latent variable generative model that can produce trajectories in a dynamic environment, i.e., sequences of state-actions pairs in a Markov Decision Process. Not only can the model accurately reproduce expert behavior, but also empirically learns a latent space of the observations that is semantically meaningful. Our approach is an extension of GAIL, where the objective is augmented with a mutual information term between the latent variables and the observed state-action pairs. We first illustrate the core concepts in a synthetic 2D example and then demonstrate an application in autonomous driving, where we learn to imitate complex driving behaviors while recovering semantically meaningful structure, without any supervision beyond the expert trajectories. <ref type="bibr" target="#b0">1</ref> Remarkably, our method performs directly on raw visual inputs, using raw pixels as the only source of perceptual information. The code for reproducing the experiments are available at https://github.com/ermongroup/InfoGAIL.</p><p>In particular, the contributions of this paper are threefold:</p><p>1. We extend GAIL with a component which approximately maximizes the mutual information between latent space and trajectories, similar to InfoGAN <ref type="bibr" target="#b13">[14]</ref>, resulting in a policy where low-level actions can be controlled through more abstract, high-level latent variables. 2. We extend GAIL to use raw pixels as input and produce human-like behaviors in complex high-dimensional dynamic environments. 3. We demonstrate an application to autonomous highway driving using the TORCS driving simulator <ref type="bibr" target="#b14">[15]</ref>. We first demonstrate that the learned policy is able to correctly navigate the track without collisions. Then, we show that our model learns to reproduce different kinds of human-like driving behaviors by exploring the latent variable space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Preliminaries</head><p>We use the tuple (S, A, P, r , ⇢ 0 , ) to define an infinite-horizon, discounted Markov decision process (MDP), where S represents the state space, A represents the action space, P : S ⇥A⇥S ! R denotes the transition probability distribution, r : S ! R denotes the reward function, ⇢ 0 : S ! R is the distribution of the initial state s 0 , and 2 (0, 1) is the discount factor. Let ⇡ denote a stochastic policy ⇡ : S ⇥ A ! [0, 1], and ⇡ E denote the expert policy to which we only have access to demonstrations. The expert demonstrations ⌧ E are a set of trajectories generated using policy ⇡ E , each of which consists of a sequence of state-action pairs. We use an expectation with respect to a policy ⇡ to denote an expectation with respect to the trajectories it generates:</p><formula xml:id="formula_0">E ⇡ [f (s, a)] , E[ P 1 t=0 t f (s t , a t )], where s 0 ⇠ ⇢ 0 , a t ⇠ ⇡(a t |s t ), s t+1 ⇠ P (s t+1 |a t , s t ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Imitation learning</head><p>The goal of imitation learning is to learn how to perform a task directly from expert demonstrations, without any access to the reinforcement signal r. Typically, there are two approaches to imitation learning: 1) behavior cloning (BC), which learns a policy through supervised learning over the stateaction pairs from the expert trajectories <ref type="bibr" target="#b15">[16]</ref>; and 2) apprenticeship learning (AL), which assumes the expert policy is optimal under some unknown reward and learns a policy by recovering the reward and solving the corresponding planning problem. BC tends to have poor generalization properties due to compounding errors and covariate shift <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. AL, on the other hand, has the advantage of learning a reward function that can be used to score trajectories <ref type="bibr" target="#b18">[19]</ref><ref type="bibr" target="#b19">[20]</ref><ref type="bibr" target="#b20">[21]</ref>, but is typically expensive to run because it requires solving a reinforcement learning (RL) problem inside a learning loop.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Generative Adversarial Imitation Learning</head><p>Recent work on AL has adopted a different approach by learning a policy without directly estimating the corresponding reward function. In particular, Generative Adversarial Imitation Learning (GAIL, <ref type="bibr" target="#b11">[12]</ref>) is a recent AL method inspired by Generative Adversarial Networks (GAN, <ref type="bibr" target="#b12">[13]</ref>). In the GAIL framework, the agent imitates the behavior of an expert policy ⇡ E by matching the generated state-action distribution with the expert's distribution, where the optimum is achieved when the distance between these two distributions is minimized as measured by Jensen-Shannon divergence. The formal GAIL objective is denoted as</p><formula xml:id="formula_1">min ⇡ max D2(0,1) S⇥A E ⇡ [log D(s, a)] + E ⇡ E [log(1 D(s, a))] H(⇡)<label>(1)</label></formula><p>where ⇡ is the policy that we wish to imitate ⇡ E with, D is a discriminative classifier which tries to distinguish state-action pairs from the trajectories generated by ⇡ and ⇡ E , and H(⇡) , E ⇡ [ log ⇡(a|s)] is the -discounted causal entropy of the policy ⇡ ✓ <ref type="bibr" target="#b21">[22]</ref>. Instead of directly learning a reward function, GAIL relies on the discriminator to guide ⇡ into imitating the expert policy.</p><p>GAIL is model-free: it requires interaction with the environment to generate rollouts, but it does not need to construct a model for the environment. Unlike GANs, GAIL considers the environment/simulator as a black box, and thus the objective is not differentiable end-to-end. Hence, optimization of GAIL objective requires RL techniques based on Monte-Carlo estimation of policy gradients. Optimization over the GAIL objective is performed by alternating between a gradient step to increase (1) with respect to the discriminator parameters, and a Trust Region Policy Optimization (TRPO, <ref type="bibr" target="#b1">[2]</ref>) step to decrease (1) with respect to ⇡.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Interpretable Imitation Learning through Visual Inputs</head><p>Demonstrations are typically collected from human experts. The resulting trajectories can show significant variability among different individuals due to internal latent factors of variation, such as levels of expertise and preferences for different strategies. Even the same individual might make different decisions while encountering the same situation, potentially resulting in demonstrations generated from multiple near-optimal but distinct policies. In this section, we propose an approach that can 1) discover and disentangle salient latent factors of variation underlying expert demonstrations without supervision, 2) learn policies that produce trajectories which correspond to these latent factors, and 3) use visual inputs as the only external perceptual information.</p><p>Formally, we assume that the expert policy is a mixture of experts</p><formula xml:id="formula_2">⇡ E = {⇡ 0 E , ⇡ 1 E , .</formula><p>. . }, and we define the generative process of the expert trajectory ⌧ E as:</p><formula xml:id="formula_3">s 0 ⇠ ⇢ 0 , c ⇠ p(c), ⇡ ⇠ p(⇡|c), a t ⇠ ⇡(a t |s t ), s t+1 ⇠ P (s t+1 |a t , s t )</formula><p>, where c is a discrete latent variable that selects a specific policy ⇡ from the mixture of expert policies through p(⇡|c) (which is unknown and needs to be learned), and p(c) is the prior distribution of c (which is assumed to be known before training). Similar to the GAIL setting, we consider the apprenticeship learning problem as the dual of an occupancy measure matching problem, and treat the trajectory ⌧ E as a set of state-action pairs. Instead of learning a policy solely based on the current state, we extend it to include an explicit dependence on the latent variable c. The objective is to recover a policy ⇡(a|s, c) as an approximation of ⇡ E ; when c is samples from the prior p(c), the trajectories ⌧ generated by the conditional policy ⇡(a|s, c) should be similar to the expert trajectories ⌧ E , as measured by a discriminative classifier.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Interpretable Imitation Learning</head><p>Learning from demonstrations generated by a mixture of experts is challenging as we have no access to the policies employed by the individual experts. We have to proceed in an unsupervised way, similar to clustering. The original Generative Adversarial Imitation Learning method would fail as it assumes all the demonstrations come from a single expert, and there is no incentive in separating and disentangling variations observed in the data. A method that can automatically disentangle the demonstrations in a meaningful way is thus needed.</p><p>The way we address this problem is to introduce a latent variable c into our policy function, ⇡(a|s, c). Without further constraints over c, applying GAIL directly to this ⇡(a|s, c) could simply ignore c and fail to separate different types of behaviors present in the expert trajectories <ref type="bibr" target="#b1">2</ref> . To incentivize the model to use c as much as possible, we utilize an information-theoretic regularization enforcing that there should be high mutual information between c and the state-action pairs in the generated trajectory. This concept was introduced by InfoGAN <ref type="bibr" target="#b13">[14]</ref>, where latent codes are utilized to discover the salient semantic features of the data distribution and guide the generating process. In particular, the regularization seeks to maximize the mutual information between latent codes and trajectories, denoted as I(c; ⌧ ),which is hard to maximize directly as it requires access to the posterior P (c|⌧ ). Hence we introduce a variational lower bound, L I (⇡, Q), of the mutual information I(c; ⌧ ) 3 :</p><formula xml:id="formula_4">L I (⇡, Q) = E c⇠p(c),a⇠⇡(·|s,c) [log Q(c|⌧ )] + H(c)  I(c; ⌧ )<label>(2)</label></formula><p>where Q(c|⌧ ) is an approximation of the true posterior P (c|⌧ ). The objective under this regularization, which we call Information Maximizing Generative Adversarial Imitation Learning (InfoGAIL), then becomes:</p><formula xml:id="formula_5">min ⇡,Q max D E ⇡ [log D(s, a)] + E ⇡ E [log(1 D(s, a))] 1 L I (⇡, Q) 2 H(⇡)<label>(3)</label></formula><p>where 1 &gt; 0 is the hyperparameter for information maximization regularization term, and 2 &gt; 0 is the hyperparameter for the casual entropy term. By introducing the latent code, InfoGAIL is able to identify the salient factors in the expert trajectories through mutual information maximization, and imitate the corresponding expert policy through generative adversarial training. This allows us to disentangle trajectories that may arise from a mixture of experts, such as different individuals performing the same task.</p><p>To optimize the objective, we use a simplified posterior approximation Q(c|s, a), since directly working with entire trajectories ⌧ would be too expensive, especially when the dimension of the observations is very high (such as images). We then parameterize policy ⇡, discriminator D and posterior approximation Q with weights ✓, ! and respectively. We optimize L I (⇡ ✓ , Q ) with stochastic gradient methods, ⇡ ✓ using TRPO <ref type="bibr" target="#b1">[2]</ref>, and Q is updated using the Adam optimizer <ref type="bibr" target="#b22">[23]</ref>.</p><p>An outline for the optimization procedure is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 InfoGAIL</head><p>Input: Initial parameters of policy, discriminator and posterior approximation ✓ 0 , ! 0 , 0 ; expert trajectories ⌧ E ⇠ ⇡ E containing state-action pairs. Output: Learned policy ⇡ ✓ for i = 0, 1, 2, ... do Sample a batch of latent codes:</p><formula xml:id="formula_6">c i ⇠ p(c) Sample trajectories: ⌧ i ⇠ ⇡ ✓i (c i )</formula><p>, with the latent code fixed during each rollout. Sample state-action pairs i ⇠ ⌧ i and E ⇠ ⌧ E with same batch size. Update ! i to ! i+1 by ascending with gradients</p><formula xml:id="formula_7">!i =Ê i [r !i log D !i (s, a)] +Ê E [r !i log(1 D !i (s, a))] Update i to i+1 by descending with gradients i = 1Ê i [r i log Q i (c|s, a)</formula><p>] Take a policy step from ✓ i to ✓ i+1 , using the TRPO update rule with the following objective:</p><formula xml:id="formula_8">E i [log D !i+1 (s, a)] 1 L I (⇡ ✓i , Q i+1 ) 2 H(⇡ ✓i ) end for</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Reward Augmentation</head><p>In complex and less well-specified environments, imitation learning methods have the potential to perform better than reinforcement learning methods as they do not require manual specification of an appropriate reward function. However, if the expert is performing sub-optimally, then any policy trained under the recovered rewards will be also suboptimal; in other words, the imitation learning agent's potential is bounded by the capabilities of the expert that produced the training data. In many cases, while it is very difficult to fully specify a suitable reward function for a given task, it is relatively straightforward to come up with constraints that we would like to enforce over the policy.</p><p>This motivates the introduction of reward augmentation <ref type="bibr" target="#b7">[8]</ref>, a general framework to incorporate prior knowledge in imitation learning by providing additional incentives to the agent without interfering 3 <ref type="bibr" target="#b13">[14]</ref> presents a proof for the lower bound.</p><p>with the imitation learning process. We achieve this by specifying a surrogate state-based reward ⌘(⇡ ✓ ) = E s⇠⇡ ✓ [r(s)] that reflects our bias over the desired agent's behavior:</p><formula xml:id="formula_9">min ✓, max ! E ⇡ ✓ [log D ! (s, a)] + E ⇡ E [log(1 D ! (s, a))] 0 ⌘(⇡ ✓ ) 1 L I (⇡ ✓ , Q ) 2 H(⇡ ✓ ) (4)</formula><p>where 0 &gt; 0 is a hyperparameter. This approach can be seen as a hybrid between imitation and reinforcement learning, where part of the reinforcement signal for the policy optimization is coming from the surrogate reward and part from the discriminator, i.e., from mimicking the expert. For example, in our autonomous driving experiment below we show that by providing the agent with a penalty if it collides with other cars or drives off the road, we are able to significantly improve the average rollout distance of the learned policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Improved Optimization</head><p>While GAIL is successful in tasks with low-dimensional inputs (in <ref type="bibr" target="#b11">[12]</ref>, the largest observation has 376 continuous variables), few have explored tasks where the input dimension is very high (such as images -110 ⇥ 200 ⇥ 3 pixels as in our driving experiments). In order to effectively learn a policy that relies solely on high-dimensional input, we make the following improvements over the original GAIL framework.</p><p>It is well known that the traditional GAN objective suffers from vanishing gradient and mode collapse problems <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25]</ref>. We propose to use the Wasserstein GAN (WGAN <ref type="bibr" target="#b25">[26]</ref>) technique to alleviate these problems and augment our objective function as follows:</p><formula xml:id="formula_10">min ✓, max ! E ⇡ ✓ [D ! (s, a)] E ⇡ E [D ! (s, a)] 0 ⌘(⇡ ✓ ) 1 L I (⇡ ✓ , Q ) 2 H(⇡ ✓ )<label>(5)</label></formula><p>We note that this modification is especially important in our setting, where we want to model complex distributions over trajectories that can potentially have a large number of modes.</p><p>We also use several variance reduction techniques, including baselines <ref type="bibr" target="#b26">[27]</ref> and replay buffers <ref type="bibr" target="#b27">[28]</ref>.</p><p>Besides the baseline, we have three models to update in the InfoGAIL framework, which are represented as neural networks: the discriminator network D ! (s, a), the policy network ⇡ ✓ (a|s, c), and the posterior estimation network Q (c|s, a). We update D ! using RMSprop (as suggested in the original WGAN paper), and update Q and ⇡ ✓ using Adam and TRPO respectively. We include the detailed training procedure in Appendix C. To speed up training, we initialize our policy from behavior cloning, as in <ref type="bibr" target="#b11">[12]</ref>.</p><p>Note that the discriminator network D ! and the posterior approximation network Q are treated as distinct networks, as opposed to the InfoGAN approach where they share the same network parameters until the final output layer. This is because the current WGAN training framework requires weight clipping and momentum-free optimization methods when training D ! . These changes would interfere with the training of an expressive Q if D ! and Q share the same network parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We demonstrate the performance of our method by applying it first to a synthetic 2D example and then in a challenging driving domain where the agent is imitating driving behaviors from visual inputs. By conducting experiments on these two environments, we show that our learned policy ⇡ ✓ can 1) imitate expert behaviors using high-dimensional inputs with only a small number of expert demonstrations, 2) cluster expert behaviors into different and semantically meaningful categories, and 3) reproduce different categories of behaviors by setting the high-level latent variables appropriately.</p><p>The driving experiments are conducted in the TORCS (The Open Source Racing Car Simulator, <ref type="bibr" target="#b14">[15]</ref>) environment. The demonstrations are collected by manually driving along the race track, and show typical behaviors like staying within lanes, avoiding collisions and surpassing other cars. The policy accepts raw visual inputs as the only external inputs for the state, and produces a three-dimensional continuous action that consists of steering, acceleration, and braking. We assume that our policies are Gaussian distributions with fixed standard deviations, thus H(⇡) is constant. Each color denotes one specific latent code. Behavior cloning deviates from the expert demonstrations due to compounding errors. GAIL does produce circular trajectories but fails to capture the latent structure for it assumes that the demonstrations are generated from a single expert, and tries to learn an average policy. Our method (InfoGAIL) successfully distinguishes expert behaviors and imitates each mode accordingly (colors are ordered in accordance to the expert for visualization purposes, but are not identifiable).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Learning to Distinguish Trajectories</head><p>We demonstrate the effectiveness of InfoGAIL on a synthetic example. The environment is a 2D plane where the agent can move around freely at a constant velocity by selecting its direction p t at (discrete) time t. For the agent, the observations at time t are positions from t 4 to t. The (unlabeled) expert demonstrations contain three distinct modes, each generated with a stochastic expert policy that produces a circle-like trajectory (see <ref type="figure" target="#fig_0">Figure 1</ref>, panel a). The objective is to distinguish these three distinct modes and imitate the corresponding expert behavior. We consider three methods: behavior cloning, GAIL and InfoGAIL (details included in Appendix A). In particular, for all the experiments we assume the same architecture and that the latent code is a one-hot encoded vector with 3 dimensions and a uniform prior; only InfoGAIL regularizes the latent code. <ref type="figure" target="#fig_0">Figure 1</ref> shows that the introduction of latent variables allows InfoGAIL to distinguish the three types of behavior and imitate each behavior successfully; the other two methods, however, fail to distinguish distinct modes. BC suffers from the compounding error problem and the learned policy tends to deviate from the expert trajectories; GAIL does learn to generate circular trajectories but it fails to separate different modes due to the lack of a mechanism that can explicitly account for the underlying structure.</p><p>In the rest of Section 4, we show how InfoGAIL can infer the latent structure of human decisionmaking in a driving domain. In particular, our agent only relies on visual inputs to sense the environment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Utilizing Raw Visual Inputs via Transfer Learning</head><p>The high dimensional nature of visual inputs poses a significant challenges to learning a policy. Intuitively, the policy will have to simultaneously learn how to identify meaningful visual features, and how to leverage them to achieve the desired behavior using only a small number of expert demonstrations. Therefore, methods to mitigate the high sample complexity of the problem are crucial to success in this domain.</p><p>In this paper, we take a transfer learning approach. Features extracted using a CNN pre-trained on ImageNet contain high-level information about the input images, which can be adapted to new vision tasks via transfer learning <ref type="bibr" target="#b28">[29]</ref>. However, it is not yet clear whether these relatively high-level features can be directly applied to tasks where perception and action are tightly interconnected; we demonstrate that this is possible through our experiments. We perform transfer learning by exploiting features from a pre-trained neural network that effectively convert raw images into relatively highlevel information <ref type="bibr" target="#b29">[30]</ref>. In particular, we use a Deep Residual Network <ref type="bibr" target="#b30">[31]</ref> pre-trained on the ImageNet classification task <ref type="bibr" target="#b31">[32]</ref> to obtain the visual features used as inputs for the policy network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Network Structure</head><p>Our policy accepts certain auxiliary information as internal input to serve as a short-term memory. This auxiliary information can be accessed along with the raw visual inputs. In our experiments, the auxiliary information for the policy at time t consists of the following: 1) velocity at time t, which is a three dimensional vector; 2) actions at time t 1 and t 2, which are both three dimensional vectors; 3) damage of the car, which is a real value. The auxiliary input has 10 dimensions in total. , which suggests that, to some extent, our method is able to generalize to cases previously unseen in the training data.</p><p>For the policy network, input visual features are passed through two convolutional layers, and then combined with the auxiliary information vector and (in the case of InfoGAIL) the latent code c. We parameterize the baseline as a network with the same architecture except for the final layer, which is just a scalar output that indicates the expected accumulated future rewards.</p><p>The discriminator D ! accepts three elements as input: the input image, the auxiliary information, and the current action. The output is a score for the WGAN training objective, which is supposed to be lower for expert state-action pairs, and higher for generated ones. The posterior approximation network Q adopts the same architecture as the discriminator, except that the output is a softmax over the discrete latent variables or a factored Gaussian over continuous latent variables. We include details of our architecture in Appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Interpretable Imitation Learning from Visual Demonstrations</head><p>In this experiment, we consider two subsets of human driving behaviors: turn, where the expert takes a turn using either the inside lane or the outside lane; and pass, where the expert passes another vehicle from either the left or the right. In both cases, the expert policy has two significant modes. Our goal is to have InfoGAIL capture these two separate modes from expert demonstrations in an unsupervised way.</p><p>We use a discrete latent code, which is a one-hot encoded vector with two possible states. For both settings, there are 80 expert trajectories in total, with 100 frames in each trajectory; our prior for the latent code is a uniform discrete distribution over the two states. The performance of a learned policy is quantified with two metrics: the average distance is determined by the distance traveled by the agent before a collision (and is bounded by the length of the simulation horizon), and accuracy is defined as the classification accuracy of the expert state-action pairs according to the latent code inferred with Q . We add constant reward at every time step as reward augmentation, which is used to encourage the car to "stay alive" as long as possible and can be regarded as another way of reducing collision and off-lane driving (as these will lead to the termination of that episode).</p><p>The average distance and sampled trajectories at different stages of training are shown in <ref type="figure" target="#fig_1">Figures 2 and  3</ref> for turn and pass respectively. During the initial stages of training, the model does not distinguish the two modes and has a high chance of colliding and driving off-lane, due to the limitations of behavior cloning (which we used to initialize the policy). As training progresses, trajectories provided by the learned policy begin to diverge. Towards the end of training, the two types of trajectories are clearly distinguishable, with only a few exceptions. In turn, [0, 1] corresponds to using the inside lane, while <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">0]</ref> corresponds to the outside lane. In pass, the two kinds of latent codes correspond to passing from right and left respectively. Meanwhile, the average distance of the rollouts steadily increases with more training.</p><p>Learning the two modes separately requires accurate inference of the latent code. To examine the accuracy of posterior inference, we select state-action pairs from the expert trajectories (where the state is represented as a concatenation of raw image and auxiliary variables) and obtain the corresponding latent code through Q (c|s, a); see <ref type="table" target="#tab_0">Table 1</ref>. Although we did not explicitly provide any label, our model is able to correctly distinguish over 81% of the state-action pairs in pass (and almost all the pairs in turn, confirming the clear separation between generated trajectories with different latent codes in <ref type="figure" target="#fig_1">Figure 2</ref>).   For comparison, we also visualize the trajectories of pass for the original GAIL objective in <ref type="figure" target="#fig_2">Figure 3</ref>, where there is no mutual information regularization. GAIL learns the expert trajectories as a whole, and cannot distinguish the two modes in the expert policy.</p><p>Interestingly, instead of learning two separate trajectories, GAIL tries to fit the left trajectory by swinging the car suddenly to the left after it has surpassed the other car from the right. We believe this reflects a limitation in the discriminators. Since D ! (s, a) only requires state-action pairs as input, the policy is only required to match most of the state-action pairs; matching each rollout in a whole with expert trajectories is not necessary. InfoGAIL with discrete latent codes can alleviate this problem by forcing the model to learn separate trajectories.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Ablation Experiments</head><p>We conduct a series of ablation experiments to demonstrate that our proposed improved optimization techniques in Section 3.2 and 3.3 are indeed crucial for learning an effective policy. Our policy drives a car on the race track along with other cars, whereas the human expert provides 20 trajectories with 500 frames each by trying to drive as fast as possible without collision. Reward augmentation is performed by adding a reward that encourages the car to drive faster. The performance of the policy is determined by the average distance. Here a longer average rollout distance indicates a better policy.</p><p>In our ablation experiments, we selectively remove some of the improved optimization methods from Section 3.2 and 3.3 (we do not use any latent code in these experiments). InfoGAIL(Ours) includes all the optimization techniques; GAIL excludes all the techniques; InfoGAIL\WGAN switches the WGAN objective with the GAN objective; InfoGAIL\RA removes reward augmentation; InfoGAIL\RB removes the replay buffer and only samples from the most recent rollouts; Behavior Cloning is the behavior cloning method and Human is the expert policy. <ref type="table" target="#tab_1">Table 2</ref> shows the average rollout distances of different policies. Our method is able to outperform the expert with the help of reward augmentation; policies without reward augmentation or WGANs perform slightly worse than the expert; removing the replay buffer causes the performance to deteriorate significantly due to increased variance in gradient estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Related work</head><p>There are two major paradigms for vision-based driving systems <ref type="bibr" target="#b32">[33]</ref>. Mediated perception is a two-step approach that first obtains scene information and then makes a driving decision <ref type="bibr" target="#b33">[34]</ref><ref type="bibr" target="#b34">[35]</ref><ref type="bibr" target="#b35">[36]</ref>;</p><p>behavior reflex, on the other hand, adopts a direct approach by mapping visual inputs to driving actions <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b15">16]</ref>. Many of the current autonomous driving methods rely on the two-step approach, which requires hand-crafting features such as the detection of lane markings and cars <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33]</ref>. Our approach, on the other hand, attempts to learn these features directly from vision to actions. While mediated perception approaches are currently more prevalent, we believe that end-to-end learning methods are more scalable and may lead to better performance in the long run.</p><p>[39] introduce an end-to-end imitation learning framework that learns to drive entirely from visual information, and test their approach on real-world scenarios. However, their method uses behavior cloning by performing supervised learning over the state-action pairs, which is well-known to generalize poorly to more sophisticated tasks, such as changing lanes or passing vehicles. With the use of GAIL, our method can learn to perform these sophisticated operations easily. <ref type="bibr" target="#b39">[40]</ref> performs end-to-end visual imitation learning in TORCS through DAgger <ref type="bibr" target="#b17">[18]</ref>, querying the reference policies during training, which in many cases is difficult.</p><p>Most imitation learning methods for end-to-end driving rely heavily on LIDAR-like inputs to obtain precise distance measurements <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>. These inputs are not usually available to humans during driving. In particular, <ref type="bibr" target="#b40">[41]</ref> applies GAIL to the task of modeling human driving behavior on highways.</p><p>In contrast, our policy requires only raw visual information as external input, which in practice is all the information humans need in order to drive.</p><p>[42] and <ref type="bibr" target="#b8">[9]</ref> have also introduced a pre-trained deep neural network to achieve better performance in imitation learning with relatively few demonstrations. Specifically, they introduce a pre-trained model to learn dense, incremental reward functions that are suitable for performing downstream reinforcement learning tasks, such as real-world robotic experiments. This is different from our approach, in that transfer learning is performed over the critic instead of the policy. It would be interesting to combine that reward with our approach through reward augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we present a method to imitate complex behaviors while identifying salient latent factors of variation in the demonstrations. Discovering these latent factors does not require direct supervision beyond expert demonstrations, and the whole process can be trained directly with standard policy optimization algorithms. We also introduce several techniques to successfully perform imitation learning using visual inputs, including transfer learning and reward augmentation. Our experimental results in the TORCS simulator show that our methods can automatically distinguish certain behaviors in human driving, while learning a policy that can imitate and even outperform the human experts using visual information as the sole external input. We hope that our work can further inspire end-to-end learning approaches to autonomous driving under more realistic scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Learned trajectories in the synthetic 2D plane environment. Each color denotes one specific latent code. Behavior cloning deviates from the expert demonstrations due to compounding errors. GAIL does produce circular trajectories but fails to capture the latent structure for it assumes that the demonstrations are generated from a single expert, and tries to learn an average policy. Our method (InfoGAIL) successfully distinguishes expert behaviors and imitates each mode accordingly (colors are ordered in accordance to the expert for visualization purposes, but are not identifiable).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Visualizing the training process of turn. Here we show the trajectories of InfoGAIL at different stages of training. Blue and red indicate policies under different latent codes, which correspond to "turning from inner lane" and "turning from outer lane" respectively. The rightmost figure shows the trajectories under latent codes [1, 0] (red), [0, 1] (blue), and [0.5, 0.5] (purple), which suggests that, to some extent, our method is able to generalize to cases previously unseen in the training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Experimental results for pass. Left: Trajectories of InfoGAIL at different stages of training (epoch 1 to 37). Blue and red indicate policies using different latent code values, which correspond to passing from right or left. Middle: Traveled distance denotes the absolute distance from the start position, averaged over 60 rollouts of the InfoGAIL policy trained at different epochs. Right: Trajectories of pass produced by an agent trained on the original GAIL objective. Compared to InfoGAIL, GAIL fails to distinguish between different modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Classification accuracies for pass.</figDesc><table>Method 
Accuracy 

Chance 
50% 
K-means 
55.4% 
PCA 
61.7% 
InfoGAIL (Ours) 
81.9% 

SVM 
85.8% 
CNN 
90.8% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Average rollout distances.</figDesc><table>Method 
Avg. rollout distance 

Behavior Cloning 
701.83 
GAIL 
914.45 
InfoGAIL \ RB 
1031.13 
InfoGAIL \ RA 
1123.89 
InfoGAIL \ WGAN 
1177.72 
InfoGAIL (Ours) 
1226.68 
Human 
1203.51 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">A video showing the experimental results is available at https://youtu.be/YtNPBAW6h5k.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For a fair comparison, we consider this form as our GAIL baseline in the experiments below.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Shengjia Zhao and Neal Jean for their assistance and advice. Toyota Research Institute (TRI) provided funds to assist the authors with their research but this article solely reflects the opinions and conclusions of its authors and not TRI or any other Toyota entity. This research was also supported by Intel Corporation, FLI and NSF grants 1651565, 1522054, 1733686.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Guided policy search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML (3)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1509.02971</idno>
		<title level="m">Continuous control with deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">High-dimensional continuous control using generalized advantage estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.02438</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Value iteration networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tamar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Thomas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2146" to="2154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Maximum entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Dey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<meeting><address><addrLine>Chicago, IL, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page" from="1433" to="1438" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inverse kkt-learning cost functions of manipulation tasks from demonstrations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Englert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Toussaint</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Symposium of Robotics Research</title>
		<meeting>the International Symposium of Robotics Research</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Guided cost learning: Deep inverse optimal control via policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Third person imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning large-scale dynamic discrete choice models of spatiotemporal preferences with application to migratory pastoralism in east africa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Damoulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Clark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Degloria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mude</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barrett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="644" to="650" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Generative adversarial imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4565" to="4573" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Torcs, the open racing car simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wymann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Espié</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guionneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dimitrakakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Coulom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sumner</surname></persName>
		</author>
		<ptr target="http://torcs.sourceforge.net" />
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Efficient training of artificial neural networks for autonomous navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient reductions for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3" to="5" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A reduction of imitation learning and structured prediction to no-regret online learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Gordon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Apprenticeship learning via inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the twenty-first international conference on Machine learning</title>
		<meeting>the twenty-first international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Apprenticeship learning using linear programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Syed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1032" to="1039" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Model-free imitation learning with policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Infinite time horizon maximum causal entropy inverse reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bambos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE 53rd Annual Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4911" to="4916" />
		</imprint>
	</monogr>
	<note>Decision and Control (CDC)</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Wasserstein gan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">3-4</biblScope>
			<biblScope unit="page" from="229" to="256" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Cnn features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Real time detection of lane markers in urban streets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="7" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sparse scene flow segmentation for moving object detection in urban environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ziegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Roser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Vehicles Symposium (IV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="926" to="932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Activity forecasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kitani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ziebart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="201" to="214" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Alvinn, an autonomous land vehicle in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">tech. rep</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
		<respStmt>
			<orgName>Carnegie Mellon University, Computer Science Department</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1231" to="1237" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Query-efficient imitation learning for end-to-end autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Imitating driver behavior with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kuefler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Morton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wheeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kochenderfer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06699</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Unsupervised perceptual rewards for imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06699</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
