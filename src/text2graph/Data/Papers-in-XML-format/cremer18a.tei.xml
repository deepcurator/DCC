<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inference Suboptimality in Variational Autoencoders</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Cremer</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuechen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Duvenaud</surname></persName>
						</author>
						<title level="a" type="main">Inference Suboptimality in Variational Autoencoders</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Amortized inference allows latent-variable models trained via variational learning to scale to large datasets. The quality of approximate inference is determined by two factors: a) the capacity of the variational distribution to match the true posterior and b) the ability of the recognition network to produce good variational parameters for each datapoint. We examine approximate inference in variational autoencoders in terms of these factors. We find that divergence from the true posterior is often due to imperfect recognition networks, rather than the limited complexity of the approximating distribution. We show that this is due partly to the generator learning to accommodate the choice of approximation. Furthermore, we show that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper, we analyze inference suboptimality: the mismatch between the true and approximate posterior. More specifically, we are interested in understanding what factors cause the gap between the marginal log-likelihood and the evidence lower bound (ELBO) in variational autoencoders (VAEs, <ref type="bibr" target="#b13">Kingma &amp; Welling (2014)</ref>; <ref type="bibr" target="#b23">Rezende et al. (2014)</ref>). We refer to this as the inference gap. Moreover, we break down the inference gap into two components: the approximation gap and the amortization gap. The approximation gap comes from the inability of the variational distribution family to exactly match the true posterior. The amortization gap refers to the difference caused by amortizing the variational parameters over the entire training set, instead of optimizing for each training example individually. We refer the reader to <ref type="table">Table 1</ref> for the definitions of the gaps and to <ref type="figure" target="#fig_0">Fig. 1</ref> for a simple illustration of the gaps. In <ref type="figure" target="#fig_0">Fig. 1</ref>, L <ref type="bibr">[q]</ref> refers to the ELBO evaluated using an amortized distribution q, as is typical of VAE training. In contrast, L[q ⇤ ] is the ELBO evaluated using the optimal approximation within its variational family.</p><p>There has been significant work on improving variational inference in VAEs through the development of expressive approximate posteriors <ref type="bibr" target="#b22">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b14">Kingma et al., 2016;</ref><ref type="bibr" target="#b21">Ranganath et al., 2016;</ref><ref type="bibr" target="#b27">Tomczak &amp; Welling, 2016;</ref><ref type="bibr" target="#b9">2017)</ref>. These works have shown that with more expressive approximate posteriors, the model learns a better distribution over the data. Our study aims to gain a better understanding of the relationship between expressive approximations and improved generative models.</p><p>Our experiments investigate how the choice of encoder, posterior approximation, decoder, and optimization affect the approximation and amortization gaps. We train VAE models in a number of settings on the MNIST <ref type="bibr" target="#b18">(LeCun et al., 1998)</ref>, Fashion-MNIST <ref type="bibr" target="#b31">(Xiao et al., 2017)</ref>, and CIFAR-10 ( <ref type="bibr" target="#b16">Krizhevsky &amp; Hinton, 2009</ref>) datasets.</p><p>Our contributions are: a) we investigate inference suboptimality in terms of the approximation and amortization gaps, providing insight to guide future improvements in VAE inference, b) we quantitatively demonstrate that the learned generative model accommodates the choice of approximation, and c) we demonstrate that parameterized functions that improve the expressiveness of the approximation play a significant role in reducing amortization error.  <ref type="table">Table 1</ref>. Summary of Gap Terms. The middle column refers to the general case where our variational objective is a lower bound on the marginal log-likelihood. The right most column demonstrates the specific case in VAEs. q ⇤ (z|x) refers to the optimal approximation within a family Q, i.e. q ⇤ (z|x) = arg min q2Q KL (q(z|x)||p(z|x)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>log $(&amp;)</head><note type="other">ℒ</note><formula xml:id="formula_0">Approximation log p(x) L[q ⇤ ] KL (q ⇤ (z|x)||p(z|x)) Amortization L[q ⇤ ] L[q] KL (q(z|x)||p(z|x)) KL (q ⇤ (z|x)||p(z|x))</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Inference in Variational Autoencoders</head><p>Let x be the observed variable, z the latent variable, and p(x, z) be their joint distribution. Given a dataset X = {x 1 , x 2 , ..., x N }, we would like to maximize the marginal log-likelihood with respect to the model parameters ✓:</p><formula xml:id="formula_1">log p ✓ (X) = N X i=1 log p ✓ (x i ) = N X i=1 log Z p ✓ (x i , z i )dz i .</formula><p>In practice, the marginal log-likelihood is computationally intractable due to the integration over the latent variable z. Instead, VAEs introduce an inference network q (z|x) to approximate the true posterior p(z|x) and optimize the ELBO with respect to model parameters ✓ and inference network parameters (parameterization subscripts omitted for brevity):</p><formula xml:id="formula_2">log p(x) = E q(z|x)  log ✓ p(x, z) q(z|x) ◆ + KL (q(z|x)||p(z|x))<label>(1)</label></formula><formula xml:id="formula_3">E q(z|x)  log ✓ p(x, z) q(z|x) ◆ = L VAE [q].<label>(2)</label></formula><p>From the above equation, we see that the ELBO is tight when q(z|x) = p(z|x). The choice of q(z|x) is often a factorized Gaussian distribution for its simplicity and efficiency. By utilizing the inference network (also referred to as encoder or recognition network), VAEs amortize inference over the entire dataset. Furthermore, the overall model is trained by stochastically optimizing the ELBO using the reparametrization trick <ref type="bibr" target="#b13">(Kingma &amp; Welling, 2014)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Expressive Approximate Posteriors</head><p>There are a number of strategies for increasing the expressiveness of approximate posteriors, going beyond the original factorized-Gaussian. We briefly summarize normalizing flows and auxiliary variables.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1.">NORMALIZING FLOWS</head><p>Normalizing flow <ref type="bibr" target="#b22">(Rezende &amp; Mohamed, 2015)</ref> is a change of variables procedure for constructing complex distributions by transforming probability densities through a series of invertible mappings. Specifically, if we transform a random variable z 0 with distribution q 0 (z), the resulting random variable z T = T (z 0 ) has a distribution:</p><formula xml:id="formula_4">q T (z T ) = q 0 (z 0 ) det @z T @z 0 1 .<label>(3)</label></formula><p>By successively applying these transformations, we can build arbitrarily complex distributions. Stacking these transformations remains tractable due to the determinant being decomposable: det(AB) = det(A)det(B). An important property of these transformations is that we can take expectations with respect to the transformed density q T (z T ) without explicitly knowing its formula due to the law of the unconscious statistician (LOTUS):</p><formula xml:id="formula_5">E q T [h(z T )] = E q0 [h(f T (f T 1 (...f 1 (z 0 ))))].<label>(4)</label></formula><p>Using equations <ref type="formula" target="#formula_4">(3)</ref> and <ref type="formula" target="#formula_5">(4)</ref>, the lower bound with the transformed approximation can be written as:</p><formula xml:id="formula_6">E z0⇠q0(z|x) 2 6 4log 0 B @ p(x, z T ) q 0 (z 0 |x) Q T t=1 det @zt @zt 1 1 1 C A 3 7 5 . (5)</formula><p>The main constraint on these transformations is that the determinant of their Jacobian needs to be easily computable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.2.">AUXILIARY VARIABLES</head><p>Deep generative models can be extended with auxiliary variables which leave the generative model unchanged but make the variational distribution more expressive. Just as hierarchical Bayesian models induce dependencies between data, hierarchical variational models can induce dependencies between latent variables. The addition of the auxiliary variable changes the lower bound to:</p><formula xml:id="formula_7">E z,v⇠q(z,v|x)  log ✓ p(x, z)r(v|x, z) q(z, v|x) ◆ (6) = E q(z|x)  log ✓ p(x, z) q(z|x) ◆ KL ⇣ q(v|z, x)kr(v|x, z) ⌘<label>(7)</label></formula><p>where r(v|x, z) is called the reverse model. From Eqn. 7, we see that this bound is looser than the regular ELBO, however the extra flexibility provided by the auxiliary variable can result in a higher lower bound. This idea has been employed in works such as auxiliary deep generative models (ADGM, ), hierarchical variational models (HVM, <ref type="bibr" target="#b21">(Ranganath et al., 2016)</ref>) and Hamiltonian variational inference (HVI, <ref type="bibr" target="#b25">(Salimans et al., 2015)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Approximation and Amortization Gaps</head><p>The inference gap G is the difference between the marginal log-likelihood log p(x) and a lower bound L <ref type="bibr">[q]</ref>. Given the distribution in the family that maximizes the bound,</p><formula xml:id="formula_8">q ⇤ (z|x) = arg max q2Q L[q]</formula><p>, the inference gap decomposes as the sum of approximation and amortization gaps:</p><formula xml:id="formula_9">G = log p(x) L[q] = log p(x) L[q ⇤ ] | {z } Approximation + L[q ⇤ ] L[q] | {z } Amortization .</formula><p>For VAEs, we can translate the gaps to KL divergences by rearranging Eqn. <ref type="formula" target="#formula_2">(1)</ref>:</p><formula xml:id="formula_10">G VAE = KL q ⇤ (z|x)||p(z|x) | {z } Approximation + KL q(z|x)||p(z|x) KL q ⇤ (z|x)||p(z|x) | {z } Amortization .<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Flexible Approximate Posteriors</head><p>Our experiments involve expressive approximations which use flow transformations and auxiliary variables. The flow transformation that we employ is of the same type as the transformations of Real NVP <ref type="bibr" target="#b3">(Dinh et al., 2017)</ref>. We partition the latent variable z into two, z 1 and z 2 , then perform the following transformations:</p><formula xml:id="formula_11">z 0 1 = z 1 1 (z 2 ) + µ 1 (z 2 ) (9) z 0 2 = z 2 2 (z 0 1 ) + µ 2 (z 0 1 )<label>(10)</label></formula><p>where 1 , 2 , µ 1 , µ 2 : R n ! R n are differentiable mappings parameterized by neural nets and takes the Hadamard or element-wise product. We partition the latent variable by simply indexing the elements of the first half and the second half. The determinant of the combined transformation's Jacobian, det</p><formula xml:id="formula_12">⇣ @z 0 @z ⌘</formula><p>, can be easily evaluated. See section 7.3 of the Supplementary material for a derivation. The lower bound of this approximation is the same as Eqn. <ref type="formula">(5)</ref>. We refer to this approximation as q F low .</p><p>We also experiment with an approximation that combines flow transformations and auxiliary variables. Let z 2 R n be the variable of interest and v 2 R n the auxiliary variable. The flow is the same as equations <ref type="formula">(9)</ref> and <ref type="formula" target="#formula_2">(10)</ref>, where z 1 is replaced with z and z 2 with v. We refer to this approximate distribution as q AF , where AF stands for auxiliary flow. We train this model by optimizing the following bound:</p><formula xml:id="formula_13">E q0(z,v|x) 2 6 4log 0 B @ p(x, z T )r(v T |x, z T ) q T (z T , v T |x) det ⇣ @ztvt @zt 1 vt 1 ⌘ 1 1 C A 3 7 5 = L[q AF ].<label>(11)</label></formula><p>Note that this lower bound is looser as explained in Section 2.2.2. We refer readers to Section 7.0.2 in the Supplementary material for specific details of the flow configuration adopted in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Marginal Log-Likelihood Estimation and Evidence Lower Bounds</head><p>In this section, we describe the estimates we use to compute the bounds of the inference gaps:</p><formula xml:id="formula_14">log p(x), L[q ⇤ ], and L[q]</formula><p>. We use two bounds to estimate the marginal log-likelihood, log p(x): IWAE <ref type="bibr" target="#b1">(Burda et al., 2016)</ref> and AIS <ref type="bibr" target="#b20">(Neal, 2001</ref>).</p><p>The IWAE bound takes multiple importance weighted samples from the variational q distribution resulting in a tighter lower bound than the VAE bound. The IWAE bound is computed as:</p><formula xml:id="formula_15">log p(x) E z1...z k ⇠q(z|x) " log 1 k k X i=1 p(x, z i ) q(z i |x) !# (12) = L IWAE [q].</formula><p>As the number of importance samples approaches infinity, the bound approaches the marginal log-likelihood. It is often used as an evaluation metric for generative models <ref type="bibr" target="#b1">(Burda et al., 2016;</ref><ref type="bibr" target="#b14">Kingma et al., 2016)</ref>. AIS is potentially an even tighter lower bound. AIS weights samples from distributions which are sequentially annealed from an initial proposal distribution to the true posterior. See Section 7.4 in the Supplementary material for further details regarding AIS. To compute the AIS bound, we use 100 chains, each with 10000 intermediate distributions, where each transition consists of one HMC trajectory with 10 leapfrog steps. The initial distribution for AIS is the prior, so that it is encoderindependent.</p><p>We estimate the marginal log-likelihood by independently computing our tightest lower bounds then take the maximum of the two:  </p><formula xml:id="formula_16">logp(x) = max(L AIS , L IWAE ).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Local Optimization of the Approximate Distribution</head><p>To compute L VAE [q ⇤ ], we optimize the parameters of the variational distribution for every datapoint. For the local optimization of q F F G , we initialize the mean and variance as the prior, i.e. N (0, I). We optimize the mean and variance using the Adam optimizer with a learning rate of 10 3 . To determine convergence, after every 100 optimization steps, we compute the average of the previous 100 ELBO values and compare it to the best achieved average. If it does not improve for 10 consecutive iterations then the optimization is terminated. For q F low and q AF , the same process is used to optimize all of its parameters. All neural nets for the flow were initialized with a variant of the Xavier initilization <ref type="bibr" target="#b5">(Glorot &amp; Bengio, 2010)</ref>. We use 100 Monte Carlo samples to compute the ELBO to reduce variance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Validation of Bounds</head><p>The soundness of our empirical analysis depends on the reliability of the marginal log-likelihood estimator. For general importance sampling based estimators, the sample variance of the normalized importance weights can serve as an indicator of accuracy <ref type="bibr" target="#b4">(Geweke, 1989;</ref><ref type="bibr" target="#b20">Neal, 2001</ref>). This quantitative measure, however, can also be unreliable, e.g. when the proposal misses an important mode of the target distribution <ref type="bibr" target="#b20">(Neal, 2001)</ref>.</p><p>In this work, we follow <ref type="bibr" target="#b30">(Wu et al., 2017)</ref> to empirically validate our AIS estimates with Bidirectional Monte Carlo (BDMC, <ref type="bibr" target="#b7">Grosse et al. (2015;</ref>). In addition to a lower bound provided by AIS, BDMC runs AIS chains backward from exact posterior samples to obtain an upper bound on the marginal log-likelihood. It should be noted that BDMC relies on the assumption that the distribution of the simulated data from the model roughly matches that of the real data. This is due to the backward chain initializes from exact posterior samples <ref type="bibr" target="#b7">(Grosse et al., 2015)</ref>.</p><p>For the MNIST and Fashion datasets, BDMC gives a gap within 0.1 nat for a linear schedule AIS with 10 4 intermediate distributions and 100 importance samples on 10 3 simulated datapoints. For 3-BIT CIFAR, the same AIS setting gives a gap within 1 nat with the sigmoidial annealing schedule <ref type="bibr" target="#b7">(Grosse et al., 2015)</ref> on 100 simulated datapoints. Loosely speaking, this should give us confidence in how well our AIS lower bounds reflect the marginal loglikelihood computed on the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Much of the earlier work on variational inference focused on optimizing the variational parameters locally for each datapoint, e.g. the original Stochastic Variational Inference scheme (SVI, <ref type="bibr" target="#b10">Hoffman et al. (2013)</ref>). To scale inference to large datasets, most related works utilize inference networks to amortize the cost of inference over the entire dataset.  <ref type="table">Table 2</ref>. Inference Gaps. The columns qFFG and qAF refer to the variational distribution used for training the model. These lower bounds are computed on the training set and are in units of nats.</p><formula xml:id="formula_17">MNIST Fashion-MNIST 3-BIT CIFAR q F F G q AF q F F G q AF q F F G q AF logp(x) -</formula><p>Our work analyses the error that these inference networks introduce.</p><p>Most relevant to our work is the recent work of <ref type="bibr" target="#b15">Krishnan et al. (2017)</ref>, which explicitly remarks on two sources of error in variational learning with inference networks, and proposes to optimize approximate inference locally from an initialization output by the inference network. They show improved training on high-dimensional, sparse data with the hybrid method, claiming that local optimization reduces the negative effects of random initialization in the inference network early on in training. Thus their work focuses on reducing the amortization gap early on in training. Similar to this idea, <ref type="bibr" target="#b9">Hoffman (2017)</ref> proposes to perform approximate inference during model training with MCMC at an initialization given by a variational distribution. Our work provides a means of explaining these improvements in terms of the sources of inference suboptimality that they reduce.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Intuition through Visualization</head><p>To begin, we would like to gain an intuitive visualization of the gaps presented in Section 3.1. To this end, we trained a VAE with a two-dimensional latent space on MNIST and in <ref type="figure" target="#fig_1">Fig. 2</ref> we show contour plots of various distributions in the latent space. The first row contains contour plots of the true posteriors p(z|x) for four different training datapoints (columns). We have selected these four examples to highlight different inference phenomena. The amortized fully-factorized Gaussian (FFG) row refers to the output of the recognition net, in this case, a FFG approximation. Optimal FFG is the FFG that best fits the posterior of the datapoint. Optimal Flow is the optimal fit of a flexible distribution to the same posterior, where the flexible distribution we use is described in Section 3.2.</p><p>Posterior A is an example of a distribution where a FFG can fit relatively well. Posterior B is an example of a posterior with dependence between dimensions, demonstrating the limitation of having a factorized approximation. Posterior C highlights a shortcoming of performing amortization with a limited-capacity recognition network, where the amortized FFG shares little support with the true posterior. Posterior D is a bi-modal distribution which demonstrates the ability of the flexible approximation to fit to complex distributions, in contrast to the simple FFG approximation. These observations raise the following question: in more typical VAEs, is the amortization of inference the leading cause of the distribution mismatch, or is it the limited expressiveness of the approximation?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Amortization vs Approximation Gap</head><p>In this section, we compare how much the approximation and amortization gaps each contribute to the total inference gap. <ref type="table">Table 2</ref> are results of inference on the training set of MNIST, Fashion-MNIST and 3-BIT CIFAR (a binarized version of CIFAR-10, see Section 7.0.3 for details). For each dataset, we trained models with two different approximate posterior distributions: a fully-factorized Gaussian, q F F G , and the flexible distribution, q AF . Due to the computational cost of optimizing the local parameters for each datapoint, our evaluation is performed on a subset of 1000 datapoints for MNIST and Fashion-MNIST and a subset of 100 datapoints for 3-BIT CIFAR.</p><p>For MNIST, we see that the amortization and approximation gaps each account for nearly half of the inference gap. On the more difficult Fashion-MNIST dataset, the amortization gap is larger than the approximation gap. For CIFAR, we see that the amortization gap is much more significant compared to the approximation gap. Thus, for the three datasets and model architectures that we consider, the amortization gap is likely to be the more prominent cause of inference suboptimality, especially when the dataset becomes more challenging to model. This indicates that improvements in inference will likely be a result of reducing amortization error, rather than approximation errors.</p><p>With these results in mind, would simply increasing the capacity of the encoder improve the amortization gap? We  <ref type="table">Table 2</ref>, we see that, for both datasets and both variational distributions, using a larger encoder results in the inference gap decreasing and the decrease is mainly due to a reduction in the amortization gap.</p><formula xml:id="formula_18">MNIST Fashion-MNIST MNIST Fashion-MNIST q F F G q AF q F F G q AF q F F G q AF q F F G q AF logp(x) -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Influence of Flows on the Amortization Gap</head><p>The common reasoning for increasing the expressiveness of the approximate posterior is to minimize the difference between the true and approximate distributions, i.e. reduce the approximation gap. However, given that the expressive approximation is often accompanied by many additional parameters, we would like to know how much influence it has on the amortization error.</p><p>To investigate this, we trained a VAE on MNIST, discarded the encoder, then retrained encoders with different approximate distributions on the fixed decoder. We fixed the decoder so that the true posterior is constant for all the retrained encoders. The initial encoder was a two-layer MLP with a factorized Gaussian distribution. In order to emphasize a large amortization gap, the retrained encoders had no hidden layers (ie. just linear transformations). For the retraiend encoders, we tested three approximate distributions: fully factorized Gaussian (q F F G ), auxiliary flow (q AV ), and Flow (q F low ). See Section 3.2 for the details of these distributions.</p><p>The inference gaps of the retrained encoders on the training set are shown in <ref type="table">Table 4</ref>. As expected, we observe that the small encoder with q F F G has a very large amortization gap. However, when we use q AF or q F low as the approximate distribution, we see the approximation gap decrease, but more importantly, there is a significant decrease in the amortization gap. This indicates that the parameters used for increasing the complexity of the approximation also play a large role in diminishing the amortization error.  <ref type="table">Table 4</ref>. Influence of expressive approximations on the amortization gap. The parameters used to increase the flexibility of the approximate distribution also reduce the amortization gap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Variational Family</head><formula xml:id="formula_19">q F F G q AF q F low logp(x) -</formula><p>These results are expected given that the parameterization of the Flow distribution can be interpreted as an instance of the RevNet <ref type="bibr" target="#b6">(Gomez et al., 2017)</ref> which has demonstrated that Real-NVP transformations <ref type="bibr" target="#b3">(Dinh et al., 2017)</ref> can model complex functions similar to typical MLPs. Thus the flow transformations we employ should also be expected to increase the expressiveness while also increasing the capacity of the encoder. The implication of this observation is that models which improve the flexibility of their variational approximation, and attribute their improved results to the increased expressiveness, may have actually been due to the reduction in amortization error.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Influence of Approximate Posterior on True Posterior</head><p>To what extent does the posterior approximation affect the learned model? <ref type="bibr" target="#b29">Turner &amp; Sahani (2011)</ref> studied the biases in parameter learning induced by the variational approximation when learning via variational Expectation-Maximization. Similarly, we ask whether a factorized Gaussian approximation causes the true posterior to be more like a factorized Gaussian? <ref type="bibr" target="#b1">Burda et al. (2016)</ref> visually demonstrate that when trained with an importance-weighted approximate posterior, the resulting true posterior is more complex than those trained with factorized Gaussian approximations. Just as it is hard to evaluate a generative model by visually inspecting samples, it is hard to judge how "Gaussian" the true posterior is by visual inspection. We can quantitatively determine how close the posterior is to a fully-factorized Gaussian (FFG) by comparing the marginal log-likelihood estimate logp(x) and the Optimal FFG bound</p><formula xml:id="formula_20">L VAE [q ⇤ F F G ]</formula><p>. This is equivalent to estimating the KL divergence between the optimal Gaussian and the true posterior, KL (q ⇤ (z|x)||p(z|x)).</p><p>In <ref type="table">Table 2</ref> on MNIST, for the FFG trained model, KL (q ⇤ (z|x)||p(z|x)) is nearly the same for both q These observations justify our results of Section 5.2. which showed that the amortization error is often the main cause of inference suboptimality. One reason for this is that the generator accommodates the choice of approximation, thus reducing the approximation error.  Given that we have seen that the generator can accommodate the choice of approximation, our next question is whether a generator with more capacity increases its ability to fit to the approximation. To this end, we trained VAEs with decoders of different sizes and measured the approximation gaps on the training set. Specifically, we trained decoders with 0, 2, and 4 hidden layers on MNIST. See <ref type="table" target="#tab_5">Table 5</ref> for the results. We see that as the capacity of the decoder increases, the approximation gap decreases. This result implies that the more flexible the generator is, the less flexible the approximate distribution needs to be to ensure accurate inference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Inference Generalization</head><p>How well does amortized inference generalize at test time? We address this question by visualizing the gaps on training and validation datapoints across the training epochs. In <ref type="figure" target="#fig_2">Fig. 3</ref>, the models are trained on 50000 binarized Fashion-MNIST datapoints and the gaps are computed on a subset of a 100 training and validation datapoints. The top and bottom boundaries of the blue region represent logp(x) and</p><formula xml:id="formula_21">L[q ⇤ ].</formula><p>The bottom boundary of the orange region represents L <ref type="bibr">[q]</ref>. In other words, the blue region is the approximation gap and the orange is the amortization gap.</p><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, the Standard model (top left) refers to a VAE of latent size 20 trained with a factorized Gaussian approximate posterior. In this case, the encoder and decoder both have two hidden layers each consisting of 200 hidden units. The Flow model (top right) augments the Standard model with a q F low variational distribution. Larger Decoder and Larger Encoder models have factorized Gaussian distributions and increase the number of hidden layers to three and the number of units in each layer to 500.</p><p>Firstly, we observe that for all models, the approximation gap on the training and validation sets are roughly equivalent. This indicates that the true posteriors of the held-out data are similar to that of the training data. Secondly, we note that for all models, the encoder overfits more than the decoder.</p><p>These observations resonate with the encoder overfitting findings by <ref type="bibr" target="#b30">Wu et al. (2017)</ref>.</p><p>How does increasing decoder capacity affect inference on held-out data? We know from Section 5.4 that increasing generator capacity results in a posterior that better fits the approximation making posterior inference easier. Furthermore, the Larger Decoder plot of <ref type="figure" target="#fig_2">Fig. 3</ref> shows that increasing generator capacity causes the model to be more prone to overfitting. Thus, there is a tradeoff between ease of inference and decoder overfitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.1.">ENCODER CAPACITY AND APPROXIMATION EXPRESSIVENESS</head><p>We have seen in Sections 5.2 and 5.3 that expressive approximations as well as increasing encoder capacity can lead to a reduction in the amortization gap. This leads us to the following question: when should we increase encoder capacity versus increasing the expressiveness of the approximation?</p><p>We answer this question in terms of how well each model can generalize its efficient inference (recognition network and variational distribution) to held-out data.</p><p>In <ref type="figure" target="#fig_2">Fig. 3</ref>, we see that the Flow model and the Larger Encoder model achieve similar logp(x) on the validation set at the end of training. However, we see that the L[q] bound of the Larger Encoder model is significantly lower than the L[q] bound of the Flow model due to the encoder overfitting to the training data. Although they both model the data nearly equally well, the recognition net of the Larger Encoder model is no longer suitable to perform inference on the held-out data due to overfitting. Thus a potential rational for utilizing expressive approximations is that they improve generalization to held-out data in comparison to increasing the encoder capacity.</p><p>We highlight that, in many scenarios, efficient test time inference is not required and consequently, encoder overfitting is not an issue, since we can use non-efficient encoderindependent methods to estimate log p(x), such as AIS, IWAE with local optimization, or potentially retraining the encoder on the held-out data. In contrast, when efficient test time inference is required, encoder generalization is important and expressive approximations are likely advantageous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Annealing the Entropy</head><p>Typical warm-up <ref type="bibr" target="#b0">(Bowman et al., 2015;</ref><ref type="bibr" target="#b26">Sønderby et al., 2016)</ref> refers to annealing the KL (q(z|x)||p(z)) term during training. This can also be interpreted as performing maximum likelihood estimation (MLE) early on during training. This optimization technique is known to help prevent the latent variable from degrading to the prior <ref type="bibr" target="#b1">(Burda et al., 2016;</ref><ref type="bibr" target="#b26">Sønderby et al., 2016)</ref>. We employ a similar annealing scheme during training by annealing the entropy of the approximate distribution:</p><formula xml:id="formula_22">E z⇠q(z|x) [log p(x, z) log q(z|x)] ,</formula><p>where is annealed from 0 to 1 over training. This can be interpreted as maximum a posteriori (MAP) in the initial phase of training.</p><p>We find that warm-up techniques, such as annealing the entropy, are important for allowing the true posterior to be more complex. AF ] is significantly smaller without entropy annealing. This indicates that the true posterior is more Gaussian when entropy annealing is not used. This suggests that, in addition to preventing the latent variable from degrading to the prior, entropy annealing allows the true posterior to better utilize the flexibility of the expressive approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we investigated how encoder capacity, approximation choice, decoder capacity, and model optimization influence inference suboptimality in terms of the approximation and amortization gaps. We discovered that the amortization gap can be a leading source to inference suboptimality and that the generator can reduce the approximation gap by learning a true posterior that fits to the choice of approximation. We showed that the parameters used to increase the expressiveness of the approximation play a role in generalizing inference rather than simply improving the complexity of the approximation. We confirmed that increasing the capacity of the encoder reduces the amortization error. Additionally, we demonstrated that optimization techniques, such as entropy annealing, help the generative model to better utilize the flexibility of expressive variational distributions. Analyzing these gaps can be useful for guiding improvements in VAEs. Future work includes evaluating other types of expressive approximations, more complex likelihood functions, and datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Gaps in Inference</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. True Posterior and Approximate Distributions of a VAE with 2D latent space. The columns represent four different datapoints. The green distributions are the true posterior distributions, highlighting the mismatch with the blue approximations. Amortized: Variational parameters learned over the entire dataset. Optimal: Variational parameters optimized for each individual datapoint. Flow: Using a flexible approximate distribution.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Inference gaps over epochs trained on binarized Fashion-MNIST. Blue is the approximation gap. Orange is the amortization gap. Standard is a VAE with FFG approximation. Flow is a VAE with a Flow approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>⇤</head><label></label><figDesc>AF . In contrast, on the model trained with q AF , KL (q ⇤ (z|x)||p(z|x)) is much larger for q ⇤ F F G than q ⇤ AF . This suggests that the true posterior of a FFG-trained model is closer to FFG than the true posterior of the Flow-trained model. The same observation can be made on the Fashion- MNIST dataset. This implies that the decoder can learn to have a true posterior that fits better to the approximation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The L[q ⇤ ] and L[q] bounds are the standard ELBOs, L VAE , from Eqn. (2), computed with either the amortized q or the optimal q ⇤ (see below). When computing L VAE and L IWAE , we use 5000 samples.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Left: Larger Encoder. Right: Models trained without entropy annealing. The columns qFFG and qAF refer to the variational distribution used for training the model. The lower bounds are computed on the training set and are in units of nats.</figDesc><table>89.61 
-88.99 
-95.99 
-96.18 
-89.82 
-89.52 
-102.56 
-102.88 
L VAE [q 

⇤ 

AF ] 
-90.65 
-90.44 
-97.40 
-97.91 
-90.96 
-90.45 
-103.73 
-104.02 
L VAE [q 

⇤ 

F F G ] 
-91.07 
-108.71 
-99.64 
-129.70 
-90.84 
-92.25 
-103.85 
-105.80 
L VAE [q] 
-92.18 
-91.19 
-102.73 
-101.67 
-92.33 
-91.75 
-106.90 
-107.01 
Approximation 
1.46 
1.45 
3.65 
1.73 
1.02 
0.93 
1.29 
1.14 
Amortization 
1.11 
0.75 
3.09 
3.76 
1.49 
1.30 
3.05 
2.29 
Inference 
2.56 
2.20 
6.74 
5.49 
2.51 
2.23 
4.34 
4.13 

examined this by training the MNIST and Fashion-MNIST 
models from above but with larger encoders. See Section 
7.0.2 for implementation details. Table 3 (left) are the results 
of this experiment. Comparing to </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Increased decoder capacity reduces the approximation gap.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 (</head><label>3</label><figDesc>right) are results from a model trained without the entropy annealing schedule. Comparing these results to Table 2, we observe that the difference be- tween L VAE [q ⇤ F F G ] and L VAE [q</figDesc><table>⇤ 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Department of Computer Science, University of Toronto, Toronto, Canada. Correspondence to: Chris Cremer &lt;ccre-mer@cs.toronto.edu&gt;.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generating Sentences from a Continuous Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Importance weighted autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Density estimation using Real NVP</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Bayesian inference in econometric models using monte carlo integration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Geweke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrica: Journal of the Econometric Society</title>
		<imprint>
			<biblScope unit="page" from="1317" to="1339" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2211" to="2221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Sandwiching the marginal likelihood using bidirectional monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02543</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Measuring the reliability of mcmc inference with bidirectional monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ancha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2451" to="2459" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning deep latent gaussian models with markov chain monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Stochastic variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paisley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1303" to="1347" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Nonequilibrium equality for free energy differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jarzynski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Physical Review Letters</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page">2690</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Auto-Encoding Variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Welling</forename></persName>
		</author>
		<title level="m">M. Improving Variational Inference with Inverse Autoregressive Flow. NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On the challenges of learning with inference networks on sparse, high-dimensional data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoffman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Classification using discriminative restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="536" to="543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Auxiliary Deep Generative Models. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<title level="m">Hierarchical Variational Models. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Variational Inference with Normalizing Flows</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Stochastic Backpropagation and Approximate Inference in Deep Generative Models. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of deep belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="872" to="879" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ladder variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Raiko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3738" to="3746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Improving Variational Auto-Encoders using Householder Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Improving Variational Auto-Encoders using convex combination linear Inverse Autoregressive Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Tomczak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Two problems with variational expectation maximisation for time-series models. inference and learning in dynamic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page" from="104" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">On the Quantitative Analysis of Decoder-Based Generative Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms. github.com/zalandoresearch/fashion-mnist</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vollgraf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
