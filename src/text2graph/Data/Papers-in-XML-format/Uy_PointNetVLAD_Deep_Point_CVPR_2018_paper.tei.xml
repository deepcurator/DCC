<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikaela</forename><surname>Angelina</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Uy</forename><surname>Gim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hee</forename><surname>Lee</surname></persName>
							<email>gimhee.lee@comp.nus.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">National University of Singapore</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">PointNetVLAD: Deep Point Cloud Based Retrieval for Large-Scale Place Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Localization addresses the question of "where am I in a given reference map", and it is of paramount importance for robots such as self-driving cars <ref type="bibr" target="#b11">[13]</ref> and drones <ref type="bibr" target="#b9">[11]</ref> to achieve full autonomy. A common method for the localization problem is to first store a map of the environment as a database of 3D point cloud built from a collection of images with Structure-from-Motion (SfM) <ref type="bibr" target="#b13">[15]</ref>, or Li-DAR scans with Simultaneous Localization and Mapping (SLAM) <ref type="bibr" target="#b42">[44]</ref>. Given a query image or LiDAR scan of a local scene, we then search through the database to retrieve the best match that will tell us the exact pose of the query image/scan with respect to the reference map.</p><p>A two-step approach is commonly used in image based localization <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b31">33,</ref><ref type="bibr" target="#b35">37,</ref><ref type="bibr" target="#b48">50]</ref> -(1) place recognition <ref type="bibr" target="#b7">[9,</ref><ref type="bibr" target="#b6">8,</ref><ref type="bibr" target="#b23">25,</ref><ref type="bibr" target="#b44">46,</ref><ref type="bibr" target="#b10">12]</ref>, followed by (2) pose estimation <ref type="bibr" target="#b12">[14]</ref>. In place 1 https://github.com/mikacuy/pointnetvlad.git Figure 1. Two pairs of 3D LiDAR point clouds (top row) and images (bottom row) taken from two different times. It can be seen that the pair of 3D LiDAR point cloud remain largely invariant to the lighting and seasonal changes that made it difficult to match the pair of images. Data from <ref type="bibr" target="#b22">[24]</ref>.</p><p>recognition, a global descriptor is computed for each of the images used in SfM by aggregating local image descriptors, e.g. SIFT, using the bag-of-words approach <ref type="bibr" target="#b24">[26,</ref><ref type="bibr" target="#b39">41]</ref>. Each global descriptor is stored in the database together with the camera pose of its associated image with respect to the 3D point cloud reference map. Similar global descriptor is extracted from the query image and the closest global descriptor in the database can be retrieved via an efficient search. The camera pose of the closest global descriptor would give us a coarse localization of the query image with respect to the reference map. In pose estimation, we compute the exact pose of the query image with the Perspective-n-Point (PnP) <ref type="bibr" target="#b12">[14]</ref> and geometric verification <ref type="bibr" target="#b19">[21]</ref> algorithms.</p><p>The success of image based place recognition is largely attributed to the ability to extract image feature descriptors e.g. SIFT, that are subsequently aggregated with bag-ofwords to get the global descriptor. Unfortunately, there is no algorithm to extract local features similar to SIFT for LiDAR scans. Hence, it becomes impossible to compute global descriptors from the bag-of-word approach to do Li-DAR based place recognition. Most existing approaches circumvent this problem by using readings from the Global Positioning System (GPS) to provide coarse localization, followed by point cloud registration, e.g. the iterative closest point (ICP) <ref type="bibr" target="#b37">[39]</ref> or autoencoder based registration <ref type="bibr" target="#b8">[10]</ref>, for pose-estimation. As a result, LiDAR based localization is largely neglected since GPS might not be always available, despite the fact that much more accurate localization results can be obtained from LiDAR compared to images due to the availability of precise depth information. Furthermore, in comparison to images, the geometric information from LiDARs are invariant to drastic lighting changes, thus making it more robust to perform localization on queries and databases taken from different times of the day, e.g. day and night, and/or different seasons of the year. <ref type="figure">Fig. 1</ref> shows an example of a pair of 3D LiDAR point clouds and images that are taken from the same scene over two different times (daytime in winter on the left column, and nighttime in fall on the right column). It is obvious that the lighting (day and night) and seasonal (with and without snow) changes made it difficult even for human eye to tell that the pair of images (bottom row) are from the same scene. In contrast, the geometric structures of the LiDAR point cloud remain largely unchanged.</p><p>In view of the potential that LiDAR point clouds could be better in the localization task, we propose the PointNetVLAD -a deep network for large-scale 3D point cloud retrieval to fill in the gap of place recognition in the 3D point cloud based localization. Specifically, our PointNetVLAD is a combination of the existing PointNet <ref type="bibr" target="#b25">[27]</ref> and NetVLAD <ref type="bibr" target="#b1">[3]</ref>, which allows end-to-end training and inference to extract the global descriptor from a given 3D point cloud. We provide the proof that NetVLAD is a symmetric function, which is essential for our PointNetVLAD to achieve permutation invariance on the 3D point cloud input. We apply metric learning <ref type="bibr" target="#b5">[7]</ref> to train our PointNetVLAD to effectively learn a mapping function that maps input 3D point clouds to discriminative global descriptors. Additionally, we propose the "lazy triplet and quadruplet" loss functions that achieve more generalizable global descriptors by maximizing the differences between all training examples from their respective hardest negative. We create benchmark datasets for point cloud based retrieval for place recognition based on the open-source Oxford RobotCar dataset <ref type="bibr" target="#b22">[24]</ref> and three additional datasets collected from three different areas with a Velodyne-64 LiDAR mounted on a car. Experimental results on the benchmark datasets verify the feasibility of our PointNetVLAD.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Unlike the maturity of handcrafted local feature extraction for 2D images <ref type="bibr" target="#b21">[23,</ref><ref type="bibr" target="#b3">5]</ref>, no similar methods proposed for 3D point cloud have reached the same level of maturity. In NARF <ref type="bibr" target="#b40">[42]</ref>, Steder et. al. proposed an interest point extraction algorithm for object recognition. In SHOT <ref type="bibr" target="#b43">[45]</ref>, Tombari et. al. suggested a method to extract 3D descriptors for surface matching. However, both <ref type="bibr" target="#b40">[42,</ref><ref type="bibr" target="#b43">45]</ref> rely on stable surfaces for descriptor calculation and are more suitable for dense rigid objects from 3D range images but not for outdoor LiDAR scans. A point-wise histogram based descriptor -FPFH was proposed in <ref type="bibr" target="#b28">[30,</ref><ref type="bibr" target="#b29">31]</ref> for registration. It works on outdoor 3D data but requires high data density, thus making it not scalable to large-scale environments.</p><p>In the recent years, handcrafted features have been increasingly replaced by deep networks that have shown amazing performances. The success of deep learning has been particularly noticeable on 2D images where convolution kernels can be easily applied to the regular 2D lattice grid structure of the image. However, it is more challenging for convolution kernels to work on 3D points that are orderless. Several deep networks attempt to mitigate this challenge by transforming point cloud inputs into regular 3D volumetric representations. Some of these works include: 3D ShapeNets <ref type="bibr" target="#b47">[49]</ref> for recognition, volumetric CNNs <ref type="bibr" target="#b26">[28]</ref> and OctNet <ref type="bibr" target="#b27">[29]</ref> for classification. Additionally, 3DMatch <ref type="bibr" target="#b49">[51]</ref> that learns local descriptors for smallscale indoor scenes and Vote3D <ref type="bibr" target="#b46">[48]</ref> for object detection on the outdoor KITTI dataset. Instead of volumetric representation, MVCNN <ref type="bibr" target="#b41">[43]</ref> projects the 3D point cloud into 2D image planes across multiple views to solve the shape recognition problem. Unfortunately, volumetric representations and 2D projections based deep networks that work well on object and small-scale indoor levels do not scale well for our large-scale outdoor place recognition problem.</p><p>It is not until the recent PointNet <ref type="bibr" target="#b25">[27]</ref> that made it possible for direct input of 3D point cloud. The key to its success is the symmetric max pooling function that enables the aggregation of local point features into a latent representation which is invariant to the permutation of the input points. PointNet focuses on the classification task: shape classification and per-point classification (i.e. part segmentation, scene semantic parsing) on rigid objects and enclosed indoor scenes. PointNet is however not shown to do largescale point cloud based place recognition. Kd-network <ref type="bibr" target="#b17">[19]</ref> also works for unordered point cloud inputs by transforming them into kd-trees. However, it is non-invariant/partiallyinvariant to rotation/noise that are both present in largescale outdoor LiDAR point clouds.</p><p>In <ref type="bibr" target="#b1">[3]</ref>, Arandjelović et. al. proposed the NetVLAD -a deep network that models after the successful bag-of-words approach VLAD <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b2">4]</ref>. The NetVLAD is an end-to-end deep network made up of the VGG/Alexnet <ref type="bibr" target="#b38">[40,</ref><ref type="bibr" target="#b18">20]</ref> for local feature extraction, followed by the NetVLAD aggregation layer for clustering the local features into VLAD global descriptor. NetVLAD is trained on images obtained from the Google Street View Time Machine, a database consist- ing of multiple instances of places taken at different times, to perform the image based place recognition tasks. Results in <ref type="bibr" target="#b1">[3]</ref> show that using the NetVLAD layer significantly outperformed the original non-deep learning based VLAD and its deep learning based max pooling counterpart. Despite the success of NetVLAD for image retrieval, it does not work for our task of point cloud based retrieval since it is not designed to take 3D points as input.</p><p>Our PointNetVLAD leverages on the success of PointNet <ref type="bibr" target="#b25">[27]</ref> and NetVLAD <ref type="bibr" target="#b1">[3]</ref> to do 3D point cloud based retrieval for large-scale place recognition. Specifically, we show that our PointNetVLAD, which is a combination/modification of the PointNet and NetVLAD, originally used for point based classification and image retrieval respectively, is capable of doing end-to-end 3D point cloud based place recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem Definition</head><p>Let us denote the reference map M as a database of 3D points defined with respect to a fixed reference frame. We further define that the reference map M is divided into a collection of M submaps {m 1 , ..., Towards this goal, we design a deep network to learn a function f (.) that maps a given downsampled 3D point cloudp = G(p), where</p><formula xml:id="formula_0">m M } such that M = M i=1 m i .</formula><formula xml:id="formula_1">AOC(p) ≈ AOC(m i ), to a fixed size global descriptor vector f (p) such that d(f (p), f (p r )) &lt; d(f (p), f (p s )), if p is structurally sim- ilar to p r but dissimilar to p s . d(.)</formula><p>is some distance function, e.g. Euclidean distance function. Our problem then simplifies to finding the submap m * ∈ M such that its global descriptor vector f (m * ) gives the minimum distance with the global descriptor vector f (q) from the query q, i.e.</p><formula xml:id="formula_2">d(f (q), f (m * )) &lt; d(f (q), f (m i )), ∀i = * .</formula><p>In practice, this can be done efficiently by a simple nearest neighbor search through a list of global descriptors {f (m i ) | i ∈ 1, 2, .., M } that can be computed once offline and stored in memory, while f (q) is computed online.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Our PointNetVLAD</head><p>In this section, we will describe the network architecture of PointNetVLAD and the loss functions that we designed to learn the function f (.) that maps a downsampled 3D point cloud to a global descriptor. We also show the proof that the NetVLAD layer is permutation invariant, thus suitable for 3D point cloud. <ref type="figure" target="#fig_1">Fig. 2</ref> shows the network architecture of our PointNetVLAD, which is made up of three main components -(1) PointNet <ref type="bibr" target="#b25">[27]</ref>, (2) NetVLAD <ref type="bibr" target="#b1">[3]</ref> and (3) a fully connected network. Specifically, we take the first part of PointNet, cropped just before the maxpool aggregation layer. The input to our network is the same as PointNet, which is a point cloud made up of a set of 3D points, P = p 1 , ..., p N | p n ∈ R 3 . Here, we denote P as a fixed size point cloud after applying the filter G(.); we drop the bar notation on P for brevity. The role of PointNet is to map each point in the input point cloud into a higher dimensional space, i.e.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">The Network Architecture</head><formula xml:id="formula_3">P = p 1 , ..., p N | p n ∈ R 3 −→ P ′ = p ′ 1 , ..., p ′ N | p ′ n ∈ R D , where D ≫ 3.</formula><p>Here, PointNet can be seen as the component that learns to extract a Ddimensional local feature descriptor from each of the input 3D points.</p><p>We feed the output local feature descriptors from Point-Net as input to the NetVLAD layer. The NetVLAD layer is originally designed to aggregate local image features learned from VGG/AlexNet into the VLAD bag-of-words global descriptor vector. By feeding the local feature descriptors of a point cloud into the layer, we create a machinery that generates the global descriptor vector for an input point cloud. The NetVLAD layer learns K cluster centers, i.e. the visual words, denoted as</p><formula xml:id="formula_4">{c 1 , ..., c K | c k ∈ R D }, and outputs a (D × K)-dimensional vector V (P ′ ). The output vector V (P ′ ) = [V 1 (P ′ ), ..., V K (P ′ )]</formula><p>is an aggregated representation of the local feature vectors, where</p><formula xml:id="formula_5">V k (P ′ ) ∈ R D is given by: V k (P ′ ) = n i=1 e w T k p ′ i +b k k ′ e w T k ′ p ′ i +b k ′ (p ′ i − c k ).<label>(1)</label></formula><p>{w k } and {b k } are the weights and biases that determine the contribution of local feature vector p</p><formula xml:id="formula_6">′ i to V k (p ′ ).</formula><p>All the weight and bias terms are learned during training.</p><p>The output from the NetVLAD layer is the VLAD descriptor <ref type="bibr" target="#b15">[17,</ref><ref type="bibr" target="#b2">4]</ref> for the input point cloud. However, the VLAD descriptor is a high dimensional vector, i.e. (D×K)-dimensional vector, that makes it computationally expensive for nearest neighbor search. To alleviate this problem, we use a fully connected layer to compress the (D × K) vector into a compact output feature vector, which is then L2-normalized to produce the final global descriptor vector f (P ) ∈ R O , where O ≪ (D × K), for point cloud P that can be used for efficient retrieval.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Metric Learning</head><p>We train our PointNetVLAD end-to-end to learn the function f (.) that maps an input point cloud P to a discriminative compact global descriptor vector f (P ) ∈ R O , where f (P ) 2 = 1. To this end, we propose the "Lazy Triplet" and "Lazy Quadruplet" losses that can learn discriminative and generalizable global descriptors. We obtain a set of training tuples from the training dataset, where each tuple is denoted as T = (P a , P pos , {P neg }). P a , P pos and {P neg } denote an anchor point cloud, a structurally similar ("positive") point cloud to the anchor and a set of structurally dissimilar ("negative") point clouds to the anchor, respectively. The loss functions are designed to minimize the distance between the global descriptor vectors of P a and P pos , i.e. δ pos = d(f (P a ), f (P pos )), and maximize the distance between the global descriptor vectors of P a and some P negj ∈ {P neg }, i.e. δ negj = d(f (P a ), f (P negj )). d(.) is a predefined distance function, which we take to be the squared Euclidean distance in this work.</p><p>Lazy triplet: For each training tuple T , our lazy triplet loss focuses on maximizing the distance between f (P a ) and the global descriptor vector of the closest/hardest negative in {P neg }, denoted as f (P − negj ). Formally, the lazy triplet loss is defined as</p><formula xml:id="formula_7">L lazyT rip (T ) = max j ([α + δ pos − δ negj ] + ),<label>(2)</label></formula><p>where [. . .] + denotes the hinge loss and α is a constant parameter giving the margin. The max operator selects the closest/hardest negative P − negj in {P neg } that gives the smallest δ negj value in a particular iteration. Note that P − negj of each training tuple changes because the parameters of the network that determine f (.) get updated during training, hence a different point cloud in {P neg } might get mapped to a global descriptor that is nearest to f (P a ) at each iteration. Our choice to iteratively use the closest/hardest negatives over all training tuples ensures that the network learns from all the hardest examples to get a more discriminative and generalizable function f (.).</p><p>Lazy quadruplet: The choice to maximize the distance between f (P a ) and f (P − negj ) might lead to an undesired reduction of the distance between f (P − negj ) and another point cloud f (P f alse ), where P f alse is structurally dissimilar to P − negj . To alleviate this problem, we maximize an additional distance</p><formula xml:id="formula_8">δ neg * k = d(f (P neg * ), f (P neg k ))</formula><p>, where P neg * is randomly sampled from the training dataset at each iteration and is dissimilar to all point clouds in T . The lazy quadruplet loss is defined as</p><formula xml:id="formula_9">L lazyQuad (T , P neg * ) = max j ([α + δ pos − δ negj ] + ) + max k ([β + δ pos − δ neg * k ] + ),<label>(3)</label></formula><p>where β is a another constant parameter giving the margin. The max operator of the second term selects the hardest negative P − neg k in {P neg } that give the smallest δ neg k value.</p><p>Discussion: Original triplet and quadruplet losses use the sum instead of the max operator proposed in our "lazy" variants. These losses have been shown to work well for different applications such as facial recognition <ref type="bibr" target="#b36">[38,</ref><ref type="bibr" target="#b4">6]</ref>. However, maximizing δ negj for all {P neg } leads to a compounding effect where the contribution of each negative training data diminishes as compared to the contribution from a single hardest negative training data. As a result, the original triplet and quadruplet losses tend to take longer to train, and lead to a less discriminative function f (.) that produces inaccurate retrieval results. Experimental results indeed show that both our "lazy" variants outperform the original losses by a competitive margin with the lazy quadruplet loss slightly outperforming the lazy triplet loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Permutation Invariance</head><p>Unlike its image counterpart, a set of points in a point cloud are unordered. Consequently, a naive design of the network could produce different results from different orderings of the input points. It is therefore necessary for the network to be input order invariant for it to be suitable for point clouds. This means that the network will output the same global descriptor f (P ) for point cloud P regardless of the order in which the points in P are arranged. We rigorously show that this property holds for PointNetVLAD. Given an input point cloud P = {p 1 , p 2 , . . . , p N }, the layers prior to NetVLAD, i.e. PointNet, transform each point in P independently into</p><formula xml:id="formula_10">P ′ = {p ′ 1 , p ′ 2 , . . . , p ′ N },</formula><p>hence it remains to show that NetVLAD is a symmetric function, which means that its output V (P ′ ) would be invariant to the order of the points in P ′ leading to an output global descriptor f (P ′ ) that is order invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1 NetVLAD is a symmetric function</head><p>Proof: Given the feature representation of input point cloud P as {p </p><formula xml:id="formula_11">V k = h k (p ′ 1 ) + h k (p ′ 2 ) + . . . + h k (p ′ K ) = K t=1 h k (p ′ t ), (4) where h k (p ′ ) = e w T k p ′ +b k k ′ e w T k ′ p ′ +b k ′ (p ′ − c k ).<label>(5)</label></formula><p>Suppose we have another point cloudP = {p 1 , . . . , p i−1 , p j , p i+1 , . . . , p j−1 , p i , p j+1 , . . . , p N } that is similar to P except for reordered points p i and p j . Then the feature representation ofP is given by {p</p><formula xml:id="formula_12">′ 1 , . . . , p ′ i−1 , p ′ j , p ′ i+1 , . . . , p ′ j−1 , p ′ i , p ′ j+1 , . . . , p ′ N }.</formula><p>Hence ∀k, we havẽ</p><formula xml:id="formula_13">V k =h k (p ′ 1 ) + . . . + h k (p ′ i−1 )+ h k (p ′ j ) + h k (p ′ i+1 ) + . . . + h k (p ′ j−1 )+ h k (p ′ i ) + h k (p ′ j+1 ) + . . . + h k (p ′ N ) = K t=1 h k (p ′ t ) = V k . (6)</formula><p>Thus, f (P ) = f (P ) and completes our proof for symmetry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Benchmark Datasets</head><p>We create four benchmark datasets suitable for LiDARbased place recognition to train and evaluate our network: one from the open-source Oxford RobotCar <ref type="bibr" target="#b22">[24]</ref> and three in-house datasets of a university sector (U.S.), a residential area (R.A.) and a business district (B.D.). These are created using a LiDAR sensor mounted on a car that repeatedly drives through each of the four regions at different times traversing a 10km, 10km, 8km and 5km route on each round of Oxford, U.S., R.A. and B.D., respectively. For each run of each region, the collected LiDAR scans are used to build a unique reference map of the region. The reference map is then used to construct a database of submaps that represent unique local areas of the region for each run. Each reference map is built with respect to the UTM coordinate frame using GPS/INS readings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Submap preprocessing</head><p>The ground planes are removed in all submaps since they are non-informative and repetitive structures. The resulting point cloud is then downsampled to 4096 points using a voxel grid filter <ref type="bibr" target="#b30">[32]</ref>. Next, it is shifted and rescaled to be zero mean and inside the range of [-1, 1]. Each downsampled submap is tagged with a UTM coordinate at its respective centroid, thus allowing supervised training and evaluation of our network. To generate training tuples, we define structurally similar point clouds to be at most 10m apart and those structurally dissimilar to be at least 50m apart. <ref type="figure" target="#fig_3">Fig. 3</ref> shows an example of a reference map, submap and downsampled submap.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data splitting and evaluation</head><p>We split each run of each region of the datasets into two disjoint reference maps used for training and testing. We further split each reference map into a set of submaps at regular intervals of the trajectory in the reference map. Refer to the supplementary material for more details on data splitting. We obtain a total of 30,153 submaps for training and 7572 submaps for test from the Oxford and in-house datasets. To test the performance of our network, we use a submap from a testing reference map as a query point cloud and all submaps from another reference map of a different run that covers the same region as the database. The query submap is successfully localized if it retrieves a point cloud within 25m.</p><p>Oxford Dataset We use 44 sets of full and partial runs from the Oxford RobotCar dataset <ref type="bibr" target="#b22">[24]</ref>, which were collected at different times with a SICK LMS-151 2D LiDAR scanner. Each run is geographically split into 70% and 30% for the training and testing reference maps, respectively. We further split each training and testing reference map into submaps at fixed regular intervals of 10m and 20m, respectively. Each submap includes all 3D points that are within a 20m trajectory of the car. This resulted in 21,711 training submaps, which are used to train our baseline network, and 3030 testing submaps (∼ 120-150 submaps/run).</p><p>In-house Datasets The three in-house datasets are constructed from Velodyne-64 LiDAR scans of five different runs of each of the regions U.S., R.A. and B.D. that were collected at different times. These are all used as testing reference maps to test the generalization of our baseline network trained only on Oxford. Furthermore, we also geographically split each run of U.S. and R.A. into training and testing reference maps, which we use for network refinement. Submaps are taken at regular intervals of 12.5m and 25m for each training and testing reference maps, respectively. All 3D points within a 25m×25m bounding box centered at each submap location are taken. <ref type="table">Table 1</ref> shows the breakdown on the number of training and testing submaps used in the baseline and refined networks.  <ref type="table">Table 1</ref>. Number of training and testing submaps for our baseline and refined networks. * approximate number of submaps/run is given because the number of submaps differ slightly between each run; + overlapping and × disjoint submaps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head><p>We present results to show the feasibility of our PointNetVLAD (PN VLAD) for large-scale point cloud based place recognition. Additionally, we compare its performance to the original PointNet architecture with the maxpool layer (PN MAX) and a fully connected layer to produce a global descriptor with output dimension equal to ours; this is also trained end-to-end for the place recognition task. Moreover, we also compare our network with the state-of-the-art PointNet trained for object classification on rigid objects in ModelNet (PN STD) to investigate whether the model trained on ModelNet can be scaled to large-scale environments. We cut the trained network just before the softmax layer hence producing a 256-dim output vector.   <ref type="table">Table 2</ref>. Baseline results showing the average recall (%) at top 1% for each of the models.</p><p>Baseline Networks We train the PN STD, PN MAX and our PN VLAD using only the Oxford training dataset. The network configurations of PN STD and PN MAX are set to be the same as <ref type="bibr" target="#b25">[27]</ref>. The dimension of the output global descriptor of PN MAX is set to be same as our PN VLAD, i.e. 256-dim. Both PN MAX and our PN VLAD are trained with the lazy quadruplet loss, where we set the margins α = 0.5 and β = 0.2. Furthermore, we set the number of clusters in our PN VLAD to be K = 64. We test the trained networks on Oxford. The Oxford RobotCar dataset is a challenging dataset due to multiple roadworks that caused some scenes to change almost completely. We verify the generalization of our network by testing on completely unseen environments with our in-house datasets. <ref type="table">Table 2</ref> shows the top1% recall of the different models on each of the datasets. It can be seen that PN STD does not generalize well for large scale place retrieval, and PN MAX does not generalize well to the new environments as compared to our PN VLAD. <ref type="figure" target="#fig_8">Fig. 5 (top row)</ref> shows the recall curves of each model for the top 25 matches from each database pair for the four test datasets, where our network outperforms the rest. Note that the recall rate is the average recall rate of all query results from each submap in the test data.    <ref type="table">Table 4</ref>. Results representing the average recall (%) at top1% of PN VLAD tested and trained using different losses on Oxford.</p><p>Output dimensionality analysis We study the discriminative ability of our network over different output dimensions of global descriptor f for both our PN VLAD and PN MAX. As show in <ref type="table">Table 3</ref>, the performance of our PN VLAD with output length of 128-dim is on par with PN MAX with output length of 512-dim on Oxford, and marginally better on our in-house datasets. The performance of our network increases from the output dimension of 128-dim to 256-dim, but did not increase further from 256-dim to 512-dim. Hence, we chose to use an output  <ref type="table">Table 5</ref>. Final results showing the average recall (%) at top 1% (@1%) and at top 1 (@1) after training on Oxford, U.S. and R.A. global descriptor of 256-dim in most of our experiments.</p><p>Comparison between losses We compared our network's performance when trained on different losses. As shown in <ref type="table">Table 4</ref>, our network performs better when trained on our lazy variants of the losses. Hence we chose to use the lazy quadruplet loss to train our PN VLAD and PN MAX.</p><p>Network refinement We further trained our network with U.S. and R.A. in addition to Oxford. This improves the generalizability of our network on the unseen data B.D. as can be seen from the last row of <ref type="table">Table 5</ref> and second row of <ref type="figure" target="#fig_8">Fig. 5-(d)</ref>. We have shown the feasibility and potential of our PointNetVLAD for LiDAR based place recognition by achieving reasonable results despite the smaller database size compared to established databases for image based place recognition (e.g. Google Street View Time Machine and Tokyo 24/7 <ref type="bibr" target="#b44">[46]</ref>). We believe that given more publicly available LiDAR datasets suitable for place recognition our network can further improve its performance and bridge the gap of place recognition in LiDAR based localization. Extended evaluation <ref type="figure" target="#fig_9">Fig. 6-(a)</ref> shows the average recall when queries from Oxford, U.S., R.A. and B.D. are retrieved from an extended database containing all four areas (∼ 33km). Moreover, <ref type="figure" target="#fig_9">Fig. 6-(b)</ref> shows the top 1 recall on unseen data B.D. with varying distance thresholds. It can be seen that on these extended evaluation metrics, our PN VLAD still outperforms PN MAX.</p><p>Image based comparisons under changing scene conditions We compare the performance of our point cloud based approach to the image based counterpart. We train NetVLAD according to the specifications specified in <ref type="bibr" target="#b1">[3]</ref> with images from the center stereo camera of <ref type="bibr" target="#b22">[24]</ref>. These images are taken at the corresponding location of each point cloud submap used to train our PN VLAD. <ref type="figure" target="#fig_9">Fig. 6</ref>-(c) shows retrieval results when query was taken from various scene conditions against an overcast database in the Oxford dataset. The performance of image based NetVLAD is comparable to our point cloud based PN VLAD in all cases, except for overcast (day) to night retrieval (a wellknown difficult problem for image based methods) where our PN VLAD significantly outperforms NetVLAD. It can be seen that the use of point clouds makes the performance more robust to scene variations as they are more invariant to illumination and weather changes. <ref type="figure" target="#fig_6">Fig. 1 and 4</ref> show some of the successfully recognized point clouds, and it can be seen that our network has learned to ignore irrelevant noise such as ground snow and cars (both parked and moving). <ref type="figure" target="#fig_11">Fig. 7</ref> shows examples of unsuccessfully retrieved point clouds, and we can see that our network struggles on continuous roads with very similar features (top row) and heavily occluded areas (bottom row).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Analysis</head><p>Usability We further studied the usability of our network for place recognition.   is the pair with the lowest initial recall before network refinement. It is shown that our network indeed has the ability to recognize places almost throughout the entire reference map. Inference through our network implemented on Tensorflow <ref type="bibr" target="#b0">[2]</ref> on an NVIDIA GeForce GTX 1080Ti takes ∼ 9ms and retrieval through a submap database takes O(log n) making this applicable to real-time robotics systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We proposed the PointNetVLAD that solves large scale place recognition through point cloud based retrieval. We showed that our deep network is permutation invariant to its input. We applied metric learning for our network to learn a mapping from an unordered input 3D point cloud to a discriminative and compact global descriptor for the retrieval task. Furthermore, we proposed the "lazy triplet and quadruplet" loss functions that achieved more discriminative and generalizable global descriptors. Our experimental results on benchmark datasets showed the feasibility and usability of our network to the largely unexplored problem of point cloud based retrieval for place recognition.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Network architecture of our PointNetVLAD.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>The area of coverage (AOC) of all submaps are made to be approximately the same, i.e. AOC(m 1 ) ≈ ... AOC(m M ), and the number of points in each submap is kept small, i.e. |m i | ≪ |M|. We apply a downsampling filter G(.) to ensure that the number of points of all down- sampled submaps are the same, i.e. |G(m 1 )| = ... |G(m M )|. The problem of large-scale 3D point cloud based retrieval can be formally defined as follows: Definition 1 Given a query 3D point cloud denoted as q, where AOC(q) ≈ AOC(m i ) and |G(q)| = |G(m i )|, our goal is to retrieve the submap m * from the database M that is structurally most similar to q.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Dataset preprocessing: (a) A full route from the Oxford RobotCar dataset. (b) Zoomed-in region of the 3D point cloud in the red box shown in (a). (c) An example of submap with the detected ground plane shown as red points. (d) A downsampled submap that is centered at origin and all points within [-1,1]m.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, we have the output vector V = [V 1 , V 2 , . . . , V K ] of NetVLAD such that ∀k,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Sample point clouds from (a) Oxford, (b) U.S., (c) R.A. and (d) B.D., respectively: left shows the query submap and right shows the successfully retrieved corresponding point cloud.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>Oxford, U.S. and R.A.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average recall of the networks. Top row shows the average recall when PN VLAD and PN MAX were only trained on Oxford. Bottom row shows the average recall when PN VLAD and PN MAX were trained on Oxford, U.S. and R.A. PN VLAD PN MAX D-128 D-256 D-512 D-128 D-256 D-512 Ox.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) Average recall @N for retrieval from all reference areas. (b) Average recall at B.D. with varying distance thresholds. (c) Average recall @N with point clouds (pc) and images (img) as queries under various scene conditions, and retrieving from an overcast database in Oxford dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head></head><label></label><figDesc>Fig. 8 shows heat maps of correctly recognized submaps for a database pair in B.D. before and after network refinement. The chosen database pair</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Network limitations: These are examples of unsuccessfully retrieved point clouds by our network, where (a) shows the query, (b) shows the incorrect match to the query and (c) shows the true match.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Figure shows the retrieved map of our PointNetVLAD for a randomly selected database-query pair of the unseen B.D. for (a) baseline model and (b) refined model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 3. Average recall (%) at top1% on the different datasets for output dimensionality analysis of PN VLAD and PN MAX. All models were trained on Oxford. Here, D-refers to global descrip- tors with output length D-dim.</figDesc><table>74.60 80.31 80.33 71.93 73.44 74.79 
U.S. 66.03 72.63 76.24 61.15 64.64 65.79 
R.A. 53.86 60.27 63.31 49.25 51.92 52.32 
B.D. 59.84 65.30 66.75 53.25 54.74 56.63 

Average recall 
Triplet Loss 
71.20 
Quadruplet Loss 
74.13 
Lazy Triplet Loss 
78.99 
Lazy Quadruplet Loss 
80.31 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement We sincerely thank Lionel Heng from DSO National Laboratories for his time and effort spent on assisting us in data collection.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">12th USENIX Symposium on Operating Systems Design and Implementation (OSDI 16)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">All about (vlad)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Journal Computer Vision and Image Understanding (CVIU)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>Speeded-up robust features (surf)</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Beyond triplet loss: a deep quadruplet network for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Computing Research</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fab-map: Probabilistic localization and mapping in the space of appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Intl. J. of Robotics Research</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Invited Applications Paper FAB-MAP: Appearance-Based Place Recognition and Mapping using a Learned Visual Vocabulary Model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cummins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">3d point cloud registration for localization using a deep neural network autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Elbaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Avraham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fischer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Vision-based autonomous mapping and exploration using a quadrotor MAV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Honegger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tanskanen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Bags of binary words for fast place recognition in image sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gálvez-López</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tardós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">3d visual perception for self-driving cars using a multi-camera system: Calibration, mapping, localization, and obstacle detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Häne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Heng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Furgale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<idno>abs/1708.09839</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Analysis and solutions of the three point perspective pose estimation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Haralick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ottenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nolle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="page" from="592" to="598" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
			<biblScope unit="page">521540518</biblScope>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large-scale image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Location Estimation of Videos and Images</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Aggregating local descriptors into a compact image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image sequence geolocation with human travel priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vesselova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Escape from cells: Deep kd-networks for the recognition of 3d point cloud models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="20178" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised learning of threshold for geometric verification in visual-based loop-closure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Worldwide pose estimation using 3D point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Huttenlocher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Seqslam: Visual route-based navigation for sunny summer days and stormy winter nights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Wyeth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scalable recognition with a vocabulary tree</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Stewenius</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computing Research Repository</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">O</forename><surname>Ulusoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fast point feature histograms (FPFH) for 3d registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Aligning point cloud views using persistent feature histograms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Blodow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">C</forename><surname>Marton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Beetz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">3D is here: Point Cloud Library (PCL)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cousins</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Hyperpoints and fine vocabularies for largescale location recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Large-scale location recognition and the geometric burstiness problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Havlena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fast image-based localization using direct 2d-to-3d matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Improving image-based localization by active correspondence search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kobbelt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Are large-scale 3d models really necessary for accurate visual localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Taira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<idno>2015. 4</idno>
		<imprint/>
	</monogr>
<note type="report_type">Computing Research Repository</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generalized-icp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Segal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Haehnel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems (RSS)</title>
		<meeting>Robotics: Science and Systems (RSS)</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Narf: 3d range image features for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learned-Miller. Multi-view convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Probabilistic Robotics (Intelligent Robotics and Autonomous Agents)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Thrun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unique signatures of histograms for local surface description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Salti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L. Di</forename><surname>Stefano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">24/7 place recognition by view synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Okutomi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems (RSS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">T L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Camera pose voting for large-scale image-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Learning the matching of local 3d geometry in range scans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
