<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Mescheder</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
						</author>
						<author>
							<affiliation>
								<orgName>1 3</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Adversarial Variational Bayes: Unifying Variational Autoencoders and Generative Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Variational Autoencoders (VAEs) are expressive latent variable models that can be used to learn complex probability distributions from training data. However, the quality of the resulting model crucially relies on the expressiveness of the inference model. We introduce Adversarial Variational Bayes (AVB), a technique for training Variational Autoencoders with arbitrarily expressive inference models. We achieve this by introducing an auxiliary discriminative network that allows to rephrase the maximum-likelihoodproblem as a two-player game, hence establishing a principled connection between VAEs and Generative Adversarial Networks (GANs). We show that in the nonparametric limit our method yields an exact maximum-likelihood assignment for the parameters of the generative model, as well as the exact posterior distribution over the latent variables given an observation. Contrary to competing approaches which combine VAEs with GANs, our approach has a clear theoretical justification, retains most advantages of standard Variational Autoencoders and is easy to implement.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative models in machine learning are models that can be trained on an unlabeled dataset and are capable of generating new data points after training is completed. As generating new content requires a good understanding of the training data at hand, such models are often regarded as a key ingredient to unsupervised learning.</p><p>In recent years, generative models have become more and 1 Autonomous Vision Group, MPI Tübingen 2 Microsoft Research Cambridge 3 Computer Vision and Geometry Group, ETH Zürich. Correspondence to: Lars Mescheder &lt;lars.mescheder@tuebingen.mpg.de&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s).</p><p>f <ref type="figure">Figure 1</ref>. We propose a method which enables neural samplers with intractable density for Variational Bayes and as inference models for learning latent variable models. This toy example demonstrates our method's ability to accurately approximate complex posterior distributions like the one shown on the right. more powerful. While many model classes such as PixelRNNs (van den <ref type="bibr" target="#b36">Oord et al., 2016b)</ref>, PixelCNNs (van den <ref type="bibr" target="#b36">Oord et al., 2016a)</ref>, real NVP <ref type="bibr" target="#b3">(Dinh et al., 2016)</ref> and Plug &amp; Play generative networks <ref type="bibr" target="#b25">(Nguyen et al., 2016)</ref> have been introduced and studied, the two most prominent ones are Variational Autoencoders (VAEs) <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014)</ref> and Generative Adversarial Networks (GANs) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>.</p><p>Both VAEs and GANs come with their own advantages and disadvantages: while GANs generally yield visually sharper results when applied to learning a representation of natural images, VAEs are attractive because they naturally yield both a generative model and an inference model. Moreover, it was reported, that VAEs often lead to better log-likelihoods <ref type="bibr" target="#b37">(Wu et al., 2016)</ref>. The recently introduced BiGANs <ref type="bibr" target="#b4">(Donahue et al., 2016;</ref><ref type="bibr" target="#b5">Dumoulin et al., 2016</ref>) add an inference model to GANs. However, it was observed that the reconstruction results often only vaguely resemble the input and often do so only semantically and not in terms of pixel values.</p><p>The failure of VAEs to generate sharp images is often attributed to the fact that the inference models used during training are usually not expressive enough to capture the true posterior distribution. Indeed, recent work shows that using more expressive model classes can lead to substantially better results , both visually and in terms of log-likelihood bounds. Recent work <ref type="bibr" target="#b1">(Chen et al., 2016</ref>) also suggests that highly expressive inference models are essential in presence of a strong decoder to allow the model to make use of the latent space at all.</p><p>In this paper, we present Adversarial Variational Bayes <ref type="bibr">(AVB)</ref> 1 , a technique for training Variational Autoencoders with arbitrarily flexible inference models parameterized by neural networks. We can show that in the nonparametric limit we obtain a maximum-likelihood assignment for the generative model together with the correct posterior distribution.</p><p>While there were some attempts at combining VAEs and GANs <ref type="bibr" target="#b23">(Makhzani et al., 2015;</ref><ref type="bibr" target="#b17">Larsen et al., 2015)</ref>, most of these attempts are not motivated from a maximumlikelihood point of view and therefore usually do not lead to maximum-likelihood assignments. For example, in Adversarial Autoencoders (AAEs) <ref type="bibr" target="#b23">(Makhzani et al., 2015)</ref> the Kullback-Leibler regularization term that appears in the training objective for VAEs is replaced with an adversarial loss that encourages the aggregated posterior to be close to the prior over the latent variables. Even though AAEs do not maximize a lower bound to the maximum-likelihood objective, we show in Section 6.2 that AAEs can be interpreted as an approximation to our approach, thereby establishing a connection of AAEs to maximum-likelihood learning.</p><p>Outside the context of generative models, AVB yields a new method for performing Variational Bayes (VB) with neural samplers. This is illustrated in <ref type="figure">Figure 1</ref>, where we used AVB to train a neural network to sample from a nontrival unnormalized probability density. This allows to accurately approximate the posterior distribution of a probabilistic model, e.g. for Bayesian parameter estimation. The only other variational methods we are aware of that can deal with such expressive inference models are based on Stein Discrepancy <ref type="bibr" target="#b30">(Ranganath et al., 2016;</ref><ref type="bibr" target="#b20">Liu &amp; Feng, 2016)</ref>. However, those methods usually do not directly target the reverse Kullback-Leibler-Divergence and can therefore not be used to approximate the variational lower bound for learning a latent variable model. Our contributions are as follows:</p><p>• We enable the usage of arbitrarily complex inference models for Variational Autoencoders using adversarial training.</p><p>• We give theoretical insights into our method, showing that in the nonparametric limit our method recovers the true posterior distribution as well as a true maximum-likelihood assignment for the parameters of the generative model.</p><p>1 Concurrently to our work, several researchers have described similar ideas. Some ideas of this paper were described independently by Huszár in a blog post on http://www. inference.vc and in <ref type="bibr" target="#b12">Huszár (2017)</ref>. The idea to use adversarial training to improve the encoder network was also suggested by Goodfellow in an exploratory talk he gave at NIPS 2016 and by <ref type="bibr" target="#b19">Li &amp; Liu (2016)</ref>. A similar idea was also mentioned by <ref type="bibr" target="#b13">Karaletsos (2016)</ref> in the context of message passing in graphical models.  • We empirically demonstrate that our model is able to learn rich posterior distributions and show that the model is able to generate compelling samples for complex data sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>As our model is an extension of Variational Autoencoders (VAEs) <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014)</ref>, we start with a brief review of VAEs.</p><p>VAEs are specified by a parametric generative model p θ (x | z) of the visible variables given the latent variables, a prior p(z) over the latent variables and an approximate inference model q φ (z | x) over the latent variables given the visible variables. It can be shown that</p><formula xml:id="formula_0">log p θ (x) ≥ −KL(q φ (z | x), p(z)) + E q φ (z|x) log p θ (x | z). (2.1)</formula><p>The right hand side of (2.1) is called the variational lower bound or evidence lower bound (ELBO). If there is φ such that</p><formula xml:id="formula_1">q φ (z | x) = p θ (z | x), we would have log p θ (x) = max φ −KL(q φ (z | x), p(z)) + E q φ (z|x) log p θ (x | z). (2.2)</formula><p>However, in general this is not true, so that we only have an inequality in (2.2).</p><p>When performing maximum-likelihood training, our goal is to optimize the marginal log-likelihood</p><formula xml:id="formula_2">E p D (x) log p θ (x),<label>(2.3)</label></formula><p>where p D is the data distribution. Unfortunately, computing log p θ (x) requires marginalizing out z in p θ (x, z) which is usually intractable. Variational Bayes uses inequality (2.1) to rephrase the intractable problem of optimizing (2.3) into</p><formula xml:id="formula_3">max θ max φ E p D (x) −KL(q φ (z | x), p(z)) + E q φ (z|x) log p θ (x | z) . (2.4)</formula><p>Due to inequality (2.1), we still optimize a lower bound to the true maximum-likelihood objective (2.3).</p><p>Naturally, the quality of this lower bound depends on the expressiveness of the inference model q φ (z | x). Usually, q φ (z | x) is taken to be a Gaussian distribution with diagonal covariance matrix whose mean and variance vectors are parameterized by neural networks with x as input <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014)</ref>. While this model is very flexible in its dependence on x, its dependence on z is very restrictive, potentially limiting the quality of the resulting generative model. Indeed, it was observed that applying standard Variational Autoencoders to natural images often results in blurry images <ref type="bibr" target="#b17">(Larsen et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this work we show how we can instead use a black-box inference model q φ (z | x) and use adversarial training to obtain an approximate maximum likelihood assignment θ * to θ and a close approximation q φ * (z | x) to the true posterior p θ * (z | x). This is visualized in <ref type="figure" target="#fig_1">Figure 2</ref>: on the left hand side the structure of a typical VAE is shown. The right hand side shows our flexible black-box inference model. In contrast to a VAE with Gaussian inference model, we include the noise 1 as additional input to the inference model instead of adding it at the very end, thereby allowing the inference network to learn complex probability distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Derivation</head><p>To derive our method, we rewrite the optimization problem in (2.4) as</p><formula xml:id="formula_4">max θ max φ E p D (x) E q φ (z|x) log p(z) − log q φ (z | x) + log p θ (x | z) . (3.1)</formula><p>When we have an explicit representation of q φ (z | x) such as a Gaussian parameterized by a neural network, (3.1) can be optimized using the reparameterization trick <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b31">Rezende &amp; Mohamed, 2015)</ref> and stochastic gradient descent. Unfortunately, this is not the case when we define q φ (z | x) by a black-box procedure as illustrated in <ref type="figure" target="#fig_1">Figure 2b</ref>.</p><p>The idea of our approach is to circumvent this problem by implicitly representing the term</p><formula xml:id="formula_5">log p(z) − log q φ (z | x) (3.2)</formula><p>as the optimal value of an additional real-valued discriminative network T (x, z) that we introduce to the problem.</p><p>More specifically, consider the following objective for the discriminator T (x, z) for a given q φ (x | z):</p><formula xml:id="formula_6">max T E p D (x) E q φ (z|x) log σ(T (x, z)) + E p D (x) E p(z) log (1 − σ(T (x, z))) . (3.3)</formula><p>Here, σ(t) := (1 + e −t ) −1 denotes the sigmoid-function. Intuitively, T (x, z) tries to distinguish pairs (x, z) that were sampled independently using the distribution p D (x)p(z) from those that were sampled using the current inference model, i.e., using</p><formula xml:id="formula_7">p D (x)q φ (z | x).</formula><p>To simplify the theoretical analysis, we assume that the model T (x, z) is flexible enough to represent any function of the two variables x and z. This assumption is often referred to as the nonparametric limit <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> and is justified by the fact that deep neural networks are universal function approximators <ref type="bibr" target="#b11">(Hornik et al., 1989)</ref>.</p><p>As it turns out, the optimal discriminator T * (x, z) according to the objective in (3.3) is given by the negative of (3.2). Proposition 1. For p θ (x | z) and q φ (z | x) fixed, the optimal discriminator T * according to the objective in (3.3) is given by</p><formula xml:id="formula_8">T * (x, z) = log q φ (z | x) − log p(z). (3.4)</formula><p>Proof. The proof is analogous to the proof of Proposition 1 in <ref type="bibr" target="#b8">Goodfellow et al. (2014)</ref>. See the Supplementary Material for details. Together with (3.1), Proposition 1 allows us to write the optimization objective in (2.4) as</p><formula xml:id="formula_9">max θ,φ E p D (x) E q φ (z|x) − T * (x, z) + log p θ (x | z) , (3.5)</formula><p>where T * (x, z) is defined as the function that maximizes (3.3).</p><p>To optimize (3.5), we need to calculate the gradients of (3.5) with respect to θ and φ. While taking the gradient with respect to θ is straightforward, taking the gradient with respect to φ is complicated by the fact that we have defined T * (x, z) indirectly as the solution of an auxiliary optimization problem which itself depends on φ. However, the following Proposition shows that taking the gradient with respect to the explicit occurrence of φ in T * (x, z) is not necessary:</p><formula xml:id="formula_10">Algorithm 1 Adversarial Variational Bayes (AVB) 1: i ← 0 2: while not converged do 3: Sample {x (1) , . . . , x (m) } from data distrib. p D (x) 4: Sample {z (1) , . . . , z (m) } from prior p(z) 5: Sample { (1) , . . . , (m) } from N (0, 1) 6:</formula><p>Compute θ-gradient (eq. 3.7):</p><formula xml:id="formula_11">g θ ← 1 m m k=1 ∇ θ log p θ x (k) | z φ x (k) ,<label>(k)</label></formula><p>7:</p><p>Compute φ-gradient (eq. 3.7):</p><formula xml:id="formula_12">g φ ← 1 m m k=1 ∇ φ −T ψ x (k) , z φ (x (k) , (k) ) + log p θ x (k) | z φ (x (k) , (k) ) 8:</formula><p>Compute ψ-gradient (eq. 3.3) :</p><formula xml:id="formula_13">g ψ ← 1 m m k=1 ∇ ψ log σ(T ψ (x (k) , z φ (x (k) , (k) ))) + log 1 − σ(T ψ (x (k) , z (k) ) 9:</formula><p>Perform SGD-updates for θ, φ and ψ:</p><formula xml:id="formula_14">θ ← θ + h i g θ , φ ← φ + h i g φ , ψ ← ψ + h i g ψ 10:</formula><p>i ← i + 1 11: end while Proposition 2. We have</p><formula xml:id="formula_15">E q φ (z|x) (∇ φ T * (x, z)) = 0. (3.6)</formula><p>Proof. The proof can be found in the Supplementary Material. Using the reparameterization trick <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014)</ref>, (3.5) can be rewritten in the form</p><formula xml:id="formula_16">max θ,φ E p D (x) E − T * (x, z φ (x, )) + log p θ (x | z φ (x, )) (3.7)</formula><p>for a suitable function z φ (x, ). Together with Proposition 1, (3.7) allows us to take unbiased estimates of the gradients of (3.5) with respect to φ and θ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Algorithm</head><p>In theory, Propositions 1 and 2 allow us to apply Stochastic Gradient Descent (SGD) directly to the objective in (2.4). However, this requires keeping T * (x, z) optimal which is computationally challenging. We therefore regard the optimization problems in (3.3) and (3.7) as a two-player game. Propositions 1 and 2 show that any Nash-equilibrium of this game yields a stationary point of the objective in (2.4).</p><p>In practice, we try to find a Nash-equilibrium by applying SGD with step sizes h i jointly to (3.3) and (3.7), see Algorithm 1. Here, we parameterize the neural network T with a vector ψ. Even though we have no guarantees that this algorithm converges, any fix point of this algorithm yields a stationary point of the objective in (2.4).</p><p>Note that optimizing (3.5) with respect to φ while keeping θ and T fixed makes the encoder network collapse to a deterministic function. This is also a common problem for regular GANs <ref type="bibr" target="#b29">(Radford et al., 2015)</ref>. It is therefore crucial to keep the discriminative T network close to optimality while optimizing (3.5). A variant of Algorithm 1 therefore performs several SGD-updates for the adversary for one SGD-update of the generative model. However, throughout our experiments we use the simple 1-step version of AVB unless stated otherwise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Theoretical results</head><p>In Sections 3.1 we derived AVB as a way of performing stochastic gradient descent on the variational lower bound in (2.4). In this section, we analyze the properties of Algorithm 1 from a game theoretical point of view.</p><p>As the next proposition shows, global Nash-equilibria of Algorithm 1 yield global optima of the objective in (2.4):</p><p>Proposition 3. Assume that T can represent any function of two variables. If (θ * , φ * , T * ) defines a Nash-equilibrium of the two-player game defined by (3.3) and (3.7), then</p><formula xml:id="formula_17">T * (x, z) = log q φ * (z | x) − log p(z) (3.8)</formula><p>and (θ * , φ * ) is a global optimum of the variational lower bound in (2.4).</p><p>Proof. The proof can be found in the Supplementary Material.</p><p>Our parameterization of q φ (z | x) as a neural network allows q φ (z | x) to represent almost any probability density on the latent space. This motivates Corollary 4. Assume that T can represent any function of two variables and q φ (z | x) can represent any probability density on the latent space. If (θ * , φ * , T * ) defines a Nashequilibrium for the game defined by (3.3) and (3.7), then</p><formula xml:id="formula_18">1. θ * is a maximum-likelihood assignment 2. q φ * (z | x) is equal to the true posterior p θ * (z | x)</formula><p>3. T * is the pointwise mutual information between x and z, i.e.</p><formula xml:id="formula_19">T * (x, z) = log p θ * (x, z) p θ * (x)p(z) . (3.9)</formula><p>Proof. This is a straightforward consequence of Proposition 3, as in this case (θ * , φ * ) optimizes the variational lower bound in (2.4) if and only if 1 and 2 hold. Inserting the result from 2 into (3.8) yields 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adaptive Contrast</head><p>While in the nonparametric limit our method yields the correct results, in practice T (x, z) may fail to become sufficiently close to the optimal function T * (x, z). The reason for this problem is that AVB calculates a contrast between the two densities p D (x)q φ (z | x) to p D (x)p(z) which are usually very different. However, it is known that logistic regression works best for likelihood-ratio estimation when comparing two very similar densities <ref type="bibr" target="#b6">(Friedman et al., 2001</ref>).</p><p>To improve the quality of the estimate, we therefore propose to introduce an auxiliary conditional probability distribution r α (z | x) with known density that approximates q φ (z | x). For example, r α (z | x) could be a Gaussian distribution with diagonal covariance matrix whose mean and variance matches the mean and variance of q φ (z | x).</p><p>Using this auxiliary distribution, we can rewrite the variational lower bound in (2.4) as</p><formula xml:id="formula_20">E p D (x) −KL (q φ (z | x), r α (z | x)) + E q φ (z|x) (− log r α (z | x) + log p φ (x, z)) . (4.1)</formula><p>As we know the density of r α (z | x), the second term in (4.1) is amenable to stochastic gradient descent with respect to θ and φ. However, we can estimate the first term using AVB as described in Section 3. If r α (z | x) approximates q φ (z | x) well, KL (q φ (z | x), r α (z | x)) is usually much smaller than KL (q φ (z | x), p(z)), which makes it easier for the adversary to learn the correct probability ratio.</p><p>We call this technique Adaptive Contrast (AC), as we are now contrasting the current inference model q φ (z | x) to an adaptive distribution r α (z | x) instead of the prior p(z). Using Adaptive Contrast, the generative model p θ (x | z) and the inference model q φ (z | x) are trained to maximize</p><formula xml:id="formula_21">E p D (x) E q φ (z|x) − T * (x, z) − log r α (z | x) + log p θ (x, z) , (4.2)</formula><p>where T * (x, z) is the optimal discriminator distinguishing samples from r α (z | x) and q φ (z | x).</p><p>Consider now the case that r α (z | x) is given by a Gaussian distribution with diagonal covariance matrix whose mean µ(x) and variance vector σ(x) match the mean and variance of q φ (z | x). As the Kullback-Leibler divergence is invariant under reparameterization, the first term in (4.1) can be rewritten as σ(x) and r 0 (z) is a Gaussian distribution with mean 0 and variance 1. This way, the adversary only has to account for the deviation of q φ (z | x) from a Gaussian distribution, not its location and scale. Please see the Supplementary Material for pseudo code of the resulting algorithm.</p><formula xml:id="formula_22">E p D (x) KL (q φ (z | x), r 0 (z))<label>(4.</label></formula><p>In practice, we estimate µ(x) and σ(x) using a MonteCarlo estimate. In the Supplementary Material we describe a network architecture for q φ (z | x) that makes the computation of this estimate particularly efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We tested our method both as a black-box method for variational inference and for learning generative models. The former application corresponds to the case where we fix the generative model and a data point x and want to learn the posterior q φ (z | x).</p><p>An additional experiment on the celebA dataset <ref type="bibr" target="#b21">(Liu et al., 2015)</ref> can be found in the Supplementary Material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Variational Inference</head><p>When the generative model and a data point x is fixed, AVB gives a new technique for Variational Bayes with arbitrarily complex approximating distributions. We applied this to the "Eight School" example from <ref type="bibr" target="#b7">Gelman et al. (2014)</ref>. In this example, the coaching effects y i , i = 1, . . . , 8 for eight schools are modeled as</p><formula xml:id="formula_23">y i ∼ N (µ + θ · η i , σ i ),</formula><p>where µ, τ and the η i are the model parameters to be inferred. We place a N (0, 1) prior on the parameters of the model. We compare AVB against two variational methods with Gaussian inference model <ref type="bibr" target="#b16">(Kucukelbir et al., 2015)</ref> as implemented in <ref type="bibr" target="#b34">STAN (Stan Development Team, 2016)</ref>. We used a simple two layer model for the posterior and a powerful 5-layer network with RESNET-blocks <ref type="bibr" target="#b9">(He et al., 2015)</ref> for the discriminator. For every posterior update step we performed two steps for the adversary. The groundtruth data was obtained by running Hamiltonian Monte-</p><formula xml:id="formula_24">(µ, τ ) (τ, η 1 ) AVB VB (full- rank)</formula><p>HMC <ref type="figure">Figure 4</ref>. Comparison of AVB to VB on the "Eight Schools" example by inspecting two marginal distributions of the approximation to the 10-dimensional posterior. We see that AVB accurately captures the multi-modality of the posterior distribution. In contrast, VB only focuses on a single mode. The ground truth is shown in the last row and has been obtained using HMC.</p><p>Carlo (HMC) for 500000 steps using STAN. Note that AVB and the baseline variational methods allow to draw an arbitrary number of samples after training is completed whereas HMC only yields a fixed number of samples.</p><p>We evaluate all methods by estimating the KullbackLeibler-Divergence to the ground-truth data using the ITEpackage <ref type="bibr" target="#b35">(Szabo, 2013)</ref> applied to 10000 samples from the ground-truth data and the respective approximation. The resulting Kullback-Leibler divergence over the number of iterations for the different methods is plotted in <ref type="figure" target="#fig_3">Figure 3</ref>. We see that our method clearly outperforms the methods with Gaussian inference model. For a qualitative visualization, we also applied Kernel-density-estimation to the 2-dimensional marginals of the (µ, τ )-and (τ, η 1 )-variables as illustrated in <ref type="figure">Figure 4</ref>. In contrast to variational Bayes with Gaussian inference model, our approach clearly captures the multi-modality of the posterior distribution. We also observed that Adaptive Contrast makes learning more robust and improves the quality of the resulting model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Generative Models</head><p>Synthetic Example To illustrate the application of our method to learning a generative model, we trained the neural networks on a simple synthetic dataset containing only the 4 data points from the space of 2 × 2 binary images shown in <ref type="figure">Figure 5</ref> and a 2-dimensional latent space. Both the encoder and decoder are parameterized by 2-layer fully connected neural networks with 512 hidden units each. The </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ELBO</head><p>-1.697 <ref type="table">Table 1</ref>. Comparison of VAE and AVB on synthetic dataset. The optimal log-likelihood score on this dataset is − log(4) ≈ −1.386.</p><formula xml:id="formula_25">≈ -1.421 KL(q φ (z), p(z)) ≈ 0.165 ≈ 0.026</formula><p>encoder network takes as input a data point x and a vector of Gaussian random noise and produces a latent code z. The decoder network takes as input a latent code z and produces the parameters for four independent Bernoullidistributions, one for each pixel of the output image. The adversary is parameterized by two neural networks with two 512-dimensional hidden layers each, acting on x and z respectively, whose 512-dimensional outputs are combined using an inner product.</p><p>We compare our method to a Variational Autoencoder with a diagonal Gaussian posterior distribution. The encoder and decoder networks are parameterized as above, but the encoder does not take the noise as input and produces a mean and variance vector instead of a single sample.</p><p>We visualize the resulting division of the latent space in <ref type="figure">Figure 6</ref>, where each color corresponds to one state in the x-space. Whereas the Variational Autoencoder divides the space into a mixture of 4 Gaussians, the Adversarial Variational Autoencoder learns a complex posterior distribution. Quantitatively this can be verified by computing the KLdivergence between the prior p(z) and the aggregated posterior q φ (z) := q φ (z | x)p D (x)dx, which we estimate using the ITE-package <ref type="bibr" target="#b35">(Szabo, 2013)</ref>, see <ref type="table">Table 1</ref>. Note that the variations for different colors in <ref type="figure">Figure 6</ref> are solely due to the noise used in the inference model.</p><p>The ability of AVB to learn more complex posterior models leads to improved performance as <ref type="table">Table 1</ref> shows. In particular, AVB leads to a higher likelihood score that is close to the optimal value of − log(4) compared to a standard VAE that struggles with the fact that it cannot divide (a) Training data (b) Random samples <ref type="figure">Figure 7</ref>. Independent samples for a model trained on MNIST</p><formula xml:id="formula_26">log p(x) ≥ log p(x) ≈ AVB (8-dim) (≈ −83.6 ± 0.4) −91.2 ± 0.6 AVB + AC (8-dim) ≈ −96.3 ± 0.4 −89.6 ± 0.6 AVB + AC (32-dim) ≈ −79.5 ± 0.3 −80.2 ± 0.4 VAE (8-dim)</formula><p>−98.1 ± 0.5 −90.9 ± 0.6 VAE (32-dim) −87.2 ± 0.3 −81.9 ± 0.4 VAE + NF (T=80) −85.1 − <ref type="bibr" target="#b31">(Rezende &amp; Mohamed, 2015)</ref> VAE + HVI (T=16) −88.3 −85.51 <ref type="bibr" target="#b33">(Salimans et al., 2015)</ref> convVAE + HVI (T=16) −84.1 −81.94 <ref type="bibr" target="#b33">(Salimans et al., 2015)</ref> VAE + VGP (2hl) −81.3 − <ref type="bibr" target="#b36">(Tran et al., 2015)</ref> DRAW + VGP −79.9 − <ref type="bibr" target="#b36">(Tran et al., 2015)</ref> VAE + IAF −80.8 −79.10  Auxiliary VAE (L=2) −83.0 − <ref type="bibr" target="#b22">(Maaløe et al., 2016)</ref>  the latent space appropriately. Moreover, we see that the reconstruction error given by the mean cross-entropy between an input x and its reconstruction using the encoder and decoder networks is much lower when using AVB instead of a VAE with diagonal Gaussian inference model. We also observe that the estimated variational lower bound is close to the true log-likelihood, indicating that the adversary has learned the correct function.</p><p>MNIST In addition, we trained deep convolutional networks based on the DC-GAN-architecture <ref type="bibr" target="#b29">(Radford et al., 2015)</ref> on the binarized MNIST-dataset <ref type="bibr" target="#b18">(LeCun et al., 1998)</ref>. For the decoder network, we use a 5-layer deep convolutional neural network. For the encoder network, we use a network architecture that allows for the efficient computation of the moments of q φ (z | x). The idea is to define the encoder as a linear combination of learned basis noise vectors, each parameterized by a small fully-connected neural network, whose coefficients are parameterized by a neural network acting on x, please see the Supplementary Material for details. For the adversary, we replace the fully connected neural network acting on z and x with a fully connected 4-layer neural networks with 1024 units in each hidden layer. In addition, we added the result of neural networks acting on x and z alone to the end result.</p><p>To validate our method, we ran Annealed Importance Sampling (AIS) <ref type="bibr" target="#b24">(Neal, 2001)</ref>, the gold standard for evaluating decoder based generative models <ref type="bibr" target="#b37">(Wu et al., 2016)</ref>   <ref type="table" target="#tab_0">Table 2</ref>. Using AIS, we see that AVB without AC overestimates the true ELBO which degrades its performance. Even though the results suggest that AVB with AC can also overestimate the true ELBO in higher dimensions, we note that the loglikelihood estimate computed by AIS is also only a lower bound to the true log-likelihood <ref type="bibr" target="#b37">(Wu et al., 2016)</ref>.</p><p>Using AVB with AC, we see that we improve both on a standard VAE and AVB without AC. When comparing to other state of the art methods, we see that our method achieves state of the art results on binarized MNIST 2 . For an additional experimental evaluation of AVB and three baselines for a fixed decoder architecture see the Supplementary Material. Some random samples for MNIST are shown in <ref type="figure">Figure 7</ref>. We see that our model produces random samples that are perceptually close to the training set.</p><p>6. Related Work 6.1. Connection to Variational Autoencoders AVB strives to optimize the same objective as a standard VAE <ref type="bibr" target="#b14">(Kingma &amp; Welling, 2013;</ref><ref type="bibr" target="#b32">Rezende et al., 2014)</ref>, but approximates the Kullback-Leibler divergence using an adversary instead of relying on a closed-form formula.</p><p>Substantial work has focused on making the class of approximate inference models more expressive. Normalizing flows <ref type="bibr" target="#b31">(Rezende &amp; Mohamed, 2015;</ref><ref type="bibr" target="#b15">Kingma et al., 2016)</ref> make the posterior more complex by composing a simple Gaussian posterior with an invertible smooth mapping for which the determinant of the Jacobian is tractable. Auxiliary Variable VAEs <ref type="bibr" target="#b22">(Maaløe et al., 2016)</ref> add auxiliary variables to the posterior to make it more flexible. However, no other approach that we are aware of allows to use black-box inference models to optimize the ELBO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Connection to Adversarial Autoencoders</head><p>Makhzani et al. <ref type="bibr" target="#b23">(Makhzani et al., 2015)</ref> introduced the concept of Adversarial Autoencoders. The idea is to replace the term</p><formula xml:id="formula_27">KL(q φ (z | x), p(z)) (6.1)</formula><p>in (2.4) with an adversarial loss that tries to enforce that upon convergence</p><formula xml:id="formula_28">q φ (z | x)p D (x)dx ≈ p(z). (6.2)</formula><p>While related to our approach, the approach by Makhzani et al. modifies the variational objective while our approach retains the objective.</p><p>The approach by Makhzani et al. can be regarded as an approximation to our approach, where T (x, z) is restricted to the class of functions that do not depend on x. Indeed, an ideal discriminator that only depends on z maximizes</p><formula xml:id="formula_29">p D (x)q(z | x) log σ(T (z))dxdz + p D (x)p(z) log (1 − σ(T (z)))dxdz (6.3)</formula><p>which is the case if and only if</p><formula xml:id="formula_30">T (z) = log q(z | x)p D (x)dx − log p(z). (6.4)</formula><p>Clearly, this simplification is a crude approximation to our formulation from Section 3, but <ref type="bibr" target="#b23">Makhzani et al. (2015)</ref> show that this method can still lead to good sampling results. In theory, restricting T (x, z) in this way ensures that upon convergence we approximately have</p><formula xml:id="formula_31">q φ (z | x)p D (x)dx = p(z), (6.5) but q φ (z | x) need not be close to the true posterior p θ (z | x). Intuitively, while mapping p D (x) through q φ (z | x)</formula><p>results in the correct marginal distribution, the contribution of each x to this distribution can be very inaccurate.</p><p>In contrast to Adversarial Autoencoders, our goal is to improve the ELBO by performing better probabilistic inference. This allows our method to be used in a more general setting where we are only interested in the inference network itself (Section 5.1) and enables further improvements such as Adaptive Contrast (Section 4) which are not possible in the context of Adversarial Autoencoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Connection to f-GANs</head><p>Nowozin et al. <ref type="bibr" target="#b27">(Nowozin et al., 2016)</ref> proposed to generalize Generative Adversarial Networks <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref> to f-divergences <ref type="bibr" target="#b0">(Ali &amp; Silvey, 1966</ref>) based on results by Nguyen et al. <ref type="bibr" target="#b26">(Nguyen et al., 2010)</ref>. In this paragraph we show that f-divergences allow to represent AVB as a zero-sum two-player game.</p><p>The family of f-divergences is given by <ref type="bibr">(6.6)</ref> for some convex functional f : R → R ∞ with f (1) = 0. <ref type="bibr" target="#b26">Nguyen et al. (2010)</ref> show that by using the convex conjugate f * of f , <ref type="bibr" target="#b10">(Hiriart-Urruty &amp; Lemaréchal, 2013)</ref>, we obtain</p><formula xml:id="formula_32">D f (p q) = E p f q(x) p(x) .</formula><formula xml:id="formula_33">D f (p q) = sup T E q(x) [T (x)] − E p(x) [f * (T (x))] , (6.7)</formula><p>where T is a real-valued function. In particular, this is true for the reverse Kullback-Leibler divergence with f (t) = t log t. We therefore obtain KL(q(z | x), p(z)) = D f (p(z), q(z | x)) = sup T E q(z|x) T (x, z) − E p(z) f * (T (x, z)), (6.8) with f * (ξ) = exp(ξ − 1) the convex conjugate of f (t) = t log t.</p><p>All in all, this yields</p><formula xml:id="formula_34">max θ E p D (x) log p θ (x) (6.9) = max θ,q min T E p D (x) E p(z) f * (T (x, z)) +E p D (x) E q(z|x) (log p θ (x | z) − T (x, z)).</formula><p>By replacing the objective (3.3) for the discriminator with</p><formula xml:id="formula_35">min T E p D (x) E p(z) e</formula><p>T (x,z)−1 − E q(z|x) T (x, z) , (6.10)</p><p>we can reformulate the maximum-likelihood-problem as a mini-max zero-sum game. In fact, the derivations from Section 3 remain valid for any f -divergence that we use to train the discriminator. This is similar to the approach taken by Poole et al.  to improve the GAN-objective. In practice, we observed that the objective (6.10) results in unstable training. We therefore used the standard GAN-objective (3.3), which corresponds to the Jensen-Shannon-divergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Connection to BiGANs</head><p>BiGANs <ref type="bibr" target="#b4">(Donahue et al., 2016;</ref><ref type="bibr" target="#b5">Dumoulin et al., 2016)</ref> are a recent extension to Generative Adversarial Networks with the goal to add an inference network to the generative model. Similarly to our approach, the authors introduce an adversary that acts on pairs (x, z) of data points and latent codes. However, whereas in BiGANs the adversary is used to optimize the generative and inference networks separately, our approach optimizes the generative and inference model jointly. As a result, our approach obtains good reconstructions of the input data, whereas for BiGANs we obtain these reconstructions only indirectly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We presented a new training procedure for Variational Autoencoders based on adversarial training. This allows us to make the inference model much more flexible, effectively allowing it to represent almost any family of conditional distributions over the latent variables.</p><p>We believe that further progress can be made by investigating the class of neural network architectures used for the adversary and the encoder and decoder networks as well as finding better contrasting distributions.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Schematic comparison of a standard VAE and a VAE with black-box inference model, where 1 and 2 denote samples from some noise distribution. While more complicated inference models for Variational Autoencoders are possible, they are usually not as flexible as our black-box approach.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>3) whereq φ (z | x) denotes the distribution of the normalized vectorz := z−µ(x)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison of KL to ground truth posterior obtained by Hamiltonian Monte Carlo (HMC).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Training examples in the synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Log-likelihoods on binarized MNIST for AVB and other methods improving on VAEs. We see that our method achieves state of the art log-likelihoods on binarized MNIST. The approx- imate log-likelihoods in the lower half of the table were not ob- tained with AIS but with importance sampling.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that the methods in the lower half of Table 2 were trained with different decoder architectures and therefore only provide limited information regarding the quality of the inference model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by Microsoft Research through its PhD Scholarship Programme.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A general class of coefficients of divergence of one distribution from another</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Syed</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mumtaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Silvey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Samuel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="131" to="142" />
			<date type="published" when="1966" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Prafulla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02731</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Variational lossy autoencoder. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Nice: Non-linear independent components estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.8516</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Samy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08803</idno>
		<title level="m">Density estimation using real nvp</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishmael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jerome</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Tibshirani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<publisher>Springer series in statistics Springer</publisher>
			<biblScope unit="volume">1</biblScope>
			<pubPlace>Berlin</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">B</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rubin</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donald</forename><forename type="middle">B</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Chapman &amp; Hall/CRC</publisher>
			<biblScope unit="volume">2</biblScope>
			<pubPlace>Boca Raton, FL, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<title level="m">Deep residual learning for image recognition</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Convex analysis and minimization algorithms I: fundamentals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Lemaréchal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">305</biblScope>
		</imprint>
	</monogr>
	<note>Springer science &amp; business media</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Multilayer feedforward networks are universal approximators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Hornik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maxwell</forename><surname>Stinchcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>White</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Halbert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="359" to="366" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08235</idno>
		<title level="m">Variational inference using implicit distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adversarial message passing for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05048</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04934</idno>
		<title level="m">Improving variational inference with inverse autoregressive flow</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic variational inference in stan</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alp</forename><surname>Kucukelbir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Rajesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boesen Lindbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Léon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Wild variational approximations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on advances in approximate Bayesian inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Two methods for wild variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yihao</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00081</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<title level="m">Auxiliary deep generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alireza</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Goodfellow</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Ian. Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Annealed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistics and Computing</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="125" to="139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yoshua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexey</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00005</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00709</idno>
		<title level="m">Training generative neural samplers using variational divergence minimization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alemi</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">A</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelova</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02780</idno>
		<title level="m">Anelia. Improved generator objectives for gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Operator variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dustin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaan</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jimenez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Markov chain monte carlo and variational inference: Bridging the gap</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1218" to="1226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Stan modeling language users guide and reference manual</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Development Team</surname></persName>
		</author>
		<idno>2.14.0</idno>
		<ptr target="http://mc-stan.org" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Information theoretical estimators (ite) toolbox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoltán</forename><surname>Szabo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Conditional image generation with pixelcnn decoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dustin</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rajesh</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M ;</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lasse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Oriol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06499</idno>
		<idno>arXiv:1601.06759</idno>
	</analytic>
	<monogr>
		<title level="m">Aaron van den, Kalchbrenner, Nal, and Kavukcuoglu, Koray. Pixel recurrent neural networks</title>
		<imprint>
			<publisher>van den Oord</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4790" to="4798" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>Advances In Neural Information Processing Systems</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">On the quantitative analysis of decoder-based generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roger</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04273</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
