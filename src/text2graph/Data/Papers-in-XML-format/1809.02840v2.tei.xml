<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Guided Constraint Logic Programming for Program Synthesis</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lisa</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rosenblatt</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Fetaya</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Byrd</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Might</surname></persName>
							<email>might@uab.edu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Alabama at Birmingham</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Uber ATG</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
							<email>zemel@cs.toronto.edu4gregr</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Toronto</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Vector Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Guided Constraint Logic Programming for Program Synthesis</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Synthesizing programs using example input/outputs is a classic problem in artificial intelligence. We present a method for solving Programming By Example (PBE) problems by using a neural model to guide the search of a constraint logic programming system called miniKanren. Crucially, the neural model uses miniKanren's internal representation as input; miniKanren represents a PBE problem as recursive constraints imposed by the provided examples. We explore Recurrent Neural Network and Graph Neural Network models. We contribute a modified miniKanren, drivable by an external agent, available at https://github.com/xuexue/neuralkanren. We show that our neural-guided approach using constraints can synthesize programs faster in many cases, and importantly, can generalize to larger problems.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Program synthesis is a classic area of artificial intelligence that has captured the imagination of many computer scientists. Programming by Example (PBE) is one way to formulate program synthesis problems, where example input/output pairs are used to specify a target program. In a sense, supervised learning can be considered program synthesis, but supervised learning via successful models like deep neural networks famously lacks interpretability. The clear interpretability of programs as code means that synthesized results can be compared, optimized, translated, and proved correct. The manipulability of code makes program synthesis continue to be relevant today.</p><p>Current state-of-the-art approaches use symbolic techniques developed by the programming languages community. These methods use rule-based, exhaustive search, often manually optimized by human experts using domain-specific knowledge. While these techniques work, they tend not to scale, and success is limited to small programs. Recent works by the machine learning community explore a variety of statistical methods to improve performance on a variety of PBE problems. Contributions generally fall under three general categories: differentiable programming <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>, direct synthesis <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, and neural guided search <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. This work falls under neural guided search, where the machine learning model guides a symbolic search technique. We take the integration with a symbolic system further: we use the internal representation of the symbolic system as input to the neural model. The symbolic system we use is a constraint logic programming system called miniKanren <ref type="bibr" target="#b0">1</ref>  <ref type="bibr" target="#b7">[8]</ref>, chosen for its ability to encode synthesis problems that are difficult to encode in other systems. Specifically, miniKanren does not rely on types, is able to to complete partially specified programs, and has a straightforward implementation <ref type="bibr" target="#b8">[9]</ref>. miniKanren searches for a candidate program that satisfies the recursive constraints imposed by the input/output examples. Our model uses these internal constraints to score candidate programs and guide miniKanren's search.</p><p>The idea of neural guided search using constraints is promising for several reasons. First, while symbolic approaches outperform statistical methods, they have not demonstrated an ability to scale to larger problems; neural guidance offers the potential to help navigate the exponentially large search space. Second, symbolic systems exploit the compositionality of synthesis problems: miniKanren's constraints select portions of the input/output examples relevant to a subproblem. This is akin to having a symbolic attention mechanism. Third, constraint lengths are relatively stable even as we synthesize more complex programs, thus our approach should be able to generalize to programs larger than those seen in training.</p><p>To summarize our contributions, we introduce a novel form of neural guided synthesis using a symbolic system's internal representations to solve an auxiliary problem of constraint satisfaction while learning neural embeddings. We explore two approaches to scoring constraints: using a Recurrent Neural Network (RNN) and Graph Neural Network (GNN) <ref type="bibr" target="#b9">[10]</ref>. We also present a "transparent" version of miniKanren with visibility into its internal constraints, usable by other researchers. The code is available at https://github.com/xuexue/neuralkanren.</p><p>We test our approach to solve PBE problems in a subset of Lisp, and show improved performance on generated problems held out from training. To gauge generalizability, we test our approaches on three families of synthesis problems, comparing against state-of-the-art systems λ 2 <ref type="bibr" target="#b10">[11]</ref>, Escher <ref type="bibr" target="#b11">[12]</ref>, and Myth <ref type="bibr" target="#b12">[13]</ref>. We show that our neural-guided approach using constraints can synthesize problems faster in many cases, and has the potential to generalize to larger problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Programming by example (PBE) problems have a long history dating to the 1970's <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>. Along the lines of early works in program synthesis, the programming languages community developed search techniques that enumerate possible programs, with pruning strategies based on types, consistency, and logical reasoning to improve the search. While such enumerative search techniques may seem simple, they can achieve impressive results. Several state-of-the-art methods are described in <ref type="table" target="#tab_0">Table 1</ref>.  <ref type="bibr" target="#b11">[12]</ref> Bottom-up Forward Search / Conditional Inference Static Myth <ref type="bibr" target="#b12">[13]</ref> Top-down Iterative Deepening Static</p><p>The method λ 2 <ref type="bibr" target="#b10">[11]</ref> is most similar to miniKanren, but specializes in numeric, statically-typed inputs and outputs. Escher <ref type="bibr" target="#b11">[12]</ref> is built as an active learner, and relies on the presence of an oracle to supply outputs for new inputs that it chooses. Myth <ref type="bibr" target="#b12">[13]</ref> searches for the smallest program satisfying a set of examples, and guarantees parsimony. These methods all use functional languages based on the λ-calculus as their target language, and aim to synthesize general, recursive functions.</p><p>Contributions by the machine learning community have grown in the last few years. Interestingly, while PBE problems can be thought of as a meta-learning or learning-to-learn problem, few works explore this relationship. Instead, methods explored include direct synthesis, differentiable programming, and neural guided synthesis.</p><p>Direct Synthesis In direct synthesis, the program is produced directly as a sequence or tree. One use case where this method has been successful is FlashFill <ref type="bibr" target="#b16">[17]</ref> and its descendants <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b17">18]</ref>. These techniques are trained on string manipulation tasks and are successfully applied to spreadsheet completion problems. The original FlashFill <ref type="bibr" target="#b16">[17]</ref> technique uses a combination of search and carefully crafted heuristics. Later works like <ref type="bibr" target="#b4">[5]</ref> introduce a "Recursive-Reverse-Recursive Neural Network" to generate a program tree conditioned on input/output embeddings. More recently, RobustFill <ref type="bibr" target="#b3">[4]</ref> uses bi-directional Long Short-Term Memory (LSTM) with attention, to generate programs as sequences. Despite flattening the tree structure, RobustFill achieved much better results (92% vs 38%) on the FlashFill benchmark. While these approaches succeed in the practical domain of string manipulation, we are interested in exploring manipulations of richer data structures.</p><p>Differentiable Programming Differentiable programming involves building a differentiable interpreter, then backpropagating through the interpreter to learn a latent program. The goal is to infer correct outputs for new inputs. Work in differentiable programming began with the Neural Turing Machine <ref type="bibr" target="#b2">[3]</ref>, a neural architecture that augments neural networks with external memory and attention. Neural Programmer <ref type="bibr" target="#b0">[1]</ref> and Neural Programmer-Interpreter <ref type="bibr" target="#b1">[2]</ref> extend the work with reusable operations, and build programs compositionally. While differentiable approaches are appealing, <ref type="bibr" target="#b18">[19]</ref> showed that this approach still underperforms discrete search-based techniques.</p><p>Neural Guided Search A recent line of work uses statistical techniques to guide a discrete search. For example, DeepCoder <ref type="bibr" target="#b5">[6]</ref> uses an encoding of the input/output examples to predict functions that are likely to appear in the program, to prioritize programs containing those functions. More recently, <ref type="bibr" target="#b6">[7]</ref> uses an LSTM to guide the symbolic search system PROSE (Microsoft Program Synthesis using Examples). The search uses a "branch and bound" technique. The neural model learns the choices that maximize the bounding function h introduced in <ref type="bibr" target="#b16">[17]</ref> and used for FlashFill problems. These approaches attempt to be search system agnostic, whereas we integrate deeply with one symbolic approach, taking advantage of its internal representation and compositional reasoning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Constraint Logic Programming with miniKanren</head><p>This section describes the constraint logic programming language miniKanren and its use for program synthesis. <ref type="figure" target="#fig_0">Figure 1</ref> summarizes the relationship between miniKanren and the neural agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Background</head><p>The constraint logic programming language miniKanren uses the relational programming paradigm, where programmers write relations instead of functions. Relations are a generalization of functions: a function f with n parameters can be expressed as a relation R with n + 1 parameters, e.g., (f x) = y implies (R x y). The notation (R x y) means that x and y are related by R.</p><p>In miniKanren queries, data flow is not directionally biased: any input to a relation can be unknown. For example, a query (R X y) where y is known and X is an unknown, or logic variable, finds values X where X and y are related by R. In other words, given R and f defined as before, the query finds inputs X to f such that (f X) = y. This property allows the relational translation of a function to run computations in reverse <ref type="bibr" target="#b15">[16]</ref>. We refer to such uses of relations containing logic variables as constraints. In this work, we are interested in using a relational form evalo of an interpreter EVAL to perform program synthesis 2 . In the functional computation (EVAL P I) = O, program P and input I are known, and the output O is the result to be computed. The same computation can be expressed relationally with (evalo P I O ) where P and I are known and O is an unknown. We can also synthesize programs from inputs and outputs, expressed relationally with (evalo P I O) where P is unknown while I and O are known. While ordinary evaluation is deterministic, there may be many valid programs P for any pair of I and O. Multiple uses of evalo, involving the same P but different pairs I and O can be combined in a conjunction, further constraining P . This is how PBE tasks are encoded using an implementation of evalo for the target synthesis language.</p><formula xml:id="formula_0">(evalo P I O) ⇒ DISJ → (evalo (quote A ) I O) → (evalo (car B ) I O) → (evalo (cdr C ), I, O) → (evalo (cons D E ), I, O) → (evalo (var F ), I, O) . . .</formula><p>A miniKanren program internally represents a query as a constraint tree built out of conjunctions, disjunctions, and calls to relations (constraints). A relation like evalo is recursive, that is, defined in terms of invocations of other constraints including itself. Search involves unfolding a recursive constraint by replacing the constraint with its definition in terms of other constraints. For example, in a Lisp interpreter, a program P can be a constant, a function call, or another expression. Unfolding reveals these possibilities as clauses of a disjunction that replaces evalo. <ref type="figure" target="#fig_1">Figure 2</ref> shows a partial unfolding of (evalo P I O).</p><p>As we unfold more nodes, branches of the constraint tree constrain P to be more specific. We refer to a partial specification of P as a "candidate" partial program. If at some point we find a fully specified P that satisfies all relevant constraints, then P is a solution to the PBE problem.</p><p>In <ref type="figure" target="#fig_2">Figure 3</ref>, we show portions of the constraint tree representing a PBE problem with two input/output pairs. Each of the gray boxes corresponds to a separate disjunct in the constraint tree, representing a candidate. Each disjunct is a conjunction of constraints, shown one on each line. A candidate is viable only if the entire conjunction can be satisfied. In the left column (a) certain "obviously" failing candidates like (quote M ) are omitted from consideration. The right column (c) also shows the unfolding of the selected disjunct for (cons D E ), where D is replaced by its possible values.</p><p>The default search process used by miniKanren is biased interleaving, alternating disjuncts to unfold. The alternation is "biased" towards disjuncts that have more of their constraints already satisfied. This search is complete: if a solution exists, it will eventually be found, time and memory permitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Transparent constraint representation</head><p>Typical implementations of miniKanren represent constraint trees as "goals" <ref type="bibr" target="#b15">[16]</ref> built from opaque, suspended computations. These suspensions entangle both constraint simplification and an implicit search policy (typically biased interleaving <ref type="bibr" target="#b15">[16]</ref>), making it difficult to inspect a constraint tree and experiment with alternative search policies.</p><p>Fortunately, miniKanren implementations are fairly straightforward to build <ref type="bibr" target="#b8">[9]</ref>, so miniKanren has been re-implemented and extended by many. One of our contributions is a miniKanren implementation that represents the constraint tree as a transparent data structure, providing an interface for choosing the next disjunct to unfold, and making it possible to define custom search policies driven by external agents. Our implementation is available at https://github.com/xuexue/neuralkanren.</p><p>Like the standard miniKanren, this transparent version is implemented in Scheme. To interface with an external agent, we have implemented a Python interface that can drive the miniKanren process via stdin/stdout, starting by submitting a query, then alternating between receiving constraint tree updates and choosing the next disjunct to unfold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Neural Guided Constraint Logic Programming</head><p>We present our neural guided synthesis approach summarized in <ref type="figure" target="#fig_2">Figure 3</ref>. To begin, miniKanren represents the PBE problem in terms of a disjunction of candidate partial programs, and constraints that must be satisfied for the partial program to be consistent with the examples. A machine learning agent makes discrete choices amongst the possible candidates. The symbolic system then expands the chosen candidate, adding expansions of the candidate to the list of partial programs.</p><p>The machine learning model follows these steps:</p><p>1. Embed the constraints. Sections 4.1 and 4.2 discuss two methods for embedding constraints that trade off ease of training and accounting for logic variable identity. 2. Score each constraint. Each constraint embedding is scored independently, using a multilayer perceptron (MLP). 3. Pool scores together. We pool constraint scores for each candidate. We pool hierarchically using the structure of the constraint tree, max-pooling along a disjunction and averagepooling along a conjunction. We find that using average-pooling instead of min-pooling helps gradient flow. In <ref type="figure" target="#fig_2">Figure 3</ref> there are no internal disjunctions. 4. Choose a candidate. We use a softmax distribution over candidates during training and choose greedily during test.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(car ) (cons )</head><p>(evalo (1) (cons <ref type="figure" target="#fig_0">(1 1 1)</ref> )) (evalo (a) <ref type="figure">(cons (a a a)</ref>  ...</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Problem:</head><p>Input Output <ref type="figure">a a a)</ref> ... Steps for synthesizing a program that repeats a symbol three times using a subset of Lisp: (a) miniKanren builds constraints representing the PBE problem; candidate programs contain unknowns, whose values are restricted by constraints; (b) a neural agent operating on the constraints scores candidates; each constraint is embedded and scored separately, then pooled per candidate; scores determine which candidate to expand; (c) miniKanren expands the chosen candidate (cons D E ); possible completions of unknown D are added to the set of candidates; (d) this process continues until a fully-specified program (with no logic variables) is found.</p><formula xml:id="formula_1">(1) (1 1 1) (a) (</formula><p>Intuitively, the pooled score for each candidate represents the plausibility of constraints associated with a candidate partial program being satisfied. So in some sense we are learning a neural constraint satisfaction system in order to solve synthesis problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Recurrent Neural Network Model (RNN)</head><p>One way to embed the constraints is using an RNN operating on each constraint as a sequence. We use an RNN with bi-directional LSTM units <ref type="bibr" target="#b19">[20]</ref> to score constraints, with each constraint separately tokenized and embedded. The tokenization process removes identifying information of logic variables, and treats all logic variables as the same token. While logic variable identity is important, since each constraint is embedded and scored separately, the logic variable identity is lost.</p><p>We learn separate RNN weights for each relation (evalo, lookupo, etc). The particular set of constraint types differs depending on the target synthesis language.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Graph Neural Network Model (GNN)</head><p>In the RNN model, we lose considerable information by removing the identity of logic variables. Two constraints associated with a logic variable may independently be satisfiable, but may be obviously unsatisfiable together.</p><p>To address this, we use a GNN model that embeds all constraints simultaneously. The use of graph or tree structure to represent programs <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref> and constraints <ref type="bibr" target="#b22">[23]</ref> is not unprecedented. An example graph structure is shown in <ref type="figure" target="#fig_3">Figure 4</ref>. Each constraint is represented as a tree, but since logic variable leaf nodes may be shared by multiple constraints, the constraint graph is in general a Directed Acyclic Graph (DAG). We do not include the constraint tree structure (disjunctions and conjunctions) in the graph structure since they are handled during pooling.</p><p>The specific type of GNN model we use is a Gated Graph Neural Network (GGNN) <ref type="bibr" target="#b23">[24]</ref>. Each node has an initial embedding, which is refined through message passing along the edges. The final root node embedding of each constraint is taken to be the embedding representation of the constraint. Since the graph structure is a DAG, we use a synchronous message schedule for message passing. One difference between our algorithm and a typical GGNN is the use of different node types. Each token in the constraint tree (e.g. evalo, cons, logic variable) has its own aggregation function and Gated Recurrent Unit weights. Further, the edge types will also follow the node type of the parent node. Most node types will have asymmetric children, so the edge type will also depend on the position of the child.</p><p>To summarize, the GNN model has the following steps:</p><p>1. Initialization of each node, depending on the node type and label. The initial embeddings e label are learned parameters of the model. 2. Upward Pass, which is ordered leaf-to-root, so that a node receives all messages from its children and updates its embedding before sending a message to its parents. Since a non-leaf node always has a fixed number of children, the merge function is parameterized as a multi-layer perceptron (MLP) with a fixed size input. 3. Downward Pass, which is ordered root-to-leaf, so that a node receives all messages from its parents and updates its embedding before sending a message to its children. Nodes that are not logic variables will only have one parent, so no merge function is required. Constant embeddings are never updated. Logic variables can have multiple parents, so an average pooling is used as a merge function. 4. Repeat. The number of upward/downward passes is a hyperparameter. We end on an upward pass so that logic variable updates are reflected in the root node embeddings.</p><p>We extract the final embedding of the constraint root nodes for scoring, pooling, and choosing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Training</head><p>We note the similarity in the setup to a Reinforcement Learning problem. The candidates can be thought of as possible actions, the ML model as the policy, and miniKanren as the non-differentiable environment which produces the states or constraints. However, during training we have access to the ground-truth optimal action at each step, and therefore use a supervised cross-entropy loss.</p><p>We do use other techniques from the Reinforcement Learning literature. We use curriculum learning, beginning with simpler training problems. We generate training states by using the current model parameters to make action choices at least some of the time. We use scheduled sampling <ref type="bibr" target="#b24">[25]</ref> with a linear schedule, to increase exploration and reduce teacher-forcing as training progresses. We use prioritized experience replay <ref type="bibr" target="#b25">[26]</ref>, to reduce correlation in a minibatch, and re-sample more difficult states. To prevent an exploring agent from becoming "stuck", we stop episodes after 20 consecutive incorrect choices. For optimization we use RMSProp <ref type="bibr" target="#b26">[27]</ref>, with weight decay for regularization.</p><p>Importantly, we choose to expand two candidates per step during training, instead of the single candidate as described earlier, allowing miniKanren to expand both candidates. We find that expanding two candidates during training allows a better balance of exploration / exploitation during training, leading to a more robust model. During test time, we resume expanding one candidate per step, and use a greedy policy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>Following the programming languages community, we focus on tree manipulation as a natural starting point towards expressive computation. We use a small subset of Lisp as our target language. This subset consists of cons, car, cdr, along with several constants and function application. The full grammar is shown in <ref type="figure" target="#fig_4">Figure 5</ref>. We present two experiments. First, we test on programmatically generated synthesis problems held out from training. We compare two miniKanren search strategies that do not use a neural guide, three of our neural-guided models, and RobustFill with a generous beam size. Then, we test the generalizability of these approaches on three families of synthesis problems. In this second set of experiments we additionally compare against state-of-the-art systems λ 2 , Escher, and Myth. All test experiments are run on Intel i7-6700 3.40GHz CPU with 16GB RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Tree Manipulation Problems</head><p>We programmatically generate training data by running (evalo P I O ) where the program, inputs, and outputs are all unknown. We put several other restrictions on the inputs and outputs so that the examples are sufficiently expressive. If input/output expressions contain constants (which they usually do), we choose random constants to ensure that a variety of constants appear in training. We use 500 generated problems for training, each with 5 input/output examples. For this section we report results on 100 generated test problems. We report results for a combinations of symbolic-only and neural guided models. Sample generated problems are included in Appendix B.</p><p>We compare two variants of symbolic methods that use miniKanren. The "Naive" model uses biased-interleaving search, as described in <ref type="bibr" target="#b27">[28]</ref>. The "+ Heuristic" model uses additional hand tuned heuristics described in <ref type="bibr" target="#b15">[16]</ref>. The neural guided models include the RNN+Constraints guided search described in Section 4.1 and the GNN+Constraints guided search in Section 4.2. The RNN model uses 2-layer bi-directional LSTMs with embedding size of 128. The GNN model uses a single up/down/up pass with embedding size 64 and message size 128. Increasing the number of passes did not yield improvements. Further, we compare against a baseline RNN model that does not take constraints as input: instead, it computes embeddings of the input, output, and the candidate partial program using an LSTM, then scores the concatenated embeddings using a MLP. This baseline model also uses 2-layer bi-directional LSTMs with embedding size of 128. All models use a 2-layer neural network with ReLU activation as the scoring function. <ref type="table" target="#tab_2">Table 2</ref> reports the percentage of problems solved within 200 steps. The maximum time the RNNGuided search used was 11 minutes, so we allow the symbolic-only models up to 30 minutes. The GNN-Guided search is significantly more computationally expensive, and the RNN baseline model (without constraints) is comparable to the RNN-Guided models (with constraints as inputs). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generalizability</head><p>In this experiment, we explore generalizability. We use the same model weights as above to synthesize three families of programs of varying complexity: Repeat(N) which repeats a token N times, DropLast(N) which drops the last element in an N element list, and BringToFront(N) which brings the last element to the front in an N element list. As a measure of how synthesis difficulty increases with N , Repeat(N) takes 4 + 3N steps, DropLast(N) takes We compare against state-of-the-art systems λ 2 , Escher, and Myth. It is difficult to compare our models against other systems fairly, since these symbolic systems use type information, which provides an advantage. Further, λ 2 assumes advanced language constructs like fold that other methods do not. Escher is built as an active learner, and requires an "oracle" to provide outputs for additional inputs. We do not enable this functionality of Escher, and limit the number of input/output examples to 5 for all methods. We allow every method up to 30 minutes. We also compare against RobustFill Attention-C with a beam size of 5000, the largest beam size supported by our test hardware. Our model is further restricted to 200 steps for consistency with Section 5.1.</p><p>Note that if given the full 30 minutes, the RNN+Constraints model is able to synthesize DropLast <ref type="formula">(7)</ref> and BringToFront <ref type="bibr" target="#b5">(6)</ref>, and the GNN+Constraints model is also able to synthesize DropLast(7). Myth solves Repeat(N) much faster than our model, taking less than 15ms per problem, but fails on DropLast and BringToFront. Results are shown in <ref type="table" target="#tab_3">Table 3</ref>.</p><p>In summary, the RNN+Constraints and GNN+Constraints models both solve problems much larger than those seen in training. The results suggest that using constraints helps generalization: though RobustFill performs best in Section 5.1, it does not generalize to larger problems out of distribution; though RNN+Constraints and RNN-without-constraints perform comparably in Section 5.1, the former shows better generalizability. The result is consistent with the observation that as program sizes grow, the corresponding constraints grow more slowly. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We have built a neural guided synthesis model that works directly with miniKanren's constraint representations, and a transparent implementation of miniKanren useful for other synthesis researchers, available at https://github.com/xuexue/neuralkanren. We have demonstrated the success of our approach on challenging tree manipulation and, more importantly, generalization tasks. These results indicate that our approach is a promising stepping stone towards more general computation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Neural Guided Synthesis Approach</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Expansion of an evalo constraint</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Steps for synthesizing a program that repeats a symbol three times using a subset of Lisp: (a) miniKanren builds constraints representing the PBE problem; candidate programs contain unknowns, whose values are restricted by constraints; (b) a neural agent operating on the constraints scores candidates; each constraint is embedded and scored separately, then pooled per candidate; scores determine which candidate to expand; (c) miniKanren expands the chosen candidate (cons D E ); possible completions of unknown D are added to the set of candidates; (d) this process continues until a fully-specified program (with no logic variables) is found.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Graph representation of constraints (evalo A (1) (cons (1 1 1) B )) and (evalo A (a) (cons (a a a) C ))</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Subset of Lisp used in this work</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>+ 4 steps. The largest training program takes optimally 22 steps to synthesize. The number of optimal steps in synthesis correlates linearly with program size.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Symbolic Methods</figDesc><table>Method 
Direction 
Search Strategy 
Type Discipline 
miniKanren [8, 16] Top-down Biased-Interleaving 
Dynamic 
λ 
2 [11] 
Top-down Template Complexity 
Static 
Escher </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Synthesis Results on Tree Manipulation ProblemsAll three neural guided models performed better than symbolic methods in our tests, with the RNN+Constraints model solving all but one problem. The RNN model without constraints also performed reasonably, but took more steps on average than other models. RobustFill [4] Attention-C with large beam size solves one more problem than RNN+Constraints on a flattened representation of these problems. Exploration of beam size is in Appendix D. We defer comparison with other symbolic systems because problems in this section involve dynamically-typed, improper list construction.</figDesc><table>Method 
Percent Solved Average Steps 
Naive [28] 
27% 
N/A 
+Heuristics (Barliman) [16] 
82% 
N/A 
RNN-Guided (No Constraints) 93% 
46.7 
GNN-Guided + Constraints 
88% 
44.5 
RNN-Guided + Constraints 
99% 
37.0 
RobustFill [4] beam 1000+ 
100% 
N/A 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Generalization Results: largest N for which synthesis succeeded, and failure modes (out of time, out of memory, requires oracle, other error)</figDesc><table>Method 

Repeat(N) 
DropLast(N) BringToFront(N) 

Naive [28] 
6 (time) 
2 (time) 
-(time) 
+Heuristics [16] 
11 (time) 
3 (time) 
-(time) 
RNN-Guided + Constraints 
20+ 
6 (time) 
5 (time) 
GNN-Guided + Constraints 
20+ 
6 (time) 
6 (time) 
RNN-Guided (no constraints) 9 (time) 
3 (time) 
2 (time) 
λ 
2 [11] 
4 (memory) 3 (error) 
3 (error) 
Escher [12] 
10 (error) 
1 (oracle) 
-(oracle) 
Myth [13] 
20+ 
-(error) 
-(error) 
RobustFill [4] beam 1000 
1 
1 
-(error) 
RobustFill [4] beam 5000 
3 
1 
-(error) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The name "Kanren" comes from the Japanese word for "relation". Preprint. Work in progress. arXiv:1809.02840v2 [cs.LG] 13 Sep 2018</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">In miniKanren convention, a relation is named after the corresponding function, with an 'o' at the end. Appendix A provides a definition of evalo used in our experiments.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Research reported in this publication was supported in part by the Natural Sciences and Engineering Research Council of Canada, and the National Center For Advancing Translational Sciences of the National Institutes of Health under Award Number OT2TR002517. The content is solely the responsibility of the authors and does not necessarily represent the official views of the funding agencies.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix A Relational Interpreter</head><p>We include below the code for the relational interpreter, written in miniKanren. For readability by machine learning audience, our main paper renames the inputs to the relational interpreter: expr or expression is called P or program in the main paper, env or environment is called I or input, and value is called O or output.</p><p>(define-relation (evalo expr env value) (conde ;; conde creates a disjunction <ref type="bibr">((fresh (body)</ref> ;; fresh creates new variables and a conjunction (== '(lambda ,body) expr)</p><p>;; expr is a lambda definition (== '(closure ,body ,env) value))) ((== '(quote ,value) expr))</p><p>;; expr is a literal constant ((fresh (a*) (== '(list . ,a*) expr) ;; expr is a list construction (eval-listo a* env value))) ((fresh (index) (== '(var ,index) expr) ;; expr is a variable (lookupo index env value))) ((fresh (rator rand arg env^body) (== '(app ,rator ,rand) expr) ;; expr is a function application (evalo rator env '(closure ,body ,env^)) (evalo rand env arg) (evalo body '(,arg . ,env^) value))) ((fresh (a d va vd) (== '(cons ,a ,d) expr) ;; expr is a cons operation (== '(,va . ,vd) value) (evalo a env va) (evalo d env vd))) ((fresh (c vd) (== '(car ,c) expr) ;; expr is a car operation (evalo c env '(,value . ,vd)))) ((fresh (c va) (== '(cdr ,c) expr) ;; expr is a cdr operation (evalo c env '(,va . ,value))))))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix B Example Generated Problems</head><p>Some examples of automatically generated problems are shown in <ref type="table">Table A1</ref>. Variables in a function body are encoded using de Bruijn indices, so that (var ()) is looking up the 0th (and only) variable. The symbol . denotes a pair. <ref type="table">Table A1</ref>: Sample auto-generated training problems  <ref type="figure">((s (1 #t s . a) . x)</ref> . #t) . #t) (1 #t s . a) <ref type="table">Table A2</ref> lists problems on which the methods failed. The single problem that RNN + Constraints failed to solve is a fairly complex problem. The problems that the GNN + Constraints failed to solve all include a complex list accessor portion. This actually makes sense: it is conceivable for multi-layer RNNs to be better at this kind of problem compared to a single-layer GNN. The RNN without constraints also fails at complex list accessor problems.  <ref type="figure">(var ())</ref>))))) GNN + Constraints (lambda (car (car (car (car (cdr (cdr (car (var ()))))))))) (lambda (car (car (car (cdr (car (cdr (car (var ()))))))))) (lambda (car (car (car (cdr (cdr (cdr (car (var ()))))))))) (lambda (car (car (cdr (car (car (var ()))))))) (lambda (car (car (cdr (car (cdr (cdr (car (var ()))))))))) (lambda (car (car (cdr (cdr (cdr (cdr (car (var ()))))))))) (lambda (car (cdr (car (car (cdr (var ()))))))) (lambda (car (cdr (car (cdr (cdr (car (var ())))))))) (lambda (car (cdr (cdr (car (car (cdr (var ())))))))) (lambda (car (cdr (cdr (cdr (cdr (car (car (var ()))))))))) (lambda (car (cdr (cdr (cdr (cdr (car (cdr (var ()))))))))) (lambda (cdr (cdr (car (car (var ())))))) RNN (No Constraints) (lambda (cons (car (var ())) (cons (var ()) (cdr (car (var ())))))) (lambda (cdr (car (car (cdr (car (cdr (var ())))))))) (lambda (cdr (car (cdr (car (car (car (var ())))))))) (lambda (cdr (car (car (car (car (car (var ())))))))) (lambda (cdr (car (car (cdr (car (cdr (var ())))))))) (lambda (cdr (car (cdr (car (car (car (var ())))))))) (lambda (cdr (car (car (car (car (car (var ()))))))))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix C Problems where Neural Guided Synthesis Fails</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix D RobustFill Results for Various Beam Sizes</head><p>To compare against RobustFill, we use a flattened representation of the problems shown in Section B, and use the Attention-C model with various beam sizes. For a beam size k, if any of the top-k generated programs are correct, we consider the synthesis a success. We report several figures in <ref type="table">Table A3</ref>: column (a) shows the percent of test problems held out from training that were successfully solved ( <ref type="table">Table 2</ref> in our paper), and column (b) shows the largest N for a family of synthesis problems for which synthesis succeeds <ref type="table">(Table 3</ref> in our paper). </p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural programmer: Inducing latent programs with gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arvind</forename><surname>Neelakantan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nando De Freitas</surname></persName>
		</author>
		<title level="m">Neural programmer-interpreters. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ivo</forename><surname>Danihelka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.5401</idno>
		<title level="m">Neural turing machines</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">RobustFill: Neural program learning under noisy I/O</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Uesato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Neuro-symbolic program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emilio</forename><surname>Parisotto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abdel-Rahman</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dengyong</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepcoder: Learning to write programs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Balog</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Neural-guided deductive search for real-time program synthesis from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashwin</forename><surname>Kalyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhishek</forename><surname>Mohta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oleksandr</forename><surname>Polozov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Prateek</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">From variadic functions to variadic relations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
		<idno>TR-2006-06</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2006 Scheme and Functional Programming Workshop</title>
		<meeting>the 2006 Scheme and Functional Programming Workshop</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="105" to="117" />
		</imprint>
		<respStmt>
			<orgName>University of Chicago</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">µkanren: A minimal functional core for relational programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Hemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Scheme and Functional Programming Workshop</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franco</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ah</forename><surname>Chung Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriele</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="61" to="80" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Synthesizing data structure transformations from input-output examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">K</forename><surname>Feser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swarat</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Isil</forename><surname>Dillig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="229" to="239" />
			<date type="published" when="2015" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Recursive program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aws</forename><surname>Albarghouthi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Kincaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Aided Verification</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="934" to="950" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Type-and-example-directed program synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steve</forename><surname>Osera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zdancewic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM SIGPLAN Notices</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="page" from="619" to="630" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A methodology for lisp program construction from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><forename type="middle">D</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the ACM (JACM)</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="161" to="175" />
			<date type="published" when="1977" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">The inference of regular lisp programs from examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">W</forename><surname>Biermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on Systems, Man, and Cybernetics</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="585" to="600" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A unified approach to solving seven programming problems (functional pearl)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Ballantyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Rosenblatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Might</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the ACM on Programming Languages</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automating string processing in spreadsheets using input-output examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sumit</forename><surname>Gulwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM SIGPLAN Notices</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="page" from="317" to="330" />
			<date type="published" when="2011" />
			<publisher>ACM</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Surya</forename><surname>Bhupatiraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04327</idno>
		<title level="m">Abdel-rahman Mohamed, and Pushmeet Kohli. Deep API programmer: Learning to program with APIs</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Terpret: A probabilistic programming language for program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">L</forename><surname>Gaunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rishabh</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nate</forename><surname>Kushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pushmeet</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04428</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning to represent programs with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miltiadis</forename><surname>Allamanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mahmoud</forename><surname>Khademi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Tree-to-tree neural networks for program translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinyun</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dawn</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a sat solver from single-bit supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Selsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Lamm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benedikt</forename><surname>Bunz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Percy</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonardo</forename><surname>De Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David L</forename><surname>Dill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Gated graph sequence neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yujia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Scheduled sampling for sequence prediction with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Shazeer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1171" to="1179" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Quan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<title level="m">Prioritized experience replay. International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Lecture 6.5-RmsProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COURSERA: Neural Networks for Machine Learning</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">miniKanren, live and untagged: Quine generation via relational interpreters (programming pearl</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">E</forename><surname>Byrd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Holk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><forename type="middle">P</forename><surname>Friedman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2012 Annual Workshop on Scheme and Functional Programming</title>
		<meeting>the 2012 Annual Workshop on Scheme and Functional Programming</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="8" to="29" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
