<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">COLA: Decentralized Linear Learning Lie He</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⇤</forename><surname>Epfl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Bian</surname></persName>
							<email>ybian@inf.ethz.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
							<email>epflmartin.jaggi@epfl.ch</email>
							<affiliation key="aff0">
								<orgName type="institution">ETH Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">COLA: Decentralized Linear Learning Lie He</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Decentralized machine learning is a promising emerging paradigm in view of global challenges of data ownership and privacy. We consider learning of linear classification and regression models, in the setting where the training data is decentralized over many user devices, and the learning algorithm must run ondevice, on an arbitrary communication network, without a central coordinator. We propose COLA, a new decentralized training algorithm with strong theoretical guarantees and superior practical performance. Our framework overcomes many limitations of existing methods, and achieves communication efficiency, scalability, elasticity as well as resilience to changes in data and allows for unreliable and heterogeneous participating devices.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>With the immense growth of data, decentralized machine learning has become not only attractive but a necessity. Personal data from, for example, smart phones, wearables and many other mobile devices is sensitive and exposed to a great risk of data breaches and abuse when collected by a centralized authority or enterprise. Nevertheless, many users have gotten accustomed to giving up control over their data in return for useful machine learning predictions (e.g. recommendations), which benefits from joint training on the data of all users combined in a centralized fashion.</p><p>In contrast, decentralized learning aims at learning this same global machine learning model, without any central server. Instead, we only rely on distributed computations of the devices themselves, with each user's data never leaving its device of origin. While increasing research progress has been made towards this goal, major challenges in terms of the privacy aspects as well as algorithmic efficiency, robustness and scalability remain to be addressed. Motivated by aforementioned challenges, we make progress in this work addressing the important problem of training generalized linear models in a fully decentralized environment.</p><p>Existing research on decentralized optimization, min x2R n F (x), can be categorized into two main directions. The seminal line of work started by Bertsekas and Tsitsiklis in the 1980s, cf. <ref type="bibr" target="#b0">[Tsitsiklis et al., 1986]</ref>, tackles this problem by splitting the parameter vector x by coordinates/components among the devices. A second more recent line of work including e.g. <ref type="bibr" target="#b1">[Nedic and Ozdaglar, 2009</ref><ref type="bibr" target="#b2">, Duchi et al., 2012</ref><ref type="bibr" target="#b3">, Shi et al., 2015</ref><ref type="bibr" target="#b4">, Mokhtari and Ribeiro, 2016</ref><ref type="bibr" target="#b5">, Nedic et al., 2017</ref> addresses sum-structured F (x) = P k F k (x) where F k is the local cost function of node k. This structure is closely related to empirical risk minimization in a learning setting. See e.g. <ref type="bibr" target="#b6">[Cevher et al., 2014]</ref> for an overview of both directions. While the first line of work typically only provides convergence guarantees for smooth objectives F , the second approach often suffers from a "lack of consensus", that is, the minimizers of {F k } k are typically different since the data is not distributed i.i.d. between devices in general.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1">Problem statement</head><p>Setup. Many machine learning and signal processing models are formulated as a composite convex optimization problem of the form min</p><formula xml:id="formula_0">u l(u) + r(u),</formula><p>where l is a convex loss function of a linear predictor over data and r is a convex regularizer. Some cornerstone applications include e.g. logistic regression, SVMs, Lasso, generalized linear models, each combined with or without L1, L2 or elastic-net regularization. Following the setup of <ref type="bibr" target="#b10">[Dünner et al., 2016</ref><ref type="bibr" target="#b7">, Smith et al., 2018</ref>, these training problems can be mapped to either of the two following formulations, which are dual to each other</p><formula xml:id="formula_1">min x2R n ⇥ F A (x) := f (Ax) + P i g i (x i ) ⇤ (A) min w2R d ⇥ F B (w) := f ⇤ (w) + P i g ⇤ i ( A &gt; i w) ⇤ ,<label>(B)</label></formula><p>where f ⇤ , g ⇤ i are the convex conjugates of f and g i , respectively. Here x 2 R n is a parameter vector and A := [A 1 ; . . . ; A n ] 2 R d⇥n is a data matrix with column vectors</p><formula xml:id="formula_2">A i 2 R d , i 2 [n]</formula><p>. We assume that f is smooth (Lipschitz gradient) and g(x) := P n i=1 g i (x i ) is separable. Data partitioning. As in <ref type="bibr" target="#b11">[Jaggi et al., 2014</ref><ref type="bibr" target="#b10">, Dünner et al., 2016</ref><ref type="bibr" target="#b7">, Smith et al., 2018</ref>, we assume the dataset A is distributed over K machines according to a partition {P k } K k=1 of the columns of A. Note that this convention maintains the flexibility of partitioning the training dataset either by samples (through mapping applications to (B), e.g. for SVMs) or by features (through mapping applications to (A), e.g. for Lasso or L1-regularized logistic regression). For x 2 R n , we write</p><formula xml:id="formula_3">x [k] 2 R n for the n-vector with elements (x [k] ) i := x i if i 2 P k and (x [k] ) i := 0 otherwise, and analogously A [k] 2 R</formula><p>d⇥n k for the corresponding set of local data columns on node k, which is of size n k = |P k |.</p><p>Network topology. We consider the task of joint training of a global machine learning model in a decentralized network of K nodes. Its connectivity is modelled by a mixing matrix W 2 R K⇥K + . More precisely, W ij 2 [0, 1] denotes the connection strength between nodes i and j, with a non-zero weight indicating the existence of a pairwise communication link. We assume W to be symmetric and doubly stochastic, which means each row and column of W sums to one.</p><p>The spectral properties of W used in this paper are that the eigenvalues of W are real, and 1 = 1 (W) · · · n (W) 1. Let the second largest magnitude of the eigenvalues of W be := max{| 2 (W)|, | n (W)|}. 1 is called the spectral gap, a quantity well-studied in graph theory and network analysis. The spectral gap measures the level of connectivity among nodes. In the extreme case when W is diagonal, and thus an identity matrix, the spectral gap is 0 and there is no communication among nodes. To ensure convergence of decentralized algorithms, we impose the standard assumption of positive spectral gap of the network which includes all connected graphs, such as e.g. a ring or 2-D grid topology, see also Appendix B for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2">Related work</head><p>Research in decentralized optimization dates back to the 1980s with the seminal work of Bertsekas and Tsitsiklis, cf. <ref type="bibr" target="#b0">[Tsitsiklis et al., 1986]</ref>. Their framework focuses on the minimization of a (smooth) function by distributing the components of the parameter vector x among agents. In contrast, a second more recent line of work <ref type="bibr" target="#b1">[Nedic and Ozdaglar, 2009</ref><ref type="bibr" target="#b2">, Duchi et al., 2012</ref><ref type="bibr" target="#b3">, Shi et al., 2015</ref><ref type="bibr" target="#b4">, Mokhtari and Ribeiro, 2016</ref><ref type="bibr" target="#b5">, Nedic et al., 2017</ref><ref type="bibr" target="#b12">, Scaman et al., 2017</ref> considers minimization of a sum of individual local cost-functions F (x) = P i F i (x), which are potentially non-smooth. Our work here can be seen as bridging the two scenarios to the primal-dual setting (A) and (B).</p><p>While decentralized optimization is a relatively mature area in the operations research and automatic control communities, it has recently received a surge of attention for machine learning applications, see e.g. <ref type="bibr" target="#b6">[Cevher et al., 2014]</ref>. Decentralized gradient descent (DGD) with diminishing stepsizes was proposed by <ref type="bibr">Ozdaglar, 2009, Jakovetic et al., 2012]</ref>, showing convergence to the optimal solution at a sublinear rate. <ref type="bibr" target="#b15">[Yuan et al., 2016]</ref> further prove that DGD will converge to the neighborhood of a global optimum at a linear rate when used with a constant stepsize for strongly convex objectives. <ref type="bibr" target="#b3">[Shi et al., 2015]</ref> present EXTRA, which offers a significant performance boost compared to DGD by using a gradient tracking technique. <ref type="bibr" target="#b5">[Nedic et al., 2017]</ref> propose the DIGing algorithm to handle a time-varying network topology. For a static and symmetric W, DIGing recovers EXTRA by redefining the two mixing matrices in EXTRA. The dual averaging method <ref type="bibr" target="#b2">[Duchi et al., 2012]</ref> converges at a sublinear rate with a dynamic stepsize. Under a strong convexity assumption, decomposition techniques such as decentralized ADMM (DADMM, also known as consensus ADMM) have linear convergence for time-invariant undirected graphs, if subproblems are solved exactly <ref type="bibr" target="#b16">[Shi et al., 2014, Wei and</ref><ref type="bibr" target="#b17">Ozdaglar, 2013]</ref>. DADMM+ <ref type="bibr" target="#b18">[Bianchi et al., 2016</ref>] is a different primal-dual approach with more efficient closed-form updates in each step (as compared to ADMM), and is proven to converge but without a rate. Compared to COLA, neither of DADMM and DADMM+ can be flexibly adapted to the communication-computation tradeoff due to their fixed update definition, and both require additional hyperparameters to tune in each use-case (including the ⇢ from ADMM). Notably COLA shows superior performance compared to DIGing and decentralized ADMM in our experiments. <ref type="bibr" target="#b12">[Scaman et al., 2017</ref><ref type="bibr" target="#b13">[Scaman et al., , 2018</ref> present lower complexity bounds and optimal algorithms for objectives in the form F (x) = P i F i (x). Specifically, <ref type="bibr" target="#b12">[Scaman et al., 2017]</ref> assumes each F i (x) is smooth and strongly convex, and <ref type="bibr" target="#b13">[Scaman et al., 2018]</ref> assumes each F i (x) is Lipschitz continuous and convex. Additionally <ref type="bibr" target="#b13">[Scaman et al., 2018</ref>] needs a boundedness constraint for the input problem. In contrast, COLA can handle non-smooth and non-strongly convex objectives (A) and (B), suited to the mentioned applications in machine learning and signal processing. For smooth nonconvex models, <ref type="bibr" target="#b19">[Lian et al., 2017]</ref> demonstrate that a variant of decentralized parallel SGD can outperform the centralized variant when the network latency is high. They further extend it to the asynchronous setting <ref type="bibr" target="#b20">[Lian et al., 2018]</ref> and to deal with large data variance among nodes <ref type="bibr" target="#b21">[Tang et al., 2018a]</ref> or with unreliable network links <ref type="bibr" target="#b22">[Tang et al., 2018b]</ref>. For the decentralized, asynchronous consensus optimization, <ref type="bibr" target="#b23">[Wu et al., 2018]</ref> extends the existing PG-EXTRA and proves convergence of the algorithm. <ref type="bibr" target="#b24">[Sirb and Ye, 2018]</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>proves a O(K/✏</head><p>2 ) rate for stale and stochastic gradients. <ref type="bibr" target="#b20">[Lian et al., 2018]</ref> achieves O(1/✏) rate and has linear speedup with respect to number of workers.</p><p>In the distributed setting with a central server, algorithms of the COCOA family <ref type="bibr" target="#b25">[Yang, 2013</ref><ref type="bibr" target="#b11">, Jaggi et al., 2014</ref><ref type="bibr" target="#b26">, Ma et al., 2015</ref><ref type="bibr" target="#b27">, Dünner et al., 2018</ref>]-see <ref type="bibr" target="#b7">[Smith et al., 2018]</ref> for a recent overvieware targeted for problems of the forms (A) and (B). For convex models, COCOA has shown to significantly outperform competing methods including e.g., ADMM, distributed SGD etc. Other centralized algorithm representatives are parallel SGD variants such as <ref type="bibr">Duchi, 2011, Zinkevich et al., 2010]</ref> and more recent distributed second-order methods <ref type="bibr" target="#b30">[Zhang and Lin, 2015</ref><ref type="bibr" target="#b31">, Reddi et al., 2016</ref><ref type="bibr" target="#b32">, Gargiani, 2017</ref><ref type="bibr" target="#b33">, Lee and Chang, 2017</ref><ref type="bibr" target="#b27">, Dünner et al., 2018</ref>.</p><p>In this paper we extend COCOA to the challenging decentralized environment-with no central coordinator-while maintaining all of its nice properties. We are not aware of any existing primaldual methods in the decentralized setting, except the recent work of <ref type="bibr" target="#b35">[Smith et al., 2017]</ref> on federated learning for the special case of multi-task learning problems. Federated learning was first described by <ref type="bibr" target="#b36">[Konecnỳ et al., 2015</ref><ref type="bibr" target="#b38">, McMahan et al., 2017</ref> as decentralized learning for on-device learning applications, combining a global shared model with local personalized models. Current </p><formula xml:id="formula_4">x (0) := 0 2 R n , v (0) := 0 2 R d , v (0) k := 0 2 R d 8 k = 1, . . . K; 2 for t = 0, 1, 2, . . . , T do 3</formula><p>for k 2 {1, 2, . . . , K} in parallel over all nodes do 4 compute locally averaged shared vector v</p><formula xml:id="formula_5">(t+ 1 2 ) k := P K l=1 W kl v (t) l 5 x [k] ⇥-approximate solution to subproblem (1) at v (t+ 1 2 ) k 6 update local variable x (t+1) [k] := x (t) [k] + x [k] 7 compute update of local estimate v k := A [k] x [k] 8 v (t+1) k := v (t+ 1 2 ) k + K v k 9</formula><p>end 10 end federated optimization algorithms (like FedAvg in <ref type="bibr" target="#b38">[McMahan et al., 2017]</ref>) are still close to the centralized setting. In contrast, our work provides a fully decentralized alternative algorithm for federated learning with generalized linear models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">The decentralized algorithm: COLA</head><p>The COLA framework is summarized in Algorithm 1. For a given input problem we map it to either of the (A) or (B) formulation, and define the locally stored dataset A <ref type="bibr">[k]</ref> and local part of the weight vector x <ref type="bibr">[k]</ref> in node k accordingly. While v = Ax is the shared state being communicated in COCOA, this is generally unknown to a node in the fully decentralized setting. Instead, we maintain v k , a local estimate of v in node k, and use it as a surrogate in the algorithm.</p><p>New data-local quadratic subproblems. During a computation step, node k locally solves the following minimization problem min</p><formula xml:id="formula_6">x [k] 2R n G 0 k ( x [k] ; v k , x [k] ),<label>(1)</label></formula><p>where</p><formula xml:id="formula_7">G 0 k ( x [k] ; v k , x [k] ) := 1 K f (v k ) + rf (v k ) &gt; A [k] x [k] + 0 2⌧ A [k] x [k] 2 + P i2P k g i (x i + ( x [k] ) i ).<label>(2)</label></formula><p>Crucially, this subproblem only depends on the local data A <ref type="bibr">[k]</ref> , and local vectors v l from the neighborhood of the current node k. In contrast, in COCOA <ref type="bibr" target="#b7">[Smith et al., 2018]</ref> the subproblem is defined in terms of a global aggregated shared vector v c := Ax 2 R d , which is not available in the decentralized setting.</p><p>2 The aggregation parameter 2 [0, 1] does not need to be tuned; in fact, we use the default := 1 throughout the paper, see <ref type="bibr" target="#b26">[Ma et al., 2015]</ref> for a discussion. Once is settled, a safe choice of the subproblem relaxation parameter 0 is given as 0 := K. 0 can be additionally tightened using an improved Hessian subproblem (Appendix E.3).</p><formula xml:id="formula_8">Algorithm description. At time t on node k, v (t+ 1 2 ) k</formula><p>is a local estimate of the shared variable after a communication step (i.e. gossip mixing). The local subproblem (1) based on this estimate is solved 2 Subproblem interpretation: Note that for the special case of := 1, 0 := K, by smoothness of f , our subproblem in (2) is an upper bound on</p><formula xml:id="formula_9">min x [k] 2R n 1 K f (A(x + K x [k] )) + P i2P k gi(xi + ( x [k] ) i ),<label>(3)</label></formula><p>which is a scaled block-coordinate update of block k of the original objective (A). This assumes that we have consensus v k ⌘ Ax 8 k. For quadratic objectives (i.e. when f ⌘ k.k 2 2 and A describes the quadratic), the equality of the formulations (2) and (3) holds. Furthermore, by convexity of f , the sum of <ref type="formula" target="#formula_9">(3)</ref> is an upper bound on the centralized updates f (x + x) + g(x + x). Both inequalities quantify the overhead of the distributed algorithm over the centralized version, see also <ref type="bibr" target="#b25">[Yang, 2013</ref><ref type="bibr" target="#b26">, Ma et al., 2015</ref><ref type="bibr" target="#b7">, Smith et al., 2018</ref>  </p><formula xml:id="formula_10">( x [k] ; v k , x [k] ) G 0 k ( x ? [k] ; v k , x [k] )] G 0 k ( 0 ; v k , x [k] ) G 0 k ( x ? [k] ; v k , x [k] )  ⇥, where x ? [k] 2 arg min x2R n G 0 k ( x [k] ; v k , x [k] ), for each k 2 [K].</formula><p>Elasticity to network size, compute resources and changing data-and fault tolerance. Realworld communication networks are not homogeneous and static, but greatly vary in availability, computation, communication and storage capacity. Also, the training data is subject to changes. While these issues impose significant challenges for most existing distributed training algorithms, we hereby show that COLA offers adaptivity to such dynamic and heterogenous scenarios.</p><p>Scalability and elasticity in terms of availability and computational capacity can be modelled by a node-specific local accuracy parameter ⇥ k in Assumption 1, as proposed by <ref type="bibr" target="#b35">[Smith et al., 2017]</ref>. The more resources node k has, the more accurate (smaller) ⇥ k we can use. The same mechanism also allows dealing with fault tolerance and stragglers, which is crucial e.g. on a network of personal devices. More specifically, when a new node k joins the network, its x <ref type="bibr">[k]</ref> variables are initialized to 0; when node k leaves, its x [k] is frozen, and its subproblem is not touched anymore (i.e. ⇥ k = 1).</p><p>Using the same approach, we can adapt to dynamic changes in the dataset-such as additions and removal of local data columns-by adjusting the size of the local weight vector accordingly. Unlike gradient-based methods and ADMM, COLA does not require parameter tuning to converge, increasing resilience to drastic changes.</p><p>Extension to improved second-order subproblems. In the centralized setting, it has recently been shown that the Hessian information of f can be properly utilized to define improved local subproblems <ref type="bibr">Chang, 2017, Dünner et al., 2018]</ref>. Similar techniques can be applied to COLA as well, details on which are left in Appendix E.</p><p>Extension to time-varying graphs. Similar to scalability and elasticity, it is also straightforward to extend COLA to a time varying graph under proper assumptions. If we use the time-varying model in <ref type="bibr" target="#b5">[Nedic et al., 2017</ref>, Assumption 1], where an undirected graph is connected with B gossip steps, then changing COLA to perform B communication steps and one computation step per round still guarantees convergence. Details of this setup are provided in Appendix E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">On the convergence of COLA</head><p>In this section we present a convergence analysis of the proposed decentralized algorithm COLA for both general convex and strongly convex objectives. In order to capture the evolution of COLA, we reformulate the original problem (A) by incorporating both x and local estimates {v k } K k=1</p><formula xml:id="formula_11">min x,{v k } K k=1 H A (x, {v k } K k=1 ) := 1 K P K k=1 f (v k ) + g(x),<label>(DA)</label></formula><p>such that</p><formula xml:id="formula_12">v k = Ax, k = 1, ..., K.</formula><p>While the consensus is not always satisfied during Algorithm 1, the following relations between the decentralized objective and the original one (A) always hold. All proofs are deferred to Appendix C. Lemma 1. Let {v k } and x be the iterates generated during the execution of Algorithm 1. At any timestep, it holds that</p><formula xml:id="formula_13">1 K P K k=1 v k = Ax,<label>(4)</label></formula><formula xml:id="formula_14">F A (x)  H A (x, {v k } K k=1 )  F A (x) + 1 2⌧ K P K k=1 kv k Axk 2 .<label>(5)</label></formula><p>The dual problem and duality gap of the decentralized objective (DA) are given in Lemma 2. Lemma 2 (Decentralized Dual Function and Duality Gap). The Lagrangian dual of the decentralized formation (DA) is</p><formula xml:id="formula_15">min {w k } K k=1 H B ({w k } K k=1 ) := 1 K P K k=1 f ⇤ (w k ) + P n i=1 g ⇤ i ⇣ A &gt; i ( 1 K P K k=1 w k ) ⌘ . (DB) Given primal variables {x, {v k } K k=1 } and dual variables {w k } K k=1</formula><p>, the duality gap is:</p><formula xml:id="formula_16">G H (x, {v k } K k=1 , {w k } K k=1 ) := 1 K P k (f (v k )+f ⇤ (w k )) + g(x)+ P n i=1 g ⇤ i 1 K P k A &gt; i w k . (6)</formula><p>If the dual variables are fixed to the optimality condition w k = rf (v k ), then the dual variables can be omitted in the argument list of duality gap, namely</p><formula xml:id="formula_17">G H (x, {v k } K k=1</formula><p>). Note that the decentralized duality gap generalizes the duality gap of COCOA: when consensus is ensured, i.e., v k ⌘ Ax and w k ⌘ rf (Ax), the decentralized duality gap recovers that of COCOA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Linear rate for strongly convex objectives</head><p>We use the following data-dependent quantities in our main theorems </p><formula xml:id="formula_18">k := max x [k] 2R n A [k] x [k] 2 /kx [k] k 2 , max = max k=1,...,K k , := P K k=1 k n k .<label>(</label></formula><formula xml:id="formula_19">s 0 = ⌧ µg ⌧ µg+ max¯ 0 2 [0, 1].<label>(8)</label></formula><p>Then after T iterations of Algorithm 1 with</p><formula xml:id="formula_20">3 T 1+⌘s0 ⌘s0 log " (0) H " H , it holds that E ⇥ H A (x (T )</formula><p>, {v</p><formula xml:id="formula_21">(T ) k } K k=1 ) H A (x ? , {v ? k } K k=1 ) ⇤  " H . Furthermore,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>after T iterations with</head><formula xml:id="formula_22">T 1+⌘s0 ⌘s0 log ✓ 1 ⌘s0 " (0) H " G H , ◆ we have the expected duality gap E[G H (x (T ) , { P K k=1 W kl v (T ) l } K k=1 )]  " G H .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Sublinear rate for general convex objectives</head><p>Models such as sparse logistic regression, Lasso, group Lasso are non-strongly convex. For such models, we show that COLA enjoys a O(1/T ) sublinear rate of convergence for all network topologies with a positive spectral gap. Theorem 2 (Non-strongly Convex Case). Consider Algorithm 1, using a local solver of quality ⇥. Let g i (·) have L-bounded support, and let f be (1/⌧ )-smooth. Let " G H &gt; 0 be the desired duality gap. Then after T iterations where</p><formula xml:id="formula_23">T T 0 + max ⇢ l 1 ⌘ m , 4L 2 ¯ 0 ⌧ " G H ⌘ , T 0 t 0 +  2 ⌘ ⇣ 8L 2 ¯ 0 ⌧ " G H 1 ⌘ + t 0 max ⇢ 0, ⇠ 1+⌘ ⌘ log 2⌧ (H A (x (0) ,{v (0) l }) H A (x ? ,{v ? })) 4L 2 ¯ 0 ⇡ and¯ 0 := (1 + ) 0 , ↵ := (1 + (1 ) 2 36(1+⇥) ) 1</formula><p>and ⌘ := (1 ⇥)(1 ↵). We have that the expected duality gap satisfies</p><formula xml:id="formula_24">E ⇥ G H (x, {v k } K k=1 , {w k } K k=1 ) ⇤  " G H at the averaged iteratex := 1 T T0 P T 1 t=T0+1 x (t)</formula><p>, and v</p><formula xml:id="formula_25">0 k := P K l=1 W kl v l andv k := 1 T T0 P T 1 t=T0+1 (v 0 k ) (t) andw k := 1 T T0 P T 1 t=T0+1 rf ((v 0 k ) (t) ).</formula><p>Note that the assumption of bounded support for the g i functions is not restrictive in the general convex case, as discussed e.g. in <ref type="bibr" target="#b10">[Dünner et al., 2016]</ref>.</p><formula xml:id="formula_26">3 "<label>(0)</label></formula><formula xml:id="formula_27">H := HA(x (0) , {v (0) k } K k=1 ) HA(x ? , {v ? k } K k=1</formula><p>) is the initial suboptimality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Local certificates for global accuracy</head><p>Accuracy certificates for the training error are very useful for practitioners to diagnose the learning progress. In the centralized setting, the duality gap serves as such a certificate, and is available as a stopping criterion on the master node. In the decentralized setting of our interest, this is more challenging as consensus is not guaranteed. Nevertheless, we show in the following Proposition 1 that certificates for the decentralized objective (DA) can be computed from local quantities: Proposition 1 (Local Certificates). Assume g i has L-bounded support, and let N k := {j : W jk &gt; 0} be the set of nodes accessible to node k. Then for any given " &gt; 0, we have</p><formula xml:id="formula_28">G H (x; {v k } K k=1 )  ",</formula><p>if for all k = 1, . . . , K the following two local conditions are satisfied:</p><formula xml:id="formula_29">v &gt; k rf (v k ) + X i2P k g i (x i ) + g ⇤ i ( A &gt; i rf (v k ))  " 2K (9) rf (v k ) 1 |N k | P j2N k rf (v j ) 2  ⇣ P K k=1 n 2 k k ⌘ 1/2 1 2L p K ",<label>(10)</label></formula><p>The local conditions <ref type="formula" target="#formula_18">(9)</ref> and <ref type="formula" target="#formula_6">(10)</ref> have a clear interpretation. The first one ensures the duality gap of the local subproblem given by v k as on the left hand side of <ref type="formula" target="#formula_18">(9)</ref> is small. The second condition <ref type="formula" target="#formula_6">(10)</ref> guarantees that consensus violation is bounded, by ensuring that the gradient of each node is similar to its neighborhood nodes. Remark 1. The resulting certificate from Proposition 1 is local, in the sense that no global vector aggregations are needed to compute it. For a certificate on the global objective, the boolean flag of each local condition (9) and (10) being satisfied or not needs to be shared with all nodes, but this requires extremely little communication. Exact values of the parameters and P K k=1 n 2 k k are not required to be known, and any valid upper bound can be used instead. We can use the local certificates to avoid unnecessary work on local problems which are already optimized, as well as to continuously quantify how newly arriving local data has to be re-optimized in the case of online training. The local certificates can also be used to quantify the contribution of newly joining or departing nodes, which is particularly useful in the elastic scenario described above.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental results</head><p>Here we illustrate the advantages of COLA in three respects: firstly we investigate the application in different network topologies and with varying subproblem quality ⇥; secondly, we compare COLA with state-of-the-art decentralized baselines: 1 , DIGing <ref type="bibr" target="#b5">[Nedic et al., 2017]</ref>, which generalizes the gradient-tracking technique of the EXTRA algorithm <ref type="bibr" target="#b3">[Shi et al., 2015]</ref>, and 2 , Decentralized ADMM (aka. consensus ADMM), which extends the classical ADMM (Alternating Direction Method of  Multipliers) method <ref type="bibr" target="#b39">[Boyd et al., 2011]</ref> to the decentralized setting <ref type="bibr" target="#b16">[Shi et al., 2014, Wei and</ref><ref type="bibr" target="#b17">Ozdaglar, 2013]</ref>; Finally, we show that COLA works in the challenging unreliable network environment where each node has a certain chance to drop out of the network.</p><p>We implement all algorithms in PyTorch with MPI backend. The decentralized network topology is simulated by running one thread per graph node, on a 2⇥12 core Intel Xeon CPU E5-2680 v3 server with 256 GB RAM. <ref type="table" target="#tab_1">Table 1</ref> describes the datasets 4 used in the experiments. For Lasso, the columns of A are features. For ridge regression, the columns are features and samples for COLA primal and COLA dual, respectively. The order of columns is shuffled once before being distributed across the nodes. Due to space limit, details on the experimental configurations are included in Appendix D. Effect of approximation quality ⇥. We study the convergence behavior in terms of the approximation quality ⇥. Here, ⇥ is controlled by the number of data passes  on subproblem (1) per node. <ref type="figure" target="#fig_0">Figure 1</ref> shows that increasing  always results in less number of iterations (less communication rounds) for COLA. However, given a fixed network bandwidth, it leads to a clear trade-off for the overall wall-clock time, showing the cost of both communication and computation. Larger  leads to less communication rounds, however, it also takes more time to solve subproblems. The observations suggest that one can adjust ⇥ for each node to handle system heterogeneity, as what we have discussed at the end of Section 2.</p><p>Effect of graph topology. Fixing K=16, we test the performance of COLA on 5 different topologies: ring, 2-connected cycle, 3-connected cycle, 2D grid and complete graph. The mixing matrix W is given by Metropolis weights for all test cases (details in Appendix B). Convergence curves are plotted in <ref type="figure" target="#fig_4">Figure 3</ref>. One can observe that for all topologies, COLA converges monotonically and especailly when all nodes in the network are equal, smaller leads to a faster convergence rate. This is consistent with the intuition that 1 measures the connectivity level of the topology.</p><p>Superior performance compared to baselines. We compare COLA with DIGing and D-ADMM for strongly and general convex problems. For general convex objectives, we use Lasso regression with = 10 4 on the webspam dataset; for the strongly convex objective, we use Ridge regression with = 10 5 on the URL reputation dataset. For Ridge regression, we can map COLA to both primal and dual problems. <ref type="figure" target="#fig_3">Figure 2</ref> traces the results on log-suboptimality. One can observe that for both generally and strongly convex objectives, COLA significantly outperforms DIGing and decentralized ADMM in terms of number of communication rounds and computation time. While DIGing and D-ADMM need parameter tuning to ensure convergence and efficiency, COLA is much easier to deploy as it is parameter free. Additionally, convergence guarantees of ADMM relies on exact subproblem solvers, whereas inexact solver is allowed for COLA.  Fault tolerance to unreliable nodes. Assume each node of a network only has a chance of p to participate in each round. If a new node k joins the network, then local variables are initialized as x [k] = 0; if node k leaves the network, then x [k] will be frozen with ⇥ k = 1. All remaining nodes dynamically adjust their weights to maintain the doubly stochastic property of W. We run COLA on such unreliable networks of different ps and show the results in <ref type="figure" target="#fig_5">Figure 4</ref>. First, one can observe that for all p &gt; 0 the suboptimality decreases monotonically as COLA progresses. It is also clear from the result that a smaller dropout rate (a larger p) leads to a faster convergence of COLA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and conclusions</head><p>In this work we have studied training generalized linear models in the fully decentralized setting. We proposed a communication-efficient decentralized framework, termed COLA, which is free of parameter tuning. We proved that it has a sublinear rate of convergence for general convex problems, allowing e.g. L1 regularizers, and has a linear rate of convergence for strongly convex objectives. Our scheme offers primal-dual certificates which are useful in the decentralized setting. We demonstrated that COLA offers full adaptivity to heterogenous distributed systems on arbitrary network topologies, and is adaptive to changes in network size and data, and offers fault tolerance and elasticity. Future research directions include improving subproblems, as well as extension to the network topology with directed graphs, as well as recent communication compression schemes <ref type="bibr" target="#b40">[Stich et al., 2018]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Algorithm 1 :</head><label>1</label><figDesc>COLA: Communication-Efficient Decentralized Linear Learning 1 Input: Data matrix A distributed column-wise according to partition {P k } K k=1 . Mixing matrix W. Aggregation parameter 2 [0, 1], and local subproblem parameter 0 as in (1). Starting point</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>7) If {g i } are strongly convex, COLA achieves the following linear rate of convergence. Theorem 1 (Strongly Convex g i ). Consider Algorithm 1 with := 1 and let ⇥ be the quality of the local solver in Assumption 1. Let g i be µ g -strongly convex for all i 2 [n] and let f be 1/⌧ -smooth.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Suboptimality for solving Lasso ( =10 6 ) for the RCV1 dataset on a ring of 16 nodes. We illustrate the performance of COLA: a) number of iterations; b) time.  here denotes the number of local data passes per communication round.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Convergence of COLA for solving problems on a ring of K=16 nodes. Left) Ridge regression on URL reputation dataset ( =10 4 ); Right) Lasso on webspam dataset ( =10 5 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance comparison of COLA on different topologies. Solving Lasso regression ( =10 6 ) for RCV1 dataset with 16 nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Performance of COLA when nodes have p chance of staying in the network on the URL dataset ( =10 4 ). Freezing x [k] when node k leaves the network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>for the non-decentralized case. and yields x [k] . Then we calculate v k := A [k] x [k] , and update the local shared vector v. We allow the local subproblem to be solved approximately: Assumption 1 (⇥-approximation solution). Let ⇥ 2 [0, 1] be the relative accuracy of the local solver (potentially randomized), in the sense of returning an approximate solution x [k] at each step t, s.t.</figDesc><table>(t+1) 
k 

E[G 

0 

k </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Datasets Used for Empirical Study</figDesc><table>Dataset 
#Training #Features Sparsity 
URL 
2M 
3M 
3.5e-5 
Webspam 
350K 
16M 
2.0e-4 
Epsilon 
400K 
2K 
1.0 
RCV1 Binary 
677K 
47K 
1.6e-3 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">https://www.csie.ntu.edu.tw/~cjlin/libsvmtools/datasets/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. We thank Prof. Bharat K. Bhargava for fruitful discussions. We acknowledge funding from SNSF grant 200021_175796, Microsoft Research JRC project 'Coltrain', as well as a Google Focused Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distributed asynchronous deterministic and stochastic gradient optimization algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimitri</forename><forename type="middle">P</forename><surname>John N Tsitsiklis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Athans</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="803" to="812" />
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Distributed subgradient methods for multi-agent optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelia</forename><surname>Nedic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="61" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dual Averaging for Distributed Optimization: Convergence Analysis and Network Scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>J C Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M J</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="592" to="606" />
			<date type="published" when="2012-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Extra: An exact first-order algorithm for decentralized consensus optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="944" to="966" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">DSA: Decentralized double stochastic averaging gradient algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aryan</forename><surname>Mokhtari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Ribeiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">61</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Achieving geometric convergence for distributed optimization over time-varying graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angelia</forename><surname>Nedic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Olshevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="2597" to="2633" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Convex Optimization for Big Data: Scalable, randomized, and parallel algorithms for big data analytics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volkan</forename><surname>Cevher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Becker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="32" to="43" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">CoCoA: A General Framework for Communication-Efficient Distributed Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">230</biblScope>
			<biblScope unit="page" from="1" to="49" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep learning with Elastic Averaging SGD</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sixin</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anna</forename><forename type="middle">E</forename><surname>Choromanska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2015 -Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="685" to="693" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Memory and Communication Efficient Distributed Stochastic Optimization with Minibatch Prox</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jialei</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Srebro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2017 -Proceedings of the 34th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017-06" />
			<biblScope unit="page" from="1882" to="1919" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Primal-Dual Rates and Certificates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celestine</forename><surname>Dünner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simone</forename><surname>Forte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2016 -Proceedings of the 33th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="783" to="792" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Communication-efficient distributed dual coordinate ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Takác</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Terhorst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3068" to="3076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Optimal algorithms for smooth and strongly convex distributed optimization in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Tat</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Massoulié</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="3027" to="3036" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Optimal algorithms for non-smooth distributed optimization in networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Scaman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francis</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sébastien</forename><surname>Bubeck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yin Tat</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Massoulié</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1806.00291</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Convergence rate analysis of distributed gradient methods for smooth optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dusan</forename><surname>Jakovetic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joao</forename><surname>Xavier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose Mf</forename><surname>Moura</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Telecommunications Forum (TELFOR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="867" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">On the convergence of decentralized gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1835" to="1854" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the Linear Convergence of the ADMM in Decentralized Consensus Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal Processing</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="1750" to="1761" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">On the O(1/k) Convergence of Asynchronous Distributed Alternating Direction Method of Multipliers. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ermin</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Asuman</forename><surname>Ozdaglar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A coordinate descent primal-dual algorithm and application to distributed asynchronous optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Bianchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Walid</forename><surname>Hachem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Franck</forename><surname>Iutzeler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Automatic Control</title>
		<imprint>
			<biblScope unit="volume">61</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2947" to="2957" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Can decentralized algorithms outperform centralized algorithms? a case study for decentralized parallel stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huan</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cho-Jui</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5336" to="5346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Asynchronous decentralized parallel stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2018 -Proceedings of the 35th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangru</forename><surname>Lian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1803.07068</idno>
		<title level="m">Decentralized training over decentralized data</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Communication compression for decentralized training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanlin</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoduo</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ce</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2018 -Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Decentralized consensus optimization with asynchrony and delays</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianyu</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wotao</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali H</forename><surname>Sayed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Signal and Information Processing over Networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="293" to="307" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Decentralized consensus algorithm with delayed and stochastic gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Sirb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojing</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="1232" to="1254" />
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trading Computation for Communication: Distributed Stochastic Dual Coordinate Ascent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianbao</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2014 -Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Averaging in Distributed Primal-Dual Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenxin</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Takác</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2015 -Proceedings of the 32th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1973" to="1982" />
		</imprint>
	</monogr>
	<note>Adding vs</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A Distributed Second-Order Algorithm You Can Trust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Celestine</forename><surname>Dünner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aurelien</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matilde</forename><surname>Gargiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">An</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML 2018 -Proceedings of the 35th International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2018-07" />
			<biblScope unit="page" from="1357" to="1365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Distributed delayed stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alekh</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Duchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="873" to="881" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Parallelized stochastic gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Zinkevich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markus</forename><surname>Weimer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihong</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2595" to="2603" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Disco: Distributed optimization for self-concordant empirical loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuchen</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Aide: Fast and communication efficient distributed optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sashank</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Reddi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Richtárik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Póczós</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06879</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Hessian-CoCoA: a general parallel and distributed framework for non-strongly convex regularizers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matilde</forename><surname>Gargiani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-06" />
			<pubPlace>ETH Zurich</pubPlace>
		</imprint>
	</monogr>
	<note>Master&apos;s thesis</note>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Distributed block-diagonal approximation methods for regularized empirical risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Pei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kai-Wei</forename><surname>Chang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03043</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A distributed quasi-newton algorithm for empirical risk minimization with nonsmooth regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ching-Pei</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cong</forename><forename type="middle">Han</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen J</forename><surname>Wright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Knowledge Discovery and Data Mining</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Federated Multi-Task Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Virginia</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao-Kai</forename><surname>Chiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maziar</forename><surname>Sanjabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ameet</forename><surname>Talwalkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2017 -Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.03575</idno>
		<title level="m">Federated optimization: Distributed optimization beyond the datacenter</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Federated learning: Strategies for improving communication efficiency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Konecnỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Felix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ananda</forename><surname>Richtarik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dave</forename><surname>Theertha Suresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bacon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.05492</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Communicationefficient learning of deep networks from decentralized data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brendan</forename><surname>Mcmahan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eider</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ramage</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seth</forename><surname>Hampson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Blaise</forename><surname>Aguera Y Arcas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1273" to="1282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neal</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Borja</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Foundations and Trends R in Machine learning</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="122" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Sparsified sgd with memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Sebastian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Stich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Cordonnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaggi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2018 -Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralph</forename><surname>Tyrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rockafellar</forename></persName>
		</author>
		<title level="m">Convex analysis</title>
		<imprint>
			<publisher>Princeton university press</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Monte carlo sampling methods using markov chains and their applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W Keith</forename><surname>Hastings</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="volume">57</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="97" to="109" />
			<date type="published" when="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Autodiff</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Scikit-learn: Machine learning in Python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pedregosa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varoquaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gramfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Michel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thirion</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Grisel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blondel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Prettenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dubourg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vanderplas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Passos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cournapeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brucher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perrot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Duchesnay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2825" to="2830" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
