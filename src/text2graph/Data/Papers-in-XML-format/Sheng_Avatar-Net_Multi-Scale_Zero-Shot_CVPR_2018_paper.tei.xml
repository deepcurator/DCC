<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lu</forename><surname>Sheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziyi</forename><surname>Lin</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jing</forename><surname>Shao</surname></persName>
							<email>shaojing@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
							<email>xgwang@ee.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK-SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Avatar-Net: Multi-scale Zero-shot Style Transfer by Feature Decoration</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>Figure 1. Exemplar stylized results by the proposed Avatar-Net, which faithfully transfers the Lena image by arbitrary style.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Unlike taking days or months to create a particular artistic style by a diligent artist, modern computational methods have enabled fast and reliable style creation for natural images. Especially inspired by the remarkable representative power of convolutional neural networks (CNNs), the seminal work by Gatys et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> discovered that multilevel feature statistics extracted from a trained CNN notably represent the characteristics of visual styles, which boosts the development of style transfer approaches, either by iterative optimizations <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b19">20]</ref> or feed-forward networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b2">3]</ref>. However, a dilemma in terms of generalization and quality versus efficiency hampers the availability of style transfer for arbitrary style. Since the optimization-based approaches render visually plausible results for various styles at the sacrifice of executing efficiency <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b29">30]</ref>, while the feed-forward networks are either restricted by a finite set of predefined styles <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28]</ref>, or simplify the zero-shot transfer with compromised visual quality <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Valuable efforts have been devoted to solving this dilemma. A common practice is employing external style signals to supervise the content modification <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b2">3]</ref> on a feed-forward network. But these methods require the networks trained by the perceptual loss <ref type="bibr" target="#b12">[13]</ref>, which has been known instable and produces compromised stylized patterns <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>. In contrast, another category <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> manipulates the content features under the guidance of the style features in a shared high-level feature space. By decoding the manipulated features back into the image space with a style-agnostic image decoder, the reconstructed images will be stylized with seamless integration of the style patterns. However, current techniques may either over-distort the content and add unconstrained patterns <ref type="bibr" target="#b18">[19]</ref>, or fail to retrieve complete style patterns when large domain gap ex-ists between the content and style features <ref type="bibr" target="#b3">[4]</ref>.</p><p>Facing the aforementioned challenges, we propose a zero-shot style transfer model Avatar-Net that follows a similar style-agnostic paradigm <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4]</ref> in the aforementioned latter category. Specifically, we introduce a novel patch-based style decorator module that decorates the content features with the characteristics of the style patterns, while keeps the content semantically perceptible. The style decorator does not only match the holistic style distribution, but also explicitly retrieves detailed style patterns without distortion. In the meanwhile, other than autoencoders <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b2">3]</ref> for semantic feature extraction and styleagnostic image decoder, we introduce an innovative hourglass network with multiple skip connections for multiscale holistic style adaptation. It is thus straightforward to perform multi-scale zero-shot style transfer by 1) extracting the content and style features via its encoding module, 2) decorating the content features by the patch-based style decorator, and 3) progressively decoding the stylized features with multi-scale holistic style adaptation. Note that the proposed hourglass network is trained solely to reconstruct natural images. As shown in <ref type="figure">Fig. 1</ref>, the proposed AvatarNet can synthesize visually plausible stylized Lena images for arbitrary style. Comprehensive evaluations have been conducted to compare with the prior style transfer methods <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13]</ref>, our method achieves the state-of-the-art performance in terms of both visual quality and efficiency.</p><p>Our contributions in this paper are threefold: (1) A novel patch-based feature manipulation module named as style decorator, transfers the content features to semantically nearest style features and simultaneously minimizes the discrepancy between their holistic feature distributions. (2) A new hourglass network equipped with multi-scale style adaptation enables visually plausible multi-scale transfer for arbitrary style in one feed-forward pass. (3) Theoretical analysis proves that the style decorator module owns superior transferring ability, and the experimental results demonstrate the effectiveness of the proposed method with superior visual quality and economical processing cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Style transfer is a kind of non-realistic rendering techniques <ref type="bibr" target="#b14">[15]</ref> that is closely related to texture synthesis <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. It usually exploits local statistics for efficient cross-view dense correspondences and texture quilting <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b20">21]</ref>. Although these methods produce appealing stylized images <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b20">21]</ref>, dense correspondences are limited to a pair of images with similar contents, thus inapplicable to zeroshot style transfer. Optimization-based Stylization. Gatys et al. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref> at the first time formulated the style as multi-level feature correlations (i.e., Gram matrix) from a trained neural network for image classification, and defined the style transfer as an iterative optimization problem that balances the content similarity and style affinity in the feature level (or termed as perceptual loss <ref type="bibr" target="#b12">[13]</ref>). A number of variants have been developed thereafter to adapt this framework to different scenarios and requirements, including photorealistic rendering <ref type="bibr" target="#b22">[23]</ref>, semantically composite transfer <ref type="bibr" target="#b1">[2]</ref>, temporal coherence <ref type="bibr" target="#b23">[24]</ref> and so on. Changing the global style statistics into Markovian feature assembling, a similar framework is also proposed for semantic image synthesis <ref type="bibr" target="#b15">[16]</ref>. Despite visual appealing performances for arbitrary style, the results are not stable <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> and the perceptual loss usually requires careful parameter tunning for different styles <ref type="bibr" target="#b9">[10]</ref>. Moreover, the inefficiency of the optimization-based approaches restrains real-time applications. Feed-Forward Approximation. Recent researches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b4">5]</ref> try to tackle the complexity issue by approximating the iterative back-propagating procedure to feed-forward neural networks, either trained by the perceptual loss or Markovian generative adversarial loss <ref type="bibr" target="#b16">[17]</ref>. However, <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b16">17]</ref> have to train a independent network for every style. To strengthen the representation power for multiple styles, StyleBank <ref type="bibr" target="#b2">[3]</ref> learns filtering kernels for styles and Li et al. <ref type="bibr" target="#b17">[18]</ref> transfered styles by binary selection units, as well as Dumoulin et al. proposed conditional instance normalization <ref type="bibr" target="#b4">[5]</ref> controlled by channel-wise statistics learned for each style. But the manually designed style abstractions are often short for the representation of unseen styles and the combinational optimization over multiple styles often compromises the rendering quality <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10]</ref>. Zero-shot Style Transfer. To achieve zero-shot style transfer in a feed-forward network, Huang et al. <ref type="bibr" target="#b11">[12]</ref> adjusted channel-wise statistics of the content features by adaptive instance normalization (AdaIN), and trained a feature decoder by a combinational scale-adapted content and style losses. Chen et al. <ref type="bibr" target="#b3">[4]</ref> swapped the content feature patches with the closest style features (i.e. Style-Swap) at the intermediate layer of a trained auto-encoder, while Li et al. <ref type="bibr" target="#b18">[19]</ref> transferred multi-level style patterns by recursively applying whitening and coloring transformation (WCT) to a set of trained auto-encoders with different levels. However, AdaIN over-simplifies the transferring procedure and StyleSwap cannot parse sufficient style features when content and style images are semantically different. WCT is the state-of-the-art method, but the holistic transformation may results in distorted style patterns in the transferred features, and generate unwanted patterns in the output image.</p><p>The proposed zero-shot style transfer is related to <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b11">12]</ref>. But the style decorator improves AdaIN and WCT as it reserves the detailed style patterns rather than the parameterized feature statistics. It also outperforms StyleSwap as it effectively parses the complete set of style features regardless of their domain gap. Moreover, the proposed network performs multi-scale style adaptation in one feed-forward pass, which surpasses AdaIN and WCT since AdaIN requires a style-oriented image decoder and WCT needs a set of recursive feed-forward passes to enable multiscale style transfer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Style Transfer via Feature Manipulation</head><p>Let a trained feed-forward network extract the features in the bottleneck layer for an image x as z = E θenc (x) ∈ R H×W ×C , with height H, width W and channel C. The extracted features are decoded viax = D θdec (z) to the reconstructed images. The encoder and decoder modules are parameterized by θ enc and θ dec , respectively.</p><p>Denote z c and z s as the features for the content image x c and style image x s . By transferring z c into the domain of z s , it is desired that the transferred features z cs = F(z c ; z s ) infer the spatial distribution of the content features and the textural characteristics of the style features. The key challenge is to design a feature transfer module that both holistically adapts the domain of z cs to that of z s and semantically links elements in z cs to paired elements in z s and z c .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Revisiting Normalized Cross-correlation</head><p>A type of modules generates z cs by non-parametrically swapping the patches in z c by their nearest neighbors in z s . It encourages concrete style patterns in z cs and explicitly binds z c and z cs by their spatial distributions. Normalized Cross-correlation (NCC). It is one effective metric for patch-wise nearest neighbor searching <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b3">4]</ref>, by scoring the cosine distance between a content patch φ i (z c ) and a style patch φ j (z s ) and returning the nearest patch</p><formula xml:id="formula_0">φ i (z cs ) = arg max j∈{1,...,Ns} φ i (z c ), φ j (z s ) φ i (z c ) φ j (z s ) , i ∈ {1, . . . , N c }.</formula><p>However, each φ i (z c ) is required to compare with any patch in z s . Thanks to the GPU accelerated convolution operations, NCC can be converted into several steps of efficient convolutions, such as the way applied in Style-Swap <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b15">16]</ref>. Discussions. Although concrete style patterns are preserved in z cs , it is highly biased towards the content features in z c , since it suggests the matched style patches strictly follow the local variations as the content patches. Therefore, the spiral patterns in sky in Vincent Van Gogh's starry night cannot be propagated to a sky patch in the content image, as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. And only a limited portion of style patterns are parsed to assemble z cs when the domain of z c and that of z s are far apart, as depicted by the swapping procedure shown in <ref type="figure">Fig. 3(c)</ref>. Thus the stylized images preserve more content patterns with only a small portion of semantically aligned style patterns. In fact, two patches can be matched as long as they are paired in some forms of semantic agreement rather than this over-restrictive patch similarity. It means that a desired patch matching can be invariant to their domain gap and the patches are paired by comparing their local statistics in a common space that the textural characteristics have been dispelled.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Style Decorator</head><p>To resolve drawbacks raised from the existing feature transfer modules, we propose a novel style decorator module that robustly propagates the thorough style patterns onto the content features so that the distributions of z cs and z s are maximally aligned and the detailed style patterns are semantically perceptible in z cs . It is achieved by a relaxed cross correlation between patches in a normalized coordinate that sufficiently whitens the textures from the original feature spaces for both features. The swapped normalized featuresz cs explicitly correspond to the original style features, thus from which we are able to reconstruct the stylized features z cs that both statistically follow the style of z s and the spatial layout of the content feature z c . The style decorator is achieved by a three-step procedure:</p><p>Step-I: Projection. We project z c and z s onto the same space, written asz c andz s , with sufficient compactness and similar scale in magnitude. In detail, the projection operation for the content (style) feature is</p><formula xml:id="formula_1">z c = W c ⊗ (z c − µ(z c )),z s = W s ⊗ (z s − µ(z s )), (1)</formula><p>where ⊗ denotes convolutional operator, and the whitening kernels W c and W s are derived by some forms of whitening matrices onto the covariance matrices C(z c ) and C(z s ). And µ(z c ) and µ(z s ) are the mean features.z c andz s keep the characteristics from their original data, but both of them statistically follow standard normal distribution.</p><p>Step-II: Matching and Reassembling. In this normalized domain, we want to align any element inz c with the nearest element inz s , so as to reconstructz c by reassembling the corresponded elements inz s . In this case, the reassembled normalized style featuresz cs contains concrete normalized style patterns but their spatial layouts come from the normalized content features. Therefore, the matching and reassembling operations seamlessly incorporate the desired content and style characteristics in the resultantz cs . We apply the patch matching and reassembling via the help of normalized cross-correlation, which can be efficiently transformed into several convolutional operations as</p><formula xml:id="formula_2">z cs = Φ(z s ) ⊤ ⊗ B(Φ(z s ) ⊗z c ),<label>(2)</label></formula><p>where Φ(z s ) ∈ R P ×P ×C×(H×W ) is the style kernel consists of patches inz s , and P is the patch size. In addition,Φ (z s ) is the normalized style kernel that is divided by the channel-wise ℓ 2 norm of each patch. The patch matching procedure is at first convolving the normalized content featurez c withΦ(z s ) and then binarizing the resultant scores (i.e., B(·)) such that the maximum value along the channel is 1 and the rest are 0. Then the reassembling procedure applies a deconvolutional operation by Φ(z s )</p><p>⊤ to regenerate the style patterns from the binary scores.</p><p>This patch matching betweenz c andz s parses effective and complete correspondences sincez c andz s have the maximal overlap with each other, thus each element inz c can find a suitable correspondence inz s whilst possibly every element inz s can be well retrieved, as demonstrated in <ref type="figure">Fig. 3(d)</ref>. The matching and reassembling step actually adds two style-controlled convolutional layers to the feedforward network and thus its implementation is usually efficient in modern deep learning platforms.</p><p>Step-III: Reconstruction. After reorganizing the normalized style patches intoz cs , we will reconstruct it into the domain of the style feature z s . In detail, we apply the coloring transformation with respect to z s into the reassembled feature, the reconstruction of the stylized feature is</p><formula xml:id="formula_3">z cs = C s ⊗z cs + µ(z s ),<label>(3)</label></formula><p>where the coloring kernel C s is also derived from the covariance matrix C(z s ). Discussions. Similar as WCT and AdaIN, the projection and reconstruction steps are designed to encourage that the second-order statistics of the stylized feature z cs to be matched to those of the style feature z s 1 , i.e., their Gram matrices G(z s ) ≃ G(z cs ). Actually, we may employ Adaptive Instance Normalization (AdaIN) <ref type="bibr" target="#b11">[12]</ref>, or Zero-phase Component Analysis (ZCA) <ref type="bibr" target="#b13">[14]</ref> used in WCT <ref type="bibr" target="#b18">[19]</ref> as the whitening and coloring kernels. Whitened featuresz by ZCA minimize the ℓ 2 distance with respect to z, thus they marginally preserve the original feature orientations and element-wise distributions. AdaIN is much faster than ZCA but it cannot optimally dispel feature correlations as what ZCA does, thus the transferred features contain a slightly more content patterns, as shown in <ref type="figure">Fig. 3(a) and (b)</ref>. But for the sake of efficiency, the AdaIN induced style decorator is good enough for a real-time zero-shot style transfer, as shown in <ref type="figure">Fig. 5</ref>. On the other hand, the matching and reassembling step pairs the elements in z c and z s by matching their normalized counterparts, thus it effectively reduces the bias and enriches the parsing diversity, which is much more effective than the way by Style-Swap <ref type="bibr" target="#b3">[4]</ref>. In addition, the patch size P affects the scale of the presented styles since a larger patch size leads to a more global style patterns, for example the blocky geometric patterns of style cubist in <ref type="figure">Fig. 4.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Multi-scale Zero-shot Style Transfer</head><p>The proposed Avatar-Net employs a hourglass network with multi-scale style adaptation modules that progressively fuse the styles from the encoded features into the corresponded decoded features, thus it enables multi-scale style transfer in one feed-forward pass, as shown in <ref type="figure" target="#fig_2">Fig. 6(a)</ref>.</p><p>As shown in <ref type="figure" target="#fig_2">Fig. 6(b)</ref>, the main branch of the Avatar-Net is analogous to conventional encoder-decoder architectures that stack an encoder E θenc (·) and an decoder D θdec (·). In detail, the encoder is a concatenation of several encoding blocks E Given the set of encoded style features and the decoded stylized feature pairs (e l s , d l cs ), l ∈ {1, . . . , L} extracted from the main branch of the Hourglass network, the Style Fusion module is similarly as AdaIN <ref type="bibr" target="#b11">[12]</ref>:</p><formula xml:id="formula_4">F SF (d l cs ; e l s ) = σ(e l s ) • d l cs − µ(d l cs ) σ(d l cs ) + µ(e l s ),<label>(4)</label></formula><p>where • denotes channel-wise multiplication and σ(·) is the channel-wise standard deviation. One may argue that this module is suboptimal to ZCA as it does not optimally match their second-order statistics, as visualized in <ref type="figure">Fig. 3(a)</ref>. But as the proposed network suggest similar pattern distribution of d l cs as that of the encoded features e l s , the training of the decoder will let the proposed module move towards ZCA. Moreover, its economical computational complexity also speeds up the stylization.</p><p>The style transfer requires a special procedure to fit the proposed network architecture. At first, Avatar-Net takes a content image x c and an arbitrary style image x s as inputs, and extracts z c and z s in the bottleneck layer through the encoder module. In the meanwhile, the style image x s also bypasses the multi-scale encoded features {e</p><formula xml:id="formula_5">l s } L l=1</formula><p>. Secondly, the content feature z c is then transferred based on the style features z s through the proposed style decorator module. In the end, the stylized imagex cs is inverted by the decoded module D θdec (z cs ) with multiple style fusion modules that progressively modify the the decoded features d l cs under the guidance of multi-scale style patterns e l s , from l = L to 1, as shown in <ref type="figure" target="#fig_2">Fig. 6(b)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experimental Results and Discussions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Network Architecture And Training</head><p>Avatar-Net copies the architecture of a pretrained VGG-19 (up to conv4 1) <ref type="bibr" target="#b26">[27]</ref> to the encoder E θenc (·). The de-  <ref type="bibr" target="#b18">[19]</ref>, Gatys et al. <ref type="bibr" target="#b8">[9]</ref>, and the proposed method.</p><p>coder D θdec (·) is randomly initialized and mirrors the encoder with all pooling layers replaced by nearest upsampling and padding layers by reflectance padding. The shortcut connections link conv1 1, conv2 1 and conv3 1 to their corresponded decoded layers. Normalization is not applied in each conv layer so as to increase the reconstruction performance and stability, as suggested by <ref type="bibr" target="#b11">[12]</ref>.</p><p>The proposed model is to reconstruct perceptually similar images as the input images <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_6">ℓ total = x −x 2 2 + λ 1 1 |I| i∈I Ψ i VGG (x) − Ψ i VGG (x) 2 2 + λ 2 ℓ TV (x),<label>(5)</label></formula><p>where Ψ i VGG (x) extract the feature of x at layer i in a fixed VGG-19 network. The set I contains conv1 1, conv2 1, conv3 1 and conv4 1 layers. Total variation loss ℓ TV (x) is also added to enforce piece-wise smoothness. The weighting parameters are simply set as λ 1 = 0.1 and λ 2 = 1 for balancing the gradients from each term.</p><p>We train the network on the MSCOCO <ref type="bibr" target="#b21">[22]</ref> dataset with roughly 80, 000 training samples. Adam optimizer is applied with a fixed learning rate of 0.001 and a batch size of 16. During the training phase, the training images are randomly resized and cropped to 256 × 256 patches. Note that our method is suitable for images with arbitrary size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with Prior Arts</head><p>To validate the effectiveness of the proposed method, we compare it with two types of zero-shot stylization methods based on (1) iterative optimization <ref type="bibr" target="#b8">[9]</ref> and (2) feed-forward network approximation <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b3">4]</ref> 2 .</p><p>Qualitative Evaluations. The optimization-based approach <ref type="bibr" target="#b8">[9]</ref> is able to transfer arbitrary style but the generated results are not stable because the weights trading off the content and style losses are sensitive and the optimization is vulnerable to be stuck onto a bad local minimum, as of that in (b) but the performances are similar except a slight lost of detailed style patterns.</p><p>shown in 4 th and 5 th columns in <ref type="figure" target="#fig_4">Fig. 7</ref>. Both AdaIN <ref type="bibr" target="#b11">[12]</ref> and WCT <ref type="bibr" target="#b18">[19]</ref> holistically adjust the content features to match the second-order statistics of the style features, but AdaIN mimics the channel-wise means and variances and thus pro-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Execution Time 256 × 256 (sec) 512 × 512 (sec) Gatys et al. <ref type="bibr" target="#b8">[9]</ref> 12.18 43.25 AdaIN <ref type="bibr" target="#b11">[12]</ref> 0.053 0.11 WCT <ref type="bibr" target="#b18">[19]</ref> 0 vides a suboptimal solution and the learned decoder usually add similar repeated texture patterns for all stylized images. Although WCT optimally matches the second-order statistics, it cannot always parse the original style patterns and may generate unseen and distorted patterns, for example the background clutters in the 2 nd column and unwanted spiral patterns in the 3 rd column, as well as missed circular patterns in the last column for the style candy. Style-Swap is too rigorous so that the stylized features strictly preserve the content features and only receive low-level style patterns. This artifact almost occurs in every examples in <ref type="figure" target="#fig_4">Fig. 7</ref>.</p><p>The proposed method seamlessly reassembles the style patterns according to the semantic spatial distribution of the content image, which works for different kinds of styles, from global hue to local strokes and sketches, as shown in <ref type="figure" target="#fig_4">Fig. 7</ref>. Even though style patterns replace the content information to regenerate the stylized images, necessary content components like human faces, building and skylines are still visually perceived as the combination of the style patterns.</p><p>We also provide result close-ups in <ref type="figure">Fig. 10</ref>, the visual result by Avatar-Net receives concrete multi-scale style patterns (e.g., color distribution, brush strokes and circular patterns in candy image). WCT distorts the brush strokes and circular patterns. AdaIN cannot even keep the color distribution, while style-swap fails in this example. Efficiency. In Tab. 1, we compare the running time with the reference methods. We implemented these methods on the Tensorflow platform for a fair comparison. The method by Gatys et al. <ref type="bibr" target="#b8">[9]</ref> is slow since it requires hundreds of forward and backward passes to converge. WCT <ref type="bibr" target="#b18">[19]</ref>, AdaIN <ref type="bibr" target="#b11">[12]</ref> and Style-Swap <ref type="bibr" target="#b3">[4]</ref> are all based on feed-forward networks, in which WCT is relatively slower as it requires several feed-forward passes and the operation of SVD has to be executed in CPU. Our implemented Style-Swap is efficient in GPU and thus much faster than the reported speed <ref type="bibr" target="#b3">[4]</ref>.</p><p>The proposed method is more efficient than WCT, because the hourglass architecture enables multi-scale processing in one feed-forward pass. But the style decorator equipped by ZCA also needs a CPU-based SVD, its speed is thus not comparable to AdaIN and Style-Swap. We can improve the efficiency by replacing it by AdaIN, in which case the execution time is tremendously reduced to the same level of Style-Swap and AdaIN. Note that the proposed method can be economical in memory, by randomly (or grid) sampling the style patches in the style decorator procedure. Due to the local coherence in the style features, we find the resultant stylized images have similar performances as the complete version (in <ref type="figure">Fig. 11</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Applications</head><p>The flexibility of the Avatar-Net is further manifested by several applications as follows:</p><p>Trade-off Between Content and Style. The degree of stylization can be adjusted by two variants. (1) Adjust the normalized stylized featuresz cs ← αz c + (1 − α)z cs , ∀α ∈ [0, 1], and then recover the stylized features z cs . (2) Directly adjust the stylized features z cs ← αz c + (1 − α)z cs . In the former variant, α = 0 lets the model degrade to the WCT, while α = 1 means the image is only reconstructed by the style patterns. It is harder to control the latter variant due to the magnitude dissimilarities between the content and style features. We depict these variants in this experiment (in <ref type="figure">Fig. 9</ref>), but we do not adjust the low-level style adaptation in the shortcut links, which can be interpolated by <ref type="bibr" target="#b11">[12]</ref> to enable multi-scale transfer. Video Stylization. In addition to image-based stylization, our model can perform video stylization merely based on per-frame style transfer, as visualized in <ref type="figure" target="#fig_0">Fig. 12</ref>. The style decorator module is stable and coherent over adjacent frames since the alignment between feature patches is usually spatially invariant and robust to small content variations. On the contrary, WCT, as an example, contains severe content distortions and temporal flickering.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Concluding Remarks</head><p>In this work, we propose a fast and reliable multi-scale zero-shot style transfer method integrating an style decorator for semantic style feature propagation and a hourglass network for multi-scale holistic style adaptation. Experimental results demonstrate its superiority in generating arbitrary stylized images. As a future direction, one may replace the projection and reconstruction steps in the style decorator by learnable modules for increased alignment robustness and executing efficiency.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Style patterns are overlaid onto the content patches according to their semantic similarity rather than their texture agreement. (a) and (b) are the style and content images. (c)-(d) are the results by Style-Swap [4] and the proposed style decorator. StyleSwap pays too much attention on the textural similarity, thus receives fewer high-level style patterns.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .Figure 4 .Figure 5 .</head><label>345</label><figDesc>Figure 3. Comparison of distribution transformation by different feature transfer modules. (a) AdaIN cannot completely dispel the textures from zc and propagate enough style textures to zcs. (b) WCT has optimal matching between G(zcs) and G(zs) but it introduces rare patterns observed in the transformed features. (c) Style-Swap matches patches from zc and zs, and thus only captures the overlapped features. (d) Style decorator matches the normalized featureszc andzs, the generated zcs parses all possible style patterns to the content features. The red boxes show the normalization and the green boxes visualize the normalized cross-correlation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. (a) Stylization comparison by auto-encoder and style-augmented hourglass networks. Style decorator is applied as the feature transfer module. The auto-encoder and the hourglass networks share the same main branch. (b) The network architecture of the proposed style-augmented hourglass network. Detailed implementation is depicted in Sec. 5.1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>), l ∈ {1, . . . , L} and produces the bottleneck feature z after the final block z = E L+1 θenc (e L ). Inversely, the decoder progressively generates intermediate features d l = D l+1 θdec (d l+1 ) starting from z, in which d l is further updated by fusing with the corresponded encoded features e l via the style adaptive feature fusion module. The output image is decoded byx = D 1 θdec (d 1 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Comparison of some exemplar stylization results. The top row shows the content and style pairs and the rest rows present the stylized results by AdaIN [12], Style-Swap [4], WCT [19], Gatys et al. [9], and the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .Figure 9 .Figure 10 .Figure 11 .</head><label>891011</label><figDesc>Figure 8. Style Interpolation between Rain Princess and Sketch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 12 .</head><label>12</label><figDesc>Figure 12. Video stylization. A sequence in MPI Sintel dataset [1] is stylized by Henri Matisse's Woman With Hat. Style Decorator: The stylized patterns in one object keep coherent among adjacent frames. WCT: The results have shape distortions around the face region and contain flickering artifacts among adjacent frames. Please refer to the supplementary materials for a video demonstration.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>adaptations in the shortcut links are also extended to a con- vex combination of the stylized features. But due to the magnitude dissimilarities among different style features, the interpolated stylization is also affected by their feature mag- nitudes, as shown in Fig. 8.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">They will not be exactly the same, since the population ofzcs has been changed to follow a similar spatial distribution ofzc.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The results by the reference methods are obtained by their public codes with default configurations.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">editor, ECCV, Part IV</title>
		<editor>A. Fitzgibbon et al.</editor>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">7577</biblScope>
			<biblScope unit="page" from="611" to="625" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Semantic style transfer and turning two-bit doodles into fine artworks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Champandard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01768</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Stylebank: An explicit representation for neural image style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Fast patch-based style transfer of arbitrary style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04337</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Image quilting for texture synthesis and transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="341" to="346" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Style transfer via texture synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="2338" to="2351" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Texture synthesis using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Characterizing and improving stability in neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th annual conference on Computer graphics and interactive techniques</title>
		<meeting>the 28th annual conference on Computer graphics and interactive techniques</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="327" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Arbitrary style transfer in realtime with adaptive instance normalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Optimal whitening and decorrelation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kessy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lewin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Strimmer</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint/>
	</monogr>
	<note>justaccepted</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">State of the &quot;art&quot;: A taxonomy of artistic stylization techniques for images and video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Kyprianidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Collomosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Isenberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Vis. Comput. Graphics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="866" to="885" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Precomputed real-time texture synthesis with markovian generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="702" to="716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diversified texture synthesis with feed-forward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Universal style transfer via feature transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Demystifying neural style transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="2230" to="2236" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.01088</idno>
		<title level="m">Visual attribute transfer through deep image analogy</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
		<title level="m">Deep photo style transfer. CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Artistic style transfer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="26" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.04111</idno>
		<title level="m">Meta networks for neural style transfer</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Style transfer for headshot portraits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1349" to="1357" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Zm-net: Real-time zero-shot image manipulation network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.07255</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wilmot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Risser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barnes</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08893</idno>
		<title level="m">Stable and controllable neural texture synthesis and style transfer using histogram losses</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Multi-style generative network for real-time transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dana</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.06953</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
