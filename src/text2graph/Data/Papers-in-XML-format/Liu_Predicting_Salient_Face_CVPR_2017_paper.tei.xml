<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Predicting Salient Face in Multiple-face Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Fa N L I U †</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Songyang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuming</forename><surname>He</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">ShanghaiTech University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Beihang University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Predicting Salient Face in Multiple-face Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Although the recent success of convolutional neural network (CNN)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Saliency prediction <ref type="bibr" target="#b0">[1]</ref> models the deployment of attention on visual inputs in biological vision systems, and has potential application in many computer vision tasks, such as object detection <ref type="bibr" target="#b2">[3]</ref> and event detection <ref type="bibr" target="#b36">[36]</ref>. Particularly, detecting salient objects, such as faces, plays an important role in video analytics, human-computer interface design and event understanding. As a matter of fact, a mass of videos, including movie, interview and variety show, contain multiple faces.</p><p>Existing literature on saliency prediction typically focuses on finding salient face in static images <ref type="bibr" target="#b20">[21]</ref>. However, few prior work has addressed the problem of predicting saliency in multiple-face videos. While the human subjects generally pay attention to only a single face <ref type="bibr" target="#b20">[21]</ref>, we find that attention of different subjects consistently transits from * Corresponding author: Mai Xu (maixu@buaa.edu.cn).</p><p>one face to another in videos, as shown in <ref type="figure">Figure 1</ref>. Our goal in this work is to capture both static and dynamic properties of the attention on faces in multiple-face videos.</p><p>Early work on image saliency prediction uses hand-craft features to predict visual attention for images <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b42">42]</ref>, based on understanding of the human visual system (HVS) <ref type="bibr" target="#b28">[29]</ref>. The representative method on predicting image saliency is Itti's model <ref type="bibr" target="#b19">[20]</ref>, which combines centersurround features of color, intensity and orientation together. In contrast, recent methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">32,</ref><ref type="bibr" target="#b40">40,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b43">43]</ref> propose a learning-based strategy to predict saliency. For example, Judd et al. combined high-level features (e.g., face and text), middle-level features (e.g., gist) and low-level features together, via learning their corresponding weights with the support vector machine (SVM). To predict visual attention in face images, Xu et al. <ref type="bibr" target="#b41">[41]</ref> proposed to precisely model saliency of face region, via learning the fixation distributions of face and facial features. Besides, Jiang et al. <ref type="bibr" target="#b20">[21]</ref> explored several face-related features to predict saliency in a scene with multiple faces. Most recently, several deep learning (DL) methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">32]</ref> have been proposed to automatically learn features for saliency prediction, instead of relying on handcrafted features. For example, Huang et al. <ref type="bibr" target="#b13">[14]</ref> proposed saliency in context (SALICON) method to learn features for image saliency prediction by incorporating convolutional neural network (CNN).</p><p>For video saliency prediction, earlier methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b16">[17]</ref><ref type="bibr" target="#b17">[18]</ref><ref type="bibr" target="#b18">[19]</ref> have investigated several dynamic features to model visual attention on videos, in light of the HVS. For example, the Itti's image model was extended in <ref type="bibr" target="#b16">[17]</ref> for video saliency prediction, by integrating with two dynamic features: motion and flicker contrast. Later, several advanced video saliency prediction methods have been proposed, which exploits other dynamic features, such as Bayesian surprise in <ref type="bibr" target="#b17">[18]</ref> and motion vector in <ref type="bibr" target="#b5">[6]</ref>. Recently, learning-based video saliency prediction methods have also emerged <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b37">37]</ref>. For example, Pang et al. <ref type="bibr" target="#b33">[33]</ref> proposed a learning-based video saliency prediction method, which explores the top-down information of eye movement patterns, i.e., passive and active states <ref type="bibr" target="#b34">[34]</ref>, <ref type="figure">Figure 1</ref>. Example of visual attention (viewed by 39 subjects) on a multiple-face video sequence. Each image shows a selected frame with its attention heat map. This figure mainly demonstrates transition of salient faces and characters of long/short-term correlation between salient faces across frames. Note that the video is chosen from our database, to be discussed in Section 2.1.</p><p>to model attention on videos. Hua et al. <ref type="bibr" target="#b12">[13]</ref> proposed to learn middle-level features, i.e., gists of a scene, as the cue in video saliency prediction. Rudoy et al. <ref type="bibr" target="#b37">[37]</ref> proposed to predict saliency of a given frame according to its high-and low-level features, conditioned on the detected saliency of the previous reference frame. However, to our best knowledge, the existing video saliency prediction methods rely on the handcrafted features, despite CNN being applied to automatically learn features for image saliency prediction in the most recent works of <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">32]</ref>. More importantly, both long-and short-term correlation of salient faces across frames, which is critical in modeling attention transition across frames for multiple-face videos (see <ref type="figure">Figure 1)</ref>, is not taken into account in these methods.</p><p>In this paper, we propose a DL-based method to predict salient face in multiple-face videos, which learns both image features and saliency transition for modeling attention on multiple faces across frames. To the best of our knowledge, our method is the first aiming at saliency prediction in multiple-face videos. Specifically, we first apply a CNN to automatically extract saliency-related features in each single frame. Built on the long short-term memory (LSTM) of recurrent neural network (RNN), we develop a multiplestream LSTM (M-LSTM) network for predicting the dynamic transitions of salient faces alongside video frames, taking the extracted CNN features as the input. Finally, saliency maps of multiple-face videos can be generated upon transited face saliency.</p><p>To evaluate our method, we create a new eye tracking database of multiple-face videos that consists of two datasets. The first dataset includes fixations of 39 subjects on 65 multiple-face videos, used as a baseline for testing saliency prediction performance. The second dataset is composed of 100 multiple-face videos viewed by 36 subjects, which is utilized for training the saliency prediction model. We provide a detailed analysis on the collected data, which shows that typically only one face (among multiple faces) in a video frame receives attention of viewing subjects, and that attention shifts across frames consistently for different subjects. We test our method on the new database, with comparisons to several state-of-the-art approaches. Our experimental results demonstrate that our method achieves significant improvements on saliency prediction in multiple-face videos.</p><p>In summary, the main contributions of our work are three-fold: (1) We introduce an eye tracking database of multiple-face videos, for facilitating the studies on video saliency prediction. (2) We find significant consistency in subjects on viewing multiple-face videos, via analysis on our eye-tracking databases. <ref type="formula" target="#formula_2">(3)</ref> We propose a DL-based method to predict the salient face with transition across frames, which integrates a CNN and an LSTM-based RNN model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Database establishment and analysis</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multiple-face Database</head><p>This section describes how we conduced the eye tracking experiment to establish our database on MUltiple-Face Videos with Eye Tracking fixations (MUFVET). To the best of our knowledge, our eye tracking database is the first one for multiple-face videos. Note that all videos in MUFVET are with either indoor or outdoor scenes, selected from Youtube and Youku, and they are all encoded by H.264 with duration varying from 10-20 seconds. Besides, MUFVET includes two datasets -MUFVET-I and MUFVET-II. These two datasets are comprised by two non-overlapping groups of videos, each of which is viewed by totally different subjects. In this paper, MUFVET-I is seen as the benchmark for test, while MUFVET-II is used for training. MUFVET is more reasonable than the existing eye tracking databases (e.g., SFU <ref type="bibr" target="#b8">[9]</ref> and DIEM <ref type="bibr" target="#b29">[30]</ref>), which only contain fixations of videos watched by same subjects. It is because both training and test utilize the fixations of same subjects are not rationale in existing saliency prediction works <ref type="bibr" target="#b0">[1]</ref>, despite videos being different.</p><p>MUFVET-I. Here, 65 multiple-face videos at diverse scenarios (see <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_0">Figure 2</ref>) are included in MUFVET-I. Then, 39 subjects (26 males and 13 females, aging from 20 to 49), with either corrected or uncorrected normal eye-sight, participated in our eye tracking experiment to watch all 65 videos. Among them, two were experts working in the field of saliency prediction. Others did not have any experience on saliency prediction, and meanwhile they were naive to the purpose of our eye tracking experiment. The eye fixations of 39 subjects on viewing each video were recorded by a Tobii X2-60 eye tracker at 60Hz. For the eye tracker, a 23-inch LCD screen was used to display the test videos at their original resolutions. During the eye tracking experiment, all subjects were required to sit on a comfortable chair with the viewing distance being ∼60 cm from the LCD screen. Before viewing videos, each  subject was required to perform a 9-point calibration for the eye tracker. Afterwards, the subjects were asked to freeview videos displayed at random order. In order to avoid eye fatigue, the 65 test videos were divided into 3 sessions, and there was a 5-minute rest after viewing each session. Moreover, a 10-second blank period with black screen was inserted between two successive videos for a short rest. Finally, 1,252,822 fixations of all 39 subjects on 65 videos were obtained. It is worth mentioning that our dataset includes the salient objects other than faces. Among 65 videos in MUFVET-I, for instance, 24 videos have salient objects other than faces, among which 3 videos have new objects appearing in the scenes. The ratio of frames containing salient objects other than faces is 37.6%. Besides, the average number of faces per frame is 3.66.</p><p>MUFVET-II. For this dataset, 100 multiple-face videos, which are totally different from MUFVET-I, were used for the eye-tracking experiment. For more details about these videos, refer to <ref type="table" target="#tab_0">Table 1</ref> and <ref type="figure" target="#fig_0">Figure 2</ref>. The overall experiment procedure for MUFVET-II is the same as that for MUFVET-I. The difference is that other 36 subjects (20 males and 16 females, aging from 20 to 55) were asked to view all 100 videos in MUFVET-II. Besides, the Tobii TX300 eye tracker was used to record fixations. During the experiment, 100 videos were equally divided into 2 sessions to avoid eye fatigue. At last, there were in total 1,737,826 fixations acquired from all 36 subjects in this dataset, which is used as the training set for learning attention model of multiple-face videos. For facilitating the future research, MUFVET is available online   the analysis, two findings are investigated as follows. <ref type="figure" target="#fig_2">Figure 3</ref> shows the proportions of fixations and pixels belonging to face and background, in MUFVET. We can see from this figure that despite taking up only 5% pixels, faces receive 79% fixations. This verifies that faces attract almost all visual attention in multiple-face videos. <ref type="figure" target="#fig_3">Figure  4</ref> further plots the proportions of fixations falling into one face to those into other faces. We can find from this figure that visual attention of different subjects is generally consistent in being attracted by one face among all faces. Beside, the subjective examples of <ref type="figure">Figure 1</ref> also imply that faces, normally one face, draw most attention in a video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding 1: In multiple-face videos, faces draw a significant amount of attention. At each video frame, attention of different subjects consistently focuses on one face among all faces.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Finding 2: Humans probably fixate on the face that is close to the video center, among all faces at a video frame.</head><p>The center-bias <ref type="bibr" target="#b0">[1]</ref> is an obvious cue to predict human fixations on generic videos. It is also intuitive that people are likely to pay their attention on the face which is close to the video center. We hence investigate the correlation of attention on a face with Euclidean distance of this face to the video center. To quantify such correlation, we measure the averaged Spearman rank correlation coefficient <ref type="bibr" target="#b14">[15]</ref> (ρ = −0.22) . This negative value of ρ indicates that humans probably fixate on the face that is close to the video center. The small value of ρ also implies that other features need to be learned for predicting salient face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>In this section, we introduce our DL-based method for saliency prediction in multiple-face videos, which integrates CNN and LSTM in a uniform framework. The overall pipeline of our method is summarized in <ref type="figure" target="#fig_4">Figure 5</ref>. First, we detect faces in each frame and feed them into CNNs, detailed in Section 3.1. Second, we design a CNN to learn the features related to salient face at each static video frame, which is discussed in Section 3.2. Section 3.3 presents M-LSTM that learns to predict salient face, by taking into consideration saliency-related features of CNN and the temporal transition of salient faces across video frames. In the end, we adopt a post-processing step to generate saliency maps of multiple-face videos, discussed in Section 3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Face Candidate Generation</head><p>Base on Finding 1, we first extract faces as our candidate regions for visual attention prediction in a multipleface video. To this end, we leverage the latest face detection method, the funnel-structured cascade (FuSt) detection model <ref type="bibr" target="#b39">[39]</ref>, to extract candidate faces from an input video. Moreover, in order to handle challenging cases, such as partial occlusion and poor light conditions, we explore temporal information to improve face detection performance in multiple-face videos.</p><p>More specifically, we first match the faces across frames, by searching the face with nearest Euclidean distance. We then identify the nearest faces of two consecutive frames as the matched face of a same person, provided their distance is less than a threshold:</p><formula xml:id="formula_0">th E = γ × w 2 + h 2 ,<label>(1)</label></formula><p>where w and h are width and height of the detected face. Otherwise, we regard them as non-matching faces, belonging to different persons. In (1), γ is a parameter to control the sensitivity of face matching, and it is simply set to 0.5 in this paper. On one hand, a smooth filter is leveraged to improve precision rate, via eliminating some false alarms of wrongly detected faces. On the other hand, we apply a linear interpolation to extend face detections to neighboring <ref type="figure">Figure 6</ref>. Architecture of our CNN for the task of predicting salient face.</p><p>frames within a sliding window, such that the missing faces can be recovered. In this paper, the length of sliding window is empirically chosen to be 17, to achieve sufficiently high recall rate on the face detection results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">CNN for Feature Extraction</head><p>We now design a CNN to automatically learn features from the detected faces, for the task of predicting whether the detected face is salient. The detected face regions are resized to be 256 × 256 before being sent to the CNN. Our CNN is based on the GoogleNet <ref type="bibr" target="#b38">[38]</ref>, with an additional batch normalization layer <ref type="bibr" target="#b15">[16]</ref> after each convolution layer to avoid over-fitting. We also use the pre-trained GoogleNet and then fine-tune the network using MUFVET-II. <ref type="figure">Figure 6</ref> shows the architecture of our CNN. After the convolutional feature extraction in GoogleNet, we use two fully connected (FC) layers, with softmax activation function, to decide whether the face is salient or not. The first FC layer has 128 units, whose outputs are used as the features for predicting the salient face. The second FC layer has 2 units, indicating the salient or non-salient face.</p><p>For training CNN, we automatically label each detected face to be salient or non-salient, according to the fixations falling into the face region. Our Finding 1 indicates that the salient face in each video frame averagely draw more than 60% fixations of all faces. Hence, the faces that take up above 60% fixations are annotated as salient faces, and other faces are seen as non-salient ones. We then train our CNN by the backpropagation (BP) algorithm using MUFVET-II of our eye tracking database as the training data. Given the trained CNN, 128-dimension features of the first FC layer can be extracted from each detected face, which are fed into our recurrent network as input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">M-LSTM for Salient Face Prediction</head><p>The CNN defined above mainly extract spatial information of each face at a single frame. To model temporal dynamics of attention transition in videos, we now develop a novel M-LSTM to predict salient face in the video setting. We formulate the multiple-face saliency prediction as a regression problem, and build an M-LSTM network to generate the continuous saliency weights of multiple faces. This differentiates our approach from the conventional L-STM <ref type="bibr" target="#b10">[11]</ref> for classification. Formally, we aim to predict saliency weight of each face defined by w n,t , which is the ground truth (GT) attention proportion of the n-th face to all faces in frame t. For such prediction, M-LSTM network generates an estimated saliency weightŵ n,t , which can be further regarded as the optimization formulation:</p><formula xml:id="formula_1">min T t=1 N n=1 ||ŵ n,t − w n,t || 2 2 s.t. N n=1ŵ n,t = N n=1</formula><p>w n,t =1,t=1, 2, ..., T, (2) Our M-LSTM takes the CNN features {f n,t } N,T n=1,t=1 as input, where f n,t stand for feature vector of the n-th face at frame t. We assume an upper limit of N faces per video. When fewer faces (&lt;N) are detected in a video frame, the corresponding input to M-LSTM is zero vectors. Note that once the face of a person disappears after a few frames in a video sequence, the corresponding feature vector f n,t is set to zero vector. On the other hand, if a new face appears after a few frames of a video sequence for one more person, its extracted input features f n,t replace the zero vector. Given f n,t , a single LSTM chunk is applied to obtain hidden variable vector h n,t , as follows,</p><formula xml:id="formula_2">h n,t = LST M (f n,t , h n,t−1 ),<label>(3)</label></formula><p>where LSTM(·) represents an LSTM chunk. For the LST-M chunk, we use the standard LSTM which includes input, forget and output gates. It is worth mentioning that the LST-M chunk is capable of learning long/short-term dependency of salient face transition as well as overcoming the problem of vanishing gradient. The hidden feature h n,t is then passed through a FC layer followed by Rectified Linear Units (ReLU) as follows,</p><formula xml:id="formula_3">s n,t =max(0, V · h n,t ),<label>(4)</label></formula><p>where V is the parameter matrix for the FC layer. To capture the correlation between faces in one video frame, we build a second FC layer that takes {s n,t } N n=1 of different faces at the t-th frame as input, and then generates N outputs. These outputs are then passed through a softmax layer to produce the final saliency weight predictions:ŵ</p><formula xml:id="formula_4">n,t = exp{θ n · N n=1 (U n,t · s n,t + b n,t )} N n=1 exp{θ n · N n=1 (U n,t · s n,t + b n,t )} ,<label>(5)</label></formula><p>where U n,t and b n,t are parameters of the FC layer, while θ n is parameter of softmax layer.</p><p>Finally,ŵ n,t can be obtained by our M-LSTM denoted as ML(P, f n,t ),w h e r eP is the parameter set of M-LSTM to be learned. For P, beyond parameter sharing across time in one conventional LSTM , our multiple LSTMs in one frame also share parameters for different faces. It is because the saliency changing mode in different faces is similar. As such, parameters from different LSTMs are updated at the same time. To learn all parameters P, the loss function of our M-LSTM derived from <ref type="formula">(2)</ref> is</p><formula xml:id="formula_5">Loss = T t=1 N n=1 ||ML(P, f n,t ) − w n,t || 2 2 .<label>(6)</label></formula><p>When training our M-LSTM with loss function (6), back propagation through time (BPTT) is utilized to learn parameters P with adaptive moment estimation (Adam) gradient descent optimization algorithm <ref type="bibr" target="#b22">[23]</ref>. After training M-LSTM,ŵ n,t can be achieved for predicting salient face.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Postprocessing</head><p>After obtaining saliency weight of each face from M-LSTM, postprocessing is required to generate final saliency map. More specifically, we first make use of predicted saliency weights {ŵ n,t } N n=1 to generate conspicuity map 2 of face channel, denoted as M F t . It can be computed by</p><formula xml:id="formula_6">M F t = N n=1ŵ n,t · c n,t · M Fn t ,<label>(7)</label></formula><p>where M Fn t denotes the conspicuity of the n-th face, and c n,t is the center-bias weight of each face. In our method, M Fn t is calculated by the latest work <ref type="bibr" target="#b41">[41]</ref>, which models the conspicuity map of a face with the Gaussian mixture model (GMM). In addition, Finding 2 has revealed that visual attention is also correlated with the center-bias feature of faces in multiple-face videos. Therefore, we apply the following way for taking into account the face center-bias by weighting Gaussian model c n,t in (7). Assuming that d n,t is the Euclidean distance of the n-th face to the video center at the t-th video frame, c n,t of <ref type="formula" target="#formula_6">(7)</ref> can be calculated by the following Gaussian model:</p><formula xml:id="formula_7">c n,t =exp − (d n,t − min n d n,t ) 2 σ 2 .<label>(8)</label></formula><p>In <ref type="formula" target="#formula_7">(8)</ref>, σ is the standard deviation of the Gaussian model, which encodes the impact of center-bias on face saliency. Obviously, we have c n,t =1for the face that is closest to the video center, and other faces have smaller c n,t .N o t e that Gaussian center-bias weights of <ref type="formula" target="#formula_7">(8)</ref> are only imposed on conspicuity of each face in our method, rather than all pixels of the conventional center-bias in <ref type="bibr" target="#b4">[5]</ref>. In order to consider background in saliency prediction, our method combines face consipicuity map M F t with consipicuity maps of three saliency-related feature channels of GBVS <ref type="bibr" target="#b9">[10]</ref> (i.e., M I t for intensity, M C t for color and M O t for orientation). Let S t be the final saliency map of the t-th video frame. It can be computed by the linear combination:</p><formula xml:id="formula_8">S t = β 1 · M F t + β 2 · M I t + β 3 · M C t + β 4 · M O t , (9) where {β k } 4 k=1</formula><p>are the channel weights of the k-th conspicuity map.</p><p>Next, we can compute (9) to predict saliency maps of multiple-face videos, once the values of {β k } 4 k=1 are known. In fact, channel weights of β k can be learnt from training data via solving the following optimization formulation:</p><formula xml:id="formula_9">argmin {β k } 4 k=1 L l=1 || 4 k=1 β k M k l − S * l || 2 , s.t. 4 k=1 β k =1,β k &gt; 0,<label>(10)</label></formula><p>where</p><formula xml:id="formula_10">{M k l } L l=1 and {S * l } L l=1</formula><p>are the conspicuity maps and GT fixation maps, for all L training video frames. In this paper, we apply the disciplined convex programming (CVX) <ref type="bibr" target="#b6">[7]</ref> to solve the above optimization formulation. Finally, saliency map S t of each multiple-face video frame can be yielded via postprocessing on the prediction of M-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>In our experiments, we tested all 65 videos from MUFVET-I (mentioned in Section 2.1). In this paper, the saliency prediction results are reported by averaging over those 65 videos. For training set, all 100 videos from MUFVET-II are selected and segmented into 3,443 4-second-clips. Note that overlap is applied in the clip segmentation for the purpose of data augmentation. For tuning hyperparameters, 5-fold cross validation is implemented on the training set. As a result, 32-dimension is applied for all hidden states {h n,t } N,T n=1,t=1 of LSTM. Besides, the batch size is set to be 128. Learning rate is 0.0001, and it is reduced by a factor of 0.01 every 500 iterations with Adam <ref type="bibr" target="#b22">[23]</ref>.</p><p>For postprocessing, a 2-dimension Gaussian filter, with the cut-off frequency being 6 dB, is applied to smooth the fixations of face regions in the training frames. Then, {S * l } L l=1 can be obtained for calculating the weight of each feature channel by <ref type="bibr" target="#b9">(10)</ref>. Moveover, σ of (8) is set to 10 −0.2 for imposing center-bias on saliency of each face, in order to make saliency prediction appropriate. The impact of different σ on saliency prediction results is to be discussed in Section 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on saliency prediction</head><p>In this section, we compare our method with 8 conventional saliency prediction methods <ref type="bibr" target="#b2">3</ref> , including Xu et al. <ref type="bibr" target="#b41">[41]</ref>, Salicon <ref type="bibr" target="#b13">[14]</ref>, Jiang et al. <ref type="bibr" target="#b20">[21]</ref>, GBVS <ref type="bibr" target="#b9">[10]</ref>, Rudoy et al. <ref type="bibr" target="#b37">[37]</ref>, PQFT <ref type="bibr" target="#b7">[8]</ref>, Surprise <ref type="bibr" target="#b17">[18]</ref> and OBDL <ref type="bibr" target="#b11">[12]</ref>. Among them, <ref type="bibr" target="#b37">[37]</ref>, <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b17">[18]</ref> and <ref type="bibr" target="#b11">[12]</ref> are the latest video saliency prediction methods. Besides, <ref type="bibr" target="#b41">[41]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b9">[10]</ref> are recent image saliency prediction methods. To be more specific, <ref type="bibr" target="#b41">[41]</ref> and <ref type="bibr" target="#b20">[21]</ref> work on saliency prediction of singleface and multiple-face images, respectively. We compare our method to these two top-down methods, as there is no multiple-face saliency prediction method for videos. Note that we use our multiple-face detection technique of Section 3.1 to detect faces for <ref type="bibr" target="#b41">[41]</ref>, as its face detection only handles the single-face scenario. On the contrary, <ref type="bibr" target="#b9">[10]</ref> is a bottom-up method, which provides background saliency for our method. Therefore, <ref type="bibr" target="#b9">[10]</ref> is also included in our comparison. In addition, <ref type="bibr" target="#b13">[14]</ref> is another latest DL-based method <ref type="figure">Figure 8</ref>. Saliency maps for different frames of a video sequence selected from MUFVET-I. These maps are generated by GT human fixations, our method, Xu et al. <ref type="bibr" target="#b41">[41]</ref>, Salicon <ref type="bibr" target="#b13">[14]</ref>, Jiang et al. <ref type="bibr" target="#b20">[21]</ref>, OBDL <ref type="bibr" target="#b11">[12]</ref>, Rudoy et al. <ref type="bibr" target="#b37">[37]</ref>, PQFT <ref type="bibr" target="#b7">[8]</ref>, Surprise <ref type="bibr" target="#b17">[18]</ref> and GBVS <ref type="bibr" target="#b9">[10]</ref>.</p><p>for saliency detection, which is also compared with our DLbased method.</p><p>The most recent work of <ref type="bibr" target="#b26">[27]</ref> has reported that normalized scanpath saliency (NSS) and correlation coefficient (C-C) perform the best among all metrics in evaluating saliency prediction accuracy <ref type="bibr" target="#b3">4</ref> . Thereby, we compare our method with other 8 methods in terms of NSS and CC. <ref type="table" target="#tab_1">Table 2</ref> reports the comparison results of saliency prediction, averaged over all 65 test videos of MUFVET-I. We can see from this table that our method is much better than all other methods in predicting saliency of multiple-face videos. Specifically, our method has 0.98 NSS and 0.13 CC improvement over <ref type="bibr" target="#b41">[41]</ref>, the performance of which ranks second. Such an improvement is mainly due to the following reason: Saliency of all faces is with equal importance in <ref type="bibr" target="#b41">[41]</ref>, while the consideration of temporal transition enables our method to accurately predict salient face across frames. Moreover, it is worth pointing out that both our method and <ref type="bibr" target="#b41">[41]</ref> are superior to <ref type="bibr" target="#b20">[21]</ref> which imposes unequal importance on different faces in an image. The main reason is that the utilization of only static features in <ref type="bibr" target="#b20">[21]</ref> may predict wrong salient face in a video. On the other hand, long short-term temporal transition of our method is really effective in finding the salient <ref type="figure">Figure 9</ref>. Saliency maps for several frames selected from different video sequences of MUFVET-I. These maps are generated by GT human fixations, our method, Xu et al. <ref type="bibr" target="#b41">[41]</ref>, Salicon <ref type="bibr" target="#b13">[14]</ref>, Jiang et al. <ref type="bibr" target="#b20">[21]</ref>, OBDL <ref type="bibr" target="#b11">[12]</ref>, Rudoy et al. <ref type="bibr" target="#b37">[37]</ref>, PQFT <ref type="bibr" target="#b7">[8]</ref>, Surprise <ref type="bibr" target="#b17">[18]</ref> and GBVS <ref type="bibr" target="#b9">[10]</ref>.</p><p>face in multiple-face videos.</p><p>Next, we move to the comparison of subjective results. We show in <ref type="figure">Figure 8</ref> the saliency maps of several frames in a video, generated by our and other 8 methods. From this figure, one may observe that our method is capable of finding the salient face. As a result, the saliency maps of our method are more accurate than those of other methods. For example, we can see from <ref type="figure">Figure 8</ref> that the salient face is changed from the girl to the man and then back to the girl, which is extremely consistent with our prediction. On the contrary, <ref type="bibr" target="#b41">[41]</ref> finds all three faces as salient ones, and <ref type="bibr" target="#b20">[21]</ref> misses the salient face of the speaking man. In addition, <ref type="figure">Figure 9</ref> provides the saliency maps of the frames selected from 5 videos. Again, this figure verifies that our method is able to precisely locate salient face by considering temporal saliency transition in M-LSTM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance analysis of saliency prediction</head><p>Since our M-LSTM presented in Section 3.3 aims at predicting saliency weights of faces across video frames, it is worth evaluating the prediction error of M-LSTM. To this end, <ref type="figure">Figure 10</ref> plots saliency weights of faces by CNN, M-LSTM and GT, for the video sequence of <ref type="figure">Figure 8</ref>. In this figure, the curves of CNN refer to the output of CNN (either 0 or 1), and the curves of M-LSTM are obtained upon  <ref type="figure">Figure 10</ref>. Saliency weights of faces along with processed frames for the video in <ref type="figure">Figure 10</ref>, predicted by our CNN (green line), M-LSTM (blue line) and GT (red line). Note that the GT in this curve is plotted with proportion of human fixations in each face to those in all faces. In this figure, the mean squared error (MSE) between M-LSTM and GT averaged over 3 faces is 0.0081.</p><p>the predicted face saliency weights output by M-LSTM. Besides, the curves for GT are the target output of M-LSTM. We can see from <ref type="figure">Figure 10</ref> that the predicted face saliency weights of M-LSTM approach to the target, with significant improvement and smooth over the curves of CNN. More importantly, similar results can be found for other videos in our database. Here, we calculate quantify mean squared error (MSE) of face saliency weights between M-LSTM and GT, averaged over all faces in MUVFET-I. The averaged MSE is 0.0081, the same as the result of the video sequence in <ref type="figure">Figure 10</ref>. This also implies the small gap of M-LSTM in predicting saliency weights of faces. Next, it is interesting to see how the gap between our predicted and GT face saliency weights influences saliency prediction performance. To this end, we use GT face saliency weights {w n,t } N,T n=1,t=1 as the input to (7) for generalizing the final saliency maps of multiple-face videos. The averaged results are reported in the second column of <ref type="table" target="#tab_1">Table 2</ref>. It can be found that saliency prediction performance of using estimated (M-LSTM) and target (GT) saliency weights of faces is close, implying that our method is approaching to the "upper bound" performance.</p><p>At last, it is necessary to investigate the effectiveness of face center-bias introduced in our method. To this end, standard deviation σ in (8) is traversed, imposing different impact of face center-bias on saliency prediction. <ref type="figure">Figure  11</ref> plots the NSS and CC results at different σ,a v e r a g e d over all videos. It is obvious that the best performance is achieved once σ = 10 −0.2 , and thus σ was set to 10 −0.2 in our above experiments. Note that center-bias is not the most important factor influencing performance improvement of our approach. We test the baseline that places all the weight  <ref type="figure">Figure 11</ref>. Saliency prediction performance versus different center-bias parameter σ of <ref type="bibr" target="#b7">(8)</ref>.</p><p>to the face closest to the center, and the average NSS of this baseline is 2.57, which is much lower than 4.12 of our approach. In addition, we conduct an experiment for the baseline relying on <ref type="bibr" target="#b41">[41]</ref> with the center prior. The results show an improvement of 0.03 in NSS and 0.007 in CC over <ref type="bibr" target="#b41">[41]</ref>, which is still much inferior to our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Interestingly, we found that when viewing multiple-face videos, humans are consistently attracted by one face in each single frame. Such a finding was verified by the statistical analysis on the eye tracking database of MUFVET established in this paper, in which MUFVET-I is set for test and MUFVET-II is for training. To predict the salient face in multiple-face videos, we proposed in this paper a DLbased method, in which both CNN and RNN are combined in a framework and then trained over MUFVET-II. Specifically, CNN, fined-tuned on Google Net, was adopted in our DL-based method, for automatically learning the features relevant to locating the salient face. After observing CN-N features in each video frame, M-LSTM, as a deep RN-N proposed in this paper, was utilized to take into account the transition of face saliency from previous frames, either in short-term or long-term. As a result, saliency maps of multiple-face videos can be generated upon the predicted salient face. Finally, the experimental results illustrated that our method is able to significantly advance state-of-the-art saliency prediction on multiple-face videos.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. One example for each category of videos in MUFVET-I and MUFVET-II. From left to right, the videos belong to TV play/movie, interview, video conference, TV show, music/talk show, and group discussion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Proportions of fixations and pixels in face and background over all videos of MUFVET.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Proportions of fixations falling into one face and other faces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Overview pipeline for our DL-based method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Structure of M-LSTM.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Video categories in MUFVET-I and MUFVET-II.</figDesc><table>Category 
TV play/movie 
interview 
video conference 
TV show 
music/talk show 
group discussion 
overall 
Number of videos (I) 
12 
20 
6 
7 
10 
10 
65 
Number of videos (II) 
21 
13 
5 
35 
18 
8 
100 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Accuracy of saliency prediction by our and other 8 methods, averaged over all test videos of MUFVET-I. Our GT Xu et al. [41] Salicon [14] Jiang et al. [21] GBVS [10] Rudoy et al. [37] PQFT [8] Surprise [18] OBDL [12]</figDesc><table>NSS 4.12 4.21 
3.14 
2.96 
0.97 
1.23 
1.42 
0.88 
0.88 
1.62 
CC 0.74 0.77 
0.61 
0.52 
0.29 
0.33 
0.36 
0.22 
0.21 
0.30 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">. 2.2. Data Analysis In this section, we thoroughly analyze the collected eye tracking data of MUFVET, in order to further learn the visual attention model on multiple-face videos. According to 1 https://github.com/yufanLIU/salient-face-in-MUVFET/tree/master/MUVFET.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Note that saliency produced by the channel of single feature is defined as the conspicuity map, in order to make difference from the saliency map which is generated by all channels.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">In our experiments, we run the codes provided by the authors with default parameters, to obtain saliency prediction results.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">[27] has also shown that area under ROC (AUC) is the worst metric in measuring accuracy of saliency prediction.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement.</head><p>We would like to thank KingFar International Inc to provide the eye tracker and its technical support. Also, we thank all participants in the eye-tracking experiment. This work was supported by the NSFC projects under Grants 61573037 and 61202139, and Fok Ying-Tong education foundation under Grant 151061.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="207" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Saliency based on information maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tsotsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Optimal scanning for faster object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Butko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Movellan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2751" to="2758" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Predicting human gaze using low-level saliency combined with face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cerf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Einhäuser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Visual saliency detection by spatially weighted dissimilarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Miao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Qing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Video saliency detection in the compressed domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-M</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="697" to="700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Cvx: Matlab software for disciplined convex programming</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grant</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stephen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ye</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
	<note>cvxr.com</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A novel multiresolution spatiotemporal saliency detection model and its applications in image and video compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="198" />
			<date type="published" when="2010-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Eye-tracking database for a set of standard video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hadizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Enriquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajić</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="898" to="903" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="545" to="552" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">How many bits does it take for a stimulus to be salient?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Khatoonabadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">V</forename><surname>Bajic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5501" to="5510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A probabilistic saliency model with memory-guided top-down cues for free-viewing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Salicon: Reducing the semantic gap in saliency prediction by adapting deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="262" to="270" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A distribution-free approach to inducing rank correlation among input variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Iman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Conover</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Statistics-Simulation and Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="311" to="334" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automatic foveation for video compression using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1304" to="1318" />
			<date type="published" when="2004-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bayesian surprise attracts human attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision research</title>
		<imprint>
			<biblScope unit="volume">49</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1295" to="1306" />
			<date type="published" when="2009-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Realistic avatar eye and head animation using a neurobiological model of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dhavale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pighin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Optical Science and Technology</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="64" to="78" />
			<date type="published" when="2004-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A model of saliency-based visual attention for rapid scene analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Niebur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1254" to="1259" />
			<date type="published" when="1998-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Saliency in crowd</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="17" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Deepfix: A fully convolutional neural network for predicting human eye fixations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Kruthiventi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ayush</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1510.02927</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kummerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Wallis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<title level="m">Deepgaze ii: Reading fixations from deep features trained on object recognition. arXiv, 1610.01563</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Visual saliency based on scale-space analysis in the frequency domain. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A data-driven metric for comprehensive evaluation of saliency models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Predicting eye fixations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="362" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Saccadic suppression: a review and an analysis. Psychological bulletin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Matin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1974-12" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="899" to="917" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">K</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">J</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Henderson</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011-03" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Static saliency vs. dynamic saliency: a comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM international conference on Multimedia (ACM MM)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="987" to="996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.00845</idno>
		<title level="m">Shallow and deep convolutional networks for saliency prediction</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A stochastic model of selective visual attention with a dynamic bayesian network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kimura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Takeuchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yamato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kashino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Multimedia and Expo (ICME)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1073" to="1076" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Beyond bottom-up: Incorporating taskdependent influences into a computational model of spatial attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Peters</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Gaffe: A gaze-attentive fixation finding engine. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rajashekar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Van Der Linde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Cormack</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Detecting events and key actors in multi-person videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gorban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1147" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Funnel-structured cascade for multi-view face detection with alignment-awareness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Neurocomputing</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Bottom-up saliency detection with sparse representation of learnt texture atoms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to predict saliency on face images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Saliency detection: A boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="153" to="160" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Learning a saliency map using fixated locations in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
