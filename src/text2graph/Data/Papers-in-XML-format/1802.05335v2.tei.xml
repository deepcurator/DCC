<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multimodal Generative Models for Scalable Weakly-Supervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mike</forename><surname>Wu</surname></persName>
							<email>wumike@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Department of Computer Science</orgName>
								<orgName type="department" key="dep2">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Goodman</surname></persName>
							<email>ngoodman@stanford.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<postCode>94025</postCode>
									<settlement>Palo Alto</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multimodal Generative Models for Scalable Weakly-Supervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Multiple modalities often co-occur when describing natural phenomena. Learning a joint representation of these modalities should yield deeper and more useful representations. Previous generative approaches to multi-modal input either do not learn a joint distribution or require additional computation to handle missing data. Here, we introduce a multimodal variational autoencoder (MVAE) that uses a product-of-experts inference network and a sub-sampled training paradigm to solve the multi-modal inference problem. Notably, our model shares parameters to efficiently learn under any combination of missing modalities. We apply the MVAE on four datasets and show that we match state-of-the-art performance using many fewer parameters. In addition, we show that the MVAE is directly applicable to weakly-supervised learning, and is robust to incomplete supervision. We then consider a case study of learning image transformations-edge detection, colorization, facial landmark segmentation, etc.-as a set of modalities. We find appealing results across this range of tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Learning from diverse modalities has the potential to yield more generalizable representations. For instance, the visual appearance and tactile impression of an object converge on a more invariant abstract characterization <ref type="bibr" target="#b29">[30]</ref>. Similarly, an image and a natural language caption can capture complimentary but converging information about a scene <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b27">28]</ref>. While fully-supervised deep learning approaches <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b20">21]</ref> can learn to bridge modalities, generative approaches promise to capture the joint distribution across modalities and flexibly support missing data. Indeed, multimodal data is expensive and sparse, leading to a weakly supervised setting of having only a small set of examples with all observations present, but having access to a larger dataset with one (or a subset of) modalities.</p><p>We propose a novel multimodal variational autoencoder (MVAE) to learn a joint distribution under weak supervision. The VAE <ref type="bibr" target="#b10">[11]</ref> jointly trains a generative model, from latent variables to observations, with an inference network from observations to latents. Moving to multiple modalities and missing data, we would naively need an inference network for each combination of modalities. However, doing so would result in an exponential explosion in the number of trainable parameters. Assuming conditional independence among the modalities, we show that the correct inference network will be a product-of-experts <ref type="bibr" target="#b6">[7]</ref>, a structure which reduces the number of inference networks to one per modality. While the inference networks can be best trained separately, the generative model requires joint observations. Thus we propose a sub-sampled training paradigm in which fully-observed examples are treated as both fully and partially observed (for each gradient update). Altogether, this provides a novel and useful solution to the multi-modal inference problem.</p><p>We report experiments to measure the quality of the MVAE, comparing with previous models. We train on MNIST <ref type="bibr" target="#b12">[13]</ref>, binarized MNIST <ref type="bibr" target="#b11">[12]</ref>, MultiMNIST <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19]</ref>, FashionMNIST <ref type="bibr" target="#b26">[27]</ref>, and CelebA <ref type="bibr" target="#b13">[14]</ref>. Several of these datasets have complex modalities-character sequences, RGB imagesrequiring large inference networks with RNNs and CNNs. We show that the MVAE is able to support heavy encoders with thousands of parameters, matching state-of-the-art performance.</p><p>We then apply the MVAE to problems with more than two modalities. First, we revisit CelebA, this time fitting the model with each of the 18 attributes as an individual modality. Doing so, we find better performance from sharing of statistical strength. We further explore this question by choosing a handful of image transformations commonly studied in computer vision-colorization, edge detection, segmentation, etc.-and synthesizing a dataset by applying them to CelebA. We show that the MVAE can jointly learn these transformations by modeling them as modalities.</p><p>Finally, we investigate how the MVAE performs under incomplete supervision by aggressively reducing the number of multi-modal examples. We find that the MVAE is able to capture a good joint representation when only a small percentage (5%) of examples are multi-modal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Methods</head><p>A variational autoencoder (VAE) <ref type="bibr" target="#b10">[11]</ref> is a latent variable generative model of the form p θ (x, z) = p(z)p θ (x|z) where p(z) is a prior, usually spherical Gaussian. The decoder, p θ (x|z), consists of a deep neural net, with parameters θ, composed with a simple likelihood (e.g. Bernoulli or Gaussian). The goal of training is to maximize the marginal likelihood of the data (the "evidence"); however since this is intractable, the evidence lower bound (ELBO) is instead optimized. The ELBO is defined via an inference network, q φ (z|x), which serves as a tractable importance distribution:</p><formula xml:id="formula_0">E q φ (z|x) [λ log p θ (x|z)] − β KL[q φ (z|x), p(z)],<label>(1)</label></formula><p>where KL[p, q] is the Kullback-Leibler divergence between distributions p and q; β <ref type="bibr" target="#b5">[6]</ref> and λ are weights balancing the terms in the ELBO. In practice, λ = 1 and β is slowly annealed to 1 <ref type="bibr">[1]</ref> to form a valid lower bound on the evidence. The ELBO is usually optimized (as we will do here) via stochastic gradient descent, using the reparameterization trick to estimate the gradient <ref type="bibr" target="#b10">[11]</ref>. In the multimodal setting we assume the N modalities, x 1 , ..., x N , are conditionally independent given the common latent variable, z (See <ref type="figure" target="#fig_0">Fig. 1a)</ref>. That is we assume a generative model of the form</p><formula xml:id="formula_1">p θ (x 1 , x 2 , ..., x N ) = p(z)p θ (x 1 |z)p θ (x 2 |z) · · · p θ (x N |z).</formula><p>With this factorization, we can ignore unobserved modalities when evaluating the marginal likelihood. If we write a data point as the collection of modalities present, that is X = {x i |i th modality present}, then the ELBO becomes:</p><formula xml:id="formula_2">E q φ (z|X) [ xi∈X λ i log p θ (x i |z)] − β KL[q φ (z|X), p(z)].<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Approximating The Joint Posterior</head><p>The first obstacle to training the MVAE is specifying the 2 N inference networks, q(z|X) for each subset of modalities X ⊆ {x 1 , x 2 , ..., x N }. Previous work (e.g. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b23">24]</ref>) has assumed that the relationship between the joint-and single-modality inference networks is unpredictable (and therefore separate training is required). However, the optimal inference network q(z|x 1 , ..., x N ) would be the true posterior p(z|x 1 , ..., x N ). The conditional independence assumptions in the generative model imply a relation among joint-and single-modality posteriors:</p><formula xml:id="formula_3">p(z|x 1 , ..., x N ) = p(x 1 , ..., x N |z)p(z) p(x 1 , ..., x N ) = p(z) p(x 1 , ..., x N ) N i=1 p(x i |z) = p(z) p(x 1 , ..., x N ) N i=1 p(z|x i )p(x i ) p(z) = N i=1 p(z|x i ) N −1 i=1 p(z) · N i=1 p(x i ) p(x 1 , ..., x N ) ∝ N i=1 p(z|x i ) N −1 i=1 p(z) (3)</formula><p>That is, the joint posterior is a product of individual posteriors, with an additional quotient by the prior. This suggests that the correct q(z|x 1 , ..., x N ) is a product and quotient of experts:</p><formula xml:id="formula_4">n i=1 q(z|xi) n−1 i=1 p(z)</formula><p>, which we call MVAE-Q.</p><p>Alternatively, if we approximate p(z|x i ) with q(z|x i ) ≡q(z|x i )p(z), whereq(z|x i ) is the underlying inference network, we can avoid the quotient term:</p><formula xml:id="formula_5">p(z|x 1 , ..., x N ) ∝ N i=1 p(z|x i ) N −1 i=1 p(z) ≈ N i=1 [q(z|x i )p(z)] N −1 i=1 p(z) = p(z) N i=1q (z|x i ).<label>(4)</label></formula><p>In other words, we can use a product of experts (PoE), including a "prior expert", as the approximating distribution for the joint-posterior <ref type="figure" target="#fig_0">(Figure 1b</ref>). This representation is simpler and, as we describe below, numerically more stable. This derivation is easily extended to any subset of modalities yielding q(z|X) ∝ p(z) xi∈Xq (z|x i ) <ref type="figure" target="#fig_0">(Figure 1c</ref>). We refer to this version as MVAE. The product and quotient distributions required above are not in general solvable in closed form. However, when p(z) andq(z|x i ) are Gaussian there is a simple analytical solution: a product of Gaussian experts is itself Gaussian <ref type="bibr" target="#b3">[4]</ref> </p><formula xml:id="formula_6">with mean µ = ( i µ i T i )( i T i ) −1 and covariance V = ( i T i ) −1</formula><p>, where µ i , V i are the parameters of the i-th Gaussian expert, and</p><formula xml:id="formula_7">T i = V −1 i</formula><p>is the inverse of the covariance. Similarly, given two Gaussian experts, p 1 (x) and p 2 (x), we can show that the quotient (QoE),</p><formula xml:id="formula_8">p1(x)</formula><p>p2(x) , is also a Gaussian with mean µ = (</p><formula xml:id="formula_9">T 1 µ 1 − T 2 µ 2 )(T 1 − T 2 ) −1 and covariance V = (T 1 − T 2 ) −1</formula><p>, where</p><formula xml:id="formula_10">T i = V −1</formula><p>i . However, this distribution is well-defined only if V 2 &gt; V 1 element-wise-a simple constraint that can be hard to deal with in practice. A full derivation for PoE and QoE can be found in the supplement.</p><p>Thus we can compute all 2 N multi-modal inference networks required for MVAE efficiently in terms of the N uni-modal components,q(z|x i ); the additional quotient needed by the MVAE-Q variant is also easily calculated but requires an added constraint on the variances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Sub-sampled Training Paradigm</head><p>On the face of it, we can now train the MVAE by simply optimizing the evidence lower bound given in Eqn. 2. However, a product-of-Gaussians does not uniquely specify its component Gaussians. Hence, given a complete dataset, with no missing modalities, optimizing Eqn. 2 has an unfortunate consequence: we never train the individual inference networks (or small sub-networks) and thus do not know how to use them if presented with missing data at test time. Conversely, if we treat every observation as independent observations of each modality, we can adequately train the inference networksq(z|x i ), but will fail to capture the relationship between modalities in the generative model.</p><p>We propose instead a simple training scheme that combines these extremes, including ELBO terms for whole and partial observations. For instance, with N modalities, a complete example, {x 1 , x 2 , ..., x N } can be split into 2 N partial examples: {x 1 }, {x 2 , x 6 }, {x 5 , x N −4 , x N }, .... If we were to train using all 2 N subsets it would require evaluating 2 N ELBO terms. This is computationally intractable. To reduce the cost, we sub-sample which ELBO terms to optimize for every gradient step. Specifically, we choose (1) the ELBO using the product of all N Gaussians, (2) all ELBO terms using a single modality, and (3) k ELBO terms using k randomly chosen subsets. For each minibatch,  <ref type="table">CelebA  VAE  730240  730240  3409536  1316936  4070472  CVAE  735360  735360  3414656  -4079688  BiVCCA  1063680  1063680  3742976  1841936  4447504  JMVAE  2061184  2061184  7682432  4075064  9052504  MVAE-Q  1063680  1063680  3742976  1841936  4447504  MVAE  1063680  1063680  3742976  1841936  4447504  JMVAE19  ----3.6259e12  MVAE19</ref> ----10857048 <ref type="table">Table 1</ref>: Number of inference network parameters. For a single dataset, each generative model uses the same inference network architecture(s) for each modality. Thus, the difference in parameters is solely due to how the inference networks interact in the model. We note that MVAE has the same number of parameters as BiVCCA. JMVAE19 and MVAE19 show the number of parameters using 19 inference networks when each of the attributes in CelebA is its own modality.</p><p>we thus evaluate a random subset of the 2 N ELBO terms. In expectation, we will be approximating the full objective. We explore the effect of k in Section 5.</p><p>A pleasant side-effect of this training scheme is that it immediately applies to weakly-supervised learning. Given an incomplete example with missing data, X = {x i |i th modality present}, we can still sample partial data from X, ignoring the modalities that are missing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Given two modalities, x 1 and x 2 , many variants of VAEs <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b9">10]</ref> have been used to train generative models of the form p(x 2 |x 1 ), including conditional VAEs (CVAE) <ref type="bibr" target="#b19">[20]</ref> and conditional multi-modal autoencoders (CMMA) <ref type="bibr" target="#b15">[16]</ref>. Critically, these models are not bi-directional. We are more interested in studying models where we can condition interchangeably. For example, the BiVCCA <ref type="bibr" target="#b25">[26]</ref> trains two VAEs together with interacting inference networks to facilitate two-way reconstruction. However, it does not attempt to directly model the joint distribution, which we find empirically to improve the ability of a model to learn the data distribution.</p><p>Several recent models have tried to capture the joint distribution explicitly. <ref type="bibr" target="#b21">[22]</ref> introduced the joint multi-modal VAE (JMVAE), which learns p(x 1 , x 2 ) using a joint inference network, q(z|x 1 , x 2 ). To handle missing data at test time, the JMVAE collectively trains q(z|x 1 , x 2 ) with two other inference networks q(z|x 1 ) and q(z|x 2 ). The authors use an ELBO objective with two additional divergence terms to minimize the distance between the uni-modal and the multi-modal importance distributions. Unfortunately, the JMVAE trains a new inference network for each multi-modal subset, which we've argued to be intractable in the general setting.</p><p>Most recently, <ref type="bibr" target="#b23">[24]</ref> introduce another objective for the bi-modal VAE, which they call the triplet ELBO. Like the MVAE, their model's joint inference network q(z|x 1 , x 2 ) combines variational distributions using a product-of-experts rule. Unlike the MVAE, the authors report a two-stage training process: using complete data, fit q(z|x 1 , x 2 ) and the decoders. Then, freezing p(x 1 |z) and p(x 2 |z), fit the uni-modal inference networks, q(z|x 1 ) and q(z|x 2 ) to handle missing data at test time. Crucially, because training is separated, the model has to fit 2 new inference networks to handle all combinations of missing data in stage two. While this paradigm is sufficient for two modalities, it does not generalize to the truly multi-modal case. To the best of our knowledge, the MVAE is the first deep generative model to explore more than 2 modalities efficiently. Moreover, the single-stage training of the MVAE makes it uniquely applicable to weakly-supervised learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>As in previous literature, we transform uni-modal datasets into multi-modal problems by treating labels as a second modality. We compare existing models (VAE, BiVCCA, JMVAE) to the MVAE and show that we equal state-of-the-art performance. For each dataset, we keep the network architectures consistent across models, varying only the objective and training procedure. Unless otherwise noted, given images x 1 and labels x 2 , we set λ 1 = 1 and λ 2 = 50. We find that upweighting the recon-struction error for the low-dimensional modalities is important for learning a good joint distribution. We next briefly review data sets and training choices; further details, including encoder/decoder architecture specification, can be found in the supplement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST/BinaryMNIST</head><p>We use the MNIST hand-written digits dataset <ref type="bibr" target="#b12">[13]</ref> with 50,000 examples for training, 10,000 validation, 10,000 testing. We also train on a binarized version to align with previous work <ref type="bibr" target="#b11">[12]</ref>. As in <ref type="bibr" target="#b21">[22]</ref>, we use the Adam optimizer <ref type="bibr" target="#b8">[9]</ref> with a learning rate of 1e-3, a minibatch size of 100, 64 latent dimensions, and train for 500 epochs. We anneal β from 0 to 1 linearly for the first 200 epochs. For the encoders and decoders, we use MLPs with 2 hidden layers of 512 nodes. We model p(x 1 |z) with a Bernoulli likelihood and p(x 2 |z) with a multinomial likelihood.</p><p>FashionMNIST This is an MNIST-like fashion dataset containing 28 x 28 grayscale images of clothing from 10 classes-skirts, shoes, t-shirts, etc <ref type="bibr" target="#b26">[27]</ref>. We use identical hyperparameters as in MNIST. However, we employ a miniature DCGAN <ref type="bibr" target="#b17">[18]</ref> for the image encoder and decoder.</p><p>MultiMNIST This is variant of MNIST where between 0 and 4 digits are composed together on a 50x50 canvas. Unlike <ref type="bibr" target="#b4">[5]</ref>, the digits are fixed in location. We generate the text modality by concatenating the digit classes from top-left to bottom-right. We use 100 latent dimensions, with the remaining hyperparameters as in MNIST. For the image encoder and decoder, we retool the DCGAN architecture from <ref type="bibr" target="#b17">[18]</ref>. For the text encoder, we use a bidirectional GRU with 200 hidden units. For the text decoder, we first define a vocabulary with ten digits, a start token, and stop token. Provided a start token, we feed it through a 2-layer GRU, linear layers, and a softmax. We sample a new character and repeat until generating a stop token. We note that previous work has not explored RNN-VAE inference networks in multi-modal learning, which we show to work well with the MVAE.</p><p>CelebA The CelebFaces and Attributes (CelebA) dataset <ref type="bibr" target="#b28">[29]</ref> contains over 200k images of celebrities. Each image is tagged with 40 attributes i.e. wears glasses, or has bangs. We use the aligned and cropped version with a selected 18 visually distinctive attributes, as done in <ref type="bibr" target="#b16">[17]</ref>. Images are rescaled to 64x64. For the first experiment, we treat images as one modality, x 1 , and attributes as a second modality, x 2 where a single inference network predicts all 18 attributes. We also explore a variation of MVAE, called MVAE19, where we treat each attribute as its own modality for a total of 19. To approximate the full objective, we set k = 1 for a total 21 ELBO terms. We use Adam with a learning rate of 10</p><formula xml:id="formula_11">−4</formula><p>, a minibatch size of 100, and anneal KL for the first 20 of 100 epochs. We again use DCGAN for image networks. For the attribute encoder and decoder, we use an MLP with 2 hidden layers of size 512. For MVAE19, we have 18 such encoders and decoders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Evaluation</head><p>In the bi-modal setting with x 1 denoting the image and x 2 denoting the label, we measure the test marginal log-likelihood, log p(x 1 ), and test joint log-likelihood log p(x 1 , x 2 ) using 100 importance samples in CelebA and 1000 samples in other datasets. In doing so, we have a choice of which inference network to use. For example, using q(z|x 1 ), we estimate log p(</p><formula xml:id="formula_12">x 1 ) ≈ log E q(z|x1) [ p(x1|z)p(z) q(z|x1) ].</formula><p>We also compute the test conditional log-likelihood log p(x 1 |x 2 ), as a measure of classification performance, as done in <ref type="bibr" target="#b21">[22]</ref>: log p(</p><formula xml:id="formula_13">x 1 |x 2 ) ≈ log E q(z|x2) [ p(x1|z)p(x2|z)p(z) q(z|x2) ] − log E p(z) [p(x 2 |z)]</formula><p>. In CelebA, we use 1000 samples to estimate E p(z) [p(x 2 |z)]. In all others, we use 5000 samples. These marginal probabilities measure the ability of the model to capture the data distribution and its conditionals. Higher scoring models are better able to generate proper samples and convert between modalities, which is exactly what we find desirable in a generative model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Quality of the Inference Network</head><p>In all VAE-family models, the inference network functions as an importance distribution for approximating the intractable posterior. A better importance distribution, which more accurately approximates the posterior, results in importance weights with lower variance. Thus, we estimate the variance of the (log) importance weights as a measure of inference network quality (see <ref type="table">Table 3</ref>). <ref type="figure" target="#fig_2">Fig. 2</ref> shows image samples and conditional image samples for each dataset using the image generative model. We find the samples to be good quality, and find conditional samples to be largely correctly  <ref type="table">Table 2</ref>: Estimates (using q(z|x 1 )) for marginal probabilities on the average test example. MVAE and JMVAE are roughly equivalent in data log-likelihood but as <ref type="table">Table 1</ref> shows, MVAE uses far fewer parameters. The CVAE is often better at capturing p(x 1 |x 2 ) but does not learn a joint distribution. </p><formula xml:id="formula_14">(a) (b) (c) (d) (e) (f) (g) (h)</formula><formula xml:id="formula_15">) x 2 = 5, (d) x 2 = Ankle boot, (f) x 2 = 1773, (h) x 2 = Male.</formula><p>matched to the target label. <ref type="table">Table 2</ref> shows test log-likelihoods for each model and dataset. <ref type="bibr">1</ref> We see that MVAE performs on par with the state-of-the-art (JMVAE) while using far fewer parameters (see <ref type="table">Table 1</ref>). When considering only p(x 1 ) (i.e. the likelihood of the image modality alone), the MVAE also performs best, slightly beating even the image-only VAE, indicating that solving the harder multi-modal problem does not sacrifice any uni-modal model capacity and perhaps helps. On CelebA, MVAE19 (which treats features as independent modalities) out-performs the MVAE (which treats the feature vector as a single modality). This suggests that the PoE approach generalizes to a larger number of modalities, and that jointly training shares statistical strength.</p><p>Tables 3 show variances of log importance weights. The MVAE always produces lower variance than other methods that capture the joint distribution, and often lower than conditional or single-modality models. Furthermore, MVAE19 consistently produces lower variance than MVAE in CelebA. Overall, this suggests that the PoE approach used by the MVAE yields better inference networks.  <ref type="table">Table 3</ref>: Average variance of log importance weights for three marginal, probabilities, estimated by importance sampling from q(z|x 1 ). 1000 importance samples were used to approximate the variance. The lower the variance, the better quality the inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of number of ELBO terms</head><p>In the MVAE training paradigm, there is a hyperparameter k that controls the number of sampled ELBO terms to approximate the intractable objective. To investigate its importance, we vary k from 0 to 50 and for each, train a MVAE19 on CelebA. We find that increasing k has little effect on data log-likelihood but reduces the variance of the importance distribution defined by the inference networks. In practice, we choose a small k as a tradeoff between computation and a better importance distribution. See supplement for more details. We plot the level of supervision as a fraction from 0 (meaning no complete examples) to 1 (meaning no missing modalities) against accuracy. For MNIST and FashionMNIST, we predict the target class. For MultiMNIST, we compute accuracy for predicting each digit correctly. For CelebA, we use the MVAE19 model and either observe all attributes or none. Since many attributes are imbalanced, (b) shows 5 attributes where prediction is not trivial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Weakly Supervised Learning</head><p>For each dataset, we simulate incomplete supervision by randomly reserving a fraction of the dataset as multi-modal examples. The remaining data is split into two datasets: one with only the first modality, and one with only the second. We examine the effect of supervision on the predictive task p(x 2 |x 1 ), e.g. predict the correct digit label, x 2 , from an image x 1 . The total number of examples shown to the model is always fixed -only the proportion of complete bi-modal examples is varied.</p><p>From <ref type="figure" target="#fig_3">Fig. 3</ref>, we see a diminishing marginal return across all datasets, suggesting that given a small set of multi-modal examples, we can still effectively learn the joint distribution using a larger set of uni-modal data. For example, in MNIST, FashionMNIST, and MultiMNIST, just 5% of multi-modal examples is sufficient to achieve near full-supervision performance. See supplement for additional experiments: we show that the MVAE19 is robust to randomly dropping modalities in CelebA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Case study: Computer Vision Applications</head><p>We use the MVAE to learn image transformations (and their inverses) as conditional distributions. In particular, we focus on colorization, edge detection, facial landmark segmentation, image completion, and watermark removal. The original image is itself a modality, for a total of six.</p><p>To build the dataset, we apply ground-truth transformations to CelebA. For colorization, we transform RGB colors to grayscale. For image completion, half of the image is replaced with black pixels. For watermark removal, we overlay a generic watermark. To extract edges, we use the Canny detector <ref type="bibr" target="#b2">[3]</ref> from Scikit-Image <ref type="bibr" target="#b22">[23]</ref>. To compute facial landscape masks, we use dlib <ref type="bibr" target="#b7">[8]</ref> and OpenCV <ref type="bibr" target="#b1">[2]</ref>.</p><p>We fit a MVAE with 250 latent dimensions and k=1. We use Adam with a 10</p><formula xml:id="formula_16">−4</formula><p>learning rate, a batch size of 50, λ i = 1 for i = 1, ..., N , β annealing for 20 out of 100 epochs. <ref type="figure" target="#fig_4">Fig. 4</ref> shows samples showcasing different learned transformations. In <ref type="figure" target="#fig_4">Fig. 4a</ref> we encode the original image with the learned encoder, then decode the transformed image with the learned generative model. We see reasonable reconstruction, and good facial landscape and edge extraction. In Figs.4b, 4c, 4d we go in the opposite direction, encoding a transformed image and then sampling from the generative model to reconstruct the original. The results are again quite good: reconstructed half-images agree on gaze direction and hair color, colorizations are reasonable, and all trace of the watermark is removed. (Though the reconstructed images still suffer from the same blurriness that VAEs do <ref type="bibr" target="#b30">[31]</ref>.) </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>In this paper, we introduced a multi-modal variational autoencoder with a new training paradigm that learns a joint distribution and is robust to missing data. By optimizing the ELBO with multi-modal and uni-modal examples, we fully utilize the product-of-experts structure to share inference network parameters in a fashion that scales to an arbitrary number of modalities. In our experiments, the MVAE matches state-of-the-art performance across four bi-modal datasets. We also demonstrate the power of our method on two truly multi-modal datasets. In future work, we plan to explore a product of mixture-of-Gaussian posteriors, efficiently incorporate normalizing flows, and apply the MVAE to problems such as tactile-visual learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supplemental Material 1 Product of a Finite Number of Gaussians</head><p>In this section, we provide the derivation for the parameters of a product of Gaussian experts (PoE). Derivation is summarized from <ref type="bibr">[1]</ref>. Lemma 1.1. Give a finite number N of multi-dimensional Gaussian distributions p i (x) with mean µ i , covariance V i for i = 1, ..., N , the product</p><formula xml:id="formula_17">N i=1 p i (x) is itself Gaussian with mean ( N i=1 T i µ i )( N i=1 T i ) −1 and covariance ( N i=1 T i ) −1</formula><p>where</p><formula xml:id="formula_18">T i = V −1 i .</formula><p>Proof. We write the probability density of a Gaussian distribution in canonical form as K exp{η</p><formula xml:id="formula_19">T x − 1 2 x T Λx} where K is a normalizing constant, Λ = V −1 , η = V −1 µ. We then write the product of N Gaussians distributions N i=1 p i ∝ exp{( N i=1 η i ) T x− 1 2 x T ( N i=1 Λ i )x}.</formula><p>We note that this product itself has the form of a Gaussian distribution with η = N i=1 η i and Λ = N i=1 Λ i . Converting back from canonical form, we see that the product Gaussian has mean µ = (</p><formula xml:id="formula_20">N i=1 T i µ i )( N i=1 T i ) −1 and covariance V = ( N i=1 T i ) −1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Quotient of Two Gaussians</head><p>Similarly, we may derive the form of a quotient of two Gaussian distributions (QoE). Lemma 2.1. Give two multi-dimensional Gaussian distributions p(x) and q(x) with mean µ p and µ q , and covariance V p and V q respectively, the quotient</p><formula xml:id="formula_21">p(x) q(x) is itself Gaussian with mean (T p µ p − T q µ q )(T p − T q ) −1 and covariance (T p − T q ) −1 where T i (x) = V −1 i (x).</formula><p>Proof. We again write the probability density of a Gaussian distribution as K exp{η</p><formula xml:id="formula_22">T x − 1 2 x T Λx}.</formula><p>We then write the quotient of two Gaussians p and q as Kp exp{η</p><formula xml:id="formula_23">T p x− 1 2 x T Λpx} Kq exp{η T q x− 1 2 x T Λqx} ∝ exp{(η p − η q ) T x − 1 2 x T (Λ p − Λ q )x}. This defines a new Gaussian distribution with Λ = V −1 p − V −1 q and η = V −1 p µ p − V −1 q µ q . If we let T p = V −1 p and T q = V −1 q , then we see that V = Λ −1 = (T p − T q ) −1 and µ = ηV −1 = (T p µ p − T q µ q )(T p − T q ) −1 .</formula><p>The QoE suggests that the constraint</p><formula xml:id="formula_24">T p &gt; T q ⇒ V −1 p &gt; V −1 q</formula><p>must hold for the resulting Gaussian to be well-defined. In our experiments, p is usually a product of Gaussians, and q is a product of prior Gaussians (see Eqn 3 in main paper). Given N modalities, we can decompose</p><formula xml:id="formula_25">V p = N i=1 V −1 i and V q = N −1 i=1 1 = N − 1</formula><p>where the prior is a unit Gaussian with variance 1. Thus, the constraint can be rewritten as</p><formula xml:id="formula_26">N i=1 V −1 i &gt; N − 1, which is satisfied if V i &gt; N N −1 , i = 1, .</formula><p>.., N . One benefit of using the regularized importance distribution q(z|x)p(z) is to remove the need for this constraint. To fit MVAE without a universal expert, we add an additional nonlinearity to each inference network such that the variance is fed into a rescaled sigmoid: V = N N −1 · sigm(V ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Additional Results using the Joint Inference Network</head><p>In the main paper, we reported marginal probabilities using q(z|x 1 ) and showed that MVAE is state-of-the-art. Here we similarly compute marginal probabilities but using q(z|x 1 , x 2 ). Because importance sampling with either induced distribution yields an unbiased estimate, using a large number of samples should result in very similar log-likelihoods. Indeed, we find that the results do not differ much from the main paper: MVAE is still at the state-at-the-art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head><p>BinaryMNIST MNIST FashionMNIST MultiMNIST CelebA Estimating log p(x1) using q(z|x1, x2)  <ref type="table">Table 1</ref>: Similar estimates as in <ref type="table">Table 2</ref> (in main paper) but using q(z|x 1 , x 2 ) as an importance distribution (instead of q(z|x 1 )). Because VAE and CVAE do not have a multi-modal inference network, they are excluded. Again, we show that the MVAE is able to match state-of-the-art.</p><p>Variance of Marginal Log Importance Weights: var(log(  <ref type="table">Table 2</ref>: Average variance of log importance weights for marginal, joint, and conditional distributions using q(z|x 1 , x 2 ). Lower variances suggest better inference networks.</p><formula xml:id="formula_27">p(x 1 ,z) q(z|x 1 ,x 2 ) )) Model BinaryMNIST MNIST FashionMNIST</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Model Architectures</head><p>Here we specify the design of inference networks and decoders used for each dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">More on Weak Supervision</head><p>In the main paper, we showed that we do not need that many complete examples to learn a good joint distribution with two modalities. Here, we explore the robustness of our model with missing data under more modalities. Using MVAE19 (19 modalities) on CelebA, we can conduct a different weak supervision experiment: given a complete multi-modal example (x 1 , ..., x 19 ), randomly keep x i with probability p for each i = 1, ..., 19. Doing so for all examples in the training set, we simulate the effect of missing modalities beyond the bi-modal setting. Here, the number of examples shown to the model is dependent on p e.g. p = 0.5 suggests that on average, 1 out of every 2 x i are dropped. We vary p from 0.001 to 1, train from scratch, and plot (1) the prediction accuracy per attribute and (2) the various data log-likelihoods. From <ref type="figure">Figure 5</ref>, we conclude that the method is fairly robust to missing data. Even with p = 0.1, we still see accuracy close to the prediction accuracy with full data.</p><p>(a) p(y|x) (b) log p(x, y) (c) log p(x) (d) log p(x|y) <ref type="figure">Figure 5</ref>: We randomly drop input features x i with probability p. <ref type="figure">Figure (a)</ref> shows the effect of increasing p from 0.001 to 1 on the accuracy of sampling the correct attribute given an image. <ref type="figure">Figure  (b</ref>) and (c) show changes in log marginal and log conditional approximations as p increases. In all cases, we see close-to-best performance using only 10% of the complete data.</p><p>6 More of the effects of sampling more ELBO terms</p><p>In the main paper, we stated that with higher k (sampling more ELBO terms), we see a steady decrease in variance. This drop in variance can be attributed to two factors: (1) additional un-correlated randomness from sampling more when reparametrizing for each ELBO <ref type="bibr" target="#b1">[2]</ref>, or (2) additional ELBO terms to better approximate the intractable objective. <ref type="figure" target="#fig_6">Fig. 6 (c)</ref> shows that the variance still drops consistently when using a fixed ∼ N (0, 1) for computing all ELBO terms, indicating independent contributions of additional ELBO terms and additional randomness. and (c) imply that switching from k = 0 to k = 1 greatly reduces the variance in the importance distribution defined by the inference network(s).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">More on the Computer Vision Transformations</head><p>We copy <ref type="figure" target="#fig_4">Fig. 4</ref> in the main paper but show more samples and increase the size of each image for visibility. The MVAE is able to learn all 6 transformations jointly under the PoE inference network. The top row shows 8 ground truth images randomly chosen from the CelebA dataset. The second to fourth rows respectively plot the reconstructed image, edge, and facial landscape masks using the trained MVAE decoders and q(z|x 1 , ..., x 6 ).   [2] Yuri Burda, Roger Grosse, and Ruslan Salakhutdinov. Importance weighted autoencoders. arXiv preprint arXiv:1509.00519, 2015.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) Graphical model of the MVAE. Gray circles represent observed variables. (b) MVAE architecture with N modalities. E i represents the i-th inference network; µ i and σ i represent the i-th variational parameters; µ 0 and σ 0 represent the prior parameters. The product-of-experts (PoE) combines all variational parameters in a principled and efficient manner. (c) If a modality is missing during training, we drop the respective inference network. Thus, the parameters of E 1 , ..., E N are shared across different combinations of missing inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Image samples using MVAE. (a, c, e, g) show 64 images per dataset by sampling z ∼ p(z) and then generating via p(x 1 |z). Similarly, (b, d, f, h) show conditional image reconstructions by sampling z ∼ q(z|x 2 ) where (b) x 2 = 5, (d) x 2 = Ankle boot, (f) x 2 = 1773, (h) x 2 = Male.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Effects of supervision level. We plot the level of supervision as a fraction from 0 (meaning no complete examples) to 1 (meaning no missing modalities) against accuracy. For MNIST and FashionMNIST, we predict the target class. For MultiMNIST, we compute accuracy for predicting each digit correctly. For CelebA, we use the MVAE19 model and either observe all attributes or none. Since many attributes are imbalanced, (b) shows 5 attributes where prediction is not trivial.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Learning Computer Vision Transformations: (a) 4 ground truth images randomly chosen from CelebA along with reconstructed images, edges, and facial landscape masks; (b) reconstructed color images; (c) image completion via reconstruction; (d) reconstructed images with the watermark removed. See supplement for a larger version with more samples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 1 :Figure 2 :Figure 3 :Figure 4 :</head><label>1234</label><figDesc>Figure 1: MVAE architectures on MNIST: (a) q(z|x 1 ), (b) p(x 1 |z), (c) q(z|x 2 ), (d) q(x 2 |z) where x 1 specifies an image and x 2 specifies a digit label.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Effect of approximating the MVAE objective with more ELBO terms on (a) the joint log-likelihood and (b) the variance of the log importance weights over 3 independent runs. Similarly, (c) compute the variance but fixes a single ∼ N (0, 1) when reparametrizing for each ELBO. (b) and (c) imply that switching from k = 0 to k = 1 greatly reduces the variance in the importance distribution defined by the inference network(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Edge Detection and Facial Landscapes: The top row shows 8 ground truth images randomly chosen from the CelebA dataset. The second to fourth rows respectively plot the reconstructed image, edge, and facial landscape masks using the trained MVAE decoders and q(z|x 1 , ..., x 6 ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Colorization: The top row shows ground truth grayscale images. The bottom row show reconstructed color images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Fill in the Blank: The top row shows ground truth CelebA images with half of each image obscured. The bottom row replaces the obscured part with a reconstruction.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Removing Watermarks: The top row shows ground truth CelebA images, each with an added watermark. The bottom row shows the reconstructed image with the watermark removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Model BinaryMNIST MNIST FashionMNIST MultiMNIST CelebA Variance of Marginal Log Importance Weights: var(log(</figDesc><table>p(x 1 ,z) 
q(z|x 1 ) 

)) 
VAE 
22.264 
26.904 
25.795 
54.554 
56.291 
BiVCCA 
55.846 
93.885 
33.930 
185.709 
429.045 
JMVAE 
39.427 
37.479 
53.697 
84.186 
331.865 
MVAE-Q 
34.300 
37.463 
34.285 
69.099 
100.072 
MVAE 
22.181 
25.640 
20.309 
26.917 
73.923 
MVAE19 
-
-
-
-
71.640 
Variance of Joint Log Importance Weights: var(log( 

p(x 1 ,x 2 ,z) 
q(z|x 1 ) 

)) 
JMVAE 
41.003 
40.126 
56.640 
91.850 
334.887 
MVAE-Q 
34.615 
38.190 
34.908 
64.556 
101.238 
MVAE 
23.343 
27.570 
20.587 
27.989 
76.938 
MVAE19 
-
-
-
-
72.030 
Variance of Conditional Log Importance Weights: var(log( 

p(x 1 ,z|x 2 ) 
q(z|x 1 ) 

)) 
CVAE 
21.203 
22.486 
12.748 
-
56.852 
JMVAE 
23.877 
26.695 
26.658 
37.726 
81.190 
MVAE-Q 
34.719 
38.090 
34.978 
44.269 
101.223 
MVAE 
19.478 
25.899 
18.443 
16.822 
73.885 
MVAE19 
-
-
-
-
71.824 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">These results used q(z|x1) as the importance distribution. See supplement for similar results using q(z|x1, x2). Because importance sampling with either q(z|x1) or q(z|x1, x2) yields an unbiased estimator of marginal likelihood, we expect the log-likelihoods to agree asymptotically.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Generating sentences from a continuous space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Samuel R Bowman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vilnis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06349</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Opencv. Dr. Dobb&apos;s journal of software tools</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adrian</forename><surname>Kaehler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A computational approach to edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Canny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Readings in Computer Vision</title>
		<imprint>
			<publisher>Elsevier</publisher>
			<date type="published" when="1987" />
			<biblScope unit="page" from="184" to="203" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Generalized product of experts for automatic and principled fusion of gaussian process predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanshuai</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.7827</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Attend, infer, repeat: Fast scene understanding with generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Sm Ali Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theophane</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Weber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Szepesvari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3225" to="3233" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Shakir Mohamed, and Alexander Lerchner. beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irina</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loic</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arka</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Botvinick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Training products of experts by minimizing contrastive divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Geoffrey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Training</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Dlib-ml: A machine learning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>King</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="1755" to="1758" />
			<date type="published" when="2009-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iain</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Fourteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Fourteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ziwei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaogang</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiquan</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingyu</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juhan</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th international conference on machine learning (ICML-11)</title>
		<meeting>the 28th international conference on machine learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="689" to="696" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Variational methods for conditional multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaurav</forename><surname>Pandey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ambedkar</forename><surname>Dukkipati</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="308" to="315" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guim</forename><surname>Perarnau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bogdan</forename><surname>Joost Van De Weijer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><forename type="middle">M</forename><surname>Raducanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Álvarez</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06355</idno>
		<title level="m">Invertible conditional gans for image editing</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kihyuk</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Multimodal learning with deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2222" to="2230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Joint multimodal learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masahiro</forename><surname>Suzuki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kotaro</forename><surname>Nakayama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yutaka</forename><surname>Matsuo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01891</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">scikit-image: image processing in python</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Van Der Walt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><surname>Nunez-Iglesias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Boulogne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><forename type="middle">D</forename><surname>Warner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><surname>Yager</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Emmanuelle</forename><surname>Gouillart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PeerJ</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">453</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Generative models of visually grounded imagination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ramakrishna</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10762</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3156" to="3164" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiran</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Livescu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03454</idno>
		<title level="m">Deep variational canonical correlation analysis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rich Zemel, and Yoshua Bengio. Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kelvin</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">From facial parts responses to face detection: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuo</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ping</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Change</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3676" to="3684" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">From perception to conception: learning multisensory representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilker</forename><surname>Yildirim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
		<respStmt>
			<orgName>University of Rochester</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Towards deeper understanding of variational autoencoding models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengjia</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaming</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08658</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Products and convolutions of gaussian probability density functions. Tina-Vision Memo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Bromiley</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
