<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Sheng</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Ching</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Man-Hsin</forename><surname>Kao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Yu</forename><surname>Chuang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Taiwan University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Photo Enhancer: Unpaired Learning for Image Enhancement from Photographs with GANs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Photographs record valuable moments of our life. With the popularization of mobile phone cameras, users enjoy taking photographs even more. However, current cameras have limitations. They have to reconstruct a complete and high-quality image from a set incomplete and imperfect samples of the scene. The samples are often noisy, incomplete in color and limited in the resolution and the dynamic range. In addition, the camera sensor responds linearly to the incoming light while human perception performs more sophisticated non-linear mapping. Thus, users could be disappointed with photographs they take because the pho-tographs do not match their expectations and visual experience. The problem is even aggravated for mobile cameras because of their small sensors and compact lenses.</p><p>Image enhancement methods attempt to address the issues with color rendition and image sharpness. There are interactive tools and semi-automatic methods for this purpose. Most interactive software provides elementary tools such as histogram equalization, sharpening, contrast adjustment and color mapping, and some advanced functions such as local and adaptive adjustments. The quality of the results however highly depends on skills and aesthetic judgement of the users. In addition, it often takes a significant amount of time to reach satisfactory retouching results. The semiautomatic methods facilitate the process by only requiring adjustments of a few parameters. However, the results could be very sensitive to parameters. In addition, these methods are often based on some heuristic rules about human perception such as enhancing details or stretching contrast. Thus, they could be brittle and lead to bad results. This paper proposes a method for image enhancement by learning from photographs. The method only requires a set of "good" photographs as the input. They have the characteristics that the user would like to have for their photographs. They can be collected easily from websites or any stock of photographs. We treat the image enhancement problem as an image-to-image translation problem in which an input image is transformed into an enhanced image with the characteristics embedded in the set of training photographs. Thus, we tackle the problem with a two-way GAN whose structure is similar to CycleGAN <ref type="bibr" target="#b25">[26]</ref>. However, GANs are notorious for their instability. For addressing the issue and obtaining high-quality results, we propose a few improvements along the way of constructing our two-way GAN. First, for the design of the generator, we augment the U-Net <ref type="bibr" target="#b19">[20]</ref> with global features. The global features capture the notion of scene setting, global lighting condition or even subject types. They are helpful for determining what local operations should be performed. Second, we propose an adaptive weighting scheme for Wasserstein GAN (WGAN) <ref type="bibr" target="#b0">[1]</ref>. WGAN uses weight clipping for enforcing the Lipschitz constraint. It was later discovered a terrible way and some proposed to use the gradient penalty for enforcing the constraint <ref type="bibr" target="#b8">[9]</ref>. However, we found that the approach is very sensitive to the weighting parameter of the penalty. Thus, we propose to use an adaptive weighting scheme to improve the convergence of WGAN training. Finally, most two-way GAN architectures use the same generator in both forward and backward passes. It makes sense since the generators in both paths perform similar mapping with the same input and output domains. However, we found that, although in the same domain, the inputs actually come from different sources, one from the input data and the other from the generated data. The discrepancy between distributions of input sources could have vicious effects on the performance of the generator. We propose to use individual batch normalization layers for the same type of generators. This way, the generator can better adapt to the input data distribution. With these improvements, our method can provide high-quality enhanced photographs with better color rendition and sharpness. The results often look more natural than previous methods. In addition, the proposed techniques, global U-Net, adaptive WGAN and individual batch normalization, can be useful for other applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Image enhancement has been studied for a long time. Many operations and filters have been proposed to enhance details, improve contrast and adjust colors. Wang et al. <ref type="bibr" target="#b21">[22]</ref> proposed a method for enhancing details while preserving naturalness. Aubry et al. <ref type="bibr" target="#b1">[2]</ref> proposed local Laplacian operator for enhancing details. Most of these operations are algorithmic and based on heuristic rules. Bychkovsky et al. <ref type="bibr" target="#b3">[4]</ref> proposed a learning-based regression method for approximating photographers' adjustment skills. For this purpose, they collected a dataset containing images before and after adjustments by photographers.</p><p>The convolutional neural networks (CNNs) have become a major workhorse for a wide set of computer vision and image processing problems. They have also been applied to the image enhancement problem. Yan et al. <ref type="bibr" target="#b22">[23]</ref> proposed the first deep-learning-based method for photo adjustment. Gharbi et al. <ref type="bibr" target="#b6">[7]</ref> proposed a fast approximation for existing filters. Ignatov et al. <ref type="bibr" target="#b9">[10]</ref> took a different approach by learning the mapping between a mobile phone camera and a DSLR camera. They collected the DPED dataset consisting of images of the same scene taken by different cameras. A GAN model was used for learning the mapping. Chen et al. <ref type="bibr" target="#b5">[6]</ref> approximated existing filters using a fully convolutional network. It can only learn existing filters and cannot do beyond what they can do. All these methods are supervised and require paired images while ours is unpaired. The unpaired nature eases the process of collecting training data.</p><p>Our method is based on the generative adversarial networks (GANs) <ref type="bibr" target="#b7">[8]</ref>. Although GANs have been proved powerful, they are notorious on training instability. Significant efforts have been made toward stable training of GANs. Wasserstein GAN uses the earth mover distance to measure the distance between the data distribution and the model distribution and significantly improves training stability <ref type="bibr" target="#b0">[1]</ref>. <ref type="bibr">Gulrajani et al.</ref> found that WGAN could still generate low-quality samples or fail to converge due to weight clipping <ref type="bibr" target="#b8">[9]</ref>. Instead of weight clipping, they proposed to penalize the norm of the gradient of the discriminator with respect to the input. The resultant model is called WGAN-GP (WGAN with gradient penalty). It often generates higher-quality samples and converges faster than WGAN. There are also energy-based GAN variants, such as BEGAN <ref type="bibr" target="#b2">[3]</ref> and EBGAN <ref type="bibr" target="#b24">[25]</ref>.</p><p>Isola et al. proposed a conditional adversarial network as a general-purpose solution to image-to-image translation problems <ref type="bibr" target="#b12">[13]</ref>, converting from one representation of a scene to another, such as from a semantic label map to a realistic image or from a day image to its night counterpart. Although generating amazing results, their method requires paired images for training. Two-way GANs were later proposed for addressing the problem by introducing cycle consistency. Famous two-way GANs include CycleGAN <ref type="bibr" target="#b25">[26]</ref>, DualGAN <ref type="bibr" target="#b23">[24]</ref> and DISCOGAN <ref type="bibr" target="#b13">[14]</ref>. We formulate image enhancement as an instance of the image-to-image translation problems and solve it with a two-way GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Overview</head><p>Our goal is to obtain a photo enhancer Φ which takes an input image x and generates an output image Φ(x) as the enhanced version of x. It is however not easy to define enhancement clearly because human perception is complicated and subjective. Instead of formulating the problem using a set of heuristic rules such as "details should be enhanced" or "contrast should be stretched", we define enhancement by a set of examples Y . That is, we ask the user to provide a set of photographs with the characteristics he/she would like to have. The proposed method aims at discovering the common characteristics of the images in Y and deriving the enhancer so that the enhanced image Φ(x) shares these characteristics while still resembling the original image x in content.</p><p>Because of its nature with set-level supervision, the problem can be naturally formulated using the framework of GANs which learns the embedding of the input samples and generates output samples locating within the subspace spanned by training samples. A GAN model often consists of a discriminator D and a generator G. The framework has been used for addressing the image-to-image translation problem which transforms an input image from the source domain X to the output image in the target domain Y <ref type="bibr" target="#b12">[13]</ref>. In our application, the source domain X represents original images while the target domain Y contains images with the desired characteristics. <ref type="figure" target="#fig_0">Figure 1</ref>(a) gives the architecture for 1-way GAN. Given an input x ∈ X, the generator</p><formula xml:id="formula_0">G X transforms x into y ′ = G X (x) ∈ Y .</formula><p>The discriminator D Y aims at distinguishing between the samples in the target domain {y} and the generated samples {y ′ = G X (x)}. To enforce cycle consistency for better results, several have proposed 2-way GANs such as CycleGAN <ref type="bibr" target="#b25">[26]</ref> and DualGAN <ref type="bibr" target="#b23">[24]</ref>. They require that G ′ Y (G X (x)) = x where the generator G ′ Y takes a G Xgenerated sample and maps it back to the source domain X. In addition, 2-way GANs often contains a forward mapping (X → Y ) and a backward mapping (Y → X). <ref type="figure" target="#fig_0">Figure 1</ref>(b) shows the architecture of 2-way GANs. In the forward pass,</p><formula xml:id="formula_1">x G X −→ y ′ G ′ Y −→ x</formula><p>′′ and we check the consistency between x and x ′′ . In the backward pass, y</p><formula xml:id="formula_2">G Y −→ x ′ G ′ X</formula><p>−→ y ′′ and we check the consistency between y and y ′′ . In the following sections, we will first present the design of our generator (Section 4). Next, we will describe the design of our 1-way GAN (Section 5) and the one for our 2-way GAN (Section 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Generator</head><p>For our application, the generator in the GAN framework plays an important role as it will act as the final photo enhancer Φ. This section proposes a generator and compares it with several options. <ref type="figure">Figure 2(a)</ref> shows the proposed generator. The size of input images is fixed at 512×512.</p><p>Our generator is based on the U-Net <ref type="bibr" target="#b19">[20]</ref> which was originally proposed for biomedical image segmentation but later also showed strong performance on many tasks. UNet however does not perform very well on our task. Our conjecture is that the U-Net does not include global features. Our vision system usually adjusts to the overall lighting conditions and scene settings. Similarly, cameras have scene settings and often apply different types of adjustments depending on the current setting. The global features could reveal high-level information such as the scene category, the subject type or the overall lighting condition which could be useful for individual pixels to determine their local adjustments. Thus, we add the global features into the U-Net.</p><p>In order to improve the model efficiency, the extraction of global features shares the same contracting part of the U-Net with the extraction of local features for the first five layers. Each contraction step consists of 5×5 filtering with stride 2 followed by SELU activation <ref type="bibr" target="#b14">[15]</ref> and batch normalization <ref type="bibr" target="#b11">[12]</ref>. Given the 32× 32×128 feature map of the 5th layer, for global features, the feature map is further reduced to 16×16×128 and then 8×8×128 by performing the aforementioned contraction step. The 8 × 8 × 128 feature map is then reduced to 1×1×128 by a fully-connected layer followed by a SELU activation layer and then another fully-connected layer. The extracted 1×1×128 global features are then duplicated 32 × 32 copies and concatenated after the 32×32×128 feature map for the low-level features, resulting a 32×32×256 feature map which fuses both local and global features together. The expansive path of the U-Net is then performed on the fused feature map. Finally, the idea of residual learning is adopted because it has been shown effective for image processing tasks and helpful on convergence. That is, the generator only learns the difference between the input image and the label image.</p><p>Global features have been explored by other image processing tasks such as colorization <ref type="bibr" target="#b10">[11]</ref>. However, their model requires an extra supervised network trained with explicit scene labels. For many applications, it is difficult to define labels explicitly. The novelty of our model is to use the U-Net itself to encode an implicit feature vector describing global features useful for the target application. The dataset. We used the MIT-Adobe 5K dataset <ref type="bibr" target="#b3">[4]</ref> for training and testing. The dataset contains 5, 000 images, each of which was retouched by five well-trained photographers using global and local adjustments. We selected the results of photographer C as labels since he was ranked the highest in the user study <ref type="bibr" target="#b3">[4]</ref>. The dataset was split into three partitions: the first one consists of 2, 250 images and their retouched versions were used for training in the supervised setting in this section; for the unpaired training in Section 5 and Section 6, the retouched images of another 2, 250 images acted as the target domain while the 2, 250 images of the first partition were used for the source domain; the rest 500 images were used for testing in either setting. The experiments. We evaluated several network architectures for the generator. (1) DPED <ref type="bibr" target="#b9">[10]</ref>: since we only evaluate on generators, we took only the generator of their GAN architecture. (2) 8RESBLK <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b16">17]</ref>: the generator has been used in CycleGAN <ref type="bibr" target="#b25">[26]</ref> and UNIT <ref type="bibr" target="#b16">[17]</ref>. (3) FCN <ref type="bibr" target="#b5">[6]</ref>: a fully convolutional network for approximating filters. (4) CRN <ref type="bibr" target="#b4">[5]</ref>: the architecture has been used to synthesize realistic images from semantic labels. (5) U-Net <ref type="bibr" target="#b19">[20]</ref>. The residual learning is augmented to all of them. Because of the image size and the limitation on memory capacity, the  number of features of the first layer is limited at 16. Otherwise, the overall architecture cannot be fitted within the memory. The loss function is to maximize PSNR:</p><formula xml:id="formula_3">arg min G X E y,y ′ log 10 (M SE(y, y ′ )) , where<label>(1)</label></formula><p>M SE(x, y) = 1 mn</p><formula xml:id="formula_4">m−1 i=0 n−1 j=0 x(i, j) − y(i, j) 2 .</formula><p>(2) <ref type="table">Table 1</ref> shows both the average PSNR and SSIM values for all compared architectures on approximating fast local Laplacian filtering for the 500 testing images from MITAdobe 5K dataset. By adding the global features, the proposed architecture provides nearly 3dB gain over its counterpart without global features, and outperforms all compared architectures. Our generator does an excellent job on approximating the fast local Laplacian filter with 33.93dB PSNR, better than FCN which is designed for such tasks. <ref type="table" target="#tab_1">Table 2</ref> reports the performance of these architectures on predicting the retouched images. This task is much more difficult as human retouching could be more complicated and less inconsistent than algorithmic filters. Again, the proposed global U-Net architecture outperforms others.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">One-way GAN</head><p>This section presents our GAN architecture for unpaired training. <ref type="figure">Figure 2(b)</ref> illustrates the architecture of our discriminator. By employing the generator <ref type="figure">(Figure 2(a)</ref>  and the discriminator <ref type="figure">(Figure 2(b)</ref>) as D Y in <ref type="figure" target="#fig_0">Figure 1</ref>(a), we have the architecture for our 1-way GAN. As mentioned in Section 4, for training GANs, to avoid the content correlation between the source and the target domains, we used 2, 250 images of the MIT-Adobe 5K dataset as the source domain while using the retouched images of another 2, 250 images as the target domain.</p><p>There are many variants of GAN formulations. We first experimented with several flavors of GANs, including GAN <ref type="bibr" target="#b7">[8]</ref>, LSGAN <ref type="bibr" target="#b17">[18]</ref>, DRAGAN <ref type="bibr" target="#b15">[16]</ref> and WGAN-GP <ref type="bibr" target="#b8">[9]</ref>, with different parameter settings. <ref type="table" target="#tab_2">Table 3</ref> reports the results for some of them. All GANs require the parameter α for the weight of the identity loss E</p><formula xml:id="formula_5">x,y ′ M SE(x, y ′ )</formula><p>which ensures the output is similar to the input. The parameter D/G represents the ratio between the numbers of discriminator and generator training passes. In our application, WGAN-GP performs better than GAN, LSGAN and DRAGAN. <ref type="table" target="#tab_2">Table 3</ref> only reports the best performance for methods other than WGAN-GP. However, the performance of WGAN-GP depends on an additional parameter λ which weights the gradient penalty. WGAN relies on the Lipschitz constraint on the training objective: a differentiable function is 1-Lipschtiz if and only if it has gradients with norm at most 1 everywhere <ref type="bibr" target="#b8">[9]</ref>. For satisfying the constraint, WGAN-GP directly constrains the gradient norm of the discriminator output with respect to its input by adding the following gradient penalty,</p><formula xml:id="formula_6">Ê y ∇ŷD Y (ŷ) 2 − 1 2 ,<label>(3)</label></formula><p>whereŷ is point sampled along straight lines between points  in the target distribution and the generator distribution. A parameter λ weights the penalty with the original discriminator loss. λ determines the tendency that gradients approaches 1. If λ is too small, the Lipschitz constraint cannot be guaranteed. On the other hand, if λ is too large, the convergence could be slow as the penalty could over-weight the discriminator loss. It makes the choice of λ important. Instead, we use the following gradient penalty,</p><formula xml:id="formula_7">Ê y max(0, ∇ŷD Y (ŷ) 2 − 1) ,<label>(4)</label></formula><p>which better reflects the Lipschitz that requires the gradients are less than or equal to 1 and only penalizes the part which is larger than 1. More importantly, we employ an adaptive weighting scheme for adjusting the weight λ, which chooses a proper weight so that the gradients locate within a desired interval, say [1.001, 1.05]. If the moving average of gradients within a sliding window (size=50) is larger than the upper bound, it means that the current weight λ is too small and the penalty is not strong enough to ensure the Lipschitz constraint. Thus, we increase λ by doubling the weight. On the other hand, if the moving average is smaller than the lower bound, we decay λ into a half so that it will not become too large. <ref type="figure" target="#fig_2">Figure 3</ref> compares WGAN-GP and the proposed A-WGAN (Adaptive WGAN) on the swissroll dataset. It is clear that the proposed adaptive weighting WGAN converges to the target distribution regardless of the initial setting of λ while the results of WGAN-GP significantly depend on λ. Table 3 also confirms that A-WGAN outperforms WGAN-GP and achieves the best performance among compared GANs. Note that the PSNR value is not very high even with A-WGAN. It is impossible to capture the exact mapping since the photographer can be subjective with different tastes and often inconsistent. The best that a method can do is to discover the common trend on image characteristics from the training data. However, even if we cannot accurately predict human labels, the learned operation can still improve image quality as shown in Section 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Two-way GAN</head><p>Section 5 has determined to use A-WGAN as our GAN formulation. For achieving better results, this section extends the GAN architecture into a 2-way GAN. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture for the 2-way GAN. <ref type="table" target="#tab_4">Table 4</ref> compares WGAN-GP and the proposed A-WGAN when extending to the 2-way GAN architectures. Both have gains from the use of the 2-way GAN architecture. However, the gain is more significant on WGAN-GP. It could be because there is less room for A-WGAN to improve.</p><p>Most 2-way GANs use the same generator for both G X and G ′ X because both map an input from the domain X to the domain Y , and similarly for G Y and G ′ Y . However, from <ref type="figure" target="#fig_0">Figure 1(b)</ref>, the input of G X is taken from the input samples X while G ′ X takes the generated samples X ′ as its inputs. They could have different distribution characteristics. Thus, it is better that the generators G X and G ′ X adapt to their own inputs. Our solution is to use individual batch normalization (iBN) layers for G X and G ′ X . That is, our generators G X and G ′ X share all layers and parameters except for the batch normalization layers. Each generator has its own batch normalization so that it better adapts to the characteristics of its own inputs. <ref type="table" target="#tab_4">Table 4</ref> confirms that the use of individual batch normalization helps the generator adapt to input distributions. With iBN, both WGAN-GP  Comparisons of our models with DPED <ref type="bibr" target="#b9">[10]</ref>, CLHE <ref type="bibr" target="#b20">[21]</ref>, NPEA <ref type="bibr" target="#b21">[22]</ref> and FLLF <ref type="bibr" target="#b1">[2]</ref>. SL and UL respectively mean supervised and unpaired learning on the MIT-Adobe 5K dataset. The label is the retouched image by a photographer.</p><p>and A-WGAN were improved, by 0.31dB and 0.16dB respectively. As shown in <ref type="figure" target="#fig_4">Figure 4</ref> and <ref type="figure" target="#fig_6">Figure 5</ref>, without iBN, the 2-way GAN could fail to capture the right distribution on more challenging cases. To sum up, our objective consists of several losses. The first one is the identity mapping loss I which requires that the content of the transformed image y should be similar to the input image x. Because of the 2-way GAN, there is also a counterpart for mapping from y to x ′ .</p><formula xml:id="formula_8">I = E x,y ′ M SE(x, y ′ ) + E y,x ′ M SE(y, x ′ ) .<label>(5)</label></formula><p>The second loss is the cycle consistency loss C defined as</p><formula xml:id="formula_9">C = E x,x ′′ M SE(x, x ′′ ) + E y,y ′′ M SE(y, y ′′ ) .<label>(6)</label></formula><p>The adversarial losses for the discriminator A D and the generator A G are defined as</p><formula xml:id="formula_10">A D = E x D X (x) − E x ′ D X (x ′ ) + (7) E y D Y (y) − E y ′ D Y (y ′ ) ,<label>(8)</label></formula><formula xml:id="formula_11">A G = E x ′ D X (x ′ ) + E y ′ D Y (y ′ ) .<label>(9)</label></formula><p>When training the discriminator, the gradient penalty P is added where</p><formula xml:id="formula_12">P = Ê x max(0, ∇xD X (x) 2 − 1) + (10) Ê y max(0, ∇ŷD Y (ŷ) 2 − 1) .<label>(11)</label></formula><p>The term ensures 1-Lipschitz functions for Wasserstein distance. Thus, the discriminator is obtained by the following optimization,</p><formula xml:id="formula_13">arg max D A D −λP ,<label>(12)</label></formula><p>where the weightλ is adaptively adjusted using A-WGAN. The generator is obtained by</p><formula xml:id="formula_14">arg min G − A G + αI + 10αC ,<label>(13)</label></formula><p>where α weights between the adversarial loss and identity/consistency losses. Finally, we take the resultant generator G X as the photo enhancer Φ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Results</head><p>In addition to training on photographer labels of the MITAdobe 5K dataset, we have also collected an HDR dataset  for training. The images were selected from Flickr images tagged with HDR. They were ranked by interestness provided by Flickr and the top ones were further selected by users. Finally, 627 images were selected as the training set. The learned deep photo enhancer is very efficient and runs at 50fps with NVidia 1080 Ti. Although the model was trained on 512 × 512 inputs, we have extended it so that it can handle arbitrary resolutions <ref type="bibr" target="#b0">1</ref> . <ref type="figure" target="#fig_4">Figure 4</ref> compares our models with several methods including DPED <ref type="bibr" target="#b9">[10]</ref>  <ref type="figure" target="#fig_4">(Figure 4(f)</ref>), CLHE <ref type="bibr" target="#b20">[21]</ref>  <ref type="figure" target="#fig_4">(Figure 4(g)</ref>), NPEA <ref type="bibr" target="#b21">[22]</ref>  <ref type="figure" target="#fig_4">(Figure 4(h)</ref>) and FLLF <ref type="bibr" target="#b1">[2]</ref>  <ref type="figure" target="#fig_4">(Figure 4(i)</ref>). By learning from photographers using the MIT-Adobe 5K dataset, both our supervised method <ref type="figure" target="#fig_4">(Figure 4(d)</ref>) and unpaired learning method <ref type="figure" target="#fig_4">(Figure 4(e)</ref>) do a reasonable job on enhancing the input image <ref type="figure" target="#fig_4">(Figure 4(a)</ref>). <ref type="figure" target="#fig_4">Figure 4(j)</ref>) shows the photographer label of this image as a reference. The result of our method trained with the collected HDR dataset <ref type="figure" target="#fig_4">(Figure 4(b)</ref>) shows the best result among all compared methods. It looks sharp and natural with good color rendition. Using the same HDR dataset, without individual batch normalization (iBN), the color rendition is weird <ref type="figure" target="#fig_4">(Figure 4(c)</ref>). It shows the importance of the proposed iBN. <ref type="bibr" target="#b0">1</ref> The demo system is available at http://www.cmlab.csie. ntu.edu.tw/project/Deep-Photo-Enhancer/. <ref type="table" target="#tab_1">CycleGAN  -32  27  23  11  93  DPED  368  -141  119 29 657  NPEA  373  259  -142 50 824  CLHE  377  281  258  -77 993  ours  389  371  350  323  -1433  Table 5</ref>. The preference matrix from the user study. <ref type="figure" target="#fig_6">Figure 5</ref> show comparisons on another example. <ref type="figure" target="#fig_7">Figure 6</ref> show more comparisons. Again, our HDR model consistently gives the best results. Note that the inputs of the first and the last rows of <ref type="figure" target="#fig_7">Figure 6</ref> were taken by mobile phones. User study. We have performed a user study with 20 participants and 20 images using pairwise comparisons on five methods. <ref type="table">Table 5</ref> reports the preference matrix. Our model trained on HDR images ranked the first and CLHE was the runner-up. When comparing our model with CLHE, 81% of users (323 among 400) preferred our results. Limitations. Our model could amplify noise if the input is very dark and contains a significant amount of noise. In addition, since some HDR images for training are products of tone mapping, our model could suffer from halo artifacts inherited from tone mapping for some images. Other applications. This paper proposes three improve- ments: global U-Net, adaptive WGAN (A-WGAN) and individual batch normalization (iBN). They generally improve results; and for some applications, the improvement is sufficient for crossing the bar and leading to success. We have applied them to some other applications. For global UNet, we applied it to trimap segmentation for pets using the Oxford-IIIT Pet dataset <ref type="bibr" target="#b18">[19]</ref>. The accuracies of U-Net and global U-Net are 0.8759 and 0.8905 respectively. The first row of <ref type="figure">Figure 7</ref> shows an example of pet trimap segmentation. The second row of <ref type="figure">Figure 7</ref> shows the results of bedroom image synthesis. With different λ values, WGAN-GP could succeed or fail. The proposed A-WGAN is less dependent with λ and succeeded with all three λ values (only one shown here). Finally, we applied the 2-way GAN to gender change of face images. As shown in the last row of <ref type="figure">Figure 7</ref>, the 2-way GAN failed on the task but succeeded after employing the proposed iBN.</p><formula xml:id="formula_15">(f) DPED (iPhone7) (g) CLHE (h) NPEA (i) FLLF (j) label</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>CycleGAN DPED NPEA CLHE ours total</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>This paper presents Deep Photo Enhancer which learns image enhancement from a set of given photographs with user's desired characteristics. With its unpaired setting, the process of collecting training images is easy. By taking images provided by different users, the enhancer can be personalized to match individual user's preference. We have made several technical contributions while searching for the best architecture for the application. We enhance U-Net for image processing by augmenting global features. We improve the stability of WGAN by an adaptive weighting scheme. We also propose to use individual batch normalization layers for generators for improving 2-way GANs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>The network architectures of 1-way and 2-way GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>) as G X variant PSNR parameters GAN 19.65 α = 10, D/G = 1 LSGAN 20.27 α = 0.1, D/G = 1 DRAGAN 20.20 α = 10, D/G = 10 WGAN-GP 21.42 α = 1000, D/G = 50, λ = 10 WGAN-GP 21.19 α = 1000, D/G = 50, λ = 1 WGAN-GP 20.78 α = 1000, D/G = 50, λ = 0.1 A-WGAN 22.17 α = 1000, D/G = 50</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>The experiment with swissroll. The orange points rep- resent the target distribution. The green points are generated sam- ples. (a) WGAN-GP (λ = 0.1). (b) A-WGAN (λ = 0.1). (c) WGAN-GP (λ = 1). (d) A-WGAN (λ = 1). (e) WGAN-GP (λ = 10). (f) A-WGAN (λ = 10).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparisons of our models with DPED [10], CLHE [21], NPEA [22] and FLLF [2]. SL and UL respectively mean supervised and unpaired learning on the MIT-Adobe 5K dataset. The label is the retouched image by a photographer.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparisons of our models with DPED [10], CLHE [21], NPEA [22] and FLLF [2].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Comparisons of our method with DPED and CLHE. The first and the last inputs were taken by iPhone7+ and Nexus respectively. input ground truth global U-Net U-Net WGAN-GP λ=0.1 WGAN-GP λ=10 WGAN-GP λ=1000 A-WGAN λ=1000 input w/ iBN w/o iBN input w/ iBN w/o iBN Figure 7. Other applications of global U-Net, A-WGAN and iBN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>The network architectures of the proposed generator (a) and the proposed discriminator (b).</figDesc><table>3 16 32 64 

128 
128 

128 
128 

128 
256 
192 
96 
48 16 3 
3 

H, W = 512 

H/2, W/2 

H/4, W/4 
H/8, W/8 
H/16, W/16 

conv, selu, BN 
conv 
selu, BN, conv 

16 
8 
1 

global concat 
FC, selu, FC 

nn_resize 

residual 

local concat 

3 16 32 64 
128 
128 
128 

H, W = 512 

H/2, W/2 

H/4, W/4 
H/8, W/8 H/16, W/16 
H/32, W/32 

conv, lrelu, IN 

FC (to single value) 

1 

1 

(a) generator 
(b) discriminator 
Figure 2. DPED 8RESBLK FCN CRN U-Net Ours 
PSNR 25.50 
31.46 
31.52 33.52 31.06 33.93 
SSIM 0.911 
0.951 
0.952 0.972 0.960 0.976 

Table 1. The average accuracy of different network architectures 
on approximating fast local Laplacian filtering on the 500 testing 
images from the MIT-Adobe 5K dataset. 

DPED 8RESBLK FCN CRN U-Net Ours 
PSNR 21.76 
23.42 
20.66 22.38 22.13 23.80 
SSIM 0.871 
0.875 
0.849 0.877 0.879 0.900 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>The average accuracy of different network architectures on predicting the retouched images by photographers on the 500 testing images from the MIT-Adobe 5K dataset.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparisons of different GANs and parameters.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>The comparison of 1-way GAN, 2-way GAN and 2-way GAN with individual batch normalization (iBN). The numbers in parenthesis indicate the performance gains.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXivpreprintarXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast local Laplacian filters: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">BEGAN: Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning photographic global tonal adjustment with a database of input/output image pairs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bychkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Photographic image synthesis with cascaded refinement networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Fast image processing with fully-convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep bilateral learning for real-time image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gharbi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">W</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">118</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improved training of Wasserstein GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">DSLR-quality photos on mobile devices with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ignatov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kobyshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Vanhoey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Let there be color!: Joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Machine Learning (ICML)</title>
		<meeting>International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Self-normalizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mayr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">On convergence and stability of GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kodali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Abernethy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07215</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">In arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Unsupervised image-toimage translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Paul</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Cats and dogs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of CVPR IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>CVPR IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2012-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>International Conference on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contrastdependent saturation adjustment for outdoor image enhancement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Abidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Paik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JOSA A</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Naturalness preserved enhancement algorithm for non-uniform illumination images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-M</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Automatic photo adjustment using deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">11</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">DualGAN: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations (ICLR</title>
		<meeting>International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of IEEE International Conference on Computer Vision (ICCV</title>
		<meeting>IEEE International Conference on Computer Vision (ICCV</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
