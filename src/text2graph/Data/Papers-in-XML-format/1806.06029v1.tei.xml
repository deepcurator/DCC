<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Unsupervised Cross Domain Translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sagie</forename><surname>Benaim</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lior</forename><surname>Wolf</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The School of Computer Science</orgName>
								<orgName type="institution">Tel Aviv University</orgName>
								<address>
									<country key="IL">Israel</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Facebook AI Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">One-Shot Unsupervised Cross Domain Translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Given a single image x from domain A and a set of images from domain B, our task is to generate the analogous of x in B. We argue that this task could be a key AI capability that underlines the ability of cognitive agents to act in the world and present empirical evidence that the existing unsupervised domain translation methods fail on this task. Our method follows a two step process. First, a variational autoencoder for domain B is trained. Then, given the new sample x, we create a variational autoencoder for domain A by adapting the layers that are close to the image in order to directly fit x, and only indirectly adapt the other layers. Our experiments indicate that the new method does as well, when trained on one sample x, as the existing domain transfer methods, when these enjoy a multitude of training samples from domain A. Our code is made publicly available at https://github.com/sagiebenaim/OneShotTranslation.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A simplification of an intuitive paradigm for accumulating knowledge by an intelligent agent is as follows. The gained knowledge is captured by a model that retains previously seen samples and is also able to generate new samples by blending the observed ones. The agent learns continuously by being exposed to a series of objects. Whenever a new sample is observed, the agent generates, using the internal model, a virtual sample that is analogous to the observed one, and compares the observed and blended objects in order to update the internal model. This variant of the perceptual blending framework <ref type="bibr" target="#b0">[1]</ref>, requires multiple algorithmic solutions. One major challenge is a specific case of "the learning paradox", i.e., how can one learn what it does not already know, or, in the paradigm above, how can the analogous mental image be constructed if the observed sample is unseen and potentially very different than anything that was already observed.</p><p>Computationally, this generation step requires solving the task we term one-shot unsupervised cross domain translation: given a single sample x from an unknown domain A and many samples or, almost equivalently, a model of domain B, generate a sample y ∈ B that is analogous to x. While there has been a great deal of research dedicated to unsupervised domain translation, where many samples from domain A are provided, the literature does not deal, as far as we know, with the one-shot case.</p><p>To be clear, since parts of the literature may refer to these type of tasks as zero-shot learning, we are not given any training images in A except for the image to be mapped x. Consider, for example, the task posed in <ref type="bibr" target="#b1">[2]</ref> of mapping zebras to horses. The existing methods can perform this task well, given many training images of zebras and horses. However, it seems entirely possible to map a single zebra image to the analogous horse image even without seeing any other zebra image.</p><p>The method we present, called OST, uses the two domains asymmetrically and employs two steps. First, a variational autoencoder is constructed for domain B. This allows us to encode samples from domain B effectively as well as generate new samples based on random latent space vectors. In order Preprint. Work in progress.</p><p>to encourage generality, we further augment B with samples produced by a slight rotation and with a random horizontal translation.</p><p>In the second phase, the variational autoencoder is cloned to create two copies that share the top layers of the encoders and the bottom layers of the decoders, one for the samples in B and one for the sample x in A. The autoencoders are trained with reconstruction losses as well as with a single-sample one-way circularity-loss. The samples from domain B continue to train its own copy as in the first step, updating both the shared and the unshared layers. The gradient from sample x updates only the unshared layers and not the shared layers. This way, the autoencoder of B is adjusted by x through the loss incurred on unshared layers for domain B by the circularity loss, and through subsequent adaptation of the shared layers to fit the samples of B. This allows the shared layers to gradually adapt to the new sample x, but prevents overfitting on this single sample. Augmentation is applied, as before, to B and also to x for added stability.</p><p>We perform a wide variety of experiments and demonstrate that OST outperforms the existing algorithms in the low-shot scenario. On most datasets the method also presents a comparable accuracy with a single training example to the accuracy obtained by the other methods for the entire set of domain A images. This success sheds new light on the potential mechanisms that underlie unsupervised domain translation, since in the one-shot case, constraints on the inter-sample correlations in domain A do not apply.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Previous Work</head><p>Unsupervised domain translation methods receive two sets of samples, one from each domain, and learn a function that maps between a sample in one domain and the analogous sample in the other domain <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>. Such methods are unsupervised in the sense that the two sets are completely unpaired.</p><p>The mapping between the domains can be recovered based on multiple cues. First, shared objects between domains can be serve as supervised samples. This is the case in the early unsupervised cross-lingual dictionary translation methods <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b15">16]</ref>, which identified international words ('computer', 'computadora','kompüter') or other words with a shared etymology by considering inter-language edit distances. These words were used as a seed set to bootstrap the mapping process.</p><p>A second cue is that of object relations. It often holds that the pairwise similarities between objects in domain A are preserved after the transformation to domain B. This was exploited in <ref type="bibr" target="#b4">[5]</ref> using the L2 distances between classes. In the work on unsupervised word to word translation <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, the relations between words in each language are encoded by word vectors <ref type="bibr" target="#b16">[17]</ref>, and translation is well approximated by a linear transformation of one language's vectors to those of the second.</p><p>A third cue is that of inner object relations. If the objects of domain A are complex and contain multiple parts, then one can expect that after mapping, the counterpart in domain B would have a similar arrangement of parts. This was demonstrated by examining the distance between halves of images in <ref type="bibr" target="#b4">[5]</ref> and it also underlies unsupervised NLP translation methods that can translate a sentence in one language to a sentence in another, after observing unmatched corpora <ref type="bibr" target="#b11">[12]</ref>.</p><p>Another way to capture these inner-object relations is by constructing separate autoencoders for the two domains, which share many of the weights <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. It is assumed that the low-level image properties, such as texture and color, are domain-specific, and that the mid-and top-level properties are common to both domains.</p><p>The third cue is also manifested implicitly (in both autoencoder architectures and in other methods) by the structure of the neural network used to perform the cross-domain mapping <ref type="bibr" target="#b17">[18]</ref>. The network's capacity constrains the space of possible solutions and the relatively shallow networks used, and their architecture dictate the form of a solution. Taken together with the GAN <ref type="bibr" target="#b18">[19]</ref> constraints that ensure that the generated images are from the target domain, and restricted further by the circularity constraint <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, much of the ambiguity in mapping is eliminated.</p><p>In the context of one-shot translation, it is not possible to find or to generate analogs in B to the given x ∈ A, since the domain-invariant distance between the two domains is not defined. One can try to use general purpose distances such as the perceptual distance, but this would make the work </p><note type="other">)</note><p>. These shared parts, marked with a snowflake, are frozen with respect to the sample x. For both phase I and phase II, we train a discriminator D B to ensure that the generated image belong to the distribution of domain B. P (x) and P (Λ) are translated to a common feature space, C E , using E U A and E U B respectively. C (resp C G ) is the space of features, constructed after passing C E (resp C) through the common encoder E S (resp common decoder G S ). R AB denotes the subspace of samples in B constructed from P (x), which is generated by augmenting x. R AA denotes the space of reconstructed samples from P (x). R ABA denotes the subspace of samples in A constructed by translating P (x) to domain B and then back to A.</p><p>semi-supervised such as <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> (these methods are also not one-shot). Since there are no inter-object relations in domain A, the only cue that can be used is of the third type.</p><p>We have made attempts to compare various image parts within x, thereby generalizing the imagehalves solution of <ref type="bibr" target="#b4">[5]</ref>. However, this did not work. Instead, our work relies on the assumption that the mid-level representation of domain A is similar to that of B, which, as mentioned above, is the underlying assumption in autoencoder based cross-domain translation work <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">One-Shot Translation</head><p>In the problem of unsupervised cross-domain translation, the learning algorithm is provided with unlabeled datasets from two domains, A and B. The goal is to learn a function T , which maps samples in domain A to analog samples in domain B. In the autoencoder based mapping technique <ref type="bibr" target="#b6">[7]</ref>, two encoders/decoders are learned. We denote the encoder for domain A (B) by E A (E B ) and the decoder by G A (G B ). In order to translate a sample x in domain A to domain B, one employs the encoder of A and the decoder of B, i.e., T AB = G B • E A . A strong constraint on the form of the translation is given by sharing layers between the two autoencoders. The lower layers of the encoder and the top layers of the decoder are domain-specific and unshared. The encoder's top layers and decoder's bottom layers are shared. This sharing enforces the same structure on the encoding of both domains and is crucial for the success of the translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Specifically, we write E</head><formula xml:id="formula_0">A = E S • E U A , E B = E S • E U B , G A = G U A • G S , and G B = G U B • G S</formula><p>, where the superscripts S and U denote shared and unshared parts, respectively, and the subscripts denote the domain. This structure is depicted in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In addition to the networks that participate in the two autoencoders, an adversarial discriminator D B is trained in both phases, in order to model domain B. Domain A does not contain enough real examples in the case of low-shot learning and, in addition, a domain A discriminator is less needed since the task is to map from A to B. When mapping x, after augmentation, to B using the transformation T , the discriminator D B is used to provide an adversarial signal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Phase One of Training</head><p>In the first phase, we employ a training set Λ of images from domain B and train a variational autoencoder for this domain. The method employs an augmentation operator that consists of small random rotations of the image and an horizontal translation. We denote by P (Λ) the training set constructed by randomly augmenting every sample s ∈ Λ.</p><p>The following losses are used:</p><formula xml:id="formula_1">L RECB = s∈P (Λ) G B (E B (s)) − s 1 (1) L V AEB = s∈P (Λ) KL(E B • P (Λ)||N (0, I))<label>(2)</label></formula><formula xml:id="formula_2">L GANB = s∈P (Λ) − (D B (G B (E B (s))), 0)<label>(3)</label></formula><formula xml:id="formula_3">L DB = s∈P (Λ) + (D B (G B (E B (s))), 0) + (D B (s), 1)<label>(4)</label></formula><p>where the first three losses are the reconstruction loss, the variational loss and the adversarial loss on the generator, respectively, and the fourth loss is the loss of the GAN's discriminator, in which we use the bar to indicate that G B is not updated during the backpropagation of this loss. can be the binary cross entropy or the least square loss, (x, y) = (x − y) 2 <ref type="bibr" target="#b21">[22]</ref>. When training E B and G B in the first phase, the following loss is minimized:</p><formula xml:id="formula_4">L I = L RECB + α 1 L V AEB + α 2 L GAN<label>(5)</label></formula><p>where α i are tradeoff parameters. At the same time we train D B to minimize L DB . Similarly to CycleGAN, D B can be a PatchGAN <ref type="bibr" target="#b22">[23]</ref> discriminator, which checks if 70 × 70 overlapping patches of the image are real or fake.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Phase Two of Training</head><p>In the second phase, we make use of the sample x from domain A, as well as the set Λ. In case we are given more than one sample from domain A, we simply add the loss terms to each one of the samples.</p><p>Denote by P (x) the set of random augmentations of x and the cross-domain encoding/decoding as:</p><formula xml:id="formula_5">T BB =G U B (G S (E S (E U B (x))))<label>(6)</label></formula><formula xml:id="formula_6">T BA =G U A (G S (E S (E U B (x))))<label>(7)</label></formula><formula xml:id="formula_7">T AA =G U A (G S (E S (E U A (x))))<label>(8)</label></formula><formula xml:id="formula_8">T AB =G U B (G S (E S (E U A (x))))<label>(9)</label></formula><p>where the bar is used, as before, to indicate a detached clone not updated during backpropagation.</p><p>The following additional losses are used:</p><formula xml:id="formula_9">L RECA = s∈P (x) T AA (s) − s 1 (10) L cycle = s∈P (x) T BA (T AB (s)) − s 1 (11) L GANAB = s∈P (x) − (D B (T AB (s)), 0)<label>(12)</label></formula><formula xml:id="formula_10">L DAB = s∈P (x) + (D B (T AB (s)), 0) + (D B (s), 1)<label>(13)</label></formula><p>namely, the reconstruction loss on x, a one-way cycle loss applied to x, and the generator and discriminator losses for domain B given the source sample x. In phase II we minimize the following loss: Note that G S and E S enforce the same structure on x as it does on samples from domain B. Enforcing this is crucial in making x and T AB (x) structurally aligned, as these layers typically encode structure common to both domains A and B <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref>. OST assumes that it is sufficient to train a VAE for domain B only, in order for G </p><formula xml:id="formula_11">L II = L I + α 3 L RECA + α 4 L cycle + α 5 L GAN_AB<label>(14)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Network Architecture and Implementation</head><p>We consider x ∈ A and samples in B to be images in R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3×256×256</head><p>. We compare our results to state of the art method, CycleGAN <ref type="bibr" target="#b1">[2]</ref> and UNIT <ref type="bibr" target="#b6">[7]</ref> and use the architecture of CycleGAN, shown to be highly successful for a variety of datasets, for the encoders, decoders and discriminator. For a fair comparison, the same architecture is used when comparing OST to the CycleGAN and UNIT baselines. The network architecture released with UNIT did not outperform the combination of the UNIT losses and the CycleGAN architecture for the datasets that are used in our experiments.</p><p>Both the shared and unshared encoders (resp. decoders) consist of between 1 and 2 2-stride convolutions (resp. deconvolutions). The shared encoder consists of between 1 and 3 residual blocks after the convolutional layers. The shared decoder also consists of between 1 and 3 residual blocks before its deconvolutional layers. The number of layers is selected to obtain the optimal CycleGAN results and is used for all architectures. Batch normalization and ReLU activations are used between layers.</p><p>CycleGAN employs a number of additional techniques to stabilize training, which OST borrows. The first is the use of a PatchGAN discriminator <ref type="bibr" target="#b22">[23]</ref>, and the second is the use of least-square loss for the discriminator <ref type="bibr" target="#b21">[22]</ref> instead of negative log-likelihood loss. For the MNIST <ref type="bibr" target="#b23">[24]</ref> to SVHN <ref type="bibr" target="#b24">[25]</ref> translation and the reverse translation, the PatchGAN discriminator is not used, and, for these experiments, where the input is in R</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3×32×32</head><p>, the standard DCGAN <ref type="bibr" target="#b25">[26]</ref> architecture is used.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We compare OST, trained on a single sample x ∈ A, to both UNIT and CycleGAN trained either on x alone or with the entire training set of images from A. We conduct a number of quantitative evaluations, including style and content loss comparison as well as a classification accuracy test for target images. For the MNIST to SVHN translation and the reverse, we conduct an ablation study, showing the importance of every component of our approach. For this task, we further evaluate our approach, when more samples are presented, showing that OST is able to perform well on larger training sets. In all cases x is sampled from the training set of the other methods. The experiments are repeated multiple times and the mean results are reported.</p><p>MNIST to SVHN Translation Using OST, we translated a randomly selected MNIST <ref type="bibr" target="#b23">[24]</ref> image to an Street View House Number (SVHN) <ref type="bibr" target="#b24">[25]</ref> image. We used a pretrained-classifier for SVHN, to predict a label for the translated image and compared it to the input MNIST image label. <ref type="figure">Fig. 2(a)</ref> shows the accuracy of the translation for increasing number of samples in A. The same random selection of images was used for baseline comparison, and that accuracy is measured on the train images translated from A to B, and not on a separate test set. The reverse translation experiment was also conducted and shown in <ref type="figure">Fig. 2(b)</ref>. While increasing the number of samples, increases the accuracy, OST outperforms the baselines even when trained on the entire training set.</p><p>In a second experiment, an ablation study is conducted. We consider our method where any of the following are left out: first, augmentation on both the input image x ∈ A and on images from B. Second, one way cycle loss, L cycle . Third, selective back propagation is lifted, and gradients from losses of L  <ref type="figure">Figure 2</ref>: (a) Translating MNIST images to SVHN images. x-axis is the number of samples in A (log-scale), y-axis is the accuracy of a pretrained classifier on the resulting translated images. The accuracy is averaged over 1000 independent runs for different samples. Blue: Our OST method. Yellow: UNIT <ref type="bibr" target="#b6">[7]</ref>. Red: CycleGAN <ref type="bibr" target="#b1">[2]</ref> . (b) The same graph in the reverse direction. in Tab. 1. We find that selective back propagation has the largest effect on translation accuracy. One-way cycle loss and augmentation contribute less to the one-shot performance.</p><p>In another experiment, we completely freezed the shared encoder and decoder in phase II. In this case, the mapping fails to produce images in the target distribution. In the SVHN to MNIST translation, for instance, the background color of the translated images is gray and not black.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Style Transfer Tasks</head><p>We consider the tasks of two-way translation from Images to Monet-style painting <ref type="bibr" target="#b1">[2]</ref>, Summer to Winter translation <ref type="bibr" target="#b1">[2]</ref> and the reverse translations. To asses the quality of these translations, we measure the perceptual distance <ref type="bibr" target="#b26">[27]</ref> between input and translated images. This supervised distance is minimized in style transfer tasks to preserve the translation's content, and so a low value indicates that much of the content is preserved. Further, we compute the style difference between translated images and target domain images, as introduced in <ref type="bibr" target="#b26">[27]</ref>. Tab. 2 shows that OST captures the target style in a similar manner to UNIT and CycleGAN when trained many samples, as well as CycleGAN trained with a single sample. While the latter captures the style of the target domain, it is unable to preserve the content, as indicated by the high perceptual distance. Sample results obtained with OST are shown in <ref type="figure">Fig. 3</ref> and in <ref type="figure">Figures 7 and 8</ref>.</p><p>Drawing Tasks We consider the translation of Google Maps to Aerial View photos <ref type="bibr" target="#b22">[23]</ref>, Facades to Images <ref type="bibr" target="#b27">[28]</ref>, Cityscapes to Labels <ref type="bibr" target="#b28">[29]</ref> and the reverse translations. Sample results are show in <ref type="figure">Fig. 3</ref> and in <ref type="bibr">Figures 4,</ref><ref type="bibr">5 and 6</ref>. OST trained on a single sample, as well as CycleGAN and UNIT trained on the entire training set obtain aligned mappings, while CycleGAN and UNIT trained on a single sample, either failed to produce samples from the target distribution or failed to create an  aligned mapping. Tab. 3 shows that OST achieves a similar perceptual distance and style difference to CycleGAN and UNIT trained on the entire training set. This indicates that OST achieves a similar content similarity to the input image, and style difference to the target domain, as these methods. To further validate this, we asked 20 persons to rate whether the source image matches the target image (presenting the methods and samples in a random order) and list in Tab. 3 the ratio of "yes" answers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head><p>Being a one-shot technique, the method we present is suitable for agents that survey the environment and encounter images from unseen domains. In phase II, the autoencoder of domain B changes in order to adapt to domain A. This is desirable in the context of "life long" unsupervised learning, where new domains are to be encountered sequentially. However, phase II is geared toward the success of translating x, and in the context of multi-one-shot domain adaptations, a more conservative approach would be required.</p><p>In this work, we translate one sample from a previously unseen domain A to domain B. An interesting question is the ability of mapping from a domain in which many samples have been seen to a new domain, from which a single training sample is given. An analog two phase approach can be attempted, in which an autoencoder is trained on the source domain, replicated, and tuned selectively on the target domain. The added difficulty in this other direction is that adversarial training cannot be employed directly on the target domain, since only one sample of it is seen. It is possible that one can still model this domain based on the variability that exists in the familiar source domain. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Illustration of the two phases of training. (Phase I): Augmented samples from domain B, P (Λ), are used to train a variational autoencoder for domain B. R BB denotes the space of reconstructed samples from P (Λ). (Phase II): the variational autoencoder of phase I is cloned, while sharing the weights of part of the encoder (E S ) and part of the decoder (G S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>4 where</head><label>4</label><figDesc>α i are tradeoff parameters. Losses not in L I are minimized over the unshared layers of the encoders and decoders. At the same time we train D B to minimize L DB and L DAB .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the features needed to represent x and its aligned counterpart T AB (x). Give this assumption, it does not rely on samples from A to train Gduring backpropagation not just from the VAE's reconstruction loss in domain A but also from the Cycle and the GAN_AB losses in L II . As our experiments show, it is important to adapt these shared parts to x. This happens indirectly: during training the unshared layers of E U B and G U B are updated via the one-shot cycle loss (Eq. 11). Due to this change, all three loss terms in L I are expected to increase and G S and E S are adapted to rectify this.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>II are passed through shared encoders and decoders, E s and G s . The results are</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :Figure 4 :Figure 5 :Figure 6 :Figure 7 :Figure 8 :</head><label>345678</label><figDesc>Figure 3: Translation for various tasks using OST (1 Sample), CycleGAN and UNIT (1 and Many Samples) 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Ablation study for the MNIST to SVHN translation (and vice versa). We consider the contribution of various parts of our method on the accuracy. Translation is done for one sample.</figDesc><table>Augment-One-way 
Selective 
Accuracy 
Accuracy 
ation 
cycle 
backprop 
(MNIST to SVHN) (SVHN to MNIST) 

False 
False 
False 
0.07 
0.10 
True 
False 
False 
0.11 
0.11 
False 
True 
False 
0.13 
0.13 
True 
True 
False 
0.14 
0.14 
False 
False 
True 
0.19 
0.20 
True 
False 
True 
0.20 
0.20 
False 
True 
True 
0.22 
0.23 

True 
True 
No Phase II update 
0.16 
0.15 
of E 

S 

and G 

S 

True 
True 
True 
0.23 
0.23 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>(i) Measuring the perceptual distance [27], between inputs and their corresponding output 
images of different style transfer tasks. Low perceptual loss indicates that much of the high-level 
content is preserved in the translation. (ii) Measuring the style difference between translated images 
and images from the target domain. We compute the average Gram matrix of translated images and 
images from the target domain and find the average distance between them, as described in [27]. 

Component Dataset 
OST UNIT [7] CycleGAN [2] UNIT [7] CycleGAN [2] 
Samples in A 
1 
1 
1 
All 
All 

(i) Content 
Summer2Winter 0.64 
3.20 
3.53 
1.41 
0.41 
Winter2Summer 0.73 
3.10 
3.48 
1.38 
0.40 
Monet2Photo 
3.75 
6.82 
5.80 
1.46 
1.41 
Photo2Monet 
1.47 
2.92 
2.98 
2.01 
1.46 

(ii) Style 
Summer2Winter 1.64 
6.51 
1.62 
1.69 
1.69 
Winter2Summer 1.58 
6.80 
1.31 
1.69 
1.66 
Monet2Photo 
1.20 
6.83 
0.90 
1.21 
1.18 
Photo2Monet 
1.95 
7.53 
1.91 
2.12 
1.88 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>(i) Perceptual distance [27] between the inputs and corresponding output images, for various drawing tasks. (ii) Style difference between translated images and images from the target domain. (iii) Correctness of translation as evaluated by a user study.</figDesc><table>Method 
Images to 
Facades 
Images 
Maps to 
Labels to 
Cityscapes 
Facades 
to Images To Maps Images Cityscapes 
to Labels 

(i) OST 1 
4.76 
5.05 
2.49 
2.36 
3.34 
2.39 
UNIT [7] All 
3.85 
4.80 
2.42 
2.30 
2.61 
2.18 
CycleGAN [2] All 
3.79 
4.49 
2.49 
2.11 
2.73 
2.28 

(ii) OST 1 
3.57 
7.88 
2.24 
1.50 
0.67 
1.13 
UNIT [7] All 
3.92 
7.42 
2.56 
1.59 
0.69 
1.21 
CycleGAN [2] All 
3.81 
7.03 
2.33 
1.30 
0.77 
1.22 

(iii) OST 1 
91% 
90% 
83% 
67% 
66% 
56% 
UNIT [7] ALL 
86% 
83% 
81% 
75% 
63% 
37% 
CycleGAN [2] ALL 93% 
84% 
97% 
81% 
72% 
45% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This project has received funding from the European Research Council (ERC) under the European Union's Horizon 2020 research and innovation programme (grant ERC CoG 725974).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">The Way We Think: Conceptual Blending and the Mind&apos;s Hidden Complexities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Fauconnier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Basic Books</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycleconsistent adversarial networkss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">DualGAN: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02510</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">One-sided unsupervised domain mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised image-to-image translation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Breuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choo</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09020</idno>
		<title level="m">Stargan: Unified generative adversarial networks for multi-domain image-to-image translation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.04087</idno>
		<title level="m">Word translation without parallel data</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Adversarial training for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics</title>
		<meeting>the 55th Annual Meeting of the Association for Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1959" to="1970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Earth mover&apos;s distance minimization for unsupervised bilingual lexicon induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing</title>
		<meeting>the 2017 Conference on Empirical Methods in Natural Language Processing</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1934" to="1945" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Unsupervised machine translation using monolingual corpora only</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Conneau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Denoyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">Y</forename><surname>Yee</surname></persName>
		</author>
		<title level="m">Proceedings of the 17th international conference on Computational linguistics</title>
		<meeting>the 17th international conference on Computational linguistics</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="1998" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="414" to="420" />
		</imprint>
	</monogr>
	<note>An IR approach for translating new words from nonparallel, comparable texts</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Automatic identification of word translations from unrelated english and german corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rapp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</title>
		<meeting>the 37th annual meeting of the Association for Computational Linguistics on Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inducing translation lexicons via diverse similarity measures and bridge languages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schafer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yarowsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">proceedings of the 6th conference on Natural language learning</title>
		<meeting>the 6th conference on Natural language learning</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning a translation lexicon from monolingual corpora</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Knight</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACL-02 workshop on Unsupervised lexical acquisition</title>
		<meeting>the ACL-02 workshop on Unsupervised lexical acquisition</meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2002" />
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The role of minimal complexity functions in unsupervised learning of semantic mappings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Galanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Benaim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Generative adversarial nets. In: NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised cross-domain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">NAM -unsupervised cross-domain image mapping without cycles or GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hoshen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR workshop</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04076</idno>
		<title level="m">Multi-class generative adversarial networks with the l2 loss function</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<title level="m">MNIST handwritten digit database</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<title level="m">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Spatial pattern templates for recognition of objects with regular structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tyleček</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Šára</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
