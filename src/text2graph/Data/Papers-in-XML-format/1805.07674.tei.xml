<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:43+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">BourGAN: Generative Networks with Metric Embeddings</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Xiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peilin</forename><surname>Zhong</surname></persName>
							<email>peilin@cs.columbia.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changxi</forename><surname>Zheng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Columbia University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">BourGAN: Generative Networks with Metric Embeddings</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper addresses the mode collapse for generative adversarial networks (GANs). We view modes as a geometric structure of data distribution in a metric space. Under this geometric lens, we embed subsamples of the dataset from an arbitrary metric space into the 2 space, while preserving their pairwise distance distribution. Not only does this metric embedding determine the dimensionality of the latent space automatically, it also enables us to construct a mixture of Gaussians to draw latent space random vectors. We use the Gaussian mixture model in tandem with a simple augmentation of the objective function to train GANs. Every major step of our method is supported by theoretical analysis, and our experiments on real and synthetic data confirm that the generator is able to produce samples spreading over most of the modes while avoiding unwanted samples, outperforming several recent GAN variants on a number of metrics and offering new features.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In unsupervised learning, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> is by far one of the most widely used methods for training deep generative models. However, difficulties of optimizing GANs have also been well observed <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b7">8]</ref>. One of the most prominent issues is mode collapse, a phenomenon in which a GAN, after learning from a data distribution of multiple modes, generates samples landed only in a subset of the modes. In other words, the generated samples lack the diversity as shown in the real dataset, yielding a much lower entropy distribution.</p><p>We approach this challenge by questioning two fundamental properties of GANs. i) We question the commonly used multivariate Gaussian that generates random vectors for the generator network. We show that in the presence of separated modes, drawing random vectors from a single Gaussian may lead to arbitrarily large gradients of the generator, and a better choice is by using a mixture of Gaussians. ii) We consider the geometric interpretation of modes, and argue that the modes of a data distribution should be viewed under a specific distance metric of data items -different metrics may lead to different distributions of modes, and a proper metric can result in interpretable modes. From this vantage point, we address the problem of mode collapse in a general metric space. To our knowledge, despite the recent attempts of addressing mode collapse <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12]</ref>, both properties remain unexamined. Technical contributions. We introduce BourGAN, an enhancement of GANs to avoid mode collapse in any metric space. In stark contrast to all existing mode collapse solutions, BourGAN draws random vectors from a Gaussian mixture in a low-dimensional latent space. The Gaussian mixture is constructed to mirror the mode structure of the provided dataset under a given distance metric. We derive the construction algorithm from metric embedding theory, namely the Bourgain Theorem <ref type="bibr" target="#b12">[13]</ref>. Not only is using metric embeddings theoretically sound (as we will show), it also brings significant advantages in practice. Metric embeddings enable us to retain the mode structure in the 2 latent space despite the metric used to measure modes in the dataset. In turn, the Gaussian mixture sampling in the We train a generator G that maps a latent-space distribution Z to the data distribution X with two modes. (a) Suppose Z is a Gaussian, and G can fit both modes. If we draw two i.i.d. samples z 1 , z 2 from Z, then with at least a constant probability, G(z 1 ) is close to the center x 1 of the first mode, and G(z 2 ) is close to another center x 2 . By the Mean Value Theorem, there exists a z between z 1 and z 2 that has the absolute gradient value, |G (z)| = | x2−x1 z2−z1 |, which can be arbitrarily large, as |x 2 − x 1 | can be arbitrarily far. (b) Since G is Lipschitz continuous, using it to map a Gaussian distribution to both modes unavoidably results in unwanted samples between the modes (highlighted by the red dots). (c) Both challenges are resolved if we can construct a mixture of Gaussian in latent space that captures the same modal structure as in the data distribution. latent space eases the optimization of GANs, and unlike existing GANs that assume a user-specified dimensionality of the latent space, our method automatically decides the dimensionality of the latent space from the provided dataset.</p><p>To exploit the constructed Gaussian mixture for addressing mode collapse, we propose a simple extension to the GAN objective that encourages the pairwise 2 distance of latent-space random vectors to match the distance of the generated data samples in the metric space. That is, the geometric structure of the Gaussian mixture is respected in the generated samples. Through a series of (nontrivial) theoretical analyses, we show that if BourGAN is fully optimized, the logarithmic pairwise distance distribution of its generated samples closely match the logarithmic pairwise distance distribution of the real data items. In practice, this implies that mode collapse is averted.</p><p>We demonstrate the efficacy of our method on both synthetic and real datasets. We show that our method outperforms several recent GAN variants in terms of generated data diversity. In particular, our method is robust to handle data distributions with multiple separated modes -challenging situations where all existing GANs that we have experimented with produce unwanted samples (ones that are not in any modes), whereas our method is able to generate samples spreading over all modes while avoiding unwanted samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>GANs and variants. The main goal of generative models in unsupervised learning is to produce samples that follow an unknown distribution X , by learning from a set of unlabelled data items {x i } n i=1 drawn from X . In recent years, Generative Adversarial Networks (GANs) <ref type="bibr" target="#b0">[1]</ref> have attracted tremendous attention for training generative models. A GAN uses a neural network, called generator G, to map a low-dimensional latent-space vector z ∈ R d , drawn from a standard distribution Z (e.g., a Gaussian or uniform distribution), to generate data items in a space of interest such as natural images and text. The generator G is trained in tandem with another neural network, called the discriminator D, by solving a minmax optimization with the following objective.</p><formula xml:id="formula_0">L gan (G, D) = E x∼X [log D(x)] + E z∼Z [log(1 − D(G(z)))] .<label>(1)</label></formula><p>This objective is minimized over G and maximized over D. Initially, GANs are demonstrated to generate locally appreciable but globally incoherent images. Since then, they have been actively improving. For example, DCGAN <ref type="bibr" target="#b7">[8]</ref> proposes a class of empirically designed network architectures that improve the naturalness of generated images. By extending the objective (1), InfoGAN <ref type="bibr" target="#b13">[14]</ref> is able to learn interpretable representations in latent space, Conditional GAN <ref type="bibr" target="#b14">[15]</ref> can produce more realistic results by using additional supervised label. Several later variants have applied GANs to a wide array of tasks <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b16">17]</ref> such as image-style transfer <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b18">19]</ref>, super-resolution <ref type="bibr" target="#b19">[20]</ref>, image manipulation <ref type="bibr" target="#b20">[21]</ref>, video synthesis <ref type="bibr" target="#b21">[22]</ref>, and 3D-shape synthesis <ref type="bibr" target="#b22">[23]</ref>, to name a few.</p><p>technique to stabilize the training process, and another line of work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b28">29]</ref> uses an additional network that maps generated samples back to latent vectors to provide feedback to the generator.</p><p>A notable problem of GANs is mode collapse, which is the focus of this work. For instance, when trained on ten hand-written digits (using MNIST dataset) <ref type="bibr" target="#b29">[30]</ref>, each digit represents a mode of data distribution, but the generator often fails to produce a full set of the digits <ref type="bibr" target="#b24">[25]</ref>. Several approaches have been proposed to mitigate mode collapse, by modifying either the objective function <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref> or the network architectures <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b30">31]</ref>. While these methods are evaluated empirically, theoretical understanding of why and to what extent these methods work is often lacking. More recently, PacGAN <ref type="bibr" target="#b10">[11]</ref> introduces a mathematical definition of mode collapse, which they used to formally analyze their GAN variant. Very few previous works consider the construction of latent space: VAE-GAN <ref type="bibr" target="#b28">[29]</ref> constructs the latent space using variational autoencoder, and GLO <ref type="bibr" target="#b31">[32]</ref> tries to optimize both the generator network and latent-space representation using data samples. Yet, all these methods still draw the latent random vectors from a multivariate Gaussian.</p><p>Differences from prior methods. Our approach differs from prior methods in several important technical aspects. Instead of using a standard Gaussian to sample latent space, we propose to use a Gaussian mixture model constructed using metric embeddings (e.g., see <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b34">35]</ref> for metric embeddings in both theoretical and machine learning fronts). Unlike all previous methods that require the latent-space dimensionality to be specified a priori, our algorithm automatically determines its dimensionality from the real dataset. Moreover, our method is able to incorporate any distance metric, allowing the flexibility of using proper metrics for learning interpretable modes. In addition to empirical validation, the steps of our method are grounded by theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Bourgain Generative Networks</head><p>We now introduce the algorithmic details of BourGAN, starting by describing the rationale behind the proposed method. The theoretical understanding of our method will be presented in the next section.</p><p>Rationale and overview. We view modes in a dataset as a geometric structure embodied under a specific distance metric. For example, in the widely tested MNIST dataset, only two modes emerge under the pixel-wise 2 distance <ref type="figure" target="#fig_1">(Fig. 2-left)</ref>: images for the digit "1" are clustered in one mode, while all other digits are landed in another mode. In contrast, under the classifier distance metric (defined in Appx. F.3), it appears that there exist 10 modes each corresponding to a different digit. Consequently, the modes are interpretable ( <ref type="figure" target="#fig_1">Fig. 2-right)</ref>. In this work, we aim to incorporate any distance metric when addressing mode collapse, leaving the flexibility of choosing a specific metric to the user.</p><p>When there are multiple separated modes in a data distribution, mapping a Gaussian random variable in latent space to the data distribution is fundamentally ill-posed. For example, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>-a and 1-b, this mapping imposes arbitrarily large gradients (at some latent space locations) in the generator network, and large gradients render the generator unstable to train, as pointed out by <ref type="bibr" target="#b36">[37]</ref>.</p><p>A natural choice is to use a mixture of Gaussians. As long as the Gaussian mixture is able to mirror the mode structure of the given dataset, the problem of mapping it to the data distribution becomes well-posed ( <ref type="figure" target="#fig_0">Fig. 1-c)</ref>. To this end, our main idea is to use metric embeddings, one that map data items under any metric to a low-dimensional 2 space with bounded pairwise distance distortion (Sec. 3.3).</p><p>After the embedding, we construct a Gaussian mixture in the 2 space, regardless of the distance metric for the data items. In this process, the dimensionality of the latent space is also automatically decided.</p><p>Our embedding algorithm, building upon the Bourgain Theorem, requires us to compute the pairwise distances of data items, resulting in an O(n 2 ) complexity, where n is the number of data items. When n is large, we first uniformly subsample m data items from the dataset to reduce the computational cost of our metric embedding algorithm (Sec. 3.2). The subsampling step is theoretically sound: we prove that when m is sufficiently large yet still much smaller than n, the geometric structure (i.e., the pairwise distance distribution) of data items is preserved in the subsamples.</p><p>Lastly, when training a BourGAN, we encourage the geometric structure embodied in the latent-space Gaussian mixture to be preserved by the generator network. Thereby, the mode structure of the dataset is learned by the generator. This is realized by augmenting GAN's objective to foster the preservation of the pairwise distance distribution in the training process (Sec. 3.4).   <ref type="bibr" target="#b35">[36]</ref> of data items after embedded from their metric space to 2 space. Color indicates labels of MNIST images ("1"-"9"). When 2 distance (left) is used, only two modes are identified: digit "1" and all others, but classifier distance (right) can group data items into 10 individual modes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Metrics of Distance and Distributions</head><p>Before delving into our method, we introduce a few theoretical tools to concretize the geometric structure in a data distribution, paving the way toward understanding our algorithmic details and subsequent theoretical analysis. In the rest of this paper, we borrow a few notational conventions from theoretical computer science: we use [n] to denote the set {1, 2, · · · , n}, R ≥0 to denote the set of all non-negative real numbers, and log(·) to denote log 2 (·) for short.</p><p>Metric space. A metric space is described by a pair (M, d), where M is a set and</p><formula xml:id="formula_1">d : M×M → R ≥0 is a distance function such that ∀x, y, z ∈ M, we have i) d(x, y) = 0 ⇔ x = y, ii) d(x, y) = d(y, x), and iii) d(x, z) ≤ d(x, y) + d(y, z).</formula><p>If M is a finite set, then we call (M, d) a finite metric space.</p><p>Wasserstein-1 distance. Wasserstein-1 distance, also known as the Earth-Mover distance, is one of the distance measures to quantify the similarity of two distributions, defined as W (P a , P b ) = inf Γ∈Π(Pa,P b ) E (x,y)∼Γ (|x − y|) , where P a and P b are two distributions on real numbers, and Π(P a , P b ) is the set of all joint distributions Γ(x, y) on two real numbers whose marginal distributions are P a and P b , respectively. Wasserstein-1 distance has been used to augment GAN's objective and improve training stability <ref type="bibr" target="#b3">[4]</ref>. We will use it to understand the theoretical guarantees of our method.</p><p>Logarithmic pairwise distance distribution (LPDD). We propose to use the pairwise distance distribution of data items to reflect the mode structure in a dataset ( <ref type="figure" target="#fig_1">Fig. 2-top)</ref>. Since the pairwise distance is measured under a specific metric, its distribution also depends on the metric choice. Indeed, it has been used in <ref type="bibr" target="#b8">[9]</ref> to quantify how well Unrolled GAN addresses mode collapse.</p><p>Concretely, given a metric space (M, d), let X be a distribution over M, and (λ, Λ) be two real values satisfying 0 &lt; 2λ ≤ Λ. Consider two samples x, y independently drawn from X , and let η be the logarithmic distance between x and y (i.e., η = log(d(x, y))). We call the distribution of η conditioned on d(x, y) ∈ [λ, Λ] the (λ, Λ)−logarithmic pairwise distance distribution (LPDD) of the distribution X . Throughout our theoretical analysis, LPDD of the distributions generated at various steps of our method will be measured in Wasserstein-1 distance.</p><p>Remark. We choose to use logarithmic distance in order to reasonably compare two pairwise distance distributions. The rationale is illustrated in <ref type="figure" target="#fig_5">Fig. 6</ref> in the appendix. Using logarithmic distance is also beneficial for training our GANs, which will become clear in Sec. 3.4. The (λ, Λ) values in the above definition are just for the sake of theoretical rigor, irrelevant from our practical implementation. They are meant to avoid the theoretical situation where two samples are identical and then taking the logarithm becomes no sense. In this section, the reader can skip these values and refer back when reading our theoretical analysis (in Sec. 4 and the supplementary material).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Preprocessing: Subsample of Data Items</head><p>We now describe how to train BourGAN step by step. Provided with a multiset of data items</p><formula xml:id="formula_2">X = {x i } n i=1</formula><p>drawn independently from an unknown distribution X , we first subsample m (m &lt; n) data items uniformly at random from X. This subsampling step is essential, especially when n is large, for reducing the computational cost of metric embeddings as well as the number of dimensions of the latent space (both described in Sec. <ref type="bibr">3.3)</ref>. From now on, we use Y to denote the multiset of data items subsampled from X (i.e., Y ⊆ X and |Y | = m). Elements in Y will be embedded in 2 space in the next step.</p><p>The subsampling strategy, while simple, is theoretically sound. Let P be the (λ, Λ)-LPDD of the data distribution X , and P be the LPDD of the uniform distribution on Y . We will show in Sec. 4 that their Wasserstein-1 distance W (P, P ) is tightly bounded if m is sufficiently large but much smaller than n. In other words, the mode structure of the real data can be captured by considering only the subsamples in Y . In practice, m is chosen automatically by a simple algorithm, which we describe in Appx. F.1. In all our examples, we find m = 4096 sufficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Construction of Gaussian Mixture in Latent Space</head><p>Next, we construct a Gaussian mixture model for generating random vectors in latent space. First, we embed data items from Y to an 2 space, one that the latent random vectors reside in. We want the latent vector dimensionality to be small, while ensuring that the mode structure be well reflected in the latent space. This requires the embedding to introduce minimal distortion on the pairwise distances of data items. For this purpose, we propose an algorithm that leverages Bourgain's embedding theorem.</p><p>Metric embeddings. Bourgain <ref type="bibr" target="#b12">[13]</ref> introduced a method that can embeds any finite metric space into a small 2 space with minimal distortion. The theorem is stated as follows:</p><formula xml:id="formula_3">Theorem 1 (Bourgain's theorem). Consider a finite metric space (Y, d) with m = |Y |. There exists a mapping g : Y → R k for some k = O(log 2 m) such that ∀y, y ∈ Y, d(y, y ) ≤ g(y) − g(y ) 2 ≤ α · d(y, y ), where α is a constant satisfying α ≤ O(log m).</formula><p>The mapping g is constructed using a randomized algorithm also given by Bourgain <ref type="bibr" target="#b12">[13]</ref>. Directly applying Bourgain's theorem results in a latent space of O(log 2 m) dimensions. We can further reduce the number of dimensions down to O(log m) through the following corollary.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Corollary 2 (Improved Bourgain embedding). Consider a finite metric space</head><formula xml:id="formula_4">(Y, d) with m = |Y |. There exist a mapping f : Y → R k for some k = O(log m) such that ∀y, y ∈ Y, d(y, y ) ≤ f (y) − f (y ) 2 ≤ α · d(y, y ), where α is a constant satisfying α ≤ O(log m).</formula><p>Proved in Appx. B, this corollary is obtained by combining Thm. 1 with the Johnson-Lindenstrauss (JL) lemma <ref type="bibr" target="#b37">[38]</ref>. The mapping f is computed through a combination of the algorithms for Bourgain's theorem and the JL lemma. This algorithm of computing f is detailed in Appx. A. Remark. Instead of using Bourgain embedding, one can find a mapping f :</p><formula xml:id="formula_5">Y → R k with bounded distortion, namely, ∀y, y ∈ Y, d(y, y ) ≤ f (y) − f (y ) 2 ≤ α · d(y, y )</formula><p>, by solving a semidefinite programming problem (e.g., see <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b32">33]</ref>). This approach can find an embedding with the least distortion α. However, solving semidefinite programming problem is much more costly than computing Bourgain embeddings. Even if the optimal distortion factor α is found, it can still be as large as O(log m) in the worst case <ref type="bibr" target="#b39">[40]</ref>. Indeed, Bourgain embedding is optimal in the worst case.</p><p>Using the mapping f , we embed data items from Y (denoted as</p><formula xml:id="formula_6">{y i } m i=1 ) into the 2 space of k dimen- sions (k = O(log m)). Let F be the multiset of the resulting vectors in R k (i.e., F = {f (y i )} m i=1</formula><p>). As we will formally state in Sec. 4, the Wasserstein-1 distance between the (λ, Λ)−LPDD of the real data distribution X and the LPDD of the uniform distribution on F is tightly bounded. Simply speaking, the mode structure in the real data is well captured by F in 2 space.</p><p>Latent-space Gaussian mixture. Now, we construct a distribution using F to draw random vectors in latent space. A simple choice is the uniform distribution over F , but such a distribution is not continuous over the latent space. Instead, we construct a mixture of Gaussians, each of which is centered at a vector f (y i ) in F . In particular, we generate a latent vector z ∈ R k in two steps: We first sample a vector µ ∈ F uniformly at random, and then draw a vector z from the Gaussian distribution N (µ, σ 2 ), where σ is a smoothing parameter that controls the smoothness of the distribution of the latent space. In practice, we choose σ empirically (σ = 0.1 for all our examples). We discuss our choice of σ in Appx. F.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Remark. By this definition, the Gaussian mixture consists of m Gaussians</head><formula xml:id="formula_7">(recall F = {f (y i )} m i=1</formula><p>). But this does not mean that we construct m "modes" in the latent space. If two Gaussians are close to each other in the latent space, they should be viewed as if they are from the same mode. It is the overall distribution of the m Gaussians that reflects the distribution of modes. In this sense, the number of modes in the latent space is implicitly defined, and the m Gaussians are meant to enable us to sample the modes in the latent space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Training</head><p>The Gaussian mixture distribution Z in the latent space guarantees that the LPDD of Z is close to (λ, Λ)−LPDD of the target distribution X (shown in Sec. 4). To exploit this property for avoiding mode collapse, we encourage the generator network to match the pairwise distances of generated samples with the pairwise 2 distances of latent vectors in Z. This is realized by a simple augmentation of the GAN's objective function, namely,</p><formula xml:id="formula_8">L(G, D) = L gan + βL dist ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_9">L dist (G) = E zi,zj ∼Z (log(d(G(z i ), G(z j ))) − log( z i − z j 2 )) 2 ,<label>(3)</label></formula><p>L gan is the objective of the standard GAN in Eq. <ref type="formula" target="#formula_0">(1)</ref>, and β is a parameter to balance the two terms. In L dist , z i and z j are two i.i.d. samples from Z conditioned on z i = z j . Here the advantages of using logarithmic distances are threefold: i) When there exists "outlier" modes that are far away from others, logarithmic distance prevents those modes from being overweighted in the objective. ii) Logarithm turns a uniform scale of the distance metric into a constant addend that has no effect to the optimization. This is desired as the structure of modes is invariant under a uniform scale of distance metric. iii) Logarithmic distances ease our theoretical analysis, which, as we will formalize in Sec. 4, states that when Eq. <ref type="formula" target="#formula_9">(3)</ref> is optimized, the distribution of generated samples will closely resemble the real distribution X . That is, mode collapse will be avoided.</p><p>In practice, when experimenting with real datasets, we find that a simple pre-training step using the correspondence between</p><formula xml:id="formula_10">{y i } m i=1 and {f (y i )} m i=1</formula><p>helps to improve the training stability. Although not a focus of this paper, this step is described in Appx. C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Theoretical Analysis</head><p>This section offers an theoretical analysis of our method presented in Sec. 3. We will state the main theorems here while referring to the supplementary material for their rigorous proofs. Throughout, we assume a property of the data distribution X : if two samples, a and b, are drawn independently from X , then with a high probability (&gt; 1 /2) they are distinct (i.e., Pr a,b∼X (a = b) ≥ 1 /2). Range of pairwise distances. We first formalize our definition of (λ, Λ)−LPDD in Sec. 3.1. Recall that the multiset X = {x i } n i=1 is our input dataset regarded as i.i.d. samples from X . We would like to find a range [λ, Λ] such that the pairwise distances of samples from X is in this range with a high probability (see Example-7 and -8 in Appx. D). Then, when considering the LPDD of X , we account only for the pairwise distances in the range [λ, Λ] so that the logarithmic pairwise distance is well defined. The values λ and Λ are chosen by the following theorem, which we prove in Appx. G.2.</p><formula xml:id="formula_11">Theorem 3. Let λ = min i∈[n−1]:xi =xi+1 d(x i , x i+1 ) and Λ = max i∈[n−1] d(x i , x i+1 ). ∀δ, γ ∈ (0, 1), if n ≥ C/(δγ)</formula><p>for some sufficiently large constant C &gt; 0, then with probability at least 1 − δ,</p><formula xml:id="formula_12">Pr a,b∼X (d(a, b) ∈ [λ, Λ] | λ, Λ) ≥ Pr a,b∼X (a = b) − γ.</formula><p>Simply speaking, this theorem states that if we choose λ and Λ as described above, then we have</p><formula xml:id="formula_13">Pr a,b∼X (d(a, b) ∈ [λ, Λ] | a = b) ≥ 1 − O( 1 /n)</formula><p>, meaning that if n is large, the pairwise distance of any two i.i.d. samples from X is almost certainly in the range [λ, Λ]. Therefore, (λ, Λ)−LPDD is a reasonable measure of the pairwise distance distribution of X . In this paper, we always use P to denote the (λ, Λ)−LPDD of the real data distribution X . Number of subsamples. With the choices of λ and Λ, we have the following theorem to guarantee the soundness of our subsampling step described in Sec. 3.2.</p><formula xml:id="formula_14">Theorem 4. Let Y = {y i } m i=1 be a multiset of m = log O(1) (Λ/λ) · log(1/δ) i.i.d</formula><p>. samples drawn from X , and let P be the LPDD of the uniform distribution on Y . For any δ ∈ (0, 1), with probability at least 1 − δ, we have W (P, P ) ≤ O(1).</p><p>Proved in Appx. G.3, this theorem states that we only need m (on the order of log O(1) (Λ/λ)) subsamples to form a multiset Y that well captures the mode structure in the real data. Discrete latent space. Next, we lay a theoretical foundation for our metric embedding step described in Sec. 3.3. Recall that F is the multiset of vectors resulted from embedding data items from Y to the 2 space (i.e., F = {f (y i )} m i=1 ). As proved in Appx. G.4, we have: Theorem 5. Let F be the uniform distribution on the multiset F . Then with probability at least 0.99, we have W (P,P) ≤ O(log log log(Λ/λ)), whereP is the LPDD of F.</p><p>Here the triple-log function (log log log(Λ/λ)) indicates that the Wasserstein distance bound can be very tight. Although this theorem states about the uniform distribution on F , not precisely the Gaussian mixture we constructed, it is about the case when σ of the Gaussian mixture approaches zero. We also empirically verified the consistency of LPDD from Gaussian mixture samples <ref type="figure" target="#fig_2">(Fig. 3)</ref>. GAN objective. Next, we theoretically justify the objective function (i.e., Eq. (3) in Sec. 3.4). LetX be the distribution of generated samples G(z) for z ∼ Z andP be the (λ, Λ)−LPDD ofX . Goodfellow et al. <ref type="bibr" target="#b0">[1]</ref> showed that the global optimum of the GAN objective <ref type="formula" target="#formula_0">(1)</ref> is reached if and only ifX = X . Then, when this optimum is achieved, we must also have W (P,P) = 0 and W (P,P) ≤ O(log log log(Λ/λ)). The latter is because W (P,P) ≤ O(log log log(Λ/λ)) from Thm. 5.</p><p>As a result, the GAN's minmax problem <ref type="formula" target="#formula_0">(1)</ref> is equivalent to the constrained minmax problem,</p><formula xml:id="formula_15">min G max D L gan (G, D)</formula><p>, subject to W (P,P) ≤ β, where β is on the order of O(log log log(Λ/λ)). Apparently, this constraint renders the minmax problem harder. We therefore consider the minmax problem, min G max D L gan (G, D), subjected to slightly strengthened constraints,</p><formula xml:id="formula_16">∀z 1 = z 2 ∈ supp(Z), d(G(z 1 ), G(z 2 )) ∈ [λ, Λ], and<label>(4)</label></formula><formula xml:id="formula_17">[log(d(G(z 1 ), G(z 2 ))) − log z 1 − z 2 2 ] 2 ≤ β 2 .<label>(5)</label></formula><p>As proved in Appx. E, if the above constraints are satisfied, then W (P,P) ≤ β is automatically satisfied. In our training process, we assume that the constraint (4) is automatically satisfied, supported by Thm. 3. Lastly, instead of using Eq. (5) as a hard constraint, we treat it as a soft constraint showing up in the objective function <ref type="bibr" target="#b2">(3)</ref>. From this perspective, the second term in our proposed objective <ref type="bibr" target="#b1">(2)</ref> can be interpreted as a Lagrange multiplier of the constraint.</p><p>LPDD of the generated samples. Now, if the generator network is trained to satisfy the constraint (5), we have W (P,P) ≤ O(log log log(Λ/λ)). Note that this satisfaction does not imply that the global optimum of the GAN in Eq. (1) has to be reached -such a global optimum is hard to achieve in practice. Finally, using the triangle inequality of the Wasserstein-1 distance and Thm. 5, we reach the conclusion that</p><formula xml:id="formula_18">W (P, P) ≤ W (P,P) + W (P,P) ≤ O(log log log(Λ/λ)).<label>(6)</label></formula><p>This means that the LPDD of generated samples closely resembles that of the data distribution. To put the bound in a concrete context, in Example 9 of Appx. D, we analyze a toy case in a thought experiment to show, if the mode collapse occurs (even partially), how large W (P, P) would be in comparison to our theoretical bound here.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>This section presents the empirical evaluations of our method. There has not been a consensus on how to evaluate GANs in the machine learning community <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b41">42]</ref>, and quantitative measure of mode collapse is also not straightforward. We therefore evaluate our method using both synthetic and real datasets, most of which have been used by recent GAN variants. We refer the reader to Appx. F for detailed experiment setups and complete results, while highlighting our main findings here. Overview. We start with an overview of our experiments. i) On synthetic datasets, we quantitatively compare our method with four types of GANs, including the original GAN <ref type="bibr" target="#b0">[1]</ref> and more recent VEEGAN <ref type="bibr" target="#b9">[10]</ref>, Unrolled GANs <ref type="bibr" target="#b8">[9]</ref>, and PacGAN <ref type="bibr" target="#b10">[11]</ref>, following the evaluation metrics used by those methods (Appx. F.2). ii) We also examine in each mode how well the distribution of generated samples matches the data distribution (Appx. F.2) -a new test not presented previously. iii) We compare the training convergence rate of our method with existing GANs (Appx. F.2), examining to what extent the Gaussian mixture sampling is beneficial. iv) We challenge our method with the difficult stacked MNIST dataset (Appx. F.3), testing how many modes it can cover. v) Most notably, we examine if there are "false positive" samples generated by our method and others <ref type="figure" target="#fig_3">(Fig. 4)</ref>. Those are unwanted samples not located in any modes. In all these comparisons, we find that BourGAN clearly produces higher-quality samples. In addition, we show that vi) our method is able to incorporate different distance metrics, ones that lead to different mode interpretations (Appx. F.3); and vii) our pre-training step (described in Appx. C) further accelerates the training convergence in real datasets (Appx. F.2). Lastly, viii) we present some qualitative results (Appx. F.4). Quantitative evaluation. We compare BourGAN with other methods on three synthetic datasets: eight 2D Gaussian distributions arranged in a ring (2D Ring), twenty-five 2D Gaussian distributions arranged in a grid (2D Grid), and a circle surrounding a Gaussian placed in the center (2D Circle). The first two were used in previous methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, and the last is proposed by us. The quantitative performance of these methods are summarized in <ref type="table" target="#tab_2">Table 1</ref>, where the column "# of modes" indicates the average number of modes captured by these methods, and "low quality" indicates number of samples that are more than 3× standard deviations away from the mode centers. Both metrics are used in previous methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>. For the 2D circle case, we also check if the central mode is captured by the methods. Notice that all these metrics measure how many modes are captured, but not how well the data distribution is captured. To understand this, we also compute the Wasserstein-1 distances between the distribution of generated samples and the data distribution (reported in <ref type="table" target="#tab_2">Table 1</ref>). It is evident that our method performs the best on all these metrics (see Appx. F.2 for more details).</p><p>Avoiding unwanted samples. A notable advantage offered by our method is the ability to avoid unwanted samples, ones that are located between the modes. We find that all the four existing GANs suffer from this problem (see <ref type="figure" target="#fig_3">Fig. 4</ref>), because they use Gaussian to draw latent vectors (recall <ref type="figure" target="#fig_0">Fig. 1</ref>). In contrast, our method generates no unwanted samples in all three test cases. We refer the reader to Appx. F.3 for a detailed discussion of this feature and several other quantitative comparisons.</p><p>Qualitative results. We further test our algorithm on real image datasets. <ref type="figure">Fig. 5</ref> illustrates a qualitative comparison between DCGAN and our method, both using the same generator and discriminator architectures and default hyperparameters. Appx. F.4 includes more experiments and details.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>This paper introduces BourGAN, a new GAN variant aiming to address mode collapse in generator networks. In contrast to previous approaches, we draw latent space vectors using a Gaussian mixture, which is constructed through metric embeddings. Supported by theoretical analysis and experiments, our method enables a well-posed mapping between latent space and multi-modal data distributions. In future, our embedding and Gaussian mixture sampling can also be readily combined with other GAN variants and even other generative models to leverage their advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Improved Bourgain Embedding</head><p>Input: A finite metric space (Y, d).</p><formula xml:id="formula_19">Output: A mapping f : Y → R O(log |Y |) . //Bourgain Embedding: Initialization: m ← |Y |, t ← O(log m), and ∀i ∈ [ log m ], j ∈ [t], Si,j ← ∅. for i = 1 → log m do for j = 1 → t do</formula><p>For each x ∈ Y, independently choose x in Si,j, i.e. Si,j = Si,j ∪ {x} with probability 2 −i . end for end for</p><formula xml:id="formula_20">Initialize g : Y → R log m ·t . for x ∈ Y do ∀i ∈ [ log m ], j ∈ [t], set the ((i − 1) · t + j)-th coordinate of g(x) as d(x, Si,j)</formula><p>. end for //Johnson-Lindenstrauss Dimentionality Reduction: Let d = O(log m), and let G ∈ R d×( log m ·t) be a random matrix with entries drawn from i.i.d. N (0, 1). </p><formula xml:id="formula_21">Let h : R log m ·t → R d satisfy ∀x ∈ R log m ·t , h(x) ← G · x. //Rescaling: Let β = min x,y∈Y :x =y h(g(x))−h(g(y)) 2 d(x,y) . Initialize f : Y → R d . For x ∈ Y, set f (x) ← h(g(x))/β. Return f .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A Algorithm of Improved Bourgain Embedding</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>B Proof of Corollary 2</head><p>Here we prove the Corollary 2 introduced in Sec. 3.3. First, we recall the Johnson-Lindenstrauss lemma <ref type="bibr" target="#b37">[38]</ref>. Theorem 6 (Johnson-Lindenstrauss lemma). Consider a set of m points X = {x i } m i=1 in a vector space R t . There exist a mapping h :</p><formula xml:id="formula_22">X → R k for some k = O(log m) such that ∀i, j ∈ [m], h(x i ) − h(x j ) 2 ≤ x i − x j 2 ≤ O(1) · h(x i ) − h(x j ) 2 .</formula><p>By combining this lemma with Bourgain's theorem 1, we reach the corollary through the following proof.</p><p>pairwise distance frequency frequency log-pairwise distance The pairwise distance distributions of both datasets are different. The distribution of orange points is a streched version of the distribution of blue points. As a result, the Wasserstein-1 distance between both distributions can become arbitrarily large, depending on the scale. (c) In contrast, the distribution of logarithmic pairwise distance remains the same up to a constant shift. In this case, the Wasserstein-1 distance of the logarithmic pairwise distance distributions is differed by only a constant addent, which can be easily accounted.</p><p>Proof. By Theorem 1, we can embed all data items from Y into the 2 space with O(log 2 m) dimensions and with O(log m) distortion. Then, according to Theorem 6, we can further reduce the number of dimensions to O(log m) with O(log m) distortion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>C Pre-training</head><p>While our method addresses mode collapse, in practice, we have to confront other challenges of training the GAN, particularly its instability and sensitivity to hyper-parameters. To this end, we pre-train the generator network G and use it to warm start the training of our GAN. Pre-training is made possible because our metric embedding step has established the correspondence between the embedding vectors f (y i ) in the latent space and the data items y i ∈ Y , i ∈ [m]. This correspondence allows us to perform a supervised learning to minimize the objective</p><formula xml:id="formula_23">L pre (G) = E yi,zi [d(G(f (y i )), y i )] .</formula><p>As will be shown in our experiments, this pre-training step leads to faster convergence when we train our GANs. Lastly, we note that our method can be straightforwardly combined with other objective function extensions <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b42">43]</ref> and network architectures <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b8">9]</ref>, ones that specifically focus on addressing other challenges such as instability, to leverage their advantages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>D Illustrative Examples for Sec. 4</head><p>The following two examples illustrate the ranges of the pairwise distance that can cover a pairwise distance sample with a high probability. They are meant to exemplify the choices of λ and Λ discussed in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Example 7.</head><p>Consider the set of all points in R 20 , and the distance measure is chosen to be the Euclidean distance. Let X be the Gaussian distribution N (0, I). Suppose we draw two i.i.d. samples x, y form X , then with probability at least 0.99999, d(x, y) should be in the range [0.1, 10].</p><p>Example 8. Consider the set of all 256 × 256 grayscale images, and the brightness of each pixel is described by a number in {0, 1, 2, · · · , 255}. Let X be a uniform distribution over all the images which contains a cat. Suppose we draw two i.i.d. samples x, y from X , then with probability 1, the distance between x and y should be in the range [1, 255 · 256 · 256] = <ref type="bibr" target="#b0">[1,</ref><ref type="bibr">16777216]</ref>.</p><p>Next, we show a concrete example in which if the generator produces samples mainly in one mode, then W (P,P) can be as large as Ω(log(Λ/λ)), drastically larger than the bound in (6). It is easy to see that A, B are two separated modes. Let d : M × M → R ≥0 be the Euclidean distance (i.e., ∀x, y ∈ M, d(x, y) = x − y 2 ), and let λ = 1. It is easy to see that ∀x = y ∈ M, we have d(x, y) ∈ [λ, Λ]. Suppose the real data distribution X is the uniform distribution on M. Also suppose the distribution of generated samples isX , and the probability that generator G generates samples near the mode B is at most 1/10. Then, consider the (λ, Λ)−LPDD (denoted by P) of X . If we draw two independent samples from X , then conditioned on this two samples being distinct, with probability at least 1 /3, they are in different modes. Thus, if we draw a sample p from P, then with probability at least 1 /3, p is at least Λ /2. Now consider the distributionX of generated samples. Since with probability at least 9 /10, a sample fromX will land in mode A, if we draw two samples fromX , then with probability at least 4 /5, the distance between these two samples is at most √ d. Thus, the Wasserstein distance is at least ( 4 /5−(1− 1 /3))·| log(</p><formula xml:id="formula_24">Λ 2 )−log √ d| ≥ 0.1 log( Λ / √ d) = Ω(log(Λ/λ)).</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>E Strengthened Constraints for GAN's Minmax Problem</head><p>As explained in Sec. 4, introducing the constraint W (P, P ) &lt; β in the GAN optimization makes the problem harder to solve. Thus, we choose to slightly strengthen the constraint. Observe that if for all</p><formula xml:id="formula_25">z 1 = z 2 ∈ supp(Z) we have | log(d(G(z 1 ), G(z 2 ))) − log( z 1 − z 2 2 )| ≤ O(log log log(Λ/λ)) and d(G(z 1 ), G(z 2 )) ∈ [λ, Λ], we have W (P,P) ≤ z1 =z2∈supp(Z) Pr Z1,Z2∼Z (Z 1 = z 1 , Z 2 = z 2 | Z 1 = Z 2 ) · log d(G(z 1 ), G(z 2 )) z 1 − z 2 2</formula><p>≤ O(log log log(Λ/λ)).</p><p>In other words, if the constraints in <ref type="formula" target="#formula_16">(4)</ref> and <ref type="formula" target="#formula_17">(5)</ref> are satisfied, then the constraint W (P, P ) &lt; β is automatically satisfied. Thus, they are a slightly strengthened version of W (P, P ) &lt; β.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F Evaluation and Experiment</head><p>In this section, we provide details of our experiments, starting with a few implementation details that are worth noting. All our experiments are performed using a Nvidia GTX 1080 Ti Graphics card and implemented in Pytorch <ref type="bibr" target="#b44">[45]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.1 Parameter setup</head><p>As discussed in Sec. 3.2, we randomly sample m data items from the provided the dataset to form the set Y for subsequent metric embeddings. In our implementation, we choose m automatically by using a simple iterative algorithm. Starting from a small m value (e.g., 32), in each iteration we double m and add more samples from the real dataset. We stop the iteration when the pairwise distance distribution of the samples converges under the Wasserstein-1 distance. The termination of this process is guaranteed because of the existence of the theoretical upper bound of m (recall Thm. 4). In all our examples, we found m = 4096 sufficient. With the chosen m, we construct the multiset Y = y i m i=1 by uniformly sampling the dataset X. Afterwards, we compute the metric embedding f (y i ) for each y i ∈ Y , and normalize each vector in {f</p><formula xml:id="formula_26">(y i )} m i=1 bȳ f (y i ) = f (y i ) − µ 0 σ 0 ,</formula><p>where µ 0 and σ 0 are the average and standard deviation of the entire set {f (y i )} m i=1 , respectively. Two other parameters are needed in our method, namely, β in Eq. (2) and the standard deviation σ used for the sampling latent Gaussian mixture model (recall Sec. 3.3). In all our experiments, we set β = 0.2 and σ = 0.1. We find that the final mode coverage of generated samples is not sensitive to σ value in the range [0.2, 0.6]. Only when σ is too small, the Gaussian mixture becomes noisy (or "spiky"), and when σ is too large, the Gaussian mixture starts to degrade into a single Gaussian as used in conventional GANs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.2 Experiment Details on Synthetic Data</head><p>Setup. We follow the experiment setup used in <ref type="bibr" target="#b9">[10]</ref> for 2D Ring and 2D Grid. In the additional 2D circle case, the input dataset is generated by using 100 Gaussian distributions on a circle with a radius r = 2, as well as three identical Gaussians located at the center of the circle. All Gaussians have the same standard deviation (i.e., 0.05).</p><p>All the GANs (including our method and compared methods) in this experiment share the same generator and discriminator architectures. They have two hidden layers, each of which has 128 units with ReLU activation and without any dropout <ref type="bibr" target="#b45">[46]</ref> or normalization layers <ref type="bibr" target="#b46">[47]</ref>. When using the Unrolled GAN <ref type="bibr" target="#b8">[9]</ref>, we set the number of unrolling steps to be five as suggested in the authors' reference implementation. When using PacGAN <ref type="bibr" target="#b10">[11]</ref>, we follow the authors' suggestion and set the number of packing to be four. In all synthetic experiments, our method is performed without the pre-training step described in Sec. C.</p><p>During training, we use a mini-batch size of 256 with 3000 iterations in total, and use the Adam <ref type="bibr" target="#b47">[48]</ref> optimization method with a learning rate of 0.001 and set β 1 = 0.5, β 2 = 0.999. During testing, we use 2500 samples from the learned generator network for evaluation, and use 2 distance as the target distance metric for Bourgain embedding. Every metric value listed in <ref type="table" target="#tab_2">Table 1</ref>   Studies. When evaluating the number of captured modes ("# modes" in <ref type="table" target="#tab_2">Table 1</ref>), a mode is considered as being "captured" when there exists at least one sample located within one standarddeviation-distance (1-std) away from the center of the mode. This criterion is slightly different from that used in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, in which they use three standard-deviation (3-std). We choose to use 1-std because we would like to have finer granularity to differentiate the tested GANs in terms of their mode capture performance.</p><p>To gain a better understanding of the mode capture performance, we also measure in each method the percentages of generated samples located within 1-, 2-, and 3-std away from mode centers for the three test datasets. The results are reported in <ref type="table" target="#tab_4">Table 2</ref>. We note that for Gaussian distribution, the percentages of samples located in 1-, 2-, and 3-std away from the center are 68.2%, 95.4%, 99.7%, respectively <ref type="bibr" target="#b48">[49]</ref>. Our method produces results that are closest to these percentages in comparison to other methods. This suggests that our method better captures not only individual modes but also the data distribution in each mode, thanks to the pairwise distance preservation term (3) in our objective function. We also note that this experiment result is echoed by the Wasserstein-1 measure reported in <ref type="table" target="#tab_2">Table 1</ref>, for which we measure the Wasserstein-1 distance between the distribution of generated samples and the true data distribution. Our method under that metric also performs the best.</p><p>Lastly, we examine how quickly these methods converges during the training process. The results are reported in <ref type="figure">Fig. 7</ref>, where we also include the results from our BourGAN but set β in the objective 2 to be zero. That is, we also test our method using standard GAN objective function. <ref type="figure">Figure 7</ref> shows that our method with augmented objective converges the most quickly: The generator becomes stable after 1000 iterations in this example, while others remain unstable even after 1750 iterations. This result also empirically supports the necessity of using the pairwise distance preservation term in the objective function. We attribute the faster convergence of our method to the fact that the latent-space Gaussian mixture in our method encodes the structure of modes in the data space and the fact that our objective function encourages the generator to preserve this structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.3 Evaluation on MNIST and Stacked MNIST</head><p>In this section, we report the evaluation results on MNIST dataset. All MNIST images are scaled to 32×32 by bilinear interpolation.</p><p>Setup. Quantitative evaluation of GANs is known to be challenging, because the implicit distributions of real datasets are hard, if not impossible, to obtain. For the same reason, quantification of mode collapse is also hard for real datasets, and no widely used evaluation protocol has been established. We take an evaluation approach that has been used in a number of existing GAN variants <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b8">9]</ref>: we use a third-party trained classifier to classify the generated samples into specific modes, and thereby estimate the generator's mode coverage <ref type="bibr" target="#b2">[3]</ref>.</p><p>Classifier distance. A motivating observation of our method is that the structure of modes depends on a specific choice of distance metric (recall <ref type="figure" target="#fig_1">Fig. 2</ref>). The widely used distance metrics on images (such as the pixel-wise 2 distance and Earth Mover's distance) may not necessarily produce interpretable mode structures. Here we propose to use the Classifier Distance metric defined as</p><formula xml:id="formula_27">d classifier (x i , x j ) = P (x i ) − P (x j ) 2 ,<label>(7)</label></formula><p>where P (x i ) is the softmax output vector of a pre-trained classification network, and x i represents an input image. Adding a third-party trained classifier turns the task of training generative models semi-supervised <ref type="bibr" target="#b14">[15]</ref>. Nevertheless, Eq. <ref type="formula" target="#formula_27">(7)</ref> is a highly complex distance metric, serving for the purpose of testing our method with an "unconventional" metric. It is also meant to show that a properly chosen metric can produce interpretable modes. Visualization of embeddings. After we apply our metric embedding algorithm with different distance metrics on MNIST images, we obtain a set of vectors in 2 space. To visualize these vectors in 2D, we use t-SNE <ref type="bibr" target="#b35">[36]</ref>, a nonlinear dimensionality reduction technique well-suited for visualization of high-dimensional data in 2D or 3D. Although not fully accurately, this visualization shreds light on how (and where) data points are located in the latent space (see <ref type="figure" target="#fig_1">Fig. 2</ref>).</p><p>MNIST experiment. First, we verify that our pre-training step (described in Appx. C) indeed accelerates the training process, as illustrated in <ref type="figure">Fig. 8</ref>. We plot the distribution of digits generated by DCGAN in orange, BourGAN ( 2 ) in green, and BourGAN (classifier) in yellow. The generated images from those GANs are classified using a pre-trained classifier. This plot shows that the classifier distance produces samples that are most uniformly distributed across all 10 digits. DCGAN fails to capture the mode of digital "1", while BourGAN ( 2 ) generates fewer samples for the modes in "3" and "9". (right) Entropy distribution of generated samples using three GANs. A lower entropy value indicates better image quality. This plot suggests that our method with both 2 and classifier distance metrics produces higher-quality MNIST images than the DCGAN.</p><p>Next, we evaluate the quality of generated samples using different distance metrics. One widely used evaluation score is the inception score <ref type="bibr" target="#b24">[25]</ref> that measures both the visual quality and diversity of generated samples. However, as pointed out by <ref type="bibr" target="#b11">[12]</ref>, a generative model can produce a high inception score even when it collapses to a visually implausible sample. Furthermore, we would like to measure the visual quality and diversity separately rather than jointly, to understand the performance of our method in each of the two aspects under different metrics. Thus, we choose to use entropy, defined as E(x) = − 9 i=0 p(y = i|x) log p(y = i|x), as the score to measure the quality of the generated sample x, where p(y = i|x) is the probability of labeling the input x as the digit i by the pre-trained classifier. The rationale here is that a high-quality sample often produces a low entropy through the pre-trained classifier.</p><p>We compare DCGAN with BourGAN using this score. Since our method can incorporate different distance metrics, we consider two of them: BourGAN using 2 distance and BourGAN using the aforementioned classifier distance. For a fair comparison, the three GANS (i.e., DCGAN, BourGAN ( 2 ), and BourGAN (classifier)) all use the same number of dimensions (k = 55) for the latent space and the same network architecture. For each type of GANs, we randomly generate 5000 samples to evaluate the entropy scores, and the results are reported in <ref type="figure" target="#fig_6">Fig. 9</ref>. We also compute the KL divergence between the generated distribution and the data distribution, following the practice of <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b35">36]</ref>. The KL divergence for DCGAN, BourGAN ( 2 ) and BourGAN (classifier) are 0.116, 0.104, and 0.012, respectively.</p><p>A well-trained generator is expected to produce a relatively uniform distribution across all 10 digits. Our experiment suggests that both BourGAN ( 2 ) and BourGAN (classifier) generate better-quality samples in comparison to DCGAN, as they both produce lower entropy scores <ref type="figure" target="#fig_6">(Fig. 9-right</ref>). Yet, BourGAN (classifier) has a lower KL divergence compared to BourGAN ( 2 ), suggesting that the classifier distance is a better metric in this case to learn mode diversity. Although a pre-trained classifier may not always be available in real world applications, here we demonstrate that some metric might be preferred over others depending on the needs, and our method has the flexibility to use different metrics.</p><p>Lastly, we show that interpretable modes can be learned when a proper distance metric is chosen. <ref type="figure" target="#fig_0">Figure 10</ref> shows the generated images when sampling around individual vectors in latent space. The BourGAN generator trained with 2 distance tends to produce images that are close to each other under 2 measure, while the generator trained with classifier distance tends to produce images that are in the same class, which is more interpretable.</p><p>Tests on Stacked MNIST. Similar to the evaluation methods in Mode-regularized GANs <ref type="bibr" target="#b11">[12]</ref>, Unrolled GANs <ref type="bibr" target="#b8">[9]</ref>, VEEGAN <ref type="bibr" target="#b9">[10]</ref> and PacGAN <ref type="bibr" target="#b10">[11]</ref>, we test BourGAN with 2 distance metric on an augmented MNIST dataset. By encapsulating three randomly selected MNIST images into three color channels, we construct a new dataset of 100,000 images, each of which has a dimension of 32×32×3. In the end, we obtain 10×10×10 = 1000 distinct classes. We refer to this dataset as the stacked MNIST dataset. In this experiment, we will treat each of the 1000 classes of images as an individual mode.  <ref type="table">Table 3</ref>: Mode coverage on stacked MNIST Dataset. Results are averaged over 10 trials As reported in <ref type="bibr" target="#b8">[9]</ref>, even regular GANs can learn all 1000 modes if the discriminator size is sufficiently large. Thus, we evaluate our method by setting the discriminator's size to be 1 /4×, 1 /2×, and 1× of the generator's size, respectively. We measure the number of modes captured by our method as well as by DCGAN, and the KL divergence between the generated distribution of modes and the expected true distribution of modes (i.e., a uniform distribution over the 1000 modes). <ref type="table">Table 3</ref> summarizes our results. In <ref type="table" target="#tab_4">Table 2</ref> and 3 of their paper, Lin et al. <ref type="bibr" target="#b10">[11]</ref> reported results on similar experiments, although we note that it is hard to directly compare our <ref type="table">Table 3</ref> with theirs, because their detailed network setup and the third-part classifier may differ from ours. We summarize our network structures in <ref type="table" target="#tab_7">Table 4</ref> and 5. During training, we use Adam optimization with a learning rate of 10 −4 , and set β 1 = 0.5 and β 2 = 0.999 with a mini-batch size of 128.</p><p>Additionally, in <ref type="figure" target="#fig_0">Fig. 11</ref> we show a qualitative comparison between our method and DCGAN on this dataset.    </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>F.4 More Qualitative Results</head><p>We also test our algorithm on other popular dataset, including CIFAR-10 <ref type="bibr" target="#b49">[50]</ref> and Fashion-MNIST <ref type="bibr" target="#b50">[51]</ref>. <ref type="figure" target="#fig_0">Figure 12</ref> and 13 illustrate our results on these datasets. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G Proofs of the Theorems in Section 4 G.1 Notations and Preliminaries</head><p>Before we delve into technical details, we first review some notation and fundamental tools in the theoretical analysis: We use 1(E) to denote an indicator variable on the event E, i.e., if E happens, then 1(E) = 1, otherwise, 1(E) = 0.</p><p>The following lemma gives a concentration bound on independent random variables. Lemma 10 (Bernstein Inequality). Let X 1 , X 2 , · · · , X n be n independent random variables. Suppose that ∀i ∈ [n],</p><formula xml:id="formula_28">|X i − E(X i )| ≤ M almost surely. Then, ∀t &gt; 0, Pr n i=1 X i − n i=1 E(X i ) &gt; t ≤ 2 exp − 1 2 t 2 n i=1 Var(X i ) + 1 3 M t .</formula><p>The next lemma states that given a complete graph with a power of 2 number of vertices, the edges can be decomposed into perfect matchings. Lemma 11. Given a complete graph G = (V, E) with |V | = m vertices, where m is a power of 2. Then, the edge set E can be decomposed into m − 1 perfect matchings. Proof. Our proof is by induction. The base case has m = 1. For the base case, the claim is obviously true. Now suppose that the claim holds for m/2. Consider a complete graph G = (V, E) with m vertices, where m is a power of 2. We can partition vertices set V into two vertices sets A, B such that |A| = |B| = m/2. The edges between A and B together with vertices A ∪ B = V compose a complete bipartite graph. Thus, the edges between A and B can be decomposed into m/2 perfect matchings. The subgraph of G induced by A is a complete graph with m/2 vertices. By our induction hypothesis, the edge set of the subgraph of G induced by A can be decomposed into m/2 − 1 perfect matchings in that induced subgraph. Similarly, the edge set of the subgraph of G induced by B can be also decomposed into m/2 − 1 perfect matchings in that induced subgraph. Notice that any perfect matching in the subgraph induced by A union any perfect matching in the subgraph induced by B is a perfect matching of G. Thus, E can be decomposed into m/2 + m/2 − 1 = m − 1 perfect matchings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.2 Proof of Theorem 3</head><p>In the following, we formally restate the theorem. Theorem 12. Consider a metric space (M, d). Let X be a distribution over M which satisfies</p><formula xml:id="formula_29">Pr a,b∼X (a = b) ≥ 1/2. Let x 1 , x 2 , · · · , x n be n i.i.d. samples drawn from X . Let λ = min i∈[n−1]:xi =xi+1 d(x i , x i+1 ), Λ = max i∈[n−1]:xi =xi+1 d(x i , x i+1 ).</formula><p>For any given parameters δ ∈ (0, 1), γ ∈ (0, 1), if n ≥ C/(δγ) for some sufficiently large constant C &gt; 0, then with probability at least</p><formula xml:id="formula_30">1 − δ, Pr a,b∼X (d(a, b) ∈ [λ, Λ] | λ, Λ) ≥ Pr a,b∼X (a = b) − γ.</formula><p>Proof. Without of loss of generality, we assume n is an even number.</p><formula xml:id="formula_31">Let λ = min i∈[n/2]:x2i−1 =x2i d(x 2i−1 , x 2i ), Λ = max i∈[n/2]:x2i−1 =x2i d(x 2i−1 , x 2i )</formula><p>, and P , Q be two i.i.d. random variables with distribution X . Then (x 1 , x 2 ), (x 3 , x 4 ), · · · , (x n−1 , x n ) are n/2 i.i.d. samples drawn from the same distribution as (P, Q). Let t = |{j ∈ [n/2] | x 2j−1 = x 2j }|. Suppose p is the probability, p = Pr(P = Q), then we have the following relationship. </p><formula xml:id="formula_32">Pr P,Q,x1,··· ,xm∼D (d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | P = Q) = Pr</formula><p>where the first inequality follows by that probability is always upper bounded by 1, the second inequality follows by symmetry of (P, Q) and (x 2j−1 , x 2j ), the third inequality follows by p ≥ 1/2 and the Chernoff bound, the forth inequality follows by that n is sufficiently large.</p><p>Notice that if with probability greater than δ, Pr(d(P, Q) &lt; λ or d(P, Q) &gt; Λ | λ , Λ ) &gt; 1−p+γ, then we have with probability greater than δ,</p><formula xml:id="formula_34">1 − p + γ &lt; Pr(d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | λ , Λ )</formula><p>= Pr(d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | λ , Λ , P = Q) · Pr(P = Q) + Pr(P = Q) = Pr(d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | λ , Λ , P = Q) · p + 1 − p which implies that with probability greater than δ, Pr(d(P, Q) &lt; λ or d(P, Q) &gt; Λ | λ , Λ , P = Q) &gt; γ/p ≥ γ. Then we have Pr(d(P, Q) &lt; λ or d(P, Q) &gt; Λ | P = Q) &gt; δγ ≥ 16/n which contradicts to Equation <ref type="bibr" target="#b9">(10)</ref>.</p><p>Notice that λ ≤ λ and Λ ≥ Λ , we complete the proof.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>G.3 Proof of Theorem 4</head><p>We restate the theorem in the following formal way.</p><p>∀i ∈ I, we have Let P denote LPDD of U. There exist a mapping f : X → R l for some l = O(log m) such that W (P ,P) ≤ O(log log m), whereP denotes LPDD of the uniform distribution on the multiset F = {f (x 1 ), f (x 2 ), . . . , f (x m )} ⊂ R l .</p><p>Proof. According to Corollary 2, there exists a mapping f : X → R l for some l = O(log m) such that ∀i, j ∈ [m], d(y i , y j ) ≤ f (y i ) − f (y j ) 2 ≤ O(log m) · d(y i , y j ). Notice that since (M, d) is a metric space and f holds the above condition, for any x, y ∈ M, d(x, y) = f (x)−f (y) 2 = 0 if and only if x = y. Let U be the uniform distribution over the multiset F = {f (x 1 ), f (x 2 ), · · · , f (x m )}. Thus, Pr a,b∼U (a = b) = Pr a ,b ∼U (a = b ). Furthermore, we have ∀y ∈ Y, Pr P ∼U (p = y) = Pr p ∼U (p = f −1 (y)).</p><p>Thus, ∀x, y ∈ Y, x = y, we have  In the following, we formally state the complete version of Theorem 5. Theorem 15. Consider a universe of the data M and a distance function d : M × M → R ≥0 such that (M, d) is a metric space. Let X be a data distribution over M which satisfies Pr a,b∼X (a = b) ≥ 1/2. Let X be a multiset which contains n i.i.d. observations x 1 , x 2 , · · · , x n ∈ M generated from the data distribution X . Let λ = min i∈[n/2−1]:xi =xi+1 d(x i , x i+1 ), and Λ = max(max i∈[n/2−1] d(x i , x i+1 ), 2λ). Let P be the (λ, Λ)−LPDD of the original data distribution X . If n ≥ log c 0 (Λ/λ) for a sufficiently large constant c 0 , then with probability at least 0.99, we can find a distribution F on F ⊂ R l for l = O (log log(Λ/λ)) , |F | ≤ C log 4 (Λ/λ) log(log(Λ/λ)) where C is a sufficiently large constant, such that W (P,P) ≤ O(log log log(Λ/λ)), whereP is the LPDD of distribution F Proof. We describe how to construct the distribution F. Let λ = min i∈[n/2−1]:xi =xi+1 d(x i , x i+1 ), and Λ = max(max i∈[n/2−1] d(x i , x i+1 ), 2λ). By applying Theorem 12, with probability at least 0.999, we have </p><p>Let the above event be E 1 . In the remaining of the proof, let us condition on E 1 .</p><p>Let m = C log 4 (Λ/λ) log(log(Λ/λ)) where C is a sufficiently large constant. Let Y = {x n/2+1 , x n/2+2 , · · · , x n/2+m }. Let P be the (λ, Λ)−LPDD of the uniform distribution on Y .</p><p>Notice that Equation <ref type="bibr" target="#b18">(19)</ref> implies Pr p∼P (p ∈ [λ, Λ]) ≥ 1/4. Then, according to Theorem 13, with probability at least 0.999, we have</p><formula xml:id="formula_36">W (P, P ) ≤ 1.<label>(20)</label></formula><p>Let the above event be E 2 . In the remaining of the proof, let us condition on E 2 .</p><p>Equation <ref type="formula">(</ref> By taking union bound over all i, j ∈ {n/2 + 1, n/2 + 2, · · · , n/2 + m}, i = j, with probability at least 0.999, we have either x i = x j or d(x i , x j ) ∈ [λ, Λ]. Let the above event be E 3 . In the remaining of the proof, let us condition on E 3 .</p><p>Due to E 3 , we can just regard P as the LPDD of the uniform distribution on Y . Then, by applying Theorem 14, we can construct a uniform distribution F on F ⊂ R l where |F | ≤ m. LetP be the LPDD of F. According to the Theorem 14, we have W (P ,P) ≤ O(log log m) ≤ O(log log log(Λ/λ)). Then by combining with Equation <ref type="formula" target="#formula_8">(20)</ref>, we have W (P,P) ≤ W (P, P ) + W (P ,P) ≤ 1 + O(log log log(Λ/λ)) = O(log log log(Λ/λ)). Thus, we complete the proof. By taking union bound over E 1 , E 2 , E 3 , the success probability is at least 0.99.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Multi-mode challenge. We train a generator G that maps a latent-space distribution Z to the data distribution X with two modes. (a) Suppose Z is a Gaussian, and G can fit both modes. If we draw two i.i.d. samples z 1 , z 2 from Z, then with at least a constant probability, G(z 1 ) is close to the center x 1 of the first mode, and G(z 2 ) is close to another center x 2 . By the Mean Value Theorem, there exists a z between z 1 and z 2 that has the absolute gradient value, |G (z)| = |</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (Top) Pairwise distance distribution on MNIST dataset under different distance metrics. Left: 2 distance, Middle: Earth Mover's distance (EMD) with a quadratic ground metric, Right: classifier distance (defined in Appx. F.3). Under 2 and EMD distances, few separated modes emerges, and the pairwise distance distributions resemble a Gaussian. Under the classifier distance, the pairwise distance distribution becomes bimodal, indicating that there are separated modes. (Bottom) t-SNE visualization [36] of data items after embedded from their metric space to 2 space. Color indicates labels of MNIST images ("1"-"9"). When 2 distance (left) is used, only two modes are identified: digit "1" and all others, but classifier distance (right) can group data items into 10 individual modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: LPDD of uniform distribution F (orange) and of samples from Gaussian mixture (blue).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Synthetic data tests. In all three tests, our method clearly captures all the modes presented in the targets, while producing no unwanted samples located between the regions of modes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 outlines our randomized algorithm that computes the improved Bourgain embedding with high probability. To embed a finite metric space (Y, d) into 2 space, Algorithm 1 takes O(m 2 · s + m 2 log 2 m) running time, where m = |Y | is the size of Y , and s is the running time needed to compute a pairwise distance d(x, y) for any x, y ∈ Y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Intuition of using LPDD. (a) Here blue points illustrate a dataset with three modes. The orange points indicate the same data but uniformly scaled up. (b) The pairwise distance distributions of both datasets are different. The distribution of orange points is a streched version of the distribution of blue points. As a result, the Wasserstein-1 distance between both distributions can become arbitrarily large, depending on the scale. (c) In contrast, the distribution of logarithmic pairwise distance remains the same up to a constant shift. In this case, the Wasserstein-1 distance of the logarithmic pairwise distance distributions is differed by only a constant addent, which can be easily accounted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Example 9 .</head><label>9</label><figDesc>Suppose M = A ∪ B ⊂ R d , where A = {0, 1} d is a Hamming cube close to the origin, and B = { Λ / √ d − 1, Λ / √ d} d is another Hamming cube far away from the origin (i.e., Λ d).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: How quickly do they converge? Our method outperforms other methods in terms of convergence rate in this example. From left to right are the samples generated after the generators are trained over an increasing number of iterations. The fifth row indicates the performance of Wasserstein GAN [4], although it is not particularly designed for addressing mode collapse. The sixth row reports the performance of BourGAN with standard GAN objective (i.e., no distance preservation term (3) is used). The seventh row indicate BourGAN with our proposed objective function, which converges in the least number of iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: MNIST dataset with different distance metrics. (left) We plot the distribution of digits generated by DCGAN in orange, BourGAN ( 2 ) in green, and BourGAN (classifier) in yellow. The generated images from those GANs are classified using a pre-trained classifier. This plot shows that the classifier distance produces samples that are most uniformly distributed across all 10 digits. DCGAN fails to capture the mode of digital "1", while BourGAN ( 2 ) generates fewer samples for the modes in "3" and "9". (right) Entropy distribution of generated samples using three GANs. A lower entropy value indicates better image quality. This plot suggests that our method with both 2 and classifier distance metrics produces higher-quality MNIST images than the DCGAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 12 :</head><label>12</label><figDesc>Figure 12: Qualitative results on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 13 :</head><label>13</label><figDesc>Figure 13: Qualitative results on Fashion-MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 14 :</head><label>14</label><figDesc>Figure 14: An 8-vertices complete graph can be decomposed into 7 perfect matchings</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>P</head><label></label><figDesc>,Q,x1,··· ,xm∼D (d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | P = Q, t ≥ pn/2) · Pr(t ≥ pn/2 | P = Q)P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | P = Q, t ≤ pn/2) · Pr(t ≤ pn/2 | P = Q) (9) ≤ Pr(d(P, Q) &lt; λ ∨ d(P, Q) &gt; Λ | P = Q, t ≥ pn/2) + Pr(t &lt; pn/</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_14"><head></head><label></label><figDesc>x, b = y | a = b) · | log(d(x, y)) − log( f (x) − f (y)x, b = y | a = b) · log d(x, y) f (x) − f (y) 2 ≤ x,y∈Y :x =y</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_15"><head></head><label></label><figDesc>x, b = y | a = b) · O(log log m) = O(log log m).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_16"><head></head><label></label><figDesc>(d(a, b) ∈ [λ, Λ]) ≥ Pr a,b∼X (a = b) − 1/Ω(n).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_17"><head></head><label></label><figDesc>a, b) ∈ [λ, Λ] | a = b) ≥ 1 − 1/(Ω(n) · Pr a,b∼X (a = b)) ≥ 1 − 1/ poly(log(Λ/λ)).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Statistics of Experiments on Synthetic Datasets</figDesc><table>Figure 5: Qualitative results on CelebA dataset using DCGAN (Left) and BourGAN (Right) under 
2 metric. It appears that DCGAN generates some samples that are visually more implausible (e.g., 
red boxes) in comparison to BourGAN. Results are fairly sampled at random, not cherry-picked. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>is evaluated and averaged over 10 trials.</figDesc><table>2D Ring 

2D Grid 
2D Circle 
1-std 
2-std 
3-std 
1-std 
2-std 
3-std 
1-std 
2-std 
3-std 
GAN 
61.46% 96.14% 99.94% 35.86% 69.86% 82.3% 
82.08% 98.26% 99.86% 
Unrolled 
70.66% 85.09% 87.96% 0.54% 
2.10% 
4.88% 
92.08% 99.35% 99.49% 
VEEGAN 51.68% 79.24% 86.76% 24.76% 60.24% 77.16% 54.72% 80.44% 89.28% 
PacGAN 
88.32% 97.28% 98.20% 28.9% 
67.76% 79.46% 58.10% 94.62% 98.62% 
BourGAN 59.54% 96.64% 99.88% 38.64% 81.54% 95.9% 
67.52% 95.64% 99.64% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Statistics of Experiments on Synthetic Datasets</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Network structure for generator. channel=3 for Stacked MNIST and channel=1 for MNIST.</figDesc><table>layer 

output size 
kernel size stride BN activation function 
input (dim 55) channel×32×32 
Conv 
256×16×16 
4 
2 
No 
LeakyReLU(0.2) 
Conv 
256×8×8 
4 
2 
Yes LeakyReLU(0.2) 
Conv 
128×4×4 
4 
2 
Yes LeakyReLU(0.2) 
Conv 
channel×1×1 
4 
1 
No 
Sigmoid 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Network structure for discriminator. Figure 11: Qualitative results on stacked MNIST dataset. (Left) Samples from real data distribution. (Middle) Samples generated by DCGAN. (Right) Samples generated by BourGAN. In all three GANs, discriminator network has a size 1 /4× of the generator. DCGAN starts to generate collapsed results, while BourGAN still generates plausible results.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Daniel Hsu, Carl Vondrick and Henrique Maia for the helpful feedback. Chang Xiao and Changxi Zheng are supported in part by the National Science Foundation (CAREER-1453101, 1717178 and 1816041) and generous donations from SoftBank and Adobe. Peilin Zhong is supported in part by National Science Foundation (CCF-1703925, CCF-1421161, CCF-1714818, CCF-1617955 and CCF-1740833), Simons Foundation (#491119 to Alexandr Andoni) and Google Research Award.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Theorem 13. Consider a metric space (M, d). Let X be a distribution over M . Let λ, Λ be two parameters such that 0 &lt; 2λ ≤ Λ. Let P be the (λ, Λ)−LPDD of X . Let y 1 , y 2 , · · · , y m be m i.i.d. samples drawn from distribution X , where m is a power of 2. Let P be the (λ, Λ)−LPDD of the uniform distribution on Y .Let γ = Pr a,b∼X (d(a, b) ∈ [λ, Λ]). Given δ ∈ (0, 1), ε ∈ (0, log(Λ/λ)),</p><p>· log log(Λ/λ) min(ε,1)γδ for some sufficiently large constant C &gt; 0, then with probability at least 1 − δ, we have W (P, P ) ≤ ε.</p><p>min(ε,1)γδ for some sufficiently large constant C &gt; 0. Let U be a uniform distribution over m samples {y 1 , y 2 , · · · , y m }. Let ε 0 = ε/2, i 0 = log 1+ε0 λ , i 1 = log 1+ε0 Λ , and α = (1 + ε 0 ). Let I be the set {i 0 , i 0 + 1, i 0 + 2, · · · , i 1 − 1, i 1 }. Then we have |I| ≤ log(Λ/λ)/ε 0 . Since P, P are (λ, Λ)−LPDD of X and uniform distribution on Y respectively, we have</p><p>Thus, to prove W (P, P ) ≤ ε = 2ε 0 , it suffices to show that</p><p>For an i ∈ I, consider Pr p∈P (p ∈ [i, i + 1) · log α), we have</p><p>where 1(·) is an indicator function. In the following parts, we will focus on giving upper bounds on the difference</p><p>and the difference</p><p>Now we look at a fixed i ∈ I. Let S be the set of all possible pairs (y j , y k ), i.e.</p><p>where the first inequality follows by plugging </p><p>In this case, we have:</p><p>By taking union bound over all i ∈ I, then with probability at least 1 − δ/2, ∀i ∈ I, we have</p><p>Thus, we have an upper bound on Equation <ref type="bibr" target="#b12">(13)</ref>. Now, let us try to derive an upper bound on Equation <ref type="bibr" target="#b13">(14)</ref>. Similar as in the previous paragraph, we let S be the set of all possible pairs (y j , y k ), i.e. </p><p>, where the first inequality follows by plugging , with probability at least 1 − δ/2, we have ∀l ∈ [2(m − 1)],</p><p>In this case, we have:</p><p>8 log 2 (Λ/λ) . </p><p>Thus now, we also obtain an upper bound for the Equation <ref type="bibr" target="#b13">(14)</ref>.</p><p>By taking union bound, we have that with probability at least 1 − δ, Equation <ref type="formula">(15)</ref> holds for all i ∈ I, and at the same time, Equation <ref type="formula">(16)</ref> holds. In the following, we condition on that Equation <ref type="formula">(15)</ref> holds for all i ∈ I, and Equation (16) also holds.</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">f-gan: Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Veegan: Reducing mode collapse in gans using implicit variational learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Akash</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lazar</forename><surname>Valkoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Michael U Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3310" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zinan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Khetan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giulia</forename><surname>Fanti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sewoong</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pacgan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04086</idno>
		<title level="m">The power of two samples in generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">On lipschitz embedding of finite metric spaces in hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Bourgain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Israel Journal of Mathematics</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="46" to="52" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rein</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2172" to="2180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Ambientgan: Generative models from lossy measurements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Compressed sensing using generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajil</forename><surname>Jalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros G</forename><surname>Dimakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03208</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Taesung</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alykhan</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun-Yan</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eli</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Generating videos with scene dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hamed</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="613" to="621" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a probabilistic latent space of object shapes via 3d generative-adversarial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chengkai</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tianfan</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bill</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="82" to="90" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xudong</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qing</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoran</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">K</forename><surname>Raymond</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><forename type="middle">Paul</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Improved training of wasserstein gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishaan</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faruk</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron C</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5769" to="5779" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yunus</forename><surname>Saatci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew G Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bayesian Gan</surname></persName>
		</author>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3622" to="3631" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Autoencoding beyond pixels using a learned similarity metric</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anders</forename><surname>Boesen Lindbo Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Søren</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.10196</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimizing the latent space of generative networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Bojanowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Armand</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Lopez-Paz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.05776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embedding finite metric spaces into normed spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matoušek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Lectures on Discrete Geometry</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="355" to="400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Courty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rémi</forename><surname>Flamary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mélanie</forename><surname>Ducoffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.07457</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Learning wasserstein embeddings. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Low-distortion embeddings of finite metric spaces. Handbook of discrete and computational geometry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jirı</forename><surname>Matoušek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Gradient descent gan optimization is locally stable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vaishnavh</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kolter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5591" to="5600" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Extensions of lipschitz mappings into a hilbert space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joram</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lindenstrauss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Contemporary mathematics</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">The geometry of graphs and some of its algorithmic applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Linial</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eran</forename><surname>London</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuri</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Combinatorica</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="215" to="245" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">An approximate max-flow min-cut theorem for uniform multicommodity flow problems with applications to approximation algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Leighton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satish</forename><surname>Rao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1988" />
			<biblScope unit="page" from="422" to="431" />
		</imprint>
	</monogr>
	<note>29th Annual Symposium on</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Pros and cons of gan evaluation measures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.03446</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Mmd gan: Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Liang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Cheng</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabás</forename><surname>Póczos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2200" to="2210" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Adagan: Boosting generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sylvain</forename><surname>Ilya O Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl-Johann</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5430" to="5439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Automatic differentiation in pytorch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zachary</forename><surname>Devito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alban</forename><surname>Desmaison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luca</forename><surname>Antiga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Lerer</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The three sigma rule</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Friedrich</forename><surname>Pukelsheim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The American Statistician</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="91" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
		<title level="m" type="main">Fashion-mnist: a novel image dataset for benchmarking machine learning algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kashif</forename><surname>Rasul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Vollgraf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07747</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">S l contains m/2 i.i.d. random samples drawn from X × X , where X × X is the joint distribution of two i.i.d. random samples a, b each with marginal distribution X . For l ∈ [2(m − 1)], by applying Bernstein inequality (see Lemma 10), we have: pair in set S l</title>
	</analytic>
	<monogr>
		<title level="m">|S l | = m/2, and furthermore, ∀l ∈ [</title>
		<imprint/>
	</monogr>
	<note>2(m − 1)], j ∈ [m], y j only appears in exactly one pair in set S l. For l ∈ [2(m − 1)], by applying Bernstein inequality (see Lemma 10), we have</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
