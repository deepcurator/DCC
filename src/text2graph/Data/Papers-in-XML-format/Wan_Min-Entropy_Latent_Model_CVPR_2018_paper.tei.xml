<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:14+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pengxu</forename><surname>Wei</surname></persName>
							<email>weipengxu11@mails.ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenjun</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<email>qxye@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Min-Entropy Latent Model for Weakly Supervised Object Detection</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Weakly supervised object detection is a challenging task when provided with image category supervision but required to learn, at the same time, object locations and object detectors. The inconsistency between the weak supervision and learning objectives introduces randomness to object locations and ambiguity to detectors. In this paper, a min-entropy latent model (MELM) is proposed for weakly supervised object detection. Min-entropy is used as a metric to measure the randomness of object localization during learning, as well as serving as a model to learn object locations. It aims to principally reduce the variance of positive instances and alleviate the ambiguity of detectors. MELM is deployed as two sub-models, which respectively discovers and localizes objects by minimizing the global and local entropy. MELM is unified with feature learning and optimized with a recurrent learning algorithm, which progressively transfers the weak supervision to object locations. Experiments demonstrate that MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, against the state-of-the-art approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Weakly supervised object detection (WSOD) solely requires image category annotations indicating the presence or absence of a class of objects in images, which significantly reduces human efforts when preparing training samples. Despite supervised object detection having become more reliable <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b14">14,</ref><ref type="bibr" target="#b15">15,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b26">26,</ref><ref type="bibr" target="#b29">29,</ref><ref type="bibr" target="#b30">30]</ref>, WSOD remains an open problem, as often indicated by low detection rates of less than 50 percent for state-of-the-art approaches <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b33">33,</ref><ref type="bibr" target="#b38">38]</ref>. Due to the lack of location annotations, WSOD approaches require learning latent objects from thousands of proposals in each image, as well as learning detectors that compromise the appearance of various ob- † Corresponding author.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>WSDDN Ours</head><p>Object Probability Learned Objects Object Probability Learned Objects Initialization Epoch 1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Epoch 2 Epoch 4</head><p>Epoch 9</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Final Epoch</head><p>Figure 1: Evolution of object locations during learning (from top to bottom). Red boxes denote proposals of high object probability, and green ones detected objects. It shows that our approach reduces localization randomness and improves localization accuracy. Best viewed in color.</p><p>jects in training images.</p><p>In the learning procedure of weakly supervised deep detection networks (WSDDN) <ref type="bibr" target="#b6">[6]</ref>, a representative WSOD approach, object locations evolved with great randomness, e.g., switching among different object parts, <ref type="figure">Fig. 1</ref>. Various object parts were capable of optimizing the learning objective, i.e., minimizing image classification loss, but experienced difficulty in optimizing object detectors due to their appearance ambiguity. The phenomenon resulted from the inconsistency between data annotations and learning objectives, i.e., image-level annotations and object-level models. It typically requires introducing latent variables and solving non-convex optimization in vast solution spaces, e.g., thousands of images and thousands of object proposals for each image, which might introduce sub-optimal solutions of considerable randomness. Recent approaches have used image segmentation <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b24">24]</ref>, context information <ref type="bibr" target="#b19">[19]</ref>, and instance classifier refinement <ref type="bibr" target="#b38">[38]</ref> to empirically regularize the learning procedure. However, the issue of quantifying sub-optimal solutions and principally reducing localization randomness remains unsolved.</p><p>In this paper, we propose a min-entropy latent model (MELM) for weakly supervised object detection, motivated by a classical thermodynamic principle: Minimizing entropy results in minimum randomness of a system. Minentropy is used as a metric to measure the randomness of object localization during learning, as well as serving as a model to learn object locations. To define the entropy, object proposals in an image are spatially separated into cliques, where spatial distributions and the probability of objects are jointly modeled. During the learning procedure, minimizing global entropy around all cliques discovers sparse proposals of high object probability, and minimizing local entropy for high-scored cliques identifies accurate object locations with minimum randomness. MELM is deployed as network branches concerning object discovery and object localization, <ref type="figure">Fig. 2</ref>, and is optimized with a recurrent stochastic gradient descent (SGD) algorithm, which progressively transfers the weak supervision, i.e., image category annotations, to object locations. By accumulating multiple iterations, MELM discovers multiple object regions, if such exist, from a single image. The contributions of this paper include:</p><p>(1) A min-entropy latent model that effectively discovers latent objects and principally minimizes the localization randomness during weakly supervised learning.</p><p>(2) A recurrent learning algorithm that jointly optimizes image classifiers, object detectors, and deep features in a progressive manner.</p><p>(3) State-of-the-art performance of weakly supervised detection, localization, and image classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>WSOD problems are often solved with a pipelined approach, i.e., an object proposal method is first applied to decompose images into object proposals, with which latent variable learning <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b5">5,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b45">45]</ref> or multiple instance learning <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b9">9,</ref><ref type="bibr" target="#b16">16,</ref><ref type="bibr" target="#b42">42]</ref> is used to iteratively perform proposal selection and classifier estimation. With the widespread acceptance of deep learning, pipelined approaches have been evolving into multiple instance learning networks <ref type="bibr">[6, 12, 17-19, 23, 28, 31, 33, 34, 38, 43, 46]</ref>. Latent Variable Learning. Latent SVM <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b45">45]</ref> learns object locations and object detectors using an EM-like optimization algorithm. Probabilistic Latent Semantic Analysis (pLSA) <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b41">41]</ref> learns object locations in a semantic clustering space. Clustering methods <ref type="bibr" target="#b5">[5,</ref><ref type="bibr" target="#b37">37]</ref> identify latent objects by discovering the most discriminative clusters. Entropy is employed in the latent variable methods <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b27">27]</ref>, but not considering the spatial relations among locations and the network fine-tuning for object detection. Various latent variable methods are required to solve the non-convex optimization problem. They often become stuck in a poor local minimum during learning, e.g., falsely localizing object parts or backgrounds. To pursue a stronger minimum, object symmetry and class mutual exclusion information <ref type="bibr" target="#b4">[4]</ref>, Nesterov's smoothing <ref type="bibr" target="#b36">[36]</ref>, and convex clustering <ref type="bibr" target="#b5">[5]</ref> have been introduced to the optimization function. These approaches can be regarded as regularization which enforces the appearance similarity among objects and reduces the ambiguity of detectors.</p><p>Multiple Instance Learning (MIL). A major approach for tackling WSOD is to formulate it as an MIL problem <ref type="bibr" target="#b2">[2]</ref>, which treats each training image as a "bag" and iteratively selects high-scored instances from each bag when learning detectors. When facing large-scale datasets, however, MIL remains puzzled by random poor solutions. The multifold MIL <ref type="bibr" target="#b8">[8,</ref><ref type="bibr" target="#b9">9]</ref> uses division of a training set and cross validation to reduce the randomness and thereby prevents training from prematurely locking onto erroneous solutions. Hoffman et al. <ref type="bibr" target="#b16">[16]</ref> train detectors with weakly annotations while transferring representations from extra object classes using full supervision (bounding-box annotation) and joint optimization. To reduce the randomness of positive instances, a bag splitting strategy has been used during the optimization procedure of MILinear <ref type="bibr" target="#b31">[31]</ref>.</p><p>Deep Multiple Instance Learning Networks. MIL has been updated to deep multiple instance learning networks <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b38">38]</ref>, where the convolutional filters behave as detectors to activate regions of interest on the deep feature maps <ref type="bibr" target="#b20">[20,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b32">32]</ref>. The beam search <ref type="bibr" target="#b3">[3]</ref> has been used to detect and localize objects by leveraging spatial distributions and informative patterns captured in the convolutional layers. To alleviate the non-convexity problem, Li et al. <ref type="bibr" target="#b23">[23]</ref> have adopted progressive optimization as regularized loss functions. Tang et al. <ref type="bibr" target="#b38">[38]</ref> propose to refine instance classifiers online by propagating instance labels to spatially overlapped instances. Diba et al. <ref type="bibr" target="#b12">[12]</ref> propose weakly supervised cascaded convolutional networks (WCCN) with multiple learning stages. It learns to produce a class activation map and candidate object locations based on imagelevel supervision, and then selects the best object locations among the candidates by minimizing the segmentation loss.</p><p>Deep multiple instance learning networks <ref type="bibr" target="#b12">[12,</ref><ref type="bibr" target="#b18">18,</ref><ref type="bibr" target="#b38">38</ref>] report state-of-the-art WSOD performance, but are misled by the problem of inconsistency between data annotations (image-level) and learning objectives (object-level  <ref type="figure">Figure 2</ref>: The proposed min-entropy latent model (MELM) is deployed as object discovery and object localization branches, which are unified with deep feature learning and optimized with a recurrent learning algorithm.</p><p>ever, their localization ability is very limited. The convolutional filters learned with image-level supervision incorporate redundant patterns, e.g., object parts and backgrounds, which cause localization randomness and model ambiguity. Recent methods have empirically used object segmentation <ref type="bibr" target="#b12">[12]</ref> and spatial label propagation <ref type="bibr" target="#b38">[38]</ref> to solve these issues. In this paper, we provide a more effective and principled way by introducing global and local entropy as a randomness metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methodology</head><p>Given image-level annotations, i.e., the presence or absence of a class of objects in images, the learning objective of our proposed MELM is to find a solution that disentangles object samples from noisy object proposals with minimum localization randomness. Accordingly, an overview of the proposed approach is presented, followed by formulation of the min-entropy latent model. We finally elaborate the recurrent learning algorithm for model optimization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>The proposed approach is implemented with an end-toend deep convolutional neural network, with two network branches added on top of the fully-connected (FC) layers, <ref type="figure">Fig. 2</ref>. The first network branch, designated as the object discovery branch, has a global min-entropy layer, which defines the distribution of object probability and targets at finding candidate object cliques by optimizing the global entropy and the image classification loss. The second branch, designated as the object localization branch, has a local min-entropy layer and a soft-max layer. The local min-entropy layer classifies the object candidates in a clique into pseudo objects and hard negatives by optimizing the local entropy and pseudo object detection loss.</p><p>In the learning phase, object proposals are generated with the Selective Search method <ref type="bibr" target="#b39">[39]</ref> for each image. An ROI-pooling layer atop the last convolutional layer is used for efficient feature extraction for these proposals.</p><p>The min-entropy latent models are optimized with a recurrently learning algorithm, which uses forward propagation to select sparse proposals as object instances, and backpropagation to optimize the parameters in the object localization branches. The object probability of each proposal is recurrently aggregated by being multiplied with the object probability learned in the preceding iteration. In the detection phase, the learned object detectors, i.e., the parameters for the soft-max and FC layers, are used to classify proposals and localize objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Min-Entropy Latent Model</head><p>Modeling. Let x ∈ X denote an image, y ∈ Y denote the label indicating whether x contains an object or not, where Y = {1, 0}. y = 1 indicates that there is at least one object in the image (positive image) while y = 0 indicates an image without any object (negative image). h denoting object locations is a latent variable and H denoting object proposals in an image is the solution space. θ denotes the network parameters. The min-entropy latent model, with object locations h * and network parameters θ * to be learned, is defined as</p><formula xml:id="formula_0">{h * , θ * } = arg min h,θ E (X ,Y) (h, θ) = arg min h,θ E d (h, θ) + E l (h, θ) ⇔ arg min h,θ L d + L l ,<label>(1)</label></formula><p>where E d (h, θ) and E l (h, θ) are the global and local entropy models. <ref type="bibr" target="#b0">1</ref> They are respectively optimized by the loss function L d and L l in the object discovery and the object localization branch, <ref type="figure">Fig. 2. (h, θ)</ref> and (X , Y) in Eq. 1 are omitted for short.</p><p>Object Discovery. The object discovery procedure is implemented by selecting those object proposals which best discriminate positive images from negative ones. Accord- ingly, a global min-entropy latent model E d (h, θ), is defined to model the probability and the spatial distribution of object probability, as</p><formula xml:id="formula_1">E d (h, θ) = − log c w Hc p Hc = − log c w Hc h∈Hc p (y, h; θ),<label>(2)</label></formula><p>where p (y, h; θ) is the joint probability of class y and latent variable h, given network parameters θ. It is calculated on the object confidences s (y, φ h ; θ) with a soft-max operation, as</p><formula xml:id="formula_2">p (y, h; θ) = exp (s (y, φ h ; θ)) y,h exp (s (y, φ h ; θ)) ,<label>(3)</label></formula><p>where φ h is the feature of object proposal h and s (·) denotes object confidence for a proposal computed by the last FC layer in the object discovery branch. w Hc , defined as</p><formula xml:id="formula_3">w Hc = 1/ |H c | h∈Hc p (y, h; θ) / y p (y, h; θ) ,<label>(4)</label></formula><p>measures the probability distribution of objects to all image classes in a spatial clique H c , <ref type="figure" target="#fig_0">Fig. 3</ref>. |·| calculates the number of elements in a clique.</p><p>The spatial cliques c, c ′ ∈ {1, ..., C} are the minimum sufficient cover to an image, i.e.,</p><formula xml:id="formula_4">C ∪ c=1 H c = H and ∀c = c ′ , H c ∩ H c ′ = ∅.</formula><p>To construct the cliques, the proposals are sorted by their object confidences and the following two steps are iteratively performed: 1) Construct a clique using the proposal of highest object confidence but not belonging to any clique. 2) Find the proposals that overlap with a proposal in the clique larger than a threshold (0.7 in this work) and merge them into the clique.</p><p>Eq. 2 and Eq. 3 show that minimizing the entropy E d (h, θ) for the positive images maximizes p (y), which means that the learning procedure selects the proposals of largest object probability to minimize image classification loss. For the negative images, all of the proposals are background and are simply modeled via a fully supervised way. Eq. 4 shows that w Hc ∈ [0, 1] is positively correlated to object confidences of the positive class in a clique, but negatively correlated to confidences of all other classes. According to the property of entropy, minimizing Eq. 2 produces a sparse selection of cliques in which proposals have significant high probability to the positive class. This sparsity of cliques with high object class confidence w Hc shows the reduction of the randomness of selected proposals.</p><p>In the learning procedure, E d (h, θ) is minimized by optimizing both the parameters in the object discovery branch and the parameters in the convolutional layers in an end-toend manner. To implement this, an SGD algorithm is used, and the loss function is defined as</p><formula xml:id="formula_5">L d = yE d (h, θ) − (1 − y) h log (1 − p (y, h; θ)). (5)</formula><p>For positive images, y = 1, the second term of Eq. 5 is zero and only E d is optimized. For negative images, y = 0, the first term of Eq. 5 is zero and the second term, image classification loss, is optimized.</p><p>Object Localization. The proposals selected by the global min-entropy model constitute good initialization for object localization, but nonetheless incorporate random false positives, e.g., objects or partial objects with backgrounds. That is a consequence of the learning objective of the object discovery branch selecting those object proposals which best discriminate positive images from negative ones, but ignoring the localization of objects. A local min-entropy model is therefore defined for accurate object localization, as</p><formula xml:id="formula_6">E l (h, θ) = − log max h∈H * c w h · p (y, h; θ),<label>(6)</label></formula><p>where H * c denotes the clique of the highest average object confidence. w h = p(y, h; θ)/ y p(y, h; θ) measures the distribution of object confidences to all image classes. Optimizing Eq. 6 produces maximum w h and sparse object proposals of high object probability p(y, h; θ), and depresses negative proposals in a clique. With optimization results, the object proposals in a clique are classified into either pseudo objects h * or hard negatives by a thresholding method, as</p><formula xml:id="formula_7">p (y, h * ; θ) = 1 if p (y, h * ; θ) &gt; τ 0 otherwise ,<label>(7)</label></formula><p>where τ = 0.6 is an empirically set threshold. With pseudo objects and hard negatives, a object detector is learned by using the loss function defined as  where f (·) denotes the object detectors with the parameters θ l of the FC layer and soft-max layer in the object localization branch, <ref type="figure">Fig. 2.</ref> </p><formula xml:id="formula_8">L l = h * − log f (h * , θ l ),<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model Learning</head><p>In MELM, the object discovery branch learns potential objects by optimizing a min-entropy latent model using image category supervision, while the object localization branch learns object classifiers using estimated pseudo objects. The objective of model learning is to transfer the image category to object locations with min-entropy constraints, i.e., minimum localization randomness.</p><p>Recurrent Learning. A recurrent learning algorithm is implemented to transfer the image-level (weak) supervision using an end-to-end forward-and back-propagation procedure. In a feed-forward procedure, the min-entropy latent models discover and localize objects which are used as pseudo-annotations for object detector learning with a back-propagation. With the learned detectors the object localization branch assigns all proposals new object probability, which is used to aggregate the object confidences with an element-wise multiply operator in the next learning iteration, <ref type="figure">Fig. 2</ref>. In the back-propagation procedure, the object discovery and object localization branches are jointly optimized with an SGD algorithm, which propagates gradients generated with image classification loss and pseudo-object detection loss. With forward-and back-propagation procedures, the network parameters are updated and the classification models and object detectors are mutually enforced. The recurrent learning algorithm is described in Alg. 1.</p><p>Accumulated Recurrent Learning. According to Eq. 6, the object localization model also performs object discovery, which may find objects different from those discovered by the object discovery model. This work extends recurrent</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Recurrent Learning</head><p>Input: Image x ∈ X , image label y ∈ Y, and region proposals h ∈ H Output: Network parameters θ and object detectors θ l 1: Initialize object confidence s (h) = s(y, φ h ; θ) = 1 for all h 2: for i = 1 to M axIter do <ref type="bibr">3:</ref> φ h ← Compute deep features for all h through forward confidence <ref type="bibr" target="#b4">4</ref>: Object localization:</p><formula xml:id="formula_9">φ ′ h ← φ h * s(h),</formula><formula xml:id="formula_10">9:</formula><p>h * ← Optimize E l using Eq. 6</p><p>10:</p><p>L l ← Compute detection loss using Eq. 8</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Network parameter update:</p><formula xml:id="formula_11">12:</formula><p>θ, θ l ← Back-propagation by using loss L d and L l</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>13:</head><p>s(h) ← Update object confidence using detectors θ l 14: end for learning to accumulated recurrent learning, <ref type="figure" target="#fig_1">Fig. 4</ref>, which accumulates different objects from both the object discovery and object localization branches, and uses them to learn object classifiers. Doing so endows this approach with the capability to localize multiple objects in a single image but also provides the robustness to process object appearance diversity by using multiple detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>The proposed MELM was evaluated on the PASCAL VOC 2007 and 2012 datasets using mean average precision (mAP) <ref type="bibr" target="#b13">[13]</ref>. Following is a description of the experimental settings, and the evaluation of the effect of min-entropy models with randomness analysis and ablation experiments. The proposed MELM is then compared with the state-ofthe-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Settings</head><p>MELM was implemented based on the widely used VGG16 CNN model <ref type="bibr" target="#b35">[35]</ref> pre-trained on the ILSVRC 2012 dataset <ref type="bibr" target="#b21">[21]</ref>. As the conventional object detection task <ref type="bibr" target="#b18">[18,</ref><ref type="bibr" target="#b31">31]</ref>, we used Selective Search <ref type="bibr" target="#b39">[39]</ref> to extract about 2000 object proposals for each image, removing those whose width or height was less than 20 pixels.</p><p>The input images were re-sized into 5 scales {480, 576, 688, 864, 1200} with respect to the larger side, height or width. The scale of a training image was randomly selected and the image was randomly horizontal flipped. In this way, each test image was augmented into a total of 10 images <ref type="bibr" target="#b6">[6,</ref><ref type="bibr" target="#b12">12,</ref><ref type="bibr" target="#b38">38]</ref>. For recurrent learning, we employed the SGD algorithm with momentum 0.9, weight decay 5e-4, and batch size 1. The model iterated 20 epochs where the learning rate was 5e-3 for the first 15 epochs and 5e-4 for  <ref type="figure" target="#fig_2">Fig. 5a</ref> shows the evolution of global and local entropy, suggesting that our approach optimizes the min-entropy objective during learning. <ref type="figure" target="#fig_2">Fig. 5b</ref> provides the gradient evolution of the FC layers. In the early learning epochs, the gradient of the global min-entropy module was slightly larger than that of the local min-entropy module, suggesting that the network focused on optimizing the image classifiers. As learning proceeded, the gradient of the global min-entropy module decreased such that the local min-entropy module dominated the training of the network, indicating that the object detectors were being optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Randomness Analysis</head><p>To evaluate the effect of min-entropy, the randomness of object locations was evaluated with localization accuracy and localization variance. Localization accuracy was calculated by weighted averaging the overlaps between the ground-truth object boxes and the learned object boxes, by using p(y, h; θ) as the weight. Localization variance was also defined as the weighted variance of the overlaps by using p(y, h; θ) as the weight. <ref type="figure" target="#fig_2">Fig. 5c</ref> and <ref type="figure" target="#fig_2">Fig. 5d</ref> show that the proposed MELM had significantly greater localization accuracy and lower localization variance than WS-DDN. This strongly indicates that our approach effectively reduces localization randomness during weakly supervised learning. Such an effect is further illustrated in <ref type="figure" target="#fig_3">Fig. 6</ref>, where the object locations learned by our approach were more accurate and less variant than those of WSDDN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Experiments</head><p>Ablation experiments were used to evaluate the respective effects of the proposal cliques, the min-entropy model, and the recurrent learning algorithm.</p><p>Baseline. The baseline approach was derived by simplifying Eq. 2 to solely model the global <ref type="figure">entropy E d (h, θ)</ref>. This is similar to WSDDN without the spatial regulariser <ref type="bibr" target="#b6">[6]</ref> where the only learning objective is to minimize the image classification loss. This baseline, referred to as "LOD-" in Tab. 1, achieved 24.7% mAP.</p><p>Clique Effect. By dividing the object proposals into cliques, the "LOD-" approach was promoted to "LOD". Tab. 1 shows that the introduction of spatial cliques improved the detection performance by 4.8% (from 24.7% to 29.5%). That occurred because using multiple cliques reduced the solution spaces of the latent variable learning, thus readily facilitating a better solution.</p><p>Multi-Entropy Latent Model. We denoted the multientropy model by "MELM-D" and "MELM-L" in <ref type="table">Table  1</ref>, which respectively corresponded to object discovery and object localization. We trained the min-entropy latent model by simply cascading the object discovery and object localization branches, without using the recurrent optimization. Tab. 1 shows that MELM-L significantly improved the baseline LOD from 29.5% to 40.1%, with a 10.6% margin at most. This fully demonstrated that the min-entropy latent model and the implementation of object discovery and object localization branches were pillars of our approach.</p><p>Recurrent Learning. In Tab. 1, the proposed recurrent learning algorithm, "MELM-D+RL" and "MELM-L+RL", respectively achieves 34.5% and 42.6% mAP, improving the "MELM-L" (without recurrent learning) by 1.9% and 2.5%. This improvement showed that with recurrent learning and the object confidence accumulation, <ref type="figure">Fig. 2</ref>, the object discovery and object localization branches benefited from each other and thus were mutually enforced.</p><p>Accumulated Recurrent Learning. When using two accumulated object localization modules, the MELM, referred to as "MELM-L2-ARL", significantly improved the mAP of the "MELM-L-RL" from 42.6% to 46.4% (+3.8%). It further improved the mAP from 46.4% to 47.3% (+0.9%) when using three accumulated detectors, but did not significantly improve when using four detectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Performance and Comparison</head><p>Weakly Supervised Object Detection. <ref type="table" target="#tab_5">Table 2</ref> shows the detection results of our MELM approach and the stateof-the-art approaches on the PASCAL VOC 2007 dataset. MELM improved the state-of-the-art to 47.3% and respectively outperformed the OICR <ref type="bibr" target="#b38">[38]</ref> 2 , Self-Taught <ref type="bibr" target="#b18">[18]</ref>, and  <ref type="bibr" target="#b6">[6]</ref>. The red solid boxes denote objects of high probability and the green solid boxes denote the detected objects. The yellow boxes in the first column denote ground-truth locations. It can be seen that the objects learned by MELM are more accurate and have less randomness, which is quantified by the localization accuracy and localization variance in the last column. Best viewed in color.   Specifically, on the "bike", "car", "chair", and "cow" classes, MELM outperformed the state-of-the-art WCCN approach up to 6∼21%. Despite of the average good performance, our approach failed on the "person" class, as shown in the last image of <ref type="figure">Fig. 7</ref>. This may have been because with a large appearance variation existing in person instances, it is difficult to learn a common appearance model. The "faces" that represent the person class with minimum randomness were falsely localized. </p><formula xml:id="formula_12">LOD- - - - - - - - - - - - - - - - - - - - -</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Dataset Splitting mAP</p><p>MILinear <ref type="bibr" target="#b31">[31]</ref> train/val 23.8 PDA <ref type="bibr" target="#b23">[23]</ref> train/val 29.1 Self-Taught <ref type="bibr" target="#b18">[18]</ref> train/val 39.0 ContextNet <ref type="bibr" target="#b19">[19]</ref> trainval/test 35.3 WCCN <ref type="bibr" target="#b12">[12]</ref> trainval/test 37.9 OICR <ref type="bibr" target="#b38">[38]</ref> trainval/test 37.9 Self-Taught <ref type="bibr" target="#b18">[18]</ref> trainval   <ref type="bibr" target="#b40">[40]</ref> 48.5 -PDA <ref type="bibr" target="#b23">[23]</ref> 52.4 -VGG16 <ref type="bibr" target="#b35">[35]</ref> -89.3 WSDDN <ref type="bibr" target="#b6">[6]</ref> 53.5 89.7 Multi-fold MIL <ref type="bibr" target="#b9">[9]</ref> 54.2 -ContextNet <ref type="bibr" target="#b19">[19]</ref> 55.1 -WCCN <ref type="bibr" target="#b12">[12]</ref> 56.7 90.9 MELM 61.4 93.1 <ref type="table">Table 4</ref>: Correct localization rate (%) and image classification average precision (%) on PASCAL VOC 2007. Comparison of MELM to the state-of-the-arts.</p><p>approach. By accumulating proposals of high confidences, MELM localized multiple object regions and therefore learned more discriminative detectors.</p><p>Weakly Supervised Object Localization. The Correct Localization (CorLoc) metric <ref type="bibr" target="#b11">[11]</ref> was employed to evaluate the localization accuracy. CorLoc is the percentage of images for which the region of highest object confidence has at least 0.5 interaction-over-union (IoU) with one of the ground-truth object regions. This experiment was done on the trainval set because the region selection exclusively worked in the training process. Tab. 4 shows that the mean CorLoc of MELM outperformed the state-of-the-art WCCN <ref type="bibr" target="#b12">[12]</ref> by 4.7% ( 61.4% vs. 56.7%). This shows that the minentropy strategy used in our approach was more effective for object localization than the image segmentation strategy used in WCCN.</p><p>Image Classification. The object discovery and object localization functionality of MELM highlights informative regions and suppresses disturbing backgrounds, which also benefits the image classification task. As shown in Tab. <ref type="bibr" target="#b4">4</ref>, with the VGG16 model, MELM achieved 93.1% mAP, which respectively outperformed WSDDN <ref type="bibr" target="#b6">[6]</ref> and WCCN <ref type="bibr" target="#b12">[12]</ref> up to 3.4% and 2.2%. It is noteworthy that MELM outperforms the VGG16 network, which was specifically trained for image classification, by 3.8% mAP (93.1% vs. 89.3%). This shows that the min-entropy latent model learned more representative feature representations by reducing the localization randomness of informative regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a simple but effective minentropy latent model (MELM) for weakly supervised object detection. MELM was deployed as two submodels of object discovery and object localization, and was unified with the deep learning framework in an end-to-end manner. Our approach, by leveraging the sparsity produced with a minentropy model, provides a new way to learn latent object regions. With the well-designed recurrent learning algorithm, MELM significantly improves the performance of weakly supervised detection, weakly supervised localization, and image classification, in striking contrast with state-of-theart approaches. The underlying reality is that min-entropy results in minimum randomness of an information system, which provides fresh insights for weakly supervised learning problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Object proposal cliques. The left column is an exemplar clique partition. The right-top shows some of the corresponding cliques in the image. The right-bottom shows the object confidence map of cliques.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: The flowchart of the proposed recurrent learning algorithm. The black solid lines denote network connections and orange dotted lines denote forward-only connections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Localization, gradient, and entropy on the VOC 2007 dataset. (a) the evolution of entropy and (b) gradient. (c) localization accuracy and (d) localization variance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Comparison about object localization of our MELM to WSDDN [6]. The red solid boxes denote objects of high probability and the green solid boxes denote the detected objects. The yellow boxes in the first column denote ground-truth locations. It can be seen that the objects learned by MELM are more accurate and have less randomness, which is quantified by the localization accuracy and localization variance in the last column. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 7 Figure 7 :</head><label>77</label><figDesc>Fig. 7 shows some of the detection results of the MELM</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>). With image-level annotations, such networks are capable of learning effective image representations for image clas- sification. Without object bounding-box annotations, how-</figDesc><table>FC 

Image 
Classification 
Loss 

Object Discovery 

Object Localization 

Local 
Min-Entropy 

Global 
Min-Entropy 

Object Probability 

Object 
Detection 
Loss 

Image 
Label 

FC 
FC 
CONVs 
with ROI 
Pooling 

Object Confidence 

Pseudo 
Objects 

Region Proposals 

Input Image 

Element-wise 
Multiplication 

Object Confidence Map 

Object Probability Map 

Object Probability Map 

Network Connection 
Forward-only Connection 

Softmax 

FC 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 1: Detection average precision (%) on the PASCAL VOC 2007 test set. Ablation experimental results of MELM.aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAP58.0 62.4 31.1 19.4 13.0 65.1 62.2 28.4 24.8 44.7 30.6 25.3 37.8 65.5 15.7 24.1 41.7 46.9 64.3 62.6 41.2</figDesc><table>24.7 
LOD 
32.2 49.6 15.9 8.1 
5.0 51.1 44.8 22.3 16.6 35.3 24.0 20.4 31.0 57.1 9.8 15.3 30.9 31.7 50.1 37.8 29.5 
MELM-D 
36.3 47.1 19.7 13.4 3.1 61.4 52.6 12.8 13.9 40.5 33.3 12.6 29.6 62.1 10.1 17.5 35.0 48.7 60.4 41.3 32.6 
MELM-L 
49.5 54.4 26.2 19.7 12.9 59.4 63.0 39.2 22.3 46.9 39.1 36.2 43.2 64.2 2.6 21.3 40.1 48.9 57.9 54.4 40.1 
MELM-D+RL 
37.4 56.8 27.4 13.1 4.4 59.2 52.0 25.8 20.3 41.5 33.1 21.3 32.8 60.0 10.0 11.6 35.7 43.6 57.2 47.3 34.5 
MELM-L+RL 
50.4 57.6 37.7 23.2 13.9 60.2 63.1 44.4 24.3 52.0 42.3 42.7 43.7 66.6 2.9 21.4 45.1 45.2 59.1 56.2 42.6 
MELM-D+ARL 
42.1 61.2 26.5 17.3 7.8 61.4 55.6 20.2 21.3 46.3 35.3 36.7 37.0 63.1 1.2 18.7 38.9 52.0 57.8 48.0 37.4 
MELM-L1+ARL 
51.3 66.9 36.1 28.1 15.5 68.6 67.1 37.3 24.8 65.2 45.1 50.7 46.9 67.5 2.1 25.3 51.3 56.4 62.9 59.0 46.4 
MELM-L2+ARL 
55.6 66.9 34.2 29.1 16.4 68.8 68.1 43.0 25.0 65.6 45.3 53.2 49.6 68.6 2.0 25.4 52.5 56.8 62.1 57.1 47.3 

Method 
MILinear [31] 
41.3 39.7 22.1 9.5 
3.9 41.0 45.0 19.1 1.0 34.0 16.0 21.3 32.5 43.4 21.9 19.7 21.5 22.3 36.0 18.0 25.4 
Multi-fold MIL [9] 
39.3 43.0 28.8 20.4 8.0 45.5 47.9 22.1 8.4 33.5 23.6 29.2 38.5 47.9 20.3 20.0 35.8 30.8 41.0 20.1 30.2 
LCL+Context [40] 
48.9 42.3 26.1 11.3 11.9 41.3 40.9 34.7 10.8 34.7 18.8 34.4 35.4 52.7 19.1 17.4 35.9 33.3 34.8 46.5 31.6 
WSDDN [6] 
39.4 50.1 31.5 16.3 12.6 64.5 42.8 42.6 10.1 35.7 24.9 38.2 34.4 55.6 9.4 14.7 30.2 40.7 54.7 46.9 34.8 
PDA [23] 
54.5 47.4 41.3 20.8 17.7 51.9 63.5 46.1 21.8 57.1 22.1 34.4 50.5 61.8 16.2 29.9 40.7 15.9 55.3 40.2 39.5 
OICR [38] 

2 

Self-Taught [18] 
52.2 47.1 35.0 26.7 15.4 61.3 66.0 54.3 3.0 53.6 24.7 43.6 48.4 65.8 6.6 18.8 51.9 43.6 53.6 62.4 41.7 
WCCN [12] 
49.5 60.6 38.6 29.2 16.2 70.8 56.9 42.5 10.9 44.1 29.9 42.2 47.9 64.1 13.8 23.5 45.9 54.1 60.8 54.5 42.8 
MELM 
55.6 66.9 34.2 29.1 16.4 68.8 68.1 43.0 25.0 65.6 45.3 53.2 49.6 68.6 2.0 25.4 52.5 56.8 62.1 57.1 47.3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Detection average precision (%) on the PASCAL VOC 2007 test set. Comparison of MELM to the state-of-the-arts. WCCN [12] by 6.1%, 5.6%, and 4.5%. In Table 3, the detection comparison results on the PASCAL VOC 2012 datasets are provided. MELM respectively outperformed the OICR [38], Self-Taught [18], and WCCN [12] by 4.5%, 4.1%, and 4.1%, which were significant margins for the challenging WSOD task.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Detection average precision (%) on the VOC 2012 test set. Comparison of MELM to the state-of-the-arts.</figDesc><table>Method 
Localization (mAP) 
Classification (mAP) 

MILinear [31] 
43.9 
72.0 
LCL+Context </table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The entropy here is Aczél and Daróczy (AD) entropy [1].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">This work reported a higher performance (47.0%) with multiple networks ensemble and Fast-RCNN re-training. For a fair comparison, the performance of OICR using a single VGG16 model is used.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This work is partially supported by the NSFC under Grant 61671427, 61771447, 61601466, and Beijing Municipal Science and Technology Commission.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Charakterisierung der entropien positiver ordnung und der shannonschen entropie</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aczél</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Daróczy</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica Academiae Scientiarum Hungarica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="95" to="121" />
			<date type="published" when="1963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Support vector machines for multiple-instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Andrews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tsochantaridis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="561" to="568" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with posterior regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Brit. Mach. Vis. Conf. (BMVC)</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weakly supervised object detection with convex clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pedersoli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1081" to="1089" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Weakly supervised deep detection networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2846" to="2854" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Entropy-based latent structured output prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bouchacourt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M. Pawan</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2920" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Multi-fold mil training for weakly supervised object lcalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshop</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. Workshop</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2409" to="2416" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with multi-fold multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">G</forename><surname>Cinbis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="189" to="203" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Weakly supervised localization and learning with generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Deselaers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">100</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="275" to="293" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Weakly supervised cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Diba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pazandeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5131" to="5139" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="580" to="587" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Detector discovery in the wild: Joint multiple instance and representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">797823</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diagnosing error in object detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chodpathumwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="340" to="353" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep self-taught learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4294" to="4302" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Contextlocnet: Context-aware deep network models for weakly supervised localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kantorov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="350" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Two-phase learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kweon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hide-and-seek: Forcing a network to be meticulous for weakly-supervised object and action localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kumar</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y. Jae</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with progressive domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3512" to="3520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Image colocalization by mimicking a good detectors confidence score distribution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="19" to="34" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Feature pyramid networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="936" to="944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max-margin min-entropy models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Packer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goodman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Artificial Intelligence and Statistics</title>
		<imprint>
			<biblScope unit="page" from="779" to="787" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Is object localization for free? weakly supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="685" to="694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Weakly supervised large scale object localization with multiple instance learning and bag splitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="405" to="416" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grad-cam: Visual explanations from deep networks via gradient-based localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using things and stuff transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization using size estimates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="105" to="121" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">On learning to localize objects with minimal supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mairal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 31st Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>31st Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1611" to="1619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Weakly supervised discovery of visual pattern configurations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Adv. in Neural Inf. Process. Syst. (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Multiple instance detection network with online instance classifier refinement</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3059" to="3067" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vis</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="154" to="171" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Large-scale weakly supervised object localization via latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maybank</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1371" to="1385" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Weakly supervised object localization with latent category learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Europ. Conf. Comput. Vis. (ECCV)</title>
		<meeting>Europ. Conf. Comput. Vis. (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="431" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Relaxed multipleinstance svm with application to object discovery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1224" to="1232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep multiple instance learning for image classification and auto-annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3460" to="3469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self-learning scene-specific pedestrian detectors using a progressive latent model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sapiro</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. Pattern Recognit. (CVPR)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2057" to="2066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning structural svms with latent variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-N</forename><forename type="middle">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. 26st Int. Conf. Mach. Learn. (ICML)</title>
		<meeting>26st Int. Conf. Mach. Learn. (ICML)</meeting>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Int. Conf. Comput. Vis. (ICCV)</title>
		<meeting>IEEE Int. Conf. Comput. Vis. (ICCV)</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1841" to="1850" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
