<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Boosting Domain Adaptation by Discovering Latent Domains</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Massimiliano</forename><surname>Mancini</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<addrLine>2 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Mapillary Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
							<email>samuel@mapillary.com</email>
							<affiliation key="aff1">
								<orgName type="department">Mapillary Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Caputo</surname></persName>
							<email>caputo@diag.uniroma1.it</email>
							<affiliation key="aff0">
								<orgName type="institution">Sapienza University of Rome</orgName>
								<address>
									<addrLine>2 Fondazione Bruno Kessler</addrLine>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Italian Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elisa</forename><surname>Ricci</surname></persName>
							<email>eliricci@fbk.eu</email>
							<affiliation key="aff3">
								<orgName type="institution">University of Trento</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Boosting Domain Adaptation by Discovering Latent Domains</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The problem that trained models perform poorly when tested on data from a different distribution is commonly referred to as domain shift. This issue is especially relevant in computer vision, as visual data is characterized by large appearance variability, e.g. due to differences in resolution, changes in camera pose, occlusions and illumination variations. To address this problem, several transfer learning and domain adaptation approaches have been proposed in the last decade <ref type="bibr" target="#b34">[35]</ref>.</p><p>Domain Adaptation (DA) methods are specifically designed to transfer knowledge from a source domain to the domain of interest, i.e. the target domain, in the form of learned models or invariant feature representations. The problem has been widely studied and both theoretical re- The idea behind our framework. We propose a novel deep architecture which, given a set of images, automatically discover multiple latent source domains and use this information to align the distributions of the internal CNN feature representations of sources and target domains for the purpose of domain adaptation. Image better seen at magnification.</p><p>sults <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b32">33]</ref> and several shallow <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b29">30]</ref> and deep learning algorithms have been developed <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b39">40]</ref>. While deep neural networks tend to produce more transferable and domain-invariant features, previous works <ref type="bibr" target="#b7">[8]</ref> have shown that the domain shift is only alleviated but not removed.</p><p>Most works on DA focus on a single-source and singletarget scenario. However, in many computer vision applications labeled training data is often generated from multiple distributions, i.e. there are multiple source domains. Examples of multi-source DA problems arise when the source set corresponds to images taken with different cameras, collected from the web or associated to multiple points of views. In these cases, a naive application of single-source DA algorithms would not suffice, leading to poor results. Therefore, in the past several research efforts have been devoted to develop DA methods operating on multiple sources <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. These approaches assume that the different source domains are known. A more challenging problem arises when labeled training data correspond to latent source domains, i.e. we can make a reasonable estimate on the number of source domains available, but we have no information, or only partial, about domain labels. To address this problem, known in the literature as latent domain discovery, previous works have proposed methods which simultaneously discover hidden source domains and use them to learn the target classification models <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>.</p><p>This paper introduces the first deep approach able to automatically discover latent source domains in multi-source domain adaptation settings. Our method is inspired by the recent works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, which revisit Batch Normalization layers <ref type="bibr" target="#b22">[23]</ref> for the purpose of domain adaptation, introducing specific Domain Alignment layers (DA-layers). The main idea behind DA-layers is to cope with domain shift by aligning representations of source and target distributions to a reference Gaussian distribution. Our approach develops from the same intuition. However, to address the additional challenges of discovering and handling multiple latent domains, we propose a novel architecture which is able to (i) learn a set of assignment variables which associate source samples to a latent domain and (ii) exploit this information for aligning the distributions of the internal CNN feature representations and learn a robust target classifier <ref type="figure" target="#fig_4">(Fig.2</ref>). Our experimental evaluation shows that the proposed approach alleviates the domain discrepancy and outperforms previous multi-source DA techniques on popular benchmarks, such as Office-31 <ref type="bibr" target="#b35">[36]</ref> and Office-Caltech <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>DA methods with hand-crafted features. Earlier DA approaches operate on hand-crafted features and attempt to reduce the discrepancy between the source and the target domains by adopting different strategies. For instance, instance-based methods <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b41">42]</ref> develop from the idea of learning classification/regression models by re-weighting source samples according to their similarity with the target data. A different strategy is exploited by feature-based methods, coping with domain shift by learning a common subspace for source and target data such as to obtain domain-invariant representations <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b29">30]</ref>. Parameterbased methods <ref type="bibr" target="#b42">[43]</ref> address the domain shift problem by discovering a set of shared weights between the source and the target models. However, they usually require labeled target data which is not always available.</p><p>While most earlier DA approaches focus on a singlesource and single-target setting, some works have considered the related problem of learning classification models when the training data spans multiple domains <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>. The common idea behind these methods is that when source data arises from multiple distributions, adopting a single source classifier is suboptimal and improved performance can be obtained leveraging information about multiple domains. However, these methods assume that the domain labels for all source samples are known in advance. In practice, in many applications the information about domains is hidden and latent domains must be discovered into the large training set. Few works have considered this problem in the literature. Hoffman et al. <ref type="bibr" target="#b20">[21]</ref> address this task by modeling domains as Gaussian distributions in the feature space and by estimating the membership of each training sample to a source domain using an iterative approach. Gong et al. <ref type="bibr" target="#b15">[16]</ref> discover latent domains by devising a nonparametric approach which aims at simultaneously achieving maximum distinctiveness among domains and ensuring that strong discriminative models are learned for each latent domain. In <ref type="bibr" target="#b40">[41]</ref> domains are modeled as manifolds and source images representations are learned decoupling information about semantic category and domain. By exploiting these representations the domain assignment labels are inferred using a mutual information based clustering method.</p><p>Deep Domain Adaptation. Most recent works on DA consider deep architectures and robust domain-invariant features are learned using either supervised neural networks <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref>, deep autoencoders <ref type="bibr" target="#b43">[44]</ref> or generative adversarial networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b36">37]</ref>. For instance, some methods attempt to align source and target features by minimizing Maximum Mean Discrepancy <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. Other approaches operate in a domain-adversarial setting, i.e. learn domain-agnostic representations by maximizing a domain confusion loss <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. Domain separation networks are proposed in <ref type="bibr" target="#b4">[5]</ref>, where feature representations are learned by decoupling the domain-specific component from a shared one. DA-layers are described in <ref type="bibr" target="#b6">[7]</ref> which, embedded into an arbitrary CNN architecture, are able to align source and target representation distributions.</p><p>While recent deep DA methods significantly outperform approaches based on hand-crafted features, they only consider single-source, single-target settings. To our knowledge, this is the first work proposing a deep architecture for discovering latent source domains and exploiting them for improving classification performance on target data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation and Notation</head><p>In this paper we are interested in predicting labels from an output space Y (e.g. object or scene categories), given elements of an input space X (e.g. images). We further assume that our data belongs to one of several domains: the k source domains, characterized by unknown probability distributions p n }, the source data and label sets, respectively. We assume to know the domain label for a (possibly empty) subsetŜ ⊂ S of source data samples and we denote by dŜ the domain labels in {s 1 . . . . , s k } of the sample points in xŜ. The set of domains labels, including target domain, is given by D = {s 1 , . . . , s k , t}.</p><p>Our main goal is to learn a predictor that is able to classify samples from the target domain. When tackling this problem we have to deal with three main difficulties: (i) the distributions of source(s) and target can be drastically different, making it hard to apply a classifier learned on one domain to the others, (ii) we lack direct observation of target labels, and (iii) the assignment of each source sample to its domain is unknown, or known for a very limited number of samples only.</p><p>Several previous works <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b39">40]</ref> have tackled the related problem of domain adaptation in the context of deep neural networks, dealing with (i) and (ii) in the case in which all source data comes from a single domain. In particular, some recent works have demonstrated a simple yet effective approach based on the replacement of standard Batch Normalization layers with specific Domain Alignment layers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>. These layers aim to reduce internal domain shift at different levels within the network by renormalizing features in a domain-dependent way, matching their distributions to a pre-determined one. In the following sections we show how the same idea can be revisited to naturally tackle the case of multiple, unknown source domains. In particular, we propose a novel Multi-domain DA layer (mDA-layer), detailed in Section 3.2, which is able to re-normalize the multi-modal feature distributions encountered in our setting. To do this, our mDA-layers exploit a side-output branch we attach to the main network (see Section 3.3), which predicts domain assignment probabilities for each input sample. Finally, in Section 3.4 we show how the predicted domain probabilities can be exploited, together with the unlabeled target samples, to construct a prior distribution over the network's parameters which is then used to define the training objective for our network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Multi-domain DA-layers</head><p>DA-layers <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref> are motivated by the observation that, in general, activations within a neural network follow domain-dependent distributions. As a way to reduce domain shift, the activations are thus normalized in a domainspecific way, shifting them according to a parameterized transformation in order to match their first and second order moments to those of a reference distribution, which is generally chosen to be normal with zero mean and unit standard deviation. While previous works only considered settings with two domains, i.e. source and target, the basic idea can in fact be applied to any number of domains, as long as the domain membership of each sample is known. Specifically, denoting as q d x the distribution of activations for a given feature channel and domain d, an input</p><formula xml:id="formula_0">x d ∼ q d</formula><p>x to the DA-layer can be normalized according to</p><formula xml:id="formula_1">DA(x d ; µ d , σ d ) = x d − µ d σ 2 d + ǫ , where µ d = E x∼q d x [x], σ 2 d = Var x∼q d x [x]</formula><p>and ǫ &gt; 0 is a small constant to avoid numerical issues. When the statistics µ d and σ 2 d are computed over the current batch, this equates in practice to applying standard Batch Normalization separately to the samples of each domain.</p><p>As mentioned above, this approach requires full domain knowledge, as, for each d, µ d and σ 2 d need to be calculated on the specific samples belonging to d. In our case, however, while the target is clearly distinct from the source, we do not know which specific source domain most or even all of the source samples belong to. To tackle this issue, we propose to model the layer's input distribution as a mixture of Gaussians, with one component for each domain. Specifically, we define a global input distribution</p><formula xml:id="formula_2">q x = d π d q d x ,</formula><p>where π d is the probability of sampling from domain d, and q  </p><formula xml:id="formula_3">µ d = b i=1 α i,d x i , σ 2 d = b i=1 α i,d (x i − µ d ) 2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_4">α i,d = q d|x (d | x i ) b i=1 q d|x (d | x i ) ,<label>(2)</label></formula><p>and q d|x (d | x i ) is the conditional probability of x i belonging to d, given x i . Clearly, the value of q d|x is known for all samples for which we have domain information. In all other cases, the missing domain assignment probabilities are inferred from data, using the domain prediction network branch which will be detailed in Section 3.3. Thus, from the perspective of the alignment layer, these probabilities become an additional input, which we denote as w i,d for the predicted probability of x i belonging to d. <ref type="formula" target="#formula_3">(1)</ref> and <ref type="formula" target="#formula_4">(2)</ref>, we obtain a new set of empirical estimates for the mixture parameters, which we denote asμ d andσ 2 d . These parameters are used to normalize the layer's inputs according to</p><formula xml:id="formula_5">By substituting w i,d for q d|x (d | x i ) in</formula><formula xml:id="formula_6">mDA(x i , w i ;μ,σ) = d∈D w i,d x i −μ d σ 2 d + ǫ ,<label>(3)</label></formula><p>where</p><formula xml:id="formula_7">w i = {w i,d } d∈D ,μ = {μ d } d∈D andσ = {σ 2 d } d∈D .</formula><p>As in previous works <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23]</ref>, during back-propagation we calculate the derivatives through the statistics and weights, propagating the gradients to both the main input and the domain assignment probabilities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Domain prediction</head><p>As explained in the previous Section 3.2, our mDAlayers take as input a set of domain assignment probabilities for each input sample, which need to be predicted. While different mDA-layers in a network have in general different input distributions, the assignment of sample points to domains should be coherent across them. Specifically, sample points at different mDA-layers corresponding to a single input element to the network should share the same probabilities. As a practical example, in the typical case in which mDA-layers are used in a CNN to normalize convolutional activations, the network would predict a single set of probabilities for each input image, which would then be given as input to all mDA-layers and broadcasted across all spatial locations and feature channels corresponding to that image. Following these consideration, we compute domain assignment probabilities using a distinct section of the network, which we call the domain prediction branch, while we refer to the main section of the network as the classification branch. The two branches share the bottom-most layers and parameters as depicted in <ref type="figure" target="#fig_4">Figure 2</ref>. The domain prediction branch is implemented as a minimal set of layers followed by a soft max operation with k outputs for the k latent source domains (more details follow in Section 4).</p><p>As</p><note type="other">the domain membership of target samples is always assumed to be known, we do not predict domain assignment probabilities for the target. Furthermore, for each sample x i with known domain membershipd, we fix in each mDAlayer w i,d = 1 if d =d, otherwise w i,d = 0 .</note><p>We split the network into a domain prediction branch and classification branch at some low level layer. This choice is motivated by the observation <ref type="bibr" target="#b0">[1]</ref> that features tend to become increasingly more domain invariant going deeper into the network, meaning that it becomes increasingly harder to compute a sample's domain as a function of deeper features. In fact, as pointed out in <ref type="bibr" target="#b5">[6]</ref>, this phenomenon is even more evident in networks that include Domain Alignment layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training the network</head><p>We want to estimate θ ∈ Θ, which comprises all trainable parameters of the classification and domain prediction branches, while taking advantage of both labeled and unlabeled data. A main difficulty lies in the fact that, when employing a discriminative model, the unlabeled samples cannot be used to express the data likelihood. However, following the approach sketched in <ref type="bibr" target="#b5">[6]</ref>, we can exploit the unlabeled data to define a prior distribution over the network's parameters. By doing this, we define a posterior distribution over θ given all data and labels as follows:</p><formula xml:id="formula_8">π(θ|S, T ,Ŝ) ∝ π(y S |x S , θ) · π(dŜ|xŜ , θ)π(θ|T )π(θ|x S\Ŝ ),<label>(4)</label></formula><p>where for notational convenience we have omitted some dependences. By maximizing (4) over Θ we obtain a maximum-a-posterior estimateθ for the parameters:</p><formula xml:id="formula_9">θ ∈ arg max θ∈Θ π(θ|S, T ,Ŝ).<label>(5)</label></formula><p>The first term on the right hand side of <ref type="formula" target="#formula_8">(4)</ref> is the likelihood of θ w.r.t. the source dataset, and can be written as </p><formula xml:id="formula_10">π(y S |x S , θ) = n i=1 f θ C (y s i ; x s i )<label>(6)</label></formula><formula xml:id="formula_11">π(dŜ |xŜ , θ) = xi∈xŜ f θ D (d i ; x i ),</formula><note type="other">where d i is the domain corresponding to x i ∈ xŜ . In the previous equation, f θ D (d; x) denotes the output of the domain prediction branch for a sample x and domain d, i.e. the predicted probability of x belonging to d.</note><p>To define our prior π(θ|T ) over the parameters, we exploit all available unlabeled data, biasing our classifier towards exhibiting low uncertainty on the unlabeled samples, similarly to <ref type="bibr" target="#b5">[6]</ref>. However, in addition, we introduce a prior term π(θ|x S\Ŝ ), which exploits source sample points with missing domain labels. Uncertainty when predicting class labels can be measured in terms of the empirical entropy</p><formula xml:id="formula_12">h C (θ|x S ) = − 1 m m i=1 y∈Y f θ C (y; x t i ) log f θ C (y; x t i ),</formula><p>and similarly for the uncertainty when predicting domains.</p><formula xml:id="formula_13">h D (θ|x S\Ŝ ) = − 1 |x S\Ŝ | x∈x S\Ŝ k i=1 f θ D (s i ; x) log f θ D (s i ; x).</formula><p>Now, π(θ|T ) can be obtained as the distribution with maximum entropy under the constraints π(θ|x S )h C (θ|x S )dθ = ε C and, similarly, π(θ|x S\Ŝ ) can be regarded as a maximum entropy distribution under the constraint π(θ|x S\Ŝ )h D (θ|x S\Ŝ )dθ = ε D , where ε C &gt; 0 and ε D &gt; 0 define the desired average uncertainties for class and domain predictions, respectively. These optimization problems can be shown to have solutions:</p><formula xml:id="formula_14">π(θ|T ) ∝ exp(−λ C h C (θ|T )) π(θ|x S\Ŝ ) ∝ exp(−λ D h D (θ|x S\Ŝ )),</formula><p>where λ C and λ D are the Lagrange multipliers corresponding to ε C and ε D , respectively.</p><p>In practice, the optimization in (5) can be replaced by the equivalent minimization of the negative logarithm of the likelihood, obtaining our loss function:</p><formula xml:id="formula_15">L(θ) = − 1 n n i=1 log f θ C (y s i ; x s i ) − λ t 1 |xŜ | xi∈xŜ log f θ D (d i ; x i ) − λ C 1 m m i=1 y∈Y f θ C (y; x t i ) log f θ C (y; x t i ) − λ D 1 |x S\Ŝ | x∈x S\Ŝ k i=1 f θ D (s i ; x) log f θ D (s i ; x). (7)</formula><p>The four terms, balanced by the hyper-parameters λ t , λ C and λ D , can be interpreted as two log-losses and two entropy losses applied to the classification and domain prediction branches of the network, respectively to samples with known and unknown labels. Interestingly, since the classification branch has a dependence on the domain prediction branch via the mDA-layers, by optimizing (7), the network learns to predict domain assignment probabilities that result in a low classification loss. In other words, the network is free to predict domain memberships that do not necessarily reflect the real ones, as long as this helps improving its classification performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets. In our evaluation we consider several common DA benchmarks: the combination of the USPS <ref type="bibr" target="#b10">[11]</ref>, MNIST <ref type="bibr" target="#b25">[26]</ref> and MNIST-m <ref type="bibr" target="#b11">[12]</ref> datasets, the Office-31 <ref type="bibr" target="#b35">[36]</ref> dataset, Office-Caltech <ref type="bibr" target="#b16">[17]</ref> and the PACS <ref type="bibr" target="#b26">[27]</ref> dataset.</p><p>MNIST, MNIST-m and USPS are three standard datasets for digits recognition. USPS <ref type="bibr" target="#b10">[11]</ref> is a dataset built using digits scanned from U.S. envelopes, MNIST <ref type="bibr" target="#b25">[26]</ref> is the popular benchmark for digits recognition and MNISTm <ref type="bibr" target="#b11">[12]</ref> its counterpart obtained by blending the original images with colored patches extracted from BSD500 photos <ref type="bibr" target="#b1">[2]</ref>. Due to their different representations (e.g. colored vs gray-scale), these datasets have been adopted as a DA benchmark by many previous works <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b11">12]</ref>. Here, we consider a multi source DA setting, using MNIST and MNIST-m as sources and USPS as target, training on the union of the training sets and testing on the test set of USPS.</p><p>Office-31 is a standard DA benchmark which contains images of 31 object categories collected from 3 different sources: Webcam (W), DSLR camera (D) and the Amazon website (A). Following <ref type="bibr" target="#b40">[41]</ref>, we perform our tests in the multi-source setting, where each domain is in turn considered as target, while the others are used as source.</p><p>Office-Caltech <ref type="bibr" target="#b16">[17]</ref> is obtained by selecting the subset of 10 common categories in the Office31 and the Cal-tech256 <ref type="bibr" target="#b18">[19]</ref> datasets. It contains 2533 images, about half of which belong to Caltech256. The different domains are Amazon (A), DSLR (D), Webcam (W) and Caltech256 (C). In our experiments we consider the set of source/target combinations used in <ref type="bibr" target="#b15">[16]</ref>.</p><p>PACS <ref type="bibr" target="#b26">[27]</ref> is a recently proposed benchmark which is especially interesting due to the significant domain shift between different domains. It contains images of 7 categories (dog, elephant, giraffe, guitar, horse) extracted from 4 different representations, i.e. Photo (P), Art paintings (A), Cartoon (C) and Sketch (S). Following the experimental protocol in <ref type="bibr" target="#b26">[27]</ref>, we train our model considering 3 domains as sources and the remaining as target, using all the images of each domain.Differently from <ref type="bibr" target="#b26">[27]</ref> we consider a DA setting (i.e. target data is available at training time) and we do not address the problem of domain generalization.</p><p>Networks and training protocols. We apply our approach to three different CNN architectures: the MNIST network described in <ref type="bibr" target="#b11">[12]</ref>, AlexNet <ref type="bibr" target="#b24">[25]</ref> and ResNet <ref type="bibr" target="#b19">[20]</ref>. We choose AlexNet due to its widespread use in state of the art DA approaches <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32]</ref>, while ResNet is taken as an exemplar for recent architectures employing batchnormalization layers. Both AlexNet and ResNet are first pre-trained on ImageNet and then fine-tuned on the datasets of interest. The MNIST architecture in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b12">13]</ref> is chosen following previous works considering digits datasets.</p><p>For the evaluation on digits datasets we employ the MNIST architecture described in <ref type="bibr" target="#b11">[12]</ref>. Since the original architecture does not contain BN layers, we add mDA-layers after each layer with parameters. We train the architecture following the schedule defined in <ref type="bibr" target="#b11">[12]</ref>, with a batch-size containing 128 images per domain. The side-branch starts from the conv1 layer, applies a second convolution with the same parameters of conv2 and a fully-connected layer with 100 output channel, before the final domain-classifier.</p><p>For the experiments on the Office-31 and Office-Caltech datasets we employ the AlexNet architecture. We follow a setup similar to the one proposed in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref>, fixing the parameters of all convolutional layers with mDA-layers inserted following each fully-connected layer and before their corresponding activation functions. The domain prediction branch is attached to the last pooling layer following conv5. It is composed of a global average pooling, followed by a fully connected layer and a softmax operation to produce the final domain probabilities. The training schedule and hyperparameters are set following <ref type="bibr" target="#b5">[6]</ref>.</p><p>For the experiments on the PACS dataset we consider the ResNet architecture and we choose the 18-layers setup described in <ref type="bibr" target="#b19">[20]</ref> and denoted as ResNet18. This architecture comprises an initial 7 × 7 convolution, denoted as conv1, followed by 4 main modules, denoted as conv2 -conv5, each containing two residual blocks. To apply our approach, we replace each Batch Normalization layer in the network with an mDA-layer. The domain prediction branch is attached to conv1, and is formed by adding a residual block (with the same number of filters as the ones in conv2) and a global average pooling layer followed by a fully connected layer and a softmax. For training we use a weight-decay of 10 −6 , with the same initial learning rate and momentum adopted for AlexNet. The network is trained for 1200 iterations with a batch-size of 48, equally divided between the domains. The learning rate is scaled by a factor 0.1 after 75% of the iterations. More details about the training procedures can be found in the supplementary material.</p><p>Regarding the hyper-parameters of our method, we set the number of source domains k equal to Q − 1, where Q is the number of different datasets used in each single experiment. Following <ref type="bibr" target="#b5">[6]</ref>, in the experiments with AlexNet architecture we fix λ C = λ D = 0.2. Similarly, for the experiments on digits classification, we keep the weights λ C , λ D of the two entropy losses fixed to the same value (0.1). For ResNet we select the values λ C = 0.1 and λ D = 0.0001 through cross-validation, following the procedure adopted in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b29">30]</ref>. When domain labels are available for a subset of source samples, we fix λ t = 0.5.</p><p>We implement 1 all the models with the Caffe <ref type="bibr" target="#b23">[24]</ref> framework and our evaluation is performed using a NVIDIA GeForce 1070 GTX GPU. We initialize both AlexNet and ResNet networks through their models pre-trained on ImageNet. For AlexNet we take the pre-trained model available in Caffe, while for ResNet we use the converted version of the original model developed in Torch 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>In this section we report the results of our evaluation. We first analyze the proposed approach, demonstrating the advantages of considering multiple sources and discovering latent domains. We then compare the proposed method with state-of-the-art approaches. For all the experiments we report the results in terms of accuracy, repeating the experiments 5 times and averaging the results.</p><p>Analysis of the Proposed Approach. In a first series of experiments, we test the performance of our approach on the MNIST-MNIST-m to USPS benchmark. We compare our method with different baselines: (i) the network trained on the union of all source domains (Single source (unified)), (ii) the model which leads to the best performance among those trained on each single source domain (Best single source) (iii) the domain adaptation method DIAL in <ref type="bibr" target="#b6">[7]</ref> which uses as source set the union of all source domains (DIAL <ref type="bibr" target="#b6">[7]</ref> -Single source (unified)) and (iv) the DIAL model which leads to the best performance among those  trained on each single source domain (DIAL <ref type="bibr" target="#b6">[7]</ref> -Best single source). Moreover, we report the results of our approach in the ideal case where the multiple source domains are known and we do not need to discover them (Multi-source DA). For our approach, we consider several different values for the hyper-parameter k, i.e. the number of discovered source domains. All these methods are based on the MNIST network in <ref type="bibr" target="#b11">[12]</ref> with the addition of BN layers. <ref type="table" target="#tab_0">Table 1</ref> shows the results of our comparison. By looking at the table several observations can be made. First, there is a large performance gap between models trained only on source data and DA methods, confirming the fact that deep architectures do not solve the domain shift problem <ref type="bibr" target="#b7">[8]</ref>. Second, in analogy with previous works on DA <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b38">39]</ref>, we found that considering multiple sources is beneficial for reducing the domain shift with respect to learning a model on the unified source set. Finally, and more importantly, when the domain labels are not available, our approach is successful in discovering latent domains and in exploiting this information for improving accuracy classification on target data, partially filling the performance gap between the single source models and Multi-source DA. Interestingly, the performance of the algorithm are comparable when the number of latent domains k changes, highlighting the robustness of our model to different values of k. This motivates our choice to always fix k to the known number of domains in the next experiments.</p><p>In a second series of experiments we consider the PACS dataset. We compare the proposed approach with the original ResNet architecture trained only on source data and with DA method DIAL <ref type="bibr" target="#b6">[7]</ref> trained on the unified source set. As in the previous experiments, we report the results of the ideal multi-source DA setting, i.e. our approach is applied to multiple known source domains. <ref type="table" target="#tab_1">Table 2</ref> shows our results. As expected, DA models are especially beneficial when considering the PACS dataset. Moreover, the multisource DA network outperforms the single source one. Remarkably, our model is able to infer domain information automatically without supervision. In fact, its accuracy is either comparable with the multi-source model (i.e. for Photo, Art and Cartoon) or in between the single-source, i.e. DIAL, and the multi-source models (i.e. Sketch).</p><p>Looking at the partial results, it is especially interesting to see that the improvements of our approach and the multisource model over DIAL trained on the unified source set are especially significant when either the Sketch or the Cartoon domains are employed as target set. Since these domains are less represented in the ImageNet database, we believe that the corresponding features derived from the pretrained model are less discriminative. In this case DA methods play a significant role.</p><p>We also conduct experiments on the Office31 dataset. As baselines we consider the standard AlexNet architecture trained on source data, AlexNet with Batch Normalization added after each fully-connected layer and the DA model of <ref type="bibr" target="#b6">[7]</ref> with all source domains unified in a single set. Again, the multi-source DA model obtained assuming the domain labels known for each source sample is taken as upper bound. The results reported in <ref type="table" target="#tab_2">Table 3</ref> trigger two main observations. First, in this dataset there is a small margin for improvement when using a multi-source model with respect to adopting a single source one. This is in accordance with findings in <ref type="bibr" target="#b26">[27]</ref>, where it is shown that, with respect to PACS dataset, in Office31 the domain shift with deep features is limited and it is linked mainly to changes in background (i.e. Webcam-Amazon, DSLR-Amazon) or acquisition camera (DSLR-Webcam). Second, in this case our approach only slightly improves performance over the single-source DA model, suggesting that accuracy in automatically inferring latent domains may not be sufficient for learning better target classifiers.</p><p>To further analyze this fact and to demonstrate the flexibility of our framework, we also perform an experiment in a semi-supervised setting. In particular, we consider different levels of supervision in terms of domain information and analyze how the performance of our method change at varying number of labeled source samples. The results of this experiment are reported in <ref type="figure">Fig. 3</ref>. Looking at the figure we can see that by using just few domain labels (5% of the source samples), our model is able to completely fill the performance gap between the unsupervised and the multi-source model. Furthermore, by increasing the level of supervision the accuracy saturates towards the value corresponding to the multi-source model. Comparison with state of the art. In this section we compare the performance of our model with previous works on DA which also consider the problem of inferring latent domains <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref>. As stated in Section 2, there are no previous works adopting deep learning models (i) in a multisource setting and (ii) discovering hidden domains. Therefore, the considered baseline methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b40">41]</ref> only employ handcrafted features. For these approaches we report results taken from the original papers. To further analyze the impact of different feature representations, we also report results obtained running the method of Gong et al. <ref type="bibr" target="#b15">[16]</ref> using features from the last layer of the AlexNet architecture. For a fair comparison, in this series of experiments we extract features from the fc7 layer, without fine-tuning, applying mDA layers to these features and after the classifier. We first consider the Office31 dataset, as this benchmark has been used in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b40">41]</ref>. <ref type="table" target="#tab_3">Table 4</ref> shows the results of our comparison. Our model outperforms all the baselines, with a clear margin in terms of accuracy. Importantly, even when the method in <ref type="bibr" target="#b15">[16]</ref> is applied to features derived from AlexNet, still our approach leads to higher accuracy. For the sake of completeness, in the same table we also report results from previous multi-source DA methods <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34]</ref>. Notice that also these methods are based on shallow models. While these approaches significantly outperform <ref type="bibr" target="#b20">[21]</ref> and <ref type="bibr" target="#b40">[41]</ref>, still their accuracy is much lower than ours.</p><p>To compare with <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b20">21]</ref>, we also consider the OfficeCaltech dataset. Following <ref type="bibr" target="#b15">[16]</ref>, we test both single target (Amazon) and multi-target (Amazon-Caltech and Webcam-DSLR) scenarios, for our model can be easily extended to the latter case. We assume to know which samples belong  to the source domains and which samples to the target domains. Then, we apply two different mDA modules: one for discovering latent source domains and one for discovering latent target domains. To this extent we need two domain prediction branches: in our implementation they share only the input features, while their parameters are independently learned. Notice that, since we do not assume to know the target domain to which a sample belongs, the task is even harder since we require a domain prediction step also at test time. Again, our approach outperforms all baselines, even the method in <ref type="bibr" target="#b15">[16]</ref> adopting features derived from AlexNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work we presented a novel deep DA model for automatically discovering latent domains within visual datasets. The proposed deep architecture is based on a sidebranch which computes the assignment of a source sample to a latent domain. These assignments are then exploited within the main network by novel domain alignment layers which reduce the domain shift by aligning the feature distributions of the discovered sources and the target domains. Our experimental results demonstrate the ability of our model to efficiently exploit the discovered latent domains for addressing challenging domain adaptation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The idea behind our framework. We propose a novel deep architecture which, given a set of images, automatically discover multiple latent source domains and use this information to align the distributions of the internal CNN feature representations of sources and target domains for the purpose of domain adaptation. Image better seen at magnification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>s1 xy , . . . , p s k xy defined over X × Y, and the target domain, characterized by p t xy . Note that the number of source domains k is not necessarily known a-priori, and is left as an hyper-parameter of our method. During training we are given a set of labeled samples from the source domains, and a set of unlabeled samples from the target domain, while we have partial or no ac- cess to domain assignment information for the source sam- ples. More formally, we model the source data as a set S = {xy , where π si is the probability of sampling from a source domain s i . Simi- larly, the target samples T = {x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>is the domain-specific distribution for d: a normal with mean µ d and variance σ 2 d . Given a batch of samples B = {x i } b i=1 , a maximum likelihood estimate of the parameters µ d and σ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Schematic representation of our method applied to the AlexNet architecture (left) and of an mDA-layer (right).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>the output of the classification branch of the network for a source sample, i.e.the target sam- ples induced by the mDA-layers. Similarly, the second term in (4) is the likelihood of θ w.r.t. the known domains:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Digits datasets: comparison of different models in the multi-source scenario. MNIST (M) and MNIST-m (Mm) are taken as source domains, USPS (U) as target.</figDesc><table>Method 
M-Mm to U 
Single source (unified) 
57.1 
Best single source 
59.8 
DIAL [7] -Single source (unified) 
81.7 
DIAL [7] -Best single source 
81.9 
Ours k = 2 
82.5 
Ours k = 3 
82.2 
Ours k = 4 
82.7 
Ours k = 5 
82.4 
Multi-source DA 
84.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>PACS dataset: comparison of different methods us-
ing the ResNet architecture. The first row indicates the tar-
get domain, while all the others are considered as sources. 

Method 
Sketch Photo Art Cartoon Mean 
ResNet [20] 
60.1 
92.9 74.7 
72.4 
75.0 
DIAL [7] 
66.8 
97.0 87.3 
85.5 
84.2 
Ours 
69.6 
97.0 87.7 
86.9 
85.3 
Multi-source DA 
71.6 
96.6 87.5 
87.0 
85.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Office-31 dataset: comparison of different methods using AlexNet. In the first row we indicate the source (top) and the target domains (bottom).</figDesc><table>Method 
Source A-D A-W W-D Mean 
Target 
W 
D 
A 
AlexNet [25] 
89.1 94.6 49.1 
77.6 
AlexNet+BN 
92.9 95.2 60.1 
82.7 
DIAL [7] 
94.3 93.8 62.5 
83.5 
Ours 
94.6 93.7 62.6 
83.6 
Multi-source DA 
95.8 94.8 62.9 
84.5 

0% 5% 10% 25% 50% 75% 90% 100% 

Domain-labels Percentage 

83.6 

83.8 

84.0 

84.2 

84.4 

84.6 

Accuracy 

Figure 3: Office31 dataset. Performance at varying number 
of domain labels (%) for source samples. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Office-31: comparison with state-of-the-art algo- rithms. In the first row we indicate the source (top) and the target domains (bottom).</figDesc><table>Method 
Sources A-D A-W W-D Mean 
Target 
W 
D 
A 
Hoffman et al. [21] 
24.8 42.7 12.8 
26.8 
Xiong et al. [41] 
29.3 43.6 13.3 
28.7 
Gong et al. (AlexNet) [16] 
91.8 94.6 48.9 
78.4 
Ours 
93.1 94.3 64.2 
83.9 
Gopalan et al. [18] 
51.3 36.1 35.8 
41.1 
Nguyen et al. [34] 
64.5 68.6 41.8 
58.3 
Lin et al. [29] 
73.2 81.3 41.1 
65.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Office-Caltech dataset: comparison with state-of- the-art algorithms. In the first row we indicate the source (top) and the target domains (bottom).</figDesc><table>Method 
Source A-C W-D C-W-D Mean 
Target W-D A-C 
A 
Gong et al. [16] -original 
41.7 35.8 
41.0 
39.5 
Hoffman et al. [21] -ensemble 31.7 34.4 
38.9 
35.0 
Hoffman et al. [21] -matching 39.6 34.0 
34.6 
36.1 
Gong et al. [16] -ensemble 
38.7 35.8 
42.8 
39.1 
Gong et al. [16] -matching 
42.6 35.5 
44.6 
40.9 
Gong et al. (AlexNet) [16] 
87.8 87.9 
93.6 
89.8 
Ours 
93.5 88.2 
93.7 
91.8 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code available at https://github.com/mancinimassimiliano/ latent_domains_DA.git 2 https://github.com/HolmesShuan/ ResNet-18-Caffemodel-on-ImageNet</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements.</head><p>We acknowledge financial support from ERC grant 637076 -RoboExNovo and project DIGIMAP, grant 860375, funded by the Austrian Research Promotion Agency (FFG).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Lightweight unsupervised domain adaptation by convolutional filter reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Aljundi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV TASK-CV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="898" to="916" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Domain separation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Autodial: Automatic domain alignment layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Just dial: Domain alignment layers for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">M</forename><surname>Carlucci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Caputo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.06332</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Domain adaptation from multiple sources via auxiliary classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unsupervised visual domain adaptation using subspace alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Habrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sebban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<meeting><address><addrLine>New York</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">59</biblScope>
			<biblScope unit="page" from="1" to="35" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep reconstruction-classification networks for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ghifary</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">B</forename><surname>Kleijn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Balduzzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Connecting the dots with landmarks: Discriminatively learning domain-invariant features for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Reshaping visual datasets for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised adaptation across domain shifts by generating intermediate data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2288" to="2302" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discovering latent domains for multisource domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Correcting sample selection bias by unlabeled data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM-Multimedia</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-Z</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.03077</idno>
		<title level="m">Deeper, broader and artier domain generalization</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Revisiting batch normalization for practical domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04779</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Cross-domain recognition by identifying joint subspaces of source domain and target domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Cybernetics</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1090" to="1101" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Transfer sparse coding for robust image representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks. NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:0902.3430</idno>
		<title level="m">Domain adaptation: Learning bounds and algorithms</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dashn: Joint hierarchical domain adaptation and feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5479" to="5491" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Knowledge and Data Engineering</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning from simulated and unsupervised images through adversarial training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Susskind</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Webb</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2107" to="2116" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Deep coral: Correlation alignment for deep domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01719</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A two-stage weighting framework for multi-source domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chattopadhyay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Panchanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Latent domains modeling for visual domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-H</forename><surname>Hsieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">No bias left behind: Covariate shift adaptation for discriminative 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yamada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Raptis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Adapting svm classifiers to data with shifted distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDM Workshops</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Deep learning of scene-specific classifier for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
