<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-scale Residual Network for Image Super-Resolution</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juncheng</forename><surname>Li</surname></persName>
							<email>cvjunchengli@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Shanghai Key Laboratory of Multidimensional Information Processing</orgName>
								<orgName type="department" key="dep2">Department of Computer Science &amp; Technology</orgName>
								<orgName type="institution">East China Normal University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">School of Computer Science and Information Engineering</orgName>
								<orgName type="institution">Jiangxi Normal University</orgName>
								<address>
									<settlement>Nanchang</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-scale Residual Network for Image Super-Resolution</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Super-resolution · Convolutional neural network · Multi- scale residual network</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Recent studies have shown that deep neural networks can significantly improve the quality of single-image super-resolution. Current researches tend to use deeper convolutional neural networks to enhance performance. However, blindly increasing the depth of the network cannot ameliorate the network effectively. Worse still, with the depth of the network increases, more problems occurred in the training process and more training tricks are needed. In this paper, we propose a novel multiscale residual network (MSRN) to fully exploit the image features, which outperform most of the state-of-the-art methods. Based on the residual block, we introduce convolution kernels of different sizes to adaptively detect the image features in different scales. Meanwhile, we let these features interact with each other to get the most efficacious image information, we call this structure Multi-scale Residual Block (MSRB). Furthermore, the outputs of each MSRB are used as the hierarchical features for global feature fusion. Finally, all these features are sent to the reconstruction module for recovering the high-quality image.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Image super-resolution (SR), particularly single-image super-resolution (SISR), has attracted more and more attention in academia and industry. SISR aims to reconstruct a high-resolution (HR) image from a low-resolution (LR) image which is an ill-posed problem since the mapping between LR and HR has multiple solutions. Thence, learning methods are widely used to learn a mapping from LR to HR images via applying large image datasets.</p><p>Currently, convolutional neural networks (CNNs) have indicated that they can provide remarkable performance in the SISR problem. In 2014, Dong et al. proposed a model for SISR problem termed SRCNN <ref type="bibr" target="#b0">[1]</ref>, which was the first successful model adopting CNNs to SR problem. SRCNN was an efficient network that could learn a kind of end-to-end mapping between the LR and HR images without requiring any engineered features and reached the most satisfactory performance at that time. Since then, many studies focused on building a more efficient network to learn the mapping between LR and HR images so that a series of CNNs-based SISR models <ref type="bibr" target="#b1">[2]</ref><ref type="bibr" target="#b2">[3]</ref><ref type="bibr" target="#b3">[4]</ref><ref type="bibr" target="#b4">[5]</ref><ref type="bibr" target="#b5">[6]</ref><ref type="bibr" target="#b6">[7]</ref><ref type="bibr" target="#b7">[8]</ref><ref type="bibr" target="#b8">[9]</ref> were proposed. EDSR <ref type="bibr" target="#b8">[9]</ref> was the champion of the NTIRE2017 SR Challenge. It based on SRResNet <ref type="bibr" target="#b7">[8]</ref> while enhanced the network by removing the normalization layers as well as using deeper and wider network structures. These models received excellent performance in terms of peak signal-to-noise ratio (PSNR) and structural similarity index (SSIM <ref type="bibr" target="#b9">[10]</ref>) in the SISR problem. Nevertheless, all of these models tend to construct deeper and more complex network structures, which means training these models consumes more resources, time, and tricks. In this work, we have reconstructed some classic SR models, such as SRCNN <ref type="bibr" target="#b0">[1]</ref>, EDSR <ref type="bibr" target="#b8">[9]</ref> and SRResNet <ref type="bibr" target="#b7">[8]</ref>. During the reconstruction experiments, we find most existing SR models have the following problems:</p><p>(a) Hard to Reproduce: The experimental results manifest that most SR models are sensitive to the subtle network architectural changes and some of them are difficult to reach the level of the original paper due to the lack of the network configuration. Also, the same model achieves different performance by using different training tricks, such as weight initialization, gradient truncation, data normalization and so on. This means that the improvement of the performance may not be owing to the change of the model architecture, but the use of some unknown training tricks.</p><p>(b) Inadequate of Features Utilization: Most methods blindly increase the depth of the network in order to enhance the the performance of the network but ignore taking full use of the LR image features. As the depth of the network increases, the features gradually disappear in the process of transmission. How to make full use of these features is crucial for the network to reconstruct highquality images.</p><p>(c) Poor Scalability: Using the preprocessed LR image as input will add computational complexity and produce visible artifacts. Therefore, recent approaches pay more attention to amplifying LR images directly. As a result, it is difficult to find a simple SR model that can accommodate to any upscaling factors, or can migrate to any upscaling factors with only minor adjustments to the network architecture.</p><p>In order to solve the mentioned problems, we propose a novel multi-scale residual network (MSRN) for SISR. In addition, a multi-scale residual block (MSRB) is put forward as the building module for MSRN. Firstly, we use the MSRB to acquire the image features on different scales, which is considered as local multi-scale features. Secondly, the outputs of each MSRB are combined for global feature fusion. Finally, the combination of local multi-scale features and global features can maximize the use of the LR image features and completely solve the problem that features disappear in the transmission process. Besides, we introduce a convolution layer with 1×1 kernel as a bottleneck layer to ob-tain global feature fusion. Furthermore, we utilize a well-designed reconstruction structure that is simple but efficient, and can easily migrate to any upscaling factors.</p><p>We train our models on the DIV2K <ref type="bibr" target="#b10">[11]</ref> dataset without special weight initialization method or other training tricks. Our base-model shows superior performance over most state-of-the-art methods on benchmark test-datasets. Besides, the model can achieve more competitive results by increasing the number of M-SRB or the size of training images. It is more exciting that our MSRB module can be migrate to other restoration models for feature extraction. Contributions of this paper are as follows:</p><p>-Different from previous works, we propose a novel multi-scale residual block (MSRB), which can not only adaptively detect the image features, but also achieve feature fusion at different scales. This is the first multi-scale module based on the residual structure. What's more, it is easy to train and outperform the existing modules. -We extend our work to computer vision tasks and the results exceed those of the state-of-the-art methods in SISR without deep network structure. Besides, MSRB can be used for feature extraction in other restoration tasks which show promising results. -We propose a simple architecture for hierarchical features fusion (HFFS) and image reconstruction. It can be easily extended to any upscaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Single-image Super-resolution</head><p>The SISR problem can be divided into three major stages roughly. Early approaches use interpolation techniques based on sampling theory like linear or bicubic. Those methods run fast, but can not rebuild the detailed, realistic textures. Improved works aim to establish complex mapping functions between LR and HR images. Those methods rely on techniques ranging from neighbor embedding to sparse coding. Recent works tend to build an end-to-end CNNs model to learn mapping functions from LR to HR images by using large training datasets. Since Dong et al. proposed the SRCNN <ref type="bibr" target="#b0">[1]</ref> model, various CNNs architectures have been used on SISR problem. Previous work often used pre-processed LR image as input, which was upscaled to HR space via an upsampling operator as bicubic. However, this method has been proved <ref type="bibr" target="#b1">[2]</ref> that it will add computational complexity and produce visible artifacts. To avoid this, new methods are proposed, such as Fast Super-Resolution Convolutional Neural Networks (FSRCNN <ref type="bibr" target="#b2">[3]</ref>) and Efficient Sub-pixel Convolutional Networks (ESPCN <ref type="bibr" target="#b1">[2]</ref>). All of the models mentioned above are shallow networks (less than 5 layers). Kim et al. <ref type="bibr" target="#b11">[12]</ref> first introduced the residual architecture for training much deeper network (20 layers) and achieved great performance. After that, many SR models have been proposed, including DRCN <ref type="bibr" target="#b4">[5]</ref>, DRNN <ref type="bibr" target="#b6">[7]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref>, SRResNet <ref type="bibr" target="#b7">[8]</ref>, and EDSR <ref type="bibr" target="#b8">[9]</ref>. Unfortunately, these models become more and more deeper and extremely difficult to train. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Feature Extraction Block</head><p>Nowadays, many feature extraction blocks have been proposed. The main idea of the inception block <ref type="bibr" target="#b12">[13]</ref> ( <ref type="figure" target="#fig_0">Fig. 1.(c)</ref>) is to find out how an optimal local sparse structure works in a convolutional network. However, these different scale features simply concatenate together, which leads to the underutilization of local features. In 2016, Kim et al. <ref type="bibr" target="#b11">[12]</ref> proposed a residual learning framework ( <ref type="figure" target="#fig_0">Fig.  1.(a)</ref>) to ease the training of networks so that they could achieve more competitive results. After that, Huang et al. introduced the dense block ( <ref type="figure" target="#fig_0">Fig. 1.(b)</ref>).</p><p>Residual block and dense block use a single size of convolutional kernel and the computational complexity of dense blocks increases at a higher growth rate. In order to solve these drawbacks, we propose a multi-scale residual block. Based on the residual structure, we introduce convolution kernels of different sizes, which designed for adaptively detecting the features of images at different scales. Meanwhile, a skip connection is applied between different scale features so that the features information can be shared and reused with each other. This helps to fully exploit the local features of the image. In addition, a 1×1 convolution layer at the end of the block can be used as a bottleneck layer, which contributes to feature fusion and reduces computation complexity. We will give a more detailed description in section 3.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Method</head><p>In this work, our intent is to reconstruct a super-resolution image I SR from a low-resolution image I LR . The I LR is the low-resolution version of I HR , which is obtained by the bicubic operation. We convert the image to the YCbCr color space and train only on the Y channel. For an image with C color channels, we describe the I LR with a tensor of size W × H × C and denote the I HR , I</p><p>SR with rW × rH × C, where C = 1, represents the Y channel and r represents the upscaling factor. Our ultimate goal is to learn an end-to-end mapping function F between the I LR and the I HR . Given a training dataset</p><formula xml:id="formula_0">I LR i , I HR i N i=1</formula><p>, we solve the following problem:θ</p><formula xml:id="formula_1">= arg min θ 1 N N i=1 L SR (F θ (I LR i ), I HR i ),<label>(1)</label></formula><p>where . Recently, researchers also focus on finding a superior loss function to improve the network performance. The most widely-used image objective optimization functions are the MSE function and L2 function. Although these methods can obtain high PSNR/SSIM, solutions for MSE optimization and L2 optimization problems often produce excessively smooth textures. Now, a variety of loss functions have been proposed such as VGG <ref type="bibr" target="#b3">[4]</ref> function and Charbonnier Penalty function <ref type="bibr" target="#b5">[6]</ref>. On the contrary, we find that their performance improvement is marginal. In order to avoid introducing unnecessary training tricks and reduce computations, we finally choose the L1 function. Thus, the loss function L SR can be defined as:</p><formula xml:id="formula_2">θ = {W 1 , W 2 , W 3 ...W m , b 1 , b 2 , b 3 ...b m },</formula><formula xml:id="formula_3">L SR (F θ (I LR i ), I HR i ) = F θ (I LR i ) − I HR i 1 .<label>(2)</label></formula><p>As shown in <ref type="figure" target="#fig_1">Fig. 2</ref>, it is the complete architecture of our proposed model. Our model takes the unprocessed LR images as input, which are directly upsampled to high-resolution space via the network. Our model can be divided into two parts: the feature extraction module and the image reconstruction module. The feature extraction module is composed of two structures: multi-scale residual block (MSRB) and hierarchical feature fusion structure (HFFS).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Multi-scale Residual Block (MSRB)</head><p>In order to detect the image features at different scales, we propose multi-scale residual block (MSRB). Here we will provide a detailed description of this structure. As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, our MSRB contains two parts: multi-scale features fusion and local residual learning. Multi-scale Features Fusion: different from previous works, we construct a two-bypass network and different bypass use different convolutional kernel. In this way, the information between those bypass can be shared with each other so that able to detect the image features at different scales. The operation can be defined as:</p><formula xml:id="formula_4">S 1 = σ(w 1 3×3 * M n−1 + b 1 ),<label>(3)</label></formula><formula xml:id="formula_5">P 1 = σ(w 1 5×5 * M n−1 + b 1 ),<label>(4)</label></formula><formula xml:id="formula_6">S 2 = σ(w 2 3×3 * [S 1 , P 1 ] + b 2 ),<label>(5)</label></formula><formula xml:id="formula_7">P 2 = σ(w 2 5×5 * [P 1 , S 1 ] + b 2 ),<label>(6)</label></formula><formula xml:id="formula_8">S ′ = w 3 1×1 * [S 2 , P 2 ] + b 3 ,<label>(7)</label></formula><p>where w and b represent the weights and bias respectively, and the superscripts represent the number of layers at which they are located, while the subscripts represent the size of the convolutional kernel used in the layer. σ(x) = max(0, x) stands for the ReLU function, and [</p><formula xml:id="formula_9">S 1 , p 1 ],[P 1 , S 1 ],[S 2 , P 2 ] denote the concate- nation operation.</formula><p>Let M denote the number of feature maps sent to the MSRB. So the input and output of the first convolutional layer have M feature maps. And the second convolutional layer has 2M feature maps, either input or output. All of these feature maps are concatenated and sent to a 1 × 1 convolutional layer. This layer reduces the number of these feature maps to M, thus the input and output of our MSRB have the same number of feature maps. The distinctive architecture allows multiple MSRBs to be used together.</p><p>Local Residual Learning: In order to make the network more efficient, we adopt residual learning to each MSRB. Formally, we describe a multi-scale residual block (MSRB) as:</p><formula xml:id="formula_10">M n = S ′ + M n−1 ,<label>(8)</label></formula><p>where M n and M n−1 represent the input and output of the MSRB, respectively. The operation S ′ +M n−1 is performed by a shortcut connection and element-wise addition. It is worth mentioning that the use of local residual learning makes the computational complexity greatly reduced. Simultaneously, the performance of the network is improved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Hierarchical Feature Fusion Structure (HFFS)</head><p>For SISR problem, input and output images are highly correlated. It is crucial to fully exploit the features of the input image and transfer them to the end of the network for reconstruction. However, as the depth of the network increases these features gradually disappear during transmission. Driven by this problem, various methods have been proposed, among which the skip connection is the most simple and efficient method. All of these methods try to create different connections between different layers. Unfortunately, these methods can't fully utilize the features of the input image, and generate too much redundant information for aimlessness.</p><p>In the experiment, we notice that with the growth of depth, the spatial expression ability of the network gradually decreases while the semantic expression ability gradually increases. Additionally, the output of each MSRB contains distinct features. Therefore, how to make full use of these hierarchical features will directly affect the quality of reconstructed images. In this work, a simple hierarchical feature fusion structure is utilized. We send all the output of the MSRB to the end of the network for reconstruction. On the one hand, these feature maps contain a large amount of redundant information. On the other hand, using them directly for reconstruction will greatly increase the computational complexity. In order to adaptively extract useful information from these hierarchical features, we introduce a bottleneck layer which is essential for a convolutional layer with 1×1 kernel. The output of hierarchical feature fusion structure (HFFS) can be formulated as:</p><formula xml:id="formula_11">F LR = w * [M 0 , M 1 , M 2 , ..., M N ] + b,<label>(9)</label></formula><p>where M 0 is the output of the first convolutional layer, M i (i = 0) represents the output of the i th MSRB, and [M 0 , M 1 , M 2 , ..., M N ] denotes the concatenation operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Image Reconstruction</head><p>The previous work paid close attention to learn a mapping function between LR and HR images, where the LR image was upsampled to the same dimensions as HR by bicubic. Yet, this approach introduced redundant information and increased the computational complexity. Inspired by it, recent work tends to use the un-amplified LR as the input image to train a network that can be directly upsampled to HR dimensions. Instead, it is difficult to find an SR model which is able to migrate to any upscaling factors with only minor adjustments to the network architecture. Moreover, most of these networks tend to be a fixed upscaling factor (x4), with no specific instructions given to migrate to other upscaling factors. PixelShuffle <ref type="bibr" target="#b1">[2]</ref> and deconvolutional layer are widely used in SISR tasks. As shown in <ref type="figure" target="#fig_4">Fig. 4</ref>, there are several common reconstruction modules. Taking the upscaling factor of ×4 as an example, all of these modules use pixelShuffle or deconvolution operation and the SR image is reconstructed gradually with upscaling factor 2 as the base. However, as the upscaling factor increases (e.g. ×8), the network becomes deeper accompanied with more uncertain training problems. Moreover, these methods does not work on odd upscaling factors, while one might expect a tardy growth in upscaling factor(e.g. ×2,×3,×4,×5) rather than exponential increase.</p><p>For this purpose, we put forward a new reconstruction module <ref type="figure" target="#fig_4">(Fig. 4(ours)</ref>), which is a simple, efficient, and flexible structure. Thanks to pixelshuffle <ref type="bibr" target="#b1">[2]</ref>, our modules can be migrated to any upscaling factor with minor adjustments. In <ref type="table" target="#tab_0">Table 1</ref>. we provide thorough configuration information about the reconstruction structure. In our network, for different upscaling factors, we only need to change the value of M whose change is negligible. Experiments indicate that this structure performs well on different upscaling factors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of our model on several benchmark test-datasets. We first introduce the dataset used for training and testing, then we give implementation details. Next, we compare our model with several stateof-the-art methods. Finally, we give a series of qualitative analysis experiments results. In addition, we show some of the results on other low-level computer vision tasks with our MSRB.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The most widely used training dataset in previous studies includes 291 images, of which 91 images are from <ref type="bibr" target="#b13">[14]</ref> and the other 200 images are from <ref type="bibr" target="#b14">[15]</ref>. And some methods take ImageNet <ref type="bibr" target="#b15">[16]</ref> as training dataset, since it contains richer samples. In our work, we choose DIV2K <ref type="bibr" target="#b10">[11]</ref> as our training dataset, a new highquality image dataset for image restoration challenge. During testing, we choose five widely used benchmark datasets: Set5 <ref type="bibr" target="#b16">[17]</ref>, Set14 <ref type="bibr" target="#b17">[18]</ref>, BSDS100 <ref type="bibr" target="#b18">[19]</ref>, Urban100 <ref type="bibr" target="#b19">[20]</ref> and Manga109 <ref type="bibr" target="#b20">[21]</ref>. These datasets contain a wide variety of images that can fully verify our model. Following previous works, all our training and testing are based on luminance channel in YCbCr colour space, and upscaling factors: ×2, ×3, ×4, ×8 are used for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>Following <ref type="bibr" target="#b5">[6]</ref>, we augment the training data in three ways: (1) scaling (2) rotation (3) flipping. In each training batch, we randomly extract 16 LR patches with the size of 64×64 and an epoch having 1000 iterations of back-propagation. We train our model with ADAM optimizer <ref type="bibr" target="#b21">[22]</ref> by setting the learning rate lr = 0.0001. In our final model, we use 8 multi-scale residual blocks (MSRB, N = 8) and the output of each MSRB has 64 feature maps. Simultaneously, the output of each bottleneck layer (1×1 convolutional layer) has 64 feature maps. We implement MSRN with the Pytorch framework and train them using NVIDIA Titan Xp GPU. We do not use a special weight initialization method or other training tricks, and code is available at https://github.com/MIVRC/MSRN-PyTorch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparisons with State-of-the-art Methods</head><p>We compare our model with 10 state-of-the-art SR methods, including Bicubic, A+ <ref type="bibr" target="#b22">[23]</ref>, SelfExSR <ref type="bibr" target="#b19">[20]</ref>, SRCNN <ref type="bibr" target="#b0">[1]</ref>, ESPCN <ref type="bibr" target="#b1">[2]</ref>, FSRCNN <ref type="bibr" target="#b2">[3]</ref>, VDSR <ref type="bibr" target="#b3">[4]</ref>, DR-CN <ref type="bibr" target="#b4">[5]</ref>, LapSRN <ref type="bibr" target="#b5">[6]</ref> and EDSR <ref type="bibr" target="#b8">[9]</ref>. For fair, we retrain most of these models (except for EDSR <ref type="bibr" target="#b8">[9]</ref>, the results of EDSR provided by their original papers).</p><p>Taking the equality of comparison into account, we evaluate the SR images with two commonly-used image quality metrics: PSNR and SSIM. Moreover, all the reported PSNR/SSIM measures are calculated on the luminance channel and remove M-pixel from each border (M stands for the upscaling factor). The evaluation results of the SR method including our model and 10 stateof-art methods are demonstrated in <ref type="table" target="#tab_1">Table 2</ref>. Our model outperforms by a large margin on different upscaling factors and test-datasets. It can be seen that our results are slightly lower than EDSR <ref type="bibr" target="#b8">[9]</ref>. But it is worth noting that EDSR <ref type="bibr" target="#b8">[9]</ref> use  <ref type="bibr" target="#b11">[12]</ref>, dense block <ref type="bibr" target="#b23">[24]</ref>, and MSRB(our)) on SISR. The green line represents our model and it achieves the best results under different upscaling factors.</p><p>RGB channels for training, meanwhile, the data augment methods are different.</p><p>To better illustrate the difference with EDSR <ref type="bibr" target="#b8">[9]</ref>, we show a comparison of model specifications in <ref type="table" target="#tab_2">Table 3</ref>. EDSR <ref type="bibr" target="#b8">[9]</ref> is an outstanding model gained amazing results. However, it is a deep and wide network which contains large quantities of convolutional layers and parameters. In other words, training this model will cost more memory, space and datasets. In contrast, the specifications of our model is much smaller than EDSR <ref type="bibr" target="#b8">[9]</ref>, which makes it easier to reproduce and promote. In <ref type="figure">Fig. 6</ref> and <ref type="figure">Fig. 7</ref> we present visual performance on different datasets with different upscaling factors. Our model can reconstruct sharp and natural images, as well as outperforms other state-of-the-art methods. This is probably owing to the MSRB module can detect the image features at different scales and use them for reconstruction. For better illustration, more SR images reconstructed by our model can be found at https://goo.gl/bGnZ8D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Qualitative Analysis</head><p>Benefit of MSRB: In this work, we propose an efficient feature extraction structure: multi-scale residual block. This module can adaptively detect image features at different scales and fully exploit the potential features of the image. To validate the effectiveness of our module, we design a set of comparative experiments to compare the performance with residual block <ref type="bibr" target="#b11">[12]</ref>, dense block <ref type="bibr" target="#b23">[24]</ref> and MSRB in SISR tasks. Based on the MSRN architecture, we replace the feature extraction block in the network. The three networks contain different feature extraction block, and each network contains only one feature extraction block. For quick verification, we use a small training dataset in this part, and all these   <ref type="bibr" target="#b11">[12]</ref>, the dense block <ref type="bibr" target="#b23">[24]</ref>, and our MSRB, respectively. models are trained in the same environment by 10 5 iterations. The results <ref type="figure">(Fig.  5)</ref> show that our MSRB module is superior to other modules at all upsampling factors. As shown in <ref type="figure">Fig. 9</ref>, we visualize the output of these feature extraction blocks. It deserves to notice that the activations are sparse (most values are zero, as the visualization shown in black) and some activation maps may be all zero which indicates dead filters. It is obvious that the output of the MSRB contains more valid activation maps, which further proves the effectiveness of the structure.</p><p>Benefit of Increasing The Number of MSRB: As is acknowledged, increasing the depth of the network can effectively improve the performance. In this work, adding the number of MSRBs is the simplest way to gain excellent result. In order to verify the impact of the number of MSRBs on network, we design a series of experiments. As shown in <ref type="figure">Fig. 8</ref>, our MSRN performance improves rapidly with the number of MSRBs growth. Although the performance of the network will further enhance by using more MSRB, but this will lead to a more complex network. While weighing the network performance and network complexity, we finally use 8 MSRBs, the result is close to EDSR, but the number of model parameters is only one-seventh of it.</p><p>Performance on Other Tasks: In order to further verify the validity of our proposed MSRB module, we apply it to other low-level computer vision tasks for feature extraction. As shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, we provide the results of image-denoising and image-dehazing, respectively. It is obvious that our model achieves promising results on other low-level computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion and Future Works</head><p>Many training tricks have been proposed to make the reconstructed image more realistic in SISR. For example, multi-scale (the scale here represents the upscaling factor) mixed training method is used in <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b8">[9]</ref>, and geometric selfensemble method is proposed in <ref type="bibr" target="#b8">[9]</ref>. We believe that these training tricks can also improve our model performance. However, we are more inclined to explore an efficient model rather than use training tricks. Although our model has shown superior performance, the reconstructed image is still not clear enough under large upscaling factors. In the future work, we will pay more attention to large-scale downsampling image reconstruction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions</head><p>In this paper, we proposed an efficient multi-scale residual block (MSRB), which is used to adaptively detect the image features at different scales. Based on MSRB, we put forward multi-scale residual network (MSRN). It is a simple and efficient SR model so that we can fully utilize the local multi-scale features and the hierarchical features to obtain accurate SR image. Additionally, we achieved promising results by applying the MSRB module to other computer vision tasks such as image-denoising and image-dehazing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Feature maps visualization. Represent the output of the residual block, the dense block, and our MSRB, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The complete architecture of our proposed model. The network is divided into feature extraction and reconstruction, different color squares represent different operations, the top-right of the picture gives a specific description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>denotes the weights and bias of our m-layer neural network. L SR is the loss function used to minimize the difference between I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The structure of multi-scale residual block (MSRB).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Comparison of some common image reconstruction structure (×4).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 8 .Fig. 9 .</head><label>89</label><figDesc>Fig. 8. Performance comparison of MSRN with different number of MSRBs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. Application examples for image denoising and image dehazing, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Detailed configuration information about the reconstruction structure. For different upscaling factors, we only need to change the value of M.</figDesc><table>Laye name 
Input channel Output channel Kernel size 
conv input 
64 
64×2×2 
3×3 
PixelShuffle(×2) 
64×2×2 
64 
/ 
conv output 
64 
1 
3×3 
conv input 
64 
64×3×3 
3×3 
PixelShuffle(×3) 
64×3×3 
64 
/ 
conv output 
64 
1 
3×3 
conv input 
64 
64×4×4 
3×3 
PixelShuffle(×4) 
64×4×4 
64 
/ 
conv output 
64 
1 
3×3 
conv input 
64 
64×8×8 
3×3 
PixelShuffle(×8) 
64×8×8 
64 
/ 
conv output 
64 
1 
3×3 
conv input 
64 
64×M×M 
3×3 
PixelShuffle(×M) 64×M×M 
64 
/ 
conv output 
64 
1 
3×3 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Quantitative</figDesc><table>comparisons of state-of-the-art methods. Red text indicates the 
best performancen and blue text indicate the second best performance. Notice that the 
EDSR results were not retrained by us, but were provided by their original paper. 

Algorithm 
Scale 
Set5 
PSNR/SSIM 

Set14 
PSNR/SSIM 

BSDS100 
PSNR/SSIM 

Urban100 
PSNR/SSIM 

Manga109 
PSNR/SSIM 
Bicubic 
x2 
33.69/0.9284 30.34/0.8675 29.57/0.8434 26.88/0.8438 30.82/0.9332 
A+ [23] 
x2 
36.60/0.9542 32.42/0.9059 31.24/0.8870 29.25/0.8955 35.37/0.9663 
SelfExSR [20] 
x2 
36.60/0.9537 32.46/0.9051 31.20/0.8863 29.55/0.8983 35.82/0.9671 
SRCNN [1] 
x2 
36.71/0.9536 32.32/0.9052 31.36/0.8880 29.54/0.8962 35.74/0.9661 
ESPCN [2] 
x2 
37.00/0.9559 32.75/0.9098 31.51/0.8939 29.87/0.9065 36.21/0.9694 
FSRCNN [3] 
x2 
37.06/0.9554 32.76/0.9078 31.53/0.8912 29.88/0.9024 36.67/0.9694 
VDSR [4] 
x2 
37.53/0.9583 33.05/0.9107 31.92/0.8965 30.79/0.9157 37.22/0.9729 
DRCN [5] 
x2 
37.63/0.9584 33.06/0.9108 31.85/0.8947 30.76/0.9147 37.63/0.9723 
LapSRN [6] 
x2 
37.52/0.9581 33.08/0.9109 31.80/0.8949 30.41/0.9112 37.27/0.9855 
EDSR [9] 
x2 
38.11/0.9601 33.92/0.9195 32.32/0.9013 
-/-
-/-
MSRN(our) 
x2 
38.08/0.9605 33.74/0.9170 32.23/0.9013 32.22/0.9326 38.82/0.9868 
Bicubic 
x3 
30.41/0.8655 27.64/0.7722 27.21/0.7344 24.46/0.7411 26.96/0.8555 
A+ [23] 
x3 
32.63/0.9085 29.25/0.8194 28.31/0.7828 26.05/0.8019 29.93/0.9089 
SelfExSR [20] 
x3 
32.66/0.9089 29.34/0.8222 28.30/0.7839 26.45/0.8124 27.57/0.7997 
SRCNN [1] 
x3 
32.47/0.9067 29.23/0.8201 28.31/0.7832 26.25/0.8028 30.59/0.9107 
ESPCN [2] 
x3 
33.02/0.9135 29.49/0.8271 28.50/0.7937 26.41/0.8161 30.79/0.9181 
FSRCNN [3] 
x3 
33.20/0.9149 29.54/0.8277 28.55/0.7945 26.48/0.8175 30.98/0.9212 
VDSR [4] 
x3 
33.68/0.9201 29.86/0.8312 28.83/0.7966 27.15/0.8315 32.01/0.9310 
DRCN [5] 
x3 
33.85/0.9215 29.89/0.8317 28.81/0.7954 27.16/0.8311 32.31/0.9328 
LapSRN [6] 
x3 
33.82/0.9207 29.89/0.8304 28.82/0.7950 27.07/0.8298 32.21/0.9318 
EDSR [9] 
x3 
34.65/0.9282 30.52/0.8462 29.25/0.8093 
-/-
-/-
MSRN(our) 
x3 
34.38/0.9262 30.34/0.8395 29.08/0.8041 28.08/0.8554 33.44/0.9427 
Bicubic 
x4 
28.43/0.8022 26.10/0.6936 25.97/0.6517 23.14/0.6599 24.91/0.7826 
A+ [23] 
x4 
30.33/0.8565 27.44/0.7450 26.83/0.6999 24.34/0.7211 27.03/0.8439 
SelfExSR [20] 
x4 
30.34/0.8593 27.55/0.7511 26.84/0.7032 24.83/0.7403 27.83/0.8598 
SRCNN [1] 
x4 
30.50/0.8573 27.62/0.7453 26.91/0.6994 24.53/0.7236 27.66/0.8505 
ESPCN [2] 
x4 
30.66/0.8646 27.71/0.7562 26.98/0.7124 24.60/0.7360 27.70/0.8560 
FSRCNN [3] 
x4 
30.73/0.8601 27.71/0.7488 26.98/0.7029 24.62/0.7272 27.90/0.8517 
VDSR [4] 
x4 
31.36/0.8796 28.11/0.7624 27.29/0.7167 25.18/0.7543 28.83/0.8809 
DRCN [5] 
x4 
31.56/0.8810 28.15/0.7627 27.24/0.7150 25.15/0.7530 28.98/0.8816 
LapSRN [6] 
x4 
31.54/0.8811 28.19/0.7635 27.32/0.7162 25.21/0.7564 29.09/0.8845 
EDSR [9] 
x4 
32.46/0.8968 28.80/0.7876 27.71/0.7420 
-/-
-/-
MSRN(our) 
x4 
32.07/0.8903 28.60/0.7751 27.52/0.7273 26.04/0.7896 30.17/0.9034 
Bicubic 
x8 
24.40/0.6045 23.19/0.5110 23.67/0.4808 20.74/0.4841 21.46/0.6138 
A+ [23] 
x8 
25.53/0.6548 23.99/0.5535 24.21/0.5156 21.37/0.5193 22.39/0.6454 
SelfExSR [20] 
x8 
25.49/0.6733 24.02/0.5650 24.19/0.5146 21.81/0.5536 22.99/0.6907 
SRCNN [1] 
x8 
25.34/0.6471 23.86/0.5443 24.14/0.5043 21.29/0.5133 22.46/0.6606 
ESPCN [2] 
x8 
25.75/0.6738 24.21/0.5109 24.37/0.5277 21.59/0.5420 22.83/0.6715 
FSRCNN [3] 
x8 
25.42/0.6440 23.94/0.5482 24.21/0.5112 21.32/0.5090 22.39/0.6357 
VDSR [4] 
x8 
25.73/0.6743 23.20/0.5110 24.34/0.5169 21.48/0.5289 22.73/0.6688 
DRCN [5] 
x8 
25.93/0.6743 24.25/0.5510 24.49/0.5168 21.71/0.5289 23.20/0.6686 
LapSRN [6] 
x8 
26.15/0.7028 24.45/0.5792 24.54/0.5293 21.81/0.5555 23.39/0.7068 
MSRN(our) 
x8 
26.59/0.7254 24.88/0.5961 24.70/0.5410 22.37/0.5977 24.28/0.7517 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Specifications comparison (x4). 'RGB' means the model is trained on RGB channels, 'Y' means the model is trained on luminance channel in YCbCr colour space, and 'M' means million.</figDesc><table>Algorithm 
Feature extraction Filters Layers Depth Parameters Updates Channel 
EDSR [9] 
32 blocks 
256 
69 
69 
43M 
1 × 10 

6 

RGB 
MSRN (our) 
8 blocks 
64 
44 
28 
6.3M 
4 × 10 

5 

Y 

Fig. 5. Quantitative comparison of three different feature extraction blocks (residual 
block </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Fig. 6. Visual comparison for ×2, ×3, ×4 SR images. Our MSRN can reconstruct realistic images with sharp edges.Fig. 7. Visual comparison of MSRN with other SR methods on large-scale (×8) SR task. Obviously, MSRN can reconstruct realistic images with sharp edges.</figDesc><table>×2: SRCNN [1] 
×2: LapSRN [6] 
×2: MSRN(our) 
Orignal(HR) 

×3: SRCNN [1] 
×3: LapSRN [6] 
×3: MSRN(our) 
Orignal(HR) 

×4: SRCNN [1] 
×4: LapSRN [6] 
×4: MSRN(our) 
Orignal(HR) 

×8: SRCNN [1] 
×8: LapSRN [6] 
×8: MSRN(our) 
Orignal(HR) 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Acknowledgments</head><p>This work are sponsored by the key project of the National Natural Science Foundation of China (No. 61731009), the National Science Foundation of China (No. 61501188), the "Chenguang Program" supported by Shanghai Education Development Foundation and Shanghai Municipal Education Commission (No. 17CG25) and East China Normal University.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wang</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Accelerating the super-resolution convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="391" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Accurate image super-resolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1646" to="1654" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kwon Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mu Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1637" to="1645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Photo-realistic single image superresolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Enhanced deep residual networks for single image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Son</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE conference on computer vision and pattern recognition workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Image super-resolution via sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on image processing</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2861" to="2873" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings Eighth IEEE International Conference on Computer Vision</title>
		<meeting>Eighth IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Low-complexity single-image super-resolution based on nonnegative neighbor embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bevilacqua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roumy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guillemot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Alberi-Morel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd British Machine Vision Conference</title>
		<meeting>the 23rd British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On single image scale-up using sparserepresentations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeyde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on curves and surfaces</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="898" to="916" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Single image super-resolution from transformed self-exemplars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5197" to="5206" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sketch-based manga retrieval using manga109 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Matsui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ito</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Aramaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fujimoto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ogawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Aizawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Multimedia Tools and Applications</title>
		<imprint>
			<biblScope unit="volume">76</biblScope>
			<biblScope unit="issue">20</biblScope>
			<biblScope unit="page" from="21811" to="21838" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Adam: Amethod for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 3rd International Conferencce on Learning Representations</title>
		<meeting>the 3rd International Conferencce on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A+: Adjusted anchored neighborhood regression for fast super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Smet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="111" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
