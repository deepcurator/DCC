<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end Recovery of Human Shape and Pose Input Reconstruction Side and top down view Part Segmentation Input Reconstruction Side and top down view Part Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angjoo</forename><surname>Kanazawa</surname></persName>
							<email>kanazawa@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">J</forename><surname>Black</surname></persName>
							<email>black@tuebingen.mpg.de</email>
							<affiliation key="aff1">
								<orgName type="institution">MPI for Intelligent Systems</orgName>
								<address>
									<settlement>Tübingen, Germany</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">W</forename><surname>Jacobs</surname></persName>
							<email>djacobs@umiacs.umd.edu</email>
							<affiliation key="aff2">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
							<email>malik@eecs.berkeley.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end Recovery of Human Shape and Pose Input Reconstruction Side and top down view Part Segmentation Input Reconstruction Side and top down view Part Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Figure 1: Human Mesh Recovery (HMR): End-to-end adversarial learning of human pose and shape. We describe a real time framework for recovering the 3D joint angles and shape of the body from a single RGB image. The first two rows show results from our model trained with some 2D-to-3D supervision, the bottom row shows results from a model that is trained in a fully weakly-supervised manner without using any paired 2D-to-3D supervision. We infer the full 3D body even in case of occlusions and truncations. Note that we capture head and limb orientations. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>We describe Human Mesh Recovery (HMR), an end-to-</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We present an end-to-end framework for recovering a full 3D mesh of a human body from a single RGB image. We use the generative human body model, SMPL <ref type="bibr" target="#b23">[24]</ref>, which parameterizes the mesh by 3D joint angles and a low- Overview of the proposed framework. An image I is passed through a convolutional encoder. This is sent to an iterative 3D regression module that infers the latent 3D representation of the human that minimizes the joint reprojection error. The 3D parameters are also sent to the discriminator D, whose goal is to tell if these parameters come from a real human shape and pose. dimensional linear shape space. As illustrated in <ref type="figure">Figure 1</ref>, estimating a 3D mesh opens the door to a wide range of applications such as foreground and part segmentation, which is beyond what is practical with a simple skeleton. The output mesh can be immediately used by animators, modified, measured, manipulated and retargeted. Our output is also holistic -we always infer the full 3D body even in cases of occlusion and truncation.</p><p>Note that there is a great deal of work on the 3D analysis of humans from a single image. Most approaches, however, focus on recovering 3D joint locations. We argue that these joints alone are not the full story. Joints are sparse, whereas the human body is defined by a surface in 3D space.</p><p>Additionally, joint locations alone do not constrain the full DoF at each joint. This means that it is non-trivial to estimate the full pose of the body from only the 3D joint locations. In contrast, we output the relative 3D rotation matrices for each joint in the kinematic tree, capturing information about 3D head and limb orientation. Predicting rotations also ensures that limbs are symmetric and of valid length. Our model implicitly learns the joint angle limits from datasets of 3D body models.</p><p>Existing methods for recovering 3D human mesh today focus on a multi-stage approach <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref>. First they estimate 2D joint locations and, from these, estimate the 3D model parameters. Such a stepwise approach is typically not optimal and here we propose an end-to-end solution to learn a mapping from image pixels directly to model parameters.</p><p>There are several challenges, however, in training such a model in an end-to-end manner. First is the lack of largescale ground truth 3D annotation for in-the-wild images. Existing datasets with accurate 3D annotations are captured in constrained environments. Models trained on these datasets do not generalize well to the richness of images in the real world. Another challenge is in the inherent ambiguities in single-view 2D-to-3D mapping. Most well known is the problem of depth ambiguity where multiple 3D body configurations explain the same 2D projections <ref type="bibr" target="#b41">[42]</ref>. Many of these configurations may not be anthropometrically reasonable, such as impossible joint angles or extremely skinny bodies. In addition, estimating the camera explicitly introduces an additional scale ambiguity between the size of the person and the camera distance.</p><p>In this paper we propose a novel approach to mesh reconstruction that addresses both of these challenges. A key insight is that there are large-scale 2D keypoint annotations of in-the-wild images and a separate large-scale dataset of 3D meshes of people with various poses and shapes. Our key contribution is to take advantage of these unpaired 2D keypoint annotations and 3D scans in a conditional generative adversarial manner. The idea is that, given an image, the network has to infer the 3D mesh parameters and the camera such that the 3D keypoints match the annotated 2D keypoints after projection. To deal with ambiguities, these parameters are sent to a discriminator network, whose task is to determine if the 3D parameters correspond to bodies of real humans or not. Hence the network is encouraged to output parameters on the human manifold and the discriminator acts as weak supervision. The network implicitly learns the angle limits for each joint and is discouraged from making people with unusual body shapes.</p><p>An additional challenge in predicting body model parameters is that regressing to rotation matrices is challenging. Most approaches formulate rotation estimation as a classification problem by dividing the angles into bins <ref type="bibr" target="#b45">[46]</ref>. However differentiating angle probabilities with respect to the reprojection loss is non-trivial and discretization sacrifices precision. Instead we propose to directly regress these values in an iterative manner with feedback. Our framework is illustrated in <ref type="figure">Figure 2</ref>.</p><p>Our approach is similar to 3D interpreter networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b49">50]</ref> in the use of reprojection loss and the more recent adversarial inverse graphics networks <ref type="bibr" target="#b46">[47]</ref> for the use of the adversarial prior. We go beyond the existing techniques in multiple ways:</p><p>1. We infer 3D mesh parameters directly from image features, while previous approaches infer them from 2D keypoints. This avoids the need for two stage training and also avoids throwing away a lot of image information.</p><p>2. Going beyond skeletons, we output meshes, which are more complex and more appropriate for many applications. Again, no additional inference step is needed.</p><p>3. Our framework is trained in an end-to-end manner. We out-perform previous approaches that output 3D meshes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> in terms of 3D joint error and run time.</p><p>4. We show results with and without paired 2D-to-3D data. Even without using any paired 2D-to-3D supervision, our approach produces reasonable 3D reconstructions. This is most exciting because it opens up possibilities for learning 3D from large amounts of 2D data.</p><p>Since there are no datasets for evaluating 3D mesh reconstructions of humans from in-the-wild images, we are bound to evaluate our approach on the standard 3D joint location estimation task. Our approach out performs previous methods that estimate SMPL parameters from 2D joints and is competitive with approaches that only output 3D skeletons. We also evaluate our approach on an auxiliary task of human part segmentation. We qualitatively evaluate our approach on challenging images in-thewild and show results sampled at different error percentiles. Our model and code is available for research purposes at https://akanazawa.github.io/hmr/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>3D Pose Estimation: Many papers formulate human pose estimation as the problem of locating the major 3D joints of the body from an image, a video sequence, either singleview or multi-view. We argue that this notion of "pose" is overly simplistic but it is the major paradigm in the field. The approaches are split into two categories: two-stage and direct estimation.</p><p>Two stage methods first predict 2D joint locations using 2D pose detectors <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b53">54]</ref> or ground truth 2D pose and then predict 3D joint locations from the 2D joints either by regression <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b28">29]</ref> or model fitting, where a common approach exploits a learned dictionary of 3D skeletons <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b52">53,</ref><ref type="bibr" target="#b53">54]</ref>. In order to constrain the inherent ambiguity in 2D-to-3D estimation, these methods use various priors <ref type="bibr" target="#b41">[42]</ref>. Most methods make some assumption about the limb-length or proportions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b33">34]</ref>. Akhter and Black <ref type="bibr" target="#b1">[2]</ref> learn a novel pose prior that captures pose-dependent joint angle limits. Two stage-methods have the benefit of being more robust to domain shift, but rely too much on 2D joint detections and may throw away image information in estimating 3D pose.</p><p>Video datasets with ground truth motion capture like HumanEva <ref type="bibr" target="#b37">[38]</ref> and Human3.6M <ref type="bibr" target="#b15">[16]</ref> define the problem in terms of 3D joint locations. They provide training data that lets the 3D joint estimation problem be formulated as a standard supervised learning problem. Beginning with Toshev et al. <ref type="bibr" target="#b44">[45]</ref>, many recent methods estimate 3D joints directly from images in a deep learning framework <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b43">44,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. Dominant approaches are fullyconvolutional, except for the very recent method of Xiao et al. <ref type="bibr" target="#b39">[40]</ref> that regresses bones and obtains excellent results on the 3D pose benchmarks. Many methods do not solve for the camera, but estimate the depth relative to root and use a predefined global scale based the average length of bones <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b51">52]</ref>. Recently Rogez et al. <ref type="bibr" target="#b35">[36]</ref> combine human detection with 3D pose prediction. The main issue with these direct estimation methods is that images with accurate ground truth 3D annotations are captured in controlled MoCap environments. Models trained only on these images do not generalize well to the real world. Weakly-supervised 3D: Recent work tackles this problem of the domain gap between MoCap and in-the-wild images in an end-to-end framework. Rogez and Schmid <ref type="bibr" target="#b34">[35]</ref> artificially endow 3D annotations to images with 2D pose annotation using MoCap data. Several methods <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b50">51]</ref> train on both in-the-wild and MoCap datasets jointly. Still others <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b27">28]</ref> use pre-trained 2D pose networks and also use 2D pose prediction as an auxiliary task. When 3D annotation is not available, Zhou et al. <ref type="bibr" target="#b50">[51]</ref> gain weak supervision from a geometric constraint that encourages relative bone lengths to stay constant. In this work, we output 3D joint angles and 3D shape, which subsumes these constraints that the limbs should be symmetric. We employ a much stronger form of weak supervision by training an adversarial prior. Methods that output more than 3D joints: There are multiple methods that fit a parametric body model to manually extracted silhouettes <ref type="bibr" target="#b7">[8]</ref> and a few manually provided correspondences <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b13">14]</ref>. More recent works attempt to automate this effort. Bogo et al. <ref type="bibr" target="#b4">[5]</ref> propose SMPLify, an optimization-based method to recover SMPL parame-ters from 14 detected 2D joints that leverages multiple priors. However, due to the optimization steps the approach is not real-time, requiring 20-60 seconds per image. They also make a priori assumptions about the joint angle limits. Lassner et al. <ref type="bibr" target="#b19">[20]</ref> take curated results from SMPLify to train 91 keypoint detectors corresponding to traditional body joints and points on the surface. They then optimize the SMPL model parameters to fit the keypoints similarly to <ref type="bibr" target="#b4">[5]</ref>. They also propose a random forest regression approach to directly regress SMPL parameters, which reduces runtime at the cost of accuracy. Our approach out-performs both methods, directly infers SMPL parameters from images instead of detected 2D keypoints, and runs in real time.</p><p>VNect <ref type="bibr" target="#b27">[28]</ref> fits a rigged skeleton model over time to estimated 2D and 3D joint locations. While they can recover 3D rotations of each joint after optimization, we directly output rotations from images as well as the surface vertices. Similarly Zhou et al. <ref type="bibr" target="#b51">[52]</ref> directly regress joint rotations of a fixed kinematic tree. We output shape as well as the camera scale and out-perform their approach in 3D pose estimation.</p><p>There are other related methods that predict SMPLrelated outputs: Varol et al. <ref type="bibr" target="#b47">[48]</ref> use a synthetic dataset of rendered SMPL bodies to learn a fully convolutional model for depth and body part segmentation. DenseReg <ref type="bibr" target="#b12">[13]</ref> similarly outputs a dense correspondence map for human bodies. Both are 2.5D projections of the underlying 3D body. In this work, we recover all SMPL parameters and the camera, from which all of these outputs can be obtained.</p><p>Kulkarni et al. <ref type="bibr" target="#b18">[19]</ref> use a generative model of body shape and pose with a probabilistic programming framework to estimate body pose from single images. They deal with visually simple images and do not evaluate 3D pose accuracy. More recently Tan et al. <ref type="bibr" target="#b40">[41]</ref> infer SMPL parameters in a two-step approach that first learns a SMPL-to-silhouette decoder using synthetic data, and then learns an image-to-SMPL encoder network with a fixed decoder, where the objective is to reconstruct the image silhouettes. While this is an interesting direction, the reliance on silhouettes limits their approach to frontal images. We train and test our model on images from all viewing directions. Unlike us they do not predict the camera parameters. Furthermore, training requires images with full body silhouettes without occlusion. They only test on 139 real images and do not evaluate on the 3D pose metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Model</head><p>We propose to reconstruct a full 3D mesh of a human body directly from a single RGB image I centered on a human in a feedforward manner. During training we assume that all images are annotated with ground truth 2D joints. We also consider the case in which some have 3D annotations as well. Additionally we assume that there is a pool of 3D meshes of human bodies of varying shape and pose.</p><p>Since these meshes do not necessarily have a corresponding image, we refer to this data as unpaired <ref type="bibr" target="#b54">[55]</ref>. <ref type="figure">Figure 2</ref> shows the overview of the proposed network architecture, which can be trained end-to-end. Convolutional features of the image are sent to the iterative 3D regression module whose objective is to infer the 3D human body and the camera such that its 3D joints project onto the annotated 2D joints. The inferred parameters are also sent to an adversarial discriminator network whose task is to determine if the 3D parameters are real meshes from the unpaired data. This encourages the network to output 3D human bodies that lie on the manifold of human bodies and acts as a weaksupervision for in-the-wild images without ground truth 3D annotations. Due to the rich representation of the 3D mesh model, this data-driven prior can capture joint angle limits, anthropometric constraints (e.g. height, weight, bone ratios), and subsumes the geometric priors used by models that only predict 3D joint locations <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b50">51]</ref>. When ground truth 3D information is available, we may use it as an intermediate loss. In all, our overall objective is</p><formula xml:id="formula_0">L = λ(L reproj + ✶L 3D ) + L adv (1)</formula><p>where λ controls the relative importance of each objective, ✶ is an indicator function that is 1 if ground truth 3D is available for an image and 0 otherwise. We show results with and without the 3D loss. We discuss each component in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Body Representation</head><p>We encode the 3D mesh of a human body using the Skinned Multi-Person Linear (SMPL) model <ref type="bibr" target="#b23">[24]</ref>. SMPL is a generative model that factors human bodies into shape -how individuals vary in height, weight, body proportions -and pose -how the 3D surface deforms with articulation. The shape β ∈ R 10 is parameterized by the first 10 coefficients of a PCA shape space. The pose θ ∈ R 3K is modeled by relative 3D rotation of K = 23 joints in axis-angle representation. SMPL is a differentiable function that outputs a triangulated mesh with N = 6980 vertices, M (θ, β) ∈ R 3×N , which is obtained by shaping the template body vertices conditioned on β and θ, then articulating the bones according to the joint rotations θ via forward kinematics, and finally deforming the surface with linear blend skinning. The 3D keypoints used for reprojection error, X(θ, β) ∈ R 3×P , are obtained by linear regression from the final mesh vertices.</p><p>We employ the weak-perspective camera model and solve for the global rotation R ∈ R 3×3 in axis-angle representation, translation t ∈ R 2 and scale s ∈ R. Thus the set of parameters that represent the 3D reconstruction of a human body is expressed as a 85 dimensional vector Θ = {θ, β, R, t, s}. Given Θ, the projection of X(θ, β) iŝ</p><formula xml:id="formula_1">x = sΠ(RX(θ, β)) + t,<label>(2)</label></formula><p>where Π is an orthographic projection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Iterative 3D Regression with Feedback</head><p>The goal of the 3D regression module is to output Θ given an image encoding φ such that the joint reprojection error</p><formula xml:id="formula_2">L reproj = Σ i ||v i (x i −x i )|| 1 ,<label>(3)</label></formula><p>is minimized. Here x i ∈ R 2×K is the ith ground truth 2D joints and v i ∈ {0, 1} K is the visibility (1 if visible, 0 otherwise) for each of the K joints.</p><p>However, directly regressing Θ in one go is a challenging task, particularly because Θ includes rotation parameters. In this work, we take inspiration from previous works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b30">31]</ref> and regress Θ in an iterative error feedback (IEF) loop, where progressive changes are made recurrently to the current estimate. Specifically, the 3D regression module takes the image features φ and the current parameters Θ t as an input and outputs the residual ∆Θ t . The parameter is updated by adding this residual to the current estimate Θ t+1 = Θ t + ∆Θ t . The initial estimate Θ 0 is set as the meanΘ. In <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> the estimates are rendered to an image space to concatenate with the image input. In this work, we keep everything in the latent space and simply concatenate the features [φ, Θ] as the input to the regressor. We find that this works well and is suitable when differentiable rendering of the parameters is non-trivial.</p><p>Additional direct 3D supervision may be employed when paired ground truth 3D data is available. The most common form of 3D annotation is the 3D joints. Supervision in terms of SMPL parameters [β, θ] may be obtained through MoSh <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b47">48]</ref> when raw 3D MoCap marker data is available. Below are the definitions of the 3D losses. We show results with and without using any direct supervision L 3D .</p><formula xml:id="formula_3">L 3D = L 3D joints + L 3D smpl (4) L joints = ||(X i −X i )|| 2 2<label>(5)</label></formula><formula xml:id="formula_4">L smpl = ||[β i , θ i ] − [β i ,θ i ]|| 2 2 .<label>(6)</label></formula><p>Both <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> use a "bounded" correction target to supervise the regression output at each iteration. However this assumes that the ground truth estimate is always known, which is not the case in our setup where many images do not have ground truth 3D annotations. As noted by these approaches, supervising each iteration with the final objective forces the regressor to overshoot and get stuck in local minima. Thus we only apply L reproj and L 3D on the final estimate Θ T , but apply the adversarial loss on the estimate at every iteration Θ t forcing the network to take corrective steps that are on the manifold of 3D human bodies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Factorized Adversarial Prior</head><p>The reprojection loss encourages the network to produce a 3D body that explains the 2D joint locations, however anthropometrically implausible 3D bodies or bodies with gross self-intersections may still minimize the reprojection loss. To regularize this, we use a discriminator network D that is trained to tell whether SMPL parameters correspond to a real body or not. We refer to this as an adversarial prior as in <ref type="bibr" target="#b46">[47]</ref> since the discriminator acts as a data-driven prior that guides the 3D inference.</p><p>A further benefit of employing a rich, explicit 3D representation like SMPL is that we precisely know the meaning of the latent space. In particular SMPL has a factorized form that we can take advantage of to make the adversary more data efficient and stable to train. More concretely, we mirror the shape and pose decomposition of SMPL and train a discriminator for shape and pose independently. The pose is based on a kinematic tree, so we further decompose the pose discriminators and train one for each joint rotation. This amounts to learning the angle limits for each joint. In order to capture the joint distribution of the entire kinematic tree, we also learn a discriminator that takes in all the rotations. Since the input to each discriminator is very low dimensional (10-D for β, 9-D for each joint and 9K-D for all joints), they can each be small networks, making them rather stable to train. All pose discriminators share a common feature space of rotation matrices and only the final classifiers are learned separately.</p><p>Unlike previous approaches that make a priori assumptions about the joint limits <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b51">52]</ref>, we do not predefine the degrees of freedom of the kinematic skeleton model. Instead this is learned in a data-driven manner through this factorized adversarial prior. Without the factorization, the network does not learn to properly regularize the pose and shape, producing visually displeasing results. The importance of the adversarial prior is paramount when no paired 3D supervision is available. Without the adversarial prior the network produces totally unconstrained human bodies as we show in section 4.3.</p><p>While mode collapse is a common issue in GANs <ref type="bibr" target="#b9">[10]</ref> we do not really suffer from this because the network not only has to fool the discriminator but also has to minimize the reprojection error. The images contain all the modes and the network is forced to match them all. The factorization may further help to avoid mode collapse since it allows generalization to unseen body shape and poses combinations.</p><p>In all we train K + 2 discriminators. Each discriminator D i outputs values between [0, 1], representing the probability that Θ came from the data. In practice we use the least square formulation <ref type="bibr" target="#b24">[25]</ref> for its stability. Let E represent the encoder including the image encoder and the 3D module. Then the adversarial loss function for the encoder is</p><formula xml:id="formula_5">min L adv (E) = i E Θ∼p E [(D i (E(I)) − 1) 2 ],<label>(7)</label></formula><p>and the objective for each discriminator is</p><formula xml:id="formula_6">min L(D i ) = E Θ∼pdata [(D i (Θ)−1) 2 ]+E Θ∼p E [D i (E(I)) 2 ]. (8)</formula><p>We optimize E and all D i s jointly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>Datasets: The in-the-wild image datasets annotated with 2D keypoints that we use are LSP, LSP-extended <ref type="bibr" target="#b16">[17]</ref> MPII <ref type="bibr" target="#b2">[3]</ref> and MS COCO <ref type="bibr" target="#b21">[22]</ref>. We filter images that are too small or have less than 6 visible keypoints and obtain training sets of sizes 1k, 10k, 20k and 80k images respectively. We use the standard train/test split of these datasets. All test results are obtained using the ground truth bounding box.</p><p>For the 3D datasets we use Human3.6M <ref type="bibr" target="#b15">[16]</ref> and MPI-INF-3DHP <ref type="bibr" target="#b27">[28]</ref>. We leave aside sequences from training Subject 8 of MPI-INF-3DHP as the validation set to tune hyper-parameters, and use the full training set for the final experiments. Both datasets are captured in a controlled environment and provide 150k training images with 3D joint annotations. For Human3.6M, we also obtain ground truth SMPL parameters for the training images using MoSh <ref type="bibr" target="#b22">[23]</ref> from the raw 3D MoCap markers.</p><p>All images are scaled to 224 × 224 preserving the aspect ratio such that the diagonal of the tight bounding box is roughly 150px (see <ref type="bibr" target="#b16">[17]</ref>). The images are randomly scaled, translated, and flipped. Mini-batch size is 64. When paired 3D supervision is employed each mini-batch is balanced such that it consists of half 2D and half 3D samples. All experiments use all datasets with paired 3D loss unless otherwise specified.</p><p>The definition of the K = 23 joints in SMPL do not align perfectly with the common joint definitions used by these datasets. We follow <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> and use a regressor to obtain the 14 joints of Human3.6M from the reconstructed mesh. In addition, we also incorporate the 5 face keypoints from the MS COCO dataset <ref type="bibr" target="#b21">[22]</ref>. New keypoints can easily be incorporated with the mesh representation by specifying the corresponding vertex IDs 1 . In total the reprojection error is computed over P = 19 keypoints. Architecture: We use the ResNet-50 network <ref type="bibr" target="#b14">[15]</ref> for encoding the image, pretrained on the ImageNet classification task <ref type="bibr" target="#b38">[39]</ref>. The ResNet output is average pooled, producing features φ ∈ R 2048 . The 3D regression module consists of two fully-connected layers with 1024 neurons each with a dropout layer in between, followed by a final layer of 85D neurons. We use T = 3 iterations for all of our experiments. The discriminator for the shape is two fully-connected layers with 10, 5, and 1 neurons. For pose, θ is first converted to K many 3 × 3 rotation matrices via the Rodrigues formula. Each rotation matrix is sent to a common embedding </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>Although we recover much more than 3D skeletons, evaluating the result is difficult since no ground truth mesh 3D annotations exist for current datasets. Consequently we evaluate quantitatively on the standard 3D joint estimation task. We also evaluate an auxiliary task of body part segmentation. In <ref type="figure">Figure 1</ref> we show qualitative results on challenging images from MS COCO <ref type="bibr" target="#b21">[22]</ref> with occlusion, clutter, truncation, and complex poses. Note how our model recovers head and limb orientations. In <ref type="figure" target="#fig_2">Figure 3</ref> we show results on the test set of Human3.6M, MPI-INF-3DHP, LSP and MS COCO at various error percentiles. Our approach recovers reasonable reconstructions even at 95th percentile error. Please see the project website 2 for more results. In all figures, results on the model trained with and without paired 2D-to-3D supervision are rendered in light blue and light pink colors respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">3D Joint Location Estimation</head><p>We evaluate 3D joint error on Human3.6M, a standard 3D pose benchmark captured in a lab environment. We also compare with the more recent MPI-INF-3DHP <ref type="bibr" target="#b26">[27]</ref>, a dataset covering more poses and actor appearances than Human3.6M. While the dataset is more diverse, it is still far from the complexity and richness of in-the-wild images.</p><p>We report using several error metrics that are used for evaluating 3D joint error. Most common evaluations report the mean per joint position error (MPJPE) and Reconstruction error, which is MPJPE after rigid alignment of the prediction with ground truth via Procrustes Analysis <ref type="bibr" target="#b10">[11]</ref>). Reconstruction error removes global misalignments and evaluates the quality of the reconstructed 3D skeleton.</p><p>Human3.6M We evaluate on two common protocols. The first, denoted P1, is trained on 5 subjects (S1, S5, S6, S7, S8) and tested on 2 (S9, S11). Following previous work <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b35">36]</ref>, we downsample all videos from 50fps to 10fps to reduce redundancy. The second protocol <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b43">44]</ref>, P2, uses the same train/test set, but is tested only on the frontal camera (camera 3) and reports reconstruction error.</p><p>We compare results for our method (HMR) on P2 in <ref type="table" target="#tab_0">Table 1</ref> with two recent approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> that also output SMPL parameters from a single image. Both approaches require 2D keypoint detection as input and we out-perform both by a large margin. We show results on P1 in <ref type="table" target="#tab_2">Table 2</ref>. Here we also out-perform the recent approach of Zhou et al. <ref type="bibr" target="#b51">[52]</ref>, which also outputs 3D joint angles in a kinematic tree instead of joint positions. Note that they specify the DoF of each joint by hand, while we learn this from data. They also assume a fixed bone length while we solve for shape. HMR is competitive with recent state-of-the-art methods that only predict the 3D joint locations.</p><p>We note that MPJPE does not appear to correlate well with the visual quality of the results. We find that many results with high MPJPE appear quite reasonable as shown in <ref type="figure" target="#fig_2">Figure 3</ref>, which shows results at various error percentiles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MPI-INF-3DHP</head><p>The test set of MPI-INF-3DHP consists of 2929 valid frames from 6 subjects performing 7 actions. This dataset is collected indoors and outdoors with a multicamera marker-less MoCap system. Because of this, the ground truth 3D annotations have some noise. In addition to MPJPE, we report the Percentage of Correct Keypoints (PCK) thresholded at 150mm and the Area Under the Curve (AUC) over a range of PCK thresholds <ref type="bibr" target="#b26">[27]</ref>.</p><p>The results are shown in <ref type="table" target="#tab_4">Table 3</ref>. All methods use the perspective correction of <ref type="bibr" target="#b26">[27]</ref>. We also report metrics after rigid alignment for HMR and VNect using the publicly available code <ref type="bibr" target="#b27">[28]</ref>. The VNect numbers are computed on 3D joints before the post-processing optimization. Again,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Reconst. Error Rogez et al. <ref type="bibr" target="#b34">[35]</ref> 87.3 Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref> 51.9 Martinez et al. <ref type="bibr" target="#b25">[26]</ref> 47.7 *Regression Forest from 91 kps <ref type="bibr" target="#b19">[20]</ref> 93.9 *SMPLify <ref type="bibr" target="#b4">[5]</ref> 82.3 *SMPLify from 91 kps <ref type="bibr" target="#b19">[20]</ref> 80.7 *HMR 56.8 *HMR unpaired 66.5  <ref type="bibr" target="#b43">[44]</ref> 88.39 Rogez et al. <ref type="bibr" target="#b35">[36]</ref> 87.7 71.6 VNect et al. <ref type="bibr" target="#b27">[28]</ref> 80.5 Pavlakos et al. <ref type="bibr" target="#b32">[33]</ref> 71.9 51.23 Mehta et al. <ref type="bibr" target="#b26">[27]</ref> 68.6 Sun et al. <ref type="bibr" target="#b39">[40]</ref> 59.1 *Deep Kinematic Pose <ref type="bibr" target="#b51">[52]</ref>   we are competitive with approaches that are trained to output 3D joints and we improve upon VNect after rigid alignment.   on LSP <ref type="bibr" target="#b19">[20]</ref>. Reporting average accuracy and F1-score (higher the better). Proposed HMR is comparable to the oracle SMPLify which uses ground truth segmentation in fitting SMPL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Human Body Segmentation</head><p>We also evaluate our approach on the auxiliary task of human body segmentation on the 1000 test images of LSP <ref type="bibr" target="#b16">[17]</ref> labeled by <ref type="bibr" target="#b19">[20]</ref>. The images have labels for six body part segments and the background. Note that LSP contains complex poses of people playing sports and no ground truth 3D labels are available for training. We do not use the segmentation label during training either.</p><p>We report the segmentation accuracy and average F1 score over all parts including the background as done in <ref type="bibr" target="#b19">[20]</ref>. We also report results on foreground-background segmentation. Note that the part definition segmentation of the SMPL mesh is not exactly the same as that of annotation; this limits the best possible accuracy to be less than 100%.</p><p>Results are shown in <ref type="table" target="#tab_5">Table 4</ref>. Our results are comparable to the SMPLify oracle <ref type="bibr" target="#b19">[20]</ref>, which uses ground truth segmentation and keypoints as the optimization target. It also out-performs the Decision Forests of <ref type="bibr" target="#b19">[20]</ref>. Note that HMR is also real-time given a bounding box.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Without Paired 3D Supervision</head><p>So far we have used paired 2D-to-3D supervision, i.e. L 3D whenever available. Here we evaluate a model trained without any paired 3D supervision. We refer to this setting as HMR unpaired and report numerical results in all the tables. All methods that report results on the 3D joint estimation task rely on direct 3D supervision and cannot train without it. Even methods that are based on a reprojection loss <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b49">50]</ref> require paired 2D-to-3D training data.</p><p>The results are surprisingly competitive given this chal-  lenging setting. Note that the adversarial prior is essential for training without paired 2D-to-3D data. <ref type="figure" target="#fig_4">Figure 5</ref> shows that a model trained with neither the paired 3D supervision nor the adversarial loss produces monsters with extreme shape and poses. It remains open whether increasing the amount of 2D data will significantly increase 3D accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper we present an end-to-end framework for recovering a full 3D mesh model of a human body from a single RGB image. We parameterize the mesh in terms of 3D joint angles and a low dimensional linear shape space, which has a variety of practical applications. In this past few years there has been rapid progress in single-view 3D pose prediction on images captured in a controlled environment. Although the performance on these benchmarks is starting to saturate, there has not been much progress on 3D human reconstruction from images in-the-wild. Our results without using any paired 3D data are promising since they suggest that we can keep on improving our model using more images with 2D labels, which are relatively easy to acquire, instead of ground truth 3D, which is considerably more challenging to acquire in a natural setting.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Figure 2: Overview of the proposed framework. An image I is passed through a convolutional encoder. This is sent to an iterative 3D regression module that infers the latent 3D representation of the human that minimizes the joint reprojection error. The 3D parameters are also sent to the discriminator D, whose goal is to tell if these parameters come from a real human shape and pose.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1</head><label></label><figDesc>The vertex ids in 0-indexing are nose: 333, left eye: 2801, right eye: 6261, left ear: 584, right ear: 4072. network of two fully-connected layers with 32 hidden neu- rons. Then the outputs are sent to K = 23 different discrim- inators that output 1-D values. The discriminator for over- all pose distribution concatenates all K * 32 representations through another two fully-connected layers of 1024 neurons each and finally outputs a 1D value. All layers use ReLU activations except the final layer. The learning rates of the encoder and the discriminator network are set to 1 × 10 −5 and 1×10 −4 respectively. We use the Adam solver [18] and train for 55 epochs. Training on a single Titan 1080ti GPU takes around 5 days. The λs and other hyper-parameters are set through validation data on MPI-INF-3DHP dataset. Implementation is in Tensorflow [1].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results sampled from different datasets at the 15th, 30th, 60th, 90th and 95th error percentiles. Percentiles are computed using MPJPE for 3D datasets (first two rows -Human3.6M and MPI-INF-3DHP) and 2D pose PCK for 2D datasets (last two rows -LSP and MS COCO). High percentile indicates high error. Note results at high error percentile are often semantically quite reasonable.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results with and without paired 3D supervision. 3D reconstructions, without direct 3D supervision, are very close to those of the supervised model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: No Discriminator No 3D. With neither the discriminator, nor the direct 3D supervision, the network produces monsters. On the right of each example we visualize the ground truth keypoint annotation in unfilled circles, and the projection in filled circles. Note that despite the unnatural pose and shape, its 2D projection error is very accurate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Human3.6M, Protocol 2. Showing reconstruction loss (mm); * indicates methods that output more than 3D joints. HMR, with and without direct 3D supervision, out-performs previous ap- proaches that output SMPL from 2D keypoints. Method MPJPE Reconst. Error Tome et al.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Human3.6M, Protocol 1. MPJPE and reconstruction loss in mm. * indicates methods that output more than 3D joints.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Absolute After Rigid Alignment Method PCK AUC MPJPE PCK AUC MPJPE Mehta et al. [27] 75.7 39.3</figDesc><table>117.6 
-
-
-
VNect [28] 
76.6 40.4 
124.7 
83.9 47.3 
98.0 
*HMR 
72.9 36.5 
124.2 
86.3 47.8 
89.8 
*HMR unpaired 59.6 27.9 
169.5 
77.1 40.7 
113.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Results on MPI-INF-3DHP with and without rigid alignment. * are methods that output more than 3D joints. Ac- curacy increases with alignment (PCK and AUC increase, while MPJPE decreases).</figDesc><table>Method 
Fg vs Bg 
Parts 
Run Time 
Acc 
F1 
Acc 
F1 
SMPLify oracle[20] 92.17 0.88 88.82 0.67 -
SMPLify [5] 
91.89 0.88 87.71 0.64 ∼1 min 
Decision Forests[20] 86.60 0.80 82.32 0.51 0.13 sec 
HMR 
91.67 0.87 87.12 0.60 0.04 sec 
HMR unpaired 
91.30 0.86 87.00 0.59 0.04 sec 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Foreground and part segmentation (6 parts + bg)</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://akanazawa.github.io/hmr/</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank N. Mahmood for the SMPL model fits to mocap data and the mesh retargeting for character animation, D. Mehta for his assistance on MPI-INF-3DHP, and S. Tulsiani, A. Kar, S. Gupta, D. Fouhey and Z. Liu for helpful discussions. This research was supported by BAIR sponsors and NSF Award IIS-1526234.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: A system for large-scale machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Operating Systems Design and Implementation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Pose-conditioned joint angle limits for 3D human pose reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Akhter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Estimating anthropometry and pose from a single uncalibrated image. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kakadiaris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="269" to="284" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Keep it SMPL: Automatic estimation of 3D human pose and shape from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cmu graphics lab -motion capture library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nsf Eia-0196217</surname></persName>
		</author>
		<ptr target="http://mocap.cs.cmu.edu/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inferring 3D shapes and deformations from single views</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Cascaded pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1078" to="1085" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Generalized procrustes analysis. Psychometrika</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Gower</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975-03" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="33" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Estimating human shape and pose from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1381" to="1388" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densereg: Fully convolutional dense shape regression in-the-wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Güler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Trigeorgis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Multilinear pose and body shape estimation of dressed subjects from image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Hasler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Rosenhahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Thormhlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Seidel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">6M: Large scale datasets and predictive methods for 3D human sensing in natural environments. pami</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Papava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Olaru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Human3</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Clustered pose and nonlinear appearance models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">bmvc</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="12" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Picture: A probabilistic programming language for scene perception</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mansinghka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4390" to="4399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unite the people: Closing the loop between 3d and 2d human representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Determination of 3D human body postures from a single view. Computer Vision Graphics and Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="148" to="168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<meeting><address><addrLine>Zrich</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">MoSh: Motion and shape capture from sparse markers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>220:1-220:13</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of ACM SIGGRAPH Asia</title>
		<meeting>ACM SIGGRAPH Asia</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">SMPL: A skinned multi-person linear model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pons-Moll</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno>248:1- 248:16</idno>
	</analytic>
	<monogr>
		<title level="m">Proc. SIGGRAPH Asia)</title>
		<meeting>SIGGRAPH Asia)</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
		<title level="m">Least squares generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A simple yet effective baseline for 3d human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martinez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hossain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Monocular 3d human pose estimation in the wild using improved cnn supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of International Conference on 3D Vision (3DV</title>
		<meeting>of International Conference on 3D Vision (3DV</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Vnect: Real-time 3d human pose estimation with a single rgb camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mehta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sridhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sotnychenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rhodin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shafiei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-P</forename><surname>Seidel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Casas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Theobalt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG) -Proceedings of ACM SIGGRAPH</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">3d human pose estimation from a single image via distance matrix regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09010</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="483" to="499" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Training a feedback loop for hand pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oberweger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lepetit</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3316" to="3324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">View independent human body pose estimation from a single perspective image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Parameswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="16" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Coarse-to-fine volumetric prediction for single-image 3D human pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pavlakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Reconstructing 3d Human Pose from 2d Image Landmarks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Mocap-guided data augmentation for 3d pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">LCR-Net: Localization-Classification-Regression for Human Pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rogez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bayesian image based 3d pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sanzari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ntouskos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pirri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="566" to="582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">HumanEva: Synchronized video and motion capture dataset and baseline algorithm for evaluation of articulated human motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Balan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
	<note>IJCV</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Tensorflowslim image classification model library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<ptr target="https" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Compositional human pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Indirect deep structured learning for 3d human shape and pose prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K V</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Budvytis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Reconstruction of articulated objects from point correspondences in single uncalibrated image. Computer Vision and Image Understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Learning to Fuse 2D and 3D Image Cues for Monocular Body Pose Estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Tekin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Marquez Neila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Lifting from the deep: Convolutional 3d pose estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">DeepPose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Viewpoints and keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tulsiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1510" to="1519" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Adversarial inverse graphics networks: Learning 2d-to-3d lifting and image-to-image translation from unpaired supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><forename type="middle">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Seto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Learning from Synthetic Humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mahmood</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Single image 3d interpreter network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision, ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Weaklysupervised transfer for 3d human pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Deep kinematic pose regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshop on Geometry Meets Deep Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Sparse representation for 3D shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4447" to="4455" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Sparseness meets deepness: 3D human pose estimation from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Derpanis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition, CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4966" to="4975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
