<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:29+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Bao</surname></persName>
							<email>jmbao@mail.ustc.edu.cndoch</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Houqiang</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<email>ganghua@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">CVAE-GAN: Fine-Grained Image Generation through Asymmetric Training</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Building effective generative models of natural images is one of the key problems in computer vision. It aims to generate diverse realistic images by varying some latent parameters according to the underlying natural image distributions. Therefore, a desired generative model is necessitated to capture the underlying data distribution. This is often a very difficult task, since a collection of image samples may lie on a very complex manifold. Nevertheless, recent advances in deep convolutional neural networks have spawned a series of deep generative models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref> that have made tremendous progress, largely due to the capability of deep networks in learning representations.</p><p>Building on top of the success of these recent works, we want to go one step further to generate images of fine-grained object categories. For example, we want to be able to synthesize images for a specific identity <ref type="figure" target="#fig_0">(Figure 1</ref>), or produce a new image of a specified species of flowers or birds, and so on. Inspired by CVAE <ref type="bibr" target="#b33">[34]</ref> and VAE/GAN <ref type="bibr" target="#b14">[15]</ref>, we propose a general learning framework that combines a variational auto-encoder with a generative adversarial network under a conditioned generative process to tackle this problem.</p><p>However, we find this naïve combination insufficient in practice. The results from VAE are usually blurry. The discriminator can easily classify them as "fake", even though they sometimes look remarkably good for face images, the gradient vanishing problem still exists. Thus, the generated images are very similar to the results from using VAE alone.</p><p>In this paper, we propose a new objective for the generator. Instead of using the same cross entropy loss as the discriminator network, the new objective requires the generator to generate data that minimize the ℓ 2 distance of the mean feature to the real data. For multi-class image generation, the generated samples of one category also need to match the average feature of real data of that category, since the feature distance and the separability are positively correlated. It solves the gradient vanishing problem to a certain extent. This kind of asymmetric loss function can partially help prevent the mode collapse problem that all outputs moving toward a single point, making the training of GAN more stable.</p><p>Although using mean feature matching will reduce the chance of mode collapse, it does not completely solve the problem. Once mode collapse occurs, the gradient descent is unable to separate identical outputs. To keep the diversity of generated samples, we take advantage of the combination of VAE and GAN. We use an encoder network to map the real image to the latent vector. Then the generator is required to reconstruct the raw pixels and match the feature of original images with a given latent vector. In this way, we explicitly set up the relationship between the latent space and real image space. Because of the existence of these anchor points, the generator is enforced to emit diverse samples. Moreover, the pixel reconstruction loss is also helpful for maintaining the structure, such as a straight line or a facial structure in an image.</p><p>As shown in <ref type="figure">Figure 2</ref> (g), our framework consists of four parts: 1) The encoder network E, which maps the data sample x to a latent representation z.</p><p>2) The generative network G, which generates image x ′ given a latent vector.</p><p>3) The discriminative network D, which distinguishes real/fake images. 4) The classifier network C, which measures the class probability of the data. These four parts are seamlessly cascaded together, and the whole pipeline is trained end-to-end. We call our approach CVAE-GAN.</p><p>Once the CVAE-GAN is trained, it can be used in different applications, e.g., image generation, image inpainting, and attributes morphing. Our approach estimates a good representation of the input image, and the generated image appears to be more realistic. We show that it outperforms CVAE, CGAN, and other state-of-the-art methods. Compared with GAN, the proposed framework is much easier to train and converges faster and more stable in the training stage. In our experiments, we further show that the images synthesized from our models can be applied to other tasks, such as data augmentation for training better face recognition models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Conventional wisdom and early research of generative models, including Principle Component Analysis (P-CA) <ref type="bibr" target="#b39">[40]</ref>, Independent Component Analysis (ICA) <ref type="bibr" target="#b9">[10]</ref>,and the Gaussian Mixture Model (GMM) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b36">37]</ref>, all assume a simple formation of data. They have difficulty modeling complex patterns of irregular distributions. Later works, such as the Hidden Markov Model (HMM) <ref type="bibr" target="#b34">[35]</ref>, Markov Random Field (MRF) <ref type="bibr" target="#b18">[19]</ref>, and restricted Boltzmann machines (RBMs) <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b31">32]</ref>, discriminatively train generative models <ref type="bibr" target="#b38">[39]</ref>, limiting their results on texture patches, digital numbers or well aligned faces, due to a lack of effective feature representations.</p><p>There have been many recent developments of deep generative models <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b5">6]</ref>. Since deep hierarchical architectures allow them to capture complex structures in the data, all these methods show promising results in generating natural images that are far more realistic than conventional generative models. Among them are <ref type="figure">Figure 2</ref>. Illustration of the structure of VAE <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>, GAN <ref type="bibr" target="#b7">[8]</ref>, VAE/GAN <ref type="bibr" target="#b14">[15]</ref>, CVAE <ref type="bibr" target="#b33">[34]</ref>, CGAN <ref type="bibr" target="#b17">[18]</ref>, PPGN <ref type="bibr" target="#b22">[23]</ref> and the proposed CVAE-GAN. Where x and x ′ are input and generated image. E,G,C,D are encoder, generative, classification, and discriminative network, respectively. z is the latent vector. y is a binary output which represents real/synthesized image. c is the condition, such as attribute or class label. three main themes: Variational Auto-encoder (VAE) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref>, Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref>, and Autoregression <ref type="bibr" target="#b13">[14]</ref>.</p><formula xml:id="formula_0">x E z G x' (a) VAE: x' D z G (b) GAN: y x' D z G (c) VAE/GAN: y x E (d) CVAE: x E z G x' c c (e) CGAN: c x' D z G y c (g) CVAE-GAN: (this work) c x' C z G x E c c y D (f) PPGN: c x' C z G x C y D</formula><p>VAE <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b30">31]</ref> pairs a differentiable encoder network with a decoder/generative network. A disadvantage of VAE is that, because of the injected noise and imperfect elementwise measures such as the squared error, the generated samples are often blurry.</p><p>Generative Adversarial Network (GAN) <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b32">33]</ref> is another popular generative model. It simultaneously trains two models: a generative model to synthesize samples, and a discriminative model to differentiate between natural and synthesized samples. However, the GAN model is hard to converge in the training stage and the samples generated from GAN are often far from natural. Recently, many works have tried to improve the quality of the generated samples. For example, the Wasserstein GAN (WGAN) <ref type="bibr" target="#b1">[2]</ref> uses Earth Mover Distance as an objective for training GANs, and McGAN <ref type="bibr" target="#b19">[20]</ref> uses mean and covariance feature matching. They need to limit the range of the parameters of the discriminator which will decrease discriminative power. Loss-Sensitive GAN <ref type="bibr" target="#b27">[28]</ref> learns a loss function which can quantify the quality of generated samples and uses this loss function to generate high-quality images. There are also methods which tried to combine GAN and VAE, e.g., VAE/GAN <ref type="bibr" target="#b14">[15]</ref> and adversarial autoencoders <ref type="bibr" target="#b16">[17]</ref>. They are closely related to and partly inspired our work.</p><p>VAEs and GANs can also be trained to conduct conditional generation, e.g., CVAE <ref type="bibr" target="#b33">[34]</ref> and CGAN <ref type="bibr" target="#b17">[18]</ref>. By introducing additional conditionality, they can handle probabilistic one-to-many mapping problems. Recently there have been many interesting works based on CVAE and CGAN, including conditional face generation <ref type="bibr" target="#b6">[7]</ref>, Attribute2Image <ref type="bibr" target="#b46">[47]</ref>, text to image synthesis <ref type="bibr" target="#b29">[30]</ref>, forecasting from static images <ref type="bibr" target="#b41">[42]</ref>, and conditional image synthesis <ref type="bibr" target="#b24">[25]</ref>. All of them achieve impressive results.</p><p>Generative ConvNet <ref type="bibr" target="#b43">[44]</ref>, demonstrates that a generative model can be derived from the commonly used discriminative ConvNet. Dosovitskiy et al. <ref type="bibr" target="#b4">[5]</ref> and Nguyen et al. <ref type="bibr" target="#b21">[22]</ref> introduce a method that generates high quality images from features extracted from a trained classification model. PPGN <ref type="bibr" target="#b22">[23]</ref> performs exceptionally well in generating samples by using a gradient ascent and prior to the latent space of a generator.</p><p>Autoregression <ref type="bibr" target="#b13">[14]</ref> follows a different idea. It uses autoregressive connections to model images pixel by pixel. Its two variants, PixelRNN <ref type="bibr" target="#b40">[41]</ref> and PixelCNN <ref type="bibr" target="#b25">[26]</ref>, also produce excellent samples.</p><p>Our model differs from all these models. As illustrated in <ref type="figure">Figure 2</ref>, we compare the structure of the proposed CVAE-GAN with all these models. Besides the difference in the structure, more importantly, we take advantages of both statistic and pairwise feature matching to make the training process converge faster and more stable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Formulation of CVAE-GAN</head><p>In this section, we introduce the proposed CVAE-GAN networks. As shown in <ref type="figure">Figure 3</ref>, our proposed method contains four parts: 1) the encoder network E; 2) the generative network G; 3) the discriminative network D; and 4) the classification network C.</p><p>The function of networks E and G is the same as that in conditional variational auto-encoder (CVAE) <ref type="bibr" target="#b33">[34]</ref>. The encoder network E maps the data sample x to a latent representation z through a learned distribution P (z|x, c), where c is the category of the data. The generative network G generates image x ′ by sampling from a learned distribution P (x|z, c). The function of network G and D is the same as that in the generative adversarial network (GAN) <ref type="bibr" target="#b7">[8]</ref>. The network G tries to learn the real data distribution by the gradients given by the discriminative network D which learns to distinguish between "real" and "fake" samples. The function of network C is to measure the posterior P (c|x).</p><p>However, the naïve combination of VAE and GAN is insufficient. Recent work <ref type="bibr" target="#b0">[1]</ref> shows that the training of GAN will suffer from a gradient vanishing or instability problem with network G. Therefore, we only keep the training process of networks E, D, and C the same as the original VAE <ref type="bibr" target="#b11">[12]</ref> and GAN <ref type="bibr" target="#b7">[8]</ref>, and propose a new mean feature matching objective for the generative network G to improve the stability of the original GAN.</p><p>Even with the mean feature matching objective, there is still some risk of mode collapse. So we use the encoder net- <ref type="figure">Figure 3</ref>. Illustration of our network structure. Our model contains four parts: 1) The encoder network E; 2) The generative network G;</p><formula xml:id="formula_1">x C ( ′) ( ) ( ′) ( ) D(x) x E z G c c D( ) ( | ) D</formula><p>3) The classification network C; and 4) The discriminative network D. Please refer to Section 3 for details.</p><p>work E and the generative network G to obtain a mapping from real samples x to the synthesized samples x ′ . By using the pixel-wise ℓ 2 loss and pair-wise feature matching, the generative model is enforced to emit diverse samples and generate structure-preserving samples.</p><p>In the following sections, we begin by describing the method of mean feature matching based GAN (Section 3.1). Then we show that the mean feature matching can also be used in conditional image generation tasks (Section 3.2). After that, we introduce pair-wise feature matching by using an additional encoder network (Section 3.3). Finally, we analyse the objective of the proposed method and provide the implementation details in the training pipeline (Section 3.4).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Mean feature matching based GAN</head><p>In traditional GANs, the generator G and a discriminator D compete in a two-player minimax game. The discriminator tries to distinguish real training data from synthesized data; and the generator tries to fool the discriminator. Concretely, the network D tries to minimize the loss function</p><formula xml:id="formula_2">L D = −E x∼Pr [logD(x)] − E z∼Pz [log(1 − D(G(z))],<label>(1)</label></formula><p>while network G tries to minimize</p><formula xml:id="formula_3">L ′ GD = −E z∼Pz [log D(G(z))].</formula><p>In practice, the distributions of "real" and "fake" images may not overlap with each other, especially at the early stage of the training process. Hence, the discriminative network D can separate them perfectly. That is, we always have</p><formula xml:id="formula_4">D(x) → 1 and D(x ′ ) → 0, where x ′ = G(z)</formula><p>is the generated image. Therefore, when updating network G, the gradient ∂L</p><formula xml:id="formula_5">′ GD /∂D(x ′ ) → −∞.</formula><p>So the training process of network G will be unstable. Recent works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref> also theo-retically show that training GAN often has to deal with the unstable gradient of G.</p><p>To address this problem, we propose using a mean feature matching objective for the generator. The objective requires the center of the features of the synthesized samples to match the center of the features of the real samples. Let f D (x) denote features on an intermediate layer of the discriminator, G then tries to minimize the loss function</p><formula xml:id="formula_6">L GD = 1 2 ||E x∼Pr f D (x) − E z∼Pz f D (G(z))|| 2 2 .<label>(2)</label></formula><p>In our experiment, for simplicity, we choose the input of the last Fully Connected (FC) layer of network D as the feature f D . Combining the features of multiple layers could marginally improve the converging speed. In the training stage, we estimate the mean feature using the data in a minibatch. We also use moving historical averages to make it more stable. Therefore, in the training stage, we update network D using Eq. 1, and update network G using Eq. 2. Using this asymmetrical loss for training GAN has the following three advantages: 1) since Eq. 2 increases with the separability, the ℓ 2 loss on the feature center solves the gradient vanishing problem; 2) when the generated images are good enough, the mean feature matching loss becomes zero, making the training more stable; 3) compared with WGAN [2], we do not need to clip the parameters. The discriminative power of network D can be kept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Mean Feature Matching for Conditional Image Generation</head><p>In this section, we introduce mean feature matching for conditional image generation. Supposing we have a set of data belonging to K categories, we use the network C to measure whether an image belongs to a specific fine-grained category. Here we use a standard method for classification. The network C takes in x as input and outputs a Kdimensional vector, which then turns into class probabilities using a softmax function. The output of each entry represents the posterior probability P (c|x). In the training stage, the network C tries to minimize the softmax loss</p><formula xml:id="formula_7">L C = −E x∼Pr [log P (c|x)].<label>(3)</label></formula><p>For the network G, if we still use the similar softmax loss function as in Eqn. 3, it will suffer from the same gradient instability problem as described in <ref type="bibr" target="#b0">[1]</ref>. Therefore, we propose using the mean feature matching objective for generative network G. Let f C (x) denote features on an intermediate layer of the classification, then G tries to minimize:</p><formula xml:id="formula_8">L GC = 1 2 c ||E x∼Pr f C (x) − E z∼Pz f C (G(z, c))|| 2 2 .<label>(4)</label></formula><p>Here, we choose the input of the last FC layer of network C as the feature for simplicity. We also try to combine features of multiple layers, it only marginally improves the ability to preserve the identity of network G. Since there are only a few samples belonging to the same category in a minibatch, it is necessary to use moving averages of features for both real and generated samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pairwise Feature Matching</head><p>Although, using mean feature matching could prevent all outputs from moving toward a single point, thus reducing the likelihood of mode collapse, it does not completely solve this problem. Once mode collapse occurs, the generative network outputs the same images for different latent vectors, thus the gradient descent will not be able to separate these identical outputs. Moreover, despite the generated samples and real samples having the same feature center, they may have different distributions.</p><p>In order to generate diverse samples, DCGAN <ref type="bibr" target="#b28">[29]</ref> uses Batch Normalization, McGAN <ref type="bibr" target="#b19">[20]</ref> uses both mean and covariance feature statistics, and Salimans et al. <ref type="bibr" target="#b32">[33]</ref> use minibatch discrimination. They are all based on using multiple generated examples. Different from these methods, we add an encoder network E to obtain a mapping from the real image x to the latent space z. Therefore, we explicitly set up the relationship between the latent space and real image space.</p><p>Similar to VAE, for each sample, the encoder network outputs the mean and covariance of the latent vector, i.e., µ and ǫ. We use the KL loss to reduces the gap between the prior P (z) and the proposal distributions, i.e.,</p><formula xml:id="formula_9">L KL = 1 2 µ T µ + sum(exp(ǫ) − ǫ − 1) .<label>(5)</label></formula><p>We can then sample the latent vector z = µ+r⊙exp(ǫ), where r ∼ N (0, I) is a random vector and ⊙ represents the element-wise multiplication. After obtaining a mapping from x to z, we obtain the generated image x ′ with network G. Then, we add a ℓ 2 reconstruction loss and pair-wise feature matching loss between x and x ′ ,</p><formula xml:id="formula_10">L G = 1 2 (||x − x ′ || 2 2 + ||f D (x) − f D (x ′ )|| 2 2 + ||f C (x) − f C (x ′ )|| 2 2 ),<label>(6)</label></formula><p>where, f D and f C are the features of an intermediate layer of discriminative network D and classification network C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Objective of CVAE-GAN</head><p>The goal of our approach is to minimize the following loss function: where the exact forms of each of the terms are presented in Eqns. 1∼6. Every term of the above formula is meaningful. L KL is only related to the encoder network E. It represents whether the distribution of the latent vector is under expectation. L G , L GD and L GC are related to the generative network G. They represent whether the synthesized image is similar to the input training sample, the real image, and other samples within the same category, respectively. L C is related to the classification network C, which represents the capability of the network to classify images from different categories, and L D is related to the discriminative network, which represents how good the network is at distinguishing between real/synthesized images. All these objectives are complementary to each other, and ultimately enable our algorithm to obtain superior results. The whole training procedure is described in Algorithm 1. In our experiments. we empirically set λ 1 = 3, λ 2 = 1, λ 3 = 10 </p><formula xml:id="formula_11">L = L D + L C + λ 1 L KL + λ 2 L G + λ 3 L GD + λ 4 L GC , (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis of Toy Example</head><p>In this section, we present and demonstrate the benefits of the mean feature matching based GAN with a toy example. We assume that we have a real data distribution which is a "ring" as shown in <ref type="figure" target="#fig_1">Figure 4(a)</ref>. The center of the ring is set to (100, 100), such that it is far from the generated distribution at the beginning. We compare the traditional GAN, WGAN, and the mean feature matching based GAN introduced in Section 3.1 to learn the real data distribution.</p><p>The three compared models share the same settings. Generator G is an MLP with 3 hidden layers with 32, 64, and 64 units, respectively. Discriminator D is also an MLP with 3 hidden layers with 32, 64, and 64 units, respectively. We use RMSProp and a fixed learning rate of 0.00005 for all methods. We trained each model for 2M iterations until they all converge. The generated samples of each model at different iterations are plotted in <ref type="figure" target="#fig_1">Figure 4</ref>. From the results we can observe that: 1) For traditional GAN (first row in <ref type="figure" target="#fig_1">Figure 4(b)</ref>), the generated samples only lie in a limited area of the real data distribution, which is known as the mode collapse problem. This problem always exists during the training process. 2) For WGAN (second row in <ref type="figure" target="#fig_1">Figure 4(b)</ref>), it cannot learn the real data distribution at early iterations, we think this problem is caused by the clamping weights trick, which influence D's ability to distinguishing between real and fake samples. We also tried to vary the clamp values to accelerate the training process, and find that if the value is too small, it will cause a gradient vanishing problem. If too large, the network will diverge.</p><p>3) The third row shows the results of the proposed feature matching based GAN. It correctly learns the real data distribution the fastest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>In this section, we use experiments to validate the effectiveness of the proposed method. We evaluate our model on three datasets: the FaceScrub <ref type="bibr" target="#b20">[21]</ref>,the 102 Category Flower <ref type="bibr" target="#b23">[24]</ref>, and CUB-200 <ref type="bibr" target="#b42">[43]</ref> datasets. These three datasets contain three completely different objects, which are human faces, birds, and flowers, respectively.</p><p>The sizes of input and synthesized images are 128 × 128 for all experiments. For the FaceScrub dataset, we first detect the facial region with the JDA face detector <ref type="bibr" target="#b2">[3]</ref>, and then locate five facial landmarks (two eyes, nose tip and two mouth corners) with SDM <ref type="bibr" target="#b44">[45]</ref>. After that, we use similarity transformation based on the facial landmarks to align faces to a canonical position. Finally, we crop a 128 × 128 face region centered around the nose tip. For the 102 Category Flower dataset, we tightly crop a rectangle region based on the ground-truth mask which contains the flower, and then resize it into 128 × 128. For the CUB-200 dataset, we just use the original images from the dataset.</p><p>In our experiments, the encoder network E is a GoogleNet <ref type="bibr" target="#b35">[36]</ref>, The category information and the image is merged at the last FC layer of the E network. The G network consists of 2 fully-connected layers, followed by 6 deconv layers with 2-by-2 upsampling. The convolution layers have 256, 256, 128, 92, 64 and 3 channels with filter size of 3 × 3, 3 × 3, 5 × 5, 5 × 5, 5 × 5, 5 × 5. For the D network we use the same D network as the D-CGAN <ref type="bibr" target="#b28">[29]</ref>.For the C network, we use an Alexnet <ref type="bibr" target="#b12">[13]</ref> structure, and change the input to 128 × 128. We fix the latent vector dimension to be 256 and find this configuration sufficient for generating images. The batch normalization layer <ref type="bibr" target="#b10">[11]</ref> is also applied after each convolution layer. The model is implemented using the deep learning toolbox Torch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Visualization comparison with other models</head><p>In this experiment, we compare the proposed mean feature matching based CGAN introduced in Section 3.2 (FM-CGAN), and CVAE-GAN model with other generative models for image synthesis of fine-grained images.</p><p>In order to fairly compare each method, we use the same network structure and same training data for all methods.</p><p>All networks are trained from scratch. In the testing stage, the network architectures are the same. All three methods only use network G to generate images. Therefore, although our approach has more parameters in the training stage, we believe this comparison is fair.</p><p>We conduct experiments on three datasets: FaceScrub, 102 Category Flower and CUB-200 dataset. We perform category conditioned image generation for all methods. For each dataset, all methods are trained with all the data in that dataset. In the test stage, we first randomly chose a category c, and then randomly generate samples of that category by sampling the latent vector z ∼ N (0, I). For evaluation, we visualize the samples generated from all methods.</p><p>The comparison results are presented in <ref type="figure" target="#fig_3">Figure 5</ref>. All images are randomly selected without any personal bias. We observe that images generated by CVAE are often blurry. For traditional CGAN, the variation within a category is very small, which is because of the mode collapse. For FM-CGAN, we observe clear images with well preserved identities, but some images lose the structure of an object, such as the shape of the face. On the other hand, images generated by the proposed CVAE-GAN models look realistic and clear, and are non-trivially different from each other,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The training pipeline of the proposed CVAE-GAN algorithm.</p><p>Require: m, the batch size. n, class number. θE, initial E network parameters. θG, initial G network parameters. θD, initial D network parameters. θC , initial C network parameters, λ1 = 3, λ2 = 1, λ3 = 10 −3 and λ4 = 10 −3 .</p><p>1: while θG has not converged do 2:</p><p>Sample {xr, cr} ∼ Pr a batch from the real data;</p><p>3:</p><p>LC ← −log(P (cr|xr))</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>z ← E(xr, cr)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>LKL ← KL(q(z|xr, cr)||Pz)</p><p>6:</p><formula xml:id="formula_12">x f ← G(z, cr) 7:</formula><p>Sample zp ∼ Pz a batch of random noise, sample cp a batch of random classes; <ref type="bibr">8:</ref> xp ← G(zp, cp)</p><p>9:</p><formula xml:id="formula_13">LD ← −(log(D(xr)) + log(1-D(x f )) + log(1 -D(xp))) 10:</formula><p>Calculate xr feature center LGD ← LGC ← </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>14:</head><p>LG ← 1 2 </p><formula xml:id="formula_14">(||xr −x f || 2 2 +||fD(xr)−fD(x f )|| 2 2 +||fC (xr)− fC (x f )||</formula><formula xml:id="formula_15">θC + ←− −∇ θ C (LC ) 16: θD + ←− −∇ θ D (LD) 17: θG + ←− −∇ θ G (λ2LG + λ3LGD + λ4LGC ) 18: θE + ←− −∇ θ E (λ1LKL + λ2LG) 19:</formula><p>end while especially for view-point and background color. Our model is also able to keep the identity information. It shows the strength of the proposed CVAE-GAN method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Quantitative Comparison</head><p>Evaluating the quality of a synthesized image is challenging due to the variety of probabilistic criteria <ref type="bibr" target="#b37">[38]</ref>. We attempt to measure the generative model on three criteria: discriminability, diversity and realism.</p><p>We use face images for this experiment. First, we randomly generate 53k samples (100 for each class) from C-VAE, CGAN, FM-CGAN, and CVAE-GAN models for evaluation.</p><p>To measure discriminability, we use a pre-trained face classification network on the real data. Here we use GoogleNet <ref type="bibr" target="#b35">[36]</ref>. With this trained model, we evaluate the top-1 accuracy of the generated samples from each method. The results are shown in <ref type="table">Table 1</ref>. Our model achieves the best top-1 accuracy with a big gap to other generative models. This demonstrates the effectiveness of the proposed method.</p><p>Following the method in <ref type="bibr" target="#b32">[33]</ref>, we use the Inception Score to evaluate the realism and diversity of generated samples. We train a classification model on the CASIA <ref type="bibr" target="#b47">[48]</ref>   datasets, and adopt exp(E x KL(p(y|x)||p(y))) as the metric to measure the realism and diversity of the generative models, where p(y|x) represents the posterior probability of each class of generated samples. Images that contain meaningful objects should have a conditional label distribution p(y|x) with low entropy. Moreover, if the model generate diverse images, the marginal p(y) = p(y|G(z))dz should have high entropy. A larger score means the generator can produce more realistic and diverse images. The results are shown in <ref type="table">Table 1</ref>. Our proposed CVAE-GAN and FM-CGAN achieve better scores than the other models, which are also very close to the real data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Attributes Morphing</head><p>In this part, we validate that the attribute in the generated images will continuously change with the latent vector. We call this phenomenon attribute morphing. We also test our model on the FaceScrub, CUB-200 and 102 Category Flower datasets. We first select a pair of images x 1 and x 2 in the same category, and then extract the latent vector z 1 and z 2 using the encoder network E. Finally, we obtain a series of latent vectors z by linear interpolation,i.e., <ref type="figure" target="#fig_8">Figure 6</ref> shows the results of attribute morphing. In each row, the attribute, such as pose, emotion, color, or flower number, gradually changes from left to right.</p><formula xml:id="formula_16">z = αz 1 + (1 − α)z 2 , α ∈ [0, 1].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Image Inpainting</head><p>In this part, we show that our model can also be applied to image inpainting. We first randomly corrupt a 50 × 50 patch of an original 128 × 128 image x <ref type="figure">(Fig.7b)</ref>, and then feed it to the E network to obtain a latent vector z, then we can synthesize an image x ′ by G(z, c) where c is the class label, then we update the image by the following equation,i.e., results are shown in <ref type="figure">Figure 7</ref> (c). We should emphasize that all input images are downloaded from websites, with none of them belonging to the training data. We can iteratively feed the resulting images into the model to obtain a better results, as shown in <ref type="figure">Figure 7</ref> (d,e).</p><formula xml:id="formula_17">x = M ⊙ x ′ + (1 − M ) ⊙ x,<label>(8)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Comparing Different Combination of Losses</head><p>In our model, we propose using pairwise feature matching at the image pixel level, the feature level in the classification network C and the discriminative network D to update the network G. To understand the effects of each loss component, we separate the</p><formula xml:id="formula_18">L G + L GD + L GC to three parts: L G (img) + L G (D) + L G (C)</formula><p>, where L G (img) is the ℓ 2 distance at the pixel level of the image, L G (D) is the ℓ 2 distance at the feature level in the discriminative network D, L G (C) is the ℓ 2 distance at the feature level in the classification network C.</p><p>We repeat the training of the CVAE-GAN model with the same settings but using different combination of losses in L G (img), L G (D), and L G (C), and compared the quality of the reconstructed samples. As shown in <ref type="figure" target="#fig_10">Fig. 8</ref>, we find that removing the adversarial loss L G (D) will cause the model to generate blurry images. Removing the pixel level reconstruction loss L G (img) causes images to lose details. Lastly, if we remove the feature level loss L G (C) in the classification network C, the generated samples will lose category info. Despite this, our model produces best results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">CVAE-GAN for Data Augmentation</head><p>We further show that the images synthesized from our model can be used for data augmentation for training better face recognition model. We use the FaceScrub dataset as training data, and test using the LFW <ref type="bibr" target="#b15">[16]</ref> dataset.</p><p>We experiment with two data augmentation strategies: 1) generating more images for existing identities in the training datasets; 2) generating new identities by mixing different identities. We test these two kinds of data augmentation methods. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Training Data Accuracy no data augmentation 80K 91.87% existing identities augmentation 80K + 100K 92.77% 5k new identities augmentation 80K + 500K 92.98% <ref type="table">Table 2</ref>. Results of face data augmentation.</p><p>per person, totaling 100k images. For 2), we create 5k new identities by randomly mixing the label of three different existing identities, and generate 100 images for each new identity. For both strategies, the generated images are combined with the Facescrub dataset to train a face recognition model.</p><p>In the testing stage, we directly use the cosine similarity of features to measure the similarity between two faces. In <ref type="table">Table 2</ref>, we compare face verification accuracy on the LFW dataset with and without additional synthesized faces. With the data augmentation of new identities, we achieve about 1.0% improvement in accuracy compared with no augmentation. This demonstrates that our generative network has a certain extrapolation ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a new CVAE-GAN model for fine-grained category image generation. The superior performance on three different datasets demonstrates the ability to generate various kinds of objects. The proposed method can support a wide variety of applications, including image generation, attribute morphing, image inpainting, and data augmentation for training better face recognition models. Our future work will explore how to generate samples of an unknown category, such as face images of a person that do not exist in the training dataset.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Synthesized images using our CVAE-GAN model at high resolution (128×128) for different classes. The generated samples are realistic and diverse within a class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results on a toy example for different generative models. The blue dots are the real points, the red dots are the generated points. a) The real data distribution which is like a "ring". b) The generated points by traditional GAN, WGAN and mean feature matching GAN at different iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of randomly generated samples from different methods on FaceScrub [21], 102 Category Flower datasets [24] and CUB-200 [43] datasets. a) 9 random real images from one category. b) Results from CVAE, which is blurry and cannot preserve the category identity, c) Results from traditional CGAN, it loses diversity and structure info. d) Results from our mean feature matching CGAN, showing diverse results, but also lose of structure info. e) Results from our CVAE-GAN, which shows realistic, diversity and category-keeping results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>class ci feature center f c i C (xr) for xr and f c i C (xp) for xp using moving average method; 13:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of attributes morphing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>10 Figure 7 .</head><label>107</label><figDesc>Figure 7. Result of image inpainting using our proposed model CVAE-GAN-1 ∼ 10 shows the results of iteration 1 ∼ 10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visualization comparison between different generator G, each trained with different combination of losses.</figDesc></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Adversarial Training. In review for ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generating images with perceptual similarity metrics based on deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="658" to="666" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Learning to generate chairs, tables and cars with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tatarchenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Conditional generative adversarial nets for convolutional face generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gauthier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Class Project for Stanford C-S231N: Convolutional Neural Networks for Visual Recognition</title>
		<imprint>
			<publisher>Winter semester</publisher>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Reducing the dimensionality of data with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="issue">5786</biblScope>
			<biblScope unit="page" from="504" to="507" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Independent component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Karhunen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Oja</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>John Wiley &amp; Sons</publisher>
			<biblScope unit="volume">46</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">The neural autoregressive distribution estimator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B L</forename><surname>Larsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.09300</idno>
		<title level="m">Autoencoding beyond pixels using a learned similarity metric</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Labeled faces in the wild: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Face Detection and Facial Image Analysis</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="189" to="248" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<title level="m">Adversarial autoencoders</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Generating more realistic images using gated mrf&apos;s</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcgan</surname></persName>
		</author>
		<idno>arX- iv:1702.08398</idno>
		<title level="m">Mean and covariance feature matching gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">A data-driven approach to cleaning large face datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Winkler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2014 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="343" to="347" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3387" to="3395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Plug &amp; play generative networks: Conditional iterative generation of images in latent space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00005</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno>arX- iv:1610.09585</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Espeholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>arX- iv:1606.05328</idno>
		<title level="m">Conditional image generation with pixelcnn decoders</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Gaussian mixture models of texture and colour for image database retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Permuter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Francos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">H</forename><surname>Jermyn</surname></persName>
		</author>
		<idno>III-569. IEEE</idno>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech, and Signal Processing</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
	<note>Proceedings.(ICASSP&apos;03)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Loss-sensitive generative adversarial networks on lipschitz densities</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06264</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.05396</idno>
		<title level="m">Generative adversarial text to image synthesis</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1401.4082</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning structured output representation using deep conditional generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3483" to="3491" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Real-time american sign language recognition from video using hidden markov models. In Motion-Based Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Starner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pentland</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="227" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Mixtures of conditional gaussian scale mixtures applied to multiscale image representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hosseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">39857</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning generative models via discriminative approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Face recognition using eigenfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Pentland</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1991" />
			<biblScope unit="page" from="586" to="591" />
		</imprint>
	</monogr>
	<note>IEEE Computer Society Conference on</note>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>arX- iv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="835" to="851" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Caltech-UCSD Birds 200</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mita</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<idno>CNS-TR-2010-001</idno>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.03264</idno>
		<title level="m">A theory of generative convnet</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">On convergence properties of the em algorithm for gaussian mixtures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="129" to="151" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.00570</idno>
		<title level="m">Conditional image generation from visual attributes</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Learning face representation from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.7923</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
