<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Spatially-Adaptive Filter Units for Deep Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Domen</forename><surname>Tabernik</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matej</forename><surname>Kristan</surname></persName>
							<email>matej.kristan@fri.uni-lj.sia.leonardis@cs.bham.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aleš</forename><surname>Leonardis</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Faculty of Computer and Information Science</orgName>
								<orgName type="institution">University of Ljubljana</orgName>
								<address>
									<settlement>Ljubljana</settlement>
									<country key="SI">Slovenia</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department" key="dep1">CN-CR Centre</orgName>
								<orgName type="department" key="dep2">School of Computer Science</orgName>
								<orgName type="institution">University of Birmingham</orgName>
								<address>
									<settlement>Birmingham</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Spatially-Adaptive Filter Units for Deep Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (ConvNet) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14]</ref> have become prevalent in visual feature learning. The integral part of these approaches are convolutional filters. In combination with other layers, the definition of the filter directly influences the kind of features a network can capture. Current state-of-the-art ConvNets define filters as rectangular windows of weights where each learnable unit is a single pixel value in the filter.</p><p>An important hyperparameter of the filters is their size, which is directly related to the number of free parameters in ConvNets. Large filters are avoided in the interest of keeping this number low and reducing overfitting. On the other hand, feature expressiveness improves with increased receptive fields <ref type="bibr" target="#b1">[2]</ref>. Classification networks thus apply small filters and implicitly increase the receptive field size by gradually reducing resolution via pooling layers and increased depth <ref type="bibr" target="#b22">[23]</ref>. But in dense prediction problems like  segmentation <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3]</ref>, sufficient resolution is required for accurate localization of segmentation boundaries. Thus large receptive fields have to explicitly be accounted for without resolution loss <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b26">27]</ref>. Increasing a receptive field without sacrificing resolution is addressed by dilated (atrous) convolution <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b26">27]</ref>. This approach increases the kernel receptive field by spreading out (dilating) the positions of the kernel sampling units (i.e., pixels). Large dilations significantly violate Nyquist theorem <ref type="bibr" target="#b0">[1]</ref>, resulting in griding artifacts <ref type="bibr" target="#b26">[27]</ref>. Mitigation of these requires additional convolutional layers with progressively smaller dilations. The dilation factors are another hyperparameter that is manually tuned. To alleviate manual specification to some extent, <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> propose to use several pre-selected dilation factors and achieve excellent results.</p><p>We define a convolutional filter as a mixture of several displaced aggregation units which is a generalization of the convolutional layers typically used for classification and those for segmentation. In contrast to a standard filter, the aggregation unit is not a single pixel, but a locally averaged response. In our implementation, a Gaussian with a fixed variance is used for averaging. In contrast to standard ConvNets, our units are not positioned on a regular grid. Their displacements are adapted during learning, thus the receptive field size of each filter is tuned separately. This allows for large or small receptive fields without changing the number of parameters, facilitating automatic and efficient allocation of parameters.</p><p>Our major contribution is an efficient formulation of the displaced aggregation units (DAU) filter with sub-pixel displacements, which allows practical use in deep architectures (see <ref type="figure" target="#fig_1">Fig. 1</ref>). The DAUs remove the requirement for hand-crafted dilation without modification of other layers, decouple the parameter count from the receptive field size and do not suffer from gridding effects. Backpropagation is derived for all parameters and the new layers are implemented in standard ConvNet package with low-level CUDA procedures <ref type="bibr" target="#b12">[13]</ref>. Our secondary contributions are analyses that have not been possible with the existing networks. We demonstrate that a single type of DAU-based filters achieve comparable performance to standard ConvNets on classification as well as dilated ConvNets on a segmentation task. We perform analysis of the dilation patterns required for accurate segmentation by recording the distributions of the learned displacements in DAUs. Our parameter study demonstrates that using only few DAUs per filter already results in excellent performance. Our tests also show that DAUs allow comparable performance to classical ConvNets at almost 3-fold reduction of the learned parameters in convolutional layers.</p><p>The remainder of this paper is structured as follows: in Sec. 2 we review most closely related works, we describe DAU in Sec. 3 and evaluate our model in Sec. 4. In Sec. 5 we present a comprehensive study of DAU filter displacements and conclude with a discussion in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Receptive field has been considered as an important factor for deep networks in several related works <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b3">4]</ref>. Luo et al. <ref type="bibr" target="#b19">[20]</ref> measured an effective receptive field in convolutional neural networks and observed it increases as the network learns. They suggest an architectural change that foregos a rectangular windows of weights for a sparsely connected units. However, they do not show how this can be implemented. Our proposed approach is in direct alignment with their suggested changes as our displaced aggregation units are a direct realization of their suggested sparsely connected units.</p><p>The importance of deforming filter units has also been indicated by recent work of Dai et al. <ref type="bibr" target="#b4">[5]</ref> and Jeon et al. <ref type="bibr" target="#b11">[12]</ref>. Dai et al. <ref type="bibr" target="#b4">[5]</ref> implemented spatial deformation of features with deformable convolutional networks. They explicitly learn feature displacement but learn them on a per-pixel location basis for input activation map and share them between all channels and features. Our model instead learns different displacements for different channels and features, and shares them over all pixel locations in the input activation map. This makes our model complementary to deformable convolutions. Jeon et al. <ref type="bibr" target="#b11">[12]</ref>, on the other hand, apply deformation on filter units similarly as we do. They use bilinear interpolation similar to ours to get displacements at a sub-pixel accuracy but they apply them to 3 × 3 filters. However, they do not learn different offsets for each channel but apply the same offset across all channels and features. This prevents them from decreeing their parameter count as they still use 9 units per filter. We show that significantly less units are needed.</p><p>Works by Luan et al. <ref type="bibr" target="#b18">[19]</ref> and Jacobsen et al. <ref type="bibr" target="#b10">[11]</ref> changed filter definition using different parametrization techniques. Both decompose filter units into a linear combination of edge filters. They show a reduction in parameters per filter but their models do not provide displacements of filter units to arbitrary values. Their models have a fixed receptive fields defined as a hyperparameter and cannot be learned as ours. This also prevents any further analysis on distribution of displacements and receptive field sizes which is possible with our model.</p><p>Our model also uses concepts for filter parametrization similar to Tabernik et al. <ref type="bibr" target="#b23">[24]</ref> but differs significantly in their design. The model by Tabernik et al. <ref type="bibr" target="#b23">[24]</ref> is limited to only small scale networks and implements only a shallow network with two convolutional layers due to inefficient parametrization design. Our proposed model enjoys an efficient parametrization and we apply it to larger problems using deeper networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Displaced aggregation units (DAU)</head><p>The activation map of the i-th feature (input into the current layer of neurons), is defined in the standard ConvNets as</p><formula xml:id="formula_0">Y i = f ( s W s * X s + b s ),<label>(1)</label></formula><p>where b s is a bias, * is a convolution operation between the input map X s and the filter W s , and f (·) is a non-linear function, such as ReLU or sigmoid <ref type="bibr" target="#b16">[17]</ref>. We define the filters W s as mixtures of localized aggregated feature responses from the input feature map. We choose Gaussians as an analytic form of aggregation units and compactly write filter as W s = k w k G(µ k ; σ), where the unit displacement and aggregation range are specified by the mean µ k and variance σ 2 , respectively, and w k is the input amplification factor. With the exception of variance σ 2 , the parameters µ k and w k are unique for each output feature i and channel s, however, we omit this in notation for clarity. Note that mixtures of Gaussians have recently been explored as potential filters in <ref type="bibr" target="#b23">[24]</ref>. But due to the computational complexity of adapting all parameters, the approach was not feasible beyond a two-layer architecture.</p><p>In our preliminary study we noticed that while the unit locations play a crucial role in the shallow network performance, the variances do not. We thus make all variances in the Gaussians equal and fixed to a selected value, making the unit aggregation perimeter a single hyperparameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Inference with DAU</head><p>The DAUs can efficiently be implemented in ConvNets by using the translational invariance property of the Gaussian convolution. The displacement of a Gaussian relative to the filter manifests in a shifted convolution result, i.e.,</p><formula xml:id="formula_1">f * G(µ k ; σ) = f * T µ k [G(σ)] (2) = T µ k [f * G(σ)],<label>(3)</label></formula><p>where</p><formula xml:id="formula_2">T x (g, y) = g(y − x) is translation of function g(·)</formula><p>and G(σ) is zero-mean Gaussian. Thus the activation map computation can be written as:</p><formula xml:id="formula_3">Y i = f s k w k T µ k (G(σ) * X s ) + b s .<label>(4)</label></formula><p>This formulation affords an efficient implementation by pre-computing convolutions of all inputs by a single Gaussian kernel, i.e.,X s = G(σ) * X s , and applying displacements by µ k to compute the aggregated responses of each output neuron. Note that due to discretization, Eq. (4) is accurate only for discrete displacements µ k . We address this by redefining the translation function in Eq. (4) as a bilinear interpolation</p><formula xml:id="formula_4">T x (g, y) = i j a i,j · g(y − ⌊x⌋ + [i, j]),<label>(5)</label></formula><p>where a i,j are bilinear interpolation weights. This now allows us to perform sub-pixel displacements and can be efficiently implemented in CUDA kernels.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning DAU filter</head><p>The DAU contains two learnable parameters: the input amplification w k and the spatial displacement µ k . In principle, the shared aggregation perimeter σ could be learned as well, but we found that fixing this value was sufficient in our experiments. Thus the hyperparameters in DAU filters are the aggregation perimeter and the number of DAUs per filter.</p><p>Since DAUs are analytic functions, the filter parameters are fully differentiable and conform with the standard ConvNet gradient-descent learning techniques with backpropagation. The required partial derivatives are</p><formula xml:id="formula_5">∂l ∂w k = n,m ∂l ∂z · x T µ k (X s * G(σ)),<label>(6)</label></formula><formula xml:id="formula_6">∂l ∂µ k = n,m ∂l ∂z · x w k · T µ k (X s * ∂G(σ) ∂µ ),<label>(7)</label></formula><p>where ∂l ∂z is back-propagated error. Similarly to inference in Sec. 3.1, the gradient can efficiently be computed using convolution with zero-mean Gaussian (or derivatives) and sampling the response at displacement specified by the mean values in the DAUs. This significantly reduces the computational cost compared to the explicit mixture model filters <ref type="bibr" target="#b23">[24]</ref>.</p><p>The backpropagated error for the lower layer is computed similarly to the classic ConvNets, which convolve the backpropagated error on the layer output with rotated filters. Since the DAUs are rotation symmetric themselves, only the displacements have to be rotated about the origin and Eq. (4) can be applied for computing the back-propagated error as well, yielding efficient and fast computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Performance evaluation</head><p>This section reports results of the experimental evaluation of DAUs. We first analyze the influence of the hyperparameters on DAU filters and then evaluate our approach by replacing the standard filters in ConvNets with DAUs filters for a classification task (Sec. 4.2) and a segmentation task (Sec.4.3). Sec. 5 reports analysis of the learned filter receptive fields and how the number of DAUs per filter impacts the ConvNet performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Hyperparameter analysis in DAU-ConvNets</head><p>We analyze the influence of two hyperparameters on our network: (a) variance σ 2 used in aggregation and (b) the number of DAUs per filter. We analyzed both on classification problem using CIFAR10 <ref type="bibr" target="#b14">[15]</ref> dataset.</p><p>For the purpose of this evaluation we used a shallow network with only three convolutional layers with DAU filters and three max-pooling layers. To classify the whole image we appended fully-connected layer. Batch normalization <ref type="bibr" target="#b8">[9]</ref> was applied to convolutional layers and weights were initialized using <ref type="bibr" target="#b6">[7]</ref>. We trained the network with softmax loss function for 100 epochs using a batch size of 256 images. Learning rate was set to 0.01 for the first 75 epochs and reduced to 0.001 for remaining epochs. We used momentum of 0.9 as well.</p><p>Variance: When evaluating variance we fixed the number of DAUs per filter to four and varied the variance σ  at only around 1%. We used variance of σ 2 = 0.5 2 for all remaining experiments in this paper.</p><p>Number of DAUs per filter: When evaluating the number of units we used a variance σ 2 = 0.5 2 and varied the number of units on the second and the third layer using 1, 2, 4 or 6 units. We fixed DAUs on the first layer to four units to capture initial edges and corners. Results are reported in Tab. 2. They indicate only a slight increase of performance when additional units are added. Difference between using a single unit or using six units is only 1%. We used two and four units in remaining experiments as a trade-off between performance and parameter count. Additional extensive evaluation of parameter count was perform in Sec. 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Classification performance</head><p>Performance of DAUs on the classification task was tested on the ILSVRC 2012 dataset <ref type="bibr" target="#b21">[22]</ref>. A standard testing protocol was used. The network was trained on 1.2 million images and tested on the validation set with 50,000 images. All images were cropped and resized to 227 pixels. To keep the experiments as clean as possible, we did not apply any advanced augmentation techniques apart from mirroring during the training with probability 0.5.</p><p>As our baseline ConvNet architecture, we chose the AlexNet model <ref type="bibr" target="#b15">[16]</ref>, which is composed of 7 layers: 5 convolutional and 2 fully connected. We retained the local normalization layers, max-pooling and dropout on fullyconnected layers of the original AlexNet <ref type="bibr" target="#b15">[16]</ref>, but we did not split channels into two streams as was done in the original work <ref type="bibr" target="#b15">[16]</ref>. We also used weight initialization technique by Glorot and Bengio <ref type="bibr" target="#b6">[7]</ref>.</p><p>The baseline ConvNet was modified into a DAUConvNet as follows. The filters in the convolutional layers from layer 2 to 5 were replaced by our DAU filters from Sec. 3. Four DAUs per filter were used in the second layer and two DAUs per per filter in the remaining three layers. This follows approximate coverage of filter sizes from classic ConvNet with 5 × 5 filter sizes for the second layer and 3 × 3 filter sizes for the remaining layers. First layer and fully connected layers remained unchanged using classic convolutional layer. This is partially due to technical limitation of our current implementation. Our recent work on alleviating this issues indicates that even fully connected layers with 36 units (6 × 6 filter sizes) can be replaced with only 6 DAUs (with comperable perfomance).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Optimization</head><p>We trained both, ConvNet as well as DAU-ConvNet, with stochastic gradient descent using batch size of 128. Both models were trained for 800,000 iterations, or 80 epochs, with initial learning rate of 0.01, which is reduced by a factor of 10 every 200,000th iteration. We used momentum with a factor of 0.9 and a weight decay factor of 0.0005. In our layers with DAUs a decay factor could be applied to weights and offsets as well, although applying to offsets has a different effect than decay on regular weights as it would prevent them from moving further from the center. We used decay only on weights but not on the offsets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Classification results</head><p>The results are reported in Tab. 3 with performance monitoring during the training reported in <ref type="figure" target="#fig_3">Fig. 2</ref>. After 600,000 iterations, the DAU-ConvNet and the baseline ConvNet converge to a comparable performance. Namely, after 80 epochs, both models achieved accuracy of slightly below 57% (see <ref type="table">Tab.</ref> 3), however, DAU-ConvNet was converging much faster, resulting in higher performance jumps before the learning rate reduction steps. Tab. 3 shows the number of free parameters in the convolutional layers. Note that DAU-ConvNet requires 30% less parameters than the baseline classic ConvNet and our analysis in Sec. 5.2 shows this can be improved even further.</p><p>Even though the final classification performance converges to the same result, our model exhibits good performance even on higher learning rates. This indicates that the DAUs modify landscape of the loss function so that it can be traversed faster with higher learning rates in DAUConvNets. This improvement may also be contributed to reduction of the number of parameters in the DAUs and supports the hypothesis that DAUs do not lose expressive power on the account of their simple functional form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semantic segmentation</head><p>We analyze the performance of DAUs on a dense prediction problem where large receptive fields and fine resolution are particularly important. In this experiment, we start from the baseline ConvNet and DAU-ConvNet trained in Sec. 4.2 and fine-tune them for a segmentation task. A standard technique is used to modify the classification networks into segmentation nets. Specifically, the last fully-connected classification layer is replaced by the expansion and classification layer from Long et al. <ref type="bibr" target="#b17">[18]</ref> that entails a 1 × 1 classification layer and up-sampling using a deconvolution layer to obtain pixel-wise loss. To keep the experiments clean we have not added advanced network adaptations that have emerged over recent years, like feature combination across layers, etc., although our approach is general enough to allow such upgrades. The object boundaries are maintained sharp by further increasing higher layer resolution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Increasing resolution at higher layers</head><p>Our classifier from Sec. 4.2 follows the AlexNet architecture and reduces the resolution by 32-fold. For the purpose of segmentation we increase the resolution at higher layers and remove the last two max-pooling layers thus reducing resolution only by 8-fold for segmentation. With this modification, the network retains finer details.</p><p>Increasing the resolution on a pre-trained model causes a misalignment of already learned filter weights and their positions w.r.t. the expected resolution. We compensate for that by modifying the parameters of the affected layers. In particular, for the layers with DAU filters we increase displacement of a unit with the appropriate factor, while in classic ConvNet layer we use dilated convolution with the same factor. The layers after the first-removedmax-pooling use a factor of two (layers 3-5) and the layers after the second-removed-max-pooling use a factor of four (layer 6).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Dataset</head><p>We evaluate our segmentation DAU-ConvNet and the baseline ConvNet with dilation on PASCAL VOC 2011 segmentation dataset. For the training we use 1,112 training images from PASCAL VOC 2011 segmentation combined with 7,386 images collected by Hariharan et al. <ref type="bibr" target="#b7">[8]</ref>. We report results on PASCAL VOC 2011 validation set excluding the images from <ref type="bibr" target="#b7">[8]</ref> that were also used for training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Optimization</head><p>We trained the models with mini-batch stochastic gradient descent and a batch size of 20 images for 65,000 iterations, or 150 epoch. We used a fixed learning rate of 0.0002, weight decay of 0.0005 and momentum of 0.9. The added classification layer was initialized with zeros, similar to <ref type="bibr" target="#b17">[18]</ref> and we used a normalized per-pixel softmax loss function applied only to pixels with a valid annotation.   <ref type="bibr" target="#b15">[16]</ref> 56.99 3.7 mio</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.4">Segmentation results</head><p>The performance of DAU-ConvNet compared to the baseline ConvNet with dilation is shown in <ref type="figure" target="#fig_4">Fig. 3</ref>. The DAUConvNet shows faster convergence in testing loss. In addition, DAU-ConvNet shows consistently better segmentation performance than the baseline ConvNet-dilation across all measures. The mean IU and per-pixel accuracy are improved by approximately 2%. Looking at the per-class mean IU in Tab. 4, we observe improved performance across all categories, with the exception of "dog", "sheep" and "train".</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Analysis of displaced aggregation units</head><p>In this section we conducted two experiments to gain further insights into DAUs. The first experiment analyzed the spatial distribution of the DAUs in the learned filters (Sec. 5.1). The second experiment explored the relation between the number of DAUs per filter and the network performance (Sec. 5.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Spatial adaptation of filter units</head><p>We investigate spatial distribution of DAUs in our network by observing distributions of the learned displacements in the segmentation DAU-ConvNet in Sec. 4.3. The aim of the experiment was to expose two aspects: (i) the dis-  tribution of the learned displacements, which indicates displacement locations favored for a given task, and (ii) overall spatial distribution, which indicates the preferred receptive field size. Such an experiment is very difficult to perform with classical ConvNets and requires a combinatorial sweep over alternative architectures with various manually-defined filter designs. For example, dilated convolutions can alter unit positions, but this must be done with a specific pre-defined dilation factor. In contrast, with displaced aggregated units in our filters we can analyze their displacements that adjust during the learning on the segmentation problem with a sub-pixel accuracy and not being confined to the same pattern across all filters. Such an analysis is not possible with the existing ConvNet architectures.</p><p>We investigate two types of distributions: (i) a 1D distance-to-center distribution and (ii) a distribution of displacements in 2D space. We obtain 1D spatial distribution by collecting displacement values of units from all features at a specific layer and compute their distances to the center of the filter. All distances are collected in a histogram with each unit contributing with its corresponding input amplification factor. We obtain the second 2D spatial distribution by plotting all displacements from a specific layer into the same graph.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Results and discussion</head><p>We compute several per-layer distributions from the DAUConvNet model trained for semantic segmentation in Sec. 4.3. All 1D distributions are shown in <ref type="figure">Fig. 4</ref> and all 2D distributions are shown in <ref type="figure">Fig. 5</ref>. The first set of distributions is computed from all absolute amplification values |w k |. The second distribution is obtained by retaining 90% of the largest absolute amplification values and the third distribution by retaining 75% of the largest absolute amplification values.</p><p>We observe in <ref type="figure">Fig. 4</ref> two significant spikes, one at 2.5 pixels and another at 4 pixels away from the center. The spike at 2.5 pixels, that occurs only at the third layer, is artificial due to the fixed initialization points. It indicates that many units did not move from their initialization point during learning. This can be observed in <ref type="figure">Fig. 5</ref> with high density at initialization center points (red dots). Further inspection shows that those units do not contribute to the filter significantly. In fact, they disappear in the plots when units with lowest amplification value are removed.</p><p>The 4th and the 5th layers have similar initialization points but no apparent spikes in their distance-to-center distributions, as shown in <ref type="figure">(Fig. 4b and Fig. 4c</ref>). This indicates a low learning rate for the 3th layer where displacements may not have been able to move quickly enough. As results in Sec. 5.2 suggest, some of these may be removed without performance reduction.</p><p>The second spike at 4 pixels away from the center <ref type="figure">(Fig. 4)</ref> is more significant since it does not disappear when removing units with small amplification factors. This spike occurs due to an artificial limit on boundary of the receptive field which in our case is set at four pixels in both spatial dimensions <ref type="bibr" target="#b0">1</ref> . Still, a significant number of those units have <ref type="bibr" target="#b0">1</ref> Our current Caffe/CUDA implementation allows distances only up to large amplification factors. This points to the need of further increasing the allowed sizes of the receptive fields. A consistent shape of distance-to-center distributions throughout the layers <ref type="figure">(Fig. 4)</ref> points to a desired spatial distribution of units for segmentation. It indicates that units must densely cover locations at a distance of 1-2 pixels away from the center Some units with high amplification factor are located far away from the center which indicates a need to covering larger receptive fields albeit with lower density. The same conclusion is drawn from 2D spatial dis-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Parameter-space analysis</head><p>We analyzed the impact of the number of DAUs per filter on the network performance to gain additional insights. Several research papers investigate the influence of parameter space in classic ConvNets with respect to the number of layers, number of features or filter sizes <ref type="bibr" target="#b5">[6]</ref>, but could not report analysis with respect to the filter units. The classic ConvNets are limited by a minimal filter size of 3 × 3 that already has a minimal spatial coverage. Reducing the parameter count by reducing the filter size would not be feasible. Our redefinition of filter units on the other hand allows us to investigate filters with even smaller number of parameters without affecting spatial coverage and the receptive field sizes.</p><p>The number of units per filter was set through a hyperparameter. Thus the number of parameters was kept equal across all filters during training. Then the units with small amplification weights were removed</p><p>We perform the experiments on a classification problem with ILSVRC 2012 and AlexNet architecture as presented in Sec. 4.2. We used the same optimization settings for all variants.</p><p>Three variations of our network are compared (see Tab. 6): Large, Medium and Small. The Medium DAUConvNet is the network from Sec. 4.2. The Small DAUConvNet uses as few as two or a single DAU per filter, while the Large DAU-ConvNet uses six to four DAUs. This affects the number of learned parameters as follows. The Small DAU-ConvNet contains 400,000 DAUs, the Medium DAU-ConvNet contains 800,000 DAUs, and the Large DAU-ConvNet contains 1.5 mio DAUs. These values translate to 4.5 mio, 2.3 mio, and 1.2 mio parameters on convolutional layers for Small, Medium and Large DAUConvNet, respectively. For the reference, the baseline ConvNet from Sec. 4.2 contained 3.7 mio units on conv. layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Results and discussion</head><p>The results are reported in Tab. 5. We observe that all three networks achieve classification accuracy of approximately 56-57% on ILSVRC 2012. These results indicate that DAUConvNets may require only one to two units per filter resulting in 3 to 6 parameters per filter on convolutional layers. This is significantly lower than classic networks that already contain 9 parameters for the smallest filter (i.e., 3 × 3) and 25 for a moderately large (i.e., 5 × 5). The low parametrization is possible in DAU-ConvNets since the network learns on its own the receptive field perimeter without the need to increase the parameter space to cover large displacements. Furthermore, looking at the performance when eliminating units with small amplification factor reveals further improvements. In all three networks we were able to eliminate 7-13% of units without affecting their classification performance at all.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion and conclusion</head><p>We proposed a displaced aggregation filter units (DAUs) to replace a fixed, grid-based unit in existing convolutional networks. The DAUs modify only the convolutional layer in standard ConvNets, but afford several advancements. The receptive field is now learned. The learning is efficient since DAUs decouple the number of parameters from the receptive field size and efficiently allocate the free parameters.</p><p>We demonstrated this on the classification and segmentation tasks, and showed faster convergence on the classification task and improved performance on the segmentation task.</p><p>The DAUs remove the filter size hyperparameter, but introduce a hyperparameter on the DAU's aggregation perimeter size and the number of DAUs per filter. We experimentally showed that both have minor affect on the classification performance. We can set aggregation perimeter size to a fixed value, while a larger number of units per filter marginally increases performance. With less than 1% drop in performance we can use only one unit per filter. This is a highly interesting result as it suggests that efficient ConvNets can be implemented by replacing general convolution layers by Gaussian filters and a single sub-pixel sampling per filter.</p><p>The analysis of learned DAU displacements showed that units are concentrated at the filter center, while some are positioned further away. This shows the capacity to learn small as well as large receptive fields within a unified framework. Our distributions directly point to locations that need to be densely sampled in filters. This can be used to adjust the dilation factors from atrous convolutions in classical ConvNets <ref type="bibr" target="#b3">[4]</ref> more efficiently.</p><p>Lastly, our comprehensive study of per-filter parameter allocation showed an inefficient allocation of parameters in existing ConvNets. DAU-ConvNets achieved comparable performance to classic CovnNets at 3-times less parameters per filter. Analysis shows there is also room for further improvements as elimination of units with lowest amplification factors (even without post-hoc fine-tunning) can save 10% of parameters without sacrificing the performance. Furthermore, our recent preliminary work on applying DAUs to fully connected layers indicates possible savings in parameters for fully connected layers as well.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: The displaced aggregation units (DAUs) afford efficient implementation. Convolution of a feature channel with a filter composed of several DAUs is implemented as blurring by a single Gaussian and subsampling at learned displacements.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>to 0.8 2 . Results are reported in Tab. 1. They in- dicate that the variances have negligible effect on classifica- tion performance with changes between different variances</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Classification top-1 accuracy on ILSVRC 2012 validation set using AlexNet architecture. Our DAUConvNet converges faster with larger learning rates than standard ConvNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Performance monitoring during fine-tuning on segmentation task. Results are reported on PASCL VOC 2011 segmentation validation set. We report testing loss value and averaged mean-iu and accuracy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Distance-to-center distributions collected from displacement of DAUs. Distributions reported per-layer (columns) and after elimination of units with smallest amplification factor using different relative thresholds (colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Variance σ 2 hyperparameter evaluation on CI- FAR10 classification task using a shallow DAU-ConvNet. Variance has minor effect on classification performance.</figDesc><table>Variance σ 

2 

0.3 

2 

0.4 

2 

0.5 

2 

0.6 

2 

0.7 

2 

0.8 

2 

DAU-ConvNet 82.9 83.4 83.8 83.6 82.9 82.8 
CIFAR10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Number of units per filter hyperparameter evalua- tion on CIFAR10 classification task using a shallow DAU- ConvNet. Larger number of units increase classification performance only slightly.</figDesc><table>Number of units per filter 
1 
2 
4 
6 

DAU-ConvNet 
82.9 83.3 83.8 84.1 
CIFAR10 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Results on ILSVRC 2012 validation set using 
AlexNet architecture and corresponding number of param-
eters on convolutional layers. We report top-1 accuracy. 

Top-1 
Number of parameters 
accuracy (%) 
on conv. layers 
DAU-ConvNet 
56.89 
2.3 mio 
ConvNet </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Results on segmentation task using a PASCAL VOC 2011 validation set. We report per-class mean-IU and averaged mean-IU over all classes.</figDesc><table>background 
aeroplane 
bicycle 
bird 
boat 
bottle 
bus 
car 
cat 
chair 
cow 
diningtable 

dog 
horse 
motorbike 
person 
potted plant 
sheep 
sofa 
train 
tv/monitor 

mean IU 

DAU-ConvNet 
86.1 58.5 29.7 55.0 41.7 47.2 61.3 56.3 57.9 14.1 47.1 27.3 47.8 36.7 54.7 63.9 28.9 53.0 19.3 59.8 45.3 
47.22 
ConvNet-dilation 85.8 54.6 27.2 51.8 39.0 45.2 56.3 54.2 57.4 12.4 43.8 26.1 50.6 35.6 54.1 61.1 26.9 53.6 18.9 60.2 42.5 
45.57 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Analysis of the number of parameters and units per filter with three variants of DAU-ConvNet: Large, Medium and Small. Rows also show the elimination of units based on their amplification value. In columns we report classification top-1 accuracy on ILSVRC2012 validation set, the number of DAU on all filters and percentage of removed units.</figDesc><table>Relative 
Large DAU-ConvNet 
Medium DAU-ConvNet 
Small DAU-ConvNet 
threshold Acc. (%) 
# units 
% removed Acc. (%) 
# units 
% removed Acc. (%) 
# units 
% removed 

0 
57.3 
1,523,712 
0 
56.9 
786,432 
0 
56.4 
393,216 
0 
0.01 
57.3 
1,389,131 
8 
56.8 
739,884 
6 
56.4 
378,692 
4 
0.02 
57.1 
1,325,057 
13 
56.7 
707,745 
10 
56.4 
366,144 
7 
0.05 
40.1 
1,157,129 
24 
54.8 
623,923 
20 
55.4 
331,137 
16 
0.10 
28.3 
925,509 
39 
47.4 
507,651 
35 
49.6 
279,162 
29 
0.25 
0.2 
453,987 
70 
1.9 
261,093 
66 
0.9 
154,624 
61 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Per-filter unit and parameter count with three vari- ants of DAU-ConvNet: Large, Medium and Small. Note, a unit in DAU has three parameters and ConvNet has one.</figDesc><table>Per-filter unit count 
Large Medium Small ConvNet 

Layer 2 
6 
4 
2 
5 × 5 
Layers 3-5 
4 
2 
1 
3 × 3 
Per-filter parameter count 

Layer 2 
18 
12 
6 
25 
Layers 3-5 
12 
6 
3 
9 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">or 8 pixels. This can be overcome with a improved implementation. tributions in Fig. 5.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments. This work was supported in part by the following research projects and programs: project GOSTOP C3330-16-529000 and ViAMaRo L2-6765, program P2-0214 financed by Slovenian Research Agency ARRS, and MURI project financed by MoD/Dstl and EPSRC through EP/N019415/1 grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mastering the Discrete Fourier Transform in One, Two or Several Dimensions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Amidror</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Return of the Devil in the Details: Delving Deep into Convolutional Nets. In arXiv preprint arXiv</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Rethinking Atrous Convolution for Semantic Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deformable Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Understanding Deep Architectures using a Recursive Convolutional Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rolfe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Semantic Contours from Inverse Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">C</forename><surname>Berkeley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Systems</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ave</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial Pyramid Pooling in Deep Convolutional Networks for Visual Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="346" to="361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="171" to="180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Jacobsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
		<title level="m">Smeulders. Structured Receptive Fields in CNNs. 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2610" to="2619" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Active Convolution: Learning the Shape of Convolution for Image Classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jeon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Caffe: Convolutional Architecture for Fast Feature Embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Mask R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Learning Multiple Layers of Features from Tiny Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="60" />
		</imprint>
		<respStmt>
			<orgName>Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">TechReport</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Fully Convolutional Networks for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">8828</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<title level="m">Gabor Convolutional Networks. British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<title level="m">Understanding the Effective Receptive Field in Deep Convolutional Neural Networks. Nips, (Nips)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Very Deep Convolutional Networks for Large-Scale Image Recoginition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="14" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Towards Deep Compositional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tabernik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Wyatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Leonardis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated Residual Transformations for Deep Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Multi-Scale Context Aggregation by Dilated Convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Dilated Residual Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
