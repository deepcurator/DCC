<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">In-Place Activated BatchNorm for Memory-Optimized Training of DNNs</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuel</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Porzi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Kontschieder</surname></persName>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mapillary</forename><surname>Research</surname></persName>
							<email>research@mapillary.com</email>
						</author>
						<title level="a" type="main">In-Place Activated BatchNorm for Memory-Optimized Training of DNNs</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>In this work we present In-Place Activated Batch Normalization (INPLACE-ABN) -a novel approach to drasti</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>High-performance computer vision recognition models typically take advantage of deep network backbones, generating rich feature representations for target applications to operate on. For example, top-ranked architectures used in the 2017 LSUN or MS COCO segmentation/detection challenges are predominantly based on ResNet/ResNeXt <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b29">30]</ref> models comprising &gt;100 layers.</p><p>Obviously, depth/width of networks strongly correlate with GPU memory requirements and at given hardware memory limitations, trade-offs have to be made to balance feature extractor performance vs. application-specific parameters like network output resolution or training data size. A particularly memory-demanding task is semantic segmentation, where one has to compromise significantly on  <ref type="figure">Figure 1</ref>. Example of residual block with identity mapping <ref type="bibr" target="#b9">[10]</ref>. Left: Implementation with standard BN and in-place activation layers, which requires storing 6 buffers for the backward pass. Right: Implementation with our proposed INPLACE-ABN layer, which requires storing only 3 buffers. Our solution avoids storing the buffers that are typically kept for the backward pass through BN and exhibits a lower computational overhead compared to state-of-the-art memory-reduction methods.</p><p>the number of training crops per minibatch and their spatial resolution. In fact, many recent works based on modern backbone networks have to set the training batch size to no more than a single crop per GPU <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b27">28]</ref>, which is partially also due to suboptimal memory management in some deep learning frameworks. In this work, we focus on increasing the memory efficiency of the training process of modern network architectures in order to further leverage performance of deep neural networks in tasks like image classification and semantic segmentation. We introduce a novel and unified layer that replaces the commonly used succession of batch normalization (BN) and nonlinear activation layers (ACT), which are integral with modern deep learning architectures like ResNet <ref type="bibr" target="#b8">[9]</ref>, ResNeXt <ref type="bibr" target="#b29">[30]</ref>, Inception-ResNet <ref type="bibr" target="#b25">[26]</ref>, WideResNet <ref type="bibr" target="#b31">[32]</ref>, Squeeze-and-Excitation Networks <ref type="bibr" target="#b10">[11]</ref>, DenseNet <ref type="bibr" target="#b11">[12]</ref>, etc. Our solution is coined INPLACE-ABN and proposes to merge batch normalization and activation layers in order to enable in-place computation, using only a single memory buffer for storing the results (see illustration in <ref type="figure">Figure 1</ref>). During the backward pass, we can efficiently recover all required quantities from this buffer by inverting the forward pass computations. Our approach yields a theoretical memory reduction of up to 50%, and our experiments on semantic segmentation show additional data throughput of up to +75% during training, when compared to prevailing sequential execution of BN+ACT. Our memory gains are obtained without introducing noticeable computational overhead, i.e. side-by-side runtime comparisons show only between +0.8-2% increase in computation time.</p><p>As additional contribution, we review the checkpointing memory management strategy <ref type="bibr" target="#b3">[4]</ref> and propose a computationally optimized application of this idea in the context of BN layers. This optimization allows us to drop recomputation of certain quantities needed during the backward pass, eventually leading to reduced computation times as per our INPLACE-ABN. However, independent of the proposed optimized application of <ref type="bibr" target="#b3">[4]</ref>, conventional checkpointing in general suffers from higher implementation complexity (with the necessity to invasively manipulate the computation graph), while our main INPLACE-ABN contribution can be easily implemented as self-contained, standard plug-in layer and therefore simply integrated in any modern deep learning framework.</p><p>Our experimental evaluations demonstrate on-par performance with state-of-the-art models trained for image classification on ImageNet <ref type="bibr" target="#b24">[25]</ref> (in directly comparable memory settings), and significantly improved results for the memory-critical application of semantic segmentation.</p><p>To summarize, we provide the following contributions:</p><p>• Introduction of a novel, self-contained INPLACE-ABN layer that enables joint, in-place computation of BN+ACT, approximately halvening the memory requirements during training of modern deep learning models.</p><p>• A computationally more efficient application of the checkpointing memory management strategy in the context of BN layers, inspired by optimizations used for INPLACE-ABN.</p><p>• Experimental evaluations for i) image classification on ImageNet-1k showing approximately on-par performance with state-of-the-art models and ii) semantic segmentation on COCO-Stuff, Cityscapes and Mapillary Vistas, considerably benefiting from the additional available memory and generating new high-scores on the challenging Cityscapes and Mapillary Vistas datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The topic of optimizing memory management in deep learning frameworks is typically addressed at different levels. Efficient deep learning frameworks like TensorFlow, MxNet or PyTorch follow distinct memory allocation strategies. Among them is checkpointing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>, which provides additional memory at the cost of runtime via storing activation buffers as so-called checkpoints, from where required quantities can be re-computed during the backward pass. The paper in <ref type="bibr" target="#b3">[4]</ref> describes how to recursively apply such a variant on sub-graphs between checkpoints. In <ref type="bibr" target="#b7">[8]</ref> this is further optimized with dynamic programming, where a storage policy is determined that minimizes the computational costs for re-computation at a fixed memory budget.</p><p>Virtually all deep learning frameworks based on NVIDIA hardware exploit low-level functionality libraries CUDA and cuDNN <ref type="bibr" target="#b0">1</ref> , providing GPU-accelerated and performance-optimized primitives and basic functionalities. Another line of research has focused on training CNNs with reduced precision and therefore smaller memoryfootprint datatypes. Such works include (partially) binarized weights/activations/gradients <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14]</ref>, which however typically lead to degraded overall performance. With mixed precision training <ref type="bibr" target="#b19">[20]</ref>, this issue seems to be overcome and we plan to exploit this as complementary technique in future work, freeing up even more memory for training deep networks without sacrificing runtime.</p><p>In <ref type="bibr" target="#b6">[7]</ref> the authors modify ResNet in a way to contain reversible residual blocks, i.e. residual blocks whose activations can be reconstructed backwards. Backpropagation through reversible blocks can be performed without having stored intermediate activations during the forward pass, which allows to save memory. However, the cost to pay is twofold. First, one has to recompute each residual function during the backward pass, thus having the same overhead as checkpointing <ref type="bibr" target="#b18">[19]</ref>. Second, the network design is limited to using blocks with certain restrictions, i.e. reversible blocks cannot be generated for bottlenecks where information is supposed to be discarded.</p><p>Finally, we stress that only training time memoryefficiency is targeted here while test-time optimization as done e.g. in NVIDIAs TensorRT 2 is beyond our scope.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">In-Place Activated Batch Normalization</head><p>Here, we describe our contribution to avoid the storage of a buffer that is typically needed for the gradient computation during the backward pass through the batch normalization layer. As opposed to existing approaches we also show that our solution minimizes the computational overhead we have to trade for saving additional memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Batch Normalization Review</head><p>Batch Normalization has been introduced in <ref type="bibr" target="#b14">[15]</ref> as an effective tool to reduce internal covariate shift in deep networks and accelerate the training process. Ever since, BN plays a key role in most modern deep learning architectures.</p><p>The key idea consists in having a normalization layer that applies an axis-aligned whitening of the input distribution, followed by a scale-and-shift operation aiming at preserving the network's representation capacity. The whitening operation exploits statistics computed on a minibatch level only. The by-product of this approximation is an additional regularizing effect for the training process.</p><p>In details, we can fix a particular unit x in the network and let x B = {x 1 , . . . , x m } be the set of values x takes from a minibatch B of m training examples. The batch normalization operation applied to x i first performs a whitening of the activation using statistics computed from the minibatch:</p><formula xml:id="formula_0">x i = BN(x i ) = x i − µ B σ 2 B + ǫ .<label>(1)</label></formula><p>Here ǫ &gt; 0 is a small constant that is introduced to prevent numerical issues, and µ B and σ 2 B are the empirical mean and variance of the activation unit x, respectively, computed with respect to the minibatch B, i.e.</p><formula xml:id="formula_1">µ B = 1 m m j=1 x j , σ 2 B = 1 m m j=1 (x j − µ B ) 2 .</formula><p>The whitened activationsx i are then scaled and shifted by learnable parameters γ and β, obtaining</p><formula xml:id="formula_2">y i = BN γ,β (x i ) = γx i + β .</formula><p>The BN transformation described above can in principle be applied to any activation in the network and is typically adopted with channel-specific (γ, β)-parameters. Using BN renders training resilient to the scale of parameters, thus enabling the use of higher learning rates. At test time, the BN statistics are fixed to µ T and σ T , estimated from the entire training set T . These statistics are typically updated at training time with a running mean over the respective minibatch statistics, but could also be recomputed before starting the testing phase. Also, the computation of networks trained with batch normalization can be sped up by absorbing BN parameters into the preceding CONV layer, by performing a simple update of the convolution weights and biases. This is possible because at testtime BN becomes a linear operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Memory Optimization Strategies</head><p>Here we sketch our proposed memory optimization strategies after introducing both, the standard (memoryinefficient) use of batch normalization and the state-of-theart coined checkpointing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. In <ref type="figure" target="#fig_1">Figure 2</ref>, we provide diagrams showing the forward and backward passes of a typical building block BN+ACT+CONV 3 as usually implemented in modern deep architectures. The activation function (e.g. RELU) is denoted by φ. Computations occurring during the forward pass are shown in green and involve the entire minibatch B (we omit the subscript B). Computations happening during the backward pass are shown in cyan and gray. The gray part aims at better highlighting the additional computation that has been introduced to compensate for the memory savings. Rectangles are in general volatile buffers holding intermediate results, except for rectangles surrounded by a dashed frame, which represent buffers that need to be stored for the backward pass and thus significantly impact the training memory footprint. E.g., in <ref type="figure" target="#fig_1">Figure 2</ref>(a) x and z will be stored for the backward pass, while in <ref type="figure" target="#fig_1">Figure 2</ref>(b) only x is stored. For the sake of presentation clarity, we have omitted two additional buffers holding µ B and σ B for the BN backward phase. Nevertheless, these buffers represent in general a small fraction of the total allocated memory. Moreover, we have also omitted the gradients with respect to the model parameters (i.e. γ, β and CONV weights).</p><p>Standard. In <ref type="figure" target="#fig_1">Figure 2</ref>(a) we present the standard implementation of the reference building block, as used in all deep learning frameworks. During the forward pass both, the input x to BN and the output of the activation function φ need to be stored for the backward pass. Variable x is used during the backward pass through BN γ,β to compute both the gradient w.r.t. its input and γ, i.e. Checkpointing <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19]</ref>. This technique allows to trade computation for memory when training neural networks, applicable in a very broad setting. In <ref type="figure" target="#fig_1">Figure 2</ref>(b), we limit its application to the building block under consideration like in <ref type="bibr" target="#b21">[22]</ref>. In contrast to the standard implementation, which occupies two buffers for the backward pass of the shown building block, checkpointing requires only a single buffer. The trick consists in storing only x and recomputing z during the backward pass by repeating the forward operations starting from x (see gray-colored operations). Clearly, the computational overhead to be paid comprises both, recomputation of the BN and activation layers. It is worth observing that recomputing BN γ,β (gray) during the backward phase can reuse values for µ B and σ B available from the forward pass and fuse together the normalization and subsequent affine transformation into a single scale-and-shift operation. Accordingly, the cost of the second forward pass over BN γ,β becomes less expensive (see also <ref type="bibr" target="#b21">[22]</ref> Checkpointing (proposed version). Direct application of the checkpointing technique in the sketched building block, which is adopted also in <ref type="bibr" target="#b21">[22]</ref>, is not computationally optimal since additional operations could be saved by storinĝ x, i.e. the normalized value of x as per Eq. (1), instead of x. Indeed, as we will see in the next subsection, the backward pass through BN requires recomputingx if not already stored. For this reason, we propose in <ref type="figure" target="#fig_1">Figure 2</ref>(c) an alternative implementation that is computationally more efficient by retainingx from the forward pass through the BN layer. Fromx we can recover z during the backward pass by applying the scale-and-shift operation π γ,β (x) = γx + β, followed by the activation function φ (see gray-colored operations). In this way, the computation of z becomes slightly more efficient than the one shown in <ref type="figure" target="#fig_1">Figure 2</ref>(b), for we save the fusion operation. Finally, an additional saving of the normalization step derives from using the storedx in the backward implementation of BN rather than recomputing it from x. To distinguish the efficient backward implementation of BN from the standard one we write BN * γ,β in place of BN γ,β (cyan-colored, see additionally § 3.3).</p><p>In-Place Activated Batch Normalization I. A limitation of the memory-reduction strategy described above is that the last layer, namely CONV in the example, depends on non-local quantities like x (orx) for the computation of the gradient. This makes the implementation of the approach within standard frameworks somewhat cumbersome, because the backward pass of any layer that follows φ, which relies on the existence of z, has to somehow trigger its recomputation. To render the implementation of the proposed memory savings easier and self-contained, we suggest an alternative strategy shown in <ref type="figure" target="#fig_1">Figure 2(d)</ref>, which relies on having only z as the saved buffer during the forward pass, thus operating an in-place computation through the BN layer (therefrom the paper's title). By doing so, any layer that follows the activation φ would have the information for the gradient computation locally available. Having stored z, we need to recomputex backwards, for it will be needed in the backward pass through the BN layer. <ref type="bibr" target="#b3">4</ref> However, this operation is only possible if the activation func-tion is invertible. Even though this requirement does not hold for RELU, i.e. one of the most dominantly used activation functions, we show in § 4.1 that an invertible function like LEAKY RELU <ref type="bibr" target="#b17">[18]</ref> with a small slope works well as a surrogate of RELU without compromising on the model quality. We also need to invert the scale-and-shift operation π γ,β , which is in general possible if γ = 0.</p><p>In-Place Activated Batch Normalization II. The complexity of the computation ofx = π <ref type="bibr" target="#b23">[24]</ref>) show that the same computation can be absorbed into the gradient </p><note type="other">−1 γ,β (y) = y−β γ used in the backward pass of INPLACE-ABN I can be further reduced by rewriting the gradients ∂L ∂γ and ∂L ∂x directly as functions of y instead ofx. The explicit inversion of π γ,β to recoverx applies m scale-and-shift operations (per feature channel). If the partial derivatives are however based on y directly, the resulting modified gradients (derivations given in</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Technical Details</head><p>The key components of our method are the computation of the inverse of both the activation function (INPLACE-ABN I &amp; II) and π γ,β (INPLACE-ABN I), and the implementation of a backward pass through the batch normalization layer that depends on y, i.e. the output of the forward pass through the same layer.</p><p>Invertible activation function. Many activation functions are actually invertible and can be computed in-place (e.g. sigmoid, hyperbolic tangent, LEAKY RELU, and others), but the probably most commonly used one, namely RELU, is not invertible. However, we can replace it with LEAKY RELU and a small slope without impacting the quality of the trained models <ref type="bibr" target="#b30">[31]</ref>. LEAKY RELU and its inverse share the same computational cost, i.e. an elementwise sign check and scaling operation. Hence, the overhead deriving from the recomputation of φ in the backward pass of the previously shown, checkpointing-based approaches and its inverse φ −1 employed in the backward pass of our method are equivalent. To give further evidence of the interchangeability of RELU and LEAKY RELU with slope a = 0.01, we have successfully retrained well-known models like ResNeXt and WideResNet on ImageNet using LEAKY RELU (see § 4.1 and <ref type="bibr" target="#b23">[24]</ref>). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPLACE-ABN I: Backward pass through BN. The gradient</head><formula xml:id="formula_3">∂L ∂γ = m i=1 ∂L ∂y ix i , ∂L ∂β = m i=1 ∂L ∂y i .</formula><p>The expression above differs from what is found in the original BN paper <ref type="bibr" target="#b14">[15]</ref>, but the refactoring was already used in the Caffe <ref type="bibr" target="#b15">[16]</ref> framework. It is implemented by BN * γ,β in the proposed solutions in <ref type="figure" target="#fig_1">Figures 2(c) and 2(d)</ref> and does not depend on µ B . Hence, we store during the forward pass only σ B (this dependency was omitted from the diagrams). Instead, BN γ,β in <ref type="figure" target="#fig_1">Figures 2(a) and 2(b)</ref>, which depends on x, requires the additional recomputation ofx from x via Eq. (1). Hence, it also requires storing µ B . Our solution is hence memory-wise more efficient than the state-of-the-art from <ref type="figure" target="#fig_1">Figure 2</ref></p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(b).</head><p>Inversion of π γ,β . In the configuration of INPLACE-ABN I, the inversion of π γ,β becomes critical if γ = 0 since π −1 γ,β (y) = y−β γ . While we never encountered such a case in practice, one can protect against it by preventing γ from getting less than a given tolerance. We can even avoid this problem by simply not considering γ a learnable parameter and by fixing it to 1, in case the activation function is scale covariant (e.g. all RELU-like activations) and when a CONV layer follows. Indeed, it is easy to show that the network retains the exact same capacity in that case, for γ can be absorbed into the subsequent CONV layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>INPLACE-ABN II:</head><p>Backward pass through BN. We obtain additional memory savings for our solution illustrated in <ref type="figure" target="#fig_1">Figure 2</ref>(e) and as outlined in § 3.2. The gradient ∂L ∂x when written as a function of y instead ofx becomes</p><formula xml:id="formula_4">∂L ∂x i = ∂L ∂y i − 1 γm ∂L ∂γ y i − 1 m ∂L ∂β + β γ ∂L ∂γ γ σ 2 B + ǫ .</formula><p>For the gradients of the BN parameters, ∂L ∂β remains as above but we get</p><formula xml:id="formula_5">∂L ∂γ = 1 γ   m j=1 ∂L ∂y j y j − β ∂L ∂β  </formula><p>and we write BN † γ,β for the actual backward implementation in <ref type="figure" target="#fig_1">Figure 2</ref>(e). Detailed derivations are provided in <ref type="bibr" target="#b23">[24]</ref>.</p><p>In summary, both of our optimized main contributions are memory-wise more efficient than the state-of-the-art solution in <ref type="figure" target="#fig_1">Figure 2</ref>(b) and INPLACE-ABN II is computationally even more efficient than the proposed, optimized checkpointing from <ref type="figure" target="#fig_1">Figure 2</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation Details</head><p>We have implemented the proposed INPLACE-ABN I layer in PyTorch, by simply creating a new layer that fuses In this way we can deal with the computation ofx from z internally in the layer, thus keeping the implementation selfcontained. We have released code at https://github. com/mapillary/inplace_abn for easy plug-in replacement of the block BN+ACT in modern architectures. The forward and backward implementations are also given as pseudocode in Algorithm 1 and 2. In the forward pass, in line 3, we explicitly indicate the buffers that are stored and needed for the backward pass. Any other buffer can be overwritten with in-place computations, e.g. x, y and z can point to the same memory location. In the backward pass, we recover the stored buffers in line 1 and, again, every computation can be done in-place if the buffer is not needed anymore (e.g. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We assess the effectiveness of our proposed, memory efficient INPLACE-ABN layer for the tasks of image classification and semantic segmentation in § 4.1 and 4.2, respectively. Additionally, we provide timing analyses in § 4.3. Experiments were run and timed on machines comprising four NVIDIA Titan Xp cards (with 12GB of RAM each).</p><p>Where not otherwise noted, the activation function used in all experiments is LEAKY RELU with slope a = 0.01.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Image Classification</head><p>We have trained several residual-unit-based models on ImageNet-1k <ref type="bibr" target="#b24">[25]</ref> to demonstrate the effectiveness of INPLACE-ABN for the task of image classification. In particular, we focus our attention on two aspects: i) whether using an invertible activation function (i.e. LEAKY RELU in our experiments) impacts on the classification performance of the models, and ii) how the memory savings obtained with our method can be exploited to improve classification accuracy. We have trained ResNeXt-101/ResNeXt-152 <ref type="bibr" target="#b29">[30]</ref> models (using cardinality 64, SGD with Nesterov updates, initial learning rate 0.1, weight decay 10 −4 and momentum 0.9, 90 epochs in total and reducing the learning rate every 30 epochs by a factor of 10). We additionally trained DenseNet-264 <ref type="bibr" target="#b11">[12]</ref> and WideResNet-38 <ref type="bibr" target="#b28">[29]</ref> with the same hyperparameters, except for the latter using a linear learning rate decay from 0.1 to 10 −6 . For all models, we proportionally scale input images so that their smallest side equals 256 pixels, before randomly taking 224 × 224 crops. RGB images are per-channel mean and variance normalized and color augmentation is applied as described in <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discussion of results.</head><p>As a baseline, we train ResNeXt-101 with standard Batch Normalization and the maximum batch size that fits in GPU memory, i.e. 256 images per batch. Then we consider two different scenarios: i) using the extra memory to fit more images per training batch while fixing the network architecture, or ii) fixing the batch size while training a larger network. For option i) we double the batch size to 512 (ResNeXt-101, INPLACE-ABN, 512 in <ref type="table">Table 1</ref>), while for option ii) we train ResNeXt-152 and WideResNet-38. Note that neither ResNeXt-152 nor WideResNet-38 would fit in memory when using 256 images per training batch and when using standard BN. As it is clear from the table, both i) and ii) result in a noticeable performance increase. Interestingly, training ResNeXt-101 with an increased batch size results in similar accuracy to the deeper (and computationally more expensive) ResNeXt-152 model. As an additional reference, we train ResNeXt-101 with synchronized Batch Normalization (INPLACE-ABN sync ), which can be seen as a "virtual" increase of batch size applied to the computation of BN statistics. In this case we only observe small accuracy improvements when compared to the baseline model. Finally, we also report results for DenseNet-264 <ref type="bibr" target="#b11">[12]</ref>, which was trained with a batch size of 256 that otherwise also would not fit in GPU memory. All models can be downloaded from our github page.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Semantic Segmentation</head><p>The goal of semantic segmentation is to assign categorical labels to each pixel in an image. State-of-the-art segmentations are typically obtained by combining classification models pretrained on ImageNet (typically referred to as body) with segmentation-specific head architectures and jointly fine-tuning them on suitable, (densely) annotated training data like Cityscapes <ref type="bibr" target="#b4">[5]</ref>, COCO-Stuff <ref type="bibr" target="#b0">[1]</ref>, ADE20K <ref type="bibr" target="#b33">[34]</ref> or Mapillary Vistas <ref type="bibr" target="#b20">[21]</ref>. 2 ), respectively. The global 1 × 1 features are computed in a channel-specific way and CONVed into 256 additional channels. Each output block is followed by BatchNorm before all 1280 features are stacked and reduced by another CONV+BN+ACT block (into 256 features) and finally CONVed to the number of target classes. We exploit our proposed INPLACE-ABN strategy also in the head architecture. Finally, we apply bilinear upsampling to the logits to obtain the original input crop resolution before computing the loss using an online bootstrapping strategy as described in <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b27">28]</ref> (setting p = 1.0 and m = 25%). We did not apply hybrid dilated convolutions <ref type="bibr" target="#b26">[27]</ref> nor added an auxiliary loss as proposed in <ref type="bibr" target="#b32">[33]</ref>. Training data is sampled in a uniform way unless otherwise stated (by shuffling the database in each epoch) and all Cityscapes experiments are run for 360 epochs using an initial learning rate of 2.5×10 −3 and polynomial learning rate decay (1 − iter max_iter ) 0.9 , following <ref type="bibr" target="#b2">[3]</ref>. COCO-Stuff experiments were trained only for 30 epochs, which however approximately matches the number of iterations on Cityscapes due to the considerably larger dataset size. For optimization, we use stochastic gradient descent with momentum 0.9 and weight decay 10 −4 . For training data augmentation, we apply random horizontal flipping (with prob. 0.5) and random scaling selected from 0.7 -2.0 before cropping the actual patches.</p><p>Discussion of Results. In <ref type="table">Table 2</ref> (top), we provide results (all scores are averaged Jaccard numbers) on validation data for Cityscapes and COCO-Stuff under different BN layer configurations. We distinguish between standard BN layers <ref type="bibr" target="#b14">[15]</ref> (coined STD-BN) and our proposed variants using in-place, activated BN (INPLACE-ABN) as well as its gradient-synchronized version INPLACE-ABN sync . All experiments are based on LEAKY RELU activations. Trainings were conducted in a way to maximize GPU memory utilization by i) fixing the training crop size and therefore pushing the amount of crops per minibatch to the limit (denoted as <ref type="bibr">FIXED CROP)</ref> or ii) fixing the number of crops per minibatch and maximizing the training crop resolutions (FIXED BATCH). Experiments are conducted for ResNeXt-101 and WideResNet-38 network bodies, where the latter seems preferable for segmentation tasks.  Both body networks were solely trained on ImageNet-1k. All results at the top of <ref type="table">Table 2</ref> derive from single-scale testing without horizontal image flipping. In general, results improve when applying more training data (in terms of both, #training crops per minibatch and input crop resolutions). The increase of data (w.r.t. pixels/minibatch) we can put in GPU memory, relative to the baseline (first row) is reported in square brackets. We observe that higher input resolution is in general even more beneficial than adding more crops to the batch. Results for tuned WideResNet-38 INPLACE-ABN sync models using even larger input crops are shown on the bottom of <ref type="table">Table 2</ref> for both, validation and test data. The combination of using INPLACE-ABN sync with fewer, but larger crops and an alternative minibatch compilation strategy coined CLASS-UNIFORM SAMPLING yields new high scores for both, Cityscapes (model is Vistas pre-trained, used also coarsely labeled data for training and multi-scale [1.0, 1.25, 1.5, 1.75, 2.0] + horizontally flipped testing) and Mapillary Vistas (initial learning rate of 3.5 × 10 −3 and trained for 90 epochs, single scale inference) test datasets. For CLASS-UNIFORM SAMPLING, we compiled the minibatches per epoch in a way to show all classes uniformly instead of randomly perturbing the dataset (thus following an oversampling strategy for underrepresented categories).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Timing analyses</head><p>Besides the discussed memory improvements and their impact on computer vision applications, we also provide actual runtime comparisons and analyses for the INPLACE-ABN I setting shown in 2(d), as this is the implementation we released on github. Isolating a single BN+ACT+CONV block, we evaluate the computational times required for a forward and backward pass over it (Figure right to <ref type="table" target="#tab_0">Table 1)</ref>. We compare the conventional approach of serially executing layers and storing intermediate results (STANDARD), our proposed INPLACE-ABN I and the CHECKPOINT-ING approach. In order to obtain fair timing comparisons, we re-implemented the checkpointing idea in PyTorch. The results are obtained by running all operations over a batch comprising 32-images and setting the meta-parameters (number of feature channels, spatial dimensions) to those encountered in the four modules of ResNeXt-101, denoted as CONV1-CONV4. The actual runtimes were averaged over 200 iterations.</p><p>We observe consistent speed advantages in favor of our method when comparing against CHECKPOINTING, with the actual percentage difference depending on block's metaparameters. As we can see, INPLACE-ABN induces computation time increase between 0.8 − 2% over STANDARD while CHECKPOINTING is almost doubling our overheads.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this work we have presented INPLACE-ABN, which is a novel, computationally efficient fusion of batch normalization and activation layers, targeting memoryoptimization for modern deep neural networks during training time. We reconstruct necessary quantities for the backward pass by inverting the forward computation from the storage buffer, and manage to free up almost 50% of the memory needed in conventional BN+ACT implementations at little additional computational costs. In contrast to stateof-the-art checkpointing attempts, our method is reconstructing discarded buffers backwards during the backward pass, thus allowing us to encapsulate BN+ACT as selfcontained layer, which is easy to implement and deploy in virtually all modern deep learning frameworks. We have validated our approach with experiments for image classification on ImageNet-1k and semantic segmentation on Cityscapes, COCO-Stuff and Mapillary Vistas. Our obtained networks have performed consistently and considerably better when trained with larger batch sizes (or training crop sizes), leading to new high-scores on the challenging Cityscapes and Mapillary Vistas datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Comparison of standard BN, state-of-the-art checkpointing from [4, 19] and our proposed methods. See § 3.2 for a detailed description.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>L denotes the loss, while z is required for the backward pass through the activation φ as well as potential subsequent op- erations like e.g. the convolution shown in the figure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>∂L ∂xi at O(1) cost (per feature channel). In Figure 2(e) we show the diagram of this optimization, where we denote as BN † γ,β the implementation of the back- ward pass as a function of y.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>}, which is obtained from the backward pass through the BN layer, can be written as a function ofx = {x 1 , . . . ,x m } and ∂L ∂y = { ∂L ∂y1 , . . . ,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>with an (invertible) activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>can share the same memory lo- cation as well asx, y and z).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>BATCHNORM</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>) .</head><label>)</label><figDesc>The three approaches that follow are all contributions of this work. The first represents a variation of checkpoint- ing, which allows us to save additional computations in the context of BN. The second and third are our main contri- butions, providing strategies that yield the same memory savings and even lower computational costs compared to the proposed, optimized checkpointing, but are both self- contained and thus much easier to integrate in existing deep learning frameworks.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Cityscapes val (single model &amp; scale) 12 × 87279.16 Cityscapes val (single model &amp; scale) + CLASS-UNIFORM SAMPLING 12 × 87279.40 Cityscapes test (single Vistas pre-trained model, 5 scales + horizontal flipping, fine + coarse label data) + CLASS-UNIFORM SAMPLING 12 × 872Mapillary Vistas val (single model &amp; scale, no horizontal flipping) + CLASS-UNIFORM SAMPLING 12 × 77653.12 Mapillary Vistas test (single model &amp; scale, no horizontal flipping) + CLASS-UNIFORM SAMPLING 12 × 776Table 2. (Top) Validation results (single scale test) for segmentation experiments on Cityscapes and COCO-Stuff, using ResNeXt-101 and WideResNet-38 network bodies and different batch compilations (see text). (Bottom) Validation and test data results using WideResNet-38+INPLACE-ABN sync models for Cityscapes and Vistas with tuned hyperparameters. All result numbers in [%].</figDesc><table>ResNeXt-101 
WideResNet-38 

Cityscapes 
COCO-Stuff 
Cityscapes 
COCO-Stuff 
STD-BN + LEAKY RELU 
16 × 512 

2 

74.42 16 × 480 

2 

20.30 20 × 512 

2 

75.82 
20 × 496 

2 

22.44 

INPLACE-ABN, FIXED CROP 
28 × 512 

2 [+75%] 

75.80 24 × 480 
2 [+50%] 22.63 28 × 512 

2 [+40%] 

77.75 
28 × 496 
2 [+40%] 22.96 
INPLACE-ABN, FIXED BATCH 
16 × 672 

2 [+72%] 

77.04 16 × 600 
2 [+56%] 23.35 20 × 640 

2 [+56%] 

78.31 
20 × 576 
2 [+35%] 24.10 
INPLACE-ABN sync , FIXED BATCH 
16 × 672 

2 [+72%] 

77.58 16 × 600 
2 [+56%] 24.91 20 × 640 

2 [+56%] 

78.06 
20 × 576 
2 [+35%] 25.11 

2 

2 

2 

82.03 

2 

2 

53.37 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://developer.nvidia.com 2 https://developer.nvidia.com/tensorrt</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Having the convolution at the end of the block is not strictly necessary, but supports comprehension.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">This solution can technically still be considered as a form of checkpointing, but instead of recovering information forwards as in [4, 19], we recover it backwards, thus bearing a similarity to reversible nets [7].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We acknowledge financial support from project DIGIMAP, funded under grant #860375 by the Austrian Research Promotion Agency (FFG).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">COCO-Stuff: Thing and stuff classes in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Caesar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<idno>abs/1612.03716</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno>abs/1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno>abs/1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Training deep nets with sublinear memory cost</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<idno>abs/1604.06174</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The Cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-P</forename><surname>David</surname></persName>
		</author>
		<title level="m">Binaryconnect: Training deep neural networks with binary weights during propagations. In (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The reversible residual network: Backpropagation without storing activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">N</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017-12" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Memory-efficient backpropagation through time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gruslys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Munos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1603.05027</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Squeeze-and-excitation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1709.01507</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks. In (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Quantized neural networks: Training neural networks with low precision weights and activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno>abs/1609.07061</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>abs/1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Caffe: Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<idno>abs/1405.0312</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Workshop on Deep Learning for Audio, Speech and Language Processing</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Training Deep and Recurrent Networks with Hessian-Free Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="479" to="535" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Micikevicius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Narang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Diamos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Elsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ginsburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Houston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Kuchaiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Venkatesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wu</surname></persName>
		</author>
		<idno>abs/1710.03740</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Mixed precision training. CoRR</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The mapillary vistas dataset for semantic understanding of street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ollmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Rota</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Memory-efficient implementation of densenets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno>abs/1707.06990</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Loss maxpooling for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Neuhold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">In-place activated batchnorm for memory-optimized training of DNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bulò</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Porzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kontschieder</surname></persName>
		</author>
		<idno>abs/1712.02616</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karphathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. (IJCV</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Cottrell</surname></persName>
		</author>
		<idno>abs/1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">High-performance semantic segmentation using very deep fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1604.04339</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno>abs/1611.05431</idno>
		<imprint>
			<date type="published" when="2006" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Empirical evaluation of rectified activations in convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<idno>abs/1505.00853</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<title level="m">Wide residual networks. In (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno>abs/1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Semantic understanding of scenes through the ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno>abs/1608.05442</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
