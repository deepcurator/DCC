<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Good Semi-supervised Learning That Requires a Bad GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
							<email>zhiliny@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fan</forename><surname>Yang</surname></persName>
							<email>fanyang1@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
							<email>wcohen@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science</orgName>
								<orgName type="institution">Carnegie Melon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Good Semi-supervised Learning That Requires a Bad GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Semi-supervised learning methods based on generative adversarial networks (GANs) obtained strong empirical results, but it is not clear 1) how the discriminator benefits from joint training with a generator, and 2) why good semi-supervised classification performance and a good generator cannot be obtained at the same time. Theoretically we show that given the discriminator objective, good semisupervised learning indeed requires a bad generator, and propose the definition of a preferred generator. Empirically, we derive a novel formulation based on our analysis that substantially improves over feature matching GANs, obtaining state-of-the-art results on multiple benchmark datasets 2 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Deep neural networks are usually trained on a large amount of labeled data, and it has been a challenge to apply deep models to datasets with limited labels. Semi-supervised learning (SSL) aims to leverage the large amount of unlabeled data to boost the model performance, particularly focusing on the setting where the amount of available labeled data is limited. Traditional graph-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> were extended to deep neural networks <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>, which involves applying convolutional neural networks <ref type="bibr" target="#b9">[10]</ref> and feature learning techniques to graphs so that the underlying manifold structure can be exploited. <ref type="bibr" target="#b14">[15]</ref> employs a Ladder network to minimize the layerwise reconstruction loss in addition to the standard classification loss. Variational auto-encoders have also been used for semi-supervised learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b11">12]</ref> by maximizing the variational lower bound of the unlabeled data log-likelihood.</p><p>Recently, generative adversarial networks (GANs) <ref type="bibr" target="#b5">[6]</ref> were demonstrated to be able to generate visually realistic images. GANs set up an adversarial game between a discriminator and a generator. The goal of the discriminator is to tell whether a sample is drawn from true data or generated by the generator, while the generator is optimized to generate samples that are not distinguishable by the discriminator. Feature matching (FM) GANs <ref type="bibr" target="#b15">[16]</ref> apply GANs to semi-supervised learning on Kclass classification. The objective of the generator is to match the first-order feature statistics between the generator distribution and the true distribution. Instead of binary classification, the discriminator employs a (K + 1)-class objective, where true samples are classified into the first K classes and generated samples are classified into the (K + 1)-th class. This (K + 1)-class discriminator objective leads to strong empirical results, and was later widely used to evaluate the effectiveness of generative models <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>Though empirically feature matching improves semi-supervised classification performance, the following questions still remain open. First, it is not clear why the formulation of the discriminator can improve the performance when combined with a generator. Second, it seems that good semisupervised learning and a good generator cannot be obtained at the same time. For example, <ref type="bibr" target="#b15">[16]</ref> observed that mini-batch discrimination generates better images than feature matching, but feature matching obtains a much better semi-supervised learning performance. The same phenomenon was also observed in <ref type="bibr" target="#b20">[21]</ref>, where the model generated better images but failed to improve the performance on semi-supervised learning.</p><p>In this work, we take a step towards addressing these questions. First, we show that given the current (K + 1)-class discriminator formulation of GAN-based SSL, good semi-supervised learning requires a "bad" generator. Here by bad we mean the generator distribution should not match the true data distribution. Then, we give the definition of a preferred generator, which is to generate complement samples in the feature space. Theoretically, under mild assumptions, we show that a properly optimized discriminator obtains correct decision boundaries in high-density areas in the feature space if the generator is a complement generator.</p><p>Based on our theoretical insights, we analyze why feature matching works on 2-dimensional toy datasets. It turns out that our practical observations align well with our theory. However, we also find that the feature matching objective has several drawbacks. Therefore, we develop a novel formulation of the discriminator and generator objectives to address these drawbacks. In our approach, the generator minimizes the KL divergence between the generator distribution and a target distribution that assigns high densities for data points with low densities in the true distribution, which corresponds to the idea of a complement generator. Furthermore, to enforce our assumptions in the theoretical analysis, we add the conditional entropy term to the discriminator objective.</p><p>Empirically, our approach substantially improves over vanilla feature matching GANs, and obtains new state-of-the-art results on MNIST, SVHN, and CIFAR-10 when all methods are compared under the same discriminator architecture. Our results on MNIST and SVHN also represent state-of-the-art amongst all single-model results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Besides the adversarial feature matching approach <ref type="bibr" target="#b15">[16]</ref>, several previous works have incorporated the idea of adversarial training in semi-supervised learning. Notably, <ref type="bibr" target="#b18">[19]</ref> proposes categorical generative adversarial networks (CatGAN), which substitutes the binary discriminator in standard GAN with a multi-class classifier, and trains both the generator and the discriminator using information theoretical criteria on unlabeled data. From the perspective of regularization, <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b12">13]</ref> propose virtual adversarial training (VAT), which effectively smooths the output distribution of the classifier by seeking virtually adversarial samples. It is worth noting that VAT bears a similar merit to our approach, which is to learn from auxiliary non-realistic samples rather than realistic data samples. Despite the similarity, the principles of VAT and our approach are orthogonal, where VAT aims to enforce a smooth function while we aim to leverage a generator to better detect the low-density boundaries. Different from aforementioned approaches, <ref type="bibr" target="#b23">[24]</ref> proposes to train conditional generators with adversarial training to obtain complete sample pairs, which can be directly used as additional training cases. Recently, Triple GAN <ref type="bibr" target="#b10">[11]</ref> also employs the idea of conditional generator, but uses adversarial cost to match the two model-defined factorizations of the joint distribution with the one defined by paired data.</p><p>Apart from adversarial training, there has been other efforts in semi-supervised learning using deep generative models recently. As an early work, <ref type="bibr" target="#b6">[7]</ref> adapts the original Variational Auto-Encoder (VAE) to a semi-supervised learning setting by treating the classification label as an additional latent variable in the directed generative model. <ref type="bibr" target="#b11">[12]</ref> adds auxiliary variables to the deep VAE structure to make variational distribution more expressive. With the boosted model expressiveness, auxiliary deep generative models (ADGM) improve the semi-supervised learning performance upon the semi-supervised VAE. Different from the explicit usage of deep generative models, the Ladder networks <ref type="bibr" target="#b14">[15]</ref> take advantage of the local (layerwise) denoising auto-encoding criterion, and create a more informative unsupervised signal through lateral connection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Theoretical Analysis</head><p>Given a labeled set L = {(x, y)}, let {1, 2, · · · , K} be the label space for classification. Let D and G denote the discriminator and generator, and P D and p G denote the corresponding distributions.</p><p>Consider the discriminator objective function of GAN-based semi-supervised learning <ref type="bibr" target="#b15">[16]</ref>:</p><formula xml:id="formula_0">max D E x,y∼L log P D (y|x, y ≤ K) + E x∼p log P D (y ≤ K|x) + E x∼p G log P D (K + 1|x), (1)</formula><p>where p is the true data distribution. The probability distribution P D is over K + 1 classes where the first K classes are true classes and the (K + 1)-th class is the fake class. The objective function consists of three terms. The first term is to maximize the log conditional probability for labeled data, which is the standard cost as in supervised learning setting. The second term is to maximize the log probability of the first K classes for unlabeled data. The third term is to maximize the log probability of the (K + 1)-th class for generated data. Note that the above objective function bears a similar merit to the original GAN formulation if we treat P (K + 1|x) to be the probability of fake samples, while the only difference is that we split the probability of true samples into K sub-classes.</p><p>Let f (x) be a nonlinear vector-valued function, and w k be the weight vector for class k. As a standard setting in previous work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b4">5]</ref>, the discriminator D is defined as</p><formula xml:id="formula_1">P D (k|x) = exp(w k f (x)) K+1 k =1 exp(w k f (x)) .</formula><p>Since this is a form of over-parameterization, w K+1 is fixed as a zero vector <ref type="bibr" target="#b15">[16]</ref>. We next discuss the choices of different possible G's.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Perfect Generator</head><p>Here, by perfect generator we mean that the generator distribution p G exactly matches the true data distribution p, i.e., p G = p. We now show that when the generator is perfect, it does not improve the generalization over the supervised learning setting. Proposition 1. If p G = p, and D has infinite capacity, then for any optimal solution D = (w, f ) of the following supervised objective,</p><formula xml:id="formula_2">max D E x,y∼L log P D (y|x, y ≤ K),<label>(2)</label></formula><p>there exists</p><formula xml:id="formula_3">D * = (w * , f * ) such that D * maximizes Eq. (1) and that for all x, P D (y|x, y ≤ K) = P D * (y|x, y ≤ K).</formula><p>The proof is provided in the supplementary material. Proposition 1 states that for any optimal solution D of the supervised objective, there exists an optimal solution D * of the (K + 1)-class objective such that D and D * share the same generalization error. In other words, using the (K + 1)-class objective does not prevent the model from experiencing any arbitrarily high generalization error that it could suffer from under the supervised objective. Moreover, since all the optimal solutions are equivalent w.r.t. the (K + 1)-class objective, it is the optimization algorithm that really decides which specific solution the model will reach, and thus what generalization performance it will achieve. This implies that when the generator is perfect, the (K + 1)-class objective by itself is not able to improve the generalization performance. In fact, in many applications, an almost infinite amount of unlabeled data is available, so learning a perfect generator for purely sampling purposes should not be useful. In this case, our theory suggests that not only the generator does not help, but also unlabeled data is not effectively utilized when the generator is perfect.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Complement Generator</head><p>The function f maps data points in the input space to the feature space. Let p k (f ) be the density of the data points of class k in the feature space. Given a threshold k , let F k be a subset of the data support where</p><formula xml:id="formula_4">p k (f ) &gt; k , i.e., F k = {f : p k (f ) &gt; k }. We assume that given { k } K k=1</formula><p>, the F k 's are disjoint with a margin. More formally, for any f j ∈ F j , f k ∈ F k , and j = k, we assume that there exists a real number 0 &lt; α &lt; 1 such that</p><formula xml:id="formula_5">αf j + (1 − α)f k / ∈ F j ∪ F k .</formula><p>As long as the probability densities of different classes do not share any mode, i.e., ∀i = j, argmax f p i (f ) ∩ argmax f p j (f ) = ∅, this assumption can always be satisfied by tuning the thresholds k 's. With the assumption held, we will show that the model performance would be better if the thresholds could be set to smaller values (ideally zero). We also assume that each F k contains at least one labeled data point. Now we present the assumption on the convergence conditions of the discriminator. Let U and G be the sets of unlabeled data and generated data. Assumption 1. Convergence conditions. When D converges on a finite training set {L, U, G}, D learns a (strongly) correct decision boundary for all training data points. More specifically, (1) for any (x, y) ∈ L, we have w y f (x) &gt; w k f (x) for any other class k = y; (2) for any x ∈ G, we have</p><formula xml:id="formula_6">Suppose ∪ K k=1</formula><formula xml:id="formula_7">0 &gt; max K k=1 w k f (x); (3) for any x ∈ U, we have max K k=1 w k f (x) &gt; 0.</formula><p>In Assumption 1, conditions <ref type="formula">(1)</ref> and <ref type="formula" target="#formula_2">(2)</ref> assume classification correctness on labeled data and true-fake correctness on generated data respectively, which is directly induced by the objective function. Likewise, it is also reasonable to assume true-fake correctness on unlabeled data, i.e., log k exp w k f (x) &gt; 0 for x ∈ U. However, condition (3) goes beyond this and assumes max k w k f (x) &gt; 0. We discuss this issue in detail in the supplementary material and argue that these assumptions are reasonable. Moreover, in Section 5, our approach addresses this issue explicitly by adding a conditional entropy term to the discriminator objective to enforce condition (3). Lemma 1. Suppose for all k, the L2-norms of weights w k are bounded by w k 2 ≤ C. Suppose that there exists &gt; 0 such that for any</p><formula xml:id="formula_8">f G ∈ F G , there exists f G ∈ G such that f G − f G 2 ≤ . With the conditions in Assumption 1, for all k ≤ K, we have w k f G &lt; C .</formula><p>Corollary 1. When unlimited generated data samples are available, with the conditions in Lemma 1,</p><formula xml:id="formula_9">we have lim |G|→∞ w k f G ≤ 0.</formula><p>See the supplementary material for the proof. Proposition 2. Given the conditions in Corollary 1, for all class k ≤ K, for all feature space points</p><formula xml:id="formula_10">f k ∈ F k , we have w k f k &gt; w j f k for any j = k.</formula><p>Proof. Without loss of generality, suppose j = arg max j =k w j f k . Now we prove it by contradiction. Suppose w k f k ≤ w j f k . Since F k 's are disjoint with a margin, B is a convex set and F G = B − ∪ k F k , there exists 0 &lt; α &lt; 1 such that f G = αf k + (1 − α)f j with f G ∈ F G and f j being the feature of a labeled data point in F j . By Corollary 1, it follows that w j f G ≤ 0. Thus,</p><formula xml:id="formula_11">w j f G = αw j f k + (1 − α)w j f j ≤ 0. By Assumption 1, w j f k &gt; 0 and w j f j &gt; 0, leading to contradiction. It follows that w k f k &gt; w j f k for any j = k.</formula><p>Proposition 2 guarantees that when G is a complement generator, under mild assumptions, a nearoptimal D learns correct decision boundaries in each high-density subset F k (defined by k ) of the data support in the feature space. Intuitively, the generator generates complement samples so the logits of the true classes are forced to be low in the complement. As a result, the discriminator obtains class boundaries in low-density areas. This builds a connection between our approach with manifold-based methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref> which also leverage the low-density boundary assumption.</p><p>With our theoretical analysis, we can now answer the questions raised in Section 1. First, the (K + 1)-class formulation is effective because the generated complement samples encourage the discriminator to place the class boundaries in low-density areas (Proposition 2). Second, good semi-supervised learning indeed requires a bad generator because a perfect generator is not able to improve the generalization performance (Proposition 1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Case Study on Synthetic Data</head><p>In the previous section, we have established the fact a complement generator, instead of a perfect generator, is what makes a good semi-supervised learning algorithm. Now, to get a more intuitive understanding, we conduct a case study based on two 2D synthetic datasets, where we can easily verify our theoretical analysis by visualizing the model behaviors. In addition, by analyzing how feature matching (FM) <ref type="bibr" target="#b15">[16]</ref> works in 2D space, we identify some potential problems of it, which motivates our approach to be introduced in the next section. Specifically, two synthetic datasets are four spins and two circles, as shown in <ref type="figure" target="#fig_1">Fig. 1</ref>.  Soundness of complement generator Firstly, to verify that the complement generator is a preferred choice, we construct the complement generator by uniformly sampling from the a bounded 2D box that contains all unlabeled data, and removing those on the manifold. Based on the complement generator, the result on four spins is visualized in <ref type="figure">Fig. 2</ref>. As expected, both the classification and true-fake decision boundaries are almost perfect. More importantly, the classification decision boundary always lies in the fake data area (left panel), which well matches our theoretical analysis.</p><p>Visualization of feature space Next, to verify our analysis about the feature space, we choose the feature dimension to be 2, apply the FM to the simpler dataset of two circles, and visualize the feature space in <ref type="figure">Fig. 3</ref>. As we can see, most of the generated features (blue points) resides in between the features of two classes (green and orange crosses), although there exists some overlap. As a result, the discriminator can almost perfectly distinguish between true and generated samples as indicated by the black decision boundary, satisfying the our required Assumption 1. Meanwhile, the model obtains a perfect classification boundary (blue line) as our analysis suggests.</p><p>Pros and cons of feature matching Finally, to further understand the strength and weakness of FM, we analyze the solution FM reaches on four spins shown in <ref type="figure">Fig. 4</ref>. From the left panel, we can see many of the generated samples actually fall into the data manifold, while the rest scatters around in the nearby surroundings of data manifold. It suggests that by matching the first-order moment by SGD, FM is performing some kind of distribution matching, though in a rather weak manner. Loosely speaking, FM has the effect of generating samples close to the manifold. But due to its weak power in distribution matching, FM will inevitably generate samples outside of the manifold, especially when the data complexity increases. Consequently, the generator density p G is usually lower than the true data density p within the manifold and higher outside. Hence, an optimal discriminator P D * (K + 1 | x) = p(x)/(p(x) + p G (x)) could still distinguish between true and generated samples in many cases. However, there are two types of mistakes the discriminator can still make 1. Higher density mistake inside manifold: Since the FM generator still assigns a significant amount of probability mass inside the support, wherever p G &gt; p &gt; 0, an optimal discriminator will incorrectly predict samples in that region as "fake". Actually, this problem has already shown up when we examine the feature space <ref type="figure">(Fig. 3</ref>).</p><p>2. Collapsing with missing coverage outside manifold: As the feature matching objective for the generator only requires matching the first-order statistics, there exists many trivial solutions the generator can end up with. For example, it can simply collapse to mean of unlabeled features, or a few surrounding modes as along as the feature mean matches. Actually, we do see such collapsing phenomenon in high-dimensional experiments when FM is used (see <ref type="figure" target="#fig_3">Fig. 5a</ref> and <ref type="figure" target="#fig_3">Fig. 5c</ref>) As a result, a collapsed generator will fail to cover some gap areas between manifolds. Since the discriminator is only well-defined on the union of the data supports of p and p G , the prediction result in such missing area is under-determined and fully relies on the smoothness of the parametric model. In this case, significant mistakes can also occur.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Approach</head><p>As discussed in previous sections, feature matching GANs suffer from the following drawbacks: 1) the first-order moment matching objective does not prevent the generator from collapsing (missing coverage); 2) feature matching can generate high-density samples inside manifold; 3) the discriminator objective does not encourage realization of condition <ref type="formula" target="#formula_14">(3)</ref> in Assumption 1 as discussed in Section 3.2.</p><p>Our approach aims to explicitly address the above drawbacks.</p><p>Following prior work <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6]</ref>, we employ a GAN-like implicit generator. We first sample a latent variable z from a uniform distribution U(0, 1) for each dimension, and then apply a deep convolutional network to transform z to a sample x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Generator Entropy</head><p>Fundamentally, the first drawback concerns the entropy of the distribution of generated features,</p><formula xml:id="formula_12">H(p G (f ))</formula><p>. This connection is rather intuitive, as the collapsing issue is a clear sign of low entropy. Therefore, to avoid collapsing and increase coverage, we consider explicitly increasing the entropy.</p><p>Although the idea sounds simple and straightforward, there are two practical challenges. Firstly, as implicit generative models, GANs only provide samples rather than an analytic density form. As a result, we cannot evaluate the entropy exactly, which rules out the possibility of naive optimization. More problematically, the entropy is defined in a high-dimensional feature space, which is changing dynamically throughout the training process. Consequently, it is difficult to estimate and optimize the generator entropy in the feature space in a stable and reliable way. Faced with these difficulties, we consider two practical solutions.</p><p>The first method is inspired by the fact that input space is essentially static, where estimating and optimizing the counterpart quantities would be much more feasible. Hence, we instead increase the generator entropy in the input space, i.e., H(p G (x)), using a technique derived from an information theoretical perspective and relies on variational inference (VI). Specially, let Z be the latent variable space, and X be the input space. We introduce an additional encoder, q : X → Z, to define a variational upper bound of the negative entropy <ref type="bibr" target="#b2">[3]</ref>, −H(p G (x)) ≤ −E x,z∼p G log q(z|x) = L VI . Hence, minimizing the upper bound L VI effectively increases the generator entropy. In our implementation, we formulate q as a diagonal Gaussian with bounded variance, i.e. q(z|x) = N (µ(x), σ 2 (x)), with 0 &lt; σ(x) &lt; θ, where µ(·) and σ(·) are neural networks, and θ is the threshold to prevent arbitrarily large variance.</p><p>Alternatively, the second method aims at increasing the generator entropy in the feature space by optimizing an auxiliary objective. Concretely, we adapt the pull-away term (PT) <ref type="bibr" target="#b24">[25]</ref> as the auxiliary</p><formula xml:id="formula_13">cost, L PT = 1 N (N −1) N i=1 j =i f (xi) f (xj ) f (xi) f (xj ) 2</formula><p>, where N is the size of a mini-batch and x are samples. Intuitively, the pull-away term tries to orthogonalize the features in each mini-batch by minimizing the squared cosine similarity. Hence, it has the effect of increasing the diversity of generated features and thus the generator entropy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Generating Low-Density Samples</head><p>The second drawback of feature matching GANs is that high-density samples can be generated in the feature space, which is not desirable according to our analysis. Similar to the argument in Section 5.1, it is infeasible to directly minimize the density of generated features. Instead, we enforce the generation of samples with low density in the input space. Specifically, given a threshold , we minimize the following term as part of our objective:</p><formula xml:id="formula_14">E x∼p G log p(x)I[p(x) &gt; ]<label>(3)</label></formula><p>where I[·] is an indicator function. Using a threshold , we ensure that only high-density samples are penalized while low-density samples are unaffected. Intuitively, this objective pushes the generated samples to "move" towards low-density regions defined by p(x). To model the probability distribution over images, we simply adapt the state-of-the-art density estimation model for natural images, namely the PixelCNN++ <ref type="bibr" target="#b16">[17]</ref> model. The PixelCNN++ model is used to estimate the density p(x) in Eq. (3). The model is pretrained on the training set, and fixed during semi-supervised training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Generator Objective and Interpretation</head><p>Combining our solutions to the first two drawbacks of feature matching GANs, we have the following objective function of the generator: min</p><formula xml:id="formula_15">G −H(p G ) + E x∼p G log p(x)I[p(x) &gt; ] + E x∼p G f (x) − E x∼U f (x) 2 .<label>(4)</label></formula><p>This objective is closely related to the idea of complement generator discussed in Section 3. To see that, let's first define a target complement distribution in the input space as follows</p><formula xml:id="formula_16">p * (x) = 1 Z 1 p(x) if p(x) &gt; and x ∈ B x C if p(x) ≤ and x ∈ B x ,</formula><p>where Z is a normalizer, C is a constant, and B x is the set defined by mapping B from the feature space to the input space. With the definition, the KL divergence (KLD) between p G (x) and p</p><formula xml:id="formula_17">* (x) is KL(p G p * ) = −H(p G )+E x∼p G log p(x)I[p(x) &gt; ]+E x∼p G I[p(x) &gt; ] log Z−I[p(x) ≤ ] log C .</formula><p>The form of the KLD immediately reveals the aforementioned connection. Firstly, the KLD shares two exactly the same terms with the generator objective (4). Secondly, while p * (x) is only defined in B x , there is not such a hard constraint on p G (x). However, the feature matching term in Eq. (4) can be seen as softly enforcing this constraint by bringing generated samples "close" to the true data (Cf. Section 4). Moreover, because the identity function I[·] has zero gradient almost everywhere, the last term in KLD would not contribute any informative gradient to the generator. In summary, optimizing our proposed objective (4) can be understood as minimizing the KL divergence between the generator distribution and a desired complement distribution, which connects our practical solution to our theoretical analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4">Conditional Entropy</head><p>In order for the complement generator to work, according to condition (3) in Assumption 1, the discriminator needs to have strong true-fake belief on unlabeled data, i.e., max</p><formula xml:id="formula_18">K k=1 w k f (x) &gt; 0.</formula><p>However, the objective function of the discriminator in <ref type="bibr" target="#b15">[16]</ref> does not enforce a dominant class. Instead, it only needs K k=1 P D (k|x) &gt; P D (K + 1|x) to obtain a correct decision boundary, while the probabilities P D (k|x) for k ≤ K can possibly be uniformly distributed. To guarantee the strong true-fake belief in the optimal conditions, we add a conditional entropy term to the discriminator objective and it becomes,</p><formula xml:id="formula_19">max D Ex,y∼L log pD(y|x, y ≤ K) + Ex∼U log pD(y ≤ K|x)+ Ex∼p G log pD(K + 1|x) + Ex∼U K k=1 pD(k|x) log pD(k|x).<label>(5)</label></formula><p>By optimizing Eq. (5), the discriminator is encouraged to satisfy condition (3) in Assumption 1. Note that the same conditional entropy term has been used in other semi-supervised learning methods <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13]</ref> as well, but here we motivate the minimization of conditional entropy based on our theoretical analysis of GAN-based semi-supervised learning.</p><p>To train the networks, we alternatively update the generator and the discriminator to optimize Eq. <ref type="formula" target="#formula_15">(4)</ref> and Eq. (5) based on mini-batches. If an encoder is used to maximize H(p G ), the encoder and the generator are updated at the same time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We mainly consider three widely used benchmark datasets, namely MNIST, SVHN, and CIFAR-10. As in previous work, we randomly sample 100, 1,000, and 4,000 labeled samples for MNIST, SVHN,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>MNIST (# errors) SVHN (% errors) CIFAR-10 (% errors)</head><p>CatGAN <ref type="bibr" target="#b18">[19]</ref> 191 ± 10 -19.58 ± 0.46 SDGM <ref type="bibr" target="#b11">[12]</ref> 132 ± 7 16.61 ± 0.24 -Ladder network <ref type="bibr" target="#b14">[15]</ref> 106 ± 37 -20.40 ± 0.47 ADGM <ref type="bibr" target="#b11">[12]</ref> 96  <ref type="table">Table 1</ref>: Comparison with state-of-the-art methods on three benchmark datasets. Only methods without data augmentation are included. * indicates using the same (small) discriminator architecture, † indicates using a larger discriminator architecture, and ‡ means self-ensembling. and CIFAR-10 respectively during training, and use the standard data split for testing. We use the 10-quantile log probability to define the threshold in Eq. (4). We add instance noise to the input of the discriminator <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b17">18]</ref>, and use spatial dropout <ref type="bibr" target="#b19">[20]</ref> to obtain faster convergence. Except for these two modifications, we use the same neural network architecture as in <ref type="bibr" target="#b15">[16]</ref>. For fair comparison, we also report the performance of our FM implementation with the aforementioned differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Main Results</head><p>We compare the the results of our best model with state-of-the-art methods on the benchmarks in <ref type="table">Table 1</ref>. Our proposed methods consistently improve the performance upon feature matching. We achieve new state-of-the-art results on all the datasets when only small discriminator architecture is considered. Our results are also state-of-the-art on MNIST and SVHN among all single-model results, even when compared with methods using self-ensembling and large discriminator architectures. Finally, note that because our method is actually orthogonal to VAT <ref type="bibr" target="#b12">[13]</ref>, combining VAT with our presented approach should yield further performance improvement in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Ablation Study</head><p>We report the results of ablation study in <ref type="table">Table 2</ref>.</p><p>In the following, we analyze the effects of several components in our model, subject to the intrinsic features of different datasets.</p><p>First, the generator entropy terms (VI and PT) (Section 5.1) improve the performance on SVHN and CIFAR by up to 2.2 points in terms of error rate. Moreover, as shown in <ref type="figure" target="#fig_3">Fig 5,</ref> our model significantly reduces the collapsing effects present in the samples generated by FM, which also indicates that maximizing the generator entropy is beneficial. On MNIST, probably due to its simplicity, no collapsing phenomenon was observed with vanilla FM training <ref type="bibr" target="#b15">[16]</ref> or in our setting. Under such circumstances, maximizing the generator entropy seems to be unnecessary, and the estimation bias introduced by approximation techniques can even hurt the performance. Setting as q-th centile q = 2 q = 10 q = 20 q = 100</p><p>Error on MNIST 77.7 ± 6.1 79.5 ± 9.8 80.1 ± 9.6 85.0 ± 11.7 <ref type="table">Table 2</ref>: Ablation study. FM is feature matching. LD is the low-density enforcement term in Eq. (3). VI and PT are two entropy maximization methods described in Section 5.1. Ent means the conditional entropy term in Eq. (5). Max log-p is the maximum log probability of generated samples, evaluated by a PixelCNN++ model. 10-quant shows the 10-quantile of true image log probability. Error means the number of misclassified examples on MNIST, and error rate (%) on others.</p><p>Second, the low-density (LD) term is useful when FM indeed generates samples in high-density areas.</p><p>MNIST is a typical example in this case. When trained with FM, most of the generated hand written digits are highly realistic and have high log probabilities according to the density model (Cf. max log-p in <ref type="table">Table 2</ref>). Hence, when applied to MNIST, LD improves the performance by a clear margin.</p><p>By contrast, few of the generated SVHN images are realistic (Cf. <ref type="figure" target="#fig_3">Fig. 5a</ref>). Quantitatively, SVHN samples are assigned very low log probabilities (Cf. <ref type="table">Table 2</ref>). As expected, LD has a negligible effect on the performance for SVHN. Moreover, the "max log-p" column in <ref type="table">Table 2</ref> shows that while LD can reduce the maximum log probability of the generated MNIST samples by a large margin, it does not yield noticeable difference on SVHN. This further justifies our analysis. Based on the above conclusion, we conjecture LD would not help on CIFAR where sample quality is even lower. Thus, we did not train a density model on CIFAR due to the limit of computational resources.</p><p>Third, adding the conditional entropy term has mixed effects on different datasets. While the conditional entropy (Ent) is an important factor of achieving the best performance on SVHN, it hurts the performance on MNIST and CIFAR. One possible explanation relates to the classic exploitationexploration tradeoff, where minimizing Ent favors exploitation and minimizing the classification loss favors exploration. During the initial phase of training, the discriminator is relatively uncertain and thus the gradient of the Ent term might dominate. As a result, the discriminator learns to be more confident even on incorrect predictions, and thus gets trapped in local minima.</p><p>Lastly, we vary the values of the hyper-parameter in Eq. <ref type="bibr" target="#b3">(4)</ref>. As shown at the bottom of <ref type="table">Table 2</ref>, reducing clearly leads to better performance, which further justifies our analysis in Sections 4 and 3 that off-manifold samples are favorable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Generated Samples</head><p>We compare the generated samples of FM and our approach in <ref type="figure" target="#fig_3">Fig. 5</ref>. The FM images in <ref type="figure" target="#fig_3">Fig. 5c</ref> are extracted from previous work <ref type="bibr" target="#b15">[16]</ref>. While collapsing is widely observed in FM samples, our model generates diverse "bad" images, which is consistent with our analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this work, we present a semi-supervised learning framework that uses generated data to boost task performance. Under this framework, we characterize the properties of various generators and theoretically prove that a complementary (i.e. bad) generator improves generalization. Empirically our proposed method improves the performance of image classification on several benchmark datasets.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>F k is bounded by a convex set B. If the support F G of a generator G in the feature space is a relative complement set in B, i.e., F G = B − ∪ K k=1 F k , we call G a complement generator. The reason why we utilize a bounded B to define the complement is presented in the supplementary material. Note that the definition of complement generator implies that G is a function of f . By treating G as function of f , theoretically D can optimize the original objective function in Eq. (1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Labeled and unlabeled data are denoted by cross and point respectively, and different colors indicate classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :Figure 3 :Figure 4 :</head><label>234</label><figDesc>Figure 2: Left: Classification decision boundary, where the white line indicates true-fake boundary; Right: True-Fake decision boundary</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparing images generated by FM and our model. FM generates collapsed samples, while our model generates diverse "bad" samples.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work was supported by the DARPA award D17AP00001, the Google focused award, and the Nvidia NVAIL award. The authors would also like to thank Han Zhao for his insightful feedback.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Towards principled methods for training generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS 2016 Workshop on Adversarial Training. In review for ICLR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2016</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Manifold regularization: A geometric framework for learning from labeled and unlabeled examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikhail</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Partha</forename><surname>Niyogi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vikas</forename><surname>Sindhwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="2399" to="2434" />
			<date type="published" when="2006-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Calibrating energy-based generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zihang</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amjad</forename><surname>Almahairi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip</forename><surname>Bachman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Hovy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.01691</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semisupervised learning with deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shakir</forename><surname>Diederik P Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danilo</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jimenez Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3581" to="3589" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thomas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Temporal ensembling for semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02242</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chongxuan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02291</idno>
		<title level="m">Triple generative adversarial nets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lars</forename><surname>Maaløe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Casper</forename><forename type="middle">Kaae</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Søren Kaae Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Winther</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.05473</idno>
		<title level="m">Auxiliary deep generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Virtual adversarial training: a regularization method for supervised and semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03976</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Takeru</forename><surname>Miyato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Masanori</forename><surname>Shin-Ichi Maeda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ken</forename><surname>Koyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shin</forename><surname>Nakae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishii</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.00677</idno>
		<title level="m">Distributional smoothing with virtual adversarial training</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Semisupervised learning with ladder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antti</forename><surname>Rasmus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Berglund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mikko</forename><surname>Honkala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harri</forename><surname>Valpola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tapani</forename><surname>Raiko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3546" to="3554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Pixelcnn++: Improving the pixelcnn with discretized logistic mixture likelihood and other modifications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik P</forename><surname>Kingma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05517</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Amortised map inference for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Casper Kaae</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.04490</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised and semi-supervised learning with categorical generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jost</forename><surname>Tobias Springenberg</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06390</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arjun</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="648" to="656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Adversarial generator-encoder networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep learning via semisupervised embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frédéric</forename><surname>Ratle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hossein</forename><surname>Mobahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronan</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks: Tricks of the Trade</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="639" to="655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Revisiting semi-supervised learning with graph embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08861</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semi-supervised qa with generative domain-adaptive nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhilin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junjie</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">W</forename><surname>Cohen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.02206</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Energy-based generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junbo</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using gaussian fields and harmonic functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojin</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John D</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 20th International conference on Machine learning (ICML-03)</title>
		<meeting>the 20th International conference on Machine learning (ICML-03)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="912" to="919" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
