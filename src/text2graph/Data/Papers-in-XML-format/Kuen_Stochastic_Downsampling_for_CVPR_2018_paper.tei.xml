<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Kuen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangfei</forename><surname>Kong</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhe</forename><surname>Lin</surname></persName>
							<email>zlin@adobe.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Adobe Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Wang</surname></persName>
							<email>gangwang6@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Alibaba AI</orgName>
								<address>
									<addrLine>Labs 4 NVIDIA</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Yin</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>See</surname></persName>
							<email>ssee@nvidia.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yap-Peng</forename><surname>Tan</surname></persName>
							<email>eyptan@ntu.edu.sg</email>
							<affiliation key="aff0">
								<orgName type="institution">Nanyang Technological University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Stochastic Downsampling for Cost-Adjustable Inference and Improved Regularization in Convolutional Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>It is desirable to train convolutional networks (CNNs)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional networks (CNNs) <ref type="bibr" target="#b6">[7]</ref> have greatly accelerated the progress of many computer vision areas and applications in recent years. Despite their powerful visual representational capabilities, CNNs are bottlenecked by their immense computational demands. Recent CNN architectures such as Residual Networks (ResNets) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and Inception <ref type="bibr" target="#b35">[36]</ref> require billions of floating-point operations (FLOPs) to perform inference on just one single input image. Furthermore, as the amount of visual data grows, we need increasingly higher-capacity (thus higher complexity) CNNs which have shown to better utilize these large visual data compared to their lower-capacity counterparts <ref type="bibr" target="#b34">[35]</ref>.</p><p>There have been works which tackle the efficiency issues of deep CNNs, mainly by lowering numerical precisions (quantization) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b42">43]</ref>, pruning network weights <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b24">25]</ref>, or adopting separable convolutions <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b39">40]</ref>. These methods result in more efficient models which have fixed inference costs (measured in floating-point operations or FLOPs). Models with fixed inference costs cannot work effectively in certain resource-constrained vision systems, where the computational budget that can be allocated to CNN inference depends on the real-time resource availability. When the system is lower in resources, it is preferable to allocate a lower budget for more efficient or cheaper inference, and vice versa. Moreover, in some cases, the exact inference budget cannot be known beforehand during training time.</p><p>As a simple solution to such a concern, one could train several CNN models such that each has a different inference cost, and then select the one that matches the given budget at inference time. However, it is extremely time-consuming to train many models, not to mention the computational storage required to store the weights of many models. In this work, we focus on CNNs whose computational costs are dynamically adjustable at inference time. A CNN with costadjustable inference only has to be trained once, and it allows users to control the trade-off of inference cost against network accuracy/performance. The different inference instances (each with different inference cost) are all derived from the same model parameters.</p><p>For cost-adjustable inference in CNNs, we propose a novel training method -Stochastic Downsampling Point (SDPoint). A SDPoint instance is a network configuration consisting of a unique downsampling point (layer index) in the network layer hierarchy as well as a unique downsampling ratio. As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, at every training iteration, a SDPoint instance is randomly selected (from a list of instances), and downsampling happens based on the downsampling point and ratio of that instance. The earlier the downsampling happens, the lower the total computational costs will be, given that spatially smaller feature maps are cheaper to process.</p><p>During inference, a SDPoint instance can be deterministically handpicked (among the SDPoint instances seen during training) to match the given inference budget. Existing approaches <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b19">20</ref>] to achieve cost-adjustable inference in CNNs work by evaluating just subparts of the network (e.g., skipping layers or skipping subpaths), and therefore not all network parameters are utilized during cheaper inference. In contrast to existing approaches, SDPoint makes full use of all network parameters regardless of the inference costs, thus making better use of network representational capacity. Moreover, the (scale-related) parameter sharing across the SDPoint instances (each with a different downsampling and downsampling ratio) provides significant improvement in terms of model regularization. On top of these advantages, SDPoint is architecture-neutral, and it adds no parameter or training overheads. We carry out experiments on image classification with a variety of recent network architectures to validate the effectiveness of SDPoint in terms of cost-accuracy performances and regularization benefits. The code to reproduce experiments will be released.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Cost-adjustable Inference: One representative method to achieve cost-adjustable inference is to train "intermediate" classifiers <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b37">38]</ref> which branch out of intermediate network layers. A lower inference cost can be attained by early-exiting, based on the intermediate classifiers' output confidence <ref type="bibr" target="#b21">[22]</ref> or entropy <ref type="bibr" target="#b37">[38]</ref> threshold. The lower the threshold is, the lower the inference cost will be, and vice versa. In <ref type="bibr" target="#b21">[22]</ref>, intermediate softmax classifiers are trained (second stage) after the base network has been completely trained (first stage). The downside of <ref type="bibr" target="#b21">[22]</ref> is that the intermediate classifier losses are not backpropagated for fine-tuning the base network weights. To make the networks more aware of intermediate classifiers, BranchyNet <ref type="bibr" target="#b37">[38]</ref> has intermediate classifiers (each with more layers per branch than <ref type="bibr" target="#b21">[22]</ref>) and final classifier trained jointly, using a weighted sum of classification losses. Unlike these works, our SDPoint method relies on the same final classifier for different inference costs. FractalNets <ref type="bibr" target="#b19">[20]</ref> which are CNNs designed to have many parallel subnetworks or "paths" which can be stochastically dropped for regularization during training. For cost-adjustable inference, some FractalNet's "paths" can be left out. But the path-dropping regularization gives inconsistent/marginal improvements if data augmentation is being used. Stochastic Regularization: Our work is closely related to stochastic regularization methods which apply certain stochastic operations to network training for regularization. Dropout <ref type="bibr" target="#b33">[34]</ref> drops network activations, while DropConnect <ref type="bibr" target="#b38">[39]</ref> drops network weights. Stochastic Depth <ref type="bibr" target="#b14">[15]</ref> allows nonlinear residual building blocks to be dropped during training. Swapout <ref type="bibr" target="#b32">[33]</ref> allows elementwise Bernoulli random variables, for each of the residual network function separately. These 4 methods are similar in the way that during inference, all stochastically dropped elements (activations, weight, residual blocks) are to be present. For any of the methods, its different stochastic instances seen during training have rather comparable forward pass costs, making them unfit for cost-adjustable inference. Multiscale parameter-sharing: Multiscale training of CNNs, first introduced by <ref type="bibr" target="#b8">[9]</ref> is quite similar to SDPoint. In the training of <ref type="bibr" target="#b8">[9]</ref>, the network is trained with 224 × 224 and 180 × 180 images alternatively (one scale per epoch). The same idea has also been applied to CNN training for other tasks <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b29">30]</ref>. While multiscale training downsamples the input images to different sizes, SDPoint only downsamples feature maps (at feature level). Downsampling at feature level encourages earlier network layers to learn to better preserve information, to compensate for loss of spatial information caused by stochastic downsampling later. This does not apply to multiscale training, where the input images are downsampled through interpolation operations which happen before network training takes place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries: Conventional CNNs with Fixed Downsampling Points</head><p>Conventionally, downsampling of feature maps happens in CNNs at several predefined fixed locations/points in the layer hierarchy, depending on the architectural designs. For example, in ResNet-50, spatial pooling (happens af-ter the first ReLU layer, and after the last residual block) and strided convolutions (or convolution with strides &gt; 1 which happens right after the 3rd, 7th, and 13th residual blocks) are used to achieve downsampling. Between these downsampling layers are network stages. Downsampling in CNNs trades low-level spatial information for richer highlevel semantic information (needed for high-level visual tasks such as image classification) in a gradual fashion.</p><p>During network inference, these fixed downsampling points have to be followed exactly as how they are configured during training, for optimal accuracy performance. In this work, we go beyond fixed downsampling points -we develop a novel stochastic downsampling method named Stochastic Downsampling Point (SDPoint) which does not restrict downsampling to happen every time at same fixed points in the layer hierarchy. The proposed method is complementary to the fixed downsampling points in existing network architectures, and do not replace them. SDPoint can be simply plugged into existing network architectures, and no major architectural modifications are required.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stochastic Downsampling Point</head><p>A Stochastic Downsampling Point (SDPoint) instance has a unique downsampling point p ∈ Z and a unique downsampling ratio r ∈ R which are stochastically/randomly selected during network training. A p and a r are stochastically selected at the beginning of each network training iteration, and downsampling occurs to the selected point (based on the selected ratio) for all samples in the current training mini-batch. The downsampling points and a downsampling ratios will be discussed more thoroughly in the upcoming sections. Downsampling is performed by a downsampling function D(·) which makes use of some downsampling operations. When the selected point falls at the lower layer in the layer hierarchy, the downsampling happens earlier (in the forward propagation), causing quicker loss of spatial information in the feature maps, but more computation savings. Conversely, spatial information can be better preserved at higher computational costs, if the stochastic downsampling happens later.</p><p>SDPoint can effectively turn the feature map spatial sizes right before prediction layers to be different from original sizes, and this could cause shape incompatibility between the prediction layer weights (as well as labels) and the convolutional outputs (before prediction layers). To prevent this, we preserve the feature map spatial size in the last network stage, regardless of stochastic downsampling taking place or not, by adjusting convolution strides and/or pooling sizes accordingly. For example, in image classification networks, we consider the global average pooling layer <ref type="bibr" target="#b23">[24]</ref> and the final classification layer to be the last network stage. Therefore, regardless of the spatial size (variable due to SDPoint) of the incoming feature maps, we globally pool them to have spatial size of 1 × 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Downsampling Operation</head><p>As discussed in Sect. 3, the downsampling operation employed in D(·) can be either pooling <ref type="bibr" target="#b0">[1]</ref> (average or max variations) or strided convolution. We opt for average pooling (the corresponding downsampling function is denoted as D avg (·)), rather than strided convolutions or max pooling for several reasons. Strided convolutions are the preferred way to do downsampling in recent network architectures, because they add extra parameters (convolution weights) and therefore improving the representational capability. In this work, we want to rule out the possible performance improvements from increase in parameter numbers (rather than the SDPoint itself). Moreover, strided convolutions with integer-valued strides cannot work well with arbitrary downsampling ratios (see Sect. 4.3). On the other hand, average pooling is preferred over max pooling in this paper due to the fact that max pooling itself is a form of nonlinearity. Using max pooling as the downsampling operation could either push for a greater non-linearity in the network (positive outcome) which is unfair to the baselines, or could exacerbate the vanishing gradient problem <ref type="bibr" target="#b12">[13]</ref> commonly associated with deep networks (negative outcome). Besides, the effectiveness of average pooling has been validated through its extensive roles in recent CNN architectures (e.g., global average pooling <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b9">10]</ref>, DenseNets' transition <ref type="bibr" target="#b13">[14]</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Downsampling Points</head><p>At every training iteration, a downsampling point p for a SDPoint instance can be drawn from a discrete uniform distribution on a set of predefined downsampling point indices P = {0, 1, 2, ..., N -1, N }, with N + 1 number of points. In this work, the downsampling point candidates are the points between two consecutive CNN "basic building blocks", mirroring the placements of fixed downsampling layers in conventional CNNs. We keep the original network (without stochastic downsampling) as an instance by assigning the index p = 0 to it, so that we can perform full-cost inference later. Let F (·) denote the function carried out by the i-th basic building block, w i denote the network weights involved in the block. For a given input x i and downsampling ratio r, the downsampling is carried out as following:</p><formula xml:id="formula_0">y i = D avg (F (x i ; w i ); s i , r)<label>(1)</label></formula><p>to obtain the output y i . The downsampling switch denoted as s i ∈ {True, False} is turned on if p = i. For non-residual CNNs (e.g., VGG-Net <ref type="bibr" target="#b31">[32]</ref>), the basic building block comprises 3 consecutive convolutional, Batch Normalization (BN) <ref type="bibr" target="#b16">[17]</ref>, non-linear activation layers. On the other hand, for residual networks, residual blocks are considered as the basic building blocks. the downsampling point p can be stochastically selected to be any point between any 2 basic building blocks in the network, where downsampling happens. Since a residual block involves two streams of information -(i.) the identity skip connection and (ii.) the non-linear function consisting of several network layers, we apply stochastic downsampling function D avg (·) to the point right after the residual addition operation. We also experiment with Densely Connected Networks (DenseNets) <ref type="bibr" target="#b13">[14]</ref> in this paper. For DenseNets, the SDPoint downsampling points are the points right behind each block concatenation operation, mirroring the fixed downsampling in DenseNets.</p><p>In principle, each mini-batch sample could have its unique downsampling point p i (for stronger stochasticity), but due to practical reasons (e.g., training efficiency, ease of implementation), we resort to using the same p i for all samples in a mini-batch. While it is possible to have more than one downsampling points in each training iteration, the number of possible combinations or SDPoint instances would become excessively large. Some of the instances would deviate too much from the original network, in terms of computational cost and accuracy performance. We opt for single stochastic downsampling point in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Downsampling Ratios</head><p>We consider a set of downsampling ratios R, which the SDPoint instance can stochastically draw a downsampling ratio r from, for use at current training iteration. As with Sect. 4.2, downsampling ratios are drawn according to discrete uniform distributions. The ratios cannot be too low that they hamper the training convergence (due to parameter-sharing unfeasibility). And, we consider only a small number of downsampling ratios in R to prevent an excessive number of SDPoint instances, which would cause great difficulty in experimentally evaluating all SDPoint instances for cost-adjustable inference. A recent experimental study <ref type="bibr" target="#b25">[26]</ref> on CNNs finds that it is sufficient to make qualitative conclusions about optimal network structure that hold for the full-sized (224 × 224 image resolution) ImageNet <ref type="bibr" target="#b30">[31]</ref> classification task, by using just 128 × 128 (roughly half the original resolution) input images. Conceivably, the same network structure/architecture that works well with a certain image resolution is likely to work well with a resolution double/half of that. Motivated by the above-mentioned heuristics and experimental finding, we come up with the downsampling ratio set R = {0.5, 0.75}. The same ratios have also been used by <ref type="bibr" target="#b1">[2]</ref> for "multiscale-input" semantic segmentation. The same hyperpameter R is used across all experiments in this paper.</p><p>Downsampling with such fractional downsampling ratios cannot be trivially achieved with integer-valued pooling hyperparameters. For example, pooling a 28 × 28 feature map to a 21 × 21 one (with r of 0.75 and minimal overlaps) cannot be easily done by tuning just the pooling size and stride. To this end, we adopt a spatial pooling strategy (which works along with the pooling choice in Sect. 4.1) akin to that of Spatial Pyramid Pooling <ref type="bibr" target="#b8">[9]</ref> that generates fixed-length representation via adaptive calculations of pooling sizes and strides. Randomly draw p from P</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5:</head><p>Randomly draw r from R 6:</p><formula xml:id="formula_1">x 1 = x 7:</formula><p>for i ∈ {1, 2, ..., N -1, N } do ⊲ Forward pass <ref type="bibr">8:</ref> if i == p then s i = True else s i = False 9:</p><formula xml:id="formula_2">x i+1 = D avg (F (x i ; w i ); s i , r) 10:</formula><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Compute loss with x N +1</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>12:</head><p>Backward pass <ref type="bibr">13:</ref> Parameter updates 14: end while</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Training with SDPoint</head><p>SDPoint gives rise to a new training algorithm for CNNs. The training algorithm consolidating all the previously introduced SDPoint concepts is given in Algorithm 1. F (·) denotes the generic nonlinear building network block in CNNs. For simplicity sake, we omit the other network layers which are not basic building blocks -typically the starting and ending layers. In a nutshell, Algorithm 1 shows that whenever a building block index i is equal to the downsampling point p, the downsampling switch s is turned on. Stochastic downsampling then happens to the output of i-th building block, with the stochastic downsampling ratio r. It is important to point out that the (stochastic) downsampling does not happen, if p is drawn to be 0, allowing the network to work in its original "unadulterated" form.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Regularization</head><p>SDPoint can be seen as a regularizer for CNNs. When stochastic downsampling takes place, the receptive field size becomes larger and it causes a sudden shrinkage of spatial information in the feature maps. The network has to learn to adapt to such variations during training, and perform parameter-sharing across the downsampled feature maps and the originally sized feature maps (when p = 0). In addition to robustness in terms of receptive field size and spatial shrinkage, SDPoint also necessitates the convolutional layers to accommodate for different "padded pixel to non-padded pixel" ratios. For example, applying a 3 × 3 convolutional filter (with zero-padding of 1) to a 8 × 8 fea-ture map gives a padded-pixel ratio of 0.44, compared to 0.56 ratio resulted from applying the same filter to 6 × 6 feature map. Zero-padded pixels are quite similar to the zero-ed out activations caused by Dropout <ref type="bibr" target="#b33">[34]</ref>, in the sense that they both are missing values. Thus, a higher paddedpixel ratio is akin to having a higher number of dropped-out activations, vice versa. This form of variation provides further regularization boost. Experimentally, we find that even with the use of heavy data augmentation -such as "scale + aspect ratio" augmentation <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b35">36]</ref>, SDPoint can still help.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Cost-adjustable Inference</head><p>A network that can perform inference at different computational costs depending on the user requirements, is considered to be capable of cost-adjustable inference. Opting for a lower inference cost usually results in a lower prediction accuracy, and vice versa. SDPoint naturally supports cost-adjustable inference, given that SDPoint instances have varying computational costs, given the different downsampling point locations and downsampling ratios. More importantly, the instances have all been trained to minimize the same prediction loss, and this helps them to work relatively well for inference. During inference, one may handpick a SDPoint instance (with its downsampling point p and downsampling ratio r) to make the inference cost fit a particular inference budget.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Instance-Specific Batch Normalization</head><p>As mentioned in Sect. 4, SDPoint instances are trained in such a way that every training mini-batch and iteration shares the same SDPoint instance. For a SDPoint instance, the prediction and loss minimization during training are based on the Batch Normalization (BN) statistics (means and standard deviations) of that particular instance. Therefore, using the BN statistics accumulated over many training iterations (and thus many different SDPoint instances) for inference causes inference-training "mismatch". A similar form of inference-training "mismatch" caused by BN statistics has also been observed by <ref type="bibr" target="#b32">[33]</ref> in another context. The BN statistics required for one SDPoint instance should differ from that of another instance. When using the same (accumulated) BN statistics to perform cost-adjustable inference, the inference accuracies could be jeopardized.</p><p>To address the "mismatch" issue, we compute SDPoint instance-specific BN statistics, and use them for costadjustable inference. Disentangling the different SDPoint instances by unsharing BN statistics makes the inference more accurate. The computational storage overhead resulted from instance-specific BN statistics is relatively low, as BN statistics of some earlier layers can be shared among certain SDPoint instances that downsample at later layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>Experiments are carried out on image classification tasks to evaluate SDPoint. We consider image classification datasets with varying dataset scales in terms of numbers of categories/classes and sample counts: CIFAR-10 <ref type="bibr" target="#b18">[19]</ref> (50k training images, 10k validation images, 10 classes), CIFAR-100 <ref type="bibr" target="#b18">[19]</ref> (50k training images, 10k validation images, 100 classes), ImageNet <ref type="bibr" target="#b30">[31]</ref> (1.2M training images, 50k validation images, 1000 classes). For inference cost comparison, we measure the model costs in terms of floating-point operation numbers (FLOPs) needed for forward propagation of single image. We treat addition and multiplication as 2 separate operations. Implementations are in PyTorch <ref type="bibr" target="#b26">[27]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">CIFAR</head><p>For CIFAR-10 and CIFAR-100, the baseline architectures are Wide-ResNet <ref type="bibr" target="#b41">[42]</ref> (WRN-d28-w10 and WRN-d40-w4) and DenseNetBC-d40-g60 <ref type="bibr" target="#b13">[14]</ref>. 'd', 'w', 'g' stand for the network depth, widen factor of WRN, and growth rate of DenseNetBC, respectively. The training hyperparameters (e.g., learning rates, schedules, batch sizes, augmentation) follow the ones in original papers, except for training epoch numbers which we fix to 400 for all. The original learning rate schedules still apply (e.g., learning rates are dropped at 50% and 75% of total number of training epochs). The numbers of SDPoint downsampling points (N ) for {WRN-d28-w10, WRN-d40-w4, and DenseNetBC-d40-g60} are {12, 18 ,12} respectively. As mentioned in Sect. 4.3, the downsampling ratios are drawn uniformly from R = {0.5, 0.75}.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.1">Baseline Comparison:</head><p>We compare SDPoint with some baseline methods related to ours, in terms of cost-adjustable inference performance. The classification error-cost performance plots on CIFAR-10 and CIFAR-100 are shown in <ref type="figure" target="#fig_2">Fig. 2</ref>. Note that for SDPoint and baseline methods, not all instances of the same model appear on the plots; if a higher-cost instance performs worse than any lower-cost instance, it is not shown. Each model (evaluated on a dataset) is trained only once to obtain its cost-error plot.</p><p>(i) Early-Exits (EE): We train models based on the WRN with intermediate classifiers (branches) which allow earlyexits (EE), following the design of BranchyNet <ref type="bibr" target="#b37">[38]</ref>. Each network stage in the main network has two evenly spaced branches, and the branches each have single-repetition of building block per branch network stage. The blocks in the branches follow the same hyperparameters (e.g., #channels) as the blocks in the original network. For cost-adjustable inference, we evaluate every branch, and make all samples "exit" at the same branch. The early-exit models have considerably more parameters than both the baseline models and SDPoint-based models. We conjecture that the rela- tively worse performance of EE is due to lack of full network parameter ultilization. Also, EE forces CNN features to be classification-ready in early stage, thus causing higher layers to rely heavily on the classification-ready features, instead of learning better features on their own.</p><p>(ii) Multiscale Training (MS): Multiscale (MS) training is a baseline method inspired by <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b29">30]</ref>. The input images are downsampled using bilinear interpolations, to an integer-valued size randomly chosen from sizes ranging from half (16 × 16) to full size (32 × 32), with step size of 1 pixel. This is done for every training iteration, similar to SDPoint. The number of "instances" <ref type="bibr" target="#b15">(16)</ref> resulted from multiscale training is close to the downsampling point numbers of applying SDPoint to WRNs and DenseNetBC(s). Also, the ranges of cost-adjustable inference costs among them are comparable. Instance-specific BN statistics are applied. The cost-adjustable performance of MS consistently trails behind that of SDPoint, as input downsampling causes more drastic information loss than feature map downsampling (see Sect. 2). (iii) Uniform Batch Normalization (UBN): To validate the effectiveness of SDPoint instance-specific BN, we show the results of a SDPoint baseline whose BN statistics are averaged from many training iterations, and are uniform for all of its instances. There are consistent classification performance gaps between using UBN statistics and instance-specific BN statistics, suggesting that it is preferable to keep instance-specific statistics for inference. <ref type="table" target="#tab_1">Table 1</ref> reports the CIFAR validation results of state-of-the-art (SOTA) ResNeXt <ref type="bibr" target="#b39">[40]</ref> and DenseNetBC <ref type="bibr" target="#b13">[14]</ref>    the best-performing SDPoint instance among its instances. Notably, WRN-d28-w10 with SDPoint is competitive to SOTA models on CIFAR-100, and it outperforms them on CIFAR-10. Overall, SDPoint considerably improves classification performance without bringing in additional parameters and computational costs, unlike the SOTA models which require about 2× model complexity to attain slight improvements. In fact, the best SDPointenabled models on CIFAR-10 have reduced inference costs (FLOPs). We reckon that a prolonged preservation of spatial details (i.e., no early downsampling) in CNN feature maps is not crucial to a dataset with relatively low label complexity such as CIFAR-10. This reveals a drawback  <ref type="table">Table 3</ref>: ImageNet top-1 and top-5 validation errors (%), with model parameter numbers and giga-FLOPs (GFLOPs).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">State-of-the-art Comparison:</head><p>of current practice of using CNNs in "one-size-fits-all" fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.3">Stochastic Training and Inference:</head><p>Since our method is related to the stochastic training family, we experimentally compare SDPoint with Dropout <ref type="bibr" target="#b33">[34]</ref> and Swapout <ref type="bibr" target="#b32">[33]</ref> (using 2 linear decay rules suggested), with WRN-d28-w10 as the baseline and CIFAR-100 dataset. The results are given in <ref type="table" target="#tab_2">Table 2</ref>. Stochastic inference was proposed by <ref type="bibr" target="#b32">[33]</ref> as a way to tackle the poor interaction of Swapout/Dropout with BatchNorm. For fair comparison, we report also the results of performing stochastic inference (50 trials). In both deterministic and stochastic inference settings, we find that SDPoint outperforms the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">ImageNet</head><p>We consider ResNeXt-d101-c32 <ref type="bibr" target="#b39">[40]</ref> and PreResNetd101 <ref type="bibr" target="#b10">[11]</ref> as baseline architectures.</p><p>'c' stands for ResNeXt's cardinality. With SDPoint, there are 33 downsampling points (N ) per model. We train the models on ImageNet-1k <ref type="bibr" target="#b30">[31]</ref> training set, and evaluate them on the validation set (224×224 center crops). All models are trained using training hyperparameters and "scale + aspect ratio" augmentation <ref type="bibr" target="#b36">[37]</ref> similar to <ref type="bibr" target="#b39">[40]</ref>. Note that we do not allocate more training epochs to models with SDPoint. The cost-error plots are given in <ref type="figure">Fig. 3 and 4</ref>, for PreResNet-d101 and ResNeXt-d101-c32 respectively, along with some fixed-cost &amp; carefully designed 1 baseline models from the same architecture families. Overall, models trained with SDPoint can roughly match the performance of baseline models in the lower-cost range, and surpass them in the upper-cost range. Notably, to obtain cost-error plots, SDPoint-enabled models only have to be trained once. The baseline models are trained separately, 1 model hyperparameters are carefully chosen by the authors <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b39">40]</ref> to optimize accuracy performances under some budget constraints. PreResNet-d50</p><p>PreResNet-d101</p><p>PreResNet-d152</p><p>PreResNet-d200</p><p>PreResNet-d101 (SDPoint) <ref type="figure">Figure 3</ref>: PreResNets' <ref type="bibr" target="#b10">[11]</ref> cost-error plots on ImageNet. PreResNet-d101 (SDPoint) only has to be trained once (as a single model), while the baseline models (without SDPoint) has to be trained separately with huge training and storage costs. ResNeXt-d50-c32</p><p>ResNeXt-d101-c32</p><p>ResNeXt-d101-c64</p><p>ResNeXt-d101-c32 (SDPoint-075)</p><p>ResNeXt-d101-c32 (SDPoint-alternate)</p><p>ResNeXt-d101-c32 (SDPoint) <ref type="figure">Figure 4</ref>: ResNeXts' <ref type="bibr" target="#b39">[40]</ref> cost-error plots on ImageNet. Like <ref type="figure">Fig. 3</ref>, any of ResNeXt-d101-c32 (SDPoint..) only has to be trained once (as a single model).</p><p>resulting in a huge total number of epochs (#models × #epochs per model) and storage cost.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">Ablation Study:</head><p>We study the effects of choice of SDPoint downsampling points and downsampling ratios on cost-adjustable inference performance. For this, we train a ResNeXt-d101-c32 with default SDPoint hyperparameters (downsampling points at the end of every residual block, downsampling ratios of {0.5,0.75}), as well as 2 baseline models with (i) downsampling points at the end of every other residual block dubbed alternate (ii) downsampling ratio of just {0.75} dubbed 075. They are shown on <ref type="figure">Fig. 4</ref>. Either removing the 0.5 downsampling ratio or alternating blocks for downsampling gives worse results, due to reduced stochasticity (and regularization strengths).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">State-of-the-art Comparison:</head><p>We compare our models with SOTA ResNeXt-d101-c64 <ref type="bibr" target="#b39">[40]</ref> and DenseNetBC-d264-g48 <ref type="bibr" target="#b27">[28]</ref> models in <ref type="table">Table 3</ref>. SDPoint pushes the top-1 and top-5 validation errors of ResNeXtd101-c32 down to 20.4% and 5.3% respectively, which are (previously) only attainable by SOTA models with roughly 2× inference costs and parameter counts. We also display the results (and mean FLOPs) of Spatially Adaptive Computation Time (SACT) <ref type="bibr" target="#b3">[4]</ref> paired with PreResNet-d101, and compare it to a SDPoint instance of our PreResNet-d101 that achieves similar classification errors. SDPoint merely needs 69% of FLOPs needed by SACT to achieve similar results. SACT saves computation by skipping layers (and network parameters) for certain locations in feature maps according to learned policy and inputs, while SDPoint downsamples feature maps to save computation (but makes full use of network parameters &amp; capacity during inference). We contend that in cost-accuracy trade-off for inference, reducing feature map spatial sizes is less harmful to accuracy than skipping network parameters/layers. As a regularization study, we train the 2 baseline models with Stochastic Depth <ref type="bibr" target="#b14">[15]</ref> (using suggested decay rule), and find that they considerably degrade the classification performance unlike SDPoint. 6.2.3 Analysis: We provide some analyses of ResNeXtd101-c32 (trained with SDPoint on ImageNet) with regards to certain aspects of downsampling and SDPoint.</p><p>Cost-dependent misclassifications: We group ImageNet validation images (which are correctly classified with full inference cost) according to the minimum inference costs required to classify them correctly, and present some examples on <ref type="figure" target="#fig_5">Fig. 5</ref>. More difficult examples that require higher inference costs (9.9, 16.0 GFLOPs) to be classified correctly, generally have size-dominant interfering objects/scenes (e.g., hair dryer, cab, caldron, cock, tench), in contrast to the easier examples (4.3 GFLOPs). Intuitively, pooling-based downsampling causes more information loss to smaller objects than to larger (size-dominant) objects, especially when it occurs at some early layer, where the semantic/context information is still relatively weak to distinguish objects of interest from interfering objects. So, for those difficult examples, it makes sense to preserve spatially informative object details longer in the CNN layer hierarchy, and downsample the feature maps only after they are semantically rich enough.</p><p>Scale sensitivity: Training CNNs with SDPoint involves stochastic downsampling of intermediate feature maps, which we hypothesize to be beneficial for scale sensitivity/invariance, as mentioned in Sect. 4.5. To validate this hypothesis, we vary the pre-cropping 2 sizes of ImageNet validation images in the range of 256, ..., 352 with step size of 16, resulting in 7 pre-cropping sizes. For every precropping size, 224 × 224 center image regions are cropped out for evaluation. The models involved are SDPointenabled ResNeXt-d101-c32, and the baseline without SDPoint. We compute the mean of all pairwise cosine similarities (a total of 21 pairs) resulted from the different precropping sizes, in terms of ImageNet 1k-class probability scores. This is done for entire ImageNet validation set. The pairwise cosine-similarity mean obtained for baseline model is 0.944, while for the SDPoint-enabled model, it is 0.961. A higher cosine similarity is a strong indicator of the model being less sensitive to scales. This demonstrates that SDPoint can indeed benefit CNNs, in terms of scale sensitivity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose Stochastic Downsampling Point (SDPoint), a novel approach to train CNNs by downsampling intermediate feature maps. At no extra parameter and training costs, SDPoint facilitates effective cost-adjustable inference and greatly improves network regularization (thus accuracy performance). Through experiments, we additionally find out that SDPoint can help to identify more optimal (yet less costly) sub-networks (Sect. 6.1.2), sort input examples by various levels of classification difficulties <ref type="figure" target="#fig_5">(Fig. 5)</ref>, and making CNNs less scale-sensitive (Sect. 6.2.3).</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Progression of feature map spatial sizes during training of a (Left) conventional CNN, (Right) with SDPoint. The costs here refer to computational costs measured in numbers of floating-point operations (FLOPs).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>P = {0, 1, 2, ..., N -1, N } ⊲ Downsampling Points 2: R = {0.5, 0.75} ⊲ Downsampling Ratios 3: while given a training mini-batch x do 4:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: WRNs' and DenseNetBC's cost-error plots on CIFAR-10 (Top) and CIFAR-100 (Bottom). It is observed that models trained with SDPoint consistently outperform their non-SDPoint counterparts, given the same computational budgets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Some Imagenet validation examples grouped according to the minimum inference costs (FLOPs) required by ResNeXt-d101-c32 (with SDPoint) to classify them correctly, in terms of top-5 accuracy. The ground-truth label names are shown below their corresponding images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>CIFAR-10 and CIFAR-100 validation errors (%). 
The GFLOPs with 2 values separated by "/" are for CIFAR-
10 and CIFAR-100 respectively. 

Model 
Error(%)/Deterministic Error(%)/Stochastic 
WRN-d28-w10 
18.51 
-
*with Dropout [34] 
18.05 
17.98 
*with Swapout [33], Linear(1,0.5) 
20.55 
18.68 
*with Swapout [33], Linear(1,0.8) 
19.21 
18.65 
*with SDPoint 
17.53 
17.20 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Deterministic</figDesc><table>and stochastic inference results of 
training WRN-d28-w10 [42] with different stochastic train-
ing methods on CIFAR-100. 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research was carried out at the Rapid-Rich Object Search (ROSE) Lab, Nanyang Technological University, Singapore. The ROSE Lab is supported by the National Research Foundation, Singapore, and the Infocomm Media Development Authority, Singapore. <ref type="bibr" target="#b1">2</ref> It is a standard practice <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b13">14]</ref> to resize images to a shorter side of 256 (pre-cropping size) before doing 224 × 224 center-cropping.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theoretical analysis of feature pooling in visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2004" />
			<publisher>TPAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Xception: Deep learning with depthwise separable convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Chollet</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Spatially adaptive computation time for residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Figurnov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vetrov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Adaptive computation time for recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.08983</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An empirical study of language cnn for image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shuai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cai</surname></persName>
		</author>
		<title level="m">Recent advances in convolutional neural networks. Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning both weights and connections for efficient neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial pyramid pooling in deep convolutional networks for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Channel pruning for accelerating very deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Untersuchungen zu dynamischen neuronalen netzen. Diploma</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1991" />
			<biblScope unit="volume">91</biblScope>
		</imprint>
		<respStmt>
			<orgName>Technische Universität München</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep networks with stochastic depth</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sedra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="646" to="661" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Hubara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Courbariaux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Soudry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>El-Yaniv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<title level="m">Binarized neural networks. In Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Speeding up convolutional neural networks with low rank expansions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fractalnet: Ultra-deep neural networks without residuals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<publisher>AIS-TATS</publisher>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Resource-constrained classification using a cascade of neural network layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leroux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bohez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Verbelen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Vankeirsbilck</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Simoens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dhoedt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conference on Neural Networks (IJCNN)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pruning filters for efficient convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Durdanovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Samet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Graf</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Thinet: A filter level pruning method for deep neural network compression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lin</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Systematic evaluation of convolution neural network advances on the imagenet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sergievskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<idno>2017. 4</idno>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint/>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://pytorch.org/.5" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pleiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.06990</idno>
		<title level="m">Memory-efficient implementation of densenets</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Xnornet: Imagenet classification using binary convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rastegari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Yolo9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Swapout: Learning an ensemble of deep architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Revisiting unreasonable effectiveness of data in deep learning era</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Branchynet: Fast inference via early exiting from deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teerapittayanon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcdanel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Designing energyefficient convolutional neural networks using energy-aware pruning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sze</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Trained ternary quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
		<idno>2017. 1</idno>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
