<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marco</forename><surname>Fraccaro</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark ‡ DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Kamronn</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark ‡ DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ulrich</forename><surname>Paquet</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark ‡ DeepMind</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ole</forename><surname>Winther</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Technical University of Denmark ‡ DeepMind</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper takes a step towards temporal reasoning in a dynamically changing video, not in the pixel space that constitutes its frames, but in a latent space that describes the non-linear dynamics of the objects in its world. We introduce the Kalman variational auto-encoder, a framework for unsupervised learning of sequential data that disentangles two latent representations: an object's representation, coming from a recognition model, and a latent state describing its dynamics. As a result, the evolution of the world can be imagined and missing data imputed, both without the need to generate high dimensional frames at each time step. The model is trained end-to-end on videos of a variety of simulated physical systems, and outperforms competing methods in generative and missing data imputation tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>From the earliest stages of childhood, humans learn to represent high-dimensional sensory input to make temporal predictions. From the visual image of a moving tennis ball, we can imagine its trajectory, and prepare ourselves in advance to catch it. Although the act of recognising the tennis ball is seemingly independent of our intuition of Newtonian dynamics <ref type="bibr" target="#b30">[31]</ref>, very little of this assumption has yet been captured in the end-to-end models that presently mark the path towards artificial general intelligence. Instead of basing inference on any abstract grasp of dynamics that is learned from experience, current successes are autoregressive: to imagine the tennis ball's trajectory, one forward-generates a frame-by-frame rendering of the full sensory input <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>To disentangle two latent representations, an object's, and that of its dynamics, this paper introduces Kalman variational auto-encoders (KVAEs), a model that separates an intuition of dynamics from an object recognition network (section 3). At each time step t, a variational auto-encoder <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> compresses high-dimensional visual stimuli x t into latent encodings a t . The temporal dynamics in the learned a t -manifold are modelled with a linear Gaussian state space model that is adapted to handle complex dynamics (despite the linear relations among its states z t ). The parameters of the state space model are adapted at each time step, and non-linearly depend on past a t 's via a recurrent neural network. Exact posterior inference for the linear Gaussian state space model can be preformed with the Kalman filtering and smoothing algorithms, and is used for imputing missing data, for instance when we imagine the trajectory of a bouncing ball after observing it in initial and final video frames (section 4). The separation between recognition and dynamics model allows for missing data imputation to be done via a combination of the latent states z t of the model and its encodings a t only, without having to forward-sample high-dimensional images x t in an autoregressive way. KVAEs are tested on videos of a variety of simulated physical systems in section 5: from raw visual stimuli, it "end-to-end" learns the interplay between the recognition and dynamics components. As KVAEs can do smoothing, they outperform an array of methods in generative and missing data imputation tasks (section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Background</head><p>Linear Gaussian state space models. Linear Gaussian state space models (LGSSMs) are widely used to model sequences of vectors a = a 1:</p><formula xml:id="formula_0">T = [a 1 , .., a T ].</formula><p>LGSSMs model temporal correlations through a first-order Markov process on latent states z = [z 1 , .., z T ], which are potentially further controlled with external inputs u = [u 1 , .., u T ], through the Gaussian distributions</p><formula xml:id="formula_1">p γt (z t |z t−1 , u t ) = N (z t ; A t z t−1 + B t u t , Q) , p γt (a t |z t ) = N (a t ; C t z t , R) .<label>(1)</label></formula><p>Matrices γ t = [A t , B t , C t ] are the state transition, control and emission matrices at time t. Q and R are the covariance matrices of the process and measurement noise respectively. With a starting state z 1 ∼ N (z 1 ; 0, Σ), the joint probability distribution of the LGSSM is given by</p><formula xml:id="formula_2">p γ (a, z|u) = p γ (a|z) p γ (z|u) = T t=1 p γt (a t |z t ) · p(z 1 ) T t=2 p γt (z t |z t−1 , u t ) ,<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">γ = [γ 1 , .., γ T ].</formula><p>LGSSMs have very appealing properties that we wish to exploit: the filtered and smoothed posteriors p(z t |a 1:t , u 1:t ) and p(z t |a, u) can be computed exactly with the classical Kalman filter and smoother algorithms, and provide a natural way to handle missing data.</p><p>Variational auto-encoders. A variational auto-encoder (VAE) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b24">25]</ref> defines a deep generative model p θ (x t , a t ) = p θ (x t |a t )p(a t ) for data x t by introducing a latent encoding a t . Given a likelihood p θ (x t |a t ) and a typically Gaussian prior p(a t ), the posterior p θ (a t |x t ) represents a stochastic map from x t to a t 's manifold. As this posterior is commonly analytically intractable, VAEs approximate it with a variational distribution q φ (a t |x t ) that is parameterized by φ. The approximation q φ is commonly called the recognition, encoding, or inference network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Kalman Variational Auto-Encoders</head><p>The useful information that describes the movement and interplay of objects in a video typically lies in a manifold that has a smaller dimension than the number of pixels in each frame. In a video of a ball bouncing in a box, like Atari's game Pong, one could define a one-to-one mapping from each of the high-dimensional frames x = [x 1 , .., x T ] into a two-dimensional latent space that represents the position of the ball on the screen. If the position was known for consecutive time steps, for a set of videos, we could learn the temporal dynamics that govern the environment. From a few new positions one might then infer where the ball will be on the screen in the future, and then imagine the environment with the ball in that position. The Kalman variational auto-encoder (KVAE) is based on the notion described above. To disentangle recognition and spatial representation, a sensory input x t is mapped to a t (VAE), a variable on a low-dimensional manifold that encodes an object's position and other visual properties. In turn, a t is used as a pseudo-observation for the dynamics model (LGSSM). x t represents a frame of a video 2 x = [x 1 , .., x T ] of length T . Each frame is encoded into a point a t on a low-dimensional manifold, so that the KVAE contains T separate VAEs that share the same decoder p θ (x t |a t ) and encoder q φ (a t |x t ), and depend on each other through a time-dependent prior over a = [a 1 , .., a T ]. This is illustrated in figure 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Generative model</head><p>We assume that a acts as a latent representation of the whole video, so that the generative model of a sequence factorizes as p θ (x|a) = T t=1 p θ (x t |a t ). In this paper p θ (x t |a t ) is a deep neural network parameterized by θ, that emits either a factorized Gaussian or Bernoulli probability vector depending on the data type of x t . We model a with a LGSSM, and following (2), its prior distribution is</p><formula xml:id="formula_4">p γ (a|u) = p γ (a|z) p γ (z|u) dz ,<label>(3)</label></formula><p>so that the joint density for the KVAE factorizes as p(x, a, z|u</p><formula xml:id="formula_5">) = p θ (x|a) p γ (a|z) p γ (z|u). A</formula><p>LGSSM forms a convenient back-bone to a model, as the filtered and smoothed distributions p γ (z t |a 1:t , u 1:t ) and p γ (z t |a, u) can be obtained exactly. Temporal reasoning can be done in the latent space of z t 's and via the latent encodings a, and we can do long-term predictions without having to auto-regressively generate high-dimensional images x t . Given a few frames, and hence their encodings, one could "remain in latent space" and use the smoothed distributions to impute missing frames. Another advantage of using a to separate the dynamics model from x can be seen by considering the emission matrix C t . Inference in the LGSSM requires matrix inverses, and using it as a model for the prior dynamics of a t allows the size of C t to remain small, and not scale with the number of pixels in x t . While the LGSSM's process and measurement noise in (1) are typically formulated with full covariance matrices <ref type="bibr" target="#b25">[26]</ref>, we will consider them as isotropic in a KVAE, as a t act as a prior in a generative model that includes these extra degrees of freedom.</p><p>What happens when a ball bounces against a wall, and the dynamics on a t are not linear any more? Can we still retain a LGSSM backbone? We will incorporate nonlinearities into the LGSSM by regulating γ t from outside the exact forward-backward inference chain. We revisit this central idea at length in section 3.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Learning and inference for the KVAE</head><p>We learn θ and γ from a set of example sequences {x (n) } by maximizing the sum of their respective log likelihoods L = n log p θγ (x (n) |u (n) ) as a function of θ and γ. For simplicity in the exposition we restrict our discussion below to one sequence, and omit the sequence index n. The log likelihood or evidence is an intractable average over all plausible settings of a and z, and exists as the denominator in Bayes' theorem when inferring the posterior p(a, z|x, u). A more tractable approach to both learning and inference is to introduce a variational distribution q(a, z|x, u) that approximates the posterior. The evidence lower bound (ELBO) F is</p><formula xml:id="formula_6">log p(x|u) = log p(x, a, z|u) ≥ E q(a,z|x,u) log p θ (x|a)p γ (a|z)p γ (z|u) q(a, z|x, u) = F(θ, γ, φ) ,<label>(4)</label></formula><p>and a sum of F's is maximized instead of a sum of log likelihoods. The variational distribution q depends on φ, but for the bound to be tight we should specify q to be equal to the posterior distribution that only depends on θ and γ. Towards this aim we structure q so that it incorporates the exact conditional posterior p γ (z|a, u), that we obtain with Kalman smoothing, as a factor of γ:</p><formula xml:id="formula_7">q(a, z|x, u) = q φ (a|x) p γ (z|a, u) = T t=1 q φ (a t |x t ) p γ (z|a, u) .<label>(5)</label></formula><p>The benefit of the LGSSM backbone is now apparent. We use a "recognition model" to encode each x t using a non-linear function, after which exact smoothing is possible. In this paper q φ (a t |x t ) is a deep neural network that maps x t to the mean and the diagonal covariance of a Gaussian distribution. As explained in section 4, this factorization allows us to deal with missing data in a principled way. Using (5), the ELBO in (4) becomes</p><formula xml:id="formula_8">F(θ, γ, φ) = E q φ (a|x) log p θ (x|a) q φ (a|x) + E pγ (z|a,u) log p γ (a|z)p γ (z|u) p γ (z|a, u) .<label>(6)</label></formula><p>The lower bound in (6) can be estimated using Monte Carlo integration with samples { a</p><formula xml:id="formula_9">(i) , z (i) } I i=1</formula><p>drawn from q,</p><formula xml:id="formula_10">F(θ, γ, φ) = 1 I i log p θ (x| a (i) )+log p γ ( a (i) , z (i) |u)−log q φ ( a (i) |x)−log p γ ( z (i) | a (i) , u) . (7)</formula><p>Note that the ratio p γ (</p><formula xml:id="formula_11">a (i) , z (i) |u)/p γ ( z (i) | a (i) , u) in (7) gives p γ ( a (i) |u)</formula><p>, but the formulation with { z (i) } allows stochastic gradients on γ to also be computed. A sample from q can be obtained by first sampling a ∼ q φ (a|x), and using a as an observation for the LGSSM. The posterior p γ (z| a, u) can be tractably obtained with a Kalman smoother, and a sample z ∼ p γ (z| a, u) obtained from it. Parameter learning is done by jointly updating θ, φ, and γ by maximising the ELBO on L, which decomposes as a sum of ELBOs in (6), using stochastic gradient ascent and a single sample to approximate the intractable expectations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Dynamics parameter network</head><p>The LGSSM provides a tractable way to structure p γ (z|a, u) into the variational approximation in (5). However, even in the simple case of a ball bouncing against a wall, the dynamics on a t are not linear anymore. We can deal with these situations while preserving the linear dependency between consecutive states in the LGSSM, by non-linearly changing the parameters γ t of the model over time as a function of the latent encodings up to time t − 1 (so that we can still define a generative model). Smoothing is still possible as the state transition matrix A t and others in γ t do not have to be constant in order to obtain the exact posterior p γ (z t |a, u).</p><p>Recall that γ t describes how the latent state z t−1 changes from time t − 1 to time t. In the more general setting, the changes in dynamics at time t may depend on the history of the system, encoded in a 1:t−1 and possibly a starting code a 0 that can be learned from data. If, for instance, we see the ball colliding with a wall at time t − 1, then we know that it will bounce at time t and change direction. We then let γ t be a learnable function of a 0:t−1 , so that the prior in (2) becomes</p><formula xml:id="formula_12">p γ (a, z|u) = T t=1 p γt(a0:t−1) (a t |z t ) · p(z 1 ) T t=2 p γt(a0:t−1) (z t |z t−1 , u t ) . (8) dt−1 dt dt+1 αt−1 αt αt+1</formula><p>at−2 at−1 at During inference, after all the frames are encoded in a, the dynamics parameter network returns γ = γ(a), the parameters of the LGSSM at all time steps. We can now use the Kalman smoothing algorithm to find the exact conditional posterior over z, that will be used when computing the gradients of the ELBO.</p><p>In our experiments the dependence of γ t on a 0:t−1 is modulated by a dynamics parameter network α t = α t (a 0:t−1 ), that is implemented with a recurrent neural network with LSTM cells that takes at each time step the encoded state as input and recurses d t = LSTM(a t−1 , d t−1 ) and α t = softmax(d t ), as illustrated in <ref type="figure" target="#fig_1">figure 2</ref>. The output of the dynamics parameter network is weights that sum to one,</p><formula xml:id="formula_13">K k=1 α (k)</formula><p>t (a 0:t−1 ) = 1. These weights choose and interpolate between K different operating modes:</p><formula xml:id="formula_14">A t = K k=1 α (k) t (a 0:t−1 )A (k) , B t = K k=1 α (k) t (a 0:t−1 )B (k) , C t = K k=1 α (k) t (a 0:t−1 )C (k) .<label>(9)</label></formula><p>We globally learn K basic state transition, control and emission matrices A (k) , B (k) and C (k) , and interpolate them based on information from the VAE encodings. The weighted sum can be interpreted as a soft mixture of K different LGSSMs whose time-invariant matrices are combined using the timevarying weights α t . In practice, each of the K sets</p><formula xml:id="formula_15">{A (k) , B (k) , C (k) } models different dynamics,</formula><p>that will dominate when the corresponding α (k) t is high. The dynamics parameter network resembles the locally-linear transitions of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>; see section 6 for an in depth discussion on the differences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Missing data imputation</head><p>Let x obs be an observed subset of frames in a video sequence, for instance depicting the initial movement and final positions of a ball in a scene. From its start and end, can we imagine how the ball reaches its final position? Autoregressive models like recurrent neural networks can only forward-generate x t frame by frame, and cannot make use of the information coming from the final frames in the sequence. To impute the unobserved frames x un in the middle of the sequence, we need to do inference, not prediction.</p><p>The KVAE exploits the smoothing abilities of its LGSSM to use both the information from the past and the future when imputing missing data. In general, if x = {x obs , x un }, the unobserved frames in x un could also appear at non-contiguous time steps, e.g. missing at random. Data can be imputed by sampling from the joint density p(a un , a obs , z|x obs , u), and then generating x un from a un . We factorize this distribution as</p><formula xml:id="formula_16">p(a un , a obs , z|x obs , u) = p γ (a un |z) p γ (z|a obs , u) p(a obs |x obs ) ,<label>(10)</label></formula><p>and we sample from it with ancestral sampling starting from x obs . Reading (10) from right to left, a sample from p(a obs |x obs ) can be approximated with the variational distribution q φ (a obs |x obs ). Then, if γ is fully known, p γ (z|a obs , u) is computed with an extension to the Kalman smoothing algorithm to sequences with missing data, after which samples from p γ (a un |z) could be readily drawn.</p><p>However, when doing missing data imputation the parameters γ of the LGSSM are not known at all time steps. In the KVAE, each γ t depends on all the previous encoded states, including a un , and these need to be estimated before γ can be computed. In this paper we recursively estimate γ in the following way. Assume that x 1:t−1 is known, but not x t . We sample a 1:t−1 from q φ (a 1:t−1 |x 1:t−1 ) using the VAE, and use it to compute γ 1:t . The computation of γ t+1 depends on a t , which is missing, and an estimate a t will be used. Such an estimate can be arrived at in two steps. The filtered posterior distribution p γ (z t−1 |a 1:t−1 , u 1:t−1 ) can be computed as it depends only on γ 1:t−1 , and from it, we sample</p><formula xml:id="formula_17">z t ∼ p γ (z t |a 1:t−1 , u 1:t ) = p γt (z t |z t−1 , u t ) p γ (z t−1 |a 1:t−1 , u 1:t−1 ) dz t−1<label>(11)</label></formula><p>and sample a t from the predictive distribution of a t ,</p><formula xml:id="formula_18">a t ∼ p γ (a t |a 1:t−1 , u 1:t ) = p γt (a t |z t ) p γ (z t |a 1:t−1 , u 1:t ) dz t ≈ p γt (a t | z t ) .<label>(12)</label></formula><p>The parameters of the LGSSM at time t + 1 are then estimated as γ t+1 ([a 0:t−1 , a t ]). The same procedure is repeated at the next time step if x t+1 is missing, otherwise a t+1 is drawn from the VAE. After the forward pass through the sequence, where we estimate γ and compute the filtered posterior for z, the Kalman smoother's backwards pass computes the smoothed posterior. While the smoothed posterior distribution is not exact, as it relies on the estimate of γ obtained during the forward pass, it improves data imputation by using information coming from the whole sequence; see section 5 for an experimental illustration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>We motivated the KVAE with an example of a bouncing ball, and use it here to demonstrate the model's ability to separately learn a recognition and dynamics model from video, and use it to impute missing data. To draw a comparison with deep variational Bayes filters (DVBFs) <ref type="bibr" target="#b15">[16]</ref>, we apply the KVAE to <ref type="bibr" target="#b15">[16]</ref>'s pendulum example. We further apply the model to a number of environments with different properties to demonstrate its generalizability. All models are trained end-to-end with stochastic gradient descent. Using the control input u t in <ref type="formula" target="#formula_1">(1)</ref> we can inform the model of known quantities such as external forces, as will be done in the pendulum experiment. In all the other experiments, we omit such information and train the models fully unsupervised from the videos only. Further implementation details can be found in the supplementary material (appendix A) and in the Tensorflow <ref type="bibr" target="#b0">[1]</ref> code released at github.com/simonkamronn/kvae.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Bouncing ball</head><p>We simulate 5000 sequences of 20 time steps each of a ball moving in a two-dimensional box, where each video frame is a 32x32 binary image. A video sequence is visualised as a single image in <ref type="figure" target="#fig_4">figure  4d</ref>, with the ball's darkening color reflecting the incremental frame index. In this set-up the initial position and velocity are randomly sampled. No forces are applied to the ball, except for the fully elastic collisions with the walls. The minimum number of latent dimensions that the KVAE requires to model the ball's dynamics are a t ∈ R 2 and z t ∈ R 4 , as at the very least the ball's position in the box's 2d plane has to be encoded in a t , and z t has to encode the ball's position and velocity. The model's flexibility increases with more latent dimensions, but we choose these settings for the sake of interpretable visualisations. The dynamics parameter network uses K = 3 to interpolate three modes, a constant velocity, and two non-linear interactions with the horizontal and vertical walls.</p><p>We compare the generation and imputation performance of the KVAE with two recurrent neural network (RNN) models that are based on the same auto-encoding (AE) architecture as the KVAE and are modifications of methods from the literature to be better suited to the bouncing ball experiments.  In the AE-RNN, inspired by the architecture from <ref type="bibr" target="#b28">[29]</ref>, a pretrained convolutional auto-encoder, identical to the one used for the KVAE, feeds the encodings to an LSTM network <ref type="bibr" target="#b12">[13]</ref>. During training the LSTM predicts the next encoding in the sequence and during generation we use the previous output as input to the current step. For data imputation the LSTM either receives the previous output or, if available, the encoding of the observed frame (similarly to filtering in the KVAE). The VAE-RNN is identical to the AE-RNN except that uses a VAE instead of an AE, similarly to the model from <ref type="bibr" target="#b5">[6]</ref>. <ref type="figure" target="#fig_3">Figure 3a</ref> shows how well missing frames are imputed in terms of the average fraction of incorrectly guessed pixels. In it, the first 4 frames are observed (to initialize the models) after which the next 16 frames are dropped at random with varying probabilities. We then impute the missing frames by doing filtering and smoothing with the KVAE. We see in figure 3a that it is beneficial to utilize information from the whole sequence (even the future observed frames), and a KVAE with smoothing outperforms all competing methods. Notice that dropout probability 1 corresponds to pure generation from the models. <ref type="figure" target="#fig_3">Figure 3b</ref> repeats this experiment, but makes it more challenging by removing an increasing number of consecutive frames from the middle of the sequence (T = 20). In this case the ability to encode information coming from the future into the posterior distribution is highly beneficial, and smoothing imputes frames much better than the other methods. <ref type="figure" target="#fig_3">Figure 3c</ref> graphically illustrates figure 3b. We plot three trajectories over a t -encodings. The generated trajectories were obtained after initializing the KVAE model with 4 initial frames, while the smoothed trajectories also incorporated encodings from the last 4 frames of the sequence. The encoded trajectories were obtained with no missing data, and are therefore considered as ground truth. In the first three plots in <ref type="figure" target="#fig_3">figure 3c</ref>, we see that the backwards recursion of the Kalman smoother corrects the trajectory obtained with generation in the forward pass. However, in the fourth plot, the poor trajectory that is obtained during the forward generation step, makes smoothing unable to follow the ground truth.</p><p>The smoothing capabilities of KVAEs make it also possible to train it with up to 40% of missing data with minor losses in performance (appendix C in the supplementary material). Links to videos of the imputation results and long-term generation from the models can be found in appendix B and at sites.google.com/view/kvae.</p><p>Understanding the dynamics parameter network. In our experiments the dynamics parameter network α t = α t (a 0:t−1 ) is an LSTM network, but we could also parameterize it with any differentiable function of a 0:t−1 (see appendix D in the supplementary material for a comparison of various architectures). When using a multi-layer perceptron (MLP) that depends on the previous encoding as mixture network, i.e. α t = α t (a t−1 ), <ref type="figure" target="#fig_4">figure 4</ref> illustrates how the network chooses the mixture of learned dynamics. We see that the model has correctly learned to choose a transition that maintains a constant velocity in the center (k = 1), reverses the horizontal velocity when in proximity of the left and right wall (k = 2), the reverses the vertical velocity when close to the top and bottom (k = 3). We test the KVAE on the experiment of a dynamic torquecontrolled pendulum used in <ref type="bibr" target="#b15">[16]</ref>. Training, validation and test set are formed by 500 sequences of 15 frames of 16x16 pixels. We use a KVAE with a t ∈ R 2 , z t ∈ R 3 and K = 2, and try two different encoder-decoder architectures for the VAE, one using a MLP and one using a convolutional neural network (CNN). We compare the performaces of the KVAE to DVBFs <ref type="bibr" target="#b15">[16]</ref> and deep Markov models 4 (DMM) <ref type="bibr" target="#b18">[19]</ref>, nonlinear SSMs parameterized by deep neural networks whose intractable posterior distribution is approximated with an inference network. In table 1 we see that the KVAE outperforms both models in terms of ELBO on a test set, showing that for the task in hand it is preferable to use a model with simpler dynamics but exact posterior inference.</p><formula xml:id="formula_19">(a) k = 1 (b) k = 2 (c) k = 3 (d) Reconstruction of x</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Pendulum experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Other environments</head><p>To test how well the KVAE adapts to different environments, we trained it end-to-end on videos of (i) a ball bouncing between walls that form an irregular polygon, (ii) a ball bouncing in a box and subject to gravity, (iii) a Pong-like environment where the paddles follow the vertical position of the ball to make it stay in the frame at all times. <ref type="figure" target="#fig_6">Figure 5</ref> shows that the KVAE learns the dynamics of all three environments, and generates realistic-looking trajectories. We repeat the imputation experiments of figures 3a and 3b for these environments in the supplementary material (appendix E), where we see that KVAEs outperform alternative models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Related work</head><p>Recent progress in unsupervised learning of high dimensional sequences is found in a plethora of both deterministic and probabilistic generative models. The VAE framework is a common workhorse in the stable of probabilistic inference methods, and it is extended to the temporal setting by <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In particular, deep neural networks can parameterize the transition and emission distributions of different variants of deep state-space models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">19]</ref>. In these extensions, inference  networks define a variational approximation to the intractable posterior distribution of the latent states at each time step. For the tasks in section 5, it is preferable to use the KVAE's simpler temporal model with an exact (conditional) posterior distribution than a highly non-linear model where the posterior needs to be approximated. A different combination of VAEs and probabilistic graphical models has been explored in <ref type="bibr" target="#b14">[15]</ref>, which defines a general class of models where inference is performed with message passing algorithms that use deep neural networks to map the observations to conjugate graphical model potentials.</p><p>In classical non-linear extensions of the LGSSM like the extended Kalman filter and in the locallylinear dynamics of <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b32">33]</ref>, the transition matrices at time t have a non-linear dependence on z t−1 . The KVAE's approach is different: by introducing the latent encodings a t and making γ t depend on a 1:t−1 , the linear dependency between consecutive states of z is preserved, so that the exact smoothed posterior can be computed given a, and used to perform missing data imputation.</p><p>LGSSM with dynamic parameterization have been used for large-scale demand forecasting in <ref type="bibr" target="#b26">[27]</ref>. <ref type="bibr" target="#b19">[20]</ref> introduces recurrent switching linear dynamical systems, that combine deep learning techniques and switching Kalman filters <ref type="bibr" target="#b21">[22]</ref> to model low-dimensional time series. <ref type="bibr" target="#b10">[11]</ref> introduces a discriminative approach to estimate the low-dimensional state of a LGSSM from input images. The resulting model is reminiscent of a KVAE with no decoding step, and is therefore not suited for unsupervised learning and video generation. Recent work in the non-sequential setting has focused on disentangling basic visual concepts in an image <ref type="bibr" target="#b11">[12]</ref>.</p><p>[10] models neural activity by finding a non-linear embedding of a neural time series into a LGSSM.</p><p>Great strides have been made in the reinforcement learning community to model how environments evolve in response to action <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b31">32]</ref>. In similar spirit to this paper, <ref type="bibr" target="#b31">[32]</ref> extracts a latent representation from a PCA representation of the frames where controls can be applied. <ref type="bibr" target="#b4">[5]</ref> introduces action-conditional dynamics parameterized with LSTMs and, as for the KVAE, a computationally efficient procedure to make long term predictions without generating high dimensional images at each time step. As autoregressive models, <ref type="bibr" target="#b28">[29]</ref> develops a sequence to sequence model of video representations that uses LSTMs to define both the encoder and the decoder. <ref type="bibr" target="#b6">[7]</ref> develops an actionconditioned video prediction model of the motion of a robot arm using convolutional LSTMs that models the change in pixel values between two consecutive frames.</p><p>While the focus in this work is to define a generative model for high dimensional videos of simple physical systems, several recent works have combined physical models of the world with deep learning to learn the dynamics of objects in more complex but low-dimensional environments <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b33">34]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusion</head><p>The KVAE, a model for unsupervised learning of high-dimensional videos, was introduced in this paper. It disentangles an object's latent representation a t from a latent state z t that describes its dynamics, and can be learned end-to-end from raw video. Because the exact (conditional) smoothed posterior distribution over the states of the LGSSM can be computed, one generally sees a marked improvement in inference and missing data imputation over methods that don't have this property. A desirable property of disentangling the two latent representations is that temporal reasoning, and possibly planning, could be done in the latent space. As a proof of concept, we have been deliberate in focussing our exposition to videos of static worlds that contain a few moving objects, and leave extensions of the model to real world videos or sequences coming from an agent exploring its environment to future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: A KVAE is formed by stacking a LGSSM (dashed blue), and a VAE (dashed red). Shaded nodes denote observed variables. Solid arrows represent the generative model (with parameters θ) while dashed arrows represent the VAE inference network (with parameters φ).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Dynamics parameter network for the KVAE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>xt missing completely at random. (b) Frames xt missing in the middle of the sequence. (c) Comparison of encoded (ground truth), generated and smoothed trajectories of a KVAE in the latent space a. The black squares illustrate observed samples and the hexagons indicate the initial state. Notice that the at's lie on a manifold that can be rotated and stretched to align with the frames of the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Missing data imputation results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: A visualisation of the dynamics parameter network α (k) t (a t−1 ) for K = 3, as a function of a t−1 . The three α (k) t 's sum to one at every point in the encoded space. The greyscale backgrounds in a) to c) correspond to the intensity of the weights α (k) t , with white indicating a weight of one in the dynamics parameter network's output. Overlaid on them is the full latent encoding a. d) shows the reconstructed frames of the video as one image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Generations from the KVAE trained on different environments. The videos are shown as single images, with color intensity representing the incremental sequence index t. In the simulation that resembles Atari's Pong game, the movement of the two paddles (left and right) is also visible.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">While our main focus in this paper are videos, the same ideas could be applied more in general to any sequence of high dimensional data.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">3 We also experimented with the SRNN model from [8] as it can do smoothing. However, the model is probably too complex for the task in hand, and we could not make it learn good dynamics.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Deep Markov models were previously referred to as deep Kalman filters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Lars Kai Hansen for helpful discussions on the model design. Marco Fraccaro is supported by Microsoft Research through its PhD Scholarship Programme. We thank NVIDIA Corporation for the donation of TITAN X GPUs.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Black box variational inference for state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">M</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Buesing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07367</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recurrent environment simulators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chiappa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Racanière</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A recurrent latent variable model for sequential data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kastner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dinh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Goel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Unsupervised learning for physical interaction through video prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Finn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sequential neural models with stochastic layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fraccaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Sønderby</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Paquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Winther</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Learning visual predictive models of physics for playing billiards</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Linear dynamical neural population models through nonlinear embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">W</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Cunningham</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Backprop KF: learning discriminative deterministic state estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Haarnoja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ajay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">beta-vae: Learning basic visual concepts with a constrained variational framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Higgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Burgess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Botvinick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lerchner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01144</idno>
		<title level="m">Categorical reparameterization with gumbel-softmax</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Composing graphical models with neural networks for structured representations and fast inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Wiltschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Deep variational bayes filters: Unsupervised learning of state space models from raw data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Soelch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smagt</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Structured inference networks for nonlinear state space models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian Learning and Inference in Recurrent Switching Linear Dynamical Systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Linderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Paninski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">The concrete distribution: A continuous relaxation of discrete random variables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Switching Kalman filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Action-conditional video prediction using deep networks in atari games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Oh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">L</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06309</idno>
		<title level="m">Spatio-temporal video autoencoder with differentiable memory</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A unifying review of linear Gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="305" to="350" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Bayesian intermittent demand forecasting for large inventories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Seeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salinas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Flunkert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using LSTMs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to filter with predictive state inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Venkatraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boots</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">What&quot; and &quot;where&quot; in the human brain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Ungerleider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Haxby</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Curr. Opin. Neurobiol</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="157" to="165" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wahlström</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">B</forename><surname>Schön</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">P</forename><surname>Deisenroth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02251</idno>
		<title level="m">From pixels to torques: Policy learning with deep dynamical models</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Embed to control: A locally linear latent dynamics model for control from raw images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Watter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Boedecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Galileo: Perceiving physical object properties by integrating a physics engine with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Yildirim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
