<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Realistic Predictors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pei</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Statistical and Visual Computing Lab</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nuno</forename><surname>Vasconcelos</surname></persName>
							<email>nvasconcelos@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Statistical and Visual Computing Lab</orgName>
								<orgName type="institution">UC San Diego</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Realistic Predictors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>hardness score prediction · realistic predictors</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. A new class of predictors, denoted realistic predictors, is defined. These are predictors that, like humans, assess the difficulty of examples, reject to work on those that are deemed too hard, but guarantee good performance on the ones they operate on. In this paper, we talk about a particular case of it, realistic classifiers. The central problem in realistic classification, the design of an inductive predictor of hardness scores, is considered. It is argued that this should be a predictor independent of the classifier itself, but tuned to it, and learned without explicit supervision, so as to learn from its mistakes. A new architecture is proposed to accomplish these goals by complementing the classifier with an auxiliary hardness prediction network (HP-Net). Sharing the same inputs as classifiers, the HP-Net outputs the hardness scores to be fed to the classifier as loss weights. Alternatively, the output of classifiers is also fed to HP-Net in a new defined loss, variant of cross entropy loss. The two networks are trained jointly in an adversarial way where, as the classifier learns to improve its predictions, the HP-Net refines its hardness scores. Given the learned hardness predictor, a simple implementation of realistic classifiers is proposed by rejecting examples with large scores. Experimental results not only provide evidence in support of the effectiveness of the proposed architecture and the learned hardness predictor, but also show that the realistic classifier always improves performance on the examples that it accepts to classify, performing better on these examples than an equivalent nonrealistic classifier. All of these make it possible for realistic classifiers to guarantee a good performance.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recent years have produced significant advances in computer vision, due to the introduction of deep convolutional neural networks. Like most other machine learning and computer vision models, they are trained to perform as well as possible on every example. In result, these models have no awareness of what they can and cannot do. This is unlike people, who have a sense of their limitations. Most humans can do certain things and do them well but, beyond these, will say 'sorry, I don't know how to do that'. Then they work on what they can do and gradually overcome their limitations. One could say that humans are realistic predictors, who would rather refuse tasks that are too hard than almost surely fail. This is unlike most classifiers, who are optimistic and attempt to classify all examples, no matter how hard. This can be a problem for applications where incorrect decisions can have very negative consequences. For example, a significant problem for smart cars is that their vision systems offer no performance guarantees. For these applications, the vision system should guarantee that the error rate will not exceed some specifications, based on the scene, weather conditions, etc. Even more importantly, it should have a reject option, refusing to operate on instances that are too hard, preferring to bring the vehicle to a stop than risk accidents. Another beneficial example is that this new type of predictors could make use of computer use and human skilled labor effectively. In supervised learning, although many automatic annotation methods <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b22">23]</ref> have been proposed, no performance guarantee of their results leads to the fact that humans still need to annotate all collected billions of data in practice, like by Amazon Turk. Instead of annotating all data manually, it is undoubtedly efficient to let realistic models handle on easy examples so as to guarantee accuracy comparable to humans, and just leave the hard ones aside for human experts.</p><p>A pre-requisite of realistic classifiers is the ability to self-assess, i.e. predict the likelihood of success or failure. This is, however, not easy in the current classification settings. One possibility is to design classifiers with a reject option. For example, classifier cascades are composed of stages that implement a series of reject decisions, efficiently zooming in on image region containing the object to detect <ref type="bibr" target="#b27">[28]</ref>. Neural network routing <ref type="bibr" target="#b19">[20]</ref>, where samples are processed by different network paths, according to their difficulty, is a neural variant of this idea. While increasing computational efficiency, these methods frequently degrade classification performance. They produce a classifier that is faster but usually less accurate than one without rejection options. Many procedures have also been proposed to account for example hardness during training. For instance, curriculum learning <ref type="bibr" target="#b3">[4]</ref> suggests using easy samples first and hard samples latter. On the other hand, hard example mining <ref type="bibr" target="#b23">[24]</ref> techniques seek examples on which a classifier does poorly, to improve its performance. The goal of these methods is not to produce a hardness predictor , which can be applied to examples unseen during training, but to improve classifier performance or enable faster optimization convergence. Instead, realistic predictions require inductive hardness predictors, capable of operating beyond the training set. This is, in general, a non-trivial pursuit. The main challenge is that there is no ground truth to train such a predictor. Even when human supervision is available, the ranking of samples is onerous and the identification of easy and hard samples is difficult. This is partly because what is intuitively hard for humans is not guaranteed to be hard for algorithms, and vice versa. <ref type="figure" target="#fig_0">Fig. 1</ref> shows an example of easy and hard assessments produced by a classifier trained with different approaches. On a dataset of simple images, like MNIST, the hardness predictions are understandable to a human. One can say that easy samples are clearly written, close to prototypical digits, while hard samples are "in between" digits, e.g. "a 6 that looks like a 0," poorly executed digits, e.g. "a 6 that looks like an i" or "an open 0," etc. On the other hand, when the images are complex, as in the MIT67 scene dataset, it is too difficult to understand why the classifiers finds the displayed examples easy or hard. In fact, the problem is not even well defined in general, since different classifiers can have different ground-truth for easy and difficult. This is certainly the case for humans, whose difficulty assessments tend to be personal and vary over time, e.g. with experience. Hence, it appears that hardness predictors should be learned in an unsupervised manner, and personalized, i.e., classifier specific. On the other hand, it does not appear that they can be self-referential, i.e. the hardness predictions cannot be produced by the classifier itself. If this were possible, the classifiers could simply implement a reject option. However, experience with hard example mining suggests that this is not very reliable. While useful for gathering difficult examples, it can produce a significant percentage of examples that are not difficult. Given all this, it appears that, for realistic prediction, the classifier should rely on an independent hardness predictor. However, this predictor should be trained without explicit supervision, tuned to the classifier, and learn from its mistakes. Motivated by this, we propose to implement the hardness predictor as an auxiliary network, which we call the auxiliary hardness prediction network (HPNet). Its input is the example to be processed by the classifier and its output a hardness score. To learn from the classifiers mistakes, the HP-Net is trained jointly with it. The two networks are trained in an adversarial setting, that resembles that of generative adversarial networks (GANs). While the proposed architecture is not a GAN, the two networks are trained alternately. During classifier training, the hardness scores produced by the HP-Net are used as loss weights, assigning more weight to harder examples. This encourages the classifier to classify all examples as best as possible. During HP-Net training, the classifier softmax probabilities are used to tune the HP-Net, using a variant of the cross entropy loss function that elicits adversarial behavior. In this way, as the classifier learns to improve its predictions, the HP-Net refines its hardness scores. At test time, the HP-Net assigns a hardness score to each example. If this is above a threshold, the example is rejected. In this way, the classifier is never asked to produce class scores for examples that are deemed too hard. This is what we call realism. Overall, the proposed architecture has three interesting properties. First, while highly tuned to the classifier, the HP-Net is an inductive model that can be applied to unseen samples. Second, training requires no HP-Net supervision, priors, or hand-crafted rules regarding the nature of hard examples. Experiments show that the harness scores are accurate enough to enable realistic prediction, without compromise of classification accuracy. These properties are demonstrated by extensive evaluation on three datasets, which also provides interesting insights on the make-up of a hardness predictor. For example, they show that the HP-Net should be tuned to the classifier, with best results when the two networks have the same architecture. On the other hand, performance degrades substantially when the two networks have shared layers, showing that they are solving fundamentally different tasks. This is strong evidence against self-referential solutions. Finally, it is shown that classifier performance always increases upon restriction to easier examples. This enables the classifier to meet a specified error rate by simple control of the rejection threshold.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>Several criteria have been proposed to assess sample hardness. One possibility is to use task-specific criteria that leverage prior human knowledge <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b26">27]</ref>. For example, Ionescu et al. <ref type="bibr" target="#b26">[27]</ref> define image difficulty as human response time for solving a visual search task. Another popular approach is the use of loss values <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b17">18]</ref>, confidence scores <ref type="bibr" target="#b28">[29]</ref>, or the magnitude of the loss gradient <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b32">33]</ref>. These criteria are mostly used to increase the speed of optimization procedures such as stochastic gradient descent. They are sensible for hardness prediction, since small losses tend to correspond to easy samples and vice versa. On the other hand, two samples of equal loss can be classified correctly and incorrectly. For example, adversarial examples of high confidence score are not necessarily easy to classify <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b10">11]</ref>. To address this, Chang et al. <ref type="bibr" target="#b4">[5]</ref> emphasize sample uncertainty when differentiating easy and hard examples. All these methods rely on handcrafted criteria for selecting and ranking examples. Similar networks with ours are proposed by <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31]</ref> to learn the significant samples for deep reinforcement learning and noisy labeled data, but the classifiers depending on unstable hyper-parameters are not realistic ones. The proposed approach is more closely related to the method of McGill et al. <ref type="bibr" target="#b19">[20]</ref>, who add a 2-way junction to neural network layers to dynamically route easy samples for direct classification and hard samples to the next layer. Nevertheless, all these methods are self-referencial, in the sense that a classifier is used to assess the hardness of the samples that it classifies. This is not easy, since hard samples are, by definition, those that the classifier makes mistakes on. We propose, instead, the use of an auxiliary predictor for this task, which learns from the classifier's mistakes. Realistic prediction is closely related to the literature on failure prediction, where the goal is to build systems that can reliably predict the failures of a predictor. Jammalamadaka et al. <ref type="bibr" target="#b13">[14]</ref> introduce evaluator algorithms to predict failures of human pose estimators, from features specific to this problem. Bansal et al. <ref type="bibr" target="#b1">[2]</ref> characterize and group misclassified images using pre-selected attributes, with clustering algorithms that learn a semantic characterization of failure modes. Zhang et al. <ref type="bibr" target="#b31">[32]</ref> reject probable failure samples with a binary SVM that predicts errors using 14 pre-defined kernels. Daftry et al. <ref type="bibr" target="#b5">[6]</ref> define failure degree as the fraction of trajectories correctly predicted by an UAV and train a linear SVM to estimate it from the feature responses of a deep network trained for autonomous navigation. These methods rely on post-hoc analysis of the predictor performance, simply learning a regressor or classifier from its mistakes. Realistic prediction aims to go beyond this, by integrating the learning of hardness predictor and classifier, so as to guarantee optimal classifier performance on non-rejected examples. To the best of our knowledge, the proposed architecture is the first implementation of this idea. Our experiments also show that the features needed for classification are fundamentally different from those needed for difficulty prediction. This suggests that simply reading feature responses from the stages of a deep predictor <ref type="bibr" target="#b5">[6]</ref> is sub-optimal even for failure prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Realistic Predictor Architecture</head><p>In this section, we introduce the proposed realistic predictor architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Architecture</head><p>While realistic prediction is of interest for many computer vision tasks, in this work we focus on image classification into one of C classes. The operation of a realistic predictor is illustrated in <ref type="figure">Figure 3</ref>. Consider a classifier F(x) faced with examples x i from a universal example set U . The classifier is denoted realistic if it rejects a subset of examples H ⊂ U that it deems too hard so as to guarantee a certain performance on a subset of examples A = U − H that it agrees to classify. Example rejection is determined by thresholding a hardness score, which is assigned to each example x by an auxiliary hardness predictor S(x), denoted the HP-Net. Note that, at inference time, S(x) predicts the hardness of unseen test examples. It must, therefore, be an inductive predictor, e.g. it does not suffice to assign weights to examples during training.</p><p>In the failure prediction literature, the classifier F is first learned from a training set</p><formula xml:id="formula_0">D = {(x i , y i )} N i=1</formula><p>, where D ⊂ U , y i is the ground truth label of image x i , and N the number of training samples. Upon training, a failure predictor is then learned from its performance on the training set, i.e. from the set {x i , y i ,ŷ i }, whereŷ i is the class prediction for sample x i . While this failure predictor could be used to implement the HP-Net of realistic prediction, this would fail to guarantee that F has optimal performance on the set A of accepted examples. One simple solution would be to use the failure predictor to reject training examples and then fine-tune F on those remaining. This, however, would make the failure predictor sub-optimal for the fine-tuned F. To prevent these problems, we propose to learn F and S jointly, as illustrated in <ref type="figure">Figure 2</ref>.</p><p>The classifier F can be any convolutional neural network (CNN), usually containing a number of convolutional layers followed by fully connected layers. Its final layer implements a softmax function with C outputs, outputting a probability distribution p i = F(x i ) in response to sample x i . The HP-Net has a similar structure. For notational convenience, we divide it into a set of convolution layers, the network trunk, and a set of fully connected layers, the network head. The network trunk is used for feature extraction while the head implements a multi-layer fully connected network with a single output node. This is implemented with a sigmoid unit and produces the predicted hardness score ′ . In all cases, x rejected if S(x) &gt; T , for some threshold T .</p><formula xml:id="formula_1">s i = S(x i ), s i ∈ [0, 1],</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Adversarial cross entropy loss function</head><p>The joint training of the classifier and HP-Net requires a loss function that induces the desired complimentary functions in the two networks. As is common in the literature, the classifier is trained by cross-entropy minimization. Denoting the one-hot code of ground truth label y i by y i , the loss of sample {x i , y i } is l(p i , y i ) = −y treats each sample equally. As is common in the cost-sensitive learning literature, we replace this with</p><formula xml:id="formula_2">L m (D) = − N i=1 s i log p c i .<label>(2)</label></formula><p>where s i ∈ [0, 1] is the hardness score of example x i , produced by the HP-Net. This makes harder examples (larger s i ) more important, while easier examples (lower s i ) are given less importance. In this way, the classifier is encouraged to learn from as many hard examples as possible and only reject examples that require an unreasonable amount of effort or expertise. This aims to reflect the behavior of a motivated human, who will attempt to learn as much as realistically possible about a problem and reject tasks that exceeds his or her expertise.</p><p>To encourage the HP-Net to produce scores s i proportional to the difficulty of the corresponding samples, the HP-Net is trained with the loss function</p><formula xml:id="formula_3">L a (D) = − N i=1 {p c i log(1 − s i ) + (1 − p c i ) log s i },<label>(3)</label></formula><p>where s i , p </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training strategy</head><p>Our attempts to optimize the complete architecture of <ref type="figure">Figure 2</ref>, using the combined loss function L(D) = L m (D) + L a (D), produced mixed results. We have experienced difficulties to guarantee convergence of the learning procedure. It is not totally clear why at this point, we leave this for future research. Instead, we found it much easier to optimize the classifier and the HP-Net alternately. Specifically, the HP-Net is first frozen and the classifier updated. The classifier is then frozen and the HP-Net updated. The process is iterated until convergence. Note that the consistency of convergence of this process is quite intuitive. Given the classifier, the optimization of the HP-Net encourages predictions s i = 1 − p c i . Given these scores, the classifier then emphasizes the samples on which it did poorly, i.e. produced a low p This is similar to boosting algorithms <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21]</ref>, but has one fundamental difference. While, in boosting, the classifier reweights the examples by how well it performs on them, the proposed architecture uses an alternate predictor. Similarly, the procedure has some similarities to generative adversarial networks (GANs) <ref type="bibr" target="#b9">[10]</ref> in the sense that there is an adversarial relationship between the classifier and hardness predictor. When the classifier produces bad predictions, the HP-Net generates an adversarial signal that encourages it to produce better predictions. Hence, the HP-Net can be seen as a signal generator that attempts to "confuse" the classifier into thinking that all samples are easy. This is similar to the GAN generator, which attempts to confuse the discriminator, rendering it unable to distinguish real from fake examples. Under this interpretation, the proposed architecture can be seen as an unsupervised generator of hardness scores. However, it is not a GAN. It is also not clear that formulating it as a GAN would add more clarity to its convergence, given the well known convergence issues surrounding GANs <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets and pre-processing</head><p>MNIST is a heavily benchmarked dataset. Although it is a relatively simple dataset, it is usefully to derive insights on network operation. We used 100 epochs, with batch size of 256, to train the network on this dataset. MIT67 dataset <ref type="bibr" target="#b21">[22]</ref> was proposed for indoor scene recognition. It contains 67 indoor categories and a total of 15,620 images. We follow the experimental setting of <ref type="bibr" target="#b21">[22]</ref>, where 5,360 images are used for training and 1,340 for testing. On this dataset, we fine-tune a pre-trained network, trained on ImageNet. The number of epochs is set to 50. Batch sizes 32 and 64 are used for VGG and ResNet. ImageNet LSVRC 2012 <ref type="bibr" target="#b6">[7]</ref> contains 1,000 classes with 1.2 million training images, 50,000 validation images, and 100,000 test images. Our evaluation is conducted on the validation set. On these two datasets, we adopt the same data augmentation and pre-processing of the previous studies <ref type="bibr" target="#b15">[16]</ref>. Each RGB image pixel is scaled to [0, 1] with mean value subtracted and standard variance divided. Then scale and aspect ratio augmentation are applied to the processed images.</p><p>The 224 × 224 crop is sampled from augmented images or random horizontal flips. Since, on this dataset, we used a pre-trained network, only 5 epochs are used. Again, batch sizes of 32, 64 batch are used for VGG, ResNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Setup</head><p>To study the impact of various network configurations, we consider several strategies for combining networks: simple classifier with complex HP-Net, complex classifier with simple HP-Net, simple classifier with simple HP-Net and complex classifier with complex HP-Net. On MNIST, LeNet5 is used as simple network and kerasNet, a network proposed by keras 1 as the complex one. On MIT67 and ImageNet, VGG16 and ResNet50 are used as simple and complex networks respectively. Additionally, we study the the setting where classifier and HP-Net have the same structure and shared convolutional layer weights. The networks are trained using SGD with a momentum of 0.9. On MNIST, the initial learning rate is set to 0.1, 1e-3 for the classifier and HP-Net, respectively. On the other datasets, it is set to 1e-3, 1e-4 for two networks, for all of the models discussed herein, respectively. The learning rate is reduced by 5% after each epoch on MNIST, and is divided by 10 after every 10 epochs on MIT67, and 1 epoch on ImageNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Learning to predict hardness scores</head><p>We start by presenting some results that provide some intuition on the joint learning of classifier and hardness predictor. <ref type="figure" target="#fig_6">Figure 6</ref> shows 1) the evolution of distribution of scores produced by the HP-Net, and 2) the test set accuracy of the classifier as a function of training iteration, on MNIST and MIT67. These results were produced with the kerasNet-kerasNet network on MNIST and VGG16-VGG16 on MIT67. On MIT67, we only show the first 20 epochs because there is little change after that. Note that, as classification accuracy increases, the bulk of the mass of the hardness score distribution moves from right to left. This shows that the predicted scores decrease gradually as training progresses. As the classifier updates its predictions, the HP-Net refines its hardness scores to reflect this improvement. This, in turn, encourages the classifier to focus on the harder examples, as in hard example mining. Over training iterations, the hardness predictor learns that samples initially considered hard are not hard after all. This enables it to make good predictions even for unseen examples. The process resembles human learning, which focuses on gradually harder examples that are eventually mastered and found easy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image recognition without rejection</head><p>We next consider image recognition results. LeNet5 and kerasNet are used as baseline on MNIST, VGG16 and ResNet50 on MIT67 and ImageNet. All baseline results are based on our experiments, and could differ slightly from results published by their authors. <ref type="table" target="#tab_1">Table 1</ref> summarizes the classification results, enabling a number of conclusions. First, the addition of the HP-Net can produce a slight performance decrease of the classifier on the entire dataset. In fact, this happened for all mixed models (different architectures for HP-Net and classifier) and when the networks are the same and share weights. Note that these numbers are for classification on the entire dataset. They do not imply that the classifier does not have improved performance on the examples that are accepted by the the HP-Net. This will be analyzed below. However, and somewhat surprisingly, when the HP-Net is based on the same model as the classifier, the performance of the latter on the entire dataset improves by some amount. This is likely due to the hard example mining aspect of the procedure. The re-weighting of hard examples with large weights allows the classifier to improve on these. Although the goal of realistic prediction is not to improve image classification performance on all samples, it is interesting to see that the classifier outperforms the baselines. Second, on all datasets, best performances occur when the classifier and the HP-Net have the same architecture. Combinations with simpler and more complex HP-Nets than the classifier have weaker performance. This is evidence that the hardness predictor has to be tuned to the classifier. Third, when this holds, different models can lead to variations of performance. On MNIST, there is no obvious difference between LeNet5 and the more complex kerasNet. This is probably because baseline performance is already saturated. On the other hand, on MIT67, ResNet50-ResNet50 outperforms VGG16-VGG16 by 3.5%. For the larger scale ImageNet, the increase in accuracy is 3.1% when the ResNet50 is adopted. Finally, when the convolutional layers are shared, all classifiers have slightly weaker performances on all datasets. This is interesting, given the best performance of identical models. While the two networks must be identical, sharing weights leads to a significant performance decrease. This shows that the networks are solving fundamentally different tasks, and argues against selfreferential solutions based on a single network, such as boosting.</p><p>Overall, this section shows that realistic prediction does not have to sacrifice recognition performance even when no examples are rejected. This, however, requires careful selection of classifier and HP-Net architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Hardness score predictions on test set</head><p>We next analyze the hardness scores produced by the various models. <ref type="figure" target="#fig_7">Fig. 7</ref> presents the test set distribution of the scores learned by various network combinations. The mean and variance of each distribution are shown below it. The distributions produced by the different networks are consistent with the classification performances in <ref type="table" target="#tab_1">Table 1</ref>. Plots a)-e), relative to configurations with equal models, assign small scores (less than 0.5) to most test examples. On the other hand, the ResNet50-VGG16 configuration produces a more uniform distribution, of larger mean value. Its lower classification performance has been learned by the hardness predictor, which assigns a larger score to many examples. Note also that, for the two network combinations tested on ImageNet (plots b) and c)), the ResNet50-ResNet50 configuration produces a distribution sharper than that of the VGG16-VGG16 and more concentrated on the neighborhood of 0. This shows that the ResNet50 hardness predictor is more confident on the outcome of the classification of the test samples. The hardness predictor, meanwhile, has learned that ResNet is a better model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6">Realistic predictors</head><p>We finish with an evaluation of realistic predictions, based on three classifiers. The first, denoted C, uses a standard (non-realistic) predictor. The second, denoted F, is the realistic predictor produced by the training procedure, without fine-tuning to accepted examples. These two classifiers are trained on the entire training set. Finally, the third, denoted F ′ , is obtained by fine tuning F on the training examples accepted by the hardness predictor S. Two strategies are also compared for the rejection of examples. In both cases, a threshold T is found such that p% of the training examples are rejected. The first strategy is the selfreferential strategy of rejecting examples based on the classifier confidence level.   <ref type="table" target="#tab_2">Table 2</ref> compares the performances of all classifiers and rejection strategies, as a function of the rejection percentage p. A few observations can be made. First, the performance of the realistic predictors is always superior to that of the standard classifier C. As is the case for humans, by refusing to classify hard examples realistic predictors have better performance on those they classify. Second, the rejection by S(x) &gt; T outperforms standard rejection (max c p c i &lt; T ) almost in all settings. The gains can be quite significant, especially as p decreases, e.g. 2.3 points of top 1 performance for p = 5%. This shows that it is important to learn the hardness predictor jointly with the classifier and that the commonly used self-referential confidence scores are not enough to guarantee good hardness predictions. Finally, when the hardness predictions are based on the HP-Net, there is little difference between the top 1 accuracy of the realistic predictor F trained on the whole training set and that (F ′ ) fine-tunned on accepted examples only. This shows that F is truly a realistic predictor, capable of close to optimal performance on the accepted examples without any finetuning. <ref type="figure" target="#fig_8">Figure 8</ref> illustrates more results and additionally compares the performance of the realistic predictor F ′ to the standard C over different network configurations (VGG16-VGG16 and ResNet50-ResNet50) on ImageNet. The realistic predictor F ′ implemented with the weaker VGG model approaches the performance of the original classifier implemented with the stronger ResNet model. This shows that, even though the VGG cannot learn everything that the ResNet can (i.e. it is not as smart as the ResNet), it can guarantee the same performance by rejecting some examples. In this case, the VGG passes the 2% rejection performance of the ResNet by rejecting around 10% test examples and the performance of the ResNet on the full test set by rejecting 5% test examples. On the other hand, in order to guarantee a target performance, the realistic predictor can accept and classify more examples than standard non-realistic predictor. For instance, to a target accuracy 93.2%, the ResNet F ′ only need to reject less than 2% samples, but for C, it has to reject about 5% samples. In summary, while better models always have better performance, a realistic predictor can provide performance guarantees "above its pay-grade" by refusing to classify examples where it is likely to fail. This applies even to the best models. On ImageNet the superior ResNet is able to improve its performance from 93% to 97% by rejecting about 10% of the examples. While part of this is due to the fact that the examples are indeed easier, the gain is much larger than for the original predictor, which only increases its performance to 95%. The ability to predict which examples are hard, through the hardness predictor, and adapt to them enables this gain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this work, we have proposed a new class of classifiers, denoted realistic classifiers. These are classifiers that, like humans, assess the difficulty of examples, reject to classify those that are deemed too hard, but guarantee good performance on the ones they classify. The central problem in realistic classification, the design of an inductive predictor of hardness scores, has been then considered. It was argued that this should be a predictor independent of the classifier itself, but tuned to it, and jointly learned, so as to learn from its mistakes. A new architecture has been proposed to accomplished these goals by complementing the classifier with an auxiliary prediction network (HP-Net). The two networks are trained in an adversarial setting, that resembles that of generative adversarial networks (GANs). Experimental results have provided evidence in support of this architecture. While best results were achieved when the HP-Net has the identical architecture to the classifier, sharing weights between the two considerably degraded classification performance. This shows that, while the hardness predictor must be tuned to the classifier, the two solve fundamentally different tasks. Extensive classification experiments have also shown that the realistic classifier always improves performance on the examples that it accepts to classify, performing better on these examples than an equivalent nonrealistic classifier.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Top 3 easiest and hardest examples on MNIST and MIT67 dataset according to different criteria. Ground truth labels are shown below each image. "Confidence score based" equates harder to smaller confidence scores. "HP-Net" refers to the ranking by the scores of the proposed hardness predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .Fig. 3 .</head><label>23</label><figDesc>Fig. 2. Proposed architecture. F is the classifier, HP-Net the hardness predictor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .Fig. 5 .</head><label>45</label><figDesc>Fig. 4. HP-Net loss surface.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>. This is the binary cross entropy loss but with reversed semantics. It measures the cross-entropy between the distributionss i , s i ). Its minimization is equivalent to minimizing the Kullback-Leibler divergence between the two distributions and has a minimum when s i = 1 − p c i . This encourages large scores for poorly classified samples (low p c i ) and small scores for well classified samples. It can, thus, be seen as an adversarial loss that measures sample hardness, while sharing the appealing properties of the cross-entropy. These can be seen in Fig. 4, which shows a surface plot of the argument of the summation in (3). Note that this is always positive and has global minimum at the configurations s i = 1, p c i = 0 and s i = 0, p c i = 1. Hence, it encourages binary hardness scores. It is also smooth, penalizing heavily the configurations inconsistent with a hardness score (s i</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, the example acquires a smaller weight s i and is ignored by the learning algorithm. Hence, the algorithm "puts away" the well classified examples and focuses on the poorly classified ones.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For notational convenience, we use 'A-B(-s)' to represent that A is used as classifier and B as HP-Net. If the '-s' added, A and B have shared weights. For the HP-Net, we also varied the structure of the network head. Three basic structures, shown in Fig. 5, are used: 'flatten layer', 'fc7' and 'fc1000' represent the flatten layer, fc7 layer and fc1000 layer of kerasNet, VGG16 and ResNet50 respectively. 'FC, [M1,M2]' represents a fully connected layer of M1 inputs and M2 outputs, 'BN' a batch normalization layer, and 'ReLU' a layer of rectified linear units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Evolution of classification accuracy and distribution of hardness scores during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. The distribution of predicted hardness scores on different settings. Sub-figure (a) is the results on MNIST; (b), (c) are on the ImageNet; The last three are on the MIT67.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Comparison between realistic predictors and standard predictors on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>for image x i . The overall operation of the realistic predictor is summarized as follows. At training time, 1. train classifier F and HP-Net S jointly on training set D. 2. run S on D and eliminate hard examples, to create realistic training set D ′ . 3. learn realistic classifier F ′ on D ′ , with S fixed. 4. output pair S, F ′ . At test time, run test example x by S, reject hard examples, classify remaining with F</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Image recognition accuracy comparison among all model combinations</figDesc><table>Classifier 
HP-Net shared MNIST MIT67 
ImageNet 
weights 
top 1 top 5 
LeNet5 
99.0% 
-
-
-
kerasNet 
99.0% 
-
-
-
LeNet5 
kerasNet 
98.4% 
-
-
-
kerasNet 
LeNet5 
98.2% 
-
-
-
LeNet5 
LeNet5 
99.1% 
-
-
-
kerasNet 
kerasNet 
s 
97.9% 
-
-
-
kerasNet 
kerasNet 
99.2% 
-
-
-
AlexNet [34] 
-
56.8% 
-
-
CaffeNet [35] 
-
56.8% 
-
-
GoogleNet [34] 
-
59.5% 
-
-
VGG16 
ResNet50 
-
67.9% 65.6% 87.3% 
ResNet50 
VGG16 
-
72.7% 70.4% 90.0% 
VGG16 
-
72.2% 71.6% 90.3% 
VGG16 
VGG16 
s 
-
67.6% 70.9% 89.9% 
VGG16 
VGG16 
-
72.3% 73.3% 91.2% 
ResNet50 
-
75.6% 76.1% 92.8% 
ResNet50 
ResNet50 
s 
-
73.2% 75.9% 92.7% 
ResNet50 
ResNet50 
-
75.8% 76.4% 93.0% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performances of different methods when removing some hard samples MIT67 (top 1 accuracy, mean(variance); VGG16-VGG16 architecture)</figDesc><table>classifier rejection 
0% 
5% 
10% 
15% 
20% 
25% 
C 
maxcp 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/keras-team/keras/blob/master/examples/cifar10_cnn. py.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Variance reduction in sgd by distributed importance sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Towards transparent systems: Semantic characterization of failure modes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="366" to="381" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Teaching classification boundaries to humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Basu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Christensen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI Conference on Artificial Intelligence</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Louradour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<title level="m">Curriculum learning. In: International Conference on Machine Learning</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Active bias: Training more accurate neural networks by emphasizing high variance samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1003" to="1013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Introspective perception: Learning to predict failures in vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Daftry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Intelligent Robots and Systems</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1743" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A largescale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Experiments with a new boosting algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">96</biblScope>
			<biblScope unit="page" from="148" to="156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Active sampler: Light-weight accelerator for complex data analytics at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jagadish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Ooi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Explaining and harnessing adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>CoRR abs/1412.6572</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adaptive sampling for sgd by exploiting side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gopal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="364" to="372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">To recognize shapes, first learn to generate images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Progress in Brain Research</title>
		<imprint>
			<biblScope unit="volume">165</biblScope>
			<biblScope unit="page" from="535" to="547" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Has my algorithm succeeded? an evaluator for human pose estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jammalamadaka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="114" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00904</idno>
		<title level="m">Screenernet: Learning curriculum for neural networks</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1311.6510</idno>
		<title level="m">Are all training examples equally valuable? arXiv preprint</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Focal loss for dense object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Online batch selection for faster training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Loshchilov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hutter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deciding how to decide: Dynamic routing in artificial neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mcgill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2363" to="2372" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Moghimi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Saberian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<title level="m">Boosted convolutional neural networks. In: British Machine Vision Conference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recognizing indoor scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Quattoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="413" to="420" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning to read chest x-rays: Recurrent neural cascade model for automated image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">C</forename><surname>Shin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Roberts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Demner-Fushman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Summers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Training region-based object detectors with online hard example mining</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="761" to="769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Baby steps: How less is more in unsupervised dependency parsing. NIPS: Grammar Induction, Representation of Language and Language Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Spitkovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Alshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jurafsky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">How hard can it be? estimating the difficulty of visual search in an image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tudor</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leordeanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Papadopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2157" to="2166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rapid object detection using a boosted cascade of simple features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<editor>I-I</editor>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Idk cascades: Fast deep learning by learning not to overthink</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Crankshaw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tumanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">E</forename><surname>Gonzalez</surname></persName>
		</author>
		<idno>arX- iv:1706.00885</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Tagging like humans: Diverse and distinct image annotation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lyu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning from massive noisy labeled data for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2691" to="2699" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Predicting failures of vision systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page">cvpr</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Stochastic optimization with importance sampling for regularized loss minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Places: A 10 million image database for scene recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
