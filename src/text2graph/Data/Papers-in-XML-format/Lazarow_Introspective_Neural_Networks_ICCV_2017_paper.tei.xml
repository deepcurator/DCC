<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Introspective Neural Networks for Generative Modeling</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Lazarow</surname></persName>
							<email>jlazarow@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of CSE</orgName>
								<orgName type="department" key="dep2">Dept. of CSE, UCSD</orgName>
								<orgName type="department" key="dep3">Dept. of CogSci, UCSD</orgName>
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Long</forename><surname>Jin</surname></persName>
							<email>lojin@ucsd.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of CSE</orgName>
								<orgName type="department" key="dep2">Dept. of CSE, UCSD</orgName>
								<orgName type="department" key="dep3">Dept. of CogSci, UCSD</orgName>
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">Dept. of CSE</orgName>
								<orgName type="department" key="dep2">Dept. of CSE, UCSD</orgName>
								<orgName type="department" key="dep3">Dept. of CogSci, UCSD</orgName>
								<orgName type="institution">UCSD</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Introspective Neural Networks for Generative Modeling</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We study unsupervised learning by developing a generative model built from progressively learned deep convolutional neural networks. The resulting generator is additionally a discriminator, capable of "introspection" in a sense -being able to self-evaluate the difference between its generated samples and the given training data. Through repeated discriminative learning, desirable properties of modern discriminative classifiers are directly inherited by the generator. Specifically, our model learns a sequence of CNN classifiers using a synthesis-by-classification algorithm. In the experiments, we observe encouraging results on a number of applications including texture modeling, artistic style transferring, face modeling, and unsupervised feature learning. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Supervised learning techniques have made a substantial impact on tasks that can be formulated as a classification/regression problem <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b26">27]</ref>. Unsupervised learning, where no task-specific labeling/feedback is provided on top of the input data, still remains one of the most difficult problems in machine learning but holds a bright future since a large number of tasks have little to no supervision.</p><p>Popular unsupervised learning methods include mixture models <ref type="bibr" target="#b8">[9]</ref>, principal component analysis (PCA) <ref type="bibr" target="#b23">[24]</ref>, spectral clustering <ref type="bibr" target="#b37">[38]</ref>, topic modeling <ref type="bibr" target="#b3">[4]</ref>, and autoencoders <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref>. In a nutshell, unsupervised learning techniques are mostly guided by the minimum description length principle (MDL) <ref type="bibr" target="#b34">[35]</ref> to best reconstruct the data whereas supervised learning methods are primarily driven by minimizing error metrics to best fit the input labeling. Unsupervised learning models are often generative and supervised classifiers are often discriminative; generative model learning has been traditionally considered to be a much harder task than discriminative learning <ref type="bibr" target="#b11">[12]</ref> due to its intrinsic learning complexity, as well as many assumptions and simplifications made about the underlying models.  Generative and discriminative models have traditionally been considered distinct and complementary to each other. In the past, connections have been built to combine the two families <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b21">22]</ref>. In the presence of supervised information with a large amount of data, a discriminative classifier <ref type="bibr" target="#b25">[26]</ref> exhibits superior capability in making robust classification by learning rich and informative representations; unsupervised generative models do not require supervision but at a price of relying on assumptions that are often too ideal in dealing with problems of real-world complexity. Attempts have previously been made to learn generative models directly using discriminative classifiers for density estimation <ref type="bibr" target="#b44">[45]</ref> and image modeling <ref type="bibr" target="#b39">[40]</ref>. There is also a wave of recent development in generative adversarial networks (GAN) <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b0">1]</ref> in which a discriminator helps a generator try not to be fooled by "fake" samples. We will discuss in detail the relations and connections of our model with these existing literature in the later sections.</p><p>In <ref type="bibr" target="#b44">[45]</ref>, a self supervised boosting algorithm was proposed to train a boosting algorithm by sequentially adding features as weak classifiers on additionally self-generated negative samples. Furthermore, the generative discrimi-native modeling work (GDL) in <ref type="bibr" target="#b39">[40]</ref> generalizes the concept that a generative model can be successfully modeled by learning a sequence of discriminative classifiers via selfgenerated pseudo-negatives.</p><p>Inspired by the prior work on generative modeling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b44">45,</ref><ref type="bibr" target="#b39">40]</ref> and development of convolutional neural networks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b12">13]</ref>, we develop an image modeling algorithm, introspective neural networks for generative modeling (INNg) that can be used simultaneously as a generator and a discriminator, consisting of two critical stages during training: (1) pseudo-negative sampling (synthesis) -a generation of samples considered to be positive examples and (2) a CNN classifier learning stage (classification) for self-evaluation and model updating from the previous synthesis. There are a number of interesting properties about INNg worth highlighting:</p><p>• CNN classifier as generator: No special conditions on the CNN architecture are needed in INNg and existing CNN classifiers can be directly made into generators, if trained properly.</p><p>• End-to-end self-evaluation and learning: Perform end-toend "introspective learning" to self-classify between synthesized samples (pseudo-negatives) and the training data, to approach the target distribution.</p><p>• All backpropagation: Our synthesis-by-classification algorithm performs efficient training using backpropagation in both stages: the sampling stage for the input images and the classification training stage for the CNN parameters.</p><p>• Model-based anysize-image-generation: Since we model the input image, we can train on images of a given size and generate an image of a larger size while maintaining coherence of the entire image.</p><p>• Agnostic to various vision applications: Due to its intrinsic modeling power being at the same time generative and discriminative, INNg can be adopted to many applications in computer vision. In addition to the applications shown here, extension of the objective (loss) function within INNg is expected to work for other tasks such as "image-to-image translation" <ref type="bibr" target="#b20">[21]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Significance and related work</head><p>Our introspective neural networks generative modeling (INNg) algorithm has connections to many existing approaches including the MinMax entropy work for texture modeling <ref type="bibr" target="#b50">[51]</ref>, and the self-supervised boosting algorithm <ref type="bibr" target="#b44">[45]</ref>. It builds on top of convolutional neural networks <ref type="bibr" target="#b26">[27]</ref> and we are particularly inspired by two lines of prior algorithms: the generative modeling via discriminative approach method (GDL) <ref type="bibr" target="#b39">[40]</ref>, and the DeepDream code <ref type="bibr" target="#b30">[31]</ref> and the neural artistic style work <ref type="bibr" target="#b12">[13]</ref>. Parallels can be drawn to ideas elaborated in <ref type="bibr" target="#b15">[16]</ref> where parameters of a distribution are learned using a (single) classifier between noise and training data. Additionally, the use of "negative" examples to bridge the gap between an unsupervised task into a supervised one is also seen in <ref type="bibr" target="#b17">[18]</ref>, although this focuses on the training of the weights of the network for classification rather than synthesis. The work of <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b12">13]</ref>, along with the Hybrid Monte Carlo literature <ref type="bibr" target="#b43">[44]</ref>, motivates us to significantly improve the time-consuming sampling process in <ref type="bibr" target="#b39">[40]</ref> by an efficient stochastic gradient descent (SGD) process via backpropagation (the reason for us to say "all backpropagation"). Next, we review some existing generative image modeling work, followed by detailed discussions about GDL <ref type="bibr" target="#b39">[40]</ref>; comparisons to generative adversarial networks (GAN) <ref type="bibr" target="#b13">[14]</ref> will be provided in Section 3.7.</p><p>The history of generative modeling on image or nonimage domains is extremely rich, including the general image pattern theory <ref type="bibr" target="#b14">[15]</ref>, deformable models <ref type="bibr" target="#b47">[48]</ref>, inducing features <ref type="bibr" target="#b7">[8]</ref>, wake-sleep <ref type="bibr" target="#b18">[19]</ref>, the MiniMax entropy theory <ref type="bibr" target="#b50">[51]</ref>, the field of experts <ref type="bibr" target="#b35">[36]</ref>, Bayesian models <ref type="bibr" target="#b48">[49]</ref>, and deep belief nets <ref type="bibr" target="#b19">[20]</ref>. Each of these pioneering works points to some promising direction in unsupervised generative modeling. However the modeling power of these existing frameworks is still somewhat limited in computational and/or representational aspects. In addition, not too many of them sufficiently explore the power of discriminative modeling. Recent works that adopt convolutional neural networks for generative modeling <ref type="bibr" target="#b46">[47]</ref> either use CNNs as a feature extractor or create separate paths <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b41">42]</ref>. The neural artistic transferring work <ref type="bibr" target="#b12">[13]</ref> has demonstrated impressive results on the image transferring and texture synthesis tasks but it is focused <ref type="bibr" target="#b12">[13]</ref> on a careful study of channels attributed to artistic texture patterns, instead of aiming to build a generic image modeling framework. The self-supervised boosting work <ref type="bibr" target="#b44">[45]</ref> sequentially learns weak classifiers under boosting <ref type="bibr" target="#b10">[11]</ref> for density estimation, but its modeling power was not adequately demonstrated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Relationship with GDL [40]</head><p>The generative via discriminative learning framework (GDL) <ref type="bibr" target="#b39">[40]</ref> learns a generator through a sequence of boosting classifiers <ref type="bibr" target="#b10">[11]</ref> using repeatedly self-generated samples, called pseudo-negatives. Our INNg algorithm takes inspiration from GDL, but we also observe a number of limitations in GDL that will be overcome by INNg: GDL uses manually specified feature types (histograms and Haar filters), which are fairly limited; the sampling process in GDL, based on Markov chain Monte Carlo (MCMC), is a big computational bottleneck. Additional differences between GDL and INNg include: (1) the adoption of convolutional networks in INNg results in a significant boost to feature learning. (2) introducing SGD based sampling schemes to the synthesis process in INNg makes a fundamental improvement to the sampling process in GDL that is otherwise slow and impractical. (3) two compromises to the algorithm, namely INNg-single (see <ref type="figure" target="#fig_6">Fig. 4</ref>) and INNgcompressed, are additionally proposed to maintain a single classifier or subset of classifiers, respectively.</p><p>Introspective Discriminative Networks <ref type="bibr" target="#b22">[23]</ref> In the sister paper <ref type="bibr" target="#b22">[23]</ref>, the formulation is extended to focus on the discriminative aspect -improvement of existing classifiers. Additional key differences are: a) the model in <ref type="bibr" target="#b22">[23]</ref> is usually composed of a single classifier with a new formulation for training a softmax multi-class classification and b) it is less concerned with human perceivable quality of its syntheses and instead focuses on their impact within the classification task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>We describe below the introspective neural networks generative modeling (INNg) algorithm. We discuss the main formulation first, which bears some level of similarity to GDL <ref type="bibr" target="#b39">[40]</ref> with the replacement of the boosting algorithm <ref type="bibr" target="#b10">[11]</ref> by convolutional neural networks <ref type="bibr" target="#b26">[27]</ref>. As a result, INNg demonstrates significant improvement over GDL in terms of both modeling and computational power. Whereas GDL relies on manually crafted features, the use of CNNs within INNg provides for automatic feature learning and tuning when backpropagating on the network parameters as well as an increase in computational power. Both are motivated by a formulation from the Bayes theory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Motivation</head><p>We start the discussion by borrowing notation from <ref type="bibr" target="#b39">[40]</ref>. Suppose we are given a set of training images (patches):</p><formula xml:id="formula_0">S = {x i | i =1 .</formula><p>.n} where we assume each x i ∈R m e.g. m =6 4× 64 for 64 × 64 patches. These will constitute positive examples of the patterns/targets we wish to model. To introduce the supervised formulation of studying these patterns, we introduce class labels y ∈{−1, +1} to indicate negative and positive examples, respectively. With this, a generative model computes p(y, x)=p(x|y)p(y), which captures the underlying generation process of x for class y. A discriminative classifier instead computes p(y|x). Under Bayes rule, similar to <ref type="bibr" target="#b39">[40]</ref>:</p><formula xml:id="formula_1">p(x|y = +1) = p(y =+1|x)p(y = −1) p(y = −1|x)p(y =+1) p(x|y = −1),<label>(1)</label></formula><p>which can be further simplified when assuming equal priors p(y = +1) = p(y = −1):</p><formula xml:id="formula_2">p(x|y = +1) = p(y =+1|x) 1 − p(y =+1|x) p(x|y = −1).<label>(2)</label></formula><p>Based on Eq. (2), a generative model for the positive samples (patterns of interest) p(x|y =+ 1 )can be fully represented by a generative model for the negatives p(x|y = −1) and a discriminative classifier p(y =+ 1 |x), if both p(x|y = −1) and p(y =+ 1 |x) can be accurately obtained/learned. However, this seemingly intriguing property is circular. To faithfully learn the positive patterns p(x|y =+ 1 ) , we need to have a representative p(x|y = −1), which is equally difficult, if not more. For clarity, we now use p − (x) to represent p(x|y = −1).I n the GDL algorithm <ref type="bibr" target="#b39">[40]</ref>, a solution was given to learning p(x|y =+ 1 )by using an iterative process starting from an initial reference distribution of the negatives p − 0 (x), e.g. a Gaussian distribution U (x) on the entire space of x ∈R m :</p><formula xml:id="formula_3">p − 0 (x)=U (x), p − t (x)= 1 Z t q t (y =+1|x) q t (y = −1|x) · p − t−1 (x),t=1..T (3)</formula><p>where</p><formula xml:id="formula_4">Z t = qt(y=+1|x) qt(y=−1|x) p − t−1 (x)dx.</formula><p>Our hope is to gradually learn p − t (x) by following this iterative process of Eq.</p><formula xml:id="formula_5">3: p − t (x) t=∞ → p(x|y =+1),<label>(4)</label></formula><p>such that the samples drawn x ∼ p − t (x) become indistinguishable from the given training samples. The samples drawn from x ∼ p − t (x) are called pseudo-negatives, following a definition in <ref type="bibr" target="#b39">[40]</ref> to indicate examples considered by the current iteration of the model to be positives but are, in reality, negative examples. Next, we present the practical realization of ideas from Eq. 3, namely INNg (consisting of a sequence of CNN classifiers composed to produce the process seen in <ref type="figure" target="#fig_2">Fig. 3</ref>) and, additionally, the extreme case of INNg-single that maintains a sequence consisting of single CNN classifier as seen in <ref type="figure" target="#fig_6">Fig. 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">INNg Training Algorithm 1 Outline of the INNg algorithm.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input:</head><p>Given a set of training data S+ = {(xi,yi = +1),i =1..n} with x ∈ℜ m . Initialization: obtain an initial distribution e.g. Gaussian for the pseudo-negative samples:</p><formula xml:id="formula_6">p − 0 (x)=U (x). Create S 0 − = {(xi, −1),i =1, ..., l} with xi ∼ p − 0 (x) For t=1..T 1. Classification-step: Train CNN classifier C t on S+ ∪ S t−1 − , resulting in qt(y =+1|x). 2. Update the model: p − t (x)= 1 Z t q t (y=+1|x) q t (y=−1|x) p − t−1 (x).</formula><p>3. Synthesis-step: sample l pseudo-negative samples xi ∼ p − t (x),i =1, ..., l from the current model p − t (x) using a SGDbased sampling procedure (backpropagation on the input) to obtain S t − = {(xi, −1),i =1, ..., l}. 4. t ← t +1and go back to step 1 until convergence (e.g. indistinguishable to the given training samples). End</p><p>As defined in the previous section, we are given an unlabeled training set by S = {x i | i =1..n} as our positive examples. Since pseudo-negatives will be added, we refer to this initial positive set as S + = {(x i ,y i =+1 )| i =1 ..n} within the discriminative formulation. Additionally, we must consider the initial set of negatives bootstrapped from noise (also referred to as the initial pseudo-negative set) denoted:</p><formula xml:id="formula_7">S 0 − = {(x i , −1) | i =1, ..., l} where x i ∼ p − 0 (x)=U (x)</formula><p>according to a Gaussian distribution. Since each stage of the algorithm will refine this set, we define the working set for stage t =1..T as S  to include the pseudo-negative samples (l of them) selfgenerated by the model after stage t. We then train the model at each stage t to obtain q t (y =+1|x),q t (y = −1|x)</p><formula xml:id="formula_8">t−1 − = {(x i , −1) | i =1, ..., l}.</formula><p>over S + ∪ S t − resulting in the classifier C t . Note that q is an approximation to the true p due to limited samples drawn from ℜ m . At each time t, we then compute an approximation to (elaborated in Section 3.2.2)</p><formula xml:id="formula_10">p − t (x)= 1 Z t q t (y =+1|x) q t (y = −1|x) p − t−1 (x),<label>(6)</label></formula><p>where</p><formula xml:id="formula_11">Z t = qt(y=+1|x) qt(y=−1|x) p − t−1 (x)dx. Then, we can draw new samples x i ∼ p − t (x)</formula><p>to produce the stages's pseudo-negative set:</p><formula xml:id="formula_12">S t+1 − = {(x i , −1),i =1, ..., l}.<label>(7)</label></formula><p>Algorithm 1 describes the learning process. The pipeline of INNg is shown in <ref type="figure" target="#fig_2">Fig. 3</ref>, which consists of: (1) a synthesis step and (2) a classification step. A sequence of CNN classifiers is progressively learned. With the pseudo-negatives being gradually generated, the classification boundary gets tightened and approaches the target distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Classification-step</head><p>The classification-step can be viewed as training a classifier on the training set</p><formula xml:id="formula_13">S + ∪ S t − where S + = {(x i ,y i = +1),i =1 ..n}. S t − = {(x i , −1),i =1 , ..., l} for t ≥ 1.</formula><p>In practice, we also keep a subset of pseudo-negatives from earlier stages to increase stability. We use a CNN as our base classifier. When training a classifier C t on S + ∪ S t − , we denote the parameters to be learned in C t by a high-</p><formula xml:id="formula_14">dimensional vector W t =( w (0) t , w<label>(1)</label></formula><p>t ) which might consist of millions of parameters. w carries all the internal representations. Without loss of generality, we assume a sigmoid function for the discriminative probability</p><formula xml:id="formula_15">q t (y|x; W t )=1/(1 + exp{−y&lt;w (1) t ,φ(x; w (0) t ) &gt;}).<label>(8)</label></formula><note type="other">Both w (1) t and w (0)</note><p>t can be learned by the standard stochastic gradient descent algorithm via backpropagation to minimize a cross-entropy loss with an additional term on the pseudo-negatives:</p><formula xml:id="formula_16">L(Wt)=− i=1..n (x i ,+1)∈S + ln qt(+1|x i ; Wt)− i=1..l (x i ,−1)∈S t − ln qt(−1|x i ; Wt)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Synthesis-step</head><p>In the classification step, we obtain q t (y|x; W t ) which is then used to update p − t (x) according to Eq. (6):</p><formula xml:id="formula_17">p − t (x)= t a=1 1 Z a q a (y =+1|x; W a ) q a (y = −1|x; W a ) p − 0 (x).<label>(9)</label></formula><p>In the synthesis-step, our goal is to draw fair samples from p − t (x). By using SGD-based sampling, we carry out backpropagation with respect to x (the image space) using the CNN models making up Eq. 9. Note that the partition function (normalization) Z a is a constant that is not dependent on the sample x. Let</p><formula xml:id="formula_18">g a (x)= q a (y =+1|x; W a ) q a (y = −1|x; W a ) =exp{&lt; w (1) a ,φ(x; w (0) a ) &gt;},<label>(10)</label></formula><p>and take its ln, which is nicely turned into the logit of q a (y =+1|x;</p><formula xml:id="formula_19">W a ) ln g a (x)=&lt; w (1) a ,φ(x; w (0) a ) &gt;.<label>(11)</label></formula><p>Starting from an initialization of x, the process allows us to directly increase</p><formula xml:id="formula_20">t a=1 &lt; w (1) a ,φ(x; w (0)</formula><p>a ) &gt; using gradient ascent on x via backpropagation to obtain fair samples subject to Eq. <ref type="bibr" target="#b8">(9)</ref>. Injecting the noise to the sampler results in a general family of stochastic gradient Langevin dynamics <ref type="bibr" target="#b43">[44]</ref>: ∆x = ∇( t a=1 ln g a (x)) + η where η ∼ N (0,ǫ) is a Gaussian distribution. In practice, to reduce the time and memory complexity, we initialize x drawn from p − t−1 (x), allowing us to primarily focus on the most recent model q t with SGD sampling for ln g t (x)= &lt; w To ensure this follows Langevin dynamics as elaborated in <ref type="bibr" target="#b43">[44]</ref>, Gaussian noise with an annealed variance would be added, however, we did not observe a big difference in the quality of samples in practice. Additionally, recent work in <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30]</ref> also show the connections and equivalence between MCMC and SGD-based sampling schemes, where the sampling bias and variance are worth further studying but pose no particular disadvantages here. We have also recently experimented on using different SGD sampling schemes (eg. early-stopping, long steps, perturbations) but did observe significant differences. This is likely partially compensated for by the inherent stochasticity introduced by the random selection of the image patches (and to what extent due to overlap) during synthesis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Overall model</head><p>The overall INNg model after T stages of training becomes:</p><formula xml:id="formula_21">p − T (x)= 1 Z T a=1 q a (y =+1|x; W a ) q t (y = −1|x; W a ) p − 0 (x) = 1 Z T a=1 exp{&lt; w (1) a ,φ(x; w (0) a ) &gt;}p − 0 (x),<label>(12)</label></formula><p>where</p><formula xml:id="formula_22">Z = T a=1 exp{&lt; w (1) a ,φ(x; w (0) a ) &gt;}p − 0 (x)dx.</formula><p>INNg shares a similar cascade aspect with GDL <ref type="bibr" target="#b39">[40]</ref> where the convergence of this iterative learning process to the target distribution was shown by the following theorem in <ref type="bibr" target="#b39">[40]</ref>.</p><formula xml:id="formula_23">Theorem 1 KL[p(x|y =+ 1 ) ||p − t+1 (x)] ≤ KL[p(x|y = +1)||p − t (x)]</formula><p>where KL denotes the Kullback-Leibler divergences, and p(x|y =+1)≡ p + (x).</p><p>This implies that after sufficiently many stages of INNg training, the distribution of the positives should be well approximated by the resulting cascade of classifiers modeled in Eq. 12.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">INNg synthesis</head><p>While the previous section describes the training process of an INNg model (which itself uses a synthesis step in it to generate pseudo-negatives for the next stage), we consider the synthesis of a new sample from the fully trained model. Since a fully trained model consists of T saved classifiers, we attempt to sample through this sequence in a similar fashion to the training. Starting with some x ∼ U (x), we perform gradient ascent with respect to x until, based off the current classifier C t , x crosses the decision boundary of C t -to now be considered a positive example. Then, C t+1 is loaded and the process is repeated again on the resulting x until it has passed through all classifiers (through C T )in a similar manner to produce the sample. Note that we perform early stopping within each stage i.e. it was not seen to be effective to produce a transformation of x that has near 1.0 probability of being considered a positive, only one that is a narrow margin over this boundary. This additionally allows for a more efficient runtime of the synthesis process.  We briefly present the INNg-single algorithm and show the pipeline of INNg-single is in <ref type="figure" target="#fig_6">Fig. 4</ref>. Note that we maintain a single CNN classifier throughout the entire learning process in INNg-single.</p><p>In the classification step, we obtain q t (y|x; W t ) (similar as Eq. 8) which is then used to update p − t (x) according to Eq. <ref type="formula" target="#formula_1">(13)</ref>:</p><formula xml:id="formula_24">p − t (x)= 1 Z t q t (y =+1|x; W t ) q t (y = −1|x; W t ) p − 0 (x).<label>(13)</label></formula><p>In the synthesis-step, we draw samples from p − t (x). The overall INNg-single model after T stages of training becomes:</p><formula xml:id="formula_25">p − T (x)= 1 Z T exp{&lt; w (1) T ,φ(x; w (0) T ) &gt;}p − 0 (x),<label>(14)</label></formula><p>where Given a particularly sized image, anysize-imagegeneration within INNg allows one to generate/synthesize an image much larger than the given one. Patches extracted from the training images are used in the training of the discriminator. However, their position within the training (or pseudo-negative) image is not lost. In particular, when performing synthesis using backpropagation, updates to the pixel values are made by considering the average loss of all patches that overlap a given pixel. Thus, up to stage T ,i n order to consider the updates to the patch of x(i, j) centered at position (i, j) for image I of size m 1 × m 2 , we perform backpropagation on the patches to increase the probability:</p><formula xml:id="formula_26">Z T = exp{&lt; w (1) T ,φ(x; w (0) T ) &gt;}p − 0 (x)dx.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Model-based anysize-image-generation</head><formula xml:id="formula_27">p T (I) ∝ T a=1 m1 i=1 m2 j=1 g a (x(i, j))p − 0 (x(i, j))<label>(15)</label></formula><p>where g a (x(i, j)) (see Eq. 10) denotes the score of the patch of size e.g. 64 × 64 for x(i, j) under the discriminator at stage a. <ref type="figure" target="#fig_7">Fig. 5</ref> gives an illustration for one stage of sampling. This allows us to synthesize much larger images by being able to enforce the coherence and interactions surrounding a particular pixel. In practice, we add stochasticity and efficiency to the synthesis process by randomly sampling these set of patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Model size reduction</head><p>As mentioned in Section 3.2.2 and 3.3, the process of INNg relies on keeping a snapshot of the classifier after each stage t =1...T. This can pose a problem for both space and time efficiency. However, it may not be all necessary to keep each classifier around. Instead, one can pick some factor b such that the classifier is only saved every b stages. We accomplish this by: i) only saving the model to disk every b stages ii) initializing the sampling process at stage b × i + k (i ∈ N and k&lt;b) from those of stage b × i. We still, however, keep the pseudo-negatives around for training purposes to stabilize the process. We do find that later stages within each "mini stage" should be allowed more backpropagation steps during the synthesis step in order to compensate for the more complex optimization landscape. The image seen in <ref type="figure" target="#fig_1">Fig. 1</ref> was generated from a reduced model (b =3and T =6 0 ), resulting in only 20 classifiers. One can view this as a cascade of multiple of INNg-singles.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Comparison with GAN [14]</head><p>Next we compare INNg with a very interesting and popular line of work, generative adversarial neural networks (GAN) <ref type="bibr" target="#b13">[14]</ref>. We summarize the key differences between INNg and GAN. Other recent algorithms alongside GAN <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b38">39]</ref> share similar properties with it.</p><p>• Unified generator/discriminator vs. separate generator and discriminator. INNg maintains a single model that is simultaneously a generator and a discriminator. The INNg generator therefore is able to self-evaluate the difference between its generated samples (pseudo-negatives) against the training data. This gives us an integrated framework to achieve competitive results in unsupervised and fully supervised learning with the generator and discriminator helping each other internally (not externally). GAN instead creates two convolutional networks, a generator and a discriminator.</p><p>• Training. Due to the internal competition between the generator and the discriminator, GAN is known to be hard to train <ref type="bibr" target="#b0">[1]</ref>. INNg instead carries out a straightforward use of backpropagation in both the sampling and the classifier training stage, making the learning process direct. For example, all the textures by INNg shown in the experiments <ref type="figure">Fig. 2</ref> and <ref type="figure">Fig. 6</ref> are obtained under the identical setting without hyper-parameter tuning.</p><p>• Speed. GAN performs a forward pass to reconstruct an image, which is generally faster than INNg where synthesis is carried out using backpropagation. INNg is still practically feasible since it takes about 10 seconds to synthesize a batch of 50 images of 64 × 64 and around 30 seconds to synthesize a texture image of size 256×256, excluding the time to load the models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate both INNg and INNg-single. In each method, we adopt the discriminator architecture of <ref type="bibr" target="#b33">[34]</ref>   <ref type="figure">Figure 6</ref>. More texture synthesis results. Gatys et al. <ref type="bibr" target="#b12">[13]</ref> and Texture</p><p>Nets <ref type="bibr" target="#b41">[42]</ref> results are from <ref type="bibr" target="#b41">[42]</ref>.</p><p>which takes an input size of 64 × 64 × 3 in the RGB colorspace by four convolutional layers using 5×5 kernel sizes with the layers using 64, 128, 256 and 512 channels, respectively. We include batch normalization after each convolutional layer (excluding the first) and use leaky ReLU activations with leak slope 0.2. The classification layer flattens the input and finally feeds it into a sigmoid activation. This serves as the discriminator for the 64×64 patches we extract from the training image(s). Note that it is a general purpose architecture with no modifications made for a specific task in mind.</p><p>In texture synthesis and artistic style, we make use of the "anysize-image-generation" architecture by adding a "head" to the network that, at each forward pass of the network, randomly selects some number (equal to the desired batch size) of 64 × 64 random patches (possibly overlapping) from the full sized images and passes them to the discriminator. This allows us to retain the whole space of patches within a training image rather than select some subset of them in advance to use during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Texture modeling</head><p>Texture modeling/rendering is a long standing problem in computer vision and graphics <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b32">33]</ref>. Here we are interested in statistical texture modeling <ref type="bibr" target="#b50">[51,</ref><ref type="bibr" target="#b45">46]</ref>, instead of just texture rendering <ref type="bibr" target="#b9">[10]</ref>. We train similar textures to <ref type="bibr" target="#b41">[42]</ref>. Each source texture is resized to 256 × 256, used as the "positive" image in the training set; a set of 200 negative images are initially sampled from a normal distribution with σ =0.3 of size 320 × 320 after adding padding of 32 pixels to each spatial dimension of the image to ensure each pixel of the 256 × 256 center has equal probability of being extracted in some patch. 1, 000 patches are extracted randomly across the training images and fed to the discriminator at each forward pass of the network (during training and synthesis stages) from a batch size of 100 patches -50 random positives and negatives when training and 100 pseudo-negatives during synthesis. At each stage, our classifier is finetuned using stochastic gradient descent with learning rate 0.01 from the previous stage's classifier. Pseudo-negatives from more recent stages are chosen in mini-batches with higher probability than those of earlier stages in order to ensure the discriminator learns from its most recent mistakes as well as provide for more efficient training when the set of accumulated negatives has grown large in later stages. During the synthesis stage, pseudo-negatives are synthesized using the previous stage's pseudo-negatives as their initialization. Adam is used with a learning rate of 0.1 and β =0 .5 and stops early when the average probability of the patches under the discriminator becomes positive (across some window of steps, usually 20). We find this sampling strategy to attain a good balance in effectiveness and efficiency.</p><p>New textures are synthesized under INNg by: initializing from the normal distribution with σ =0 .3 followed by SGD based sampling via backpropagation using the saved networks for each stage, and feeding the resulting synthesis to the next stage. The number of patches is decided based on the image size to be synthesized, typically 10 patches when synthesizing a 256 × 256 image since this matches the average number of patches extracted per image during training. For INNg-single which consists of a single CNN classifier, SGD-based sampling is performed directly using this CNN classifier to transform the initial normal distribution to a desired texture.</p><p>Considering the results in <ref type="figure">Fig. 2</ref>, we see that INNg (60 CNN classifiers each with 4 layers) generates images of similar quality to <ref type="bibr" target="#b41">[42]</ref>, but of higher quality than those by Gatys et al. <ref type="bibr" target="#b12">[13]</ref>, Portilla &amp; Simoncelli <ref type="bibr" target="#b32">[33]</ref>, and DCGAN <ref type="bibr" target="#b33">[34]</ref>. In general, synthesis by INNg is usually more faithful to the structure of the input images.</p><p>More texture modeling results are provided in <ref type="figure">Fig. 6</ref>.W e make an interesting observation that the "diamond" texture (the fourth row) generated by INNg (same as that used in <ref type="figure">Fig. 2)</ref> shows to preserve the near-regular patterns much better than the other methods. In the bottom row of <ref type="figure">Fig. 6</ref>, the "pebbles" synthesis of INNg captures the size variation as well as the variation in color and shading, better than TextureNets <ref type="bibr" target="#b41">[42]</ref> and Gatys et al. <ref type="bibr" target="#b12">[13]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Artistic style transfer</head><p>We also attempt to transfer artistic style as shown in <ref type="bibr" target="#b12">[13]</ref>. However, our architecture makes no use of additional networks for content and texture transferring task uses a loss functions during synthesis to minimize where I is an input image and I style is its stylized version, and p − style (I) denotes the model learned from the training style image. We include a L 2 fidelity term during synthesis, weighted by a parameter γ, making I style not too far away from the input image I. We choose γ =0 .3 and average the L 2 difference between the original content image and the current stylized image at each step of synthesis. Two examples of the artistic style transfer are shown in <ref type="figure" target="#fig_10">Fig. 7.</ref>  INNg is also demonstrated on a face modeling task. The CelebA dataset <ref type="bibr" target="#b28">[29]</ref> is used in our face modeling experiment, which consists of 202, 599 face images. We crop the center 64×64 patches in these images as our positive examples. For the classification step, we use stochastic gradient descent with learning rate 0.01 and a batch size of 100 images, which contains 50 random positives and 50 random negatives. For the synthesis step, we use the Adam optimizer with learning rate 0.02 and β =0 .5 and stop early when the pseudo-negatives cross the decision boundary. In <ref type="figure" target="#fig_11">Fig. 8</ref>, we show some face examples generated by the DC-GAN model <ref type="bibr" target="#b33">[34]</ref>, our INNg-single model (1 CNN classifier with 4 conv layers), and our INNg model (12 CNN classifiers each with 4 conv layers). <ref type="figure">Figure 9</ref>. Generated images learned on the SVHN dataset. The first, the second, and the third column are respectively results by DCGAN <ref type="bibr" target="#b33">[34]</ref> (using tensorflow implementation <ref type="bibr" target="#b24">[25]</ref>), INNg-single (1 CNN classifier with 4 layers), and INNg (10 CNN classifiers each with 4 layers).</p><formula xml:id="formula_28">− ln p(I style | I) ∝ γ·||I style −I|| 2 −(1−γ)·ln p − style (I style ),</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Face modeling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">SVHN unsupervised learning</head><p>The SVHN <ref type="bibr" target="#b31">[32]</ref> dataset consists of color images of house numbers collected by Google Street View. The training set consists of 73, 257 images, the extra set consists of 531, 131 images, and the test set has 26, 032 images. The images are of the size 32 × 32. We combine the training and extra set as our positive examples for unsupervised learning. Following the same settings in the face modeling experiments, we shown some examples generated by the DCGAN model <ref type="bibr" target="#b33">[34]</ref>, INNg-single (1 CNN classifier with 4 conv layers), and INNg (10 CNN classifiers each with 4 conv layers).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Unsupervised feature learning</head><p>We perform the unsupervised feature learning and semisupervised classification experiment by following the procedure outlined in <ref type="bibr" target="#b33">[34]</ref>. We first train a model on the SVHN training and extra set in an unsupervised way, as in Section 4.4. Then, we train an L2-SVM on the learned representations of this model. The features from the last three convolutional layers are concatenated to form a 14336-dimensional feature vector. A 10, 000 example held-out validation set is taken from the training set and is used for model selection. The SVM classifier is trained on 1000 examples taken at random from the remainder of the training set. The test error rate is averaged over 100 different SVMs trained on random 1000-example training sets. Within the same setting, our INNg model achieves the test error rate of 32.81% and the DCGAN model achieves 33.13% (we ran the DCGAN code <ref type="bibr" target="#b24">[25]</ref> in an identical setting as INNg for a fair comparison).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Generative modeling using introspective neural networks points to an encouraging direction for unsupervised image modeling that capitalizes on the power of discriminative deep convolutional neural networks. It can be adopted for a wide range of problems in computer vision. The source code will be made available on GitHub.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The first row shows the development of two 64 × 64 pseudonegative samples (patches) over the course of the training process on the "tree bark" texture at selected stages. We can see the initial "scaffold" created and then refined by the networks in later stages. The input "tree bark" texture and a synthesized image by our INNg algorithm are shown in the second row. This texture was synthesized by INNg using 20 CNN classifiers each with 4 layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Schematic illustration of the pipeline of INNg. The top figure shows the input training samples shown in red circles. The bottom figure shows the pseudo-negative samples drawn by the learned final model. The left panel displays pseudo-negative samples drawn at each time stamp t. The right panel shows the classification by the CNN on the training samples and pseudo-negatives at each time stamp t.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>&gt;. Therefore, generating pseudo-negative samples does not need a large overhead.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Schematic illustration of the pipeline of INNg-single.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Illustration of model-based anysize-image-generation strategy.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>•</head><label></label><figDesc>Model size. Since a sequence of CNN classifiers (10 − 60) are included in INNg, INNg has a much larger model complexity than GAN. This is an advantage of GAN over INNg. Our al- ternative INNg-single model maintains a single CNN classifier but its generative power is worse than those of INNg and GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Artistic style transfer results using the "Starry Night" and "Scream" style on the image from Amsterdam.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Generated images learned on the CelebA dataset. The first, the second, and the third column are respectively results by DCGAN [34] (using tensorflow implementation [25]), INNg-single (1 CNN classifier with 4 layers), and INNg (12 CNN classifiers each with 4 layers).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 2. Texture synthesis algorithm comparison. Gatys et al. [13], Texture Nets [42], Portilla &amp; Simoncelli [33], and DCGAN [34] results are from [42].</figDesc><table>input 

Gatys et al. 
TextureNets 
Portilla &amp; Simoncelli 
DCGAN 
INNg-single (ours) 
INNg (ours) 

Classification 
(training samples vs. 
pseudo-negatives) 

Introspective Neural Networks for Generative Modeling 
INNg 

Synthesis 
(pseudo-negatives) 

training samples 

Learned Model 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported by NSF IIS-1618477, NSF IIS-1717431, and a Northrop Grumman Contextual Robotics grant. We thank Saining Xie, Weijian Xu, Jun-Yan Zhu, Jiajun Wu, Stella Yu, Alexei Efros, Jitendra Malik, Sebastian Nowozin, Yingnian Wu, and Song-Chun Zhu for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">Wasserstein gan. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Autoencoders, unsupervised learning, and deep architectures. ICML unsupervised and transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Baldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">27</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Scaling learning algorithms towards ai. Large-scale kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">34</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Latent dirichlet allocation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of machine Learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="993" to="1022" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Breiman</surname></persName>
		</author>
		<title level="m">Random Forests. Machine Learning</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ritchie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Weston</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.07093</idno>
		<title level="m">Neural photo editing with introspective adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic gradient hamiltonian monte carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">B</forename><surname>Fox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Inducing features of random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Della</forename><surname>Pietra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE PAMI</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="380" to="393" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">O</forename><surname>Duda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">E</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Stork</surname></persName>
		</author>
		<title level="m">Pattern Classification</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
	<note>2nd edition</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Texture synthesis by nonparametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A decision-theoretic generalization of on-line learning and an application to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Comp. and Sys. Sci</title>
		<imprint>
			<biblScope unit="volume">55</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">The elements of statistical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Friedman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hastie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tibshirani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Springer series in statistics</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">General pattern theory-A mathematical study of regular structures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Grenander</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
			<publisher>Clarendon Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Pyramid-based texture analysis/synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Heeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Bergen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="229" to="238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised discovery of nonlinear structure using contrastive backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive science</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="725" to="731" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The&quot; wake-sleep&quot; algorithm for unsupervised neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">268</biblScope>
			<biblScope unit="issue">5214</biblScope>
			<biblScope unit="page">1158</biblScope>
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="1527" to="1554" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
		<title level="m">Machine learning: discriminative and generative</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Introspective classifier learning: Empower generatively</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lazarow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>In arXiv preprint arXiv</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Principal component analysis. Wiley Online Library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Jolliffe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dcgan-Tensorflow</surname></persName>
		</author>
		<ptr target="https://github.com/carpedm20/DCGAN-tensorflow" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">ImageNet Classification with Deep Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Computation</title>
		<imprint>
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">An asymptotic analysis of generative, discriminative, and pseudolikelihood estimators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04289</idno>
		<title level="m">Stochastic gradient descent as approximate bayesian inference</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deepdream -a code example for visualizing neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mordvintsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tyka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Google Research</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Reading Digits in Natural Images with Unsupervised Feature Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">A parametric texture model based on joint statistics of complex wavelet coefficients. Int&apos;l j. of computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Portilla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="49" to="70" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Modeling by shortest data description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Rissanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Automatica</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="465" to="471" />
			<date type="published" when="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fields of experts: A framework for learning image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03498</idno>
		<title level="m">Improved techniques for training gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Normalized cuts and image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="888" to="905" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02386</idno>
		<title level="m">Adagan: Boosting generative models</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning generative models via discriminative approaches</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Brain anatomical structure segmentation by hybrid discriminative/generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">L</forename><surname>Narr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Dinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Thompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Toga</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Tran. on Medical Imag</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">The nature of statistical learning theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">N</forename><surname>Vapnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
			<publisher>Springer-Verlag New York, Inc</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Bayesian learning via stochastic gradient langevin dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Self supervised boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09408</idno>
		<title level="m">Cooperative training of descriptor and generator networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">A theory of generative convnet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Feature extraction from faces using deformable templates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Hallinan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="111" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Vision as bayesian inference: analysis by synthesis?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kersten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trends in cognitive sciences</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="301" to="308" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03126</idno>
		<title level="m">Energy-based generative adversarial network</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Minimax entropy principle and its application to texture modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">N</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mumford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1627" to="1660" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
