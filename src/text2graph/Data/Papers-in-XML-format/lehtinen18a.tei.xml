<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Noise2Noise: Learning Image Restoration without Clean Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jaakko</forename><surname>Lehtinen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jacob</forename><surname>Munkberg</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Hasselgren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samuli</forename><surname>Laine</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tero</forename><surname>Karras</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miika</forename><surname>Aittala</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename><surname>Aila</surname></persName>
						</author>
						<title level="a" type="main">Noise2Noise: Learning Image Restoration without Clean Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We apply basic statistical reasoning to signal reconstruction by machine learning -learning to map corrupted observations to clean signals -with a simple and powerful conclusion: it is possible to learn to restore images by only looking at corrupted examples, at performance at and sometimes exceeding training using clean data, without explicit image priors or likelihood models of the corruption. In practice, we show that a single model learns photographic noise removal, denoising synthetic Monte Carlo images, and reconstruction of undersampled MRI scans -all corrupted by different processes -based on noisy data only.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Signal reconstruction from corrupted or incomplete measurements is an important subfield of statistical data analysis. Recent advances in deep neural networks have sparked significant interest in avoiding the traditional, explicit a priori statistical modeling of signal corruptions, and instead learning to map corrupted observations to the unobserved clean versions. This happens by training a regression model, e.g., a convolutional neural network (CNN), with a large number of pairs (x i , y i ) of corrupted inputsx i and clean targets y i and minimizing the empirical risk</p><formula xml:id="formula_0">argmin θ i L (f θ (x i ), y i ) ,<label>(1)</label></formula><p>where f θ is a parametric family of mappings (e.g., CNNs), under the loss function L. We use the notationx to underline the fact that the corrupted inputx ∼ p(x|y i ) is a random variable distributed according to the clean target. Training data may include, for example, pairs of short and long exposure photographs of the same scene, incomplete and complete k-space samplings of magnetic resonance images, fast-but-noisy and slow-but-converged ray-traced 1 NVIDIA 2 Aalto University 3 MIT CSAIL. Correspondence to: Jaakko Lehtinen &lt;jlehtinen@nvidia.com&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 35</head><p>th International Conference on Machine Learning, <ref type="bibr">Stockholm, Sweden, PMLR 80, 2018</ref><ref type="bibr">. Copyright 2018</ref> by the author(s).</p><p>renderings of a synthetic scene, etc. Significant advances have been reported in several applications, including Gaussian denoising, de-JPEG, text removal <ref type="bibr" target="#b12">(Mao et al., 2016)</ref>, super-resolution <ref type="bibr" target="#b9">(Ledig et al., 2017)</ref>, colorization <ref type="bibr" target="#b23">(Zhang et al., 2016)</ref>, and image inpainting <ref type="bibr" target="#b7">(Iizuka et al., 2017</ref>). Yet, obtaining clean training targets is often difficult or tedious: a noise-free photograph requires a long exposure; full MRI sampling precludes dynamic subjects; etc.</p><p>In this work, we observe that we can often learn to turn bad images into good images by only looking at bad images, and do this just as well -sometimes even better -as if we were using clean examples. Further, we require neither an explicit statistical likelihood model of the corruption nor an image prior, and instead learn these indirectly from the training data. (Indeed, in one of our examples, synthetic Monte Carlo renderings, the non-stationary noise cannot be characterized analytically.) In addition to denoising, our observation is directly applicable to inverse problems such as MRI reconstruction from undersampled data. While our conclusion is almost trivial from a statistical perspective, it significantly eases practical learned signal reconstruction by lifting requirements on availability of training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Theoretical Background</head><p>Assume that we have a set of unreliable measurements (y 1 , y 2 , ...) of the room temperature. A common strategy for estimating the true unknown temperature is to find a number z that has the smallest average deviation from the measurements according to some loss function L:</p><formula xml:id="formula_1">argmin z E y {L(z, y)}.<label>(2)</label></formula><p>For the L 2 loss L(z, y) = (z − y) 2 , this minimum is found at the arithmetic mean of the observations:</p><formula xml:id="formula_2">z = E y {y}.<label>(3)</label></formula><p>The L 1 loss, the sum of absolute deviations L(z, y) = |z − y|, in turn, has its optimum at the median of the observations. The general class of deviation-minimizing estimators are known as M-estimators <ref type="bibr" target="#b6">(Huber, 1964)</ref>. From a statistical viewpoint, summary estimation using these common loss functions can be seen as ML estimation by interpreting the loss function as the negative log likelihood.</p><p>Training neural network regressors is a generalization of this point estimation procedure. Observe the form of the typical training task for a set of input-target pairs (x i , y i ), where the network function f θ (x) is parameterized by θ:</p><formula xml:id="formula_3">argmin θ E (x,y) {L(f θ (x), y)}.<label>(4)</label></formula><p>Indeed, if we remove the dependency on input data, and use a trivial f θ that merely outputs a learned scalar, the task reduces to (2). Conversely, the full training task decomposes to the same minimization problem at every training sample; simple manipulations show that (4) is equivalent to</p><formula xml:id="formula_4">argmin θ E x {E y|x {L(f θ (x), y)}}.<label>(5)</label></formula><p>The network can, in theory, minimize this loss by solving the point estimation problem separately for each input sample. Hence, the properties of the underlying loss are inherited by neural network training.</p><p>The usual process of training regressors by Equation 1 over a finite number of input-target pairs (x i , y i ) hides a subtle point: instead of the 1:1 mapping between inputs and targets (falsely) implied by that process, in reality the mapping is multiple-valued. For example, in a superresolution task <ref type="bibr" target="#b9">(Ledig et al., 2017)</ref> over all natural images, a low-resolution image x can be explained by many different high-resolution images y, as knowledge about the exact positions and orientations of the edges and texture is lost in decimation. In other words, p(y|x) is the highly complex distribution of natural images consistent with the low-resolution x. Training a neural network regressor using training pairs of lowand high-resolution images using the L 2 loss, the network learns to output the average of all plausible explanations (e.g., edges shifted by different amounts), which results in spatial blurriness for the network's predictions. A significant amount of work has been done to combat this well known tendency, for example by using learned discriminator functions as losses <ref type="bibr" target="#b9">(Ledig et al., 2017;</ref><ref type="bibr" target="#b8">Isola et al., 2017)</ref>.</p><p>Our observation is that for certain problems this tendency has an unexpected benefit. A trivial, and, at first sight, useless, property of L 2 minimization is that on expectation, the estimate remains unchanged if we replace the targets with random numbers whose expectations match the targets. This is easy to see: Equation (3) holds, no matter what particular distribution the ys are drawn from. Consequently, the optimal network parameters θ of Equation <ref type="formula" target="#formula_4">(5)</ref>  </p><formula xml:id="formula_5">argmin θ i L (f θ (x i ),ŷ i ) ,<label>(6)</label></formula><p>where both the inputs and the targets are now drawn from a corrupted distribution (not necessarily the same), conditioned on the underlying, unobserved clean target y i such that E{ŷ i |x i } = y i . Given infinite data, the solution is the same as that of (1). For finite data, the variance is the average variance of the corruptions in the targets, divided by the number of training samples (see supplemental material). Interestingly, none of the above relies on a likelihood model of the corruption, nor a density model (prior) for the underlying clean image manifold. That is, we do not need an explicit p(noisy|clean) or p(clean), as long as we have data distributed according to them.</p><p>In many image restoration tasks, the expectation of the corrupted input data is the clean target that we seek to restore. Low-light photography is an example: a long, noise-free exposure is the average of short, independent, noisy exposures. With this in mind, the above suggests the ability to learn to remove photon noise given only pairs of noisy images, with no need for potentially expensive or difficult long exposures. Similar observations can be made about other loss functions. For instance, the L 1 loss recovers the median of the targets, meaning that neural networks can be trained to repair images with significant (up top 50%) outlier content, again only requiring access to pairs of such corrupted images.</p><p>In the next sections, we present a wide variety of examples demonstrating that these theoretical capabilities are also efficiently realizable in practice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Practical Experiments</head><p>We now experimentally study the practical properties of noisy-target training. We start with simple noise distributions (Gaussian, Poisson, Bernoulli) in Sections 3.1 and 3.2, and continue to the much harder, analytically intractable Monte Carlo image synthesis noise (Section 3.3). In Section 3.4, we show that image reconstruction from subNyquist spectral samplings in magnetic resonance imaging (MRI) can be learned from corrupted observations only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Additive Gaussian Noise</head><p>We will first study the effect of corrupted targets using synthetic additive Gaussian noise. As the noise has zero mean, we use the L 2 loss for training to recover the mean.</p><p>Our baseline is a recent state-of-the-art method "RED30" <ref type="bibr" target="#b12">(Mao et al., 2016)</ref>, a 30-layer hierarchical residual network with 128 feature maps, which has been demonstrated to be very effective in a wide range of image restoration tasks, including Gaussian noise. We train the network using 256×256-pixel crops drawn from the 50k images in  the IMAGENET validation set. We furthermore randomize the noise standard deviation σ ∈ [0, 50] separately for each training example, i.e., the network has to estimate the magnitude of noise while removing it ("blind" denoising).</p><p>We use three well-known datasets: BSD300 <ref type="bibr" target="#b13">(Martin et al., 2001)</ref>, <ref type="bibr">SET14 (Zeyde et al., 2010)</ref>, and KODAK 1 . As summarized in <ref type="table" target="#tab_1">Table 1</ref>, the behavior is qualitatively similar in all three sets, and thus we discuss the averages. When trained using the standard way with clean targets (Equation 1), RED30 achieves 31.63 ± 0.02 dB with σ = 25. The confidence interval was computed by sampling five random initializations. The widely used benchmark denoiser BM3D <ref type="bibr" target="#b3">(Dabov et al., 2007)</ref> gives ∼0.7 dB worse results. When we modify the training to use noisy targets (Equation 6) instead, the denoising performance remains equally good. Furthermore, the training converges just as quickly, as shown in <ref type="figure" target="#fig_0">Figure 1a</ref>. This leads us to conclude that clean targets are unnecessary in this application. This perhaps surprising observation holds also with different networks and network capacities. <ref type="figure" target="#fig_2">Figure 2a</ref> shows an example result.</p><p>For all further tests, we switch from RED30 to a shallower U-Net <ref type="bibr" target="#b16">(Ronneberger et al., 2015)</ref> that is roughly 10× faster to train and gives similar results (−0.2 dB in Gaussian noise).</p><p>1 http://r0k.us/graphics/kodak/ The architecture and training parameters are described in the supplemental material.</p><p>Convergence speed Clearly, every training example asks for the impossible: there is no way the network could succeed in transforming one instance of the noise to another. Consequently, the training loss does actually not decrease during training, and the loss gradients continue to be quite large. Why do the larger, noisier gradients not affect convergence speed? While the activation gradients are indeed noisy, the weight gradients are in fact relatively clean because Gaussian noise is independent and identically distributed (i.i.d.) in all pixels, and the weight gradients get averaged over 2 16 pixels in our fully convolutional network.</p><p>Figure 1b makes the situation harder by introducing interpixel correlation to the noise. This brown additive noise is obtained by blurring white Gaussian noise by a spatial Gaussian filter of different bandwidths and scaling to retain σ = 25. An example is shown in <ref type="figure" target="#fig_0">Figure 1b</ref>. As the correlation increases, the effective averaging of weight gradients decreases, and the weight updates become noisier. This makes the convergence slower, but even with extreme blur, the eventual quality is similar (within 0.1 dB).</p><p>Finite data and capture budget The previous studies relied on the availability of infinitely many noisy examples produced by adding synthetic noise to clean images. We now study corrupted vs. clean training data in the realistic scenario of finite data and a fixed capture budget. Our experiment setup is as follows. Let one ImageNet image with white additive Gaussian noise at σ = 25 correspond to one "capture unit" (CU). Suppose that 19 CUs are enough for a clean capture, so that one noisy realization plus the clean version (the average of 19 noisy realizations) consumes 20 CU. Let us fix a total capture budget of, say, 2000 CUs. This budget can be allocated between clean latents (N ) and noise realizations per clean latent (M ) such that N * M = 2000. In the traditional scenario, we have only 100 training pairs (N = 100, M = 20): a single noisy realization and the corresponding clean image (= average of 19 noisy images; <ref type="figure" target="#fig_0">Figure 1c</ref>, Case 1). We first observe that using the same captured data as 100 * 20 * 19 = 38000 training pairs with corrupted targets -i.e., for each latent, forming all the 19 * 20 possible noisy/clean pairs -yields notably better results (several .1s of dB) than the traditional, fixed noisy+clean pairs, even if we still only have N = 100 latents ( <ref type="figure" target="#fig_0">Figure 1c</ref>, Case 2). Second, we observe that setting N = 1000 and M = 2, i.e., increasing the number of clean latents but only obtaining two noisy realizations of each (resulting in 2000 training pairs) yields even better results (again, by several .1s of dB, <ref type="figure" target="#fig_0">Figure 1c</ref>, Case 3).</p><p>We conclude that for additive Gaussian noise, corrupted targets offer benefits -not just the same performance but better -over clean targets on two levels: both 1) seeing more realizations of the corruption for the same latent clean image, and 2) seeing more latent clean images, even if just two corrupted realizations of each, are beneficial.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Other Synthetic Noises</head><p>We will now experiment with other types of synthetic noise. The training setup is the same as described above.</p><p>Poisson noise is the dominant source of noise in photographs. While zero-mean, it is harder to remove because it is signal-dependent. We use the L 2 loss, and vary the noise magnitude λ ∈ [0, 50] during training. Training with clean targets results in 30.59 ± 0.02 dB, while noisy targets give an equally good 30.57 ± 0.02 dB, again at similar convergence speed. A comparison method <ref type="bibr" target="#b14">(Mäkitalo &amp; Foi, 2011)</ref> that first transforms the input Poisson noise into Gaussian (Anscombe transform), then denoises by BM3D, and finally inverts the transform, yields 2 dB less.</p><p>Other effects, e.g., dark current and quantization, are dominated by Poisson noise, can be made zero-mean <ref type="bibr" target="#b5">(Hasinoff et al., 2016)</ref>, and hence pose no problems for training with noisy targets. We conclude that noise-free training data is unnecessary in this application. That said, saturation (gamut clipping) renders the expectation incorrect due to removing part of the distribution. As saturation is unwanted for other reasons too, this is not a significant limitation.</p><p>Multiplicative Bernoulli noise (aka binomial noise) constructs a random mask m that is 1 for valid pixels and 0 for zeroed/missing pixels. To avoid backpropagating gradients from missing pixels, we exclude them from the loss:</p><formula xml:id="formula_6">argmin θ i (m (f θ (x i ) −ŷ i )) 2 ,<label>(7)</label></formula><p>as described by <ref type="bibr" target="#b19">Ulyanov et al. (2017)</ref> in the context of their deep image prior (DIP).</p><p>The probability of corrupted pixels is denoted with p; in our training we vary p ∈ [0.0, 0.95] and during testing p = 0.5.  Training with clean targets gives an average of 31.85 ± 0.03 dB, noisy targets (separate m for input and target) give a slightly higher 32.02 ± 0.03 dB, possibly because noisy targets effectively implement a form of dropout <ref type="bibr" target="#b18">(Srivastava et al., 2014)</ref> at the network output. DIP was almost 2 dB worse -DIP is not a learning-based solution, and as such very different from our approach, but it shares the property that neither clean examples nor an explicit model of the corruption is needed. We used the "Image reconstruction" setup as described in the DIP supplemental material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Text removal <ref type="figure">Figure 3</ref> demonstrates blind text removal. The corruption consists of a large, varying number of random strings in random places, also on top of each other, and furthermore so that the font size and color are randomized as well. The font and string orientation remain fixed.</p><p>The network is trained using independently corrupted input and target pairs.  <ref type="figure">Figure 4</ref>. For random impulse noise, the approx. mode-seeking L0 loss performs better than the mean (L2) or median (L1) seeking losses. PSNR delta from clean targets L0 L1 <ref type="figure">Figure 5</ref>. PSNR of noisy-target training relative to clean targets with a varying percentage of target pixels corrupted by RGB impulse noise. In this test a separate network was trained for each corruption level, and the graph was averaged over the KODAK dataset.</p><p>testing. In this test the mean (L 2 loss) is not the correct answer because the overlaid text has colors unrelated to the actual image, and the resulting image would incorrectly tend towards a linear combination of the right answer and the average text color (medium gray). However, with any reasonable amount of overlaid text, a pixel retains the original color more often than not, and therefore the median is the correct statistic. Hence, we use L 1 = |f θ (x) −ŷ| as the loss function. <ref type="figure">Figure 3</ref> shows an example result.</p><p>Random-valued impulse noise replaces some pixels with noise and retains the colors of others. Instead of the standard salt and pepper noise (randomly replacing pixels with black or white), we study a harder distribution where each pixel is replaced with a random color drawn from the uniform distribution [0, 1] 3 with probability p and retains its color with probability 1 − p. The pixels' color distributions are a Dirac at the original color plus a uniform distribution, with relative weights given by the replacement probability p. In this case, neither the mean nor the median yield the correct result; the desired output is the mode of the distribution (the Dirac spike). The distribution remains unimodal. For approximate mode seeking, we use an annealed version of the "L 0 loss" function defined as (|f θ (x) −ŷ| + ) γ , where = 10 −8 , where γ is annealed linearly from 2 to 0 during training. This annealing did not cause any numerical issues in our tests. The relationship of the L 0 loss and mode seeking is analyzed in the supplement.</p><p>We again train the network using noisy inputs and noisy targets, where the probability of corrupted pixels is randomized separately for each pair from <ref type="bibr">[0, 0.95]</ref>. <ref type="figure">Figure 4</ref> shows the inference results when 70% input pixels are randomized. Training with L 2 loss biases the results heavily towards gray, because the result tends towards a linear combination the correct answer and and mean of the uniform random corruption. As predicted by theory, the L 1 loss gives good results as long as fewer than 50% of the pixels are randomized, but beyond that threshold it quickly starts to bias dark and bright areas towards gray ( <ref type="figure">Figure 5</ref>). L 0 , on the other hand, shows little bias even with extreme corruptions (e.g. 90% pixels), because of all the possible pixel values, the correct answer (e.g. 10%) is still the most common.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Monte Carlo Rendering</head><p>Physically accurate renderings of virtual environments are most often generated through a process known as Monte Carlo path tracing. This amounts to drawing random sequences of scattering events ("light paths") in the scene that connect light sources and virtual sensors, and integrating the radiance carried by them over all possible paths <ref type="bibr" target="#b20">(Veach &amp; Guibas, 1995)</ref>. The Monte Carlo integrator is constructed such that the intensity of each pixel is the expectation of the random path sampling process, i.e., the sampling noise is zero-mean. However, despite decades of research into importance sampling techniques, little else can be said about the distribution. It varies from pixel to pixel, heavily depends on the scene configuration and rendering parameters, and can be arbitrarily multimodal. Some lighting effects, such as focused caustics, also result in extremely long-tailed distributions with rare, bright outliers.</p><p>All of these effects make the removal of Monte Carlo noise much more difficult than removing, e.g., Gaussian noise. On the other hand, the problem is somewhat alleviated by the possibility of generating auxiliary information that has been empirically found to correlate with the clean result during data generation. In our experiments, the denoiser input consists of not only the per-pixel luminance values, but also the average albedo (i.e., texture color) and normal vector of the surfaces visible at each pixel.</p><p>High dynamic range (HDR) Even with adequate sampling, the floating-point pixel luminances may differ from each other by several orders of magnitude. In order to construct an image suitable for the generally 8-bit display devices, this high dynamic range needs to be compressed to a fixed range using a tone mapping operator <ref type="bibr" target="#b1">(Cerdá-Company et al., 2016)</ref>. We use a variant of Reinhard's global operator <ref type="bibr" target="#b15">(Reinhard et al., 2002)</ref>:</p><formula xml:id="formula_7">T (v) = (v/(1 + v))</formula><p>1/2.2 , where v is a scalar luminance value, possibly pre-scaled with an image-wide exposure constant. This operator maps any v ≥ 0 into range 0 ≤ T (v) &lt; 1.</p><p>The combination of virtually unbounded range of luminances and the nonlinearity of operator T poses a problem. If we attempt to train a denoiser that outputs luminance values v, a standard MSE loss L 2 = (f θ (x) −ŷ) 2 will be dominated by the long-tail effects (outliers) in the targets, and training does not converge. On the other hand, if the denoiser were to output tonemapped values T (v), the nonlinearity of T would make the expected value of noisy target images E{T (v)} different from the clean training target T (E{v}), leading to incorrect predictions.</p><p>A metric often used for measuring the quality of HDR images is the relative MSE <ref type="bibr" target="#b17">(Rousselle et al., 2011)</ref>, where the squared difference is divided by the square of approximate luminance of the pixel, i.e., (f θ (x) −ŷ) 2 /(ŷ + ) 2 . However, this metric suffers from the same nonlinearity problem as comparing of tonemapped outputs. Therefore, we propose to use the network output, which tends towards the correct value in the limit, in the denominator:</p><formula xml:id="formula_8">L HDR = (f θ (x) −ŷ) 2 /(f θ (x) + 0.01) 2 .</formula><p>It can be shown that L HDR converges to the correct expected value as long as we consider the gradient of the denominator to be zero.</p><p>Finally, we have observed that it is beneficial to tone map the input image T (x) instead of using HDR inputs. The network continues to output non-tonemapped (linear-scale) luminance values, retaining the correctness of the expected value. <ref type="figure">Figure 6</ref> evaluates the different loss functions.</p><p>Denoising Monte Carlo rendered images We trained a denoiser for Monte Carlo path traced images rendered using 64 samples per pixel (spp). Our training set consisted of 860 architectural images, and the validation was done using 34 images from a different set of scenes. Three versions of the training images were rendered: two with 64 spp using different random seeds (noisy input, noisy target), and one with 131k spp (clean target). The validation images were rendered in both 64 spp (input) and 131k spp (reference) versions. All images were 960×540 pixels in size, and as mentioned earlier, we also saved the albedo and normal buffers for all of the input images. Even with such a small dataset, rendering the 131k spp clean images was a strenuous effort -for example, <ref type="figure">Figure 7d</ref> took 40 minutes to render on a high-end graphics server with 8 × NVIDIA Tesla P100 GPUs and a 40-core Intel Xeon CPU.</p><p>The average PSNR of the 64 spp validation inputs with respect to the corresponding reference images was 22.31 dB (see <ref type="figure">Figure 7a</ref> for an example). The network trained for 2000 epochs using clean target images reached an average PSNR of 31.83 dB on the validation set, whereas the similarly trained network using noisy target images gave 0.5 dB less. Examples are shown in <ref type="figure">Figure 7b</ref>,c -the training took 12 hours with a single NVIDIA Tesla P100 GPU.</p><p>At 4000 epochs, the noisy targets matched 31.83 dB, i.e., noisy targets took approximately twice as long to converge. However, the gap between the two methods had not narrowed appreciably, leading us to believe that some quality difference will remain even in the limit. This is not surprising, since the training dataset contained only a limited number of training pairs (and thus noise realizations) due to the cost of generating the clean target images, and we wanted to test both methods using matching data. That said, given that noisy targets are 2000 times faster to produce, one could trivially produce a larger quantity of them and still realize vast gains. The finite capture budget study (Section 3.1) supports this hypothesis.</p><p>Online training Since it can be tedious to collect a sufficiently large corpus of Monte Carlo images for training a generally applicable denoiser, a possibility is to train a model specific to a single 3D scene, e.g., a game level or a movie shot <ref type="bibr" target="#b2">(Chaitanya et al., 2017)</ref>. In this context, it can even be desirable to train on-the-fly while walking through the scene. In order to maintain interactive frame rates, we can afford only few samples per pixel, and thus both input and target images will be inherently noisy. <ref type="figure">Figure 8</ref> shows the convergence plots for an experiment where we trained a denoiser from scratch for the duration of 1000 frames in a scene flythrough. On an NVIDIA Titan V GPU, path tracing a single 512×512 pixel image with 8 spp took 190 ms, and we rendered two images to act as input and target. A single network training iteration with a random 256×256 pixel crop took 11.25 ms and we performed eight of them per frame. Finally, we denoised both rendered images, each taking 15 ms, and averaged the result to produce the final image shown to the user. Rendering, training and inference took 500 ms/frame. <ref type="figure">Figure 8</ref> shows that training with clean targets does not perform appreciably better than noisy targets. As rendering a single clean image takes approx. 7 minutes in this scene (resp. 190 ms for a noisy target), the quality/time tradeoff clearly favors noisy targets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Magnetic Resonance Imaging (MRI)</head><p>Magnetic Resonance Imaging (MRI) produces volumetric images of biological tissues essentially by sampling the Fourier transform (the "k-space") of the signal. Modern MRI techniques have long relied on compressed sensing (CS) to cheat the Nyquist-Shannon limit: they undersample k-space, and perform non-linear reconstruction that removes aliasing by exploiting the sparsity of the image in a suitable transform domain <ref type="bibr" target="#b11">(Lustig et al., 2008)</ref>.</p><p>We observe that if we turn the k-space sampling into a random process with a known probability density p(k) over the frequencies k, our main idea applies. In particular, we model the k-space sampling operation as a Bernoulli process where each individual frequency has a probability p(k) = e −λ|k| of being selected for acquisition. <ref type="bibr">3</ref> The frequencies that are retained are weighted by the inverse of the selection probability, and non-chosen frequencies are set to zero. Clearly, the expectation of this "Russian roulette" process is the correct spectrum. The parameter λ controls the overall fraction of k-space retained; in the following experiments, we choose it so that 10% of the samples are retained relative to a full Nyquist-Shannon sampling. The undersampled spectra are transformed to the primal image domain by the standard inverse Fourier transform. An example of an undersampled input/target picture, the corresponding fully sampled reference, and their spectra, are shown in <ref type="figure" target="#fig_5">Figure 9</ref>(a, d). Now we simply set up a regression problem of the form (6) and train a convolutional neural network using pairs of two independent undersampled imagesx andŷ of the same volume. As the spectra of the input and target are correct on expectation, and the Fourier transform is linear, we use the L 2 loss. Additionally, we improve the result slightly by enforcing the exact preservation of frequencies that are present in the input imagex by Fourier transforming the result f θ (x), replacing the frequencies with those from the input, and transforming back to the primal domain before computing the loss: the final loss reads (F −1 (Rx(F(f θ (x)))) −ŷ) 2 , where R denotes the replacement of non-zero frequencies from the input. This process is trained end-to-end.</p><p>We perform experiments on 2D slices extracted from the IXI brain scan MRI dataset. <ref type="bibr">4</ref> To simulate spectral sampling, we draw random samples from the FFT of the (already reconstructed) images in the dataset. Hence, in deviation from actual MRI samples, our data is real-valued and has the periodicity of the discrete FFT built-in. The training set contained 4936 images in 256×256 resolution from 50 subjects, and for validation we chose 500 random images from 10 different subjects. The baseline PSNR of the sparsely-sampled input images was 20.03 dB when reconstructed directly using IFFT. The network trained for 300 epochs with noisy targets reached an average PSNR of 31.10 dB on the validation data, and the network trained with clean targets reached 3 Our simplified example deviates from practical MRI in the sense that we do not sample the spectra along 1D trajectories. However, we believe that designing pulse sequences that lead to similar pseudo-random sampling characteristics is straightforward. 31.14 dB. Here the training with clean targets is similar to prior art <ref type="bibr" target="#b21">(Wang et al., 2016;</ref><ref type="bibr" target="#b10">Lee et al., 2017)</ref>. Training took 13 hours on an NVIDIA Tesla P100 GPU. <ref type="figure" target="#fig_5">Figure 9</ref>(b, c) shows an example of reconstruction results between convolutional networks trained with noisy and clean targets, respectively. In terms of PSNR, our results quite closely match those reported in recent work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Discussion</head><p>We have shown that simple statistical arguments lead to new capabilities in learned signal recovery using deep neural networks; it is possible to recover signals under complex corruptions without observing clean signals, without an explicit statistical characterization of the noise or other corruption, at performance levels equal or close to using clean target data. That clean data is not necessary for denoising is not a new observation: indeed, consider, for instance, the classic BM3D algorithm <ref type="bibr" target="#b3">(Dabov et al., 2007</ref>) that draws on self-similar patches within a single noisy image. We show that the previously-demonstrated high restoration performance of deep neural networks can likewise be achieved entirely without clean data, all based on the same generalpurpose deep convolutional model. This points the way to significant benefits in many applications by removing the need for potentially strenuous collection of clean data.</p><p>AmbientGAN (Ashish Bora, 2018) trains generative adversarial networks <ref type="bibr" target="#b4">(Goodfellow et al., 2014</ref>) using corrupted observations. In contrast to our approach, AmbientGAN needs an explicit forward model of the corruption. We find combining ideas along both paths intriguing.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Denoising performance (dB in KODAK dataset) as a function of training epoch for additive Gaussian noise. (a) For i.i.d. (white) Gaussian noise, clean and noisy targets lead to very similar convergence speed and eventual quality. (b) For brown Gaussian noise, we observe that increased inter-pixel noise correlation (wider spatial blur; one graph per bandwidth) slows convergence down, but eventual performance remains close. (c) Effect of different allocations of a fixed capture budget to noisy vs. clean examples (see text).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example results for Gaussian, Poisson, and Bernoulli noise. Our result was computed by using noisy targets -the corresponding result with clean targets is omitted because it is virtually identical in all three cases, as discussed in the text. A different comparison method is used for each noise type.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .Figure 8 .</head><label>678</label><figDesc>Figure 6. Comparison of various loss functions for training a Monte Carlo denoiser with noisy target images rendered at 8 samples per pixel (spp). In this high-dynamic range setting, our custom relative loss LHDR is clearly superior to L2. Applying a non-linear tone map to the inputs is beneficial, while applying it to the target images skews the distribution of noise and leads to wrong, visibly too dark results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. MRI reconstruction example. (a) Input image with only 10% of spectrum samples retained and scaled by 1/p. (b) Reconstruction by a network trained with noisy target images similar to the input image. (c) Same as previous, but training done with clean target images similar to the reference image. (d) Original, uncorrupted image. PSNR values refer to the images shown here, see text for averages over the entire validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc></figDesc><table>PSNR results from three test datasets KODAK, BSD300, 
and SET14 for Gaussian, Poisson, and Bernoulli noise. The com-
parison methods are BM3D, Inverse Anscombe transform (ANSC), 
and deep image prior (DIP). 

clean noisy BM3D clean noisy ANSC clean noisy DIP 
Kodak 
32.50 32.48 31.82 31.52 31.50 29.15 33.01 33.17 30.78 
BSD300 31.07 31.06 30.34 30.18 30.16 27.56 31.04 31.16 28.97 
Set14 
31.31 31.28 30.50 30.07 30.06 28.36 31.51 31.72 30.67 
Average 31.63 31.61 30.89 30.59 30.57 28.36 31.85 32.02 30.14 

Bernoulli (p=0.5) 
Gaussian (σ=25) 
Poisson (λ=30) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">https://dmitryulyanov.github.io/deep image prior</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>Bill Dally, David Luebke, Aaron Lefohn for discussions and supporting the research; NVIDIA Research staff for suggestions and discussion; Runa Lober and Gunter Sprenger for synthetic off-line training data; Jacopo Pantaleoni for the interactive renderer used in on-line training; Samuli Vuorinen for initial photography test data; Koos Zevenhoven for discussions on MRI; Peyman Milanfar for helpful comments.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">AmbientGAN: Generative models from lossy measurements. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Bora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandros</forename><forename type="middle">G</forename><surname>Dimakis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Which tone-mapping operator is the best? A comparative study of perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cerdá-Company</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Párraga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alejandro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Otazu</surname></persName>
		</author>
		<idno>abs/1601.04450</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive reconstruction of Monte Carlo image sequences using a recurrent denoising autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chakravarty</forename><forename type="middle">R</forename><surname>Chaitanya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><forename type="middle">S</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christoph</forename><surname>Schied</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Marco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Aaron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Nowrouzezahrai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aila</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timo</forename></persName>
		</author>
		<idno>98:1-98:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image denoising by sparse 3-D transform-domain collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dabov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Foi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Katkovnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2080" to="2095" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative Adversarial Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mehdi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Warde</forename><forename type="middle">-</forename><surname>Bing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sherjil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Burst photography for high dynamic range and low-light imaging on mobile cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Hasinoff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sharlet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dillon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kainz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Florian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiawen</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Levoy</surname></persName>
		</author>
		<idno>192:1-192:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robust estimation of a location parameter</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><forename type="middle">J</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Ann. Math. Statist</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="73" to="101" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Globally and locally consistent image completion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satoshi</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hiroshi</forename><surname>Ishikawa</surname></persName>
		</author>
		<idno>107:1-107:14</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jun-Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinghui</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR 2017</title>
		<meeting>CVPR 2017</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ferenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Alykhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zehan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="105" to="114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for compressed sensing MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ye</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 14th International Symposium on Biomedical Imaging</title>
		<meeting>IEEE 14th International Symposium on Biomedical Imaging</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="15" to="18" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Compressed sensing MRI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Lustig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">L</forename><surname>Donoho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juan</forename><forename type="middle">M</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pauly</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Signal Processing Magazine</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="72" to="82" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Image restoration using convolutional auto-encoders with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao-Jiao</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Yu-Bin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS</title>
		<meeting>NIPS</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A database of human segmented natural images and its application to evaluating segmentation algorithms and measuring ecological statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Malik</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="416" to="423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Optimal inversion of the Anscombe transformation in low-count Poisson image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Markku</forename><surname>Mäkitalo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Foi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Image Process</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="99" to="109" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Photographic tone reproduction for digital images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Reinhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Shirley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Ferwerda</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="267" to="276" />
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Convolutional networks for biomedical image segmentation. MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">9351</biblScope>
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adaptive sampling and reconstruction using greedy error minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabrice</forename><surname>Rousselle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Knaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<idno>159:1-159:12</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Deep image prior. CoRR, abs/1711.10925</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmitry</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Victor</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Optimally combining sampling techniques for Monte Carlo rendering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eric</forename><surname>Veach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH 95</title>
		<meeting>ACM SIGGRAPH 95</meeting>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="page" from="419" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Accelerating magnetic resonance imaging via deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE 13th International Symposium on Biomedical Imaging (ISBI)</title>
		<meeting>IEEE 13th International Symposium on Biomedical Imaging (ISBI)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="514" to="517" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On single image scaleup using sparse-representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Elad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Protter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Noise2Noise: Learning Image Restoration without Clean Data Zeyde</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="711" to="730" />
		</imprint>
	</monogr>
	<note>Proc. Curves and Surfaces: 7th International Conference</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phillip</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexei</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="649" to="666" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
