<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Revisiting IM2GPS in the Deep Learning Era</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nam</forename><surname>Vo</surname></persName>
							<email>namvo@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nathan</forename><surname>Jacobs</surname></persName>
							<email>jacobs@cs.uky.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
							<email>hays@gatech.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georgia</forename><surname>Tech</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Kentucky</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Revisiting IM2GPS in the Deep Learning Era</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In recent years, the recognition community has broadened its focus beyond object categorization to the understanding of a litany of object, scene, material, or 3D attributes. One of the most important attributes of an image is geolocation -if we know the location of a photo, we trivially know hundreds of additional attributes (any attribute for which a map exists, e.g. population density, average temperature, crime rate, elevation, distance to a McDonald's, etc.). Knowing the location of an image is also a common photo forensics task. For example, the mobile app TraffickCam collects hotel room images to locate incidents of abuse. Unlike many computer vision tasks, computational systems typically exceed the performance of humans at image geolocalization because it is hard for humans to have an accurate visual model of the entire world. given a large set of GPS-tagged images, learn to infer the GPS coordinate of a query image with unknown location.</p><p>Estimating the geolocation of an arbitrary photo is still a challenging task <ref type="figure" target="#fig_0">(Figure 1</ref>). In particular, we examine the task of predicting the location of a single photo given only the image content with no metadata. We consider this task at a global scale and attempt to estimate the GPS coordinates for any query image. For this task, localization can be considered successful if the estimated location is within a specified error threshold. Depending on the application, this threshold could be street level (1km), city level (25km), region level (200km), country level (750km), or continent level (2500km). We adopt these five levels of granularity from prior work and examine the performance of geolocalization strategies at these error thresholds.</p><p>One natural approach to the image geolocalization task would be to to treat it like an instance retrieval task and match local features from the query image (and perhaps their geometric layout) to a reference database of images with known locations <ref type="bibr" target="#b15">[16]</ref>. Such approaches work well if <ref type="bibr" target="#b0">(1)</ref> there are images in the reference database with a field of view that significantly overlaps with that of the query image and (2) if the content of the query image is well suited to local feature matching (i.e., it has distinctive manmade or geological features). Unfortunately, this is often not the case, especially for query images away from tourist destinations and dense urban areas. Therefore, it is necessary to infer location without requiring direct local-feature matching. In these cases, image geolocalization is similar to scene classification or scene attribute estimation in that a system needs to achieve a higher-level, more qualitative understanding of an image, e.g. recognizing that buildings are typical of Greek islands even though this particular island isn't in the reference database.</p><p>Im2GPS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> introduced the global geolocalization problem and used hand-crafted features from the instance recognition and scene classification literature jointly to retrieve nearest neighbors in a database of 6 million geotagged images. Im2GPS found that roughly half of successful geolocalizations are instance level matches whereas half are more qualitative matches based on shared scene attributes (geology, architecture, land cover, etc.).</p><p>More recently, PlaNet <ref type="bibr" target="#b35">[36]</ref> formulates image geolocalization as a classification task. This is done by mapping the GPS coordinate (a pair of real numbers) to a discrete class label by dividing the surface of the earth into distinct regions. PlaNet proposes a deep convolutional neural network to estimate a probability distribution over regions from raw pixel values. PlaNet not only significantly outperforms Im2GPS in terms of accuracy, it is also dramatically faster since it requires only a forward pass through a deep network instead of a nearest neighbor search through millions of image features.</p><p>Is the deep image classification formulation of PlaNet the best approach to geolocalization (as it seems to be for most other scene understanding tasks)? There are two reasons to suspect it might not be ideal -first, discretizing the Earth's surface is lossy since we are ultimately interested in a real-valued location estimate (potentially expressed through GPS coordinates). Second, and more limiting, is that a single deep network, even with tens of millions of parameters, will struggle to memorize the visual appearance of the entire Earth. An effective deep network needs to learn to do both instance matching and more qualitative scene understanding. Can contemporary deep networks implicitly 'memorize' tens of millions of photographic features necessary for the instance matching?</p><p>In this paper, we adopt the retrieval approach of Im2GPS but pair it with deep feature learning as in PlaNet. We outperform PlaNet by a significant margin -47.7% accuracy vs 37.6% for PlaNet on the Im2GPS test set with a 200km threshold . Interestingly, while we approach geolocalization as a retrieval task with learned deep features, we don't see a benefit to using embedding formulations (e.g. Siamese networks with contrastive or triplet loss) typical for retrieval tasks. Our best performance comes from training a classification network, in the spirit of PlaNet, and using its intermediate activations as our image feature.</p><p>The contributions of this study are:</p><p>• We significantly improve the state-of-the-art accuracy for global image geolocalization. Our increase in accuracy is similar in magnitude to that achieved by PlaNet <ref type="bibr" target="#b35">[36]</ref> over the hand-crafted retrieval approach of Im2GPS. We achieve this with as little as 5% of the training data used by PlaNet, and increase the gap further while using 28% as much reference data.</p><p>• Our increase in accuracy comes from changing the formulation from classification to retrieval. The benefit of retrieval in this setting is a reflection of the geolocalization problem and the nature of current deep models -the visual world is too complex for a deep model to memorize, but a retrieval approach does so trivially.</p><p>• We investigate different strategies for learning a deep feature embedding for geolocalization. Surprisingly, deep feature learning methods typically used for retrieval applications do not outperform training with a classification loss. For classification-based localization, we find that different discretization strategies also have a significant impact.</p><p>• Through extensive experimentation, we find that some training procedures lead to higher accuracy at the street scale (1km) and others at the country scale (750km). We observe a trade off between fine-scale and coarse-scale performance, the regimes traditionally approached with instance-level matching methods and scene classification methods, respectively.</p><p>Related works are discussed in the next section. We describe image geolocalization system designs in Section 3. Experiments and analysis are reported in Section 4 and we conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recent years have seen a dramatic expansion of deep learning methods for scene understanding tasks <ref type="bibr" target="#b13">[14]</ref>. Deep learning has been applied successfully to location prediction <ref type="bibr" target="#b35">[36]</ref> and other tasks related to our problem: predicting scene type <ref type="bibr" target="#b39">[40]</ref>, perceptual attributes <ref type="bibr" target="#b6">[7]</ref> such as safety, liveliness and geo-informative attributes <ref type="bibr" target="#b14">[15]</ref> like GDP, elevation.</p><p>Image retrieval using learned, deep representations is useful to a wide range of tasks such as product ranking <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12]</ref>, sketch based image retrieval <ref type="bibr" target="#b24">[25]</ref>, face recognition <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b25">26]</ref>, cross-view localization <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b33">34]</ref> and scene retrieval <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>. Distance metric learning (DML) is usually employed with a deep network, most commonly using the contrastive loss <ref type="bibr" target="#b8">[9]</ref> or triplet ranking (hinge loss)   <ref type="bibr" target="#b27">[28]</ref> followed by a global max pooling layer. Depending on the task, we append to this an output layer and the corresponding loss layer. For classification, we use a fully connected layer and Softmax-CrossEntropy loss, for retrieval, we use a DML loss. <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b34">35]</ref>. New loss functions and example mining strategies have been proposed as they play important role in the learning process <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b33">34]</ref>.</p><p>We are studying image retrieval geolocalization which has overlap with instance-level scene retrieval <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>; Since this line of work mostly focuses on instance-level matching, benchmarks designed for this task consist of popular scenes or landmarks <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b12">13]</ref> and similarity between matched images are visually recognizable (by humans or geometric verification). In this regime, with manual labeling and/or clever example mining, it is beneficial to apply distance metric learning. Techniques such as geometry verification or query expansion typically improve instance retrieval mAP, but these techniques are less useful when geolocalizing scenes that do not have instance matches.</p><p>Many previous works on image localization are at limited spatial scale (urban areas) or on special class of images (landmarks, streetview) <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b37">38]</ref>. Many approaches make used of aerial imagery for localization <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b16">17]</ref>. In <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b36">37]</ref>, images of the same scene from the ground viewpoint and overhead viewpoint are embedded in the same feature space through deep learning DML; the resulting system then does localization by image retrieval using reference database of aerial images.</p><p>Also related, to match aerial images across wide baselines, Altwaijry et al. <ref type="bibr" target="#b0">[1]</ref> propose a deep attentive architecture to classify whether two views match.</p><p>Image geolocalization at planet scale is challenging and less studied. An effort to advance this area is the placing task at MediaEval <ref type="bibr" target="#b5">[6]</ref>. Two more notable works that aim for global coverage are Im2GPS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and PlaNet <ref type="bibr" target="#b35">[36]</ref>, which we build upon.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Image Geolocalization using Deep Learning</head><p>Given a large training data of images with GPS labels, we examine two deep learning approaches for geolocalization. For both cases we use the same architecture shown in <ref type="figure" target="#fig_2">Figure 3</ref> which has been popular for landmark recognition <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b7">8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Geolocalization by classification</head><p>One approach is to formulate geolocalization as a classification problem <ref type="bibr" target="#b35">[36]</ref>: the GPS label is converted to class label by quantizing all GPS labels to a fixed number of classes, so that each class represents a physical region in the real world. The classification result then can be converted back to the GPS coordinate of the corresponding region.</p><p>PlaNet <ref type="bibr" target="#b35">[36]</ref> divides the Earth into a set of geographical cells based on image density. We derive a similar adaptive scheme: starting with a single cell of the entire world, repeatedly divide each cells along latitude or longitude whichever side is bigger (either evenly or randomly) until the number of images in each cell is smaller than a threshold t img or the physical area is smaller than a threshold t area ; these parameters define how fine the partitioning is.</p><p>To predict the location as precisely as possible, one would prefer a fine-grained partitioning (for example <ref type="bibr" target="#b35">[36]</ref>'s partitioning has 26,263 cells). However we should take into account the training data's size, the learning model's capacity and especially the localization error tolerance. We <ref type="figure">Figure 4</ref>. A visual overview of our image-retrieval approach to image geolocalization. We extract a feature from our CNN, find nearby neighbors in feature space, and estimate the GPS coordinate using either the top NN or the density. <ref type="figure">Figure 5</ref>. When performing distance metric learning, we sample images based on their distance, either in label space (geographic distance) or feature space, to an anchor image. Some example images that are close/far from an anchor image in the label space/feature space.</p><p>investigate 6 different partitionings shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Admittedly these choices are somewhat arbitrary, as we do not directly control the number of cells, nor do we try to "optimize" each partitioning. We used similar parameter to <ref type="bibr" target="#b35">[36]</ref> to obtain a fine grained partitioning (though <ref type="bibr" target="#b35">[36]</ref>'s data is ∼14 times bigger so they still have √ 14 times more cells); then we loosen the thresholds to obtain the other 5 coarser partitionings.</p><p>Multiple class labeling: We investigate the effect of using multiple partitionings simultaneously. The motivation is that different proximity information is preserved at different levels of granularity (and not the others). Moreover classification results at multiple coarse partitionings can be combined to produce a more fine grained prediction. Therefore we experiment with training multiple classification losses as these tasks are heavily correlated and benefit each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Geolocalization by image retrieval</head><p>This approach looks up images that are similar to the query images and makes use of the known locations of those images <ref type="bibr" target="#b9">[10]</ref>. This requires learning a representation for comparing images (for which we will use deep learning) and indexing a large reference database.</p><p>To learn such a representation, we employ distance metric learning (ranking/triple hinge loss, contrastive loss and similar loss functions) which requires pairs of images labeled 'similar' or 'different'. When not available, such labeling can be automatically generated using geometry verification <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b22">23]</ref> or class labels <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24]</ref>. In our case, we make use of the class label described in the previous section or directly threshold the GPS distance between the 2 images. Similar to <ref type="bibr" target="#b1">[2]</ref>, we can also match images that are not only close in the GPS label space but also close in the current feature space. Even so, with the data we are dealing with, this supervision is very weak in the sense that matched images (taken at the same location/region) are most likely not of the same or even similar scene/object ( <ref type="figure">Figure 5</ref>).</p><p>After training we use the CNN as a feature extractor and index a large dataset of reference image features. At test time, we look up the nearest neighbor (NN) of the query image in the feature space using approximate NN search and output its location <ref type="figure">(Figure 4</ref>). This approach works based on the assumption that, after learning, images close in the feature space are likely to be close in the label (GPS coordinate) space too.</p><p>k-NN density estimation: we can make use of the top k NN instead of only 1. We perform weighted kernel density estimation using each NN as a Gaussian kernel, the density at a point x in GPS coordinate space can be written as:</p><formula xml:id="formula_0">f (x) = k i=1 w i N (x; x i , σ 2 I)<label>(1)</label></formula><p>Where x i is the GPS coordinate of the i-th NN, we also weight each NN w i = s m i depending on its similarity score s i (defined to be the inverse of the distance between the query image's feature and the reference image's feature). The point with highest density is chosen as output.</p><p>Note that as k decrease, m increase or σ decreases, this output becomes the NN. These parameters can be optimized: bigger reference data allows bigger k and looser error threshold allows bigger σ. Given our dataset (described in the next section) We choose m = 10, k = 100 through validation (these parameters were not precisely tuned) and experimentally manipulate σ.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Training data: We use the Im2GPS dataset from <ref type="bibr" target="#b9">[10]</ref>. It consists of more than 6 million images collected from Flickr that are tagged with countries or states' name and also have GPS coordinates. This data is used for GPS quantization <ref type="figure" target="#fig_1">(Figure 2</ref>), training deep networks, and as retrieval reference database.</p><p>Testing data: for analysis, we construct 2 test sets; we make sure that no image from training and test data come from the same photographer.</p><p>• Im2GPS3k: 3000 images from Im2GPS. Note that this is different from the Im2GPS test set <ref type="bibr" target="#b9">[10]</ref>.</p><p>• YFCC4k: 4000 random images from the YFCC100m dataset <ref type="bibr" target="#b31">[32]</ref>. Since it is designed for general computer vision purpose, its image distribution is different from Im2GPS making this test set more challenging.</p><p>Training for classification: we train the following networks:</p><p>• Lone: We trained a network with a single classification loss corresponding to the most fine grained partition (7011 classes). This can be considered an analog of PlaNet <ref type="bibr" target="#b35">[36]</ref> at smaller scale. We also train another version L2 for the 359 ways classification loss.</p><p>• Multi: We train another classification network with 6 different losses corresponding to 6 partitions scheme described in section 3.1. Hence this network produces 6 localization outputs . We'll treat these outputs independently and evaluate the performance of each of them.</p><p>Training for retrieval: we fine-tune the model L with ranking loss (triplet hinge loss) to learn a better representation, resulting in a Ranking network. To do localization by retrieval, we experiment with different networks as feature extractor: the classification networks (L and M) and the ranking network (R).</p><p>We also evaluate two other publicly available state-ofthe-art models, NetVLAD <ref type="bibr" target="#b1">[2]</ref> and Siamac <ref type="bibr" target="#b22">[23]</ref>, which have similar architecture (VGG-conv layers), but different training data (weakly supervised Google streetview time machine and SfM landmark images hard example mining), global pooling layer (NetVLAD and R-Max) and loss function (triplet hinge loss and contrastive loss). Different from us, these models have an additional PCA &amp; whitening step. Features from all models are L2-normalized when used for retrieval.</p><p>Notation: we will use [M odel]Approach to refer to each method, where M odel can be L, L2, M, R, NetVLAD, Siamac described above, and Approach can be C (for classification), NN, kNN (for retrieval). Metric: the geolocalization accuracy is defined as the percentage of test images whose predicted location is within the error threshold from the true location. Similar to <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b35">36]</ref>, 5 error thresholds are used: 1km, 5km, 25km, 750km, 2500km corresponding to 5 levels of localization: street, city, region, country, continent.</p><p>Result: Qualitative results are shown in <ref type="figure" target="#fig_3">Figures 6 and  7</ref>. Quantitative results on two test sets are shown in <ref type="figure" target="#fig_6">Figure  8</ref>. For comparison we add a simple baseline: always outputting London, which is the region with the most images. This baseline is practically the best one can do without looking at the input image; its performance is much better than guessing a random location on the Earth.</p><p>We will ensure that our results can be replicated by sharing our datasets, source code, and trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparing classification performance</head><p>An example output of classification is in <ref type="figure" target="#fig_3">Figure 6</ref>. In the case of less ambiguous image, the network would be able to predict the correct region/cell. Since the center of the region is used, a finer partitioning will lead to a prediction <ref type="bibr">Figure 7</ref>. Example results of our geolocalization by image retrieval system (kNN, σ=4). Each row shows the input image on the left, the first few NNs on the right, together with their locations (blue *). At the end of the row we show the density result, red * denotes the predicted location and green o denotes the true location.</p><p>that is closer to the true location (top row). Though in case of the image being very ambiguous, correctly localizing it at coarser level is more likely (bottom row).</p><p>As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, the geolocalization accuracy of the 10 way classification output is quite bad, this is mostly because at this scale the Earth is under-divided. We can see that as the partitioning is finer, the localization performance at lower error threshold gets better as expected. The finegrained classification output (7011C) outperforms others at street and city level.</p><p>Most interesting, the geolocalization accuracy at coarse level gets worse if the partitioning is too fine: for example at continent level, the 80C and 359C achieve highest accuracy; At country level, the 359C and 1060C have the advantage. This seems to indicate a trade off between the accuracy at coarse and fine level, which may be a shortcoming of the partitioning in PlaNet <ref type="bibr" target="#b35">[36]</ref>. . This suggests that when training with multiple classification losses, the fine-grained one seems to help the coarse one a little, but not vice versa. <ref type="figure">Figure 7</ref> shows example image retrieval results. The NNs are similar scenes to the input image. In the case of landmarks and popular sites, they are usually instance level matches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparing retrieval performance</head><p>As shown in <ref type="figure" target="#fig_6">Figure 8</ref>, with localization by NN image retrieval, all 5 models (R, M, L, NetVLAD, Siamac) perform well and outperform the classification result at street and city level. This makes sense as these successful localizations are likely correct instance-level matches. While classification network can learn the general characteristics of each regions, it doesn't have enough capacity to 'remember' all specific instances, while the retrieval approach 'remembers' this by directly saving all reference features.</p><p>Among all 5 models, NetVLAD is the worst. Siamac is the most discriminative at street level. As a trade off, it has slightly lower performance at coarse level (country and continent). The L and M models are comparable and they perform relatively well even though they are trained for classification. Coarse partitioning classification approaches still have the advantage at country and continent scale.</p><p>Finally, using kNN-kernel density estimation improves the accuracy (here we only show [L] and [Siamac] but the changes when using other models are similar); especially at coarse scales (as σ increases) this makes retrieval competitive with the classification approach. However bigger σ can potentially lower the accuracy at fine grained level. Interestingly, we arrive at a similar trade off between fine and coarse geolocalization accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training a ranking network with GPS label</head><p>Model R (which was fine-tuned from L) doesn't produce a noticeable improvement over L or M <ref type="figure" target="#fig_6">(Figure 8)</ref>. In further investigations, we train a dozen versions of R, fine-tuned from different pretrained models and varied the way we sample/mine training examples. In all cases, little progress is observed in term of both training loss and geolocalization performance.</p><p>However when using landmark matches from <ref type="bibr" target="#b22">[23]</ref> for training instead of Im2GPS data, we observe slight improvement at street level, but worse results at other scales. This is consistent with the fact that Siamac <ref type="bibr" target="#b22">[23]</ref> is very good at street level.</p><p>Distance metric learning losses like triplet hinge loss seems to be very sensitive to noisy labels. Different from classification loss (where the label for each image is fixed during training), the "target" of each training image keep changing while they are adjusting distance from each other, usually making convergence slower.</p><p>We hypothesize that the inter-class ambiguity and intraclass diversity are too large and DML is not able to learn from GPS supervision ( <ref type="figure">Figure 5</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparing with IM2GPS and PlaNet</head><p>On the Im2GPS test set, we can directly compare Im2GPS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> and PlaNet <ref type="bibr" target="#b35">[36]</ref> with two of our models:</p><p>• The fine-grained classification network ([L] 7011C). This can be considered the equivalent of Google's PlaNet <ref type="bibr" target="#b35">[36]</ref> at smaller scale.</p><p>• kNN kernel density estimation retrieval ([L] kNN, σ=4). This can be considered the equivalent of Im2GPS approach <ref type="bibr" target="#b9">[10]</ref>, but using deep features instead of classical features.</p><p>The result is shown in table 1. Our classification network outperforms Im2GPS even though it is still not as good as PlaNet. On the other hand, our localization by deep learnt image retrieval method produces even better accuracies. This result highlights the advantage of retrieval approach for fine-grain localization.</p><p>Complexity analysis: in term of number of parameters without counting the output layers, PlaNet is 3 times bigger than our 13 layers deep VGG model. Note that PlaNet uses an Inception architecture which has been heavily designed to optimize for complexity <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b29">30]</ref> (for reference, it is 8 times bigger than 22 layers deep GoogLeNet <ref type="bibr" target="#b28">[29]</ref> and 2 times bigger than 42 layers deep InceptionV2 <ref type="bibr" target="#b29">[30]</ref>). Also PlaNet's training data has more than 90 million images and it takes 2.5 months to train on clusters (approximately 40 years of CPU time). However in term of space complexity, our image retrieval approach requires all reference features be available during testing, not just the deep network. More over, the cost of indexing and perform NN search is not negligible; though indexing needs to be done only once and in our experience the cost of approximate NN search is smaller than that of feature extraction.</p><p>Comparing to Im2GPS <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref>, deep learning feature extraction is orders of magnitudes faster than computing many classical computer vision features. Im2GPS's combined feature has more than 100k dimensions; in <ref type="bibr" target="#b10">[11]</ref> lazy learning is done for each query adding more time complexity. In contrast, our deep feature with 512 dimensions is suitable for direct comparisons in Euclidean space. Because of this, our kNN kernel density estimation is a more efficient and effective post-processing procedure than the similar kNN mean shift clustering and lazy learning in <ref type="bibr" target="#b10">[11]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Effect of retrieval reference database</head><p>One advantage of retrieval approach is that we can simply index more examples to improve the performance. To that end we collect another 22 million GPS-tagged images from the YFCC100m dataset <ref type="bibr" target="#b31">[32]</ref>, increasing our database size to a total of 28 million images. As shown in table 1 (last row), this results in better performance of [L]kNN,σ=4 on the Im2GPS test set.</p><p>We vary the reference retrieval database (Im2GPS-6 millions images, YFCC-22 millions and the combined 28 million) and show the geolocalization accuracy in table 2. The performance when using YFCC22m is actually no better than when simply using Im2GPS; though the combined database of 28 million images result in an improvement. We attribute this to the fact that the IM2GPS test set and the IM2GPS database come from the same distribution, which makes IM2GPS more useful for referencing. To quantify this, we measure the percentage of IM2GPS images among the top 1, 10, 100, 1000 nearest neighbors result, they are 53.2%, 50.1%, 44.6% and 40.1% respectively, which is quite high given that IM2GPS only constitutes 22.8% of the combined database.</p><p>Similar to result on Im2GPS3k and YFCC4k, we can change σ to optimize the accuracy at a localization level (at the expense of the others). If the system is allowed to produce different outputs at different levels, this further outperforms the result in <ref type="table" target="#tab_0">Table 1</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented a deep learning study on image geolocalization, where we experimented with several settings of image classification and image retrieval approaches adapted to this task. We do not claim technical novelty for any components of this study. Our approaches are relatively simple yet achieve state-of-the-art accuracy. In the end, the best performing models can efficiently and accurately localize at coarse level using classification, and if needed can search for instance matches using retrieval techniques.</p><p>The main goal of this paper is to investigate the effectiveness of deep learning methods for geolocalization. With the newly obtained insights, we think the following lines of future work would be important: <ref type="formula" target="#formula_0">(1)</ref> we have shown the dependency between partitioning scheme and geolocalization accuracy, which begs the question: what is the best way to partition and how can the partitioning be optimized given a particular error threshold? (2) Are GPS labels too weak a supervision for traditional deep distance metric learning? There is likely an opportunity for better weakly supervised DML to improve the geolocalization.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. This work addresses the image geolocalization problem: given a large set of GPS-tagged images, learn to infer the GPS coordinate of a query image with unknown location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. We study six schemes for discretizing geographic location. These vary from coarse to fine (10, 80, 359, 1060, 1693 and 7011 regions respectively).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our proposed CNN architecture consists of the convolutional layers of the VGG-16 network [28] followed by a global max pooling layer. Depending on the task, we append to this an output layer and the corresponding loss layer. For classification, we use a fully connected layer and Softmax-CrossEntropy loss, for retrieval, we use a DML loss.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Example results of geolocalization by image classification using different partitionings. From left to right: input images, classification result with 80, 1060 and 7011 classes respectively (lighter region means higher probability). Red * denotes the predicted location and green o denotes the true location.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>For example [M]311C refers to the 311 way classification output of model M, and [M]NN refer to the NN retrieval approach using model M as feature extractor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>[</head><label></label><figDesc>M]7011C and [L]7011C achieve similar accuracy ([L] is slightly better). However in the case of 359C, [M] is slightly better than [L2]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Geolocalization accuracy on two test sets. Note that the accuracy is presented as the top of the bars, not the length of each single color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Performance on Im2GPS test set. (Human* performance is average from 30 mturk workers over 940 trials) Street City Region Country Cont.Table 2. Performance on Im2GPS test set based on different re- trieval reference database.YFCC22m 11.8 31.2 42.2 58.7 70.0 Both(28m) 14.4 33.3 47.7 61.6 73.4 [L] kNN σ = 16Both(28m) 11.8 24.9 36.7 60.8 77.2</figDesc><table>Threshold (km) 
1 
25 
200 
750 
2500 
Human* 
3.8 
13.9 
39.3 
Im2GPS [10] 
12.0 15.0 
23.0 
47.0 
Im2GPS [11] 
02.5 
21.9 32.1 
35.4 
51.9 
PlaNet [36] 
08.4 
24.5 37.6 
53.6 
71.3 
[L] 7011C 
06.8 
21.9 34.6 
49.4 
63.7 
[L] kNN, σ=4 
12.2 
33.3 44.3 
57.4 
71.3 
... 28m database 14.4 
33.3 47.7 
61.6 
73.4 

Retrieval Database 
Stre. City Reg. Cou. Cont. 

[L] NN 

Im2GPS 
12.7 33.3 40.9 53.2 71.7 
YFCC22m 12.2 30.4 37.6 51.1 67.1 
Both(28m) 13.9 32.9 40.5 54.4 70.9 

[L] kNN 
σ = 1 

Im2GPS 
13.1 36.3 44.3 56.1 70.0 
YFCC22m 12.7 34.2 43.9 55.3 68.8 
Both(28m) 15.2 37.6 46.0 57.0 69.2 

[L] kNN 
σ = 4 

Im2GPS 
12.2 33.3 44.3 57.4 71.3 
Im2GPS 
10.6 24.9 35.4 59.5 75.9 
YFCC22m 8.4 
19.8 34.6 58.2 74.7 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: the authors were partially supported by a NSF Grant (IIS-1553116) and NSF CAREER award 1149853 to James Hays.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Learning to match aerial images with deep attentive architectures</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Altwaijry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Netvlad: Cnn architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5297" to="5307" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Ultra-wide baseline facade matching for geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sawhney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2012. Workshops and Demonstrations</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="175" to="186" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning visual similarity for product design with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">98</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The placing task at mediaeval 2015</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hauff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Van Laere</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<idno>14-15</idno>
	</analytic>
	<monogr>
		<title level="m">Ceur Workshop Proceedings 1436</title>
		<meeting><address><addrLine>Wurzen, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>CEUR</publisher>
			<date type="published" when="2015-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep learning the city: Quantifying urban perception at a global scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Hidalgo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="196" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="241" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Im2gps: estimating geographic information from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Large-scale image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimodal Location Estimation of Videos and Images</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="41" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Cross-domain image retrieval with a dual attribute-aware ranking network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1062" to="1070" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Hamming embedding and weak geometric consistency for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="304" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Predicting geoinformative attributes in large-scale image collections using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applications of Computer Vision (WACV), 2015 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="550" to="557" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Landmark classification in large-scale image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Huttenlocher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision, 2009 IEEE 12th international conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1957" to="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cross-view image geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2013 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="891" to="898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning deep representations for ground-to-aerial geolocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5007" to="5015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Hamming distance metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1061" to="1069" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Zisserman. Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition, 2007. CVPR&apos;07. IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
	<note>CVPR 2008. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric learning with adaptive density discrimination</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">The sketchy database: learning to retrieve badly drawn bunnies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sangkloy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Burnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics (TOG)</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">119</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Accurate geo-registration by ground-to-aerial image matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2nd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="525" to="532" />
		</imprint>
	</monogr>
	<note>3D Vision (3DV)</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deepface: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Yfcc100m: The new data in multimedia research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Thomee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Friedland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Elizalde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Poland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Borth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">59</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="64" to="73" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sicre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jégou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05879</idno>
		<title level="m">Particular object retrieval with integral max-pooling of cnn activations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Localizing and orienting street views using overhead imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">N</forename><surname>Vo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="494" to="509" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2014 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Planet-photo geolocation with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Weyand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="37" to="55" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Wide-area image geolocalization with aerial reference imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Workman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Souvenir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jacobs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV 2015</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Large-scale visual geo-localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hakeem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in computer vision and pattern recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Accurate image localization based on google maps street view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision-ECCV 2010</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="255" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
