<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Neural Aggregation Network for Video Face Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaolong</forename><surname>Yang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Beijing Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peiran</forename><surname>Ren</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongqing</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dong</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Wen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongdong</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">The Australian National University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gang</forename><surname>Hua</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Neural Aggregation Network for Video Face Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video face recognition has caught more and more attention from the community in recent years <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b9">10]</ref>. Compared to image-based face recognition, more information of the subjects can be exploited from the input videos, which naturally incorporate faces of the same subject in varying poses and illumination conditions. The key issue in video face recognition is to build an appropriate representation of the video face, such that it can effectively integrate the information across different frames together, maintaining beneficial while discarding noisy information. * Part of this work was done when J. Yang was an intern at MSR supervised by G. Hua.  <ref type="figure">Figure 1</ref>. Our network architecture for video face recognition. All input face images {x k } are processed by a feature embedding module with a deep CNN, yielding a set of feature vectors {f k }. These features are passed to the aggregation module, producing a single 128-dimensional vector r 1 to represent the input faces images. This compact representation is used for recognition.</p><p>One naive approach would be representing a video face as a set of frame-level face features such as those extracted from deep neural networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref>, which have dominated face recognition recently <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b40">41]</ref>. Such a representation comprehensively maintains the information across all frames. However, to compare two video faces, one needs to fuse the matching results across all pairs of frames between the two face videos. Let n be the average number of video frames, then the computational complexity is O(n 2 ) per match operation, which is not desirable especially for large-scale recognition. Besides, such a setbased representation would incur O(n) space complexity per video face example, which demands a lot of memory storage and confronts efficient indexing.</p><p>We argue that it is more desirable to come with a compact, fixed-size feature representation at the video level, irrespective of the varied length of the videos. Such a representation would allow direct, constant-time computation of the similarity or distance without the need for frame-to-frame matching. A straightforward solution might be extracting a feature at each frame and then conducting a certain type of pooling to aggregate the frame-level features together to form a video-level representation.</p><p>The most commonly adopted pooling strategies may be average and max pooling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. While these naive pooling strategies were shown to be effective in the previous works, we believe that a good pooling or aggregation strategy should adaptively weigh and combine the framelevel features across all frames. The intuition is simple: a video (especially a long video sequence) or an image set may contain face images captured at various conditions of lighting, resolution, head pose etc., and a smart algorithm should favor face images that are more discriminative (or more "memorizable") and prevent poor face images from jeopardizing the recognition.</p><p>To this end, we look for an adaptive weighting scheme to linearly combine all frame-level features from a video together to form a compact and discriminative face representation. Different from the previous methods, we neither fix the weights nor rely on any particular heuristics to set them. Instead, we designed a neural network to adaptively calculate the weights. We named our network the Neural Aggregation Network (NAN), whose coefficients can be trained through supervised learning in a normal face recognition training task without the need for extra supervision signals.</p><p>The proposed NAN is composed of two major modules that could be trained end-to-end or one by one separately. The first one is a feature embedding module which serves as a frame-level feature extractor using a deep CNN model. The other is the aggregation module that adaptively fuses the feature vectors of all the video frames together.</p><p>Our neural aggregation network is designed to inherit the main advantages of pooling techniques, including the ability to handle arbitrary input size and producing orderinvariant representations. The key component of this network is inspired by the Neural Turing Machine <ref type="bibr" target="#b11">[12]</ref> and the work of <ref type="bibr" target="#b37">[38]</ref>, both of which applied an attention mechanism to organize the input through accessing an external memory. This mechanism can take an input of arbitrary size and work as a tailor emphasizing or suppressing each input element just via a weighted averaging, and very importantly it is order independent and has trainable parameters. In this work, we design a simple network structure of two cascaded attention blocks associated with this attention mechanism for face feature aggregation.</p><p>Apart from building a video-level representation, the neural aggregation network can also serve as a subject level feature extractor to fuse multiple data sources. For example, one can feed it with all available images and videos, or the aggregated video-level features of multiple videos from the same subject, to obtain a single feature representation with fixed size. In this way, the face recognition system not only enjoys the time and memory efficiency due to the compact representation, but also exhibits superior performance, as we will show in our experiments.</p><p>We evaluated the proposed NAN for both the tasks of video face verification and identification. We observed consistent margins in three challenging datasets, including the YouTube Face dataset <ref type="bibr" target="#b41">[42]</ref>, the IJB-A dataset <ref type="bibr" target="#b17">[18]</ref>, and the Celebrity-1000 dataset <ref type="bibr" target="#b21">[22]</ref>, compared to the baseline strategies and other competing methods. Last but not least, we shall point out that our proposed NAN can serve as a general framework for learning contentadaptive pooling. Therefore, it may also serve as a feature aggregation scheme for other computer vision tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related Works</head><p>Face recognition based on videos or image sets has been actively studied in the past. This paper is concerned with the input being an orderless set of face images. Existing methods exploiting temporal dynamics will not be considered here. For set based face recognition, many previous methods have attempted to represent the set of face images with appearance subspaces or manifolds and perform recognition via computing manifold similarity or distance <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b36">37]</ref>. These traditional methods may work well under constrained settings but usually cannot handle the challenging unconstrained scenarios where large appearance variations are present.</p><p>Along a different axis, some methods build video feature representation based on local features <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b26">27]</ref>. For example, the PEP methods <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>  Recently, state-of-the-art face recognition methods has been dominated by deep convolution neural networks <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. For video face recognition, most of these methods either use pairwise frame feature similarity computation <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b30">31]</ref> or naive (average/max) frame feature pooling <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9]</ref>. This motivated us to seek for an adaptive aggregation approach.</p><p>As previously mentioned, this work is also related to the Neural Turing Machine <ref type="bibr" target="#b11">[12]</ref> and the work of <ref type="bibr" target="#b37">[38]</ref>. However, it is worth noting that while they use Recurrent Neural Networks (RNN) to handle sequential inputs/outputs, there is no RNN structure in our method. We only borrow their differentiable memory addressing/attention scheme for our feature aggregation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Neural Aggregation Network</head><p>As shown in <ref type="figure">Fig. 1</ref>, the NAN network takes a set of face images of a person as input and outputs a single feature vector as its representation for the recognition task. It is built upon a modern deep CNN model for frame feature embedding, and becomes more powerful for video face recognition by adaptively aggregating all frames in the video into a compact vector representation. <ref type="figure">Figure 2</ref>. Face images in the IJB-A dataset, sorted by their scores (values of e in Eq. 2) from a single attention block trained in the face recognition task. The faces in the top, middle and bottom rows are sampled from the faces with scores in the highest 5%, a 10% window centered at the median, and the lowest 5%, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image ID Score</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Feature embedding module</head><p>The image embedding module of our NAN is a deep Convolution Neural Network (CNN), which embeds each frame of a video to a face feature representation. To leverage modern deep CNN networks with high-end performances, in this paper we adopt the GoogLeNet <ref type="bibr" target="#b33">[34]</ref> with the Batch Normalization (BN) technique <ref type="bibr" target="#b15">[16]</ref>. Certainly, other network architectures are equally applicable here as well. The GoogLeNet produces 128-dimension image features, which are first normalized to be unit vectors then fed into the aggregation module. In the rest of this paper, we will simply refer to the employed GoogLeNet-BN network as CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Aggregation module</head><p>Consider the video face recognition task on n pairs of video face data (</p><formula xml:id="formula_0">X i , y i ) n i=1</formula><p>, where X i is a face video sequence or a image set with varying image number K i , i.e.</p><formula xml:id="formula_1">X i = {x i 1 , x i 2 , ..., x i Ki } in which x i k , k = 1, .</formula><p>.., K i is the k-th frame in the video, and y i is the corresponding subject ID of X i . Each frame x i k has a corresponding normalized feature representation f i k extracted from the feature embedding module. For better readability, we omit the upper index where appropriate in the remaining text. Our goal is to utilize all feature vectors from a video to generate a set of linear weights {a k } K k=1 , so that the aggregated feature representation becomes</p><formula xml:id="formula_2">r = k a k f k .<label>(1)</label></formula><p>In this way, the aggregated feature vector has the same size as a single face image feature extracted by the CNN. Obviously, the key of Eq. 1 is its weights {a k }. If a k ≡ 1 K , Eq. 1 will degrades to naive averaging, which is usually non-optimal as we will show in our experiments. We instead try to design a better weighting scheme.</p><p>Three main principles have been considered in designing our aggregation module. First, the module should be able to process different numbers of images (i.e. different K i 's), as the video data source varies from person to person. Second, the aggregation should be invariant to the image order -we prefer the result unchanged when the image sequence are reversed or reshuffled. This way, the aggregation module can handle an arbitrary set of image or video faces without temporal information (e.g. that collected from different Internet locations). Third, the module should be adaptive to the input faces and has parameters trainable through supervised learning in a standard face recognition training task.</p><p>Our solution is inspired by the memory attention mechanism described in <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. The idea therein is to use a neural model to read external memories through a differentiable addressing/attention scheme. Such models are often coupled with Recurrent Neural Networks (RNN) to handle sequential inputs/outputs <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b37">38]</ref>. Although an RNN structure is not needed for our purpose, its memory attention mechanism is applicable to our aggregation task. In this work, we treat the face features as the memory and cast feature weighting as a memory addressing procedure. We employ in the aggregation module the "attention blocks", to be described as follows.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.1">Attention blocks</head><p>An attention block reads all feature vectors from the feature embedding module, and generate linear weights for them. Specifically, let {f k } be the face feature vectors, then an attention block filters them with a kernel q via dot product, yielding a set of corresponding significances {e k }. They are then passed to a softmax operator to generate positive weights {a k } with k a k = 1. These two operations can be described by the following equations, respectively:</p><formula xml:id="formula_3">e k = q T f k (2) a k = exp(e k ) j exp(e j ) .<label>(3)</label></formula><p>It can be seen that our algorithm essentially selects one point inside of the convex hull spanned by all the feature vectors. One related work is <ref type="bibr" target="#b2">[3]</ref> where each face image set is approximated with a convex hull and set similarities are defined as the shortest path between two convex hulls.</p><p>In this way, the number of inputs {f k } does not affect the size of aggregation r, which is of the same dimension as a single feature f k . Besides, the aggregation result is invariant to the input order of f k : according to Eq. 1, 2, and 3, permuting f k and f k ′ has no effects on the aggregated representation r. Furthermore, an attention block is modulated by the filter kernel q, which is trainable through standard backpropagation and gradient descent.</p><p>Single attention block -Universal face feature quality measurement. We first try using one attention block for aggregation. In this case, vector q is the parameter to learn. It has the same size as a single feature f and serves as a universal prior measuring the face feature quality.</p><p>We train the network to perform video face verification (see Section 2.3 and Section 3 for details) in the IJB-A dataset <ref type="bibr" target="#b17">[18]</ref> on the extracted face features, and <ref type="figure">Figure 2</ref> shows the sorted scores of all the faces images in the dataset. It can be seen that after training, the network favors highquality face images, such as those of high resolutions and with relatively simple backgrounds. It down-weights face images with blur, occlusion, improper exposure and extreme poses. <ref type="table" target="#tab_1">Table 1</ref> shows that the network achieves higher accuracy than the average pooling baseline in the verification and identification tasks.</p><p>Cascaded two attention blocks -Content-aware aggregation. We believe a content-aware aggregation can perform even better. The intuition behind is that face image variation may be expressed differently at different geo- </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High weight Low weight</head><p>Samples from a video/image set All weights graphic locations in the feature space (i.e. for different persons), and content-aware aggregation can learn to select features that are more discriminative for the identity of the input image set. To this end, we employ two attention blocks in a cascaded and end-to-end fashion described as follows. Let q 0 be the kernel of the first attention block, and r 0 be the aggregated feature with q 0 . We adaptively compute q 1 , the kernel of the second attention block, through a transfer layer taking r 0 as the input:</p><formula xml:id="formula_4">q 1 = tanh(Wr 0 + b)<label>(4)</label></formula><p>where W and b are the weight matrix and bias vector of the neurons respectively, and tanh(x) = e x −e −x e x +e −x imposes the hyperbolic tangent nonlinearity. The feature vector r 1 generated by q 1 will be the final aggregation results. Therefore, (q 0 , W, b) are now the trainable parameters of the aggregation module.</p><p>We train the network on the IJB-A dataset again, and <ref type="table" target="#tab_1">Table 1</ref> shows that the network obtained better results than using single attention block. <ref type="figure" target="#fig_2">Figure 3</ref> shows some typical examples of the weights computed by the trained network for different videos or image sets.</p><p>Our current full solution of NAN, based on which all the remaining experimental results are obtained, adopts such a cascaded two attention block design (as per <ref type="figure">Fig. 1</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Network training</head><p>The NAN network can be trained either for face verification and identification tasks with standard configurations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1">Training loss</head><p>For verification, we build a siamese neural aggregation network structure <ref type="bibr" target="#b7">[8]</ref> with two NANs sharing weights, and minimize the average contrastive loss <ref type="bibr" target="#b12">[13]</ref></p><note type="other">: l i,j = y i,j ||r 1 i − r 1 j || 2 2 + (1−y i,j ) max(0, m − ||r 1 i − r 1 j || 2 2 ), where y i,j = 1 if the pair (i, j) is from the same identity and y i,j = 0 otherwise. The constant m is set to 2 in all our experiments.</note><p>For identification, we add on top of NAN a fullyconnected layer followed by a softmax and minimize the average classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2">Module training</head><p>The two modules can be trained either simultaneously in an end-to-end fashion, or separately one by one. The latter option is chosen in this work. Specifically, we first train the CNN on single images with the identification task, then we train the aggregation module on top of the features extracted by CNN. More details can be found in Section 3.1.</p><p>We chose this separate training strategy mainly for two reasons. First, in this work we would like to focus on analyzing the effectiveness and performance of the aggregation module with the attention mechanism. Despite the huge success of applying deep CNN in image-based face recognition task, little attention has been drawn to CNN feature aggregation to our knowledge. Second, training a deep CNN usually necessitates a large volume of labeled data. While millions of still images can be obtained for training nowadays <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31</ref>], it appears not practical to collect such amount of distinctive face videos or sets. We leave an endto-end training of the NAN as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Experiments</head><p>This section evaluates the performance of the proposed NAN network. We will begin with introducing our training details and the baseline methods, followed by reporting the results on three video face recognition datasets: the IARPA Janus Benchmark A (IJB-A) <ref type="bibr" target="#b17">[18]</ref>, the YouTube Face dataset <ref type="bibr" target="#b41">[42]</ref>, and the Celebrity-1000 dataset <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Training details</head><p>As mentioned in Section 2.3, two networks are trained separately in this work. To train the CNN, we use about 3M face images of 50K identities crawled from the Internet to perform image-based identification. The faces are detected using the JDA method <ref type="bibr" target="#b4">[5]</ref>, and aligned with the LBF method <ref type="bibr" target="#b28">[29]</ref>. The input image size is 224x224. After training, the CNN is fixed and we focus on analyzing the effectiveness of the neural aggregation module.</p><p>The aggregation module is trained on each video face dataset we tested on with standard backpropagation and an RMSProp solver <ref type="bibr" target="#b35">[36]</ref>. An all-zero parameter initialization is used, i.e., we start from average pooling. The batch size, learning rate, and iteration are tuned for each dataset. As the network is quite simple and image features are compact (128-d), the training process is quite efficient: training on 5K video pairs with ∼1M images in total only takes less than 2 minutes on a CPU of a desktop PC.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Baseline methods</head><p>Since our goal is compact video face representation, we compare the results with simple aggregation strategies such as average pooling. We also compare with some set-toset similarity measurements leveraging pairwise comparison on the image level. To keep it simple, we simply use the L 2 feature distances for face recognition (all features are normalized), although it is possible to combine with an extra metric learning or template adaption technique <ref type="bibr" target="#b9">[10]</ref> to further boost the performance on each dataset.</p><p>In the baseline methods, CNN+Min L 2 , CNN+Max L 2 , CNN+Mean L 2 and CNN+SoftMin L 2 measure the similarity of two video faces based on the L 2 feature distances of all frame pairs. They necessitate storing all image features of a video, i.e., with O(n) space complexity. The first three use the minimum, maximum and mean pairwise distance respectively, thus having O(n 2 ) complexity for similarity computation. CNN+SoftMin L 2 corresponds to the SoftMax similarity score advocated in some works such as <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b0">1]</ref>. It has O(m·n 2 ) complexity for computation <ref type="bibr" target="#b0">1</ref> . CNN+MaxPool and CNN+AvePool are respectively max-pooling and average-pooling along each feature dimension for aggregation. These two methods as well as our NAN produce a 128-d feature representation for each video and compute the similarity in O(1) time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Results on IJB-A dataset</head><p>The IJB-A dataset <ref type="bibr" target="#b17">[18]</ref> contains face images and videos captured from unconstrained environments. It features full pose variation and wide variations in imaging conditions  <ref type="bibr" target="#b38">[39]</ref> 0  thus is very challenging. There are 500 subjects with 5,397 images and 2,042 videos in total and 11.4 images and 4.2 videos per subject on average. We detect the faces with landmarks using STN <ref type="bibr" target="#b3">[4]</ref> face detector, and then align the face image with similarity transformation. In this dataset, each training and testing instance is called a 'template', which comprises 1 to 190 mixed still images and video frames. Since one template may contain multiple medias and the dataset provides the media id for each image, another possible aggregation strategy is first aggregating the frame features in each media then the media features in the template <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b29">30]</ref>. This strategy is also tested in this work with CNN+AvePool and our NAN. Note that media id may not be always available in practice.</p><formula xml:id="formula_5">0.8 1 False Positive Identification Rate Max L 2 Min L 2 Mean L 2 SoftMin L 2 MaxPool AvePool AvePool † NAN NAN † False Positive Rate</formula><p>We test the proposed method on both the 'compare' protocol for 1:1 face verification and the 'search' protocol for 1:N face identification. For verification, the true accept rates (TAR) vs. false positive rates (FAR) are reported. For identification, the true positive identification rate (TPIR) vs. false positive identification rate (TPIR) and the Rank-N accuracies are reported. <ref type="table" target="#tab_2">Table 2</ref> presents the numerical results of different methods, and <ref type="figure" target="#fig_4">Figure 4</ref> shows the receiver operating characteristics (ROC) curves for verification as well as the cumulative match characteristic (CMC) and decision error trade-off (DET) curves for identification. The metrics are calculated according to <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26]</ref> on the 10 splits.</p><p>In general, the CNN+Max L 2 , CNN+Min L 2 and CNN+MaxPool perform worst among the baseline methods.</p><p>CNN+SoftMin L 2 performs slightly better than CNN+MaxPool. The use of media id significantly improves <ref type="table">Table 3</ref>. Verification accuracy comparison of state-of-the-art methods, our baselines and NAN network on the YTF dataset.</p><p>Method Accuracy (%) AUC LM3L <ref type="bibr" target="#b14">[15]</ref> 81.3 ± 1.2 89.3 DDML(combined) <ref type="bibr" target="#b13">[14]</ref> 82.3 ± 1.5 90.1 EigenPEP <ref type="bibr" target="#b20">[21]</ref> 84.8 ± 1.4 92.6 DeepFace-single <ref type="bibr" target="#b34">[35]</ref> 91.4 ± 1.1 96.3 DeepID2+ <ref type="bibr" target="#b32">[33]</ref> 93.2 ± 0.2 -Wen et al. <ref type="bibr" target="#b40">[41]</ref> 94.9 -FaceNet <ref type="bibr" target="#b30">[31]</ref> 95.12 ± 0.39 -VGG-Face <ref type="bibr" target="#b27">[28]</ref> 97  <ref type="figure" target="#fig_2">Figure 3</ref> has shown some typical examples of the weighting results. NAN exhibits the ability to choose high-quality and more discriminative face images while repelling poor face images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Results on YouTube Face dataset</head><p>We then test our method on the YouTube Face (YTF) dataset <ref type="bibr" target="#b41">[42]</ref> which is designed for unconstrained face verification in videos. It contains 3,425 videos of 1,595 different people, and the video lengths vary from 48 to 6,070 frames with an average length of 181.3 frames. Ten folds of 500 video pairs are available, and we follow the standard verification protocol to report the average accuracy with crossvalidation. We again use the STN and similarity transformation to align the face images.</p><p>The results of our NAN, its baselines, and other methods are presented in <ref type="table">Table 3</ref>, with their ROC curves shown in <ref type="figure" target="#fig_5">Fig. 5</ref>. It can be seen that the NAN again outperforms all its </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>High weight Low weight</head><p>Samples from a video All weights <ref type="figure">Figure 6</ref>. Typical examples on the YTF dataset showing the weights of the video frames computed by our NAN. In each row, five frames are sampled from a video and sorted based on their weights (numbers in the rectangles); the rightmost bar chart shows the sorted weights of all the frames (heights scaled).</p><p>baselines. The gaps between NAN and the best-performing baselines are smaller compared to the results on IJB-A. This is because the face variations in this dataset are relatively small (compare the examples in <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_2">Fig. 3</ref>), thus no much beneficial information can be extracted compared to naive average pooling or computing mean L 2 distances.</p><p>Compared to previous methods, our NAN achieves a mean accuracy of 95.72%, reducing the error of FaceNet by 12.3%. Note that FaceNet is also based on a GoogLeNet style network, and the average similarity of all pairs of 100 frames in each video (i.e., 10K pairs) was used <ref type="bibr" target="#b30">[31]</ref>. To our knowledge, only the VGG-Face <ref type="bibr" target="#b27">[28]</ref> achieves an accuracy (97.3%) higher than ours. However, that result is based on a further discriminative metric learning on YTF, without which the accuracy is only 91.5% <ref type="bibr" target="#b27">[28]</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Results on Celebrity-1000 dataset</head><p>The Celebrity-1000 dataset <ref type="bibr" target="#b21">[22]</ref> is designed to study the unconstrained video-based face identification problem. It contains 159,726 video sequences of 1,000 human subjects, with 2.4M frames in total (∼15 frames per sequence). We use the provided 5 facial landmarks to align the face images. Two types of protocols -open-set and close-set -exist on this dataset. More details about the protocols and the dataset can be found in <ref type="bibr" target="#b21">[22]</ref>.</p><p>Close-set tests. For the close-set protocol, we first train the network on the video sequences with the identification loss. We take the FC layer output values as the scores and the subject with the maximum score as the result. We also train a linear classifier for CNN+AvePool to classify each video feature. As the features are built on video sequences, we call this approach 'VideoAggr' to distinguish it from another approach to be described next. Each subject in the dataset has multiple video sequences, thus we can build a single representation for a subject by aggregating all available images in all the training (gallery) video sequences. We call this approach 'SubjectAggr'. This way, the linear classifier can be bypassed, and identification can be achieved simply by comparing the feature L 2 distances.</p><p>The results are presented in <ref type="table" target="#tab_4">Table 4</ref>. Note that <ref type="bibr" target="#b21">[22]</ref> and <ref type="bibr" target="#b20">[21]</ref> are not using deep learning and no deep network based method reported result on this dataset. So we mainly compare with our baselines in the following. It can be seen from <ref type="table" target="#tab_4">Table 4</ref> and <ref type="figure" target="#fig_6">Fig. 7 (a)</ref> that NAN consistently outperforms the baseline methods for both 'VideoAggr' and 'SubjectAggr'. Significant improvements upon the baseline are achieved for the 'SubjectAggr' approach. It is interesting to see that, 'SubjectAggr' leads to a clear performance drop for CNN+AvePool compared to its 'VideoAggr'. This indicates that the naive aggregation gets even worse when applied on the subject level with multiple videos. However, our NAN can benefit from 'SubjectAggr', yielding results consistently better than or on par with the 'VideoAggr' approach and delivers a considerable accuracy boost compared to the baseline. This suggests our NAN works quite well on handling large data variations.  Open-set tests. We then test our NAN with the close-set protocol. We first train the network on the provided training video sequences. In the testing stage, we take the 'SubjectAggr' approach described before to build a highly-compact face representation for each gallery subject. Identification is perform simply by comparing the L 2 distances between aggregated face representations. The results in both <ref type="table" target="#tab_5">Table 5</ref> and <ref type="figure" target="#fig_6">Fig. 7 (b)</ref> show that our NAN significantly reduces the error of the baseline CNN+AvePool. This again suggests that in the presence of large face variances, the widely used strategies such as average-pooling aggregation and the pairwise distance computation are far from optimal. In such cases, our learned NAN model is clearly more powerful, and the aggregated feature representation by it is more favorable for the video face recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Conclusions</head><p>We have presented a Neural Aggregation Network for video face representation and recognition. It fuses all input frames with a set of content adaptive weights, resulting in a compact representation that is invariant to the input frame order. The aggregation scheme is simple with small computation and memory footprints, but can generate quality face representations after training. The proposed NAN can be used for general video or set representation, and we plan to apply it to other vision tasks in our future work.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Typical examples showing the weights of the images in the image sets computed by our NAN. In each row, five faces images are sampled from an image set and sorted based on their weights (numbers in the rectangles); the rightmost bar chart shows the sorted weights of all the images in the set (heights scaled).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>0</head><label>0</label><figDesc>.514 ± 0.060 0.733 ± 0.034 0.895 ± 0.013 0.383 ± 0.063 0.613 ± 0.032 0.820 ± 0.024 0.929 ± 0.013 - DCNN manual +metric[7] - 0.787 ± 0.043 0.947 ± 0.011 - - 0.852 ± 0.018 0.937 ± 0.010 0.954 ± 0.007 Triplet Similarity [30] 0.590 ± 0.050 0.790 ± 0.030 0.945 ± 0.002 0.556±0.065 * 0.754±0.014 * 0.880±0.015 * 0.95 ± 0.007 0.974±0.005 * Pose-Aware Models [23] 0.652 ± 0.037 0.826 ± 0.838 ± 0.042 0.967 ± 0.009 0.577±0.094 * 0.790±0.033 * 0.903 ± 0.012 0.965 ± 0.008 0.977 ± 0.007 Masi et al.805±0.030 * - 0.461±0.077 * 0.670±0.031 * 0.913±0.011 * - 0.981±0.005 * Template Adaptation[10] 0.836 ± 0.027 0.939 ± 0.013 0.979 ± 0.004 0.774 ± 0.049 0.882 ± 0.016 0.928 ± 0.010 0.977 ± 0.004 0.986 ± 0.003 CNN+Max L 2 0.202 ± 0.029 0.345 ± 0.025 0.601 ± 0.024 0.149 ± 0.033 0.258 ± 0.026 0.429 ± 0.026 0.632 ± 0.033 0.722 ± 0.030 CNN+Min L 2 0.038 ± 0.008 0.144 ± 0.073 0.972 ± 0.006 0.026 ± 0.009 0.293 ± 0.175 0.853 ± 0.012 0.903 ± 0.010 0.924 ± 0.009 CNN+Mean L 2 0.688 ± 0.080 0.895 ± 0.016 0.978 ± 0.004 0.514 ± 0.116 0.821 ± 0.040 0.916 ± 0.012 0.973 ± 0.005 0.980 ± 0.004 CNN+SoftMin L 2 0.697 ± 0.085 0.904 ± 0.015 0.978 ± 0.004 0.500 ± 0.134 0.831 ± 0.039 0.919 ± 0.010 0.973 ± 0.005 0.981 ± 0.004 CNN+MaxPool 0.202 ± 0.029 0.345 ± 0.025 0.601 ± 0.024 0.079 ± 0.005 0.179 ± 0.020 0.757 ± 0.025 0.911 ± 0.013 0.945 ± 0.009 CNN+AvePool 0.771 ± 0.064 0.913 ± 0.014 0.977 ± 0.004 0.634 ± 0.109 0.879 ± 0.023 0.931 ± 0.011 0.972 ± 0.005 0.979 ± 0.004 CNN+AvePool † 0.856 ± 0.021 0.935 ± 0.010 0.978 ± 0.004 0.793 ± 0.044 0.909 ± 0.011 0.951 ± 0.005 0.976 ± 0.004 0.984 ± 0.004 NAN 0.860 ± 0.012 0.933 ± 0.009 0.979 ± 0.004 0.804 ± 0.036 0.909 ± 0.013 0.954 ± 0.007 0.978 ± 0.004 0.984 ± 0.003 NAN † 0.881 ± 0.011 0.941 ± 0.008 0.978 ± 0.003 0.817 ± 0.041 0.917 ± 0.009 0.958 ± 0.005 0.980 ± 0.005 0.986 ±</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Average ROC (Left), CMC (Middle) and DET (Right) curves of the NAN and the baselines on the IJB-A dataset over 10 splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Average ROC curves of different methods and our NAN on the YTF dataset over the 10 splits.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. The CMC curves of different methods on Celebrity 1000.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>take a part-based represen- tation by extracting and clustering local features. The Video</figDesc><table>Fisher Vector Faces (VF 
2 ) descriptor [27] uses Fisher Vec-
tor coding to aggregate local features across different video 
frames together to form a video-level representation. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Performance comparison on the IJB-A dataset. TAR/FAR: True/False Accept Rate for verification. TPIR/FPIR: True/False Positive Identification Rate for identification.</figDesc><table>1:1 Verification 1:N Identification 
TAR@FAR of: TPIR@FPIR of: 
Method 
0.001 0.01 
0.01 
0.1 

CNN+AvgPool 
0.771 0.913 
0.634 0.879 
NAN single attention 
0.847 0.927 
0.778 0.902 
NAN cascaded attention 0.860 0.933 
0.804 0.909 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance evaluation on the IJB-A dataset. For verification, the true accept rates (TAR) vs. false positive rates (FAR) are re- ported. For identification, the true positive identification rate (TPIR) vs. false positive identification rate (TPIR) and the Rank-N accuracies are presented. ( † : first aggregating the images in each media then aggregate the media features in a template.</figDesc><table>*  : results cited from [10].) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>It has a same verification TAR at FAR=0.1 and identifica- tion Rank-10 CMC as the state-of-the-art method of [10], but outperforms it on all other metrics (e.g. 0.881 vs. 0.836 TARs at FAR=0.01, 0.817 vs. 0.774 TPIRs at FPIR=0.01 and 0.958 vs. 0.928 Rank-1 accuracy).</figDesc><table>.3 
-

CNN+Max. L2 
91.96 ± 1.1 
97.4 
CNN+Min. L2 
94.96 ± 0.79 
98.5 
CNN+Mean L2 
95.30 ± 0.74 
98.7 
CNN+SoftMin L2 
95.36 ± 0.77 
98.7 
CNN+MaxPool 
88.36 ± 1.4 
95.0 
CNN+AvePool 
95.20 ± 0.76 
98.7 

NAN 
95.72 ± 0.64 
98.8 

the performance of CNN+AvePool, but gives a relatively 
small boost to NAN. We believe NAN already has the ro-
bustness to templates dominated by poor images from a few 
media. Without the media aggregation, NAN outperforms 
all its baselines by appreciable margins, especially on the 
low FAR cases. For example, in the verification task, the 
TARs of our NAN at FARs of 0.001 and 0.01 are respec-
tively 0.860 and 0.933, reducing the errors of the best results 
from its baselines by about 39% and 23%, respectively. 
To our knowledge, with the media aggregation our NAN 
achieves top performances compared to previous methods. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Identification performance (rank-1 accuracies, %) on the Celebrity-1000 dataset for the close-set tests.</figDesc><table>Number of Subjects 
Method 
100 
200 
500 
1000 

MTJSR [22] 
50.60 40.80 35.46 30.04 
Eigen-PEP [21] 
50.60 45.02 39.97 31.94 
CNN+Mean L2 
85.26 77.59 74.57 67.91 
CNN+AvePool -VideoAggr 86.06 82.38 80.48 74.26 
CNN+AvePool -SubjectAggr 84.46 78.93 77.68 73.41 
NAN -VideoAggr 
88.04 82.95 82.27 76.24 
NAN -SubjectAggr 
90.44 83.33 82.27 77.17 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Identification performance (rank-1 accuracies, %) on the Celebrity-1000 dataset for the open-set tests.</figDesc><table>Number of Subjects 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">m is the number of scaling factor β used (see [24] for details). We tested 20 combinations of (negative) β's, including single [1] or multiple values [23, 24] and report the best results obtained.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments GH was partly supported by NSFC Grant 61629301. HL's work was supported in part by Australia ARC Centre of Excellence for Robotic Vision (CE140100016) and by CSIRO Data61.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Face recognition using deep multi-pose representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Abdalmageed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lekust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Face recognition with image sets using manifold density divergence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="581" to="588" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Face recognition based on image sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cevikalp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Supervised transformer network for efficient face detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="122" to="138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Joint cascade face detection and alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="109" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Unconstrained face verification using deep cnn features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">An end-to-end system for unconstrained face verification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">One-to-many face recognition with bilinear cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learnedmiller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Winter Conference on Applications of Computer Vision (WACV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Template adaptation for face verification and identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Crosswhite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Byrne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stauffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.03958</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fusing robust face region descriptors via multiple metric learning for face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3554" to="3561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Neural turing machines. CoRR, abs/1410</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wayne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">5401</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Discriminative deep metric learning for face verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large margin multimetric learning for face and kinship verification in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-P</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="252" to="267" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Boosted manifold principal angles for image set-based recognition. Pattern Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Arandjelović</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="page" from="2475" to="2484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Pushing the frontiers of unconstrained face detection and recognition: Iarpa janus benchmark a</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">F</forename><surname>Klare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Taborsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blanton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Allen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Burge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Videobased face recognition using probabilistic appearance manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-C</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Probabilistic elastic matching for pose variant face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3499" to="3506" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Eigen-PEP for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="17" to="33" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Toward largepopulation face identification in unconstrained videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1874" to="1884" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pose-aware face recognition in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Rawls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natarajan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Do we really need to collect millions of faces for effective face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Masi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">T</forename><surname>Trãn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leksut</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Medioni</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Volume structured ordinal features with background similarity measure for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mendez-Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Martinez-Diaz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Biometrics (ICB)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Face recognition vendor test (FRVT) performance of automated gender classification algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ngan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grother</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Technical Report NIST IR 8052. National Institute of Standards and Technology</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A compact and discriminative face track descriptor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1693" to="1700" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Triplet probabilistic embedding for face verification and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alavi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.05417</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">FaceNet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">End-to-end memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2440" to="2448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deeply learned face representations are sparse, selective, and robust</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">DeepFace: Closing the gap to human-level performance in face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1701" to="1708" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">RMSProp: Divide the gradient by a running average of its recent magnitude</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Statistical computations on grassmann and stiefel manifolds for image and video-based recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veeraraghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2273" to="2286" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Order matters: sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Face search at scale: 80 million gallery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Otto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Jain</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.07242</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Manifold-manifold distance with application to face recognition based on image set</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative feature learning approach for deep face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Face recognition in unconstrained videos with matched background similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hassner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Maoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="529" to="534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">The SVM-Minus similarity score for video face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Levy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3523" to="3530" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
