<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dual Discriminator Generative Adversarial Nets</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tu</forename><forename type="middle">Dinh</forename><surname>Nguyen</surname></persName>
							<email>tu.nguyen@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Le</surname></persName>
							<email>trung.l@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><surname>Vu</surname></persName>
							<email>hungv@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Phung</surname></persName>
							<email>dinh.phung@deakin.edu.au</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Centre for Pattern Recognition and Data Analytics</orgName>
								<orgName type="institution">Deakin University</orgName>
								<address>
									<settlement>Geelong</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dual Discriminator Generative Adversarial Nets</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose in this paper a novel approach to tackle the problem of mode collapse encountered in generative adversarial network (GAN). Our idea is intuitive but proven to be very effective, especially in addressing some key limitations of GAN. In essence, it combines the Kullback-Leibler (KL) and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We term our method dual discriminator generative adversarial nets (D2GAN) which, unlike GAN, has two discriminators; and together with a generator, it also has the analogy of a minimax game, wherein a discriminator rewards high scores for samples from data distribution whilst another discriminator, conversely, favoring data from the generator, and the generator produces data to fool both two discriminators. We develop theoretical analysis to show that, given the maximal discriminators, optimizing the generator of D2GAN reduces to minimizing both KL and reverse KL divergences between data distribution and the distribution induced from the data generated by the generator, hence effectively avoiding the mode collapsing problem. We conduct extensive experiments on synthetic and real-world large-scale datasets (MNIST, CIFAR-10, STL-10, ImageNet), where we have made our best effort to compare our D2GAN with the latest state-of-the-art GAN's variants in comprehensive qualitative and quantitative evaluations. The experimental results demonstrate the competitive and superior performance of our approach in generating good quality and diverse samples over baselines, and the capability of our method to scale up to ImageNet database.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative models are a subarea of research that has been rapidly growing in recent years, and successfully applied in a wide range of modern real-world applications (e.g., see chapter 20 in <ref type="bibr" target="#b8">[9]</ref>). Their common approach is to address the density estimation problem where one aims to learn a model distribution p model that approximates the true, but unknown, data distribution p data . Methods in this approach deal with two fundamental problems. First, the learning behaviors and performance of generative models depend on the choice of objective functions to train them <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">15]</ref>. The most widely-used objective, considered the de-facto standard one, is to follow the principle of maximum likelihood estimate that seeks model parameters to maximize the likelihood of training data. This is equivalent to minimizing the Kullback-Leibler (KL) divergence between data and model distributions: D KL (p data p model ). It has been observed that this minimization tends to result in p model that covers multiple modes of p data , but may produce completely unseen and potentially undesirable samples <ref type="bibr" target="#b29">[29]</ref>. By contrast, another approach is to swap the arguments and instead, minimize: D KL (p model p data ), which is usually referred to as the reverse KL divergence <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b29">29]</ref>. It is observed that optimization towards the reverse KL divergence criteria mimics the mode-seeking process where p model concentrates on a single mode of p data while ignoring other modes, known as the problem of mode collapse. These behaviors are well-studied in <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>.</p><p>The second problem is the choice of formulation for the density function of p model <ref type="bibr" target="#b8">[9]</ref>. One might choose to define an explicit density function, and then straightforwardly follow maximum likelihood framework to estimate the parameters. Another idea is to estimate the data distribution using an implicit density function, without the need for analytical forms of p model (e.g., see <ref type="bibr" target="#b10">[11]</ref> for further discussions). One of the most notably pioneered class of the latter is the generative adversarial network (GAN) <ref type="bibr" target="#b9">[10]</ref>, an expressive generative model that is capable of producing sharp and realistic images for natural scenes. Different from most generative models that maximize data likelihood or its lower bound, GAN takes a radical approach that simulates a game between two players: a generator G that generates data by mapping samples from a noise space to the input space; and a discriminator D that acts as a classifier to distinguish real samples of a dataset from fake samples produced by the generator G. Both G and D are parameterized via neural networks, thus this method can be categorized into the family of deep generative models or generative neural models <ref type="bibr" target="#b8">[9]</ref>.</p><p>The optimization of GAN formulates a minimax problem, wherein given an optimal D, the learning objective turns into finding G that minimizes the Jensen-Shannon divergence (JSD):</p><formula xml:id="formula_0">D JS (p data p model ).</formula><p>The behavior of JSD minimization has been empirically proven to be more similar to reverse KL than to KL divergence <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">15]</ref>. This, however, leads to the aforementioned issue of mode collapse, which is indeed a notorious failure of GAN <ref type="bibr" target="#b10">[11]</ref> where the generator only produces similarly looking images, yielding a low entropy distribution with poor variety of samples.</p><p>Recent attempts have been made to solve the mode collapsing problem by improving the training of GAN. One idea is to use the minibatch discrimination trick <ref type="bibr" target="#b27">[27]</ref> to allow the discriminator to detect samples that are unusually similar to other generated samples. Although this heuristics helps to generate visually appealing samples very quickly, it is computationally expensive, thus normally used in the last hidden layer of discriminator. Another approach is to unroll the optimization of discriminator by several steps to create a surrogate objective for the update of generator during training <ref type="bibr" target="#b19">[20]</ref>. The third approach is to train many generators that discover different modes of the data <ref type="bibr" target="#b13">[14]</ref>. Alternatively, around the same time, there are various attempts to employ autoencoders as regularizers or auxiliary losses to penalize missing modes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b30">30]</ref>. These models can avoid the mode collapsing problem to a certain extent, but at the cost of computational complexity with the exception of DFM in <ref type="bibr" target="#b31">[31]</ref>, rendering them unscalable up to ImageNet, a large-scale and challenging visual dataset.</p><p>Addressing these challenges, we propose a novel approach to both effectively avoid mode collapse and efficiently scale up to very large datasets (e.g., ImageNet). Our approach combines the KL and reverse KL divergences into a unified objective function, thus it exploits the complementary statistical properties from these divergences to effectively diversify the estimated density in capturing multi-modes. We materialize our idea using GAN's framework, resulting in a novel generative adversarial architecture containing three players: a discriminator D 1 that rewards high scores for data sampled from p data rather than generated from the generator distribution p G whilst another discriminator D 2 , conversely, favoring data from p G rather p data , and a generator G that generates data to fool both two discriminators. We term our proposed model dual discriminator generative adversarial network (D2GAN).</p><p>It turns out that training D2GAN shares the same minimax problem as in GAN, which can be solved by alternatively updating the generator and discriminators. We provide theoretical analysis showing that, given G, D 1 and D 2 with enough capacity, i.e., in the nonparametric limit, at the optimal points, the training criterion indeed results in the minimal distance between data and model distribution with respect to both their KL and reverse KL divergences. This helps the model place fair distribution of probability mass across the modes of the data generating distribution, thus allowing one to recover the data distribution and generate diverse samples using the generator in a single shot. In addition, we further introduce hyperparameters to stabilize the learning and control the effect of each divergence.</p><p>We conduct extensive experiments on one synthetic dataset and four real-world large-scale datasets (MNIST, CIFAR10, STL-10, ImageNet) of very different nature. Since evaluating generative models is notoriously hard <ref type="bibr" target="#b29">[29]</ref>, we have made our best effort to adopt a number of evaluation metrics from literature to quantitatively compare our proposed model with the latest state-of-the-art baselines whenever possible. The experimental results reveal that our method is capable of improving the diversity while keeping good quality of generated samples. More importantly, our proposed model can be scaled up to train on the large-scale ImageNet database, obtain a competitive variety score and generate reasonably good quality images.</p><p>In short, our main contributions are: (i) a novel generative adversarial model that encourages the diversity of samples produced by the generator; (ii) a theoretical analysis to prove that our objective is optimized towards minimizing both KL and reverse KL divergence and has a global optimum where p G = p data ; and (iii) a comprehensive evaluation on the effectiveness of our proposed method using a wide range of quantitative criteria on large-scale datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Generative Adversarial Nets</head><p>We first review the generative adversarial network (GAN) that was introduced in <ref type="bibr" target="#b9">[10]</ref> to formulate a game of two players: a discriminator D and a generator G. The discriminator, D (x), takes a point x in data space and computes the probability that x is sampled from data distribution P data , rather than generated by the generator G. At the same time, the generator first maps a noise vector z drawn from a prior P (z) to the data space, obtaining a sample G (z) that resembles the training data, and then uses this sample to challenge the discriminator. The mapping G (z) induces a generator distribution P G in data domain with probability density function p G (x). Both G and D are parameterized by neural networks (see <ref type="figure" target="#fig_1">Fig. 1a</ref> for an illustration) and learned by solving the following minimax optimization:</p><formula xml:id="formula_1">min G max D J (G, D) = E x∼P data (x) [log (D (x))] + E z∼Pz [log (1 − D (G (z)))]</formula><p>The learning follows an iterative procedure wherein the discriminator and generator are alternatively updated. Given a fixed G, the maximization subject to D results in the optimal discriminator</p><formula xml:id="formula_2">D (x) = pdata(x) pdata(x)+p G (x)</formula><p>, whilst given this optimal D , the minimization of G turns into minimizing the Jensen-Shannon (JS) divergence between the data and model distributions: D JS (P data P G ) <ref type="bibr" target="#b9">[10]</ref>. At the Nash equilibrium of a game, the model distribution recovers the data distribution exactly: P G = P data , thus the discriminator D now fails to differentiate real or fake data as D (x) = 0.5, ∀x.  Since the JS divergence has been empirically proven to have the same nature as that of the reverse KL divergence <ref type="bibr" target="#b29">[29,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b10">11]</ref>, GAN suffers from the model collapsing problem, and thus its generated data samples have low level of diversity <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b4">5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dual Discriminator Generative Adversarial Nets</head><p>To tackle GAN's problem of mode collapse, in what follows we present our main contribution of a framework that seeks an approximated distribution to effectively cover many modes of the multimodal data. Our intuition is based on GAN, but we formulate a three-player game that consists of two different discriminators D 1 and D 2 , and one generator G. Given a sample x in data space, D 1 (x) rewards a high score if x is drawn from the data distribution P data , and gives a low score if generated from the model distribution P G . In contrast, D 2 (x) returns a high score for x generated from P G whilst giving a low score for a sample drawn from P data . Unlike GAN, the scores returned by our discriminators are values in R + rather than probabilities in [0, 1]. Our generator G performs a similar role to that of GAN, i.e., producing data mapped from a noise space to synthesize the real data and then fool both two discriminators D 1 and D 2 . All three players are parameterized by neural networks wherein D 1 and D 2 do not share their parameters. We term our proposed model dual discriminator generative adversarial network (D2GAN). <ref type="figure" target="#fig_1">Fig. 1b</ref> shows an illustration of D2GAN.</p><p>More formally, D 1 , D 2 and G now play the following three-player minimax optimization game: min</p><formula xml:id="formula_3">G max D1,D2 J (G, D 1 , D 2 ) = α × E x∼Pdata [log D 1 (x)] + E z∼Pz [−D 1 (G (z))] + E x∼Pdata [−D 2 (x)] + β × E z∼Pz [log D 2 (G (z))]</formula><p>(1) wherein we have introduced hyperparameters 0 &lt; α, β ≤ 1 to serve two purposes. The first is to stabilize the learning of our model. As the output values of two discriminators are positive and unbounded, D 1 (G (z)) and D 2 (x) in Eq. <ref type="formula">(1)</ref> can become very large and have exponentially stronger impact on the optimization than log D 1 (x) and log D 2 (G (z)) do, rendering the learning unstable. To overcome this issue, we can decrease α and β, in effect making the optimization penalize D 1 (G (z)) and D 2 (x), thus helping to stabilize the learning. The second purpose of introducing α and β is to control the effect of KL and reverse KL divergences on the optimization problem. This will be discussed in the following part once we have the derivation of our optimal solution. Similar to GAN <ref type="bibr" target="#b9">[10]</ref>, our proposed network can be trained by alternatively updating D 1 , D 2 and G. We refer to the supplementary material for the pseudo-code of learning parameters for D2GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Theoretical analysis</head><p>We now provide formal theoretical analysis of our proposed model, that essentially shows that, given G, D 1 and D 2 are of enough capacity, i.e., in the nonparametric limit, at the optimal points, G can recover the data distributions by minimizing both KL and reverse KL divergences between model and data distributions. We first consider the optimization problem with respect to (w.r.t) discriminators given a fixed generator.</p><formula xml:id="formula_4">Proposition 1. Given a fixed G, maximizing J (G, D 1 , D 2 ) yields to the following closed-form optimal discriminators D 1 , D 2 : D 1 (x) = αp data (x) p G (x) and D 2 (x) = βp G (x) p data (x)</formula><p>Proof. According to the induced measure theorem <ref type="bibr" target="#b11">[12]</ref>, two expectations are equal:</p><formula xml:id="formula_5">E z∼Pz [f (G (z))] = E x∼P G [f (x)] where f (x) = −D 1 (x) or f (x) = log D 2 (x).</formula><p>The objective function can be rewritten as below:</p><formula xml:id="formula_6">J (G, D 1 , D 2 ) = α × E x∼Pdata [log D 1 (x)] + E x∼P G [−D 1 (x)] + E x∼Pdata [−D 2 (x)] + β × E x∼P G [log D 2 (x)] =ˆx [αp data (x) log D 1 (x) − p G D 1 (x) − p data (x) D 2 (x) + βp G log D 2 (x)] dx</formula><p>Considering the function inside the integral, given x, we maximize this function w.r.t two variables</p><formula xml:id="formula_7">D 1 , D 2 to find D 1 (x) and D 2 (x).</formula><p>Setting the derivatives w.r.t D 1 and D 2 to 0, we gain:</p><formula xml:id="formula_8">αp data (x) D 1 − p G (x) = 0 and βp G (x) D 2 − p data (x) = 0<label>(2)</label></formula><p>The second derivatives: −αpdata(x) /D </p><formula xml:id="formula_9">J (G , D 1 , D 2 ) = α (log α − 1) + β (log β − 1) D 1 (x) = α and D 2 (x) = β, ∀x at p G = p data</formula><p>Proof. Substituting D 1 , D 2 from Eq. (2) into the objective function in Eq. (1) of the minimax problem, we gain:</p><formula xml:id="formula_10">J (G, D 1 , D 2 ) = α × E x∼Pdata log α + log p data (x) p G (x) − αˆx p G (x) p data (x) p G (x) dx − βˆx p data p G (x) p data (x) dx + β × E x∼P G log β + log p G (x) p data (x) = α (log α − 1) + β (log β − 1) + αD KL (P data P G ) + βD KL (P G P data ) (3)</formula><p>where D KL (P data P G ) and D KL (P G P data ) is the KL and reverse KL divergences between data and model (generator) distributions, respectively. These divergences are always nonnegative and only zero when two distributions are equal: p G = p data . In other words, the generator induces a distribution p G that is identical to the data distribution p data , and two discriminators now fail to recognize the real or fake samples since they return the same score of 1 for both samples. This concludes the proof.</p><p>The loss of generator in Eq. (3) becomes an upper bound when the discriminators are not optimal. This loss shows that increasing α promotes the optimization towards minimizing the KL divergence D KL (P data P G ), thus helping the generative distribution cover multiple modes, but may include potentially undesirable samples; whereas increasing β encourages the minimization of the reverse KL divergence D KL (P G P data ), hence enabling the generator capture a single mode better, but may miss many modes. By empirically adjusting these two hyperparameters, we can balance the effect of two divergences, and hence effectively avoid the mode collapsing issue.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Connection to f-GAN</head><p>Next we point out the relations between our proposed D2GAN and f-GAN -the model extends the Jensen-Shannon divergence (JSD) of GAN to more general divergences, specifically f -divergences <ref type="bibr" target="#b22">[23]</ref>. A divergence in the f -divergence family has the following form:</p><formula xml:id="formula_11">D f (P Q) =ˆX q (x) f q (x) p (x) dx</formula><p>where f : R + → R is a convex, lower-semicontinuous function satisfying f (1) = 0. This function has a convex conjugate function f * , also known as Fenchel conjugate <ref type="bibr" target="#b12">[13]</ref> : f * (t) = sup u∈dom f {ut − f (u)}. The function f * is again convex and lower-semicontinuous.</p><p>Considering P the true distribution and Q the generator distribution, we resemble the learning problem in GAN by minimizing the f -divergence between P and Q. Based on the variational lower bound of f -divergence proposed by Nguyen et al. <ref type="bibr" target="#b21">[22]</ref>, the objective function of f-GAN can be derived as follows:</p><formula xml:id="formula_12">min θ max φ F (θ, φ) = E x∼P [g f (V φ (x))] + E x∼Q θ [−f * (g f (V φ (x)))]</formula><p>where Q is parameterized by θ (as the generator in GAN), V φ : X → R is a function parameterized by φ (as the discriminator in GAN) and g f : R → dom f * is an output activation function (i.e., the discriminator's decision function) specific to the f -divergence used. Using appropriate functions g f and f * (see Tab. 2 in <ref type="bibr" target="#b22">[23]</ref>), we recover the minimization of corresponding divergences such as JSD in GAN, KL (associated with discriminator D 1 ) and reverse KL (associated with discriminator D 2 ) of our D2GAN.</p><p>The f-GAN, however, only considers a single divergence. On the other hand, our proposed method combines KL and reserve KL divergences. Our idea is conceived upon pondering the advantages and disadvantages of these two divergences in covering multiple modes of data. Combining them into a unified objective function as in Eq. (3) helps us reversely engineer to finally obtain the optimization game in Eq. (1) that can be efficiently formulated and solved using the principle of GAN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we conduct comprehensive experiments to demonstrate the capability of improving mode coverage and the scalability of our proposed model on large-scale datasets. We use a synthetic 2D dataset for both visual and numerical verification, and four datasets of increasing diversity and size for numerical verification. We have made our best effort to compare the results of our method with those of the latest state-of-the-art GAN's variants by replicating experimental settings in the original work whenever possible.  and zero biases. Our implementation is in TensorFlow <ref type="bibr" target="#b0">[1]</ref> and we have published a version for reference <ref type="bibr" target="#b0">1</ref> . We now present our experiments on synthetic data followed by those on large-scale real-world datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Synthetic data</head><p>In the first experiment, we reuse the experimental design proposed in <ref type="bibr" target="#b19">[20]</ref> to investigate how well our D2GAN can deal with multiple modes in the data. More specifically, we sample training data from a 2D mixture of 8 Gaussian distributions with a covariance matrix 0.02I and means arranged in a circle of zero centroid and radius 2.0. Data in these low variance mixture components are separated by an area of very low density. The aim is to examine properties such as low probability regions and low separation of modes.</p><p>We use a simple architecture of a generator with two fully connected hidden layers and discriminators with one hidden layer of ReLU activations. This setting is identical, thus ensures a fair comparison with UnrolledGAN 2 <ref type="bibr" target="#b19">[20]</ref>. <ref type="figure" target="#fig_4">Fig. 2c</ref> shows the evolution of 512 samples generated by our models and baselines through time. It can be seen that the regular GAN generates data collapsing into a single mode hovering around the valid modes of data distribution, thus reflecting the mode collapse in GAN. At the same time, UnrolledGAN and D2GAN distribute data around all 8 mixture components, and hence demonstrating the abilities to successfully learn multimodal data in this case. At the last steps, our D2GAN captures data modes more precisely than UnrolledGAN as, in each mode, the UnrolledGAN generates data that concentrate only on several points around the mode's centroid, thus seems to produce fewer samples than D2GAN whose samples fairly spread out the entire mode.</p><p>Next we further quantitatively compare the quality of generated data. Since we know the true distribution p data in this case, we employ two measures, namely symmetric KL divergence and Wasserstein distance. These measures compute the distance between the normalized histograms of 10,000 points generated from our D2GAN, UnrolledGAN and GAN to true p data . <ref type="figure" target="#fig_4">Figs. 2a</ref> and 2b again clearly demonstrate the superiority of our approach over GAN and UnrolledGAN w.r.t both distances (lower is better); notably with Wasserstein metric, the distance from ours to the true distribution almost reduces to zero. These figures also demonstrate the stability of our D2GAN (red curves) during training as it is much less fluctuating compared with GAN (green curves) and UnrolledGAN (blue curves).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Real-world datasets</head><p>We now examine the performance of our proposed method on real-world datasets with increasing diversities and sizes. For networks containing convolutional layers, we closely follow the DCGAN's design <ref type="bibr" target="#b23">[24]</ref>. We use strided convolutions for discriminators and fractional-strided convolutions for generator instead of pooling layers. Batch normalization is applied for each layer, except the generator output layer and the discriminator input layers. We also use Leaky ReLU activations for discriminators, and use ReLU for generator, except its output is tanh since we rescale the pixel intensities into the range of [-1, 1] before feeding images to our model. Only one difference is that, for our model, initializing the weights from N (0, 0.01) yields slightly better results than from N (0, 0.02). We again refer to the supplementary material for detailed architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Evaluation protocol</head><p>Evaluating the quality of image produced by generative models is a notoriously challenging due to the variety of probability criteria and the lack of a perceptually meaningful image similarity metric <ref type="bibr" target="#b29">[29]</ref>. Even when a model can generate plausible images, it is not useful if those images are visually similar. Therefore, in order to quantify the performance of covering data modes as well as producing high quality samples, we use several different ad-hoc metrics for different experiments to compare with other baselines.</p><p>First we adopt the Inception score proposed in <ref type="bibr" target="#b27">[27]</ref>, which are computed by:</p><formula xml:id="formula_13">exp (E x [D KL (p (y | x) p (y))])</formula><p>, where p (y | x) is the conditional label distribution for image x estimated using a pretrained Inception model <ref type="bibr" target="#b28">[28]</ref>, and p (y) is the marginal distribution:</p><formula xml:id="formula_14">p (y) ≈ 1 /N N n=1 p (y | x n = G (z n ))</formula><p>. This metric rewards good and varied samples, but sometimes is easily fooled by a model that collapses and generates to a very low quality image, thus fails to measure whether a model has been trapped into one bad mode. To address this problem, for labeled datasets, we further recruit the so-called MODE score introduced in <ref type="bibr" target="#b4">[5]</ref>:</p><formula xml:id="formula_15">exp (E x [D KL (p (y | x) p (y))] − D KL (p (y) p (y)))</formula><p>wherep (y) is the empirical distribution of labels estimated from training data. The score can adequately reflect the variety and visual quality of images, which is discussed in <ref type="bibr" target="#b4">[5]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Handwritten digit images</head><p>We start with the handwritten digit images -MNIST <ref type="bibr" target="#b18">[19]</ref> that consists of 60,000 training and 10,000 testing 28×28 grayscale images of digits from 0 to 9. Following the setting in <ref type="bibr" target="#b4">[5]</ref>, we first assume that the MNIST has 10 modes, representing connected component in the data manifold, associated with 10 digit classes. We then also perform an extensive grid search of different hyperparameter configurations, wherein our two regularized constants α, β in Eq. (1) are varied in {0.01, 0.05, 0.1, 0.2}. For a fair comparison, we use the same parameter ranges and fully connected layers for our network (c.f. the supplementary material for more details), and adopt results of GAN and mode regularized GAN (Reg-GAN) from <ref type="bibr" target="#b4">[5]</ref>.</p><p>For evaluation, we first train a simple, yet effective 3-layer convolutional nets <ref type="bibr" target="#b2">3</ref> that can obtain 0.65% error on MNIST testing set, and then employ it to predict the label probabilities and compute MODE scores for generated samples. <ref type="figure" target="#fig_5">Fig. 3 (left)</ref> shows the distributions of MODE scores obtained by three models. Clearly, our proposed D2GAN significantly outperforms the standard GAN and Reg-GAN by achieving scores mostly around the maximum <ref type="bibr">[8.0-9.0]</ref>. It is worthy to note that we did not observe substantial differences in the average MODE scores obtained by varying the network size through the parameter searching. We here report the result of the minimal network with the smallest number of layers and hidden units.</p><p>To study the effect of α and β, we inspect the results obtained by this minimal network with varied α, β in <ref type="figure" target="#fig_5">Fig. 3 (right)</ref>. There is a pattern that, given a fixed α, our D2GAN obtains better MODE score when increasing β to a certain value, after which the score could significantly decrease.</p><p>MNIST-1K. The standard MNIST data with 10-mode assumption seems to be fairly trivial. Hence, based on this data, we test our proposed model on a more challenging one. We continue following the technique used in <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> to construct a new 1000-class MNIST dataset (MNIST-1K) by stacking three randomly selected digits to form an RGB image with a different digit image in each channel. The resulting data can be assumed to contain 1,000 distinct modes, corresponding to the combinations of digits in 3 channels from 000 to 999.</p><p>In this experiment, we use a more powerful model with convolutional layers for discriminators and transposed convolutions for the generator. We measure the performance by the number of modes for which the model generated at least one in total 25,600 samples, and the reverse KL divergence between the model distribution (i.e., the label distribution predicted by the pretrained MNIST classifier used in the previous experiment) and the expected data distribution. Tab. 1 reports the results of our D2GAN compared with those of GAN, UnrolledGAN taken from <ref type="bibr" target="#b19">[20]</ref>, DCGAN and Reg-GAN from <ref type="bibr" target="#b4">[5]</ref>. Our proposed method again clearly demonstrates the superiority over baselines by covering all modes and achieving the best distance that is close to zero. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Natural scene images</head><p>We now extend our experiments to investigate the scalability of our proposed method on much more challenging large-scale image databases from natural scenes. We use three widely-adopted datasets: CIFAR-10 <ref type="bibr" target="#b16">[17]</ref>, STL-10 <ref type="bibr" target="#b5">[6]</ref> and ImageNet <ref type="bibr" target="#b25">[26]</ref>. CIFAR-10 is a well-studied dataset of 50,000 32×32 training images of 10 classes: airplane, automobile, bird, cat, deer, dog, frog, horse, ship, and truck. STL-10, a subset of ImageNet, contains about 100,000 unlabeled 96×96 images, which is more diverse than CIFAR-10, but less so than the full ImageNet. We rescale all images down 3 times and train our networks on 32×32 resolution. ImageNet is a very large database of about 1.2 million natural images from 1,000 classes, normally used as the most challenging benchmark to validate the scalability of deep models. We follow the preprocessing in <ref type="bibr" target="#b17">[18]</ref>, except subsampling to 32×32 resolution. We use the code provided in <ref type="bibr" target="#b27">[27]</ref> to compute the Inception score for 10 independent partitions of 50,000 generated samples. 3.82±0.06 MIX+WGAN <ref type="bibr" target="#b2">[3]</ref> 4.04±0.07 Improved-GAN <ref type="bibr" target="#b27">[27]</ref> 4.36±0.04 ALI <ref type="bibr" target="#b7">[8]</ref> 5.34±0.05 BEGAN <ref type="bibr" target="#b3">[4]</ref> 5.62 MAGAN <ref type="bibr" target="#b30">[30]</ref> 5.67 DCGAN <ref type="bibr" target="#b23">[24]</ref> 6.40±0.05 DFM <ref type="bibr" target="#b31">[31]</ref> 7.72±0.13 Tab. 2 and <ref type="figure" target="#fig_6">Fig. 4</ref> show the Inception scores on CIFAR-10, STL-10 and ImageNet datasets obtained by our model and baselines collected from recent work in literature. It is worthy to note that we only compare with methods trained in a completely unsupervised manner without label information. As the result, there exist 8 baselines on CIFAR-10 whilst only DCGAN <ref type="bibr" target="#b23">[24]</ref> and denoising feature matching (DFM) <ref type="bibr" target="#b31">[31]</ref> are available on STL-10 and ImageNet. We use our own TensorFlow implementation of DCGAN with the same network architecture with our model for fair comparisons. In all 3 experiments, the D2GAN fails to beat the DFM, but outperforms other baselines by large margins. The lower results compared with DFM suggest that using autoencoders for matching high-level features appears to be an effective way to encourage the diversity. This technique is compatible with our method, thus integrating it could be a promising avenue for our future work.</p><p>Two discriminators D 1 and D 2 have almost identical architectures, thus they potentially can share parameters in many different schemes. We explore this direction by creating two version of our D2GAN with the same hyperparameter setting. The first version shares all parameters of D 1 and D 2 except the last (output) layer. This model has failed because the discriminator now contains much fewer parameters, rendering it unable to capture two inverse ratios of two density functions. The second one shares all parameters of D 1 and D 2 except the last two layers. This version performed better than the previous one, and could obtain promising Inception scores (7.01 on CIFAR10, 7.44 on STL10 and 7.81 on ImageNet), but these results are still worse than those of our proposed model without sharing parameters.</p><p>Finally, we show several samples generated by our proposed model trained on these three datasets in <ref type="figure">Fig. 5</ref>. Samples are fair random draws, not cherry-picked. It can be seen that our D2GAN is able to produce visually recognizable images of cars, trucks, boats, horses on CIFAR-10. The objects are getting harder to recognize, but the shapes of airplanes, cars, trucks and animals still can be identified on STL-10, and images with various backgrounds such as sky, underwater, mountain, forest are shown on ImageNet. This confirms the diversity of samples generated by our model. Figure 5: Samples generated by our proposed D2GAN trained on natural image datasets. Due to the space limit, please refer to the supplementary material for larger plot.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>To summarize, we have introduced a novel approach to combine Kullback-Leibler (KL) and reverse KL divergences in a unified objective function of the density estimation problem. Our idea is to exploit the complementary statistical properties of two divergences to improve both the quality and diversity of samples generated from the estimator. To that end, we propose a novel framework based on generative adversarial nets (GANs), which formulates a minimax game of three players: two discriminators and one generator, thus termed dual discriminator GAN (D2GAN). Given two discriminators fixed, the learning of generator moves towards optimizing both KL and reverse KL divergences simultaneously, and thus can help avoid mode collapse, a notorious drawback of GANs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the standard GAN and our proposed D2GAN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>non-positive, thus verifying that we have obtained the maximum solution and concluding the proof. Next, we fix D 1 = D 1 , D 2 = D 2 and find the optimal solution G for the generator G. Theorem 2. Given D 1 , D 2 , at the Nash equilibrium point (G , D 1 , D 2 ) for minimax optimization problem of D2GAN, we have the following form for each component:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>For each experiment, we refer to the supplementary material for model architectures and additional results. Common points are: i) discriminators' outputs with softplus activations :f (x) = ln (1 + e x ), i.e., positive version of ReLU; (ii) Adam optimizer [16] with learning rate 0.0002 and the first-order momentum 0.5; (iii) minibatch size of 64 samples for training both generator and discriminators; (iv) Leaky ReLU with the slope of 0.2; and (v) weights initialized from an isotropic Gaussian: N (0, 0.01)of data (in blue) generated from GAN (top row), Un- rolledGAN (middle row) and our D2GAN (bottom row) on 2D data of 8 Gaussians. Data sampled from the true mixture are red.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The comparison of standard GAN, UnrolledGAN and our D2GAN on 2D synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Distributions of MODE scores (left) and average MODE scores (right) with varied α, β.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Inception scores on STL-10 and ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Numbers of modes covered and reverse KL divergence between model and data distributions.</figDesc><table>Model 
GAN [20] UnrolledGAN [20] DCGAN [5] Reg-GAN [5] 
D2GAN 
# modes covered 
628.0±140.9 
817.4±37.9 849.6±62.7 
955.5±18.7 1000.0±0.00 
D KL (model data) 
2.58±0.75 
1.43±0.12 
0.73±0.09 
0.64±0.05 
0.08±0.01 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Inception scores on CIFAR-10.</figDesc><table>Model 
Score 
Real data 
11.24±0.16 
WGAN [2] 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/tund/D2GAN 2 We obtain the code of UnrolledGAN for 2D data from the link authors provided in [20].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Network architecture is similar to https://github.com/fchollet/keras/blob/master/examples/mnist_cnn.py.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>We have established extensive experiments to demonstrate the effectiveness and scalability of our proposed approach using synthetic and large-scale real-world datasets. Compared with the latest state-of-the-art baselines, our model is more scalable, can be trained on the large-scale ImageNet dataset, and obtains Inception scores lower than those of the combination of denoising autoencoder and GAN (DFM), but significantly higher than the others. Finally, we note that our method is orthogonal and could integrate techniques in those baselines such as semi-supervised learning <ref type="bibr" target="#b27">[27]</ref>, conditional architectures <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b24">25]</ref> and autoencoder <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b31">31]</ref>.</p><p>Acknowledgments. This work was partially supported by the Australian Research Council (ARC) Discovery Grant Project DP160109394.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<editor>Ilya Sutskever, Kunal Talwar, Paul Tucker, Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden, Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</editor>
		<imprint>
			<date type="published" when="2015" />
			<pubPlace>Dan Mané, Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner</pubPlace>
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org. 4</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Generalization and equilibrium in generative adversarial nets (gans)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rong</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingyu</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tengyu</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00573</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Began: Boundary equilibrium generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanran</forename><surname>Tong Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenjie</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
		<title level="m">Mode regularized generative adversarial networks</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>. 1, 2, 4.2.1, 4.2.2, 4.2.2, 1</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">An analysis of single-layer networks in unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AISTATS)</title>
		<imprint>
			<biblScope unit="page" from="215" to="223" />
		</imprint>
	</monogr>
	<note>2011. 4.2.3</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Emily L Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ishmael</forename><surname>Vincent Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olivier</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<title level="m">Adversarially learned inference</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<ptr target="http://www.deeplearningbook.org.1" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bing</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sherjil</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in 27th Neural Information Processing Systems (NIPS)</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks. CoRR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Das</forename><surname>Somesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical statistics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Fundamentals of convex analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jean-Baptiste</forename><surname>Hiriart-Urruty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Claude</forename><surname>Lemaréchal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>Springer Science &amp; Business Media</publisher>
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Multi-generator gernerative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Hoang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trung</forename><surname>Tu Dinh Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dinh</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Phung</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02556</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">How (not) to train your generative model: Scheduled sampling, likelihood, adversary?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ferenc</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05101</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Tech. Rep</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
		</imprint>
		<respStmt>
			<orgName>Computer Science Department, University of Toronto</orgName>
		</respStmt>
	</monogr>
	<note>2009. 4.2.3</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoff</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th Annual Conference on Neural Information Processing Systems (NIPS)</title>
		<meeting>the 26th Annual Conference on Neural Information Processing Systems (NIPS)<address><addrLine>Lake Tahoe, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012-06" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
	<note>printed;. 4.2.3</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">The MNIST database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
	<note>. 4.2.2</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ben</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jascha</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
		<title level="m">Unrolled generative adversarial networks</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Estimating divergence functionals and the likelihood ratio by convex risk minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanlong</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael I Jordan</forename><surname>Wainwright</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Information Theory</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="5847" to="5861" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Training generative neural samplers using variational divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Botond</forename><surname>Cseke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryota</forename><surname>Tomioka. F-Gan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="271" to="279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
	<note>4.2, 2, 4.2.3</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generative adversarial text to image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeynep</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xinchen</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lajanugen</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Honglak</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>The 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olga</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjeev</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sean</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiheng</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>2015. 4.2.3</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xi</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<biblScope unit="page" from="2226" to="2234" />
		</imprint>
	</monogr>
	<note>2016. 1, 4.2.1, 4.2.3, 2, 5</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zbigniew</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
	<note>2016. 4.2.1</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lucas</forename><surname>Theis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
		<title level="m">Aäron van den Oord, and Matthias Bethge. A note on the evaluation of generative models</title>
		<imprint>
			<date type="published" when="2001-02-04" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruohan</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Cully</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03817</idno>
		<title level="m">Hyung Jin Chang, and Yiannis Demiris. Magan: Margin adaptation for generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Improving generative adversarial networks with denoising feature matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">-</forename><surname>Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
	<note>ICLR submissions, 8, 2017. 1, 2, 4.2.3</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
