<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Simple Baselines for Human Pose Estimation and Tracking</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Xiao</surname></persName>
							<email>bin.xiao@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haiping</forename><surname>Wu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Electronic Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yichen</forename><surname>Wei</surname></persName>
							<email>yichenw@microsoft.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Simple Baselines for Human Pose Estimation and Tracking</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Human Pose Estimation, Human Pose Tracking</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. There has been significant progress on pose estimation and increasing interests on pose tracking in recent years. At the same time, the overall algorithm and system complexity increases as well, making the algorithm analysis and comparison more difficult. This work provides simple and effective baseline methods. They are helpful for inspiring and evaluating new ideas for the field. State-of-the-art results are achieved on challenging benchmarks. The code will be available at https://github. com/leoxiaobin/pose.pytorch.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Similar as many vision tasks, the progress on human pose estimation problem is significantly advanced by deep learning. Since the pioneer work in <ref type="bibr" target="#b28">[31,</ref><ref type="bibr" target="#b27">30]</ref>, the performance on the MPII benchmark <ref type="bibr" target="#b2">[3]</ref> has become saturated in three years, starting from about 80% PCKH@0.5 <ref type="bibr" target="#b27">[30]</ref> to more than 90% <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">33]</ref>. The progress on the more recent and challenging COCO human pose benchmark <ref type="bibr" target="#b19">[20]</ref> is even faster. The mAP metric is increased from 60.5 (COCO 2016 Challenge winner <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5]</ref>) to 72.1(COCO 2017 Challenge winner <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>) in one year. With the quick maturity of pose estimation, a more challenging task of "simultaneous pose detection and tracking in the wild" has been introduced recently <ref type="bibr" target="#b1">[2]</ref>.</p><p>At the same time, the network architecture and experiment practice have steadily become more complex. This makes the algorithm analysis and comparison more difficult. For example, the leading methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">33]</ref> on MPII benchmark <ref type="bibr" target="#b2">[3]</ref> have considerable difference in many details but minor difference in accuracy. It is hard to tell which details are crucial. Also, the representative works <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref> on COCO benchmark are also complex but differ significantly. Comparison between such works is mostly on system level and less informative. About pose tracking, although there has not been much work <ref type="bibr" target="#b1">[2]</ref>, the system complexity can be expected to further increase due to the increased problem dimension and solution space.</p><p>This work aims to ease this problem by asking a question from the opposite direction, how good could a simple method be? To answer the question, this work provides baseline methods for both pose estimation and tracking. They are quite simple but surprisingly effective. Thus, they hopefully would help inspiring new ideas and simplifying their evaluation. The code, as well as pre-trained models, will be released to facilitate the research community.</p><p>Our pose estimation is based on a few deconvolutional layers added on a backbone network, ResNet <ref type="bibr" target="#b12">[13]</ref> in this work. It is probably the simplest way to estimate heat maps from deep and low resolution feature maps. Our single model's best result achieves the state-of-the-art at mAP of 73.7 on COCO testdev split, which has an improvement of 1.6% and 0.7% over the winner of COCO 2017 keypoint Challenge's single model and their ensembled model <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9]</ref>.</p><p>Our pose tracking follows a similar pipeline of the winner <ref type="bibr" target="#b10">[11]</ref> of ICCV'17 PoseTrack Challenge <ref type="bibr" target="#b1">[2]</ref>. The single person pose estimation uses our own method as above. The pose tracking uses the same greedy matching method as in <ref type="bibr" target="#b10">[11]</ref>. Our only modification is to use optical flow based pose propagation and similarity measurement. Our best result achieves a mAP score of 74.6 and a MOTA score of 57.8, an absolute 15% and 6% improvement over 59.6 and 51.8 of the winner of ICCV'17 PoseTrack Challenge <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">26]</ref>. It is the new state-of-the-art.</p><p>This work is not based on any theoretic evidence. It is based on simple techniques and validated by comprehensive ablation experiments, at our best. Note that we do not claim any algorithmic superiority over previous methods, in spite of better results. We do not perform complete and fair comparison with previous methods, because this is difficult and not our intent. As stated, the contribution of this work are solid baselines for the field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Pose Estimation Using A Deconvolution Head Network</head><p>ResNet <ref type="bibr" target="#b12">[13]</ref> is the most common backbone network for image feature extraction. It is also used in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b5">6]</ref> for pose estimation. Our method simply adds a few deconvolutional layers over the last convolution stage in the ResNet, called C 5 . The whole network structure is illustrated in <ref type="figure" target="#fig_1">Fig. 1(c)</ref>. We adopt this structure because it is arguably the simplest to generate heatmaps from deep and low resolution features and also adopted in the state-of-the-art Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>.</p><p>By default, three deconvolutional layers with batch normalization <ref type="bibr" target="#b14">[15]</ref> and ReLU activation <ref type="bibr" target="#b18">[19]</ref> are used. Each layer has 256 filters with 4 × 4 kernel. The stride is 2. A 1 × 1 convolutional layer is added at last to generate predicted heatmaps {H 1 . . . H k } for all k key points.</p><p>Same as in <ref type="bibr" target="#b27">[30,</ref><ref type="bibr" target="#b21">22]</ref>, Mean Squared Error (MSE) is used as the loss between the predicted heatmaps and targeted heatmaps. The targeted heatmapĤ k for joint k is generated by applying a 2D gaussian centered on the k th joint's ground truth location.</p><p>Discussions To understand the simplicity and rationality of our baseline, we discuss two state-of-the-art network architectures as references, namely, Hourglass <ref type="bibr" target="#b21">[22]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref>. They are illustrated in <ref type="figure" target="#fig_1">Fig. 1</ref>. <ref type="figure" target="#fig_1">Fig. 1</ref>. Illustration of two state-of-the-art network architectures for pose estimation (a) one stage in Hourglass <ref type="bibr" target="#b21">[22]</ref>, (b) CPN <ref type="bibr" target="#b5">[6]</ref>, and our simple baseline (c).</p><p>Hourglass <ref type="bibr" target="#b21">[22]</ref> is the dominant approach on MPII benchmark as it is the basis for all leading methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">33]</ref>. It features in a multi-stage architecture with repeated bottom-up, top-down processing and skip layer feature concatenation.</p><p>Cascaded pyramid network (CPN) <ref type="bibr" target="#b5">[6]</ref> is the leading method on COCO 2017 keypoint challenge <ref type="bibr" target="#b8">[9]</ref>. It also involves skip layer feature concatenation and an online hard keypoint mining step.</p><p>Comparing the three architectures in <ref type="figure" target="#fig_1">Fig. 1</ref>, it is clear that our method differs from <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref> in how high resolution feature maps are generated. Both works <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref> use upsampling to increase the feature map resolution and put convolutional parameters in other blocks. In contrary, our method combines the upsampling and convolutional parameters into deconvolutional layers in a much simpler way, without using skip layer connections.</p><p>A commonality of the three methods is that three upsampling steps and also three levels of non-linearity (from the deepest feature) are used to obtain highresolution feature maps and heatmaps. Based on above observations and the good performance of our baseline, it seems that obtaining high resolution feature maps is crucial, but no matter how. Note that this discussion is only preliminary and heuristic. It is hard to conclude which architecture in <ref type="figure" target="#fig_1">Fig. 1</ref> is better. This is not the intent of this work. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Pose Tracking Based on Optical Flow</head><p>Multi-person pose tracking in videos first estimates human poses in frames, and then tracks these human pose by assigning a unique identification number (id) to them across frames. We present human instance P with id as P = (J, id), where J = {j i } 1:N J is the coordinates set of N J body joints and id indicates the tracking id. When processing the k th frame I k , we have the already processed human instances set</p><formula xml:id="formula_0">P k−1 = {P k−1 i } 1:N k−1 in frame I k−1 and the instances set P k = {P k i } 1:</formula><p>N k in frame I k whose id is to be assigned, where N k−1 and N k are the instance number in frame I k−1 and I k . If one instance P k j in current frame I k is linked to the instance P</p><formula xml:id="formula_1">k−1 i in I k−1 frame, then id k−1 i</formula><p>is propagated to id k j , otherwise a new id is assigned to P k j , indicating a new track. The winner <ref type="bibr" target="#b10">[11]</ref> of ICCV'17 PoseTrack Challenge <ref type="bibr" target="#b1">[2]</ref> solves this multi-person pose tracking problem by first estimating human pose in frames using Mask R-CNN <ref type="bibr" target="#b11">[12]</ref>, and then performing online tracking using a greedy bipartite matching algorithm frame by frame.</p><p>The greedy matching algorithm is to first assign the id of P</p><formula xml:id="formula_2">k−1 i in frame I k−1 to P k j in frame I k if the similarity between P k−1 i</formula><p>and P k j is the highest, then remove these two instances from consideration, and repeat the id assigning process with the highest similarity. When an instance P k j in frame I k has no</p><formula xml:id="formula_3">existing P k−1 i</formula><p>left to link, a new id number is assigned, which indicates a new instance comes up.</p><p>We mainly follow this pipeline in <ref type="bibr" target="#b10">[11]</ref> with two differences. One is that we have two different kinds of human boxes, one is from a human detector and the other are boxes generated from previous frames using optical flow. The second difference is the similarity metric used by the greedy matching algorithm. We propose to use a flow-based pose similarity metric. Combined with these two modifications, we have our enhanced flow-based pose tracking algorithm, illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>. We elaborate our flow-based pose tracking algorithm in the following.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Joint Propagation using Optical Flow</head><p>Simply applying a detector designed for single image level (e.g. Faster-RCNN [27], R-FCN <ref type="bibr" target="#b15">[16]</ref>) to videos could lead to missing detections and false detections due to motion blur and occlusion introduced by video frames. As shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, the detector misses the left black person due to fast motion. Temporal information is often leveraged to generate more robust detections <ref type="bibr" target="#b33">[36,</ref><ref type="bibr" target="#b32">35]</ref>.</p><p>We propose to generate boxes for the processing frame from nearby frames using temporal information expressed in optical flow.</p><p>Given one human instance with joints coordinates set J</p><formula xml:id="formula_4">k−1 i in frame I k−1</formula><p>and the optical flow field F k−1→k between frame I k−1 and I k , we could estimate the corresponding joints coordinates setĴ k i in frame I k by propagating the joints coordinates set J</p><formula xml:id="formula_5">k−1 i according to F k−1→k . More specifically, for each joint location (x, y) in J k−1 i</formula><p>, the propagated joint location would be (x + δx, y + δy), where δx, δy are the flow field values at joint location (x, y). Then we compute a bounding of the propagated joints coordinates setĴ k i , and expand that box by some extend (15% in experiments) as the candidated box for pose estimation.</p><p>When the processing frame is difficult for human detectors that could lead to missing detections due to motion blur or occlusion, we could have boxes propagated from previous frames where people have been detected correctly. As shown in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>, for the left black person in images, since we have the tracked result in previous frames in <ref type="figure" target="#fig_0">Fig. 2(a)</ref>, the propagated boxes successfully contain this person.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flow-based Pose Similarity</head><p>Using bounding box IoU(Intersection-over-Union) as the similarity metric (S Bbox ) to link instances could be problematic when an instance moves fast thus the boxes do not overlap, and in crowed scenes where boxes may not have the corresponding relationship with instances. A more fine-grained metric could be a pose similarity (S P ose ) which calculates the body joints distance between two instances using Object Keypoint Similarity (OKS). The pose similarity could also be problematic when the pose of the same person is different across frames due to pose changing. We propose to use a flow-based pose similarity metric.</p><p>Given one instance J k i in frame I k and one instance J l j in frame I l , the flow-based pose similarity metric is represented as where OKS represents calculating the Object Keypoint Similarity (OKS) between two human pose, andĴ l i represents the propagated joints for J k i from frame I k to I l using optical flow field F k→l .</p><formula xml:id="formula_6">S F low (J k i , J l j ) = OKS(Ĵ l i , J l j ),<label>(1)</label></formula><p>Due to occlusions with other people or objects, people often disappear and re-appear again. Considering consecutive two frames is not enough, thus we have the flow-based pose similarity considering multi frames, denoted as S M ulti−f low , meaning the propagatedĴ k comes from multi previous frames. In this way, we could relink instances even disappearing in middle frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Flow-based Pose Tracking Algorithm</head><p>With the joint propagation using optical flow and the flow-based pose similarity, we propose the flow-based pose tracking algorithm combining these two, as presented in Algorithm 1. <ref type="table" target="#tab_0">Table 1</ref> summarizes the notations used in Algorithm 1.</p><p>First, we solve the pose estimation problem. For the processing frame in videos, the boxes from a human detector and boxes generated by propagating joints from previous frames using optical flow are unified using a bounding box Non-Maximum Suppression (NMS) operation. The boxes generated by progagating joints serve as the complement of missing detections of the detector (e.g. in <ref type="figure" target="#fig_0">Fig. 2(c)</ref>). Then we estimate human pose using the cropped and resized images by these boxes through our proposed pose estimation network in Section 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1</head><p>The flow-based inference algorithm for video human pose tracking</p><formula xml:id="formula_7">1: input: video frames {I k }, Q = [], Q's max capacity LQ. 2: B 0 det = N det (I 0 ) 3: J 0 = Npose(I 0 , B 0 det ) 4: P 0 = (J 0 , id) ⊲ initialize the id for the first frame 5: Q = [P0]</formula><p>⊲ append the instance set P0 to Q 6:</p><formula xml:id="formula_8">for k = 1 to ∞ do 7: B k det = N det (I k ) 8: B k flow = F FlowBoxGen (J k−1 , F k−1→k ) 9: B k unified = FNMS(B k det , B k flow )</formula><p>⊲ unify detection boxes and flow boxes 10:</p><formula xml:id="formula_9">J k = Npose(I k , B k unified ) 11: Msim = Fsim(Q, J k ) 12: P k = FAssignID(Msim, J k ) 13:</formula><p>append P k to Q ⊲ update the Q 14: end for Second, we solve the tracking problem. We store the tracked instances in a double-ended queue(Deque) with fixed length L Q , denoted as</p><formula xml:id="formula_10">Q = [P k−1 , P k−2 , ..., P k−L Q ]<label>(2)</label></formula><p>where P k−i means tracked instances set in previous frame I k−i and the Q's length L Q indicates how many previous frames considered when performing matching.</p><p>The Q could be used to capture previous multi frames' linking relationship, initialized in the first frame in a video. For the k th frame I k , we calculate the flow-based pose similarity matrix M sim between the untracked instances set of body joints J k (id is none) and previous instances sets in Q . Then we assign id to each body joints instance J in J k to get assigned instance set P k by using greedy matching and M sim . Finally we update our tracked instances Q by adding up k th frame instances set P k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Pose Estimation on COCO</head><p>The COCO Keypoint Challenge <ref type="bibr" target="#b19">[20]</ref> requires localization of multi-person keypoints in challenging uncontrolled conditions. The COCO train, validation, and test sets contain more than 200k images and 250k person instances labeled with keypoints. 150k instances of them are publicly available for training and validation. Our models are only trained on all COCO train2017 dataset (includes 57K images and 150K person instances) no extra data involved, ablation are studied on the val2017 set and finally we report the final results on test-dev2017 set to make a fair comparison with the public state-of-the-art results <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">24,</ref><ref type="bibr" target="#b5">6]</ref>. The COCO evaluation defines the object keypoint similarity (OKS) and uses the mean average precision (AP) over 10 OKS thresholds as main competition metric <ref type="bibr" target="#b8">[9]</ref>. The OKS plays the same role as the IoU in object detection. It is  <ref type="bibr" target="#b1">2</ref> calculated from the distance between predicted points and ground truth points normalized by scale of the person.</p><note type="other">ResNet-50 256 × 192 3 4 70.4 b ResNet-50 256 × 192 2 4 67.9 c ResNet-50 256 × 192 3 2 70.1 d ResNet-50 256 × 192 3 3 70.3 e ResNet-101 256 × 192 3 4 71.4 f ResNet-152 256 × 192 3 4 72.0 g ResNet-50 128 × 96 3 4 60.6 h ResNet-50 384 × 288 3 4 72.</note><p>Training The ground truth human box is made to a fixed aspect ratio, e.g., height : width = 4 : 3 by extending the box in height or width. It is then cropped from the image and resized to a fixed resolution. The default resolution is 256 : 192. It is the same as the state-of-the-art method <ref type="bibr" target="#b5">[6]</ref> for a fair comparison. Data augmentation includes scale(±30%), rotation(±40 degrees) and flip. Our ResNet <ref type="bibr" target="#b12">[13]</ref> backbone network is initialized by pre-training on ImageNet classification task <ref type="bibr" target="#b25">[28]</ref>. In the training for pose estimation, the base learning rate is 1e-3. It drops to 1e-4 at 90 epochs and 1e-5 at 120 epochs. There are 140 epochs in total. Mini-batch size is 128. Adam <ref type="bibr" target="#b17">[18]</ref> optimizer is used. Four GPUs on a GPU server is used.</p><p>ResNet of depth 50, 101 and 152 layers are experimented. ResNet-50 is used by default, unless otherwise noted.</p><p>Testing A two-stage top-down paradigm is applied, similar as in <ref type="bibr" target="#b22">[24,</ref><ref type="bibr" target="#b5">6]</ref>. For detection, by default we use a faster-RCNN [27] detector with detection AP 56.4 for the person category on COCO val2017. Following the common practice in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b21">22]</ref>, the joint location is predicted on the averaged heatmpaps of the original and flipped image. A quarter offset in the direction from highest response to the second highest response is used to obtain the final location. <ref type="table" target="#tab_1">Table 2</ref> investigates various options in our baseline in Section 2.   <ref type="figure">a, g, h)</ref> show that image size is critical for performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study</head><p>From method (a) to (g), the image size is reduced by half and AP drops points. On the other hand, relative 75% computation is saved. Method (h) uses a large image size and increases 1.8 AP from method (a), at the cost of higher computational cost.</p><p>Comparison with Other Methods on COCO val2017 <ref type="table" target="#tab_2">Table 3</ref> compares our results with a 8-stage Hourglass <ref type="bibr" target="#b21">[22]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref>. All the three methods use a similar top-down two-stage paradigm. For reference, the person detection AP of hourglass <ref type="bibr" target="#b21">[22]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref> is 55.3 <ref type="bibr" target="#b5">[6]</ref>, which is comparable to ours 56.4. Compared with Hourglass <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6]</ref>, our baseline has an improvement of 3.5 in AP. Both methods use an input size of 256 × 192 and no Online Hard Keypoints Mining(OHKM) involved.</p><p>CPN <ref type="bibr" target="#b5">[6]</ref> and our baseline use the same backbone of ResNet-50. When OHKM is not used, our baseline outperforms CPN [6] by 1.8 AP for input size 256 × 192, and 1.6 AP for input size 384×288. When OHKM is used in CPN <ref type="bibr" target="#b5">[6]</ref>, our baseline is better by 0.6 AP for both input sizes.</p><p>Note that the results of Hourglass <ref type="bibr" target="#b21">[22]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref> are cited from <ref type="bibr" target="#b5">[6]</ref> and not implemented by us. Therefore, the performance difference could come from implementation difference. Nevertheless, we believe it is safe to conclude that our baseline has comparable results but is simpler. <ref type="table" target="#tab_3">Table 4</ref> summarizes the results of other state-of-the-art methods in the literature on COCO Keypoint Leader- Compared with CMU-Pose <ref type="bibr" target="#b4">[5]</ref>, which is a bottom-up approach for multiperson pose estimation, our method is significantly better. Both G-RMI <ref type="bibr" target="#b22">[24]</ref> and CPN <ref type="bibr" target="#b5">[6]</ref> have a similar top-down pipeline with ours. G-RMI also uses ResNet as backbone, as ours. Using the same backbone Resnet-101, our method outperforms G-RMI for both small (256 × 192) and large input size (384 × 288). CPN uses a stronger backbone of ResNet-Inception <ref type="bibr" target="#b26">[29]</ref>. As evidence, the top-1 error rate on ImageNet validation set of Resnet-Inception and ResNet-152 are 18.7% and 21.4% respectively <ref type="bibr" target="#b26">[29]</ref>. Yet, for the same input size 384 × 288, our result 73.7 outperforms both CPN's single model and their ensembled model, which have 72.1 and 73.0 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparisons on COCO test-dev dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pose Estimation and Tracking on PoseTrack</head><p>PoseTrack <ref type="bibr" target="#b1">[2]</ref> dataset is a large-scale benchmark for multi-person pose estimation and tracking in videos. It requires not only pose estimation in single frames, but also temporal tracking across frames. It contains 514 videos including 66,374 frames in total, split into 300, 50 and 208 videos for training, validation and test set respectively. For training videos, 30 frames from the center are annotated. For validation and test videos, besides 30 frames from the center, every fourth frame is also annotated for evaluating long range articulated tracking. The annotations include 15 body keypoints location, a unique person id and a head bounding box for each person instance.</p><p>The dataset has three tasks. Task 1 evaluates single-frame pose estimation using mean average precision (mAP) metric as is done in <ref type="bibr" target="#b23">[25]</ref>. Task 2 also eval- uates pose estimation but allows usage of temporal information across frames. Task 3 evaluates tracking using multi-object tracking metrics <ref type="bibr" target="#b3">[4]</ref>. As our tracking baseline uses temporal information, we report results on Task 2 and 3. Note that our pose estimation baseline also performs best on Task 1 but is not reported here for simplicity.</p><p>Training Our pose estimation model is fine-tuned from those pre-trained on COCO in Section 4.1. As only key points are annotated, we obtain the ground truth box of a person instance by extending the bounding box of its all key points by 15% in length (7.5% on both sides). The same data augmentation as in Section 4.1 is used. During training, the base learning rate is 1e-4. It drops to 1e-5 at 10 epochs and 1e-6 at 15 epochs. There are 20 epochs in total. Other hyper parameters are the same as in Section 4.1.</p><p>Testing Our flow based tracking baseline is closely related to the human detector's performance, as the propagated boxes could affect boxes from a detector. To investigate its effect, we experiment with two off-the-shelf detectors, a faster but less accurate R-FCN <ref type="bibr" target="#b15">[16]</ref> and a slower but more accurate FPN-DCN <ref type="bibr" target="#b9">[10]</ref>. Both use ResNet-101 backbone and are obtained from public implementation <ref type="bibr" target="#b0">[1]</ref>. No additional fine tuning of detectors on PoseTrack dataset is performed. Similar as in <ref type="bibr" target="#b10">[11]</ref>, we first drop low-confidence detections, which tends to decrease the mAP metric but increase the MOTA tracking metric. Also, since the tracking metric MOT penalizes false positives equally regardless of the scores, we drop low confidence joints first to generate the result as in <ref type="bibr" target="#b10">[11]</ref>. We choose the boxes and joints drop threshold in a data-driven manner on validation set, 0.5 and 0.4 respectively.</p><p>For optical flow estimation, the fastest model FlowNet2S in FlowNet family <ref type="bibr" target="#b13">[14]</ref> is used, as provided on <ref type="bibr">[23]</ref>. We use the PoseTrack evaluation toolkit for results on validation set and report final results on test set from the evaluation server. <ref type="figure" target="#fig_2">Fig. 3</ref> illustrates some results of our approach on PoseTrack test dataset. Our main ablation study is performed on ResNet-50 with input size 256×192, which is already strong when compared with state-of-the-art. Our best result is on ResNet-152 with input size 384 × 288. <ref type="table" target="#tab_4">Table 5</ref> shows that using boxes from joint propagation introduces improvement on both mAP and MOTA metrics using different backbones and detectors. With R-FCN detector, using boxes from joint propagation (method a 3 vs. a 1 ) introduces improvement of 4.3 % mAP and 3.8 % MOTA. With the better FPN-DCN detector, using boxes from joint propagation (method b 3 vs. b 1 ) introduces improvement of 3.1 %mAP and 2.3 % MOTA. With ResNet-152 as backbone (method c 3 vs. c 1 ), improvement is 3.8 % mAP and 2.8 % MOTA. Note that such improvement does not only come from more boxes. As noted in <ref type="bibr" target="#b10">[11]</ref>, simply keeping more boxes of a detector, e.g., by using a smaller threshold, would lead to an improvement in mAP, but a drop in MOTA since more false positives would be introduced. The joint propagation improves both mAP and MOTA metrics, indicating that it finds more persons that are missed by the detector, possibly due to motion blur or occlusion in video frames. Another interesting observation is that the less accurate R-FCN detector benefits more from joint propagation. For example, the gap between using FPN-DCN and R-FCN detector in ResNet-50 is decreased from 3.3% mAP and 2.2% MOTA (from a 1 to b 1 ) to 2.1% mAP and 0.4% MOTA (from a 3 to b 3 ). Also, method a 3 outperforms method b 1 by 1.0% mAP and 1.6% MOTA, indicating that a weak detector R-FCN combined with joint propagation could perform better than a strong detector FPN-DCN along. While, the former is more efficient as joint propagation is fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Effect of Joint Propagation</head><p>Effect of Flow-based Pose Similarity Flow-based pose similarity is shown working better when compared with bounding box similarity and pose similarity in <ref type="table" target="#tab_4">Table 5</ref>. For example, flow-based similarity using multi frames (method b 6 ) and single frame (method b 5 ) outperforms bounding box similarity (method b 3 ) by 0.8% MOTA and 0.3% MOTA.</p><p>Note that flow-based pose similarity is better than bounding box similarity when person moves fast and their boxes do not overlap. Method b 6 with flow-based pose similarity considers multi frames and have an 0.5% MOTA improvement when compared to method b 5 , which considers only one previous frame. This improvement comes from the case when people are lost shortly due to occlusion and appear again.</p><p>Comparison with State-of-the-Art We report our results on both Task 2 and Task 3 on PoseTrack dataset. As verified in <ref type="table" target="#tab_4">Table 5</ref>, method b 6 and c 6 are the best settings and used here. Backbones are ResNet-50 and ResNet-152, respectively. The detector is FPN-DCN <ref type="bibr" target="#b9">[10]</ref>. <ref type="table" target="#tab_5">Table 6</ref> reports the results on pose estimation (Task 2). Our small model (ResNet-50) outperforms the other methods already by a large margin. Our larger model (ResNet-152) further improves the state-of-the-art. On validation set it has an absolute 16.1% improvement in mAP over <ref type="bibr" target="#b10">[11]</ref>, which is the winner  of ICCV'17 PoseTrack Challenge, and also has an 10.2% improvement over a recent work <ref type="bibr" target="#b29">[32]</ref>, which is the previous best. <ref type="table" target="#tab_6">Table 7</ref> reports the results on pose tracking (Task 3). Compared with <ref type="bibr" target="#b10">[11]</ref> on validation and test dataset, our larger model (ResNet-152) has an 10.2 and 5.8 improvement in MOTA over its 55.2 and 51.8 respectively. Compared with the recent work <ref type="bibr" target="#b29">[32]</ref>, our best model (ResNet-152) has 7.1% and 6.6% improvement on validation and test dataset respectively. Note that our smaller model (ResNet-50) also outperform the other methods <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b29">32]</ref>. <ref type="table" target="#tab_7">Table 8</ref> summarizes the results on PoseTrack's leaderboard. Our baseline outperforms all public entries by a large margin. Note that all methods differ significantly and this comparison is only on system level.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We present simple and strong baselines for pose estimation and tracking. They achieve state-of-the-art results on challenging benchmarks. They are validated via comprehensive ablation studies. We hope such baselines would benefit the field by easing the idea development and evaluation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. The proposed flow-based pose tracking framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 .</head><label>1</label><figDesc>Heat map resolution. Method (a) uses three deconvolutional layers to gen- erate 64 × 48 heatmaps. Method (b) generates 32 × 24 heatmaps with two deconvolutional layers. (a) outperform (b) by 2.5 AP with only slightly in- creased model capacity. By default, three deconvolutional layers are used.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Some sample results on PoseTrack Challenge test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Notations in Algorithm 1.FlowBoxGen function for generating boxes by joint propagating FAssignID function for assigning instance id</figDesc><table>I 

k 

k 
th frame 
Q 
tracked instances queue 
LQ 
max capacity of Q 
P 

k 

instances set in k 
th frame 
J 

k 

instances set of body joints in k 
th frame 
P 

k 
i 

i 
th instance in k 
th frame 
J 

k 
i 

body joints set of i 
th instance in k 
th frame 
F k→l 
flow field from k 
th frame to l 
th frame 
Msim 
similariy matrix 
B 

k 
det 

boxes from person detector in k 
th frame 
B 

k 
flow 

boxes generated by joint propagating in k 
th frame 
B 

k 
unified 

boxes unified by box N M S in k 
th frame 
N det 
person detection network 
Npose 
human pose estimation network 
N flow 
flow estimation network 
Fsim 
function for calculating similarity matrix 
FNMS 
function for N M S operation 
F </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Ablation study of our method on COCO val2017 dataset. Those settings used in comparison are in bold. For example, (a, e, f) compares backbones.</figDesc><table>Method Backbone 
Input Size #Deconv. Layers Deconv. Kernel Size AP 

a 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Comparison with Hourglass [22] and CPN [6] on COCO val2017 dataset. Their results are cited from [6]. OHKM means Online Hard Keypoints Mining.Backbone. As in most vision tasks, a deeper backbone model has better performance. Methods (a, e, f) show steady improvement by using deeper backbone models. AP increase is 1.0 from ResNet-50 to Resnet-101 and 1.6 from ResNet-50 to ResNet-152. 4. Image size. Methods (</figDesc><table>Method 
Backbone Input Size OHKM AP 

8-stage Hourglass -
256 × 192 
✗ 
66.9 
8-stage Hourglass -
256 × 256 
✗ 
67.1 
CPN 
ResNet-50 256 × 192 
✗ 
68.6 
CPN 
ResNet-50 384 × 288 
✗ 
70.6 
CPN 
ResNet-50 256 × 192 
✓ 
69.4 
CPN 
ResNet-50 384 × 288 
✓ 
71.6 
Ours 
ResNet-50 256 × 192 
✗ 
70.4 
Ours 
ResNet-50 384 × 288 
✗ 
72.2 

2. Kernel size. Methods (a, c, d) show that a smaller kernel size gives a marginally 
decrease in AP, which is 0.3 point decrease from kernel size 4 to 2. By default, 
deconvolution kernel size of 4 is used. 
3. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Comparisons on COCO test-dev dataset. Top: methods in the literature, trained only on COCO training dataset. Middle: results submitted to COCO test-dev leaderboard [9], which have either extra training data (*) or models ensamled ( + ). Bottom: our single model results, trained only on COCO training dataset.board [9] and COCO test-dev dataset. For our baseline here, a human detector with person detection AP of 60.9 on COCO std-dev split dataset is used. For reference, CPN [6] use a human detector with person detection AP of 62.9 on COCO minival split dataset.</figDesc><table>Method 
Backbone 
Input Size AP AP50 AP75 APm AP l AR 

CMU-Pose [5] 
-
-
61.8 84.9 67.5 57.1 68.2 66.5 
Mask-RCNN [12] ResNet-50-FPN 
-
63.1 87.3 68.7 57.8 71.4 -
G-RMI [24] 
ResNet-101 
353 × 257 64.9 85.5 71.3 62.3 70.0 69.7 
CPN [6] 
ResNet-Inception 384 × 288 72.1 91.4 80.0 68.7 77.2 78.5 
FAIR* [9] 
ResNeXt-101-FPN -
69.2 90.4 77.0 64.9 76.3 75.2 
G-RMI* [9] 
ResNet-152 
353 × 257 71.0 87.9 77.7 69.0 75.2 75.8 
oks* [9] 
-
-
72.0 90.3 79.7 67.6 78.4 77.1 
bangbangren* 
+ [9] ResNet-101 
-
72.8 89.4 79.6 68.6 80.0 78.7 
CPN 
+ [6, 9] 
ResNet-Inception 384 × 288 73.0 91.7 80.9 69.5 78.1 79.0 
Ours 
ResNet-152 
384 × 288 73.7 91.9 81.1 70.3 80.0 79.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Ablation study on PoseTrack Challenge validation dataset. Top: Results of ResNet-50 backbone using R-FCN detector. Middle: Results of ResNet-50 backbone using FPN-DCN detector. Bottom: Results of ResNet-152 backbone using FPN-DCN detector.</figDesc><table>Method Backbone Detector 
With Joint 
Propagation 

Similarity 
Metric 

mAP 
Total 

MOTA 
Total 

a1 
ResNet-50 R-FCN 
✗ 
S Bbox 
66.0 57.6 
a2 
ResNet-50 R-FCN 
✗ 
SP ose 
66.0 57.7 
a3 
ResNet-50 R-FCN 
✓ 
S Bbox 
70.3 61.4 
a4 
ResNet-50 R-FCN 
✓ 
SP ose 
70.3 61.8 
a5 
ResNet-50 R-FCN 
✓ 
S F low 
70.3 61.8 
a6 
ResNet-50 R-FCN 
✓ 
S M ulti−F low 70.3 62.2 

b1 
ResNet-50 FPN-DCN 
✗ 
S Bbox 
69.3 59.8 
b2 
ResNet-50 FPN-DCN 
✗ 
SP ose 
69.3 59.7 
b3 
ResNet-50 FPN-DCN 
✓ 
S Bbox 
72.4 62.1 
b4 
ResNet-50 FPN-DCN 
✓ 
SP ose 
72.4 61.8 
b5 
ResNet-50 FPN-DCN 
✓ 
S F low 
72.4 62.4 
b6 
ResNet-50 FPN-DCN 
✓ 
S M ulti−F low 72.4 62.9 

c1 
ResNet-152 FPN-DCN 
✗ 
S Bbox 
72.9 62.0 
c2 
ResNet-152 FPN-DCN 
✗ 
SP ose 
72.9 61.9 
c3 
ResNet-152 FPN-DCN 
✓ 
S Bbox 
76.7 64.8 
c4 
ResNet-152 FPN-DCN 
✓ 
SP ose 
76.7 64.9 
c5 
ResNet-152 FPN-DCN 
✓ 
S F low 
76.7 65.1 
c6 
ResNet-152 FPN-DCN 
✓ 
S M ulti−F low 76.7 65.4 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Multi-person Pose Estimation Performance on PoseTrack Challenge dataset. "*" means models trained on train+validation set. Top: Results on PoseTrack valida- tion set. Bottom: Results on PoseTrack test set</figDesc><table>Method 
Dataset 
Head 
mAP 

Sho. 
mAP 

Elb. 
mAP 

Wri. 
mAP 

Hip 
mAP 

Knee 
mAP 

Ank. 
mAP 

Total 
mAP 

Girdhar et al. [11] val 
67.5 70.2 62.0 51.7 60.7 58.7 49.8 60.6 
Xiu et al. [32] 
val 
66.7 73.3 68.3 61.1 67.5 67.0 61.3 66.5 
Ours:ResNet-50 
val 
79.1 80.5 75.5 66.0 70.8 70.0 61.7 72.4 
Ours:ResNet-152 
val 
81.7 83.4 80.0 72.4 75.3 74.8 67.1 76.7 
Girdhar et al.* [11] test 
-
-
-
-
-
-
-
59.6 
Xiu et al. [32] 
test 
64.9 67.5 65.0 59.0 62.5 62.8 57.9 63.0 
Ours:ResNet-50 
test 
76.4 77.2 72.2 65.1 68.5 66.9 60.3 70.0 
Ours:ResNet-152 
test 
79.5 79.7 76.4 70.7 71.6 71.3 64.9 73.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 7 .</head><label>7</label><figDesc>Multi-person Pose Tracking Performance on PoseTrack Challenge dataset."*" means models trained on train+validation set. Top: Results on PoseTrack validation set. Bottom: Results on PoseTrack test set</figDesc><table>Method 
Dataset 
MOTA 
Head 

MOTA 
Sho. 

MOTA 
Elb. 

MOTA 
Wri. 

MOTA 
Hip 

MOTA 
Knee 

MOTA 
Ank. 

MOTA 
Total 

MOTP 
Total 

Prec 
Total 

Rec 
Total 

Girdhar et al. [11] val 
61.7 
65.5 
57.3 
45.7 
54.3 
53.1 
45.7 
55.2 
61.5 
66.4 
88.1 
Xiu et al. [32] 
val 
59.8 
67.0 
59.8 
51.6 
60.0 
58.4 
50.5 
58.3 
67.8 
70.3 
87.0 
Ours:ResNet-50 
val 
72.1 
74.0 
61.2 
53.4 
62.4 
61.6 
50.7 
62.9 
84.5 
86.3 
76.0 
Ours:ResNet-152 
val 
73.9 
75.9 
63.7 
56.1 
65.5 
65.1 
53.5 
65.4 
85.4 
85.5 
80.3 

Girdhar et al.* [11] test 
-
-
-
-
-
-
-
51.8 
-
-
-
Xiu et al. [32] 
test 
52.0 
57.4 
52.8 
46.6 
51.0 
51.2 
45.3 
51.0 
16.9 
71.2 
78.9 
Ours:ResNet-50 
test 
65.9 
67.0 
51.5 
48.0 
56.2 
54.6 
46.9 
56.4 
45.5 
81.0 
75.7 
Ours:ResNet-152 
test 
67.1 
68.4 
52.2 
48.9 
56.1 
56.6 
48.8 
57.6 
62.6 
79.4 
79.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 8 .</head><label>8</label><figDesc>Results of Mulit-Person Pose Tracking on PoseTrack Challenge Leader- board."*" means models trained on train+validation set.</figDesc><table>Entry 
Additional Training Dataset mAP MOTA 

ProTracker [11] COCO 
59.6 51.8 
PoseFlow [26] 
COCO+MPII-Pose 
63.0 51.0 
MVIG [26] 
COCO+MPII-Pose 
63.2 50.7 
BUTD2 [17] 
COCO 
59.2 50.6 
SOPT-PT [26] 
COCO+MPII-Pose 
58.2 42.0 
ML-LAB [34] 
COCO+MPII-Pose 
70.3 41.8 
Ours:ResNet152* COCO 
74.6 57.8 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deformable-Convnet</surname></persName>
		</author>
		<ptr target="https://github.com/msracver/Deformable-ConvNets" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Posetrack: A benchmark for human pose estimation and tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="5167" to="5176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Evaluating multiple object tracking performance: the clear mot metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bernardin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal on Image and Video Processing</title>
		<imprint>
			<biblScope unit="page">1</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Cascaded pyramid network for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structureaware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1212" to="1221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1831" to="1840" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coco: Coco Leader</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Board</surname></persName>
		</author>
		<ptr target="http://cocodataset.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deformable convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="764" to="773" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Detect-and-track: Efficient pose estimation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="350" to="359" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2980" to="2988" />
		</imprint>
	</monogr>
	<note>Computer Vision (ICCV</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International conference on machine learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jifeng</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">H</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Towards multi-person pose tracking: Bottom-up and top-down methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Qian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2274" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<ptr target="https://github.com/NVIDIA/flownet2-pytorch" />
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page">23</biblScope>
		</imprint>
	</monogr>
	<note>NVIDIA: flownet2-pytorch. Online; accessed March-2018</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Towards accurate multi-person pose estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kanazawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3711" to="3719" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4929" to="4937" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Posetrack ; Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<ptr target="https://posetrack.net/leaderboard.php27" />
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<publisher>PoseTrack Leader Board</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Alemi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">12</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deeppose: Human pose estimation via deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1653" to="1660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.00977</idno>
		<title level="m">Pose flow: Efficient online pose tracking</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Multi-person pose estimation for posetrack with enhanced part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV PoseTrack Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Flow-guided feature aggregation for video object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="408" to="417" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Deep feature flow for video recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
