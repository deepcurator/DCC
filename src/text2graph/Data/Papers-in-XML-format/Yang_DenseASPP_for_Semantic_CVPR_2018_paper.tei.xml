<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">DenseASPP for Semantic Segmentation in Street Scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maoke</forename><surname>Yang</surname></persName>
							<email>maokeyang@deepmotion.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kun</forename><surname>Yu</surname></persName>
							<email>kunyu@deepmotion.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chi</forename><surname>Zhang</surname></persName>
							<email>chizhang@deepmotion.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiwei</forename><surname>Li</surname></persName>
							<email>zhiweili@deepmotion.ai</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kuiyuan</forename><surname>Yang</surname></persName>
							<email>kuiyuanyang@deepmotion.ai</email>
						</author>
						<author>
							<affiliation>
								<orgName>DeepMotion</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">DenseASPP for Semantic Segmentation in Street Scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>With Fully Convolutional Network (FCN) <ref type="bibr" target="#b15">[16]</ref>, semantic image segmentation has achieved promising results with significantly improved feature representation. High level semantic information plays a crucial role in achieving high performance for a segmentation network. To extract high level information, FCN uses multiple pooling layers to increase the receptive field size of an output neuron. However, increased number of pooling layers leads to reduced feature map size, which poses serious challenges to up-sample the segmentation output back to full resolution. On the other hand, if we output the segmentation from an early layer with larger resolution, we were not able to make use of higher <ref type="figure" target="#fig_2">Figure 1</ref>. Illustration of challenging scale variations of street scenes from Cityscapes <ref type="bibr" target="#b3">[4]</ref>. In the first exemplar image, the same category such as person varies largely in scale caused by distance to the camera. The second exemplar image illustrates an even challenging case where a large bus is close while several small traffic lights are far away.</p><p>level semantics for better reasoning.</p><p>Atrous convolution <ref type="bibr" target="#b13">[14]</ref> is proposed to resolve the contradictory requirements between larger feature map resolution and larger receptive fields. An atrous kernel can be dilated in varied rates by inserting zeros into appropriate positions in the kernel mask. Compared to the traditional convolution operator, atrous convolution is able to achieve a larger receptive field size without increasing the numbers of kernel parameters. A feature map produced by an atrous convolution can be as the same size as the input, but with each output neuron possessing a larger receptive field, and therefore encoding higher level semantics.</p><p>Although atrous convolution solves the contradiction between feature map resolution and receptive field size, a method that simply generates a semantic mask from atrous-convolved feature map still suffers from a limitation. Specifically, all neurons in the atrous-convolved feature map share the same receptive field size, which means the process of semantic mask generation only made use of features from a single scale. However, experiences <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref> show that multi-scale information would help resolve ambiguous cases and results in more robust classifi-cation. To this end, ASPP <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref> proposed to concatenate feature maps generated by atrous convolution with different dilation rates, so that the neurons in the output feature map contain multiple receptive field sizes, which encode multiscale info and eventually boost performance.</p><p>However, ASPP still suffers from another limitation. Specifically, input images under the autonomous driving scenarios are of high resolution, which requires neurons to have even larger receptive field. To achieve a large enough receptive field in ASPP, a large enough dilation ratio has to be employed. However, as the dilation rate increases (e.g. d &gt; 24), the atrous convolution becomes more and more ineffective and gradually loses it modeling power <ref type="bibr" target="#b2">[3]</ref>. Therefore, it is important to design a network structure that is able to encode multi-scale information, and simultaneously achieves a large enough receptive field size.</p><p>This motivates us to propose Dense Atrous Spatial Pyramid Pooling (DenseASPP) to solve challenging scale variations in street scenes as illustrated in <ref type="figure" target="#fig_2">Fig. 1</ref>. DenseA-SPP consists of a base network followed by a cascade of atrous convolution layers. It uses dense connections to feed the output of each atrous convolution layer to all unvisited atrous convolution layers ahead, see <ref type="figure" target="#fig_0">Fig. 2</ref>. In DenseASPP, each atrous convolution layer only makes use of atrous filters with reasonable dilation rate (d â‰¤ 24). By the series of atrous convolutions, neurons at later layers obtain larger and larger receptive field without suffering from the kernel degradation issue in ASPP. And by the series of feature concatenations, neurons at each intermediate feature map encode semantic information from multiple scales, and different intermediate feature maps encode multi-scale information from different scale ranges. Therefore, the final output feature map in DenseASPP not only covers semantic information in a large scale range, but also covers that range in a very dense manner, see <ref type="figure">Fig. 3</ref>. We evaluates DenseASPP on Cityscapes datasets and achieves state-ofthe-art performance with an mean Intersection-over-Union score of 80.6%.</p><p>To summarize, this paper makes two following contributions:</p><p>1. DenseASPP is able to generate features that covers a very large scale range (in terms of receptive field sizes).</p><p>2. The generated features of DenseASPP are able to cover the above scale range in a very dense manner.</p><p>It is worth to note that the above two properties cannot be simultaneously achieved by simply stacking atrous convolutional layers in cascade or in parallel.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Semantic image segmentation requires high-level features to represent each pixel for semantic prediction, and ConvNets become the backbone for high-level feature extraction <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>Since ConvNets are designed to do prediction at the whole image level, multiple modifications are made for pixel-level prediction. Fully convolutional network transforms fully connected layers into convolution layers to enable a classification net to output a heat-map <ref type="bibr" target="#b15">[16]</ref>. With several down-sampling layers, resolution of high-level feature maps is with quite low resolution (typically 1/32 of the input image), and bilinear up-sampling or deconvolution is used to recover the resolution.</p><p>To compensate the low resolution of high-level features, feature maps from middle or early layers are also used by skip-connections <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b19">20]</ref>. Due the low-resolution is caused by down-sampling layers, DeepLab <ref type="bibr" target="#b13">[14]</ref> proposed to remove last few max-pooling layers and reconfigure the network use atrous convolution to reuse pre-trained weights. Instead of adding atrous convolution layers to remove pooling layers, more atrous convolution layers are stacked in cascade to further increase the receptive field size <ref type="bibr" target="#b2">[3]</ref> to cover large objects and bring broad contexts.</p><p>In addition, multi-scale features is another important factor to segment objects are with various scales and ambiguous pixels requiring diverse range of contextual information. Following spatial pyramid matching <ref type="bibr" target="#b12">[13]</ref>, PSPNet <ref type="bibr" target="#b23">[24]</ref> and ASPP <ref type="bibr" target="#b1">[2]</ref> are proposed to concatenate features of multiple receptive field sizes together for final prediction, where PSPNet employs four spatial pyramid pooling (downsampling) layers in parallel to aggregate information from multiple receptive field sizes and assign to each pixel via upsampling. ASPP concatenates features from multiple atrous convolution layers with different dilation rates arranged in parallel. The proposed DenseASPP combines the advantages of using atrous convolution layers in parallel and in cascade, and generates features of more scales in a larger range.</p><p>DenseASPP is named after DenseNet <ref type="bibr" target="#b7">[8]</ref> which can be viewed as a special case of DenseASPP by setting dilation rate as 1. Thus, DenseASPP also shares the advantages of DenseNet including alleviating gradient-vanishing problem and substantially fewer parameters. In dilated DenseNet, a concurrent work by Yee <ref type="bibr" target="#b22">[23]</ref>, the same idea is used for cardiac MRI segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dense Atrous Spatial Pyramid Pooling</head><p>In this section, we start with preliminary knowledges of atrous convolution and atrous spatial pyramid pooling, then introduce the proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Atrous convolution and pyramid pooling</head><p>Atrous convolution is first introduced in <ref type="bibr" target="#b13">[14]</ref> to increasing receptive field while keeping the feature map resolution unchanged. In one dimensional case, let y[i] denote output </p><formula xml:id="formula_0">y [i] = K k=1 x [i + d Â· k] Â· w [k]<label>(1)</label></formula><p>where d is the dilation rate, w[k] denotes the k-th parameter of filter, and K is the filter size. This equation reduces to a standard convolution when d = 1. Atrous convolution is equivalent to convolving the input x with up-sampled filters produced by inserting d âˆ’ 1 zeros between two consecutive filter values. Thus, a large dilation rate means a large receptive field. In street scene segmentation, objects usually have very different sizes. To handle this case, the feature maps must be able to cover different scales of receptive fields. For this goal, DeepLabV3 <ref type="bibr" target="#b2">[3]</ref> proposed two strategies, i.e. cascading and parallel of several atrous convolutional layers with different dilation rates. In cascading mode, since a upper atrous layer accepts output of a lower atrous layer, it can efficiently produce large receptive fields. In parallel mode, since multiple atrous layers accept the same input and their outputs are concatenated together, the obtained output is indeed a sampling of the input with different scales of receptive fields. The parallel mode is formally termed as ASPP, which is an abbreviation of Atrous Spatial Pyramid Pooling in <ref type="bibr" target="#b1">[2]</ref>.</p><p>To simplify notations, we use H K,d (x) to term an atrous convolution, and consequently write ASPP as</p><formula xml:id="formula_1">y = H 3,6 (x) + H 3,12 (x) + H 3,18 (x) + H 3,24 (x) (2)</formula><p>In this work, motivated by DenseNets <ref type="bibr" target="#b7">[8]</ref>, we further push the boundaries of cascading and parallel strategies to a novel architecture which is able to generate much more densely scaled receptive fields than <ref type="bibr" target="#b2">[3]</ref>. Experimental results on the Cityscapes <ref type="bibr" target="#b3">[4]</ref> dataset demonstrated its effectiveness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Denser feature pyramid and larger receptive field</head><p>The structure of DenseASPP is illustrated in <ref type="figure" target="#fig_0">Fig. 2</ref>(a). The atrous convolutional layers are organized in a cascade fashion, where the dilation rate of each layer increases layer by layer. Layers with small dilation rates are put in the lower part, while layers with large dilation rates are put in the upper part. The output of each atrous layer is concatenated with the input feature map and all the outputs from lower layers, and the concatenated feature map is fed into the following layer. The final output of DenseASPP is a feature map generated by multi-rate, multi-scale atrous convolutions. The proposed structure can simultaneously compose a much denser and larger feature pyramid using only a few atrous convolutional layers. Following equation <ref type="formula">(2)</ref>, each atrous layer in DenseASPP can be formulated as follows:</p><formula xml:id="formula_2">y l = H K,d l ([y lâˆ’1 , y lâˆ’2 , Â· Â· Â· , y 0 ])<label>(3)</label></formula><p>where d l represents the dilation rate of layer l, and [Â· Â· Â· ] denotes the concatenation operation. [y lâˆ’1 , Â· Â· Â· , y 0 ] means the feature map formed by concatenating the outputs from all previous layers. Compared with the original ASPP <ref type="bibr" target="#b2">[3]</ref>, DenseASPP stacks all dilated layers together, and connects them with dense connections. This change brings us mainly two benefits: denser feature pyramid, and larger receptive field. We explain our network design in detail in terms of these two benefits in the following two subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Denser feature pyramid</head><p>DenseASPP composes a denser feature pyramid than that in ASPP. The word 'denser' not only means better scale diversities of the feature pyramid, but also means there are more pixels involved in convolution than that in ASPP. Denser scale sampling: DenseASPP is an effective architecture to sample inputs at different scales. A key design of DenseASPP is using dense connections to enable diverse ensembling of layers with different dilation rates. Each ensemble is equivalent to a kernel in different scale, i.e. different receptive field. Consequently, we get a feature map with many more scales than that in ASPP <ref type="bibr" target="#b1">[2]</ref>.</p><p>Dilation is able to increase receptive field of a convolution kernel. For an atrous convolutional layer with dilation rate d and kernel size K, the equivalent receptive field size is:</p><formula xml:id="formula_3">R = (d âˆ’ 1) Ã— (K âˆ’ 1) + K<label>(4)</label></formula><p>For example, for a 3 Ã— 3 convolutional layer with d = 3, the corresponding receptive field size is 7.</p><p>Stacking two convolutional layers together can give us a larger receptive field. Suppose we have two convolution layers with the filter size K 1 and K 2 respectively, the new receptive field is:</p><formula xml:id="formula_4">K = K 1 + K 2 âˆ’ 1<label>(5)</label></formula><p>For example, a convolutional layer with kernel size 7 stacked with a convolutional layer with kernel size 13 will result in a receptive field of size 19.</p><p>Fig3 illustrates a simplified feature pyramid of DenseA-SPP to help readers better understand its scale diversity.</p><p>This DenseASPP is constructed with dilation rate of 3, 6, 12, 18. The number(s) in each strip represent the combination of different dilation rate, and the length of each strip represents the equivalent kernel size of each combination. It is obvious that dense connections between stacked atrous layers are able to compose feature pyramid with much denser scale diversity. The receptive fields that DenseASPP ensembles are a super set of that in ASPP. <ref type="figure">Figure 3</ref>. Illustration of DenseASPP's scale pyramid corresponding to the setting of densely stacking atrous convolutions with dilation rates <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18)</ref>. DenseASPP produces feature pyramid with much larger scale diversity (i.e. high resolution in the scaleaxis) and larger receptive field. k on the right side of each strip represents the receptive field size of the corresponding combination.</p><p>Denser pixel sampling: Compared to ASPP, DenseA-SPP gets more pixels involved in the computation of feature pyramid. ASPP composes feature pyramid using 4 atrous convolutional layers with dilation rate of 6, 12, 18, 24. The pixel sampling rate of an atrous convolutional layer with large dilation rate is quite sparse compared to a traditional convolution layer of the same receptive field. <ref type="figure" target="#fig_1">Fig. 4(a)</ref> illustrates a traditional one-dimensional atrous convolution layer, which has a dilation rate of 6. This convolution have a receptive field size 13. However, in such a large kernel, only 3 pixels are sampled for calculation. This phenomenon gets worse in the two-dimensional case. Although large receptive fields are obtained, a lot of information is abandoned in the calculation process.</p><p>The situation is quite different in DenseASPP. Dilation rate increases layer by layer in DenseASPP, thus, convolutions in the upper layer can employ features from the lower layers, and make pixel sampling denser. <ref type="figure" target="#fig_1">Fig. 4</ref>(b) illustrates this process: an atrous layer with dilation rate 3 is put below the layer with dilation rate 6. For the original atrous layer with dilation rate of 6, information of 7 pixels will contribute for the final calculation, which is denser than the original 3 pixels. In the two-dimensional case, 49 pixels will contribute for the final prediction while in the standard one-layer dilated convolution only 9 pixels will contribute. This phenomenon becomes more obvious when the dilation rate goes larger. <ref type="figure" target="#fig_1">Fig. 4(c)</ref> illustrates the phenomenon for the 2D version. The convolutional layer with larger dilation rate can draw help from the filter with smaller dilation rate, and samples pixels in a much denser way. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Larger receptive field</head><p>Another benefit brought by DenseASPP is the larger receptive field. Atrous convolutional layers works in parallel in traditional ASPP, and four sub-branches do not share any information in the feed forward process. To the opposite, atrous convolutional layers in DenseASPP share information through skip connections. Layers with small dilation rate and large dilation rate work interdependently, in which the feed forward process will not only compose a denser feature pyramid, but also come up with a larger filter to perceive larger context. Following equation <ref type="formula" target="#formula_4">(5)</ref>, let R max denote the largest receptive filed of a feature pyramid, and function R K,d means that of a convolutional layer with kernel size K and dilation rate d. Thus, the largest receptive field of ASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> is:</p><formula xml:id="formula_5">R max = max [R 3,6 , R 3,12 , R 3,18 , R 3,24 ] = R 3,24 = 51<label>(6)</label></formula><p>while in the case of DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref>, the largest receptive field is:</p><formula xml:id="formula_6">R max = R 3,6 + R 3,12 + R 3,18 + R 3,24 âˆ’ 3 = 122<label>(7)</label></formula><p>Such a large receptive field can provide global information for large objects in high resolution images. For example, the resolution of Cityscapes <ref type="bibr" target="#b3">[4]</ref> is 2048 Ã— 1024, and the last feature map of our segmentation network is 256 Ã— 128. DenseASPP(6, 12, 18, 24) covers a feature map size of 122, and DenseASPP(3, 6, 12, 18, 24) covers a larger feature map size of 128.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Model size control</head><p>To control model size and to prevent the network from growing too wide, we followed <ref type="bibr" target="#b7">[8]</ref> to add a 1 Ã— 1 convolutional layer before every dilated layer in DenseASPP to reduce feature map's depth into half of its original size. Besides, thin filters are used to further control the output size.</p><p>Suppose every atrous layer outputs n feature maps, DenseASPP have c 0 feature maps as input, and the l-th 1Ã—1 convolutional layer before l-th dilated layer have c l input feature maps, we have:</p><formula xml:id="formula_7">c l = c 0 + n Ã— (l âˆ’ 1)<label>(8)</label></formula><p>In our setting, every 1 Ã— 1 convolutional layer before the dilated layer reduces the dimension into c 0 /2 channels. And we set n = c 0 /8 for all atrous layers in DenseASPP. Thus, the number of parameters in DenseASPP can be calculated as follows:</p><formula xml:id="formula_8">S = L l=1 c l Ã— 1 2 Ã— c 0 2 + c 0 2 Ã— K 2 Ã— n = L l=1 c 0 2 c 0 + (l âˆ’ 1) Ã— c 0 8 + c 0 2 Ã— K 2 Ã— c 0 8 = c 2 0 32 15 + L + 2K 2 L (9)</formula><p>where L is the number of atrous layers in DenseASPP, and K is the kernel size. For example, the feature map of DenseNet121 have 512 channels, thus n is set to 64 for DenseNet121-based model. Besides, before every atrous layer, the number of channels in a feature map is reduced into 256 by a 1 Ã— 1 convolutional layer. Consequently, DenseASPP <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> outputs a feature map with 832 channels, totaling 1,556,480 parameters, which is much smaller than the model size of DenseNet121 (nearly 1 Ã— 10 7 parameters).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>DenseASPP is proposed to tackle the scale variations and contextual information demanding in street scenes, we empirically verify it on Cityscapes dataset in this section. Cityscapes <ref type="bibr" target="#b3">[4]</ref> is comprised of a large, diverse set of highresolution (2048 Ã— 1024) images recorded in streets, where 5000 of these images have high quality pixel-level labels of 19 classes and results 9.43Ã—10 9 labeled pixels in total. Following the standard setting of Cityscapes, the 5000 images are split into 2975 training and 500 validation images with publicly available annotation, as well as 1525 test images with annotations withheld and comparison to other methods is performed via a dedicated evaluation server. For quantitative evaluation, mean of class-wise Intersection over Union (mIoU) are used. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We implement our methods on Pytorch <ref type="bibr" target="#b16">[17]</ref>. For a ConvNet pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref>, we first remove the last two pooling layers and the last classification layer, and set the dilation rates of the convolution layers after the two removed pooling layers to be 2 and 4 respectively to make the pre-trained weights reusable. The modified ConvNets outputs the basic feature map of input image resolution. Followed the feature map, a convolution layer with nineteen 1Ã—1 filters are used to predict the 1 8 label map, which is further up-sampled by a factor of 8 to define the cross entropy loss using the ground-truth label map.</p><p>Batch normalization <ref type="bibr" target="#b8">[9]</ref> is used before each weight layer in our implementation to ease the training and make it is comparable to concatenate feature maps from different layers. To avoid over-fitting, common data augmentations are used as preprocessing, including random flipping horizontally, random scaling in the range of [0.5, 2], random brightness jittering within the range of <ref type="bibr">[-10, 10]</ref>, and random crop of 512 Ã— 512 image patches.</p><p>For training, we use the Adam optimizer <ref type="bibr" target="#b11">[12]</ref> with an initial learning rate of 0.0003 and weight decay of 0.00001. The learning rate is scheduled by multiplying the initial learning rate with 1 âˆ’ epoch maxEpoches 0.9</p><p>. All models are trained for 80 epochs with mini-batch size of 8. The statistics of batch normalization is updated on the whole minibatch.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.1">DenseASPP</head><p>We followed <ref type="bibr" target="#b1">[2]</ref> to build our baseline model, the only difference is that DenseNet121 is used to replace ResNet101 <ref type="bibr" target="#b6">[7]</ref>. We compare the proposed DenseASPP with the original ASPP. For both ASPP and DenseASPP, we use four atrous convolutional layers with dilation rates 6, 12, 18, 24 respectively. All other settings are kept the same. The results are evaluated on the validation set of Cityscapes, and listed in <ref type="table" target="#tab_1">Table 2</ref>. Results shows that DenseASPP significantly improve the segmentation performance over the baseline model by 4.2%, and some examples are shown in <ref type="figure">Fig. 5</ref>. Deeper pre-trained models are helpful to further improve performance. ASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 72.0% DenseNet121 DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 76.2% DenseNet169 DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 77.7% DenseNet201 DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 78.9%</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.2">Detailed study on DenseASPP components</head><p>DenseASPP is composed of multiple atrous convolutional layers with different dilation rates. In this part, experiments are designed to study how different settings of DenseA-SPP influence the performance quantitatively. The results are evaluated on Cityscapes' validation set, and illustrated in <ref type="table">Table 3</ref>. From these experiments, we can get two conclusions. First, the segmentation performance improves with increasing of max feature scales(largest receptive of DenseASPP) of DenseASPP. In fact, both adding more layers and use large dilation rates can increase max scale of DenseASPP. After the max scale goes larger than 128, which is the width of feature map of Cityscapes' image, the performance stopped increasing. Second, even with a relatively weak base model, i.e. DenseNet121, DenseASPP can obtain reasonably high performances.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.3">Comparing with state-of-the-art</head><p>We train DenseASPP based on DenseNet161(wider) <ref type="bibr" target="#b7">[8]</ref> with only fine annotated data, and submit the results on test set to the evaluation system of Cityscapes <ref type="bibr" target="#b3">[4]</ref>. With multi-scale {0.5, 0.8, 1.0, 1.2, 1.5, 2.0} testing, this model achieves mIoU of 80.6% on the test set. We compare our <ref type="figure">Figure 5</ref>. Due to the the ability to capture larger context, DenseASPP can correctly classify confusing categories 'vegetation' and 'terrain', and can distinguish 'fence' from the background. <ref type="table">Table 3</ref>. Performance of DenseASPP with different network settings</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Structure</head><p>Max Scale mIoU DenseNet121 + DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12)</ref> 37 74.8% DenseNet121 + DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18)</ref> 73 75.6% DenseNet121 + DenseASPP <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18)</ref> 79 75.7% DenseNet121 + DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 122 76.2% DenseNet121 + DenseASPP <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> 128 76.6% DenseNet121 + DenseASPP <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr">30)</ref> 179 76.5%</p><p>results with state-of-the-art methods on Cityscapes, and the results is illustrated in <ref type="table" target="#tab_0">Table 1 and Table 4</ref>. It is noted that we use the results reported in the original paper instead of the Cityscapes leader board. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Ablation Studies</head><p>The quality of the last feature map, which is the input of the decision making layer, i.e. softmax layer, is critical for an accurate segmentation. In this section, we first evaluate the feature map quality from both feature level and results level. Then, we study the two major reasons which affect feature maps significantly, i.e. size of receptive field and scale/pixel sampling rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">Feature similarities</head><p>Context information is of great significance for distinguishing confusing categories and classifying large objects. In the Cityscapes dataset, some categories are easily to be misclassified each other due to similar appearances, e.g. 'vegetation' and 'terrain', 'bus','car', 'truck' and 'train'. For these categories, sufficient context information is critical. <ref type="figure">Fig. 5</ref> illustrates two examples where some confusing categories exist. Without enough context information, our baseline model cannot correctly classify 'terrain' and 'fence' in the first example, and misclassified the category of 'bus' and 'truck' and 'wall' in the second example. Our proposed model can correctly classify these images. These examples shows the ability of DenseASPP on modeling large context information. Feature level analysis are further performed to see how our proposed model classifies each pixel. The output of DenseASPP is a multi-channel feature map, based on which a softmax classification is employed to classify each pixel into one class. Thus, each position of the feature map correspond to a pixel, and pixels with the same label are likely have similar features. In order to study how DenseASPP <ref type="figure">Figure 7</ref>. Effects of different spatial and pixel sampling rates. We gradually remove top atrous convolutional layers to visualize the results.</p><p>classifies a pixel, feature similarities between pixels are calculated in the whole feature map, and the results are illustrated by a heat-map as shown in <ref type="figure" target="#fig_3">Fig. 6</ref>. Cosine similarity is used to measure the similarities between features of pixels.</p><p>The similarity map of DenseASPP shows that most of pixels in a continuous region with the same label have similar features. This means we get similar features for pixels in this region. However, the hot regions of results are much smaller and spotted when the DenseASPP is removed. Consequently, pixels in this region are unlikely to be classified to the same category. Large context information brought by DenseASPP is critical for correct segmentation in this case.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Visualization of receptive field</head><p>Since the empirical receptive field sizes are often much smaller than the theoretical ones, we use the method proposed by <ref type="bibr" target="#b24">[25]</ref> to visualize the empirical receptive field sizes. Specifically, for a feature vector representing a image patch with theoretical receptive field size, we use a 8Ã—8 mean image to cover the image patch in a sliding-window way and record the changes of the feature vector measured by the Euclidean distances as a heat-map. The heat-map indicates which pixels actually affect the feature vector.</p><p>The convolution layer with largest dilation rate of ASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> and DenseASPP <ref type="bibr" target="#b5">(6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b23">24)</ref> are visualized respectively, and illustrated in <ref type="figure" target="#fig_4">Fig. 8</ref>. It is obviously to see that dilated layer of DenseASPP sampling denser, and captures larger receptive field.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Illustration of scale diversity</head><p>The more densely connected atrous convolution layers we use in DenseASPP, the more densely sampled feature map we can get. Thus, gradually removing top atrous convolutional layers in DenseASPP will reduce the sampling rates in scale space, and consequent segmentation performances. In this part, we did experiments to visualize such kinds of effects. <ref type="figure">Fig. 7</ref> shows the results.</p><p>It is obvious that, with removing of top layers, the sampling rates in scale space decrease quickly. Large objects such as 'truck' and 'wall' is severely impacted. 'Train' in the middle scale is less affected. Results of small and easy objects, like 'person', are fine. This results further demonstrate the necessity of using densely connected cascaded atrous convolution layers in street scene segmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose DenseASPP to tackle the challenging problem of street scene segmentation where objects vary largely in scale. DenseASPP connects a set of atrous convolution layers in a dense way, which effectively generates densely spatial-sampled and scale-sampled features in a very large range. Theoretical analysis, visualization and quantitative experimental results on Cityscapes dataset are presented to demonstrate the effectiveness of DenseASPP.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The structure of DenseASPP, (a) illustrate DenseASPP in detail, the output of each dilated convolutional layer is concatenated with input feature map, and then feed into the next dilated layer. Each path of DenseASPP compose a feature representation of correspond scale. (b) illustrate this structure in a more concrete version</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Standard one-dimensional atrous convolution with dilation rate of 6. (b)Stacking an atrous layer with small dilation rate below an atrous layer with larger dilation rate makes a denser sampling rate. Red color denotes where the information come from. (c)Two-dimensional version of (b).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>1 8</head><label>1</label><figDesc>input image resolution. ASPP, PSPNet and the proposed DenseASPP are all added on the basic feature map, and all output feature maps of 1 8</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Illustration of feature similarities of all pixels to a pointed pixel. Hotter color means more similar in feature level. (a) and (d) are input image and corresponding ground-truth, (b) and (c) are results of DenseASPP at point 1 and 2 respectively, which are computed by output of DenseASPP. (e) and (f) are correspond results after DenseASPP is removed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Comparing the receptive fields of ASPP and DenseASPP. Red dot means the reference pixel. Lighter pixels indicate strong correlations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Category-wise comparison on the Cityscapes test set.</figDesc><table>Methods 
mIoU 
road 
sidewalk 
building 

wall 
fence 
pole 
traffic light 
traffic sign 
vegetation 

terrain 
sky 
person 
rider 
car 
truck 
bus 
train 
motorcycle 

bicycle 

FCN-8s [16] 
65.3 
97.4 78.4 89.2 34.9 44.2 47.4 60.1 65 
91.4 69.3 93.9 77.1 51.4 92.6 35.3 48.6 46.5 51.6 66.8 
DeepLabv2-CRF[2] 
70.4 
97.9 81.3 90.3 48.8 47.4 49.6 57.9 67.3 91.9 69.4 94.2 79.8 59.8 93.7 56.5 67.5 57.5 57.7 68.8 
FRRN[19] 
71.8 
98.2 83.3 91.6 45.8 51.1 62.2 69.4 72.4 92.6 70 
94.9 81.6 62.7 94.6 49.1 67.1 55.3 53.5 69.5 
RefineNet[15] 
73.6 
98.2 83.3 91.3 47.8 50.4 56.1 66.9 71.3 92.3 70.3 94.8 80.9 63.3 94.5 64.6 76.1 64.3 62.2 70 
PEARL[11] 
75.4 
98.4 84.5 92.1 54.1 56.6 60.4 69 
74 
92.9 70.9 95.2 83.5 65.7 95 
61.8 72.2 69.6 64.8 72.8 
GCN[18] 
76.9 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
DUC[21] 
77.6 
98.5 85.5 92.8 58.6 55.5 65 
73.5 77.9 93.3 72 
95.2 84.8 68.5 95.4 70.9 78.8 68.7 65.9 73.8 
PSPNet[24] 
78.4 
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
-
ResNet-38[22] 
78.4 
98.5 85.7 93.1 55.5 59.1 67.1 74.8 78.7 93.7 72.6 95.5 86.6 69.2 95.7 64.5 78.8 74.1 69 
76.7 
DenseASPP(Ours) 
80.6 
98.7 87.1 93.4 60.7 62.7 65.6 74.6 78.5 93.6 72.5 95.4 86.2 71.9 96.0 78.0 90.3 80.7 69.7 76.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>DenseASPP improve the performance of segmentation by a huge level.</figDesc><table>Base model 
Structure 
mIoU 
DenseNet121 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Performance comparison on Cityscapes test set.</figDesc><table>Method 
mIoU cla 
iIoU cla 
mIoU cat 
iIoU cat 
FCN-8s[16] 
65.3 
41.7 
85.7 
70.1 
DeepLabv2-CRF[2] 
70.4 
42.6 
86.4 
67.7 
FRRN[19] 
71.8 
45.5 
88.9 
75.1 
RefineNet[15] 
73.6 
47.2 
87.9 
70.6 
PEARL[11] 
75.4 
51.6 
89.2 
75.1 
GCN[18] 
76.9 
-
-
-
DUC[21] 
77.6 
53.6 
90.1 
75.2 
PSPNet[24] 
78.4 
56.7 
90.6 
78.6 
ResNet-38[22] 
78.4 
59.1 
90.9 
81.1 
DenseASPP(Ours) 
80.6 
57.9 
90.7 
78.1 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Rethinking atrous convolution for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.05587</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>ArbelÃ¡ez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint/>
	</monogr>
<note type="report_type">arxiv 2015. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The one hundred layers tiramisu: Fully convolutional densenets for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>JÃ©gou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPRW</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Video scene parsing with predictive feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Jie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00119</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Semantic Image Segmentation with Deep Convolutional Nets and Fully Connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liang-Chieh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Refinenet: Multi-path refinement networks for high-resolution semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06612</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Large kernel matters-improve semantic segmentation by global convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.02719</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Fullresolution residual networks for semantic segmentation in street scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pohlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08323</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In International Conference on Medical Image Computing and Computer-Assisted Intervention</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Understanding convolution for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08502</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wider or deeper: Revisiting the resnet model for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.10080</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Yee</surname></persName>
		</author>
		<ptr target="https://chuckyee.github.io/cardiac-segmentation/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01105</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Object detectors emerge in deep scene cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6856</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
