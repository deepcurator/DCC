<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Image Water Hazard Detection using FCN with Reflection Attention Units</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Han</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuong</forename><surname>Nguyen</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>2600</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CSIRO DATA61</orgName>
								<address>
									<postCode>2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="laboratory">Australian Centre of Excellence for Robotic Vision</orgName>
								<address>
									<addrLine>2 George Street</addrLine>
									<postCode>4001</postCode>
									<settlement>Brisbane</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaodi</forename><surname>You</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Australian National University</orgName>
								<address>
									<postCode>2600</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">CSIRO DATA61</orgName>
								<address>
									<postCode>2601</postCode>
									<settlement>Canberra</settlement>
									<region>ACT</region>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianfeng</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Nanjing University of Science and Technology</orgName>
								<address>
									<postCode>210094</postCode>
									<settlement>Nanjing Jiangsu</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Image Water Hazard Detection using FCN with Reflection Attention Units</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Water puddle detection</term>
					<term>Road hazard detection</term>
					<term>Fully con- volutional network</term>
					<term>Deep learning</term>
					<term>Reflection attention unit</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Water bodies, such as puddles and flooded areas, on and off road pose significant risks to autonomous cars. Detecting water from moving camera is a challenging task as water surface is highly refractive, and its appearance varies with viewing angle, surrounding scene, weather conditions. In this paper, we present a water puddle detection method based on a Fully Convolutional Network (FCN) with our newly proposed Reflection Attention Units (RAUs). An RAU is a deep network unit designed to embody the physics of reflection on water surface from sky and nearby scene. To verify the performance of our proposed method, we collect 11455 color stereo images with polarizers, and 985 of left images are annotated and divided into 2 datasets: On Road (ONR) dataset and Off Road (OFR) dataset. We show that FCN-8s with RAUs improves significantly precision and recall metrics as compared to FCN-8s, DeepLab V2 and Gaussian Mixture Model (GMM). We also show that focal loss function can improve the performance of FCN-8s network due to the extreme imbalance of water versus ground classification problem.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>It is well-known that adverse weather conditions affect traffic safety <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b10">11]</ref>. Weather-related driving risks are elevated in wet weather not only for human but also for autonomous cars <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b14">15]</ref>. Water and reflection on water surface can cause serious problems in many scenarios. Running into deep water puddle could cause damages to mechanical and electronic parts of the vehicle.</p><p>Detection of water puddles on road is, however, not a trivial task because of the wide varieties of appearance and reflection of surrounding environment. Many existing methods rely on special sensors such as dual-polarized cameras <ref type="bibr" target="#b24">[25]</ref>, near field Radar <ref type="bibr" target="#b1">[2]</ref>. However, such devices are not general applicable for normal autonomous cars and are not providing sufficient detection accuracy.</p><p>Existing image based detection methods simplify the problem by utilizing multi images along with hand-crafted features, such as average brightness <ref type="bibr" target="#b7">[8]</ref>, Gaussian fitting of brightness and saturation <ref type="bibr" target="#b15">[16]</ref>. However, the water puddle detection are highly ill-posed for hand-crafted features because the real outdoor environments are far too complex to be properly modeled with those handcrafted features. For example, in <ref type="figure" target="#fig_0">Fig. 1</ref>, the reflections are coming from sky, clouds and a variety of environment objects.</p><p>On the other hand, Deep neural nets can learn features autonomously and have achieved great performance in outdoor navigation nowadays. However, to the best of our knowledge, there is no existing work using deep nets to tackle the water hazard detection. Note that it is not a trivial task because water puddles do not have a well-defined appearance which varies drastically with surrounding environment. Furthermore, there is no existing dataset that is large enough for the training of deep nets.</p><p>In this paper, we propose a water detection method based on a Fully Convolutional Network (FCN) with reflection attention units (RAUs). The RAUs are designed to allow the network to capture reflection correspondences between different parts of the images. Because the reflection correspondences are mostly vertical, feature maps in multi-scales are divided into several patches along vertical directions. Then average of each patch is calculated. All the pixels are compared with the averages in the same column to determine whether it is a reflection of a certain patch. As shown in <ref type="figure" target="#fig_0">Fig. 1 (c)</ref>, since the X 8 is the reflection of X 6 , the subtraction results between pixels in X 8 and the average of X 6 should be lower than that of other pairs. <ref type="figure" target="#fig_0">Fig. 1</ref> also shows the water hazard detection results of our method (d) and FCN-8s (e). Because the reflections on the water surface are detected by (c), our method clearly outperforms FCN-8s. In addition we propose to replace cross entropy loss function by focal loss <ref type="bibr" target="#b11">[12]</ref> to deal with the data imbalance problem, as the size of the water puddle various tremendously between images.</p><p>Furthermore, in order to verify the performance of our proposed method and encouraging new research, we propose the 'Puddle-1000' Dataset. We collect 11455 color stereo images with polarizers, and 985 of left images are annotated and divided into 2 datasets: On Road (ONR) sub-dataset and Off Road (OFR) sub-dataset.</p><p>As far as we know, this is the first work to exploit deep neural networks on water hazard detection. And our main contributions are as follows:</p><p>-We propose a reflection attention unit (RAU), and insert it after every last convolutional layer of 5 group layers in the FCN-8s network <ref type="bibr" target="#b12">[13]</ref>. These units are designed to pick up reflection correspondences relationships between different vertical parts of images. -We take the advantage of focal loss <ref type="bibr" target="#b11">[12]</ref> to deal with this imbalanced water detection problem where water puddles account for a small fraction of total number of pixels. -To the best of knowledge, we propose the first single image deep neural net based method on water hazard detection in real driving scenes. And the proposed method achieves the state-of-the-art performance. -We have pixel-wise annotated 985 images mostly containing water puddles. These include 357 on road images and 628 off road images. This newly annotated dataset and source codes of deep networks are available to public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>Traversable area detection and semantic segmentation deep neural networks Water hazard detection is often considered as complementary to the traversable area detection, or the road detection. Traditional methods are based on handcrafted features and color priors. For example, Lu et al. <ref type="bibr" target="#b13">[14]</ref> used Gaussian Mixture Model (GMM) to estimate the priors.</p><p>Recently, CNN based semantic segmentation methods have demonstrated a superior performance. Long et al. <ref type="bibr" target="#b12">[13]</ref> first proposed a fully convolutional network (FCN). And after that, by taking the benefit from ResNet <ref type="bibr" target="#b6">[7]</ref>, Chen et al. <ref type="bibr" target="#b3">[4]</ref> proposed the Deeplab and futher improved the performance. Zhao et al. <ref type="bibr" target="#b30">[31]</ref> proposed a new network structure, called pyramid pooling module, to exploit global context information. Recently, Han et al. <ref type="bibr" target="#b5">[6]</ref> proposed a semi-supervised learning semantic segmentation method based on generative adversarial network (GAN). However, the hyper-parameters of GAN were selected empirically which are not robust to various water detection scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Puddle detection</head><p>Active imaging based methods: To determine different road surface conditions such as dry, wet, snowy and frozen, Fukamizu et al. <ref type="bibr" target="#b4">[5]</ref> relied on the reflectance of a projected visible and infrared light source. Viikari et al. <ref type="bibr" target="#b24">[25]</ref> proposed to use the backscattering of dual polarized RADARs, while Bertozzi et al. <ref type="bibr" target="#b1">[2]</ref> used backscattering of short infrared light source. These techniques however have limited working range which is up to 2m. For larger distance, images extracted from cameras are required.</p><p>Single image method Zhao et al. <ref type="bibr" target="#b31">[32]</ref> exploits the water region detection using color constancy and texture. However they assume a close-up image of water puddles with uniform reflection of the sky. Such assumptions simply do not hold in real driving conditions. Stereo images/Video based methods Texture cues of water surface have been used including smoothness descriptor <ref type="bibr" target="#b28">[29]</ref>, and local binary pattern <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref>. Color cue in HSV space have also been used by Yan <ref type="bibr" target="#b28">[29]</ref>. Rankin and Matthies <ref type="bibr" target="#b20">[21]</ref> further showed that color of water puddle varies with viewing distance and angle. Temporal fluctuation (or dynamic texture) of water surface was also exploited by Santana et al. <ref type="bibr" target="#b22">[23]</ref> (optical flow), and Mettes et al. <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b17">18]</ref> (temporal deviation and L1-norm FFT) to successfully detect running water bodies or still water under windy condition from a fixed camera position. Stereo depth was also exploited by Matthies et al. <ref type="bibr" target="#b15">[16]</ref>, Yan <ref type="bibr" target="#b28">[29]</ref>, and Kim et al. <ref type="bibr" target="#b8">[9]</ref>. As light reflected from water surfaces is polarized <ref type="bibr" target="#b25">[26]</ref>, this provides a strong cue to detect water puddles as used in several works <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>. While Xie et al. <ref type="bibr" target="#b27">[28]</ref> used 3 cameras, others used stereo cameras attached to horizontal and vertical polarizers. Nguyen et al. <ref type="bibr" target="#b18">[19]</ref> further showed that sky polarization strongly effect the appearance of water.</p><p>Other imaging wavelengths including infrared and thermal imaging are also used by <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22</ref>] to allow water detection at any time without active light sources. Rankin et al. <ref type="bibr" target="#b21">[22]</ref> also showed that the relative intensity of water versus ground changes distinctively between night and day and this provide a strong cue of water.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Classification techniques</head><p>From obtained cues of water, different classification techniques have been utilized including hard coded and adaptive thresholding <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b8">9]</ref>, K-means <ref type="bibr" target="#b22">[23]</ref>, decision forest <ref type="bibr" target="#b16">[17]</ref>, support vector machine <ref type="bibr" target="#b8">[9]</ref>, and Gaussian mixture model <ref type="bibr" target="#b18">[19]</ref>.</p><p>To exploit temporal constraint, state propagation techniques have been used including segmentation guided label <ref type="bibr" target="#b22">[23]</ref> and Markov random field <ref type="bibr" target="#b16">[17]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Problem formation and physical insights</head><p>In this section, we aim to formulate the problem and explicitly explain why water puddle detection in a single image is a challenging problem. Also, we aim to introduce concept on reflection attention. Appearance of water puddle: reflection Detecting water hazard from distance is challenging due to the very nature of reflection on water surface. Examples of water are given in <ref type="figure" target="#fig_1">Fig. 2</ref>. In (a) the shape and boundary of puddles are very irregular. In (b) even though there are only reflections of sky, the colors of water surfaces change with the distances. Both in (a) and (b), reflections of puddles far away are very bright. In (c) the reflections are mainly about clouds and trees, while in (d) they consist of blue sky, red fences, clouds and trees, which makes the puddles look very different. The colors of the puddles in (a) and (b) are different from those in (c) and (d), as the latter ones have more soil sediments. In (e) the right puddle has waves due to the wind. The left puddle in (f) looks very similar with road areas. (g) and (h) contain the same puddle but captured at different distances. The puddle reflects of different parts of trees, therefore the textures on puddle surface are quite different. Appearance of water puddle: Inter reflection/refraction and scattering. The process of light reflecting and scattering from a water puddle is illustrated in <ref type="figure" target="#fig_2">Fig 3.</ref> Modeling the appearance of a puddle with a provided environmental luminance is, yet, ill-posed. Light source S 1 from the sky or nearby objects hit water surface at O 1 . It partially reflects back into the air and partially refracts into water column as shown in the left of the figure. What we see from the water puddle is a summation of a) reflection of the light source as at O 1 , and b) the fraction from inside the water as at O 2 and O 3 . Reflection as shown in the top right in fact is the combination of specular reflection (i.e. clear image) R ref lect and scatter reflection (i.e. blurry image) R scatter . Similarly, the refraction as shown in the bottom right is the combination of light coming straight from the ground bottom R bottom as at S 3 and light from sediment particles R particles as at S 2 . This process is expressed as the following summation:</p><formula xml:id="formula_0">R total = R ref lect + R scatter + R bottom + R particles<label>(1)</label></formula><p>An important property of reflection is that the light source and its reflected image lie on a straight line perpendicular to the water surface (or the ground) and that they have same height from the ground as shown in <ref type="figure" target="#fig_3">Fig 4.</ref> Perspective view and imperfect camera lens introduce some distortions to captured images. The line connecting the source and its reflection may not be exactly vertical in the image, and the distances of the source and its reflection to the ground are not exactly the same. As the distance from the camera increases, the different in the height of the object and its reflection reduces. We aim to design a deep network that captures this reflection effect and tolerates the distortion and camera rotation. Mining visual priors through deep learning. With recent rapid advancements of Convolutional Neural Network (CNN) to effectively solve various traditional computer vision problems, we aim to apply this powerful tool to the  problem of water hazard detection. CNN in general and Fully Convolutional Network (FCN) in particular recognise objects with distinct structures and patterns. Therefore these networks do not work well with water reflection which varies drastically depending on what is reflected. As a result, we want to extend the networks recognise the physics of the reflection phenomenon. The main characteristics of reflection on water surface is that the reflection is a inverted and disturbed transform of the sky or nearby objects above the water surface. Specifically, we propose in this paper a new network module called Reflection Attention Unit (RAU) that matches image pattern in the vertical direction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Fully convolutional network with reflection attention units</head><p>Water hazard is hard to recognize for existing semantic segmentation methods because of the reflections on the water surface. Therefore, we want to exploit more information about local and global reflection contexts especially in the vertical direction. The proposed Reflection Attention Units (RAUs) are then used to learn the reflection relationships. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Reflection Attention Unit</head><p>We propose RAU based on a strong cue in a single image that water puddles usually contain vertical reflections. As illustrated by <ref type="figure" target="#fig_3">Fig 4, a reflection</ref> and its source lies along a line nearly vertical in the image. Therefore to detect water puddles, we can search for reflections by matching image regions along pixel columns of an image. Furthermore, to tolerate perspective distortion, small camera rotation (angle with line of horizon) and blurry reflection, multiple resolutions or scales are used in the vertical matching. The architecture of the proposed RAU is illustrated in <ref type="figure" target="#fig_4">Fig. 5</ref>. Given an input feature map I of size [h, w, c], horizontal average pooling is applied to I reduce to size [h, w/2, c]. Then vertical average pooling is applied to reduce this to X of size [n, w/2, c]. In <ref type="figure" target="#fig_4">Fig. 5</ref>, n is set to be 8 for illustration purpose. After that, each row X i sized [1, w/2] of X is tiled or self replicated to size [n, w/2]. Those obtained from all rows are concatenated along the feature axis into a new feature map of size of [n, w/2, c * n]. Then, this feature map are up-sampled to size [h, w, c * n] and denoted as X ′ . We then concatenate n times of I along the feature axis to get I ′ with size [h, w, c * n]. I ′ is subtracted from X ′ to produce D with size [h, w, c * n] which encode the reflection relationships. The subtracted feature map is concatenated with I again, fed into a convolutional layer and activated by ReLU function to generate the final output of the same size as I. <ref type="figure">Fig.6</ref> illustrate how the RAU computes the similarities between one pixel and the averages of other different parts along vertical direction in the neighboring 2 columns in a certain scale. Take the first channel of X ′ as an example. This single-channel image has 16 tiled rows and pixels along each column are the same. A single row represents the blurry version of the top rows of the first channel of I. Because the top rows of I contain mostly clouds, therefore in the difference D, the clouds and reflection of clouds on the water surface has lower intensity than ground, fences and trees. Furthermore, as the reflection lines are not strictly <ref type="figure">Fig. 6</ref>. Working principle of a Reflection Attention Unit. After the Convolutional layer group 1, the input color image is transformed to a feature map I, and average pooling is applied to I to get the downsampled feature map X. Then, self replication, concatenation and upsampling of I produce feature map X ′ . I are concatenated to get I ′ . I ′ is subtracted from X ′ to produce a difference map D. Finally, D and I are concatenated and fed into a convolutional layer and a ReLU to output a new feature map of the same size as I. This is fed to pooling layer 1 of a normal convolutional layer group.</p><p>vertical due to distortion and image rotation, the two average poolings allow for such misalignments from these. In addition, such misalignments are also taken account for when several RAUs are applied to feature maps of different scales as outputs of successive convolutional layer groups. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the network architectures of standard FCN-8s <ref type="bibr" target="#b26">[27]</ref> (a) and our method (b). To study the usefulness of different RAUs in a FCN-8s network, we increasingly insert 5 RAUs after each group of convolutional layers to enable the reflection awareness at different scales. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Network Architecture</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Focal Loss</head><p>The area of water puddles is found much smaller than that of the ground. Particularly, in ONR dataset and OFR dataset, introduced in the next section, the ratios between the pixel number of water and non-water are approximately 1:61 and 1:89. These lead to skewed training when using a common loss function, such as cross entropy loss.</p><p>To deal with unbalanced classification, we propose to use focal loss with our network. Given such a binary classification problem, we define y = {0, 1} as the ground truth classes and p as the probability that one sample belongs to class y = 1. Then standard cross entropy loss is defined as follows:</p><formula xml:id="formula_1">CE(p, y) = −log(p) if y = 1 −log(1 − p) otherwise<label>(2)</label></formula><p>If we define p t as follows:</p><formula xml:id="formula_2">p t = p if y = 1 1 − p otherwise (3)</formula><p>Then equation 2 can be rewritten as:</p><formula xml:id="formula_3">CE(p, y) = CE(p t ) = −log(p t )<label>(4)</label></formula><p>The focal loss is defined as follows to down-weight easy examples and focus on hard examples:</p><formula xml:id="formula_4">F L(p t ) = −(1 − p t ) γ log(p t )<label>(5)</label></formula><p>where −(1 − p t ) γ is the modulating factor and γ ≥ 0 is a tunable focusing parameter.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Puddle-1000 dataset</head><p>To enable deep learning based methods and systematical tests, in this paper we present a new and practical water puddle detection dataset. Note that a previous dataset of water puddles was published by <ref type="bibr" target="#b18">[19]</ref> recorded at two different locations for on road and off road conditions near Canberra city, Australia. This dataset only contains 126 and 157 annotated left frames for on road sequence (ONR) and off road sequence (OFR). For this paper, the annotated frame are too limited for training deep networks. Proposed dataset In this paper, we extend the existing dataset with 5 times more pixel-wise labeled images. The labeled data are all from the above dataset captured by the ZED stereo camera, and since we aim for a single image solution, only left images are annotated and used to validate the performance of different networks. Specifically, ONR now has 357 annotated frames, while OFR now has 628 annotated frames. <ref type="figure" target="#fig_6">Fig. 8(a) and (b)</ref> show color images from ONR and OFR datasets, and (c) and (d) show examples of the pixel-wise annotation masks with two classes. In the ONR dataset, the waters are very muddy and the reflections are mainly from sky, clouds, pillars and fences, while in the OFR dataset the water surfaces are the combined reflections of blue sky, clouds, trees, telegraphs, buildings and some fences. The appearances of ground and other obstacles in those two datasets are also much different. In ONR, there are asphalt roads with moving cars and containers. In OFR the grounds are just wet dirt roads, however it has more different kinds of obstacles, such as fences, buildings, many kinds of trees, mounds and building materials. Therefore there are significant differences in water reflections. These new annotated frames will also be released to the public.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We systematically evaluate the proposed network using the proposed dataset. We compare with existing single image based methods and also compare with a various of existing network structures. We also provide detailed analysis on training time, robustness to over-fitting in the supplementary material (because of the length limit.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">Implementation details</head><p>Our network is implemented using Tensorflow[1] framework and is trained on an NVIDIA TITAN XP GPU with 12GB of memory. In experiments, the resolution of images and ground truths are downsampled to 360px×640px. The batch size is 1 during training. Learning rate is set to 10 −6 at first, and decreases by a factor of 0.2 every 5K iterations after 20K iterations. The number of training iteration of both FCN-8s and FCN-8S-focal-loss is 100K, and that of both Deeplab-V2 and our proposed network is 60K.</p><p>The ONR and OFR datasets are randomly divided into training and testing categories in the following experiments. For ONR dataset, we use 272 images to train the networks, and 85 images to verify the performances. As for OFR dataset, 530 images are used for training and 98 images for testing. Furthermore, we also carry out the experiments on both datasets combined together. The metrics used for evaluation are F-measure, precision, recall and accuracy. In all experiments, we do not use data augmentation.</p><p>The details of 5 RAUs are shown in <ref type="table" target="#tab_0">Table 1</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Validation of the Reflection Attention Units</head><p>We train four different networks on the dataset. Three of them are FCN-8s with RAUs, the difference is that, the number of RAUs in these networks are 1, 3 and 5 respectively. They are named as FCN-8s-FL-1RAU, FCN-8s-FL-3RAU and FCN-8s-FL-5RAU. In FCN-8s-FL-1RAU the single RAU is placed before the  first pooling layer. In FCN-8s-FL-3RAU the RAUs are added before the first, the third and the fifth pooling layers. And FCN-8s-FL-5RAU is the proposed network. In the last one network we do not use RAUs and only add 5 more convolutional layers. In all the networks we use the focal loss. <ref type="table" target="#tab_1">Table 2</ref> shows that, the performances are improved with increasing using of RAUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Validation of the Focal Loss</head><p>We train the FCN-8s with the cross entropy loss function and the focal loss, respectively. The results are shown in <ref type="table">table 3</ref>. From this table, we can see that using focal loss can get obviously better performances on OFR and BOTH datasets. The reason why there is no significant improvement on ONR dataset is because the data imbalance of ONR dataset is much slighter than that of OFR dataset. Even so, the FCN-8s-FL still gets better F-measure and Recall.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Cross dataset validation</head><p>To further validate the robustness of our method, we train two networks on ONR and OFR dataset, and verify them on the opposite datasets. The results <ref type="table">(Table  4</ref>) of our method are better than the FCN-8s-FL in all the experiments. This indicates our method has much better generalization performance on different datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5">Comparison with existing methods</head><p>For further comparison, we implement and re-train other image segmentation networks including FCN-8s, FCN-8s-FL, FCN-8s-5Conv and DeepLab (version 2) <ref type="bibr" target="#b3">[4]</ref>. We also compared our method with the non deep learning method, such as GMM&amp;polarisation <ref type="bibr" target="#b18">[19]</ref>. For DeepLab, we fine-tune it based on a pre-trained Resnet101 model provided by Deeplab, and we do not apply CRF after the inference.</p><p>5 <ref type="table" target="#tab_3">Table 5</ref> shows the performances of different methods and the average inference time for one frame. <ref type="figure" target="#fig_8">Fig. 9</ref> demonstrates the water hazard detection results of them. We can see that our RAUs help improve the performances a lot. The precision and recall have great improvements, showing that our RAUs can help the networks to reduce the false-positives and false-negatives. <ref type="figure" target="#fig_8">Fig. 9</ref> also demonstrates that improvements too.  <ref type="bibr" target="#b4">5</ref> We respectfully mention that we don't have access to source codes and datasets from some other previous publications. And methods not working with single images are not compared too. <ref type="bibr" target="#b5">6</ref> The accuracy dose not increase significantly because the number of water pixels is much smaller than that of non-water pixels, however, when we calculate the accuracy, we count the detection accuracies both of water and non-water pixels.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Discussion and Conclusion</head><p>Challenging Cases Even with the help with RAUs, the water hazard detection is still very challenging in various cases. <ref type="figure" target="#fig_0">Fig. 10</ref> shows some examples. As shown in red rectangles, some puddle areas are too small to be recognized, because they only contain a few pixels. Besides, the wet areas are very similar with puddles, as we present in blue rectangles in (c) and (e). Lastly, in (d) the green rectangles show that some of the water surfaces almost look the same with the road. In all these cases, the water surfaces contain very little reflection information, so that our RAUs can not improve the performances. Conclusion We propose a robust single image water hazard detection based on fully convolutional networks with reflection attention units (RAUs) and focal loss. We also collect on road and off road color images with water hazards, and pixel-wisely annotate 985 images of them to build a dataset and verify the performances. We apply RAUs on multi-scale feature maps. In this novelly proposed RAUs, we calculate the distances between one pixel and the averages of different patches in each 2 columns along vertical direction. The focal loss is also used to deal with the serious data imbalance. Experiments of several deep neural networks and one traditional method on these datasets are carried out, and the results show the great effectiveness of our proposed method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Single image water hazard detection using a deep net with and without RAUs. (c) illustrates the proposed Reflection Attention Unit (RAU) to automatically learn the reflection relationship within a single image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Examples from the proposed 'Puddle-1000' dataset. Waters on ground could have different reflections, colors, brightness, and shapes. Water surface can be still or moving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a) shows the process of water reflection (S1O1R1), refraction (S2O2R2 and S3O3R3) and scattering (O1S2O2 and O1S3O3) at a water puddle. (b) and (c) show light components from the same water puddle. (b) shows reflection light on water surface, magnified by passing through horizontal polariser. (c) shows fraction light from inside water puddle, magnified passing through vertical polariser (wikipedia.org).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. (a) shows image formation of an object and its reflection. The light source and its image is on a line SS" perpendicular at ground G where GS = GS". However the pixel distances on camera image are different, G'S' = G'R'. (b) shows reflection on water with vertical correspondences between tree tops and their reflections (wikipedia.org).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Illustration of a Reflection Attention Unit (RAU).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 7 .</head><label>7</label><figDesc>Fig. 7. Architectures of FCN-8s and our proposed FCN-8s with RAUs. For compactness, we only show 2 out of 5 groups of convolutional layers and their corresponding RAUs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Examples in the proposed dataset and the ZED camera.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Fig. 9 .</head><label>9</label><figDesc>Fig. 9. Water hazards detection results trained on both datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Fig. 10 .</head><label>10</label><figDesc>Fig. 10. The challenging cases for water hazard detection by our method. The first row are the color images, the second row are the ground truths and the last row are our results.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Details of RAUs</figDesc><table>Name N 
Convolutional Kernel 
Feature map size 
Input channels Output channels kernel size 

RAU1 16 
64*(16+1) 
64 
[3,3] 
360×640 
RAU2 16 
128*(16+1) 
128 
[3,3] 
180×320 
RAU3 16 
256*(16+1) 
256 
[3,3] 
90×160 
RAU4 16 
512*(16+1) 
512 
[3,3] 
45×80 
RAU5 8 
512*(8+1) 
512 
[3,3] 
23×40 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Performances of the networks with different number of RAUs.</figDesc><table>Dataset 
Method 
F-measure Precision Recall Accuracy 

ONR 

FCN-8s-FL-5Conv 
55.76% 
55.68% 
55.85% 
99.06% 
FCN-8s-FL-1RAU 
59.63% 
61.64% 
57.75% 
99.17% 
FCN-8s-FL-3RAU 
61.97% 
63.43% 
60.57% 
99.20% 
FCN-8s-FL-5RAU 70.11% 
67.78% 72.61% 99.35% 

OFR 

FCN-8s-FL-5Conv 
78.56% 
89.33% 
70.11% 
99.32% 
FCN-8s-FL-1RAU 
71.67% 
87.45% 
60.71% 
99.14% 
FCN-8s-FL-3RAU 
79.09% 
91.36% 69.72% 
99.34% 
FCN-8s-FL-5RAU 81.67% 
87.21% 76.79% 99.38% 

BOTH 

FCN-8s-FL-5Conv 
63.44% 
62.52% 
64.44% 
99.21% 
FCN-8s-FL-1RAU 
67.64% 
74.66% 
61.82% 
99.14% 
FCN-8s-FL-3RAU 
67.63% 
75.27% 
64.40% 
99.15% 
FCN-8s-FL-5RAU 76.91% 
78.03% 75.81% 99.34% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .Table 4 .</head><label>34</label><figDesc>Performances of FCN-8s with and without focal loss.Performances of different methods on cross dataset validations.</figDesc><table>Dataset 
Method 
F-measure Precision Recall Accuracy 

ONR 
FCN-8s 
56.99% 
59.01% 55.11% 99.12% 
FCN-8s-FL 57.85% 
50.49% 67.71% 98.96% 

OFR 
FCN-8s 
64.33% 
78.18% 
54.64% 
98.92% 
FCN-8s-FL 74.05% 
84.88% 65.66% 99.18% 

BOTH 
FCN-8s 
65.21% 
69.81% 
61.18% 
99.05% 
FCN-8s-FL 70.62% 
74.38% 67.22% 99.19% 

Testing 
Dataset 

Training 
Dataset 

Method 
F-measure Precision Recall Accuracy 

ONR 
OFR 
FCN-8s-FL 
9.16% 
15.80% 
6.45% 
98.65% 
FCN-8s-FL-RAU 31.43% 
50.99% 22.72% 98.95% 

OFR 
ONR 
FCN-8s-FL 
22.98% 
27.20% 
19.89% 
97.62% 
FCN-8s-FL-RAU 36.60% 
60.71% 26.20% 98.38% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Performance comparison between our proposed network and others.</figDesc><table>Dataset 
Method 
F-measure Precision Recall Accuracy Time 

ONR 

FCN-8s-FL-RAU (ours) 
70.11% 
67.78% 72.61% 99.35% 0.32s 
FCN-8s-FL 
57.85% 
50.49% 
67.71% 
98.96% 0.06s 
FCN-8s [27] 
56.99% 
59.01% 
55.11% 
99.12% 0.06s 
DeepLab [4] 
21.97% 
37.18% 
15.60% 
98.83% 0.27s 
FCN-8s-FL-5Conv 
55.76% 
55.68% 
55.85% 
99.06% 0.07s 
GMM &amp; polarisation [19] 
31.0% 
18.7% 
90.2% 
96.5% 
NA 

OFR 

FCN-8s-FL-RAU (ours) 
81.67% 
87.21% 
76.79% 99.38% 0.32s 
FCN-8s-FL 
74.05% 
84.88% 
65.66% 
99.18% 0.06s 
FCN-8s [27] 
64.33% 
78.18% 
54.64% 
98.92% 0.06s 
DeepLab [4] 
45.05% 
71.31% 
32.92% 
98.56% 0.27s 
FCN-8s-FL-5Conv 
78.56% 
89.33% 70.11% 
99.32% 0.07s 
GMM &amp; polarization [19] 
28.1% 
16.8% 
85.4% 
95.2% 
NA 

BOTH 

FCN-8s-FL-RAU (ours) 
76.91% 
78.03% 75.81% 99.34% 0.32s 
FCN-8s-FL 
70.62% 
74.38% 
67.22% 
99.19% 0.06s 
FCN-8s [27] 
65.21% 
69.81% 
61.18% 
99.05% 0.06s 
DeepLab [4] 
30.36% 
53.52% 
21.19% 
98.59% 0.27s 
FCN-8s-FL-5Conv 
63.44% 
62.52% 
64.44% 
99.21% 0.07s 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8">Acknowledgement</head><p>New ground-truth video frames were made possible thanks to the support from National Natural Science </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<title level="m">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Adverse driving conditions alert: Investigations on the swir bandwidth for road status monitoring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bertozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Fedriga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dambrosio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Analysis and Processing</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="592" to="601" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Self-driving cars: Will they be safe during bad weather?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Byrne</surname></persName>
		</author>
		<ptr target="https://www.accuweather.com/en/weather-news/self-driving-cars-will-they-be-safe-during-bad-weathr/60524998" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Road surface condition detection system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fukamizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nakano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Iba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yamasaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sano</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1987-09-01" />
			<biblScope unit="volume">690</biblScope>
			<biblScope unit="page">553</biblScope>
		</imprint>
	</monogr>
	<note>uS Patent 4</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semi-supervised and weakly-supervised road detection based on generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint/>
	</monogr>
	<note>in press</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition pp</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Morel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Meriaudeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Raya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Gunadarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fakultas</surname></persName>
		</author>
		<title level="m">A survey on outdoor water hazard detection. Institus Teknologi Sepuluh November</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="2085" to="1944" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wet area and puddle detection for advanced driver assistance systems (adas) using a stereo camera</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Baek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Control, Automation and Systems</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="263" to="271" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The impact of climate change and weather on transport: An overview of empirical findings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Koetse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rietveld</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transportation Research Part D: Transport and Environment</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="221" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Haze visibility enhancement: A survey and quantitative benchmarking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02002</idno>
		<title level="m">Focal loss for dense object detection</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A hierarchical approach for road detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Robotics and Automation</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="517" to="522" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">To let self-driving cars go anywhere</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Marshall</surname></persName>
		</author>
		<ptr target="https://www.wired.com/story/waymo-self-driving-michigan-testing/" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>train them everywhere</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Detecting water hazards for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bellutta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mchenry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Unmanned Ground Vehicle Technology V</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5083</biblScope>
			<biblScope unit="page" from="231" to="243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">On the segmentation and classification of water in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision Theory and Applications (VISAPP), 2014 International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="283" to="292" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Water detection through spatio-temporal invariant descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mettes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">154</biblScope>
			<biblScope unit="page" from="182" to="191" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">3d tracking of water hazards with polarized stereo cameras</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">V</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Milford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mahony</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5251" to="5257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">How do weather events impact roads</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Of Operation</surname></persName>
		</author>
		<ptr target="https://ops.fhwa.dot.gov/weather/q1roadimpact.htm" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Daytime water detection based on color variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Matthies</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Intelligent Robots and Systems (IROS), 2010 IEEE/RSJ International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="215" to="221" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Daytime water detection based on sky reflections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Rankin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Matthies</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bellutta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Automation (ICRA), 2011 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="5329" to="5336" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Water detection with segmentation guided dynamic texture recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mendonça</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Barata</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics and Biomimetics (ROBIO), 2012 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1836" to="1841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Detection of small water-bodies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Simon</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
<note type="report_type">Tech. rep., DTIC Document</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Road-condition recognition using 24-ghz automotive radar</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename><surname>Viikari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Varpula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kantanen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on intelligent transportation systems</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="639" to="648" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Polarization vision-a uniform sensory capacity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wehner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Experimental Biology</title>
		<imprint>
			<biblScope unit="volume">204</biblScope>
			<biblScope unit="issue">14</biblScope>
			<biblScope unit="page" from="2589" to="2596" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Science</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Polarization-based water hazards detection for autonomous off-road navigation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 International Conference on Mechatronics and Automation</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1666" to="1670" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Water body detection using two camera polarized stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Research in Computer Engineering &amp; Electronics</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Adherent raindrop modeling, detection and removal in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>You</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kawakami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mukaigawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ikeuchi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1721" to="1733" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Pyramid scene parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6230" to="6239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Research of water hazard detection based on color and texture features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Sensors &amp; Transducers</title>
		<imprint>
			<biblScope unit="volume">157</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">428</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
