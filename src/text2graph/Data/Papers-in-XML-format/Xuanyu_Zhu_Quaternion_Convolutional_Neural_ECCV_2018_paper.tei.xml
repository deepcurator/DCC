<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Quaternion Convolutional Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuanyu</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Xu</surname></persName>
							<email>xuyi@sjtu.edu.cn</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongteng</forename><surname>Xu</surname></persName>
							<email>hongteng.xu@duke.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Infinia ML, Inc</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">Duke University</orgName>
								<address>
									<settlement>Durham</settlement>
									<region>NC</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><roleName>3⋆</roleName><forename type="first">Changjian</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shanghai Jiao Tong University</orgName>
								<address>
									<settlement>Shanghai</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Quaternion Convolutional Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Quaternion convolutional neural network · quaternion-based layers · color image denoising · color image classification</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Neural networks in the real domain have been studied for a long time and achieved promising results in many vision tasks for recent years. However, the extensions of the neural network models in other number fields and their potential applications are not fully-investigated yet. Focusing on color images, which can be naturally represented as quaternion matrices, we propose a quaternion convolutional neural network (QCNN) model to obtain more representative features. In particular, we re-design the basic modules like convolution layer and fullyconnected layer in the quaternion domain, which can be used to establish fully-quaternion convolutional neural networks. Moreover, these modules are compatible with almost all deep learning techniques and can be plugged into traditional CNNs easily. We test our QCNN models in both color image classification and denoising tasks. Experimental results show that they outperform the real-valued CNNs with same structures.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>As a powerful feature representation method, convolutional neural networks (CNNs) have been widely applied in the field of computer vision. Since the success of AlexNet <ref type="bibr" target="#b19">[20]</ref>, many novel CNNs have been proposed, e.g., VGG <ref type="bibr" target="#b30">[31]</ref>, ResNet <ref type="bibr" target="#b12">[13]</ref>, and DenseNet <ref type="bibr" target="#b15">[16]</ref>, etc., which achieved state-of-the-art performance in almost all vision tasks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b22">23]</ref>. One key module of CNN model is the convolution layer, which extracts features from high-dimensional structural data efficiently by a set of convolution kernels. When dealing with multi-channel inputs (e.g., color images), the convolution kernels merges these channels by summing up the convolution results and output one single channel per kernel accordingly, as <ref type="figure" target="#fig_0">Fig. 1(a)</ref> shows.</p><p>Although such a processing strategy performs well in many practical situations, it congenitally suffers from some drawbacks in color image processing tasks. Firstly, for each kernel it just sums up the outputs corresponding to different channels and ignores the complicated interrelationship between them. Accordingly, we may lose important structural information of color and obtain non-optimal representation of color image <ref type="bibr" target="#b35">[36]</ref>. Secondly, simply summing up the outputs gives too many degrees of freedom to the learning of convolution kernels, and thus we may have a high risk of over-fitting even if imposing heavy regularization terms. How to overcome these two challenges is still not fullyinvestigated.</p><p>Focusing on the problems mentioned above, we propose a novel quaternion convolutional neural network (QCNN) model, which represents color image in the quaternion domain. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the scheme of QCNN model. In particular, each color pixel in a color image (i.e., the yellow dot in <ref type="figure" target="#fig_0">Fig. 1</ref>) is represented as a quaternion, and accordingly, the image is represented as a quaternion matrix rather than three independent real-valued matrices. Taking the quaternion matrix as the input of our network, we design a series of basic modules, e.g., quaternion convolution layer, quaternion fully-connected layer. While the traditional real-valued convolution is only capable to enforce scaling transformation on the input, specifically, the quaternion convolution achieves the scaling and the rotation of input in the color space, which provides us with more structural representation of color information. Based on these modules, we can establish fully-quaternion CNNs to represent color images in a more effective way. Moreover, we study the relationship between our QCNN model and existing realvalued CNNs and find a compatible way to combine them together in a same algorithmic framework.</p><p>Essentially, our QCNN imposes an implicit regularizer on the architecture of network, which ensures that the representations of color image under the guidance of quaternion operations. Such a strategy considers more complicated relationships across different channels while suppress the degrees of freedom of model's parameters during training. As a result, using quaternion CNNs, we can achieve better learning results with fewer parameters compared with real-valued CNNs. Additionally, a color image is represented as a quaternion matrix in our QCNN, so that we can transform a color pixel throughout the color space using independent and physically-meaningful parameters (i.e., the magnitude and the angle on the color cone shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), which enhances the interpretability of the model. As <ref type="figure" target="#fig_0">Fig. 1</ref> shows, our QCNN preserves more color information than real-valued CNN, which is suitable for color image processing, especially for lowlevel color feature extraction. Experimental results show that our QCNN model provides benefits for both high-level vision task (i.e., color image classification) and low-level vision task (i.e., color image denoising), which outperforms its competitors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Quaternion-based color image processing</head><p>Quaternion is a kind of hyper complex numbers, which is first described by Hamilton in 1843 and interpreted as points in three-dimensional space. Mathematically, a quaternionq in the quaternion domain H, i.e., q ∈ H, can be represented asq = q 0 + q 1 i + q 2 j + q 3 k, where q l ∈ R for l = 0, 1, 2, 3, and the imaginary units i, j, k obey the quaternion rules that</p><formula xml:id="formula_0">i 2 = j 2 = k 2 = ijk = −1. Accordingly, a N -dimensional quaternion vector can be denoted asq = [q 1 , ...,q N ]</formula><p>⊤ ∈ H N . Similar to real numbers, we can define a series of operations for quaternions: </p><formula xml:id="formula_1">-Addition:p +q = (p 0 + q 0 ) + (p 1 + q 1 )i + (p 2 + q 2 )j + (p 3 + q 3 )k. -Scalar multiplication: λq = λq 0 + λq 1 i + λq 2 j + λq 3 k. -Element multiplication: pq =(p 0 q 0 − p 1 q 1 − p 2 q 2 − p 3 q 3 ) + (p 0 q 1 + p 1 q 0 + p 2 q 3 − p 3 q 2 )i + (p 0 q 2 − p 1 q 3 + p 2 q 0 + p 3 q 1 )j + (p 0 q 3 + p 1 q 2 − p 2 q 1 + p 3 q 0 )k. -Conjugation:q * = q 0 − q 1 i − q 2 j − q 3 k.</formula><formula xml:id="formula_2">whereq = 0 + q 1 i + q 2 j + q 3 k andp = 0 + p 1 i + p 2 j + p 3 k are pure quaternion<label>(1)</label></formula><p>representations of these two vectors, and</p><formula xml:id="formula_3">w = cos θ 2 + sin θ 2 (w 1 i + w 2 j + w 3 k).<label>(2)</label></formula><p>Since its convenience in representing rotations of 3-D vectors, quaternion is widely used in mechanics and physics <ref type="bibr" target="#b9">[10]</ref>. In recent years, the theory of quaternion-based harmonic analysis has been well developed and many algorithms have been proposed, e.g., quaternion Fourier transform (QFT) <ref type="bibr" target="#b28">[29]</ref>, quaternion wavelet transform (QWT) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b34">35]</ref>, and quaternion Kalman filter <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>. Most of these algorithms have been proven to work better for 3D objects than realvalued ones. In the field of computer vision and image processing, quaternionbased methods also show its potentials in many tasks. The advantages of quaternion wavelet transform <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b16">17]</ref>, quaternion principal component analysis <ref type="bibr" target="#b39">[40]</ref> and other quaternion color image processing techniques <ref type="bibr" target="#b36">[37]</ref> have been proven to extract more representative features for color images and achieved encouraging results in high-level vision tasks like color image classification. In low-level vision tasks like image denoising and super-resolution, the quaternion-based methods <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b37">38]</ref> preserve more interrelationship information across different channels, and thus, can restore images with higher quality. Recently, a quaternion-based neural network is also put forward and used for classification tasks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b29">30]</ref>. However, how to design a quaternion CNN is still an open problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Real-valued CNNs and their extensions</head><p>Convolutional neural network is one of the most successful models in many vision tasks. Since the success of LeNet <ref type="bibr" target="#b20">[21]</ref> in digit recognition, great progresses have been made. AlexNet <ref type="bibr" target="#b19">[20]</ref> is the first deep CNN that greatly outperforms all past models in image classification task. Then, a number of models with deep and complicated structures are proposed, such as VGG <ref type="bibr" target="#b30">[31]</ref> and ResNet <ref type="bibr" target="#b12">[13]</ref>, which achieve incredible success in ILSVRC <ref type="bibr" target="#b5">[6]</ref>. Recently, the CNN models are also introduced for low-level vision tasks. For example, SRCNN <ref type="bibr" target="#b6">[7]</ref> applies convolutional neural networks to image super-resolution and outperforms classical methods. For other tasks like denoising <ref type="bibr" target="#b23">[24]</ref> and inpainting <ref type="bibr" target="#b33">[34]</ref>, CNNs also achieve encouraging results.</p><p>Some efforts have been made to extend real-valued neural networks to other number fields. Complex-valued neural networks have been built and proved to have advantage on generalization ability <ref type="bibr" target="#b14">[15]</ref> and can be more easily optimized <ref type="bibr" target="#b25">[26]</ref>. Audio signals can be naturally represented as complex numbers, so the complex CNNs are more suitable for such a kind of tasks than real-valued CNNs. It has been proven that deep complex networks can obtain competitive results with real-valued models on audio-related tasks <ref type="bibr" target="#b31">[32]</ref>. In <ref type="bibr" target="#b8">[9]</ref>, a deep quaternion network is proposed. However, its convolution simply replaces the real multiplications with quaternion ones, and its quaternion kernel is not further parameterized. Our proposed quaternion convolution, however, is physically-meaningful for color image processing tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Proposed Quaternion CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Quaternion convolution layers</head><p>Focusing on color image representation, our quaternion CNN treats a color image as a 2D pure quaternion matrix, denoted as A = [â nn ′ ] ∈ H N ×N , where N represents the size of the image. <ref type="bibr" target="#b3">4</ref> In particular, the quaternion matrix A is</p><formula xml:id="formula_4">A = 0 + Ri + Gj + Bk,<label>(3)</label></formula><p>where R, G, B ∈ R N ×N represent red, green and blue channels, respectively. Suppose that we have an L × L quaternion convolution kernel W = [ŵ ll ′ ] ∈ H L×L . We aim to design an effective and physically-meaningful quaternion convolution operation, denoted as "⊛", between the input A and the kernel W . Specifically, this operation should (i) apply rotations and scalings to color vectors in order to find the best representation in the whole color space; (ii) play the same role as real-valued convolution when processing grayscale images. To achieve this aim, we take advantage of the rotational nature of quaternion shown in <ref type="bibr" target="#b0">(1,</ref><ref type="bibr" target="#b1">2)</ref> and propose a quaternion convolution in a particular form. Specifically, we set the element of the quaternion convolution kernel aŝ</p><formula xml:id="formula_5">w ll ′ = s ll ′ (cos θ ll ′ 2 + sin θ ll ′ 2 µ),<label>(4)</label></formula><p>where</p><formula xml:id="formula_6">θ ll ′ ∈ [−π, π] and s ll ′ ∈ R.</formula><p>µ is the gray axis with unit length(i.e.,</p><formula xml:id="formula_7">√ 3 3 (i + j + k)).</formula><p>As shown in Eq. 2, we want a unit quaternion to perform rotation. Accordingly, the quaternion convolution is defined as</p><formula xml:id="formula_8">A ⊛ W = F = [f kk ′ ] ∈ H (N −L+1)×(N −L+1) ,<label>(5)</label></formula><formula xml:id="formula_9">wheref kk ′ = L l=1 L l ′ =1 1 s ll ′ŵ ll ′â (k+l)(k ′ +l ′ )ŵ * ll ′ .<label>(6)</label></formula><p>The collection of all such convolution kernels formulates the proposed quaternion convolution layer. Different from real-valued convolution operation, whose elementary operation is the multiplication between real numbers, the elementary operation of quaternion convolution in (6) actually applies a series of rotations and scalings to the quaternionsâ nn ′ 's in each patch. The rotation axis is set as (</p><formula xml:id="formula_10">√ 3 3 , √ 3 3 , √ 3</formula><p>3 ) (i.e., grayscale axis in color space) for all operations, while the rotation angle and the scaling factor are specified for each operation by θ ll ′ and s ll ′ , respectively.</p><p>The advantage of such a definition is interpretable. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>(a), the convolution in traditional CNNs operates triple scaling transforms to each pixel independently to walk through three color axes and it needs to find the best representation in the whole color space accordingly. For our QCNNs, one pixel is a quaternion or a 3D vector in color space, but the proposed convolution find its best representation in a small part of the color space because we restrict the convolution to apply only a rotate and a scaling transform. Such a convolution actually impose implicit regularizers on the model, such that we can suppress the risk of over-fitting brought by too many degrees of freedom to the learning of kernels. Additionally, in real-valued CNNs, the input layer transfers 3-channel images to single-channel feature maps, ignoring the interrelationship among channels, which causes information loss. Although the loss can be recovered with multiple different filters, the recovery requires redundant iterations, and there's no guarantee that the loss can be recovered perfectly. In QCNNs, the convolution causes no order reduction in the input layer, thus the information of interrelationship among channels can be fully conserved.</p><p>Although our convolution operation is designed for color image, it can be applied to grayscale image as well. For grayscale images, they can be seen as color images whose channels are the same. Because all the corresponding color vectors are parallel to the gray axis, the rotate transform equals to identical transformation, thus the quaternion convolution performs the same function as real-valued convolution. From this viewpoint, real-valued convolution is a special case of quaternion convolution for grayscale image.</p><p>According to the rule of quaternion computations, if we represent eachâ nn ′ as a 3D vector</p><formula xml:id="formula_11">a nn ′ = [a 1 a 2 a 3 ]</formula><p>⊤ , then the operation in (6) can be represented by a set of matrix multiplications:</p><formula xml:id="formula_12">f kk ′ = L l=1 L l ′ =1 s ll ′   f 1 f 2 f 3 f 3 f 1 f 2 f 2 f 3 f 1   a (k+l)(k ′ +l ′ ) ,<label>(7)</label></formula><p>where f kk ′ is a vectorized representation of quaternionf kk ′ , and</p><formula xml:id="formula_13">f 1 = 1 3 + 2 3 cos θ ll ′ , f 2 = 1 3 − 2 3 cos(θ ll ′ − π 3 ), f 3 = 1 3 − 2 3 cos(θ ll ′ + π 3 ). (8)</formula><p>The detailed derivation from (6) to <ref type="formula" target="#formula_12">(7)</ref> is given in the supplementary file. Additionally, because the inputs and outputs of quaternion convolutions are both pure quaternion matrices, quaternion convolution layers can be stacked like what we do in real-valued CNNs and most architectures of real-valued CNNs can also be used in QCNNs. In other words, the proposed quaternion convolution is compatible with traditional real-valued convolution. According to <ref type="formula" target="#formula_12">(7)</ref>, we can find that a quaternion convolution layer has twice as many parameters as the real-valued convolution layer with same structure and same number of filtering kernels since an arbitrary element of quaternion convolution kernel has two trainable parameters s and θ. Denote K as the number of kernels, L as kernel size and C as the number of input channels. A real-valued convolution layer with K L × L × C kernels has KCL 2 parameters, and we require L 2 N 2 KC multiplications to process C N × N feature maps. A quaternion layer with K L × L × C kernels has 2KCL 2 multiplications, as shown in the (7). By reducing the number of the kernels and channels to</p><formula xml:id="formula_14">K √ 2 and C √ 2</formula><p>, the number of the quaternion layer's parameters is halved and equal to that of real-valued layer.</p><p>Since the number of channels C in one layer is equal to the number of kernels K in the previous layer, by reducing the number of the kernels in all layers with the ratio</p><formula xml:id="formula_15">1 √ 2</formula><p>, we half the number of QCNN's parameters and half the number of operations to 4.5 times as that of the real-valued CNN. Note that the matrix multiplication in the (7) can be optimized and parallelized when implemented by Tensorflow. In our experiments, our QCNNs only takes about twice as much time as real-valued CNNs with same number of parameters. According to our following experiments, such a simplification will not do harm to our QCNN model -experimental results show that the QCNNs with comparable number of parameters to real-valued CNNs can still have superior performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Quaternion fully-connected layers</head><p>The quaternion convolution layer mentioned above preserves more interrelationship information and extracting better features than real-valued one. However, if we had to connect it to a common fully-connected layer, that kind of information preserved would be lost. Therefore, here we design a quaternion fully-connected layer that performs same operation as quaternion convolution layer to keep the interrelationship information between channels. Specifically, similar to the realvalued CNNs, whose fully-connected layers can be seen as special cases of onedimensional convolution layers with kernels having same shapes with inputs, our quaternion fully-connected layers follow the same rule. </p><formula xml:id="formula_16">b m = N i=1 1 s iŵ m iâiŵ m * i ,<label>(9)</label></formula><p>where s i is the magnitude ofŵ m i . Similar to our quaternion convolution layer, the computation of the proposed quaternion fully-connected layer can also be reformulated as a set of matrix multiplications, and thus, it is also compatible with real-valued CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Typical nonlinear layers</head><p>Pooling and activation are import layers to achieve nonlinear operations. For our QCNN model, we extend those widely-used real-valued nonlinear layers to quaternion versions. For average-pooling, the average operation of quaternion is same as averaging the 3 imaginary parts respectively. For max-pooling, we can define various criterions such as magnitude or projection to gray axis to judge which element to choose.</p><p>In our experiments, we find that simply applying max-pooling to 3 imaginary parts respectively can provides us with good learning results. Similarly, we use same activation functions with real-valued CNNs for each channel respectively in QCNNs. For ReLU, if a vector of quaternion is rotated out of valid value range in color space, e.g. negative color value for RGB channels, we reset it to the nearest point in color space.</p><p>For softmax, we split the output of the quaternion layer in to real numbers and connect them to real-valued softmax layers and train classifiers accordingly.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Connecting with real-valued networks</head><p>Using the modules mentioned above, we can establish arbitrary fully-quaternion CNNs easily. Moreover, because of the compatibility of these modules, we can also build hybrid convolutional neural networks using both quaternion-based layers and common real-valued layers. In particular, -Connect to real-valued convolution layer: The feature map that a quaternion layer outputs can be split into 3 grayscale feature maps, each corresponding to one channel. Then, we can connect each of these three maps to real-valued convolution layers independently, or concatenate them together and connect with a single real-valued convolution layers. -Connect to real-valued fully-connected layer: Similarly, we flatten the output of a quaternion layer and treat each quaternion element as 3 real numbers. Thus, we obtain a real-valued and vectorized output which can be connected to real-valued fully-connected layer easily.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Learning Quaternion CNNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Weight initialization</head><p>Proper weight initialization is essential for a network to be successfully trained. This principle is also applicable to our QCNN model. According to our analysis above, the scaling factor s corresponds to the parameters in real-valued CNNs, which controls the magnitude of transformed vector, while the rotation angle θ is an additional parameter, which only makes the transformed vector an rotation of input vector. Additionally, when transformed vectors are added together, though the magnitude is affected by θ, its projection to gray axis is still independent of θ. Therefore, we follow the suggestion proposed in <ref type="bibr" target="#b10">[11]</ref> and perform normalized initialization in order to keep variance of the gradients same during training. Specifically, for each scaling factor and each rotation factor of the j-th layer, i.e., s j and θ, and we initialize them as two uniform random variables:</p><formula xml:id="formula_17">s j ∼ U − √ 6 √ n j + n j+1 , √ 6 √ n j + n j+1 , θ ∼ U − π 2 , π 2 .<label>(10)</label></formula><p>where U [·] represents a uniform distribution, and n j means the dimension of the j-th layer's input.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Backpropagation</head><p>Backpropagation is the key of training a network, which applies the chain rule to compute gradients of the parameters and updates them. Denote L as the realvalued loss function used to train our quaternion CNN model.p = p 1 i+p 2 j +p 3 k andq = q 1 i + q 2 j + q 3 k are two pure quaternion variables. For the operation we perform in the QCNN, i.e.,p = 1 sŵqŵ * , it can be equivalently represented by a set of matrix multiplications. So is the corresponding quaternion gradient. Particularly, we have:</p><formula xml:id="formula_18">∂L ∂q = ∂L ∂p ∂p ∂q , ∂L ∂θ = ∂L ∂p ∂p ∂θ , ∂L ∂s = ∂L ∂p ∂p ∂s ,<label>(11)</label></formula><p>where</p><formula xml:id="formula_19">p = [p 1 , p 2 , p 3 ] ⊤ and q = [q 1 , q 2 , q 3 ]</formula><p>⊤ are vectors corresponding top and q. When p and q are arbitrary elements of feature maps and filtering kernels, corresponding to a nn ′ and w ll ′ in <ref type="formula" target="#formula_12">(7)</ref>, we have</p><formula xml:id="formula_20">∂p ∂q = s   f 1 f 2 f 3 f 3 f 1 f 2 f 2 f 3 f 1   , ∂p ∂θ = s   f ′ 1 f ′ 2 f ′ 3 f ′ 3 f ′ 1 f ′ 2 f ′ 2 f ′ 3 f ′ 1     q 1 q 2 q 3   , ∂p ∂s =   f 1 f 2 f 3 f 3 f 1 f 2 f 2 f 3 f 1     q 1 q 2 q 3   (12)</formula><p>where f i , i = 1, 2, 3, is defined as <ref type="formula">(8)</ref> does. The matrix of f i 's is exactly same as that in <ref type="formula" target="#formula_12">(7)</ref>, but the operation switches from left multiplication to right multiplication. In other words, the backward process can be explained as a rotate transform with the same axis and a reverse angle.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Loss and activation functions</head><p>In neural networks, loss and activation functions must be differentiable for the gradient to generate and propagate. For fully-quaternion CNNs, any functions which are differentiable with respect to each part of the quaternion variables also make the quaternion chain rule hold, and thus, can be used as loss (and activation) functions. For hybrid CNNs, we select loss functions according to the category of tasks. In classification tasks, the top of the networks are realvalued fully-connected layers, before which the quaternion inputs are flattened as section 3.4 suggested, and the loss function is cross entropy loss. In other tasks (e.g., regression tasks) that the network outputs images, quaternion outputs of the top layer are regarded as the 3-channel images, and the loss function can be mean square error (MSE) or other similar functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experiments</head><p>To demonstrate the superiority and the universality of our QCNN model, we test it on two typical vision tasks: color image classification and color image denoising. These two tasks represent typical high-level and low-level vision tasks. Compared with real-valued CNN models in these two tasks, our QCNN models show improvements on learning results consistently. Some typical experimental results are shown and analyzed below, and more representative results and details are given in the supplementary file. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Color image classification</head><p>We have tested two QCNN architectures in our research, a shallow network for cifar-10 <ref type="bibr" target="#b18">[19]</ref>, and a relatively deep one for 102 Oxford flowers <ref type="bibr" target="#b24">[25]</ref>. For comparison, real-valued networks with same structure and comparable number of parameters are also trained in the same datasets. Both quaternion and realvalued networks use a real-valued fully-connected layer with softmax function, or a softmax layer to classify the input images. The real-valued networks use ReLU as activation functions, while the quaternion ones adapt ReLU for each imaginary part separately. All those networks are trained with cross entropy loss. Input data is augmented by shifting and flipping. The proposed shallow network for cifar-10 contains 2 convolution blocks, each has 2 convolution layers and a max-pooling layer, and ends with 2 fullyconnected layers. In the experiment, each layer of real-valued CNN and QCNN are of same number of filters, so actually QCNN has more parameters. Both models are optimized using RMSProp <ref type="bibr" target="#b13">[14]</ref> with learning rate set at 0.0001, and learning rate decay set at 1e-6. The training ends at epoch 80.</p><p>The network for 102 Oxford flowers is VGG-S <ref type="bibr" target="#b4">[5]</ref>, which has 5 convolution layers, 3 pooling layers and 3 fully-connected layers. In this experiment, a QCNN with same number of filters as real-valued one and another one with fewer filters to keep the similar number of parameters are both tested. Models are optimized using Adam <ref type="bibr" target="#b17">[18]</ref> with learning rate set at 0.0001. The training ends at epoch 50.</p><p>In <ref type="figure" target="#fig_2">Fig. 2</ref>, we can find that the performance of our QCNNs is consistently better than that of real-valued CNNs. For each data set, the loss function of our QCNN converges more quickly than that of real-valued CNNs in the training phase and reaches smaller loss finally. The classification accuracy on the testing set obtained by our QCNN is also better than that of real-valued CNN even in the very beginning of training phase. Moreover, even if we reduce the number of QCNN's parameters, the proposed QCNN model is still superior to the real-valued CNN with the same size. These phenomena verify our claims before. Firstly, although a QCNN can have more parameters than real-valued CNN, it can suffer less from the risk of over-fitting because of the implicit regularizers imposed by the computation of quaternions. Secondly, the quaternion convolution achieves both the scaling and the rotation of inputs in color space, which preserves more discriminative information for color images, and this information is beneficial for classifying color images, especially for classifying those images  in which the objects have obvious color attributes (i.e., the flowers in 102 Oxford flower data set). The quantitative experimental results are given in <ref type="table" target="#tab_2">Table 1</ref>, which further demonstrates the superiority of our model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Color image denoising</head><p>Besides the high-level vision tasks like image classification, the proposed QCNN can also obtain improvements in the low-level vision tasks. In fact, because our QCNN model can obtain more structural representation of color information, it is naturally suitable for extracting low-level features and replacing the bottom convolution layers of real-valued CNNs. To demonstrate our claim, we test our QCNN model in color image denoising task. Inspired from the encoder-decoder networks with symmetric skip connections for image restoration <ref type="bibr" target="#b23">[24]</ref> and denoising autoencoders <ref type="bibr" target="#b32">[33]</ref>, a U-Net-like <ref type="bibr" target="#b27">[28]</ref> encoder-decoder structure with skip connections is used for denoising in our research. The encoder contains two 2 × 2 average-pooling layers, each following after two 3×3 convolution layers, then two 3 × 3 convolution layers and a fully-connected layer. The decoder is symmetrical to the encoder, containing up-sampling and transposed convolution layers. The layers before pooling and that after up-sampling are connected by shortcuts. A QCNN and a real-valued CNN with this structure are both built, and the QCNN has fewer filters each layer to ensure a similar number of parameters to the real-valued CNN. Similar to networks for classification, both networks use ReLU as activation functions except the top layer, whose activation function is "tanh" function. Both networks are trained with MSE loss.  We trained and tested these two models on two data sets: the 102 Oxford flower data set and a subset of COCO data set <ref type="bibr" target="#b21">[22]</ref>. These two data sets are representative for our research: the flower data set is a case having colorful images, which is used to prove the superiority of our QCNN model conceptually; while the COCO subset is a more general set of natural images, which have both colorful and colorless images and can be used to prove the performance of our model in practice.</p><p>In our experiments, both the training and the testing images are cut and resized to 128 × 128 pixels with values normalized to <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Then a salt and pepper noise which corrupts 30% of pixels and a Gaussian noise with zero mean and 0.01 variance are added. The inputs of networks are corrupted images, and target outputs are original images. For both real-valued CNN and our QCNN model, the optimizer is Adam with 0.001 learning rate, and the batch size is 64 for the 102 Oxford flower data set and 32 for the COCO subset, respectively.  <ref type="table" target="#tab_3">Table 2</ref> shows quantitative comparisons for the real-valued CNN model and the proposed QCNN model. We can find that our QCNN model obtains higher PSNR values consistently on both data sets. The change of loss function and that of PSNR on testing set are given in <ref type="figure" target="#fig_3">Fig. 3</ref> for the two data sets. Similar to the experiments in color image classification task, the loss function of our QCNN converges more quickly to a smaller value and its PSNR on testing images becomes higher than that of the real-valued CNN after 100 epochs. Furthermore, we show a visual comparison for the denosing results of the real-valued CNN and our QCNN in <ref type="figure" target="#fig_4">Fig. 4</ref>. We can find that our QCNN preserves more detailed structures in the image (e.g., the pattern on the plate) than the real-valued CNN does. Suffering from information loss during feature encoding, real-valued CNNs cannot perfectly preserve the details of color images, especially when the structure presents sharp color variations. Our QCNN, on the contrary, can avoid this information loss and learn more texture features even in bottom layers, so it outputs images of higher fidelity. High-resolution visual comparisons can be found in the supplementary file.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Discussion on advantages and limitations</head><p>As aforementioned, our QCNN is motivated for color image representation. When it comes to the images with little variety of colors, our QCNN degrades to a model similar to real-valued CNN, <ref type="bibr" target="#b4">5</ref> and thus, obtains just comparable or slightly worse results in the denoising task, which is confirmed on the COCO subset.</p><p>In particular, according to the results shown above, we can find that the superiority of our QCNN on the COCO subset is not so significant as that on the 102 Oxford flower data set. To further analysis this phenomenon, we pick up those COCO images for which our QCNN shows great advantage as well as those for which our QCNN shows no advantage in the denoising task, and compare them visually in <ref type="figure" target="#fig_5">Fig. 5</ref>. We can find that the images on which our QCNN shows better performance are often colorful, while images where our QCNN is inferior to the real-valued CNN are close to grayscale images.</p><p>To further investigate QCNN's advantages, we use two metrics as quantitative descriptions of "colorful images". The first metric is the mean saturation of color image, denoted as S. For an image, a low S indicates that this image is similar to a grayscale image, while a high S value implies this image is with high color saturation (i.e. many colorful parts). The second metric is the averaged angle between the pixel (color vector) of color image and grayscale axis, denoted as A. For an image, the larger the averaged angle is, the colorful the image is. We show the quantile-quantile plots of these two metrics with respect to the difference between PSNR value of real-valued CNN and that of our QCNN (denoted as D) in <ref type="figure" target="#fig_5">Fig. 5(c)</ref> and <ref type="figure" target="#fig_5">Fig. 5(d)</ref>, respectively. We can find that both S and A are correlated with D positively. It means that our QCNN can show its dominant advantages over real-valued CNNs when the target images are colorful. Otherwise, its performance is almost the same with that of real-valued CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusions and Future Work</head><p>In this paper, we introduce QCNN, a quaternion-based neural network, which obtains better performance on both color image classification and color image denoising than traditional real-valued CNNs do. A novel quaternion convolution operation is defined to represent color information in a more structural way. A series of quaternion-based layers are designed with good compatibility to existing real-valued networks and reasonable computational complexity. In summary, the proposed model is a valuable extension of neural network model in other number fields. In the future, we plan to explore more efficient algorithms for the learning of QCNNs. For example, as we mentioned in section 4.2, for QCNNs their backpropagation of gradients can be represented by reverse rotations of color vectors with respect to the forward propagation of inputs. Such a property provides us a chance to reduce the computation of the backpropagation given the intermediate information of forward propagation and accelerate the learning of QCNNs accordingly. Additionally, we will extend our QCNN model to large-scale data and more applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 .</head><label>1</label><figDesc>Fig. 1. Illustration of the difference between CNN and QCNN on convolution layers.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Suppose that the input is an N -dimensional quaternion vectorâ = [â i ] ∈ H N , for i = 1, 2, 3...N . Applying M 1D quaternion filtering kernels, i.e.,ŵ m = [ŵ m i ] ∈ H M for m = 1, .., M , we obtain an outputb = [b m ] ∈ H M with element</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. (a, b) The loss and the test accuracy of the shallow networks during training on cifar-10. (c, d) The loss and the test accuracy of the VGG-S networks during training on 102 Oxford flower data set (256 test images picked randomly from the test set for each epoch).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. (a, b) The loss and the PSNR of test images using proposed denoising networks during training on 102 Oxford flower data set. (c, d) The loss and the PSNR of test images using proposed denoising networks during training on COCO subset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Denoising experiment on a image of snacks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. In the denoising task, QCNN shows at least 0.5dB higher PSNR than CNN for images in (a). For images in (b), CNN offers better result. (c) The quantile-quantile plot of saturation versus PSNR difference. (d) The quantile-quantile plot of average angle between color vectors and gray axis versus PSNR difference.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>These quaternion operations can be used to represent rotations in a three- dimensional space. Suppose that we rotate a 3D vector q = [q 1 q 2 q 3 ] ⊤ to get a new vector p = [p 1 p 2 p 3 ] ⊤ , with an angle θ and along a rotation axis w = [w 1 w 2 w 3 ]</figDesc><table>⊤ , w 

2 

1 + w 

2 

2 + w 

2 

3 = 1. Such a rotation is equivalent to the following 
quaternion operation:p 

=ŵqŵ 
 *  , 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>2 parameters: each kernel has CL 2 angle parameters [θ ll ′ c ] and CL 2 scaling parameters [s ll ′ c ]. To process C N × N × 3 color feature maps, we need 9L 2 N 2 KC multiplications because each output quaternion f kk ′ requires 9L</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Experiment results in classification tasks</figDesc><table>Model 
Dataset Test accuracy 
Shallow real network 
Cifar-10 
0.7546 
Shallow quaternion network 
Cifar-10 
0.7778 
Real-valued VGG-S 
102 flowers 
0.7308 
Quaternion VGG-S 
102 flowers 
0.7695 
Quaternion VGG-S with fewer filters 102 flowers 
0.7603 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Experiment results in denoising tasks</figDesc><table>Model 
Dataset Test PSNR (dB) 
Dataset 
Test PSNR (dB) 
Real-valued CNN 102 flowers 
30.9792 
subset of COCO 
30.4900 
Quaternion CNN 102 flowers 
31.3176 
subset of COCO 
30.7256 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Without the loss of generality, we assume that both the width and the height of image are equal to N in the following content.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">As we mentioned in section 3.1, for grayscale images, QCNNs perform exactly the same as real-valued CNNs with same number of filters.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported in part by National Science Foundation of China (61671298, U1611461, 61502301, 61521062), STCSM (17511105400, 17511105402, 18DZ2270700), China's Thousand Youth Talents Plan, the 111 project B07022, the MoE Key Lab of Artificial Intelligence, AI Institute of Shanghai Jiao Tong University, and the SJTU-UCLA Joint Center for Machine Perception and Inference. The corresponding author of this paper is Yi Xu (xuyi@sjtu.edu.cn).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The theory and use of the quaternion wavelet transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bayro-Corrochano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="19" to="35" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The motor extended kalman filter: A geometric approach for rigid motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bayro-Corrochano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Mathematical Imaging and Vision</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="205" to="228" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric neural computing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Bayro-Corrochano</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="968" to="986" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multi-person 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning a deep convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="184" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Sparse representation based on vector extension of reduced quaternion matrix for multiscale image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IET Image Processing</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="598" to="607" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gaudet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Maida</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1712.04604</idno>
		<title level="m">Deep quaternion networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">The quaternion group and modern physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">R</forename><surname>Girard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">European Journal of Physics</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">25</biblScope>
			<date type="published" when="1984" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<title level="m">Mask r-cnn</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Generalization characteristics of complex-valued feedforward neural networks in relation to signal coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hirose</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yoshida</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans Neural Netw Learn Syst</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Midkiff: Color face recognition using quaternionic gabor filters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">F</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Abbott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Chair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Conners</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ehrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jacobs</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Image restoration using very deep convolutional encoder-decoder networks with symmetric skip connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2802" to="2810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, Graphics &amp; Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
	<note>ICVGIP&apos;08. Sixth Indian Conference on</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the critical points of the complex-valued neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Neural Information Processing</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1099" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A quaternary version of the back-propagation algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nitta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks, 1995. Proceedings., IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1995" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="2753" to="2756" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Fourier transforms of colour images using quaternion or hypercomplex, numbers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Sangwine</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronics letters</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">21</biblScope>
			<biblScope unit="page" from="1979" to="1980" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Quaternion neural-network-based polsar land classification in poincare-sphere-parameter space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hirose</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Geoscience and Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">52</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="5693" to="5703" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Trabelsi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bilaniuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Serdyuk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Subramanian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mehri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rostamzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Pal</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09792</idno>
		<title level="m">Deep complex networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1096" to="1103" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Image denoising and inpainting with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Traversoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">QWT: Retrospective and New Applications</title>
		<meeting><address><addrLine>London</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Vector sparse representation of color image using quaternion matrix analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Nguyen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1315" to="1329" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Quaternion-based sparse representation of color image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Multimedia and Expo (ICME), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="7" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Single color image super-resolution using quaternion-based sparse representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="5804" to="5808" />
		</imprint>
	</monogr>
	<note>2014 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Design, implementation, and experimental results of a quaternion-based kalman filter for human body motion tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">R</forename><surname>Bachmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Robotics</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1216" to="1227" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Color image classification via quaternion principal component analysis network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Senhadji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">216</biblScope>
			<biblScope unit="page" from="416" to="428" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
