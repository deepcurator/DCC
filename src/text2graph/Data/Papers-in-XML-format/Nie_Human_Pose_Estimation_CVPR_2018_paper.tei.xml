<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Human Pose Estimation with Parsing Induced Learner</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xuecheng</forename><surname>Nie</surname></persName>
							<email>niexuecheng@u.nus.eduelefjia@nus.edu.sgzuoym15@mail.tsinghua.edu.cnyanshuicheng@360.cn</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiming</forename><surname>Zuo</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electrical and Computer Engineering</orgName>
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Qihoo 360 AI Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Human Pose Estimation with Parsing Induced Learner</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human pose estimation is a fundamental task in computer vision, aiming to estimate joint locations of human body. Recent years have witnessed many efforts made to push its performance frontier <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>, but it remains challenging to apply it to realistic scenarios. Distracting factors, e.g. occlusion, self-similarity, large deformation, huge variation in pose configuration and appearance, often lead to inaccurate joint localization and even false joint categorization. As shown in <ref type="figure" target="#fig_0">Figure 1</ref> (a), partial occlusion on left arm causes inaccurate localizations of left wrist, while high similarity between left and right legs results in false categorization of right ankle.</p><p>Human body parts, generated by human parsing methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37]</ref>, can provide useful contextual cues to help localize body joints in these challenging scenarios. For instance, when body part cues (from left-lower arm and right-lower leg, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (b)) are taken into account in joint localization, estimation errors on the joints of left wrist and right ankle can be effectively corrected, as shown in <ref type="figure" target="#fig_0">Figure 1</ref> (c). Motivated by this, some research works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> exploit the parsing information to help improve pose estimation performance. However, they generally perform human body parsing and pose estimation separately and utilize parsing results to refine body joint localization as post processing. Although some improvement has been achieved, they do not fully utilize parsing information in an effective and efficient way, thus suffer several limitations. First, they hand-craft features from parsing results which are not powerful or robust to large pose variations in the wild. Second, they only use parsing information for inference other than learning pose models and therefore do not strengthen pose models essentially. Third, their overall frameworks are not end-to-end learnable.</p><p>In this work, we propose to leverage human parsing information more effectively and efficiently for learning better pose estimation models and improving their performance. Targeting at the above limitations of existing works, we make following observations. First, the parsing representations should be learned towards being beneficial to pose estimation, instead of solely learned from the parsing supervision.</p><p>Second, the learned parsing representations should be effectively transferable to the pose estimation domain. Traditional multi-task learning frameworks <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b11">12]</ref> keep the architecture for two tasks tied, which blurs the feature distinctiveness for pose estimation and parsing and limits their mutual benefits. Third, the pose estimation model should be dynamic and can fast adapt to various testing samples of different characteristics, relying on the transferred parsing information. Furthermore, human parsing annotation is available <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b37">38]</ref> nowadays, providing us easy access to such information.</p><p>According to the above observations, we design a novel Parsing Induced Learner (PIL) that learns to fast adapt the pose estimation model conditioned on the parsing information extracted from a specific sample, and therefore effectively improves both performance and flexibility of the model. The proposed PIL refines inaccurate localization and corrects false categorization of body joints effectively, which are difficult to address for a static pose estimation model. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, PIL can exploit body part cues to constrain joint location and pose structure.</p><p>In particular, PIL consists of two components: an encoder that encodes an input image into high-level parsing representations, and an adapter that learns to adapt parameters of the pose model by leveraging parsing representations. The adaptive parameters predicted by PIL can help the pose model learn more tailored representations for estimating poses for each specific input, in which body part cues are effectively integrated for constraining joint locations and pose structures. Moreover, PIL can efficiently learn to adapt pose parameters in one-shot manner, yielding fast adaption of the base pose model according to parsing information. We implement PIL by combining a parsing encoder network and a parameter adapter network, which can be directly applied to various deep pose models and across different datasets. The parameter adapter network is trained with supervision from human pose to learn adaptive parameters for boosting pose estimation. The parsing encoder is learned from both the human parsing and pose annotations. Our whole model integrating pose estimation and PIL is end-to-end learnable.</p><p>Comprehensive experiments on popular benchmarks show that the PIL effectively improves performance for both single-and multi-person pose estimation to new state-of-theart. We also conduct cross-dataset experiments to demonstrate its generalizability and transferability of exploiting parsing information from one dataset to another. Our contributions are three-fold. Firstly and most importantly, we propose a novel Parsing Induced Learner for efficiently learning to adapt pose estimation models by exploiting parsing information, achieving better pose estimation performance. Secondly, the proposed PIL is transferable across datasets, verified via experiments to apply the PIL trained on LIP dataset to MPII dataset, achieving both performance improvement and learning acceleration. Thirdly, with the help of PIL, an Hourglass network <ref type="bibr" target="#b24">[25]</ref> based pose estimation model achieves new state-of-the-art on multiple benchmarks for both single-and multi-person pose estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Recently, research efforts have been devoted to both single-and multi-person pose estimation problems, via exploring network architecture engineering <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b39">40]</ref>, enhancing training supervision <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref>, and improving inference strategy <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. However, they are still challenged by some distracting factors, e.g., occlusion and selfsimilarity causing inaccurate joint locations and false joint categorization. Human body part cues from human parsing methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b36">37]</ref> can provide useful guidance to address the above challenges for constraining joint locations and pose structures. Motivated by this, some works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> have exploited parsing information to improve the performance of pose estimation.</p><p>In <ref type="bibr" target="#b38">[39]</ref>, <ref type="bibr">Yamaguchi et al.</ref> proposed to use the normalized histograms of parsing labels around each location as additional features for refining the pose estimation results. In <ref type="bibr" target="#b19">[20]</ref>, <ref type="bibr">Ladicky et al.</ref> proposed to use semantic segmentation of body parts to provide information on the appearance and shape of body joints. In <ref type="bibr" target="#b12">[13]</ref>, Dong et al. proposed the Grid Layout Feature to model the pairwise geometry relations between semantic parts and mixtures of joint-group templates, and then constructed the "And-Or" graph for simultaneously estimating joint locations and semantic labels. In <ref type="bibr" target="#b37">[38]</ref>, Xia et al. proposed to utilize semantic segmentation results to formulate additional features as the segment-joint smoothness term to encourage semantic and spatial consistency between parts and joints, and they modeled the multi-person pose estimation problem as a fully-connected conditional random field and solved it based on an Integer Linear Programming.</p><p>Despite the success of these existing works, they suffer from some obvious drawbacks on feature extraction from parsing information and guidance exploitation to learn pose models, which hamper them from sufficiently leveraging body part cues to estimate joint allocations. In contrast, our proposed Parsing Induced Learner can utilize parsing information to directly learn to extract features beneficial to pose estimation models rather than hand-crafting them. In addition, it can be integrated into the learning procedure of human pose estimation models by auxiliary parsing supervision. Moreover, our whole framework can be efficiently end-to-end learnable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Approach</head><p>body joints with assistance of the corresponding (estimated) human parsing map S ∈ {0, 1, . . ., L} M ×N of I. Here, (x i , y i ) are coordinates of the ith joint, and J and L are the number of joint and body part categories, respectively. In particular, 0 in S denotes the background category.</p><p>Existing works <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b38">39]</ref> suggest applying the parsing map S in post processing to refine the pose estimation P . However, such a strategy does not essentially refine or enhance the pose estimation model. Differently, we propose a generic parsing induced pose estimation model f [θ,θ ′ ] (parameterized by θ and θ ′ together) to fully leverage parsing information learned from the pair (I, S) in a flexible and effective way for getting more accurate pose estimation P , which is formulated as</p><formula xml:id="formula_0">f [θ,θ ′ ] : I → P, where θ ′ = g(I, S).<label>(1)</label></formula><p>The above formulation features the essential difference between our proposed pose estimation model and existing ones.</p><p>We enforce a part of the pose model parameters θ ′ to explicitly depend on the parsing map S for better leveraging the parsing information through the function g(·, ·). Different parsing maps S will induce different parameters θ ′ and thus modify the pose model f [θ,θ ′ ] dynamically. In this way, the pose model can be adaptive to the input image I and parsing result S fast and favorably through learning a proper model parameter prediction g(·, ·) end-to-end.</p><p>In particular, inspired by the "learning to learn" framework <ref type="bibr" target="#b1">[2]</ref>, we design a Parsing Induced Learner (PIL) to learn a well-performing function g(·, ·) such that the predicted parameters θ ′ can tailor the pose model to each input image based on parsing information and provide better pose estimation results. The proposed PIL consists of a parsing encoder for extracting the parsing features and a parameter adapter for learning the dynamic parameters θ ′ , denoted as E S θ S (·) and K φ (·), respectively.</p><p>In PIL, the parameter adapter K φ (·) takes in the features output by the parsing encoder E S θ S (·) and predicts proper parameters θ ′ for the pose estimation model. Namely,</p><formula xml:id="formula_1">θ ′ = g(I, S) := K φ E S θ S (I) . The pose estimation model f [θ,θ ′ ] contains an adaptive pose encoder E P [θ P ,θ ′ ] (·)</formula><p>, which adopts the predicted parameter θ ′ from the PIL model and the other global parameter θ P to output good features for body joint localizations. Introducing such a PIL model provides dynamic parameters θ ′ and enables the adaptive pose encoder to fully exploit parsing information to extract better features for more accurate pose estimation through efficiently adapting its model parameters to specific input in one-shot.</p><p>On top of these two encoders are pose and parsing classifiers C S from a parsing encoder. Then, our model feeds F P and θ ′ to an adaptive convolution to extract parsing induced features F a for fast adaption of the pose model. Our model regards F a as residual information and fuses it with F P via addition, leading to the refined features F P * for body joint localization. Finally, our model inputs F P * and F S to pose and parsing classifiers, respectively, to produce pose estimation and parsing prediction (ignored during testing).</p><p>and S, to jointly learn PIL with the pose and parsing models, we define the following loss function for training:</p><formula xml:id="formula_2">L := L P C P w P (E P [θ P ,θ ′ ] (I)), P +βL S C S w S (E S θ S (I)), S ,<label>(2)</label></formula><p>where L P and L S represent the pose and parsing loss functions respectively, defined in Sec. 3.3, and β is a trade-off coefficient. Below, we will explain the implementation details for each component in our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Network Architecture</head><p>Our implementation is based on deep Convolutional Neural Networks (CNNs). The overall architecture is shown in <ref type="figure" target="#fig_1">Figure 2</ref>. We now explain each component in details. Pose Encoder The pose encoder E P θ P (·) extracts discriminative features F P = E P θ P (I) from the input image I for pose estimation. We implement it via CNNs and evaluate two different network architectures in this work: one is the VGG16 based Fully Convolutional Network (FCN) <ref type="bibr" target="#b31">[32]</ref> and the other is the state-of-the-art Hourglass network <ref type="bibr" target="#b24">[25]</ref>. For the VGG16 based FCN architecture, we further remove the last two max pooling layers to reduce its total stride from 32 to 8 for more accurate joint localizations. For the Hourglass network, we follow the configurations in <ref type="bibr" target="#b24">[25]</ref>. Parsing Encoder The parsing encoder E The parameter adapter takes the features F S from the parsing encoder as input and outputs the adaptive convolution parameters θ ′ . It is composed by stacking three convolution layers and two pooling layers. For each layer, the kernel size, the number of channels/pooling type, stride and padding size are specified from top to bottom. E S θ S (I) for encoding useful information for both parsing and pose estimation. In our implementation, the parsing encoder is also based on CNNs. Similarly, we also explore it with two different network architectures: the VGG16 based FCN and the Hourglass network. Note the parsing encoder need not be the same as the pose encoder as they are independent. Parameter Adapter The parameter adapter K φ (·) is the other component of the PIL model, which is a one-shot learner to predict the dynamic parameters θ ′ via taking in the output F S of I from the parsing encoder network. We implement it by a small CNN with learnable parameters φ. In particular, the parameter adapter network predicts certain convolutional kernels of the pose encoder network. Its architecture is shown in <ref type="figure" target="#fig_2">Figure 3</ref>. The tensor θ ′ ∈ R h×h×c output from the last layer of the parameter adapter network is taken as the predicted dynamic convolutional kernels. Here h=7 is the convolution kernel size and c=c i ×c o is the number of channels to learn for adaptive convolution with c i and c o as the number of input and output channels, respectively.</p><p>In practice, however, it is infeasible for the parameter adapter to directly predict all convolution parameters due to their large scale. For instance, given input feature maps with 256 channels and output feature maps with 256 channels, the number of convolution filters to be predicted by the parameter adapter is 256×256. The large scale parameters to predict would cause high space and time cost, and may result in overfitting <ref type="bibr" target="#b1">[2]</ref>. To avoid these issues, we perform the following factorization <ref type="bibr" target="#b1">[2]</ref> on the dynamic convolutional kernels θ ′ to reduce the number of free parameters:</p><formula xml:id="formula_3">θ ′ = U * θ * c V,<label>(3)</label></formula><p>where * denotes the convolution operation, * c is the channelwise convolution, U and V are auxiliary parameters to learn for the adaptive convolution and are explained in the next part.θ∈R h×h×ci are the actual parameters to predict by the parameter adapter, with smaller size than original θ ′ by a magnitude. Eqn. <ref type="formula" target="#formula_3">(3)</ref> is analogous to SVD, whereθ can be seen as coefficients over the parameter bases U and V . Adaptive Convolution To make best use of dynamic convolutional kernels θ ′ from the parameter adapter for directly extracting features to assist human pose estimation, we apply θ ′ on the highest-level features F P generated from the pose encoder, resulting in an adaptive convolution layer. The adaptive convolution layer is similar with the traditional convolution layer, just with the static convolution kernels replaced by the predicted dynamic convolution kernels θ ′ :</p><formula xml:id="formula_4">F a = θ ′ * F P = U * θ * c V * F P ,</formula><p>where F a denotes the extracted features by dynamic parameters θ ′ , U ∈R 1×1×ci×co and V ∈R 1×1×ci×ci are auxiliary learnable parameters. In particular, we ignore the bias parameters in the adaptive convolution layer, due to the residual feature fusion strategy explained in the next part. Different from traditional CNN based features, F a is extracted in an efficient way by the dynamic parameters θ ′ based on parsing information for a given input image, rather than previous hand-crafted parsing based features for human pose estimation. Moreover, the parameter basis θ ′ of the adaptive convolution layer can be efficiently learned by the proposed PIL in one-shot, getting rid of iteratively updating weights based on large training datasets. In the implementation for the adaptive convolution layer, given F P , we first use a 1×1 convolution on it with V , then conduct the dynamic convolution by group withθ, and finally adopt another 1×1 convolution with U to generate F a .</p><p>Feature Fusion Different with features F P from the pose encoder network, F a is extracted based on parsing information and complementary to F P for human pose estimation. Hence, we adopt the residue learning idea <ref type="bibr" target="#b16">[17]</ref> to regard F a as a residue component, and fuse it with the original features F P via addition:</p><formula xml:id="formula_5">F P * = F P + F a ,</formula><p>where F P * is the final feature refined by parsing information for human pose estimation. Classifiers After generating the final feature F P * for human pose estimation, we exploit a linear classifier C P w P (·) on it to generate the predicted confidence maps for each kind of joints, by implementing a 1×1 convolution on F P * . Similarly, we exploit another linear classifier C S w S (·) on F S to generate the parsing prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training and Inference</head><p>As defined in Eqn. (2), we introduce two supervision to train the overall network model. We use Mean Square Error loss as L P for training the pose model and PIL, and use Cross Entropy loss as L S together with L P to train the parsing model. The overall model is end-to-end trainable by gradient backpropagation.</p><p>In fact, the PIL can also be pretrained on one dataset and directly applied to assist pose estimation on new datasets. In other words, the PIL is able to transfer acquired parsing information across different application datasets. We verify this property of PIL in the experiments.</p><p>During inference, the pose and parsing encoder networks take in the same image. The parameter adapter learns the parsing related convolution kernels in one feed-forward pass. We ignore the predicted parsing results and only take the output from the pose classifier for human pose estimation. For single-person pose estimation, we directly output the positions with maximum responses for each type of body joints. For multi-person pose estimation, we perform NMS to find joint candidates on the predicted confidence maps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Setup</head><p>Datasets We evaluate our proposed model on three most popular benchmarks for human pose estimation: Look into Person (LIP) <ref type="bibr" target="#b15">[16]</ref>, extended PASCAL-Person-Part <ref type="bibr" target="#b37">[38]</ref>, and MPII Human Pose Single-Person (MPII) <ref type="bibr" target="#b0">[1]</ref>, ranging from single-person to multi-person pose estimation and presenting various challenging scenarios.</p><p>The LIP dataset is a large-scale single-person dataset providing both human pose and parsing annotations, including locations for 16 body joints and annotations for 19 semantic body parts with one background category. In total, there are 50,462 images, which are split into three subsets: 30,462 for training, 10,000 for validation, and 10,000 for testing.</p><p>The extended PASCAL-Person-Part dataset presents multi-person images with both pose and parsing annotations for 14 body joints and 6 body parts. The total 3,533 images are split into 1,716 for training and 1,817 for testing.</p><p>The MPII dataset is another large-scale benchmark for single-person pose estimation. It contains 19,185 training and 7,247 testing images but only provides pose annotations for 16 body joints. On this dataset, we aim to evaluate the cross-dataset generalizability and transferability of the proposed PIL, i.e., how well the model learns useful and transferable parsing information from one dataset (LIP) to assist pose estimation on another new dataset (MPII). , and horizontally mirror, but no translation augmentation. We resize and pad training samples to size 256×256 before inputting to CNNs. These augmentations are common and also used by previous works for both single-and multi-person pose estimation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Data Augmentation</head><p>Implementation For the LIP and extended PASCALPerson-Part datasets, we train the overall network from scratch on their individual training samples. Since PASCALPerson-Part images contain multiple persons, we need to associate joint candidates to corresponding person instances. In experiments, we follow the approach in <ref type="bibr" target="#b25">[26]</ref>, which learns to allocate joints simultaneously with the joint detection model. For MPII dataset, we directly use the parsing encoder and parameter adapter trained on the LIP dataset as PIL without further fine-tuning. We train the pose estimation network and auxiliary parameters in the adaptive convolution layer from scratch using the training samples from this dataset. We implement the proposed model using PyTorch <ref type="bibr" target="#b26">[27]</ref>. We use RMSProp <ref type="bibr" target="#b32">[33]</ref> as the optimizer. The learning rate is initially set as 0.0025, and decreased by multiplying 0.5 at the 150th, 170th and 200th epoch. We train all the models for 250 epochs in total. Testing is conducted on six-scale image pyramids with flipping. Our code will be made available.</p><p>Metrics The PCK <ref type="bibr" target="#b40">[41]</ref> and Mean Average Precision (mAP) <ref type="bibr" target="#b28">[29]</ref> are used for performance evaluation on the LIP and extended PASCAL-Person-Part datasets, respectively. We use the official PCKh metric <ref type="bibr" target="#b0">[1]</ref> for performance evaluation on MPII dataset, following conventions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results on LIP Dataset</head><p>Ablation Analysis To evaluate our proposed model, we investigate two different backbone networks on the LIP validation set: the prevalent VGG16 network <ref type="bibr" target="#b31">[32]</ref> successfully applied to various computer vision tasks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b30">31]</ref>, and the state-of-the-art Hourglass network <ref type="bibr" target="#b24">[25]</ref> for human pose estimation. We first evaluate our proposed Parsing Induced Learner (PIL) based on VGG16 and compare it with various popular strategies (including feature fusion through adding, multiplying and concatenating) on exploiting parsing features for pose estimation, in order to demonstrate its efficacy. The results are summarized in <ref type="table" target="#tab_1">Table 1</ref>, where VGG16-PIL(VGG16) denotes our proposed full model with VGG16 for both pose and parsing encoder networks, and VGG16-Add/Multi/Concat represent the models using other parsing utilization strategies. We also compare the adaptive ability of PIL with the traditional multi-task learning framework <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b41">42]</ref> for joint human parsing and pose estimation, which is implemented based on VGG16 for representation learning with both pose and parsing supervision, denoted as VGG16-MTL. To disentangle effects of the residual module followed pose encoder on the estimation performance from network architecture engineering, we also evaluate another variant of our model by removing the proposed PIL and replacing the adaptive convolution layer with the traditional convolution layer, denoted as VGG16-Self.</p><p>From <ref type="table" target="#tab_1">Table 1</ref>, one can observe that the proposed VGG16-PIL(VGG16) improves the baseline VGG16 by 8.5% in terms of the average PCK, from 69.1% to 75.0%. This result clearly shows our proposed model is effective at exploiting parsing information to learn powerful representations for  Comparing VGG16-PIL(VGG16) with VGG16-Add/Multi/Concat baselines again demonstrates the improvement is not simply due to using parsing features. Although accessing and fusing parsing features, VGG16-Add/Multi/Concat even harm the pose estimation performance. This demonstrates naive feature fusion is not an effective way of utilizing parsing information as expected. Traditional multi-task learning framework VGG16-MTL suffers performance decline on human pose estimation, showing that directly introducing parsing supervision in training cannot effectively adapt parsing information to the pose estimation model. Comparing with VGG16-MTL, our PIL can effectively adapt parsing information to both learning and inference processes of human pose estimation. Adding residual module to the VGG16 backbone as VGG16-Self improves performance incrementally (from 69.1% to 69.8%). This confirms that the proposed PIL extracts valuable information from parsing for human pose estimation rather than benefiting from the network architecture engineering.</p><p>We conduct similar ablation analysis on the proposed model using state-of-the-art architecture, the Hourglass network <ref type="bibr" target="#b24">[25]</ref>, for human pose estimation. The results are shown in <ref type="table">Table 2</ref>, in which HG-ms-nu denotes the Hourglass network consisting of m stacked Hourglass modules and each module with n unit depth (32 layers). HG-ms-nu-PIL (HG- m ′ s-n ′ u) denotes the model with the proposed PIL. We make such choices to comprehensively evaluate the Hourglass network and its counterparts with our proposed PIL on the LIP dataset, aiming to analyze the effects of the depth and stages of Hourglass network on the performance of our proposed model for human pose estimation.</p><p>From <ref type="table">Table 2</ref>, one can observe that the proposed model always brings performance improvement over the backbone networks even though their performance is already very high. We can also observe that although HG-1s-1u-PIL (HG-1s-1u) and HG-2s-1u have similar numbers of parameters, HG-1s-1u-PIL (HG-1s-1u) achieves superior performance 82.2% PCK, compared with HG-2s-1u that achieves 80.8% PCK, which also shows the efficiency and effectiveness of the proposed model in exploiting parsing information for human pose estimation. In addition, we can find that PIL with a smaller parsing network can also improve the performance. More importantly, the proposed model gives new state-ofthe-art of 85.6% PCK on the LIP validation dataset.</p><p>We also conduct ablation experiments to study how different parsing networks (with different parsing qualities) affect human pose estimation. We fix the pose network as HG-4s-2u, and increase the depth of the parsing network from HG-1s-1u to HG-1s-8u to obtain increasingly better parsing performance measured by Mean Intersection over Union (mIOU) <ref type="bibr" target="#b15">[16]</ref> from 41.1% to 43.9%. The results in <ref type="table">Table 3</ref> reveal the trend that better parsing performance gives better pose estimation, reflecting the proposed PIL is good at exploiting parsing information.</p><p>Comparison with State-of-the-arts We compare the proposed model HG-8s-1u-PIL(HG-1s-2u) with state-of-thearts on the LIP testing set. The results are shown in <ref type="table" target="#tab_2">Table 4</ref>. In particular, the model proposed in <ref type="bibr" target="#b9">[10]</ref> wins the CVPR 2017 LIP Human Pose Estimation Challenge. It uses a self adversarial training strategy for refining the pose estimation. The other two models, BUPTMM-POSE and Hybrid Pose Machines, combine the predictions of Hourglass networks and convoutional pose machines. Without sophisticated refinement or model ensemble, our proposed model outperforms all these well established baselines and achieves new state-of-the-art 87.5% PCK. The proposed model is superior to <ref type="bibr" target="#b9">[10]</ref> for most body joints, though the adversarial training is slightly better at refining joints of knee and ankle. By the PCK measurement, there is a tolerance between the prediction and the groundtruth, and the estimated pose structure is more important than the absolute joint location. Hence, our approach is not significantly superior to <ref type="bibr" target="#b9">[10]</ref> under the PCK measurement, since the adversarial training strategy can also appropriately constrain the predicted pose structure. Nevertheless, our proposed model can help generate more accurate joint locations. From experiments on MPII dataset in Sec. 4.4, we can observe that our proposed model significantly outperforms <ref type="bibr" target="#b9">[10]</ref> under the AUC measurement.</p><p>Qualitative Results We visualize some qualitative results in <ref type="figure" target="#fig_6">Figure 5</ref> (a) to better show the effectiveness of the proposed model in exploiting parsing information. From the results, one can observe the PIL corrects false detections on elbows caused by occlusion, benefiting from left and right arm part cues. In addition, PIL helps recover the missed detection on right lower arm due to large pose variations in the second image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on PASCAL-Person-Part Dataset</head><p>As the extended PASCAL-Person-Part dataset involves multi-person pose estimation, we re-implement the model proposed in <ref type="bibr" target="#b25">[26]</ref> with Hourglass network as the backbone. In particular, the pose network adopts Hourglass with 8 stacked modules HG-8s-1u and the parsing network of PIL is a much smaller one with only 1 Hourglass module HG-1s-2u.</p><p>Our performance and comparison with state-of-the-arts are shown in <ref type="table" target="#tab_3">Table 5</ref>. The vanilla baseline model (without PIL) achieves 39.6% mAP. Introducing the PIL improves the performance to 41.0% mAP, offering a new state-of-the-art on this dataset. Moreover, our proposed model outperforms the best performing baseline <ref type="bibr" target="#b37">[38]</ref> by a margin 2% mAP. These results also demonstrate the strong generalizability of our proposed model from single-person to multi-person pose estimation domain. <ref type="figure" target="#fig_6">Figure 5</ref> (c) shows qualitative results for multi-person pose estimation. The proposed PIL effectively refines the joint localizations by better exploiting the part cues. In particular, it successfully helps isolate joints from neighboring persons that are easy to confuse (see the first and second examples). Moreover, PIL corrects the false joint detections, as shown in the third and forth examples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Results on MPII Dataset</head><p>We consider a more challenging scenario where the PIL model is trained on a different dataset, aiming to evaluate the transferability of our proposed model on "learning to adapt"   across different datasets. In the experiments, we use HG8s-1u as the backbone pose network and HG-1s-2u as the parsing network of PIL. As MPII does not provide parsing annotations, we directly exploit the PIL trained on the LIP dataset. We fix the parameters of PIL to predict dynamic filtersθ from LIP dataset, and learn auxiliary parameters U and V with the pose model, together.</p><p>Ablation Analysis We use the same validation set with Tompson et al. <ref type="bibr" target="#b33">[34]</ref> to conduct the ablation analysis, and show results in <ref type="table" target="#tab_4">Table 6</ref>. Our implementation of HG-8s-1u achieves 90.2% PCKh. Introducing PIL improves the performance to 91.0% PCKh. We also find that PIL improves prediction accuracy for all body joints, although it is trained on a different dataset. This demonstrates that our proposed model can successfully transfer useful parsing information from LIP dataset to MPII dataset. In <ref type="figure" target="#fig_4">Figure 4</ref>, we also plot the training accuracy and loss of models with or without PIL on MPII training set. we can find  that both increase of training accuracy and decrease of loss go much faster with the help of PIL. These results demonstrate the effectiveness of the proposed PIL on accelerating the learning speed of human pose estimation model.</p><p>Comparison with State-of-the-arts <ref type="table" target="#tab_5">Table 7</ref> shows comparison of our model with state-of-the-arts. With the PIL from LIP dataset, our model achieves new state-of-the-art 92.4% PCKh. One can also observe that the PIL improves the performance for most of the joints, except for the knee. The reason lies in the differences between LIP dataset and MPII dataset, which make the PIL model from LIP dataset unable to cover all variations in the configuration of knees in MPII dataset. Hence, our model cannot outperform <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10]</ref> with adversarial training for constraining joint configurations of human body. Moreover, our model significantly improves AUC over the best performing baseline from 64.2% to 65.9%, showing PIL can indeed effectively utilize parsing information to better localize body joints.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Qualitative Results</head><p>Qualitative results are shown in <ref type="figure" target="#fig_6">Figure 5 (b)</ref>. One can observe that the PIL model trained on LIP dataset can perform well for some images on MPII dataset, e.g. the first example. In this case, PIL successfully transfers parsing information and corrects false detections on legs of the person due to self ambiguity. In the second example, the parsing model fails to generate high-quality parsing results. However, we surprisingly find that PIL is still able to provide useful cues to refine the pose estimation on hands. This shows the good generalizability of PIL to transfer "learning to adapt" information across datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we proposed a novel Parsing Induced Learner (PIL) to assist human pose estimation by effectively exploiting parsing information. PIL learns to predict certain pose model parameters from parsing features and adapts the pose model to extracting complementary useful features. The whole model is end-to-end trainable. Comprehensive experiments on single-and multi-person pose estimation benchmarks LIP and extended PASCAL-Person-Part demonstrated advantages of the proposed PIL over other parsing utilization approaches, including traditional multi-task learning. In addition, cross-dataset evaluation by utilizing PIL trained on LIP dataset to MPII dataset showed the PIL offers appealing transferability. Even if the applied dataset does not provide any parsing information, externally pre-trained PIL still helps the model achieve new state-of-the-art.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of our motivation for the proposed Parsing Induced Learner. (a) Pose estimation result without exploiting parsing information. (b) Parsing information generated from the proposed PIL. (c) Pose estimation result with the proposed PIL. The proposed PIL effectively leverages parsing information to refine the inaccurate locations and correct false categorizations for the highlighted body joints.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Overall architecture of our model. Given an input image, our model first utilizes a pose encoder to extract pose features F P and the proposed PIL to predict dynamic parameters θ ′ through a parameter adapter taking in parsing features F S from a parsing encoder. Then, our model feeds F P and θ ′ to an adaptive convolution to extract parsing induced features F a for fast adaption of the pose model. Our model regards F a as residual information and fuses it with F P via addition, leading to the refined features F P * for body joint localization. Finally, our model inputs F P * and F S to pose and parsing classifiers, respectively, to produce pose estimation and parsing prediction (ignored during testing).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The architecture of the parameter adapter in PIL. The parameter adapter takes the features F S from the parsing encoder as input and outputs the adaptive convolution parameters θ ′ . It is composed by stacking three convolution layers and two pooling layers. For each layer, the kernel size, the number of channels/pooling type, stride and padding size are specified from top to bottom.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Training accuracy and loss on MPII training set, shown in (a) and (b) respectively, to demonstrate the proposed PIL can accelerate learning speed of human pose estimation model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative results on (a) LIP dataset, (b) MPII dataset, and (c) extended PASCAL-Person-Part dataset. For each row, the left image shows the pose estimation result of the baseline model HG-8s-1u, the middle one shows the parsing map predicted by the proposed PIL, and the right one shows the pose estimation result of our proposed model HG-8s-1u-PIL(HG-1s-2u).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>For the LIP and extended PASCAL- Person-Part datasets, we crop training samples on original images based on the person center. We augment each train- ing sample with rotation degrees in [−40</figDesc><table>• , 40 
• ], scaling 
factors in [0.8, 1.5], translational offset [−40px, 40px], and 
horizontally mirror. For MPII dataset, we augment each 
training sample with rotation degrees in [−30 
• , 30 
• ], scaling 
factors in [0.7, 1.3]</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>VGG16 based ablation studies on LIP validation set. The model in parenthesis denotes the parsing network used in PIL. Head Sho. Elb. Wri. Hip Knee Ank. U.Body PCKTable 2. Hourglass based ablation studies on LIP validation set. The model in parenthesis denotes the parsing network used in PIL.Table 3. Experiments on impacts of the parsing performance (mea- sured by mIOU) on human pose estimation in our model. The model in parenthesis denotes the parsing network used in PIL. mIOU Head Sho. Elb. Wri. Hip Knee Ank. U.Body PCK</figDesc><table>VGG16 
88.0 80.0 68.0 69.5 48.9 60.2 64.5 76.6 69.1 
VGG16-Add 
88.1 79.5 68.0 69.9 48.3 59.7 61.7 76.6 68.6 
VGG16-Multi 
87.3 75.0 60.2 65.1 42.7 51.9 58.4 72.2 63.7 
VGG16-Concat 
88.2 77.7 64.1 67.4 44.0 55.2 61.0 74.6 66.1 
VGG16-MTL 
87.4 76.5 61.9 66.1 46.0 53.8 59.8 73.3 65.3 
VGG16-Self 
88.0 81.1 70.0 69.5 50.3 61.2 63.5 77.4 69.8 

VGG16-PIL(VGG16) 90.0 83.3 75.2 75.0 57.0 69.3 72.0 81.1 75.0 

Head Sho. Elb. Wri. Hip Knee Ank. U.Body PCK 

HG-1s-1u: 
91.7 86.7 80.6 77.4 67.8 73.5 69.0 84.3 78.8 
HG-1s-1u-PIL(HG-1s-1u) 92.6 89.0 84.2 81.3 70.8 78.4 75.5 86.9 82.2 
HG-2s-1u: 
91.8 88.3 82.4 79.1 70.6 75.9 73.7 85.6 80.8 
HG-2s-1u-PIL(HG-1s-2u) 92.7 89.8 84.9 81.6 72.8 79.2 76.7 87.4 83.0 
HG-4s-2u 
93.2 90.7 86.7 83.2 72.6 81.7 80.8 88.6 84.5 
HG-4s-2u-PIL(HG-1s-2u) 93.3 91.1 87.5 84.7 73.5 82.6 81.9 89.3 85.3 
HG-8s-1u 
93.4 91.2 87.3 84.4 73.3 81.8 80.8 89.2 84.9 
HG-8s-1u-PIL(HG-1s-2u) 93.3 91.3 88.0 85.1 73.5 83.5 82.1 89.5 85.6 

HG-4s-2u 
-
93.2 90.7 86.7 83.2 72.6 81.7 80.8 88.6 84.5 
+PIL(HG-1s-1u) 41.</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Comparison with state-of-the-arts on LIP testing set. Head Sho. Elb. Wri. Hip Knee Ank. PCK</figDesc><table>Hybrid Pose Machine 
71.7 87.1 82.3 78.2 69.2 77.0 73.5 77.2 
BUPTMM-POSE 
90.4 87.3 81.9 78.8 68.5 75.3 75.8 80.2 
Pyramid Stream Network 91.1 88.4 82.2 79.4 70.1 80.8 81.2 82.1 
Chou et al. [10] 
94.9 93.1 89.1 86.5 75.7 85.5 85.7 87.4 

Our model 
94.9 93.1 89.9 87.6 75.9 84.9 84.4 87.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 5 .</head><label>5</label><figDesc>Experiments on the extended PASCAL-Person Part dataset. Head Sho. Elb. Wri. Hip Knee Ank. mAP</figDesc><table>Chen and Yuille [8] 
45.3 34.6 24.8 21.7 9.8 8.6 7.7 21.8 
Insafutdinov et al. [19] 41.5 39.3 34.0 27.5 16.3 21.3 20.6 28.6 
Xia et at. [38] 
58.0 52.1 43.1 37.2 22.1 30.8 31.1 39.2 

Our baseline (w/o PIL) 66.2 54.8 43.8 40.2 23.7 24.9 23.5 39.6 
Our model 
67.8 56.6 45.7 41.9 24.2 26.4 24.2 41.0 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Ablation experiments on MPII validation set. The model in parenthesis denotes the parsing network used in PIL. Head Sho. Elb. Wri. Hip Knee Ank. U.Body PCKhHG-8s-1u-PIL(HG-1s-2u) 97.8 96.5 91.4 87.3 90.7 87.3 83.7 93.3 91.0</figDesc><table>HG-8s-1u 
97.7 96.2 90.6 86.3 89.8 85.9 82.1 92.7 
90.2 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 7 .</head><label>7</label><figDesc>Comparison with state-of-the-arts on MPII testing set. Head Sho. Elb. Wri. Hip Knee Ank. PCKh AUC</figDesc><table>Pishchulin et al. [28] 74.3 49.0 40.8 34.1 36.5 34.4 35.2 44.1 24.5 
Tompson et al. [35] 
95.8 90.3 80.5 74.3 77.6 69.7 62.8 79.6 51.8 
Carreira et al. [5] 
95.7 91.7 81.7 72.4 82.8 73.2 66.4 81.3 49.1 
Tompson et al. [34] 
96.1 91.9 83.9 77.8 80.9 72.3 64.8 82.0 54.9 
Hu&amp;Ramanan [18] 
95.0 91.6 83.0 76.6 81.9 74.5 69.5 82.4 51.1 
Pishchulin et al. [29] 94.1 90.2 83.4 77.3 82.6 75.7 68.6 82.4 56.5 
Lifshitz et al. [22] 
97.8 93.3 85.7 80.4 85.3 76.6 70.2 85.0 56.8 
Gkioxary et al. [15] 
96.2 93.1 86.7 82.1 85.2 81.4 74.1 86.1 57.3 
Rafi et al. [30] 
97.2 93.9 86.4 81.3 86.8 80.6 73.4 86.3 57.3 
Insafutdinov et al. [19] 96.8 95.2 89.3 84.4 88.4 83.4 78.0 88.5 60.8 
Wei et al. [36] 
97.8 95.0 88.7 84.0 88.4 82.8 79.4 88.5 61.4 
Newell et al. [25] 
98.2 96.3 91.2 87.1 90.1 87.4 83.6 90.9 62.9 
Chu et al. [11] 
98.5 96.3 91.9 88.1 90.6 88.0 85.0 91.5 63.8 
Chou et al. [10] 
98.2 96.8 92.2 88.0 91.3 89.1 84.9 91.8 63.9 
Chen et al. [9] 
98.1 96.5 92.5 88.5 90.2 89.6 86.0 91.9 61.6 
Yang et al. [40] 
98.5 96.7 92.5 88.7 91.1 88.6 86.0 92.0 64.2 

Our model 
98.6 96.9 93.0 89.1 91.7 89.0 86.2 92.4 65.9 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Jiashi Feng was partially supported by NUS startup R-263-000-C08-133, MOE Tier-I R-263-000-C21-112, NUS IDS R-263-000-C67-646 and ECRA R-263-000-C87-133.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">2d human pose estimation: New benchmark and state of the art analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning feed-forward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Integrated perception with recurrent multi-task neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2d pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentatin with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Parsing occluded people by flexible compositions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Adversarial posenet: A structure-aware convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-S</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Self adversarial training for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Chou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-T</forename><surname>Chien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Multi-context attention for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Towards unified human parsing and pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">RMPE: Regional multiperson pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Chained predictions using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Look into person: Self-supervised structure-sensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down reasoning with hierarchical rectified gaussians</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deepercut: A deeper, stronger, and faster multiperson pose estimation model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human pose estimation using a joint pixel-wise and part-wise formulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Human pose estimation using deep consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Lifshitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fetaya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ullman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Matching-cnn meets knn: Quasiparametric human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Generative partition networks for multi-person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07422</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Paszke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Strong appearance and expressive spatial models for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deepcut: Joint subset partition and labeling for multi person pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Pishchulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Insafutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">An efficient convolutional network for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Rafi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kostrikov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. ICLR</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Efficient object localization using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Joint training of a convolutional network and a graphical model for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Tompson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human part segmentation with auto zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Joint multi-person pose estimation and semantic part segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Parsing clothing in fashion photographs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yamaguchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Kiapour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">E</forename><surname>Ortiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning feature pyramids for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Facial landmark detection by deep multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
