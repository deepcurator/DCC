<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:11+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">MIT-IBM Watson AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gerard</forename><surname>De Melo</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Rutgers University</orgName>
								<address>
									<addrLine>4 MIT</addrLine>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiajun</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Liu</surname></persName>
							<email>liuxiao12@baidu.com</email>
							<affiliation key="aff3">
								<address>
									<settlement>Baidu</settlement>
									<region>IDL</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shilei</forename><surname>Wen</surname></persName>
							<email>wenshilei@baidu.com</email>
							<affiliation key="aff3">
								<address>
									<settlement>Baidu</settlement>
									<region>IDL</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Attention Clusters: Purely Attention Based Local Feature Integration for Video Classification</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Video classification remains one of the prime challenges in computer vision as well as machine learning. It has received a substantial amount of attention in recent years, owing not least to its numerous potential use cases, such as video tagging, surveillance, autonomous driving, and stock footage search. Thanks to recent large datasets, e.g. YouTube-8M <ref type="bibr" target="#b0">[1]</ref> and Kinetics <ref type="bibr" target="#b3">[4]</ref>, the recognition accuracy in video classification has advanced considerably, although the current state-of-the-art remains subpar in comparison with human performance. We observe several important characteristics of local features of a video: high degree of similarity, local identifiability, approximate unorderedness, and multi-component inputs.</p><p>Most of the existing effective video classification methods are based on convolutional neural networks (CNNs). CNNs have shown their powerful representation learning abilities in various image classification tasks. Convolutional and pooling layers together essentially act as potent feature extractors, which are able to mine local features from different regions of an image. Unsurprisingly, CNNs can also be used as a feature extractor for local feature extraction from videos, extracting a sequence of features for relevant video frames in accordance with their temporal order.</p><p>Many existing methods use convolutional neural networks (CNNs) or recurrent neural networks (RNNs) based on such local feature sequences to capture the temporal interactions within a video. The latter are particularly often considered for their ability to capture longer-term temporal patterns by retaining pertinent state information across time. In this paper, however, we cast some doubt on whether temporal patterns, especially long-term ones, are truly indispensable for common trimmed video classification tasks. This is motivated by the following observations regarding the characteristics of such local features for video classification.</p><p>First, local features within a video tend to have a high degree of similarity across frames. In short video clips, the changes between RGB frames tend to be small. Especially during phases of slow movement, adjacent RGB frames exhibit substantial redundancy, sometimes differing in just minuscule ways. For instance, in the short example clip portraying golf chipping in <ref type="figure" target="#fig_0">Figure 1</ref> (top), the frames are almost identical except for gradual changes in the position of the club. For classification, it may suffice to view these similar features holistically, packaged as a whole, while disregarding the particular details of their evolution over time.</p><p>Second, the local features of a video often possess the property of local identifiability. When people watch videos, they are frequently able to classify them based on just a few, occasionally even just a single frame. For example, in the video for brushing teeth in <ref type="figure" target="#fig_0">Figure 1</ref> (middle), we can infer the class having observed just the initial frame. That is to say, even a tiny fraction of local features may singlehandedly provide exhaustive classification information. For classification, the key is to spot the relevant local features of these most informative frames, without needing to ponder over their temporal patterns.</p><p>Third, the local features of a video may be regarded as approximately unordered in many classification settings. Of course, the input video itself is ordered, and a thorough semantic interpretation of the portrayed narrative requires some level of understanding of the temporal progression. For classification problems, however, we conjecture that the order may not be crucial. Even if the local features are permuted, a correct classification of the video may remain achievable. For example, in the pole vault video in <ref type="figure" target="#fig_0">Figure 1</ref> (bottom), the frames have been reordered, showing first the landing, then the jump, and finally the run up. Yet, humans can still easily categorize it. Hence, the order of local features may not need to be preserved in video classification.</p><p>Accounting for the above considerations, we investigate an approach that completely abandons temporal cues. Instead, we explore the potential of purely attention based local feature integration methods to generate a global representation. This is because attention mechanisms naturally possess the following properties. First of all, attention outputs are essentially weighted averages, which implies that repeated local features will automatically be aggregated to reduce their redundancy. Secondly, an attention model may assign higher weights to significant local features so as to focus on a small number of key signals, and their local identifiability determines to what extent the classification results on a small number of key frames can be taken as a class label for the entire video. Finally, the inputs to an attention model are naturally unordered sets of varying sizes, which fits the properties of the local features, and also facilitates a generalization ability to varying numbers of local features.</p><p>We also observe a further important property, the multicomponent nature of local features of a video. Multiple cues in the signal may simultaneously make important contributions towards enabling the classification of a given video. For instance, in the pole vault video <ref type="figure" target="#fig_0">(Figure 1 bottom)</ref>, the landing, jump, and run up all may yield useful information. Combining the signals across these different aspects ought to be better than focusing on just one of them.</p><p>A single attention unit can be viewed as focusing on just one aspect of the video, hence discarding a considerable amount of information. It turns out that it is near-impossible to achieve our aims with a single attention unit. Thus, we propose using multiple attention units to construct an attention cluster that constitutes a global representation of the video. Furthermore, we find that attention clusters resulting from a simple concatenation of the outputs of attention units only lead to weak gains, making them an inefficient choice. Instead, we propose a very simple and efficient procedure, the shifting operation, which effectively increases the diversity between attention units, speeding up the training efficiency and improving the classification accuracy.</p><p>In the following, we first review pertinent related work in Section 2. Then, in Section 3, we present our proposed attention clusters approach with the shifting operation, as well as our overall architecture for video classification. In <ref type="bibr">Section 4</ref>, in order to analyze the effect of various attention cluster approaches and visualize the inner workings of the attention mechanism, we propose Flash-MNIST as a new toy dataset, and conduct various comparative experiments on it. Finally, we show the results of using attention clusters on challenging real-world video classification datasets in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Attention Mechanisms</head><p>Attention networks were originally proposed on the basis of the REINFORCE algorithm. In particular, Mnih et al. <ref type="bibr" target="#b19">[20]</ref> and Ba et al. <ref type="bibr" target="#b1">[2]</ref> proposed attention for object recognition with recurrent neural networks. These attention networks select regions by making hard binary choices, which may face difficulties in training.</p><p>Soft attention mechanisms were proposed by using weighted averages instead of hard selections. Bahdanau et al. <ref type="bibr" target="#b2">[3]</ref> apply soft attention to machine translation with the aim of capturing soft alignments between source and target words. Sharma et al. <ref type="bibr" target="#b23">[24]</ref> proposed a Soft-Attention LSTM model built on top of multi-layered RNNs to selectively focus on parts of the video frames and classify videos after taking a few glimpses. Wang et al. proposed an attention-based method for weakly supervised action recognition <ref type="bibr" target="#b36">[37]</ref>, but it mainly targets untrimmed videos and is not ideal when directly applied to trimmed videos. Li et al. proposed an end-to-end sequence learning model called VideoLSTM <ref type="bibr" target="#b16">[17]</ref>, which hardwires convolutions in the Soft-Attention LSTM. These soft attention models require the introduction of supplementary sources of information to guide the weighted averages, which incur a substantial computational cost while failing to yield sufficient improvements in classification tasks.</p><p>To address the problem of attention on single sequences, many self-attentive models have been proposed for a variety of tasks, such as reading comprehension <ref type="bibr" target="#b4">[5]</ref> and abstractive summarization <ref type="bibr" target="#b21">[22]</ref>. Lin et al. <ref type="bibr" target="#b17">[18]</ref> applied multiple attention units to learn task-independent sentence representations, relying on a penalization term to force each attention to attend to different parts. However, penalty functions forcing each weight vector to be different are too restrictive for video classification. Due to highly similar features between frames, many videos lack sufficient diversity, so this method fails to obtain good results. Our proposed attention clusters are another form of self-attentive architecture, which introduces a shifting operation to learn diversified attention units.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video Classification</head><p>Since CNNs enjoy great success in image classification <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b10">11]</ref>, they have also been applied to video classification tasks. Karpathy et al. <ref type="bibr" target="#b13">[14]</ref> studied multiple fusion methods based on pooling local spatio-temporal features extracted by 2D CNNs from RGB frames. This can be viewed as a preliminary exploration of the idea of integrating local feature sets, although simple pooling methods do not yield significant gains.</p><p>Many architectures have been proposed for modeling spatio-temporal information. The optical flow method <ref type="bibr" target="#b38">[39]</ref> captures temporally local information by considering the variation in the surrounding frames. Simonyan et al. <ref type="bibr" target="#b24">[25]</ref> devised a method that uses both RGB and stacked optical flow frames as appearance and motion signals, respectively. The accuracy is significantly boosted even by simply fusing probability scores, which indicates that optical flow can contribute useful short-term motion information. Feichtenhofer et al. <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref> compared a number of ways of fusing CNNs both spatially and temporally and combined them with ResNets <ref type="bibr" target="#b10">[11]</ref> to extract better spatio-temporal information. C3D <ref type="bibr" target="#b32">[33]</ref> extends 2D CNNs by using 3D convolution kernels to capture spatio-temporal information. Varol et al. <ref type="bibr" target="#b33">[34]</ref> found that better results could be achieved by expanding the temporal length of inputs and using optical flows instead of RGB inputs for 3D CNNs. Carreira et al. <ref type="bibr" target="#b3">[4]</ref> incorporated the Inception architecture <ref type="bibr" target="#b30">[31]</ref> into 3D CNNs.</p><p>To model long-term temporal interactions in video classification, recurrent neural networks (RNN), particularly long short-term memory (LSTM) <ref type="bibr" target="#b12">[13]</ref> have been applied in numerous papers. Ng et al. <ref type="bibr" target="#b20">[21]</ref> devised two-stream LSTMs. Donahue et al. <ref type="bibr" target="#b5">[6]</ref> proposed an end-to-end architecture based on LSTMs. Srivastava et al. <ref type="bibr" target="#b27">[28]</ref> attempted to improve the representation ability of LSTMs by first pretraining them in an unsupervised manner to reconstruct the input. However, the accuracy on video classification with these RNN-based methods has been unsatisfactory, which may indicate that long-term temporal interactions are not crucial for existing video classification datasets. Our proposed method explores the potential of local feature integration without any recourse to long-term order information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>We now describe our approach of using attention clusters with a shifting operation, and show how to apply it to the task of video classification. We broadly consider three major parts: local feature extraction, local feature integration, and global feature classification. Each of these is addressed by suitable neural networks. Among them, the local feature extraction uses existing CNNs, and the global feature classification invokes fully connected and softmax layers. The main contribution lies in the local feature integration step, that is, our investigation of how to generate global representations given a set of local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local Feature Set</head><p>In neural network settings, we often obtain local features of a video, since CNNs can naturally be used as a feature extractor.</p><p>The local feature set is defined as a set of unordered local features corresponding to different parts of the same video. Here, for convenience, we use a M × L matrix X to represent a set containing L local features, each column of which is a separate local feature vector x i :</p><formula xml:id="formula_0">X = (x 1 , x 2 , ..., x L ).<label>(1)</label></formula><p>Note that, in fact, the set of local features is unordered, and hence permuting the columns of the matrix should not affect the results. Also, the number of local features L can vary across different objects. The challenge we seek to address at this point is how to generate fixed-length global vectors g to classify objects based on pertinent information from the local feature sets as given by X.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Attention</head><p>We rely on an attention mechanism to obtain such global features. In classification settings, the attention is static, and the input contains only the local feature vector set itself. Its responsibility is to first analyze the importance of each local feature and then to bestow the global feature with as much useful information as possible, while ignoring irrelevant signals and noise. Such attention outputs can essentially be regarded as weighted averages on a vector set:</p><formula xml:id="formula_1">v = Xa,<label>(2)</label></formula><p>where a is a weight vector of dimension L, which is determined by a weighting function.</p><p>The choice of weighting function is the most crucial design decision to be made. Its input is the local feature set X, while its output is the weight vector a, whose ℓ 1 norm is 1. Each dimension of the weight vector corresponds to a local feature.</p><p>There are many methods to compute the weights of local features. For instance, global averages can be considered as a degenerate form of attention, and the corresponding weighting function can be expressed as:</p><formula xml:id="formula_2">a = 1 L 1,<label>(3)</label></formula><p>where 1 is a vector of dimensionality L with all elements equal to 1. For a more malleable attention weighting function, we can use a single fully-connected layer that has only one cell (FC1), such as:</p><formula xml:id="formula_3">a = softmax(w ⊺ X + b),<label>(4)</label></formula><p>where w and b are parameter vectors of dimensionality M and L, respectively. Similarly, we may use two successive fully-connected layers of size H and one hidden cell (FC2):</p><formula xml:id="formula_4">a = softmax (w ⊺ 2 tanh(W ⊺ 1 X + b 1 ) + b 2 ) ,<label>(5)</label></formula><p>where W 1 is a parameter matrix of dimensionality M × H, b 1 , w 2 are parameter vectors of dimensionality H, and b 2 are parameter vectors of size L.</p><p>In the experiments in Section 4, we compare the effects of these different weighting functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Attention Clusters</head><p>The output of one such attention unit typically focuses on a specific part of the video, e.g. a particular set of related frames or similar sounds. Normally, a single attention unit can only be expected to reflect one aspect of the video. However, there can be multiple pertinent parts in a video that together describe the overall event portrayed in the entire video. Therefore, to be able to represent multiple components, we need multiple attention units that focus on different parts of local features. We refer to a group of attention units that operate on the same input but have independent parameters as an attention cluster. The size N of an attention cluster is defined by the number of independent attention units in it. The global feature g resulting from an attention cluster is a vector of dimensionality N M which  can be obtained by concatenating the outputs of all involved attention units:</p><formula xml:id="formula_5">g = [v 1 , v 2 , ..., v N ],<label>(6)</label></formula><p>where v k is output of k-th attention unit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Shifting Operation</head><p>Although we expect attention clusters to be able to focus on different components, through experiments, which we describe in Section 4.4, we found that simply concatenating the outputs of attention units yields unsatisfactory results, since they tend to focus on similar signals. In order to address this problem, we propose a shifting operation, which is added onto each attention unit. This is achieved by adapting Eq. 2 as follows:</p><formula xml:id="formula_6">v = α · Xa + β √ N α · Xa + β 2 ,<label>(7)</label></formula><p>where α and β are learnable scalars, which act as a linear transformation in the feature space. After the linear transformation, we ℓ 2 -normalize each attention unit separately. The factor 1/ √ N finally acts as a global ℓ 2 -normalization on the cluster. Combining the linear transformation and normalization, the shifting operation shifts the weighted sum in the feature space and at the same time ensures scaleinvariance. The shift operation efficiently enables different attention units to flexibly diverge from each other and have different distributions, and the scale-invariance facilitates the optimization of the entire network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Overall Architecture for Video Classification</head><p>In order to collect multimodal information from videos, we extract a variety of different local feature sets, such as appearance (RGB), motion (flow), and audio signals. However, it is unrealistic to process all feature sets simultaneously within the same attention cluster, because features of different modalities have different distributions, dimensionalities, and scales. Instead, we propose multimodal attention clusters with the shifting operation to train attention clusters for different modalities simultaneously. The layout of the proposed overall architecture is illustrated in <ref type="figure" target="#fig_2">Figure  2</ref>.</p><p>First, we extract multiple feature sets from the video. For each feature set, we apply independent attention clusters with shifting operation to obtain a modality-specific representation vector. Next, the output of all attention clusters are concatenated to form a global representation vector of the video. Finally, the global representation vector is used for classification through a fully-connected layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Analysis and Visualization</head><p>Because real-world video classification datasets conflate many different forms of variation, they are not easy to analyze and visualize directly. We first propose a new toy video classification dataset, Flash-MNIST, which we synthetically generate from the MNIST handwritten digit dataset. The Flash-MNIST dataset has fewer irrelevant factors of influence and requires only modest amount of computation. Nevertheless, its local feature set shares many properties with real video classification, which is convenient for analysis and visualization. This allows us to observe the behavior of the model under simplified conditions and achieve a deeper understanding of how the model works. We make our code for generating the dataset and training on it publicly available <ref type="bibr" target="#b0">1</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Flash-MNIST Dataset</head><p>In the well-known MNIST dataset, the goal is classify 28 × 28 pixel images of handwritten digits into 10 classes for the respective digits. Flash-MNIST extends this image classification task to video. The videos in Flash-MNIST consist of 25 frames with noisy backgrounds, on which various MNIST digits briefly flash up. The goal is to identify the specific set of digits that appear in the video, which entails choosing from a total of 2 10 = 1024 categories. <ref type="figure" target="#fig_3">Figure  3</ref>   <ref type="table">Table 1</ref>. Accuracy (%) on Flash-MNIST to show the effect of different weighting functions , various cluster sizes N , and aggregation with or without the shifting operation.</p><p>possible set of digits, and then randomly select corresponding digit images from the MNIST training set and overlay them on the random frames. For the test set samples, a similar process is used, except that images are selected from the MNIST test data. We randomly generate 102,400 samples for training, and 10,240 samples for testing. We pretrain CNNs on MNIST with noisy backgrounds to extract local features. The CNNs consist of 2 successive convolutional layers with 5 × 5 kernels, 10/20 filters followed by relu activations and max-pooling with stride 2, and one fully connected layer with 50 hidden units. Through the CNNs, we can obtain 25 local features with a dimensionality of 50, each local feature corresponding to a frame in the video. These 25 local features consist of the local feature set, and we apply a variety of attention cluster alternatives, as described in Section 3, to induce a global representation. Finally, this is passed through one fully connected layer for classification. The accuracy scores for different settings for the attention clusters are given in <ref type="table">Table  1</ref>. We describe and analyze the results in the following. For more details of the dataset generation and network training, please refer to the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Effect of Weighting Function</head><p>First of all, we analyze the effect of the choice of weighting function. We consider Average as described in Eq. 3, and two different attention weighting functions, FC1 as described in Eq. 4, and FC2 with 10 hidden units as described in Eq. 5. As shown in <ref type="table">Table 1</ref>, we observe a significant gap between the results of using Average and the other attention weighting functions, which means that attention can play an effective role in this situation to focus on the parts that merit consideration. We also observe that FC2 fares slightly better than FC1 when the cluster size is small, but FC2 performs worse than FC1 when the cluster size is large. This, we speculate, may stem from the expressive power of attention clusters saturating as the size increases, even if the form of attention itself is simple enough. Considering that FC2 contains more parameters and requires more computation, but is unable to yield any benefits, we rely on the FC1 weighting function as the default in all subsequent experiments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effect of Attention Cluster Size</head><p>Next, we consider the effect of different cluster sizes N . Because a video may include a variety of digits, these should not be attended to by the same unit. Consider an ideal situation, in which 10 attention units each pay attention to whether a specific digit occurs. This is obviously more reasonable than using a single attention function. Of course, during training, we lack control over which attention unit learns information about which digit. Still, we may hope that using multiple attention units may have the potential to learn more beneficial information.</p><p>In order to verify our idea, for fairness of comparison, we ensure that the numbers of network parameters are completely identical, except for parameters contained in the weighting functions, by also replicating the output vectors for the Average method N times. As shown in <ref type="table">Table 1</ref> and <ref type="figure" target="#fig_4">Figure 4</ref>, we find that with an increase in the cluster size N , the classification results increase significantly when the size is small and subsequently almost remain unchanged until reaching a certain level. Furthermore, the gap between using attention and Average becomes larger as N increases, indicating that this improvement is not due to an increase in the number of network parameters, but that the model genuinely pays attention to different aspects of local features.</p><p>Besides, the convergence speed also increases for increasing cluster sizes. Although a larger cluster size requires more computation, a smaller overall training time is required for reasonably large cluster sizes, since the computation of the attention and shifting operations is very efficient.</p><p>We visualize the attention weight maps for attention clusters with 8 units in <ref type="figure" target="#fig_5">Figure 5</ref>. We observe that each attention unit learns about different kinds of information. For instance, with shifting operation (middle, bottom), the first attention unit learns to attend to the digit 4, and the fifth attention unit learns to attend to digits 6 and 7.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Effect of Shifting Operation</head><p>Finally, we consider the effect of the shifting operation. As shown in <ref type="table">Table 1</ref>, we find that applying attention clusters with shifting yields substantial improvements, as the accuracy increases from 83.3% to 87.1%. Inspecting the attention weight maps in <ref type="figure" target="#fig_5">Figure 5</ref>, we find that the attention weights diverge entirely when using shifting, while the third and fourth attention weights match the fifth and sixth when not using the shifting operation. This indicates that the shifting operation can help us learn more diversified information for better generalization, and, ultimately, a higher accuracy. Simultaneously, the shifting operation can also help the model converge more rapidly. As plotted in <ref type="figure" target="#fig_4">Figure  4</ref>, at the same cluster size, the approach with shifting converges much more rapidly than when forgoing the shifting operation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiment on Real Video Classification</head><p>In this section, we proceed to evaluate and compare our proposed methods on real-world video classification tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets</head><p>Specifically, we evaluate our methods on three popular trimmed video classification datasets. UCF101 [27] contains 13,320 web video clips with heterogeneous forms of camera motion and illumination. Each clip contains about 180 frames and is labeled with one of 101 action classes, ranging from daily life activities to unusual sports. HMDB51 <ref type="bibr" target="#b15">[16]</ref>   <ref type="table">Table 2</ref>. Top-1 accuracy (%) on Kinetics to show the effect of different cluster sizes and training with (w/) or without (w/o) shifting operation for RGB, flow, and audio.</p><p>categories. For UCF101 and HMDB51, we report the average accuracy over three training/testing splits, following the original evaluation scheme. Kinetics <ref type="bibr" target="#b3">[4]</ref> is a large-scale trimmed video dataset with more than 300K video clips in total, in which 246,535 serve as training data, and 19,907 for validation. Each video clip is taken from a different YouTube video and lasts around 10s. The clips are labeled using a set of 400 human action classes. We report experimental results on the validation split, due to the unavailability of the test split at the time of conducting these experiments. Since Kinetics is largerscale and has more categories, it is convenient for stable and reliable experimental analysis. We mainly perform comparative experiments on Kinetics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Local Feature Extraction</head><p>Considering that video is inherently multimodal, we extract three kinds of local features -RGB, flow, and audio -to represent the video. We rely on CNNs to extract these features.</p><p>RGB and flow features are extracted from RGB video frames or optical flow images, which are created by stacking the two channels for the horizontal and vertical vector fields <ref type="bibr" target="#b24">[25]</ref>. For UCF101 and HMDB51, we initialize ResNet-152 <ref type="bibr" target="#b10">[11]</ref> with a pre-trained ImageNet model and fine-tune it using the frames from training videos and then apply it to extract RGB and flow features. For Kinetics, we rely on Inception-ResNet-v2 <ref type="bibr" target="#b29">[30]</ref> to extract these features. The RGB model is initialized with a pre-trained ImageNet model and fine-tuned using the training split based on the temporal segment network framework <ref type="bibr" target="#b37">[38]</ref> with 7 segments. Then the flow model is initialized by the RGB model and also fine-tuned the same way. After training, we can extract local RGB and flow features for every frame.</p><p>To extract audio features, we generate audio spectrogram patches first. For every 10ms, we decompose the signal with a short-time Fourier transform and then rely on aggregated, logarithm-transformed 64 mel-spaced frequency bins following <ref type="bibr" target="#b11">[12]</ref>. Each 96 consecutive bins yield one log-mel 96 × 64 spectrogram patch, which can be processed just like an image. After this, we can extract audio features using VGG-16 <ref type="bibr" target="#b25">[26]</ref>   <ref type="table">Table 3</ref>. Top-1 and top-5 accuracy (%) of multimodal integration of different cluster sizes for different modality on Kinetics.</p><p>features. For videos without audio, we feed in the average of audio features over the training set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Local Feature Set Augmentation</head><p>Data augmentation plays a very important role in training neural networks, making use of properties of the data to effectively reduce overfitting. Here, we can similarly exploit the properties of the local feature sets to design new data enhancement methods. Local feature sets are approximately unordered, and most of the time, we do not need to understand the video using all of the local features, since we often only need a few key frames for classification. Hence, when we train the model, we can randomly sample a part of the features from the local feature set, but use all the features during testing. This data augmentation method can reduce the amount of computation during training, effectively prevent overfitting, and allow us to make use of all information during testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Implementation Details</head><p>In order to reduce overfitting, we apply dropout with probability 0.9 before the final fully connected layer. For local feature set augmentation, we sample 15/15/20 local features during training on UCF101/HMDB51/Kinetics, respectively, and we extract 20/20/25 local features, respectively, at equal intervals during testing. To balance the dataset, we set the sample weight to 1/S if a given sample belongs to a class that contains S samples during training. We rely on the RMSPROP algorithm <ref type="bibr" target="#b31">[32]</ref> to update parameters with a learning rate of 0.001 and clip the gradient ℓ 2 -norm of all parameters to 5 for better convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Result of Single Modality</head><p>We explore how many attention units we need to use for a single modality and whether we should use the shifting operation or not to achieve the best results on Kinetics. Based on the previous experiment, we use the weighting function FC1 as default. <ref type="table">Table 2</ref> describes the relationship between the Top-1 accuracy of RGB, flow, and audio as single modalities, for different attention cluster sizes N , and with or without the shifting operation. We also show the Average results as the baseline.</p><p>We find that often with increasing N , the accuracy increases first and then decreases without shifting, or remains Method Top-1(%) Top-5(%) C3D <ref type="bibr" target="#b32">[33]</ref> 55.6 79.1 3D ResNet <ref type="bibr" target="#b9">[10]</ref> 58  <ref type="table">Table 4</ref>. Kinetics top-1 and top-5 accuracy (%) on the validation set, except for results marked with ' * ', which were reported based on the test set.</p><p>unchanged with the shifting operation. When N is small, due to limitations of the expressive power of attention models, the accuracy increases with increasing N both with and without the shifting operation. When N is sufficiently large, the expressive power is adequate. Without the shifting operation, for increases in N , the number of parameters also increases, and harmful overfitting becomes a serious issue, while the training is also more difficult. This leads to a decrease in accuracy. With the shifting operation, the training remains stable and reliable even for large cluster sizes. We observe that the accuracy while using the shifting operation is universally better than without it. This suggests that the shifting operation can increase the diversity of the attention mechanism effectively, to improve the accuracy. We have also observed that attention clusters with shifting operation converged more quickly than without, similar to the observations on Flash-MNIST.</p><p>As shown in <ref type="table">Table 4</ref>, comparing to the pretrained TSN models used for feature extraction, our attention clusters achieve excellent improvements of 2.0% for RGB, 1.5% for flow, and 2.6% for audio, in terms of top-1 accuracy. We also find that our results can beat other fusion methods using the same local features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Result of Multimodal Integration</head><p>We investigate the effects of various combinations of different attention cluster sizes for multimodal integration. The results are shown in <ref type="table">Table 3</ref>. We find that we can use smaller cluster sizes for multimodal integration rather than for a single modality. We can achieve the best top-1 accuracy (79.4%) and the best top-5 accuracy (94.0%) with 64 attention units for RGB, and 32 for flow and audio. We also implement a series of three stream fusion methods using the Method UCF101(%) HMDB51 (%) iDT + FV <ref type="bibr" target="#b34">[35]</ref> 85.9 57.2 iDT + HSV <ref type="bibr" target="#b22">[23]</ref> 87.9 61.1 EMV-CNN <ref type="bibr" target="#b39">[40]</ref> 86.4 -VideoLSTM <ref type="bibr" target="#b16">[17]</ref> 89.2 -FSTCN <ref type="bibr" target="#b28">[29]</ref> 88.1 59.1 TDD+FV <ref type="bibr" target="#b35">[36]</ref> 90.3 63.2 TSN (2 modalities) <ref type="bibr" target="#b37">[38]</ref> 94.0 68.5 Two Stream <ref type="bibr" target="#b24">[25]</ref> 88.0 59.4 Temporal-Inception <ref type="bibr" target="#b18">[19]</ref> 93.9 67.5 TS-LSTM <ref type="bibr" target="#b18">[19]</ref> 94.1 69.0 Fusion <ref type="bibr" target="#b7">[8]</ref> 92.5 65.4 ST-ResNet <ref type="bibr" target="#b6">[7]</ref> 93.4 66.4 ActionVLAD <ref type="bibr" target="#b8">[9]</ref> 92.7 66.9 Attention Cluster RGB+Flow 94.6 69.2 <ref type="table">Table 5</ref>. Mean classification accuracy (%) comparing with Stateof-the-Art methods on UCF101 and HMDB51.</p><p>same local features (see supplementary material for details).</p><p>As shown in <ref type="table">Table 4</ref>, our approach improved over them by a large margin.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.7.">Comparison with State-of-the-Art</head><p>Finally, we compare our method against the state-of-theart methods.</p><p>On UCF101 and HMDB, our approach obtains robust improvements over the two-stream fusion results for CNNs. As shown in <ref type="table">Table 5</ref>, our approach can achieve competitive results in comparison with existing published methods <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b18">19]</ref>.</p><p>On Kinetics, as shown in <ref type="table">Table 4</ref>, we compare our method against many published results <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b37">38]</ref>. Since the local feature extractors are trained using TSN <ref type="bibr" target="#b37">[38]</ref>, the results with CNNs are already very strong. The implemented three stream fusion methods also act as a strong baseline. Our approach again enjoys great improvements over all of them and obtains the start-of-the-art result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>To explore the potential of pure attention networks for video classification, a new architecture based on attention clusters with a shifting operation is proposed to integrate local feature sets. We analyze and visualize attention on the proposed Flash-MNIST data to get a better understanding of how our attention clusters work. We also have conducted experiments on three well-known video classification datasets and find that this architecture can achieve excellent results for a single modality or integrating multiple modalities, while also accelerating the training phase.</p><p>In terms of future work, we hope to apply this architecture to low-level local features and assess to what extent it can uncover relationships between features in different spatial coordinates. We further hope to integrate it into end-toend-trained networks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Example of RGB frames sampled from videos. The bottom ones are shown in a randomly permuted temporal order. We observe several important characteristics of local features of a video: high degree of similarity, local identifiability, approximate unorderedness, and multi-component inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Multimodal Attention Clusters with Shifting Operation: The overall architecture for video classification. Separate attention clusters are applied for different feature sets and then the outputs are concatenated for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Frames of 5 videos in Flash-MNIST and the corresponding label. The purple bold digits in label are digits flashing in the video.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. The accuracy on Flash-MNIST in each epoch, learned with different cluster sizes, with (w/) or without (w/o) the shifting operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Visualization of attention weight maps of 8-unit attention clusters with shifting operation (middle, bottom), and without (top). We show frames in HSV space, in which larger values indicate a larger weight. The first row for each sample provides the video frames, and the respective following 8 rows correspond to the 8 weights of 8 attention units.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>shows 5 samples and their corresponding labels. Specif- ically, in order to generate training samples, we first ran- domly generate 25 different 28 × 28 noise frames, sample a 1 https://github.com/longxiang92/Flash-MNIST</figDesc><table>N 
Average 
Without Shifting 
With Shifting 
FC1 
FC2 
FC1 
FC2 
1 
0.2 
0.2 
0.2 
51.2 
53.7 
2 
0.4 
0.4 
0.5 
64.8 
66.8 
4 
0.6 
2.2 
2.3 
75.9 
76.8 
8 
0.9 
31.7 
22.5 
80.6 
83.1 
16 
1.2 
82.3 
82.0 
86.9 
84.9 
32 
2.4 
83.3 
83.2 
87.1 
85.6 
64 
5.0 
83.2 
82.4 
87.1 
85.6 
128 
8.9 
81.8 
80.9 
87.1 
85.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>consists of 6,766 video clips from movies and web videos. Each clip is labeled with one of 51 action</figDesc><table>N 

RGB 
Flow 
Audio 
Average w/o 
w/ 
Average w/o 
w/ 
Average w/o 
w/ 
1 
72.5 
73.1 73.4 
63.5 
63.7 65.2 
20.6 
21.3 21.5 
2 
73.0 
73.6 73.9 
64.5 
64.6 66.1 
21.4 
22.3 22.3 
4 
73.2 
73.8 74.2 
65.0 
65.0 66.5 
22.2 
22.8 22.9 
8 
73.2 
74.1 74.4 
65.4 
65.5 66.8 
22.5 
23.3 23.4 
16 
73.1 
74.0 74.6 
65.2 
65.9 67.1 
22.4 
23.3 23.7 
32 
73.0 
73.6 74.7 
64.9 
65.5 67.4 
22.3 
23.1 23.9 
64 
72.5 
73.2 74.9 
64.4 
65.1 67.5 
22.2 
22.7 24.1 
128 
72.6 
73.2 75.0 
65.1 
63.9 67.5 
21.8 
22.5 24.2 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>on Kinetics just as for RGB and flow</figDesc><table>N RGB N Flow N Audio Top-1 (%) Top-5 (%) 
1 
1 
1 
77.9 
93.6 
4 
4 
4 
78.7 
94.0 
16 
16 
16 
79.1 
94.0 
32 
16 
16 
79.2 
94.0 
32 
32 
32 
79.3 
93.9 
64 
32 
32 
79.4 
94.0 
64 
64 
64 
79.3 
94.0 
128 
128 
128 
79.3 
93.9 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Gerard de Melo is funded in part by the DARPA SocialSim program (ARO grant W911NF-17-C-0098).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Abu-El-Haija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kothari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Varadarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08675</idno>
		<title level="m">YouTube-8M: A large-scale video classification benchmark</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.7755</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07750</idno>
		<title level="m">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Long short-term memory-networks for machine reading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lapata</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06733</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">ActionVLAD: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning spatio-temporal features with 3d residual networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kataoka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Satoh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07632</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chaudhuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P W</forename><surname>Ellis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Gemmeke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jansen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Plakal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Saurous</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Seybold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Slaney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wilson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09430</idno>
		<title level="m">CNN architectures for large-scale audio classification</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In CVPR</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">HMDB: A large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">G M</forename><surname>Snoek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.01794</idno>
		<title level="m">Videolstm convolves, attends and flows for action recognition</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A structured self-attentive sentence embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nogueira Dos Santos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.03130</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Kira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alregib</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10667</idno>
		<title level="m">TS-LSTM and temporal-inception: Exploiting spatiotemporal dynamics for activity recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A deep reinforced model for abstractive summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.04304</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bag of visual words and fusion methods for action recognition: Comprehensive study and good practice</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">150</biblScope>
			<biblScope unit="issue">C</biblScope>
			<biblScope unit="page" from="109" to="125" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Action recognition using visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.04119</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NIPS</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Roshan</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Inceptionv4, Inception-ResNet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alemi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural Networks for Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3D convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Long-term temporal convolutions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1" to="1" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Untrimmednets for weakly supervised action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Annunal Symp. German Association Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">4713</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
