<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wen</forename><surname>Liu</surname></persName>
							<email>liuwen@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weixin</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dongze</forename><surname>Lian</surname></persName>
							<email>liandz@shanghaitech.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shenghua</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ShanghaiTech University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Future Frame Prediction for Anomaly Detection -A New Baseline</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Anomaly detection in videos refers to the identification of events that do not conform to expected behavior <ref type="bibr" target="#b2">[3]</ref>. It is an important task because of its applications in video surveillance. However, it is extremely challenging because abnormal events are unbounded in real applications, and it is almost infeasible to gather all kinds of abnormal events and tackle the problem with a classification method. <ref type="figure">Figure 1</ref>. Some predicted frames and their ground truth in normal and abnormal events. Here the region is walking zone. When pedestrians are walking in the area, the frames can be well predicted. While for some abnormal events (a bicycle intrudes/ two men are fighting), the predictions are blurred and with color distortion. Best viewed in color.</p><p>Lots of efforts have been made for anomaly detection <ref type="bibr" target="#b13">[14]</ref>[22] <ref type="bibr" target="#b24">[25]</ref>. Of all these work, the idea of feature reconstruction for normal training data is a commonly used strategy. Further, based on the features used, all existing methods can be roughly categorized into two categories: i) hand-crafted features based methods <ref type="bibr" target="#b5">[6]</ref> <ref type="bibr" target="#b21">[22]</ref>. They represent each video with some hand-crafted features including appearance and motion ones. Then a dictionary is learnt to reconstruct normal events with small reconstruction errors. It is expected that the features corresponding to abnormal events would have larger reconstruction errors. But since the dictionary is not trained with abnormal events and it is usually overcomplete, we cannot guarantee the expectation. ii) deep learning based methods <ref type="bibr" target="#b4">[5]</ref>[14] <ref type="bibr" target="#b27">[28]</ref>. They usually learn a deep neural network with an Auto-Encoder way and they enforce it to reconstruct normal events with small reconstruction errors. But the capacity of deep neural network is high, and larger reconstruction errors for abnormal events do not necessarily happen. Thus, we can see that almost all training data reconstruction based methods cannot guarantee the finding of abnormal events.</p><p>It is interesting that even though anomaly is defined as  <ref type="figure">Figure 2</ref>. The pipeline of our video frame prediction network. Here we adopt U-Net as generator to predict next frame. To generate high quality image, we adopt the constraints in terms of appearance (intensity loss and gradient loss) and motion (optical flow loss).</p><p>Here Flownet is a pretrained network used to calculate optical flow. We also leverage the adversarial training to discriminate whether the prediction is real or fake.</p><p>those events do not conform the expectation, most existing work in computer vision solve the problem within a framework of reconstructing the current frame or its feature on training data <ref type="bibr" target="#b13">[14]</ref>[22] <ref type="bibr" target="#b41">[42]</ref> in an Auto-Encoder way. We presume it is probable that the video frame prediction is far from satisfactory at that time. Recently, as the emergence of Generative Adversarial Network (GAN) <ref type="bibr" target="#b12">[13]</ref>, the performance of video prediction has been greatly advanced <ref type="bibr" target="#b26">[27]</ref>. In this paper, rather than reconstructing training data for anomaly detection, we propose to identify abnormal events by comparing them with their expectation, and introduce a future video frame prediction based anomaly detection method. Specifically, given a video clip, we predict the future frame based on its historical observation. We first train a predictor that can well predict the future frame for normal training data. In the testing phase, if a frame agrees with its prediction, it potentially corresponds to a normal event. Otherwise, it potentially corresponds to an abnormal event. Thus a good predictor is a key to our task. We implement our predictor with an U-Net <ref type="bibr" target="#b29">[30]</ref> network architecture given its good performance at image-to-image translation <ref type="bibr" target="#b16">[17]</ref>. First, we impose a constraint on the appearance by enforcing the intensity and gradient maps of the predicted frame to be close to its ground truth; Then, motion is another important feature for video characterization <ref type="bibr" target="#b31">[32]</ref>, and a good prediction should be consistent with real object motion. Thus we propose to introduce a motion constraint by enforcing the optical flow between predicted frames to be close to their ground truth. Further, we also add a Generative Adversarial Network (GAN) <ref type="bibr" target="#b12">[13]</ref> module into our framework in light of its success for image generation <ref type="bibr" target="#b8">[9]</ref> and video generation <ref type="bibr" target="#b26">[27]</ref>. We summarize our contributions as follows: i) We propose a future frame prediction based framework for anomaly detection. Our solution agrees with the concept of anomaly detection that normal events are predictable while abnormal ones are unpredictable. Thus our solution is more suitable for anomaly detection. To the best of our knowledge, it is the first work that leverages video prediction for anomaly detection; ii) For the video frame prediction framework, other than enforcing predicted frames to be close to their ground truth in spatial space, we also enforce the optical flow between predicted frames to be close to their optical flow ground truth. Such a temporal constraint is shown to be crucial for video frame prediction, and it is also the first work that leverages a motion constraint for anomaly detection; iii) Experiments on toy dataset validate the robustness to the uncertainty for normal events, which validates the robustness of our method. Further, extensive experiments on real datasets show that our method achieve the best performance on the most of datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Hand-crafted Features Based Methods</head><p>Hand-crafted features based anomaly detection is mainly comprised of three modules: i) extracting features; In this module, the features are either hand-crafted or learnt on training set; ii) learning a model to characterize the distribution of normal scenarios or encode regular patterns; iii) identifying the isolated clusters or outliers as anomalies. For feature extraction module, early work usually utilizes low-level trajectory features, a sequence of image coordinates, to represent the regular patterns <ref type="bibr" target="#b34">[35]</ref> <ref type="bibr" target="#b38">[39]</ref>. However, these methods are not robust in complex or crowded scenes with multiple occlusions and shadows, because trajectory features are based on object tracking and it is very easy to fail in these cases. Taking consideration of the shortcomings of trajectory features, low-level spatial-temporal features, such as histogram of oriented gradients (HOG) <ref type="bibr" target="#b6">[7]</ref>, histogram of oriented flows (HOF) <ref type="bibr" target="#b7">[8]</ref> are widely used. Based on spatial-temporal features, Zhang et al. <ref type="bibr" target="#b40">[41]</ref> ex-ploit a Markov random filed (MRF) for modeling the normal patterns. Adam et al. <ref type="bibr" target="#b1">[2]</ref> characterize the regularly local histograms of optical flow by an exponential distribution. <ref type="bibr">Kim and Grauman [18]</ref> model the local optical flow pattern with a mixture of probabilistic PCA (MPPCA). Mahadevan et al. <ref type="bibr" target="#b24">[25]</ref> fit a Gaussian mixture model to mixture of dynamic textures (MDT). Besides these statistic models, sparse coding or dictionary learning is also a popular approach to encode the normal patterns <ref type="bibr" target="#b5">[6]</ref>[22] <ref type="bibr" target="#b41">[42]</ref>. The fundamental underlying assumption of these methods is that any regular pattern can be linearly represented as a linear combination of basis of a dictionary which encodes normal patterns on training set. Therefore, a pattern is considered as an anomaly if its reconstruction error is high and vice verse. However, optimizing the sparse coefficients is usually timeconsuming in sparse reconstruction based methods. In order to accelerate both in training and testing phase, Lu et al <ref type="bibr" target="#b21">[22]</ref> propose to discard the sparse constraint and learn multiple dictionaries to encode normal scale-invariant patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Deep Learning Based Methods</head><p>Deep learning approaches have demonstrated their successes in many computer vision tasks <ref type="bibr" target="#b11">[12]</ref>[20] as well as anomaly detection <ref type="bibr" target="#b13">[14]</ref>. In the work <ref type="bibr" target="#b39">[40]</ref>, Xu et al. design a multi-layer Auto-Encoder for feature learning, which demonstrates the effectiveness of deep learning features. In another work <ref type="bibr" target="#b13">[14]</ref>, a 3D convolutional Auto-Encoder (Conv-AE) is proposed by Hasan to model regular frames. Further, motivated by the observation that Convolutional Neural Networks (CNN) has strong capability to learn spatial features, while Recurrent Neural Network (RNN)and its long short term memory (LSTM) variant have been widely used for sequential data modeling. Thus, by taking both advantages of CNN and RNN, <ref type="bibr" target="#b4">[5]</ref>[23] leverage a Convolutional LSTMs Auto-Encoder (ConvLSTM-AE) to model normal appearance and motion patterns at the same time, which further boosts the performance of the Conv-AE based solution. In <ref type="bibr" target="#b23">[24]</ref>, Luo et al. propose a temporally coherent sparse coding based method which can map to a stacked RNN framework. Besides, Ryota et al. <ref type="bibr" target="#b14">[15]</ref> combine detection and recounting of abnormal events. However, all these anomaly detections are based on the reconstruction of regular training data, even though all these methods assume that abnormal events would correspond to larger reconstruction errors, due to the good capacity and generalization of deep neural network, this assumption does not necessarily hold. Therefore, reconstruction errors of normal and abnormal events will be similar, resulting in less discrimination.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Video Frame Prediction</head><p>Recently, prediction learning is attracting more and more researchers' attention in light of its potential applications in unsupervised feature learning for video representation <ref type="bibr" target="#b26">[27]</ref>.</p><p>In <ref type="bibr" target="#b30">[31]</ref>, <ref type="bibr">Shi et al.</ref> propose to modify original LSTM with ConvLSTM and use it for precipitation forecasting. In <ref type="bibr" target="#b26">[27]</ref>, a multi-scale network with adversarial training is proposed to generate more natural future frames in videos. In <ref type="bibr" target="#b20">[21]</ref>, a predictive neural network is designed and each layer in the network also functions as making local predictions and only forwarding deviations. All aforementioned work focuses on how to directly predict future frames. Different from these work, recently, people propose to predict transformations needed for generating future frames <ref type="bibr" target="#b3">[4]</ref> [36] <ref type="bibr" target="#b37">[38]</ref>, which further boosts the performance of video prediction. <ref type="bibr" target="#b36">[37]</ref> uses a LSTM based motion encoder to encode all history motion for prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Future Frame Prediction Based Anomaly Detection Method</head><p>Since anomaly detection is the identification of events that do not conform the expectation, it is more natural to predict future video frames based on previous video frames, and compare the prediction with its ground truth for anomaly detection. Thus we propose to leverage video prediction for anomaly detection. To generate a high quality video frame, most existing work <ref type="bibr" target="#b16">[17]</ref>[27] only considers appearance constraints by imposing intensity loss <ref type="bibr" target="#b26">[27]</ref>, gradient loss <ref type="bibr" target="#b26">[27]</ref>, or adversarial training loss <ref type="bibr" target="#b16">[17]</ref>. However, only appearance constraints cannot guarantee to characterize the motion information well. Besides spatial information, temporal information is also an important feature of videos. So we propose to add an optical flow constraint into the objective function to guarantee the motion consistency for normal events in training set, which further boosts the performance for anomaly detection, as shown in the experiment section (section 4.5 and 4.6). It is worth noting abnormal events can be justified by either appearance (A giant monster appears in a shopping mall) or motion (A pickpocket walks away from an unlucky guy), and our future frame prediction solution leverages both the appearance and motion loss for normal events, therefore these abnormal events can be easily identified by comparing the prediction and ground truth. Thus the appearance and motion losses based video prediction are more consistent with anomaly detection.</p><p>Mathematically, given a video with consecutive t frames I 1 , I 2 , . . . , I t , we sequentially stack all these frames and use them to predict a future frame I t+1 . We denote our prediction asÎ t+1 . To makeÎ t+1 close to I t+1 , we minimize their distance regarding intensity as well as gradient. To preserve the temporal coherence between neighboring frames, we enforce the optical flow between I t+1 and I t and that betweenÎ t+1 and I t to be close. Finally, the difference between a future frame's prediction and itself determines whether it is normal or abnormal. The network architecture of our framework is shown in <ref type="figure">Figure 2</ref>. Next, we will  introduce all the components of our framework in details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Future Frame Prediction</head><p>The network used for frame generation or image generation in existing work <ref type="bibr" target="#b13">[14]</ref>[27] usually contains two modules: i) an encoder which extracts features by gradually reducing the spatial resolution; and ii) a decoder which gradually recovers the frame by increasing the spatial resolution. However, such a solution confronts with the gradient vanishing problem and information imbalance in each layer. To avoid this, U-Net <ref type="bibr" target="#b29">[30]</ref> is proposed by adding a shortcut between a high level layer and a low level layer with the same resolution. Such a manner suppresses gradient vanishing and results in information symmetry. We slightly modify U-Net for future frame prediction in our implementation. Specifically, for each two convolution layers, we keep output resolution unchanged. Consequently, it does not need the crop and resize operations anymore when adding shortcuts. The details of this network are illustrated in <ref type="figure" target="#fig_1">Figure 3</ref>. The kernel sizes of all convolution and deconvolution are set to 3 × 3 and that of max pooling layers are set to 2 × 2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The Constraints on Intensity and Gradient</head><p>To make the prediction close to its ground truth, following the work <ref type="bibr" target="#b26">[27]</ref>, intensity and gradient difference are used. The intensity penalty guarantees the similarity of all pixels in RGB space, and the gradient penalty can sharpen the generated images. Specifically, we minimize the ℓ 2 distance between a predicted frameÎ and its ground true I in intensity space as follows:</p><formula xml:id="formula_0">L int (Î, I) = Î − I 2 2 (1)</formula><p>Further, we define the gradient loss by following previous work <ref type="bibr" target="#b26">[27]</ref> as follows:</p><formula xml:id="formula_1">L gd (Î, I) = i,j |Î i,j −Î i−1,j | − |I i,j − I i−1,j | 1 + |Î i,j −Î i,j−1 | − |I i,j − I i,j−1 | 1 (2)</formula><p>where i, j denote the spatial index of a video frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">The Constraint on Motion</head><p>Previous work <ref type="bibr" target="#b26">[27]</ref> only considers the difference between intensity and gradient for future frame generation, and it can not guarantee to predict a frame with the correct motion. This is because even a slight change in terms of the pixel intensity of all pixels in a predicted frame may result in a totally different optical flow (a good estimator of motion <ref type="bibr" target="#b31">[32]</ref>), though it corresponds to a small prediction error in terms of intensity and gradient. So it is desirable to guarantee the correctness of motion prediction. Especially for anomaly detection, the coherence of motion is an important factor for the evaluation of normal events <ref type="bibr" target="#b39">[40]</ref>. Therefore, we introduce a temporal loss defined as the difference between optical flow of prediction frames and ground truth. Recently, a CNN based approach has been proposed for optical flow estimation <ref type="bibr" target="#b9">[10]</ref>. Thus we use the Flownet <ref type="bibr" target="#b9">[10]</ref> for optical flow estimation. We denote f as the Flownet, then the loss in terms of optical flow can be expressed as follows:</p><formula xml:id="formula_2">L op (Î t+1 , I t+1 , I t ) = f (Î t+1 , I t ) − f (I t+1 , I t ) 1 (3)</formula><p>In our implementation, f is pre-trained on a synthesized dataset <ref type="bibr" target="#b9">[10]</ref>, and all the parameters in f are fixed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Adversarial Training</head><p>Generative adversarial networks (GAN) have demonstrated its usefulness for image and video generation <ref type="bibr" target="#b8">[9]</ref>[27]. It is the difference from [27] that we leverage a variant of GAN, Least Square GAN <ref type="bibr" target="#b25">[26]</ref>, module for generating a more realistic frame. Usually GAN contains a discriminative network D and a generator network G. G learns to generate frames that are hard to be classified by D, while D aims to discriminate the frames generated by G. Ideally, when G is well trained, D cannot predict better than chance. In practice, adversarial training is implemented with an alternative update manner. Moreover, we treat the U-Net based prediction network as G. As for D, we follow <ref type="bibr" target="#b16">[17]</ref> and utilize a patch discriminator which means each output scalar of D corresponds a patch of an input image. Totally, the training schedule is illustrated as follows:</p><p>Training D. The goal of training D is to classify I t+1 into class 1 and G(I 1 , I 2 , ..., I t ) =Î t+1 into class 0, where 0 and 1 represent fake and genuine labels, respectively. When training D, we fix the weights of G, and a Mean Square Error (MSE) loss function is imposed:</p><formula xml:id="formula_3">L D adv (Î, I) = i,j 1 2 L M SE (D(I) i,j , 1) + i,j 1 2 L M SE (D(Î) i,j , 0)<label>(4)</label></formula><p>where i, j denotes the spatial patches indexes and L M SE is a MSE function, which is defined as follows:</p><formula xml:id="formula_4">L M SE (Ŷ , Y ) = (Ŷ − Y ) 2 (5)</formula><p>where Y takes values in {0,1} andŶ ∈ [0, 1]</p><p>Training G. The goal of training G is to generate frames where D classify them into class 1. When training G, the weights of D are fixed. Again, a MSE function is imposed as follows:</p><formula xml:id="formula_5">L G adv (Î) = i,j 1 2 L M SE (D(Î) i,j , 1)<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Objective Function</head><p>We combine all these constraints regarding appearance, motion, and adversarial training, into our objective function, and arrive at the following objective function:</p><formula xml:id="formula_6">L G = λ int L int (Î t+1 , I t+1 ) + λ gd L gd (Î t+1 , I t+1 ) + λ op L op (Î t+1 , I t+1 , I t ) + λ adv L G adv (Î t+1 )<label>(7)</label></formula><p>When we train D, we use the following loss function:</p><formula xml:id="formula_7">L D = L D adv (Î t+1 , I t+1 )<label>(8)</label></formula><p>To train the network, the intensity of pixels in all frames are normalized to [-1, 1] and the size of each frame is resized to 256 × 256. Similar to <ref type="bibr" target="#b26">[27]</ref>, we set t = 4 and randomly clip 5 sequential frames. Adam <ref type="bibr" target="#b18">[19]</ref> based Stochastic Gradient Descent method is used for parameter optimization. The mini-batch size is 4. For gray scale datasets, the learning rate of generator and discriminator are set to 0.0001 and 0.00001, separately. While for color scale datasets, they start from 0.0002 and 0.00002, respectively. λ int , λ gd , λ op and λ adv slightly vary from datasets and an easy way is to set them as 1.0, 2.0 and 0.05, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Anomaly Detection on Testing Data</head><p>We assume that normal events can be well predicted. Therefore, we can use the difference between predicted frameÎ and its ground truth I for anomaly prediction. MSE is one popular way to measure the quality of predicted images by computing a Euclidean distance between the prediction and its ground truth of all pixels in RGB color space. However, Mathieu <ref type="bibr" target="#b26">[27]</ref> shows that Peak Signal to Noise Ratio (PSNR) is a better way for image quality assessment, shown as following:</p><formula xml:id="formula_8">P SN R(I,Î) = 10 log 10 [maxÎ ] 2 1 N N i=0 (I i −Î i ) 2</formula><p>High PSNR of the t-th frame indicates that it is more likely to be normal. After calculating each frame's PSNR of each testing video, following the work <ref type="bibr" target="#b26">[27]</ref>, we normalize PSNR of all frames in each testing video to the range [0, 1] and calculate the regular score for each frame by using the following equation:</p><formula xml:id="formula_9">S(t) = P SN R(I t ,Î t ) − min t P SN R(I t ,Î t ) max t P SN R(I t ,Î t ) − min t P SN R(I t ,Î t )</formula><p>Therefore, we can predict whether a frame is normal or abnormal based its score S(t). One can set a threshold to distinguish regular or irregular frames.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we evaluate our proposed method as well as the functionalities of different components on three publicly available anomaly detection datasets, including the CUHK Avenue dataset <ref type="bibr" target="#b21">[22]</ref>, the UCSD Pedestrian dataset <ref type="bibr" target="#b24">[25]</ref> and the ShanghaiTech Campus dataset <ref type="bibr" target="#b23">[24]</ref>. We further use a toy dataset to validate the robustness of our method, i.e., even if there exists some uncertainties in normal events, our method can still correctly classify normal and abnormal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Here we briefly introduce the datasets used in our experiments. Some samples are shown in <ref type="figure" target="#fig_2">Figure 4</ref>.</p><p>• CUHK Avenue dataset contains 16 training videos and 21 testing ones with a total of 47 abnormal events, including throwing objects, loitering and running. The size of people may change because of the camera position and angle.</p><p>• The UCSD dataset contains two parts: The UCSD Pedestrian 1 (Ped1) dataset and the UCSD Pedestrian 2 (Ped2) dataset. The UCSD Pedestrian 1 (Ped1) dataset includes 34 training videos and 36 testing ones with 40 irregular events. All of these abnormal cases are about vehicles such as bicycles and cars. The UCSD Pedestrian 2 (Ped2) dataset contains 16 training videos and 12 testing videos with 12 abnormal events. The definition of anomaly for Ped2 is the same with Ped1. Usually different methods are evaluated on these two parts separately.</p><p>• The ShanghaiTech dataset is a very challenging anomaly detection dataset. It contains 330 training videos and 107 testing ones with 130 abnormal events. Totally, it consists of 13 scenes and various anomaly types. Following the setting used in <ref type="bibr" target="#b23">[24]</ref>, we train the model on all scenes altogether.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>In the literature of anomaly detection <ref type="bibr" target="#b21">[22]</ref>[25], a popular evaluation metric is to calculate the Receiver Operation Characteristic (ROC) by gradually changing the threshold of regular scores. Then the Area Under Curve (AUC) is cumulated to a scalar for performance evaluation. A higher value indicates better anomaly detection performance. In this paper, following the work <ref type="bibr" target="#b13">[14]</ref>[24], we leverage framelevel AUC for performance evaluation.  <ref type="bibr" target="#b10">[11]</ref> 78.3% N/A N/A N/A Conv-AE <ref type="bibr" target="#b13">[14]</ref> 80.0% 75.0% 85.0% 60.9% ConvLSTM-AE <ref type="bibr" target="#b22">[23]</ref> 77.0% 75.5% 88.1% N/A GrowingGas <ref type="bibr" target="#b33">[34]</ref> N/A 93.8% 94.1% N/A AbnormalGAN <ref type="bibr" target="#b28">[29]</ref> N/A 97.4% 93.5% N/A DeepAppearance <ref type="bibr" target="#b32">[33]</ref> 84.6% N/A N/A N/A Hinami et al. <ref type="bibr" target="#b14">[15]</ref> N/A N/A 92.2% N/A Unmasking <ref type="bibr" target="#b15">[16]</ref> 80.6% 68.4% 82.2% N/A Stacked RNN <ref type="bibr" target="#b23">[24]</ref> 81  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with Existing Methods</head><p>In this section, we compare our method with different hand-craft features based method <ref type="bibr" target="#b10">[11]</ref>  <ref type="bibr" target="#b32">[33]</ref>. The AUC of different methods is listed in <ref type="table" target="#tab_1">Table 1</ref>. We can see that our method outperforms most of the existing methods, which demonstrates the effectiveness of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Design of Prediction Network</head><p>In our anomaly detection framework, the future frame prediction network is an important module. To evaluate how different prediction networks affect the performance of anomaly detection, we compare our U-Net prediction network with Beyond Mean Square Error (Beyond-MSE) <ref type="bibr" target="#b26">[27]</ref> which achieves state-of-the-art performance for video generation. Beyond-MSE leverages a multi-scale prediction network to gradually generate video frames with larger spatial resolution. Because of its multi-scale strategy, it is much slower than U-Net. To be consistent with Beyond-MSE, we adapt our network architecture by removing the motion constraint and only use the intensity loss, the gradient loss and adversarial training in our U-Net based solution.</p><p>Quantitative comparison for anomaly detection. We first compute the gap between average score of normal frames and that of abnormal frames, denoted as ∆ s . Then, we compare the result of U-Net with that of Beyond-MSE on the Ped1 and Ped2 datasets, respectively. Larger ∆ s means the network can be more capable to distinguish normal and abnormal patterns. Finally, we compare the U-Net based solution and Beyond-MSE with the AUC metric on the Ped1 and Ped2 datasets, respectively. We demonstrate the results in <ref type="table" target="#tab_4">Table 2</ref>. We can see that our method both achieves a larger ∆ s and higher AUC than Beyond-MSE, which show that our network is more suitable for anomaly detection than Beyond-MSE. Therefore, we adapt U-Net architecture as our prediction network. As we aforementioned, the results listed here do not contain motion constraint, which would further boost the AUC. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Impact of Constraint on Motion</head><p>To evaluate the importance of motion constraint for video frame generation as well as anomaly detection, we conduct the experiment by removing the constraint from the objective in the training. Then we compare such a baseline with our method.</p><p>Qualitative evaluation of motion constraint with optical flow maps. We show the optical flow maps generated with/without motion constraint in <ref type="figure" target="#fig_3">Figure 5</ref>, we can see that the optical flow generated with motion constraint is more consistent with ground truth, which shows that such motion constraint term helps our prediction network to capture motion information more precisely. We also compare the MSE between optical flow maps generated with/without motion constraint and the ground truth, which is 7.51 and 8.26, respectively. This further shows the effectiveness of motion constraint.</p><p>Quantitative evaluation of motion with anomaly detection. The result in <ref type="table">Table 3</ref> shows that the model trained with motion constraint consistently achieves higher AUC than that without the constraint on Ped1 and Ped2 dataset. This also proves that it is necessary to explicitly impose the motion consistency constraint into the objective for anomaly detection. <ref type="table">Table 3</ref>. AUC for anomaly detection of networks with/wo the motion constraint in Ped1 and Ped2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ped1</head><p>Ped2 without motion constraint 81.8% 93.5% with motion constraint 83.1% 95.4% <ref type="figure">Figure 6</ref>. The evaluation of different components in our future frame prediction network in the Avenue dataset. Each column in the histogram corresponds to a method with different loss functions. We calculate the average scores of normal and abnormal events in the testing set. The gap is calculated by subtracting the abnormal score from the normal one. <ref type="figure">Figure 7</ref>. We firstly compute the average score for normal frames and that for abnormal frames in the testing set of the Ped1, Ped2 and Avenue datasets. Then, we calculate the difference of these two scores(∆s) to measure the ability of our method and Conv-AE to discriminate normal and abnormal frames. A larger gap(∆s) corresponds to small false alarm rate and higher detection rate. The results show that our method consistently outperforms Conv-AE in term of the score gap between normal and abnormal events.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Impact of Different Losses</head><p>We also analyze the impact of different loss functions for anomaly detection by ablating different terms gradually. We combine different losses to conduct experiments on the Avenue dataset. To evaluate how different losses affect the performance of anomaly detection, we also utilize the score gap(∆ s ) mentioned above. The larger gap represents the more discriminations between normal and abnormal frames. The results in <ref type="figure">Figure 6</ref> show more constraints usually achieve a higher gap as well as AUC value, and our method achieves the highest value under all settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Comparison of Prediction Network and AutoEncoder Networks for Anomaly Detection</head><p>We also compare the video prediction network based and Auto-Encoder network based anomaly detection. Here for Auto-Encoder network based anomaly detection, we use the Conv-AE <ref type="bibr" target="#b13">[14]</ref> which is the latest work and achieves stateof-the-art performance for anomaly detection. Because of the capacity of deep neural network, Auto-Encoder based methods may well reconstruct normal and abnormal frames  <ref type="figure">Figure 8</ref>. The visualization of predicted testing frames in our toy pedestrian dataset. There are two abnormal cases including vehicle intruding(left column) and humans fighting(right column). The orange circles correspond to normal events with uncertainty in prediction while the red ones correspond to abnormal events. It is noticeable that the predicted truck is blurred, because no vehicles appear in the training set. Further, in the fighting case, two persons cannot be predicted well because fighting motion never appear in the training phase.</p><p>in the testing phase. To evaluate the performance of prediction network and the Auto-Encoder one, we also utilize the aforementioned gap(∆ s ) between normal and abnormal scores. The result in <ref type="figure">Figure 7</ref> shows that our solution always achieves higher gaps than Conv-AE, which validates the effectiveness of video prediction for anomaly detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Evaluation with A Toy Dataset</head><p>We also design a toy pedestrian dataset for performance evaluation. In the training set, only a pedestrian walks on the road and he/she can choose different directions when he/she comes to a crossroad. In the testing set, there are some abnormal cases such as vehicles intruding, humans fighting, etc.. We have uploaded our toy dataset in the supplementary material. Totally, the training data contains 210 frames and testing data contains 1242 frames.</p><p>It is interesting that the motion direction is sometimes also uncertain for normal events, for example, a pedestrian stands at the crossroad. Even though we cannot predict the motion well, we only cannot predict the next frame at a moment which leads a slightly instant drop in terms of PSNR. After observing the pedestrian for a while when the pedestrian has made his or her choice, it becomes predictable and PSNR would go up, shown in <ref type="figure">Figure 8</ref>. Therefore the uncertainty of normal events does not affect our solution too much. However, for the real abnormal events, for example, a truck breaks into the scene and hits the pedestrian and it would lead to a continuous lower PSNR, which facilitates the anomaly prediction. Totally, the AUC is 98.9%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.9.">Running Time</head><p>Our framework is implemented with TensorFlow <ref type="bibr" target="#b0">[1]</ref>. We benchmark the performance of our system on above datasets. All tests are performed on NVIDIA GeForce TI-TAN GPUs with Intel Xeon(R) E5-2643 3.40GHz CPUs and Samsung SSD 850 PRO. The average running time is about 25 fps, which contains both the video frame generation and anomaly prediction. We also report the running time of other methods such as 20 fps in <ref type="bibr" target="#b15">[16]</ref>, 150 fps <ref type="bibr" target="#b21">[22]</ref> and 0.5 fps in <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>Since normal events are predictable while abnormal events do not conform to the expectation, therefore we propose a future frame prediction network for anomaly detection. Specifically, we use a U-Net as our basic prediction network. To generate a more realistic future frame, other than adversarial training and constraints in appearance, we also impose a loss in temporal space to ensure the optical flow of predicted frames to be consistent with ground truth. In this way, we can guarantee to generate the normal events in terms of both appearance and motion, and the events with larger difference between prediction and ground truth would be classified as anomalies. Extensive experiments on three datasets show our method outperforms existing methods by a large margin, which proves the effectiveness of our method for anomaly detection.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The network architecture of our main prediction network (U-Net). The resolutions of input and output are the same.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Some samples including normal and abnormal frames in the UCSD, CUHK Avenue and ShanghaiTech datasets are illustrated. Red boxes denote anomalies in abnormal frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The visualization of optical flow and the predicted images on the Ped1 dataset. The red boxes represent the difference of optical flow predicted by the model with/without motion constraint. We can see that the optical flow predicted by the model with motion constraint is closer to ground truth. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>AUC of different methods on the Avenue, Ped1, Ped2 and ShanghaiTech datasets. All methods are listed by the published year.</figDesc><table>CUHK Avenue 
UCSD Ped1 
UCSD Ped2 
ShanghaiTech 
MPPCA [18] 
N/A 
59.0% 
69.3% 
N/A 
MPPC+SFA [25] 
N/A 
66.8% 
61.3% 
N/A 
MDT [25] 
N/A 
81.8% 
82.9% 
N/A 
Del et al. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 .</head><label>2</label><figDesc>The gap (∆s) and AUC of different prediction networks in the Ped1 and Ped2 datasets.</figDesc><table>Ped1 
Ped2 
∆ s 
AUC 
∆ s 
AUC 
Beyond-MSE 0.200 75.8% 0.396 88.5% 
U-Net 
0.243 81.8% 0.435 93.5% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Uncertainty in A Normal Event &amp; Vehicle Intruding</figDesc><table>0 

20 

40 

60 

1 
9 
17 
25 
33 
41 
49 
57 
65 
73 
81 
89 
97 
105 
113 
121 
129 
137 
145 
153 
161 
169 
177 

PSNR 

#Frame 

crossroad 

Vehicle Intrude 
crossroad 

• ! 
• ""# 

• $% 

• &amp;&amp; 

• "$$ 

• &amp;% 

• "'! 

• "($ 

prediction 
ground truth 

0 

20 

40 

60 

1 
15 
29 
43 
57 
71 
85 
99 
113 
127 
141 
155 
169 
183 
197 
211 
225 
239 
253 
267 
281 
295 
309 
323 
337 
351 

PSNR 

#Frame 

Jump fight 
crossroad 

normal 
abnormal 
normal 
abnormal 

prediction 
ground truth 

Fighting 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The project is supported by NSFC (NO. 61502304 ) and Shanghai Subject Chief Scientist (A type) (No. 15XD1502900).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Robust real-time unusual event detection using multiple fixedlocation monitors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rivlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Shimshoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Reinitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="555" to="560" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Anomaly detection: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Chandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Banerjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kumar</surname></persName>
		</author>
		<idno>15:1-15:58</idno>
	</analytic>
	<monogr>
		<title level="j">ACM Comput. Surv</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Video imagination from a single image with transformation generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the on Thematic Workshops of ACM Multimedia</title>
		<meeting>the on Thematic Workshops of ACM Multimedia<address><addrLine>Mountain View, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-23" />
			<biblScope unit="page" from="358" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using spatiotemporal autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">S</forename><surname>Chong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Networks -ISNN 2017 -14th International Symposium</title>
		<meeting><address><addrLine>Sapporo, Hakodate, and Muroran, Hokkaido, Japan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-21" />
			<biblScope unit="page" from="189" to="196" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Sparse reconstruction cost for abnormal event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3449" to="3456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2006, 9th European Conference on Computer Vision</title>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A discriminative framework for anomaly detection in large videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">D</forename><surname>Giorno</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference, Amsterdam</title>
		<meeting><address><addrLine>The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="334" to="349" />
		</imprint>
	</monogr>
	<note>Proceedings, Part V</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE International Conference on Computer Vision, ICCV 2015</title>
		<meeting><address><addrLine>Santiago, Chile</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger, editors</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning temporal regularity in video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Roy-Chowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Las Vegas, NV, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-06-27" />
			<biblScope unit="page" from="733" to="742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint detection and recounting of abnormal events by learning deep generic knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hinami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satoh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="3639" to="3647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Unmasking the abnormal events in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2914" to="2922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-21" />
			<biblScope unit="page" from="5967" to="5976" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Observe locally, infer globally: A space-time MRF for detecting abnormal activities with incremental updates</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Miami, Florida, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2009-06" />
			<biblScope unit="page" from="2921" to="2928" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 25: 26th Annual Conference on Neural Information Processing Systems 2012. Proceedings of a meeting held</title>
		<meeting><address><addrLine>Lake Tahoe, Nevada, United States</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1106" to="1114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Deep predictive coding networks for video prediction and unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Lotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kreiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.08104</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Abnormal event detection at 150 FPS in MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision, ICCV 2013</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2720" to="2727" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Remembering history with convolutional LSTM for anomaly detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Multimedia and Expo</title>
		<meeting><address><addrLine>Hong Kong, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-07-10" />
			<biblScope unit="page" from="439" to="444" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">A revisit of sparse coding based anomaly detection in stacked RNN framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="341" to="349" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Bhalodia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="1975" to="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Least squares generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">Y K</forename><surname>Lau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Smolley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<meeting><address><addrLine>Venice, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-10-22" />
			<biblScope unit="page" from="2813" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Deep multiscale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno>abs/1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Anomaly detection in video using predictive convolutional long short-term memory networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Medel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Savakis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00390</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Abnormal event detection in videos using generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Nabi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sangineto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marcenaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Regazzoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing</title>
		<meeting><address><addrLine>Beijing, China</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-09-17" />
			<biblScope unit="page" from="1577" to="1581" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Medical Image Computing and Computer-Assisted Intervention -MICCAI 2015 -18th International Conference Munich</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
	<note>Proceedings, Part III</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Convolutional LSTM network: A machine learning approach for precipitation nowcasting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Woo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 28: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-12-07" />
			<biblScope unit="page" from="802" to="810" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>De- cember 8-13</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 27: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Montreal, Quebec, Canada</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Deep appearance features for abnormal behavior detection in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Smeureanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">T</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Popescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Alexe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing -ICIAP 2017 -19th International Conference</title>
		<meeting><address><addrLine>Catania, Italy</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="779" to="789" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Online growing neural gas for anomaly detection in changing surveillance scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Harada</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="187" to="201" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Goal-based trajectory analysis for unusual behaviour detection in intelligent surveillance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Zelek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Clausi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Image Vision Comput</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="230" to="240" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Transformation-based models of video sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Van Amersfoort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.08435</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Decomposing motion and content for natural video sequence prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Villegas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Generating the future with adversarial transformers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting><address><addrLine>Honolulu, HI, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2017</biblScope>
			<biblScope unit="page" from="2992" to="3000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Chaotic invariants of lagrangian particle trajectories for anomaly detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Twenty-Third IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2010</title>
		<meeting><address><addrLine>San Francisco, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2010-06" />
			<biblScope unit="page" from="2054" to="2060" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Learning deep representations of appearance and motion for anomalous event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the British Machine Vision Conference</title>
		<meeting>the British Machine Vision Conference<address><addrLine>Swansea, UK</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015-09-07" />
		</imprint>
	</monogr>
	<note>pages 8.1-8.12</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Semi-supervised adapted hmms for unusual event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gatica-Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Mccowan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR 2005</title>
		<meeting><address><addrLine>San Diego, CA, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2005-06" />
			<biblScope unit="page" from="611" to="618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Online detection of unusual events in videos via dynamic sparse coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 24th IEEE Conference on Computer Vision and Pattern Recognition, CVPR 2011</title>
		<meeting><address><addrLine>Colorado Springs, CO, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2011-06" />
			<biblScope unit="page" from="3313" to="3320" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
