<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ICSI</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese University of Hong Kong ‡ Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">⋆</forename></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuanjun</forename><surname>Xiong</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stella</forename><forename type="middle">X</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">ICSI</orgName>
								<address>
									<settlement>Berkeley</settlement>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">Chinese University of Hong Kong ‡ Amazon Rekognition</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dahua</forename><surname>Lin</surname></persName>
						</author>
						<title level="a" type="main">Unsupervised Feature Learning via Non-Parametric Instance Discrimination</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The rise of deep neural networks, especially convolutional neural networks (CNN), has led to several breakthroughs in computer vision benchmarks. Most successful models are trained via supervised learning, which requires large datasets that are completely annotated for a specific task. However, obtaining annotated data is often very costly or even infeasible in certain cases. In recent years, unsupervised learning has received increasing attention from the community <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b1">2]</ref>.</p><p>Our novel approach to unsupervised learning stems from a few observations on the results of supervised learning for object recognition. On ImageNet, the top-5 classification error is significantly lower than the top-1 error <ref type="bibr" target="#b17">[18]</ref>, and the second highest responding class in the softmax output to an leopard jaguar cheetah lifeboat shopcart bookcase <ref type="figure">Figure 1</ref>: Supervised learning results that motivate our unsupervised approach. For an image from class leopard, the classes that get highest responses from a trained neural net classifier are all visually correlated, e.g., jaguar and cheetah. It is not the semantic labeling, but the apparent similarity in the data themselves that brings some classes closer than others. Our unsupervised approach takes the class-wise supervision to the extreme and learns a feature representation that discriminates among individual instances.</p><p>image is more likely to be visually correlated. <ref type="figure">Fig. 1</ref> shows that an image from class leopard is rated much higher by class jaguar rather than by class bookcase <ref type="bibr" target="#b10">[11]</ref>. Such observations reveal that a typical discriminative learning method can automatically discover apparent similarity among semantic categories, without being explicitly guided to do so. In other words, apparent similarity is learned not from semantic annotations, but from the visual data themselves.</p><p>We take the class-wise supervision to the extreme of instance-wise supervision, and ask: Can we learn a meaningful metric that reflects apparent similarity among instances via pure discriminative learning? An image is distinctive in its own right, and each could differ significantly from other images in the same semantic category <ref type="bibr" target="#b22">[23]</ref>. If we learn to discriminate between individual instances, without any notion of semantic categories, we may end up with a representation that captures apparent similarity among instances, just like how class-wise supervised learning still retains apparent similarity among classes. This formulation of unsupervised learning as an instance-level discrimination is also technically appealing, as it could benefit from latest advances in discriminative supervised learning, e.g. on new network architectures.</p><p>However, we also face a major challenge, now that the number of "classes" is the size of the entire training set. For ImageNet, it would be 1.2-million instead of 1,000 classes. Simply extending softmax to many more classes becomes infeasible. We tackle this challenge by approximating the full softmax distribution with noise-contrastive estimation (NCE) <ref type="bibr" target="#b8">[9]</ref>, and by resorting to a proximal regularization method <ref type="bibr" target="#b28">[29]</ref> to stabilize the learning process.</p><p>To evaluate the effectiveness of unsupervised learning, past works such as <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b30">31]</ref> have relied on a linear classifier, e.g. Support Vector Machine (SVM), to connect the learned feature to categories for classification at the test time. However, it is unclear why features learned via a training task could be linearly separable for an unknown testing task.</p><p>We advocate a non-parametric approach for both training and testing. We formulate instance-level discrimination as a metric learning problem, where distances (similarity) between instances are calculated directly from the features in a non-parametric way. That is, the features for each instance are stored in a discrete memory bank, rather than weights in a network. At the test time, we perform classification using k-nearest neighbors (kNN) based on the learned metric. Our training and testing are thus consistent, since both learning and evaluation of our model are concerned with the same metric space between images. We report and compare experimental results with both SVM and kNN accuracies.</p><p>Our experimental results demonstrate that, under unsupervised learning settings, our method surpasses the stateof-the-art on image classification by a large margin, with top-1 accuracy 42.5% on ImageNet 1K <ref type="bibr" target="#b0">[1]</ref> and 38.7% for Places 205 <ref type="bibr" target="#b48">[49]</ref>. Our method is also remarkable for consistently improving test performance with more training data and better network architectures. By fine-tuning the learned feature, we further obtain competitive results for semi-supervised learning and object detection tasks. Finally, our non-parametric model is highly compact: With 128 features per image, our method requires only 600MB storage for a million images, enabling fast nearest neighbour retrieval at the run time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>There has been growing interest in unsupervised learning without human-provided labels. Previous works mainly fall into two categories: 1) generative models and 2) selfsupervised approaches.</p><p>Generative Models. The primary objective of generative models is to reconstruct the distribution of data as faithfully as possible. Classical generative models include Restricted Bolztmann Machines (RBMs) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b20">21]</ref>, and Autoencoders <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20]</ref>. The latent features produced by generative models could also help object recognition. Recent approaches such as generative adversarial networks <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b3">4]</ref> and variational auto-encoder <ref type="bibr" target="#b13">[14]</ref> improve both generative qualities and feature learning.</p><p>Self-supervised Learning. Self-supervised learning exploits internal structures of data and formulates predictive tasks to train a model. Specifically, the model needs to predict either an omitted aspect or component of an instance given the rest. To learn a representation of images, the tasks could be: predicting the context <ref type="bibr" target="#b1">[2]</ref>, counting the objects <ref type="bibr" target="#b27">[28]</ref>, filling in missing parts of an image <ref type="bibr" target="#b30">[31]</ref>, recovering colors from grayscale images <ref type="bibr" target="#b46">[47]</ref>, or even solving a jigsaw puzzle <ref type="bibr" target="#b26">[27]</ref>. For videos, self-supervision strategies include: leveraging temporal continuity via tracking <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b44">45]</ref>, predicting future <ref type="bibr" target="#b41">[42]</ref>, or preserving the equivariance of egomotion <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b49">50,</ref><ref type="bibr" target="#b29">30]</ref>. Recent work <ref type="bibr" target="#b2">[3]</ref> attempts to combine several self-supervised tasks to obtain better visual representations. Whereas self-supervised learning may capture relations among parts or aspects of an instance, it is unclear why a particular self supervision task should help semantic recognition and which task would be optimal.</p><p>Metric Learning. Every feature representation F induces a metric between instances x and y: d F (x, y) = F (x) − F (y) . Feature learning can thus also be viewed as a certain form of metric learning. There have been extensive studies on metric learning <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b32">33]</ref>. Successful application of metric learning can often result in competitive performance, e.g. on face recognition <ref type="bibr" target="#b34">[35]</ref> and person reidentification <ref type="bibr" target="#b45">[46]</ref>. In these tasks, the classes at the test time are disjoint from those at the training time. Once a network is trained, one can only infer from its feature representation, not from the subsequent linear classifier. Metric learning has been shown to be effective for few-shot learning <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b36">37]</ref>. An important technical point on metric learning for face recognition is normalization <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b42">43]</ref>, which we also utilize in this work. Note that all the methods mentioned here require supervision in certain ways. Our work is drastically different: It learns the feature and thus the induced metric in an unsupervised fashion, without any human annotations.</p><p>Exemplar CNN. Exemplar CNN <ref type="bibr" target="#b4">[5]</ref> appears similar to our work. The fundamental difference is that it adopts a parametric paradigm during both training and testing, while our method is non-parametric in nature. We study this essential difference experimentally in Sec 4.1. Exemplar CNN is computationally demanding for large-scale datasets such as ImageNet. The pipeline of our unsupervised feature learning approach. We use a backbone CNN to encode each image as a feature vector, which is projected to a 128-dimensional space and L2 normalized. The optimal feature embedding is learned via instance-level discrimination, which tries to maximally scatter the features of training samples over the 128-dimensional unit sphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Approach</head><p>Our goal is to learn an embedding function v = f θ (x) without supervision. f θ is a deep neural network with parameters θ, mapping image x to feature v. This embedding would induces a metric over the image space, as</p><formula xml:id="formula_0">d θ (x, y) = f θ (x) − f θ (y)</formula><p>for instances x and y. A good embedding should map visually similar images closer to each other.</p><p>Our novel unsupervised feature learning approach is instance-level discrimination. We treat each image instance as a distinct class of its own and train a classifier to distinguish between individual instance classes ( <ref type="figure" target="#fig_0">Fig.2)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Non-Parametric Softmax Classifier</head><p>Parametric Classifier. We formulate the instance-level classification objective using the softmax criterion. Suppose we have n images x 1 , . . . , x n in n classes and their features v 1 , . . . , v n with v i = f θ (x i ). Under the conventional parametric softmax formulation, for image x with feature v = f θ (x), the probability of it being recognized as i-th example is</p><formula xml:id="formula_1">P (i|v) = exp w T i v n j=1 exp w T j v .<label>(1)</label></formula><p>where w j is a weight vector for class j, and w T j v measures how well v matches the j-th class i.e., instance. Non-Parametric Classifier. The problem with the parametric softmax formulation in Eq. <ref type="formula" target="#formula_1">(1)</ref> is that the weight vector w serves as a class prototype, preventing explicit comparisons between instances.</p><p>We propose a non-parametric variant of Eq. (1) that replaces w T j v with v T j v, and we enforce v = 1 via a L2-normalization layer. Then the probability P (i|v) becomes:</p><formula xml:id="formula_2">P (i|v) = exp v T i v/τ n j=1 exp v T j v/τ ,<label>(2)</label></formula><p>where τ is a temperature parameter that controls the concentration level of the distribution <ref type="bibr" target="#b10">[11]</ref>. τ is important for supervised feature learning <ref type="bibr" target="#b42">[43]</ref>, and also necessary for tuning the concentration of v on our unit sphere. The learning objective is then to maximize the joint probability</p><formula xml:id="formula_3">n i=1 P θ (i|f θ (x i ))</formula><p>, or equivalently to minimize the negative log-likelihood over the training set, as</p><formula xml:id="formula_4">J(θ) = − n i=1 log P (i|f θ (x i )).<label>(3)</label></formula><p>Learning with A Memory Bank. To compute the probability P (i|v) in Eq. (2), {v j } for all the images are needed. Instead of exhaustively computing these representations every time, we maintain a feature memory bank V for storing them <ref type="bibr" target="#b45">[46]</ref>. In the following, we introduce separate notations for the memory bank and features forwarded from the network. Let V = {v j } be the memory bank and f i = f θ (x i ) be the feature of x i . During each learning iteration, the representation f i as well as the network parameters θ are optimized via stochastic gradient descend. Then f i is updated to V at the corresponding instance entry</p><formula xml:id="formula_5">f i → v i .</formula><p>We initialize all the representations in the memory bank V as unit random vectors. Discussions. The conceptual change from class weight vector w j to feature representation v j directly is significant. The weight vectors {w j } in the original softmax formulation are only valid for training classes. Consequently, they are not generalized to new classes, or in our setting, new instances. When we get rid of these weight vectors, our learning objective focuses entirely on the feature representation and its induced metric, which can be applied everywhere in the space and to any new instances at the test time.</p><p>Computationally, our non-parametric formulation eliminates the need for computing and storing the gradients for {w j }, making it more scalable for big data applications.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Noise-Contrastive Estimation</head><p>Computing the non-parametric softmax in Eq. <ref type="formula" target="#formula_2">(2)</ref> is cost prohibitive when the number of classes n is very large, e.g. at the scale of millions. Similar problems have been well addressed in the literature for learning word embeddings <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, where the number of words can also scale to millions. Popular techniques to reduce computation include hierarchical softmax <ref type="bibr" target="#b25">[26]</ref>, noise-contrastive estimation (NCE) <ref type="bibr" target="#b8">[9]</ref>, and negative sampling <ref type="bibr" target="#b23">[24]</ref>. We use NCE <ref type="bibr" target="#b8">[9]</ref> to approximate the full softmax.</p><p>We adapt NCE to our problem, in order to tackle the difficulty of computing the similarity to all the instances in the training set. The basic idea is to cast the multi-class classification problem into a set of binary classification problems, where the binary classification task is to discriminate between data samples and noise samples. Specifically, the probability that feature representation v in the memory bank corresponds to the i-th example under our model is,</p><formula xml:id="formula_6">P (i|v) = exp(v T f i /τ ) Z i (4) Z i = n j=1 exp v T j f i /τ (5)</formula><p>where Z i is the normalizing constant. We formalize the noise distribution as a uniform distribution: P n = 1/n. Following prior work, we assume that noise samples are m times more frequent than data samples. Then the posterior probability of sample i with feature v being from the data distribution (denoted by D = 1) is:</p><formula xml:id="formula_7">h(i, v) := P (D = 1|i, v) = P (i|v) P (i|v) + mP n (i) .<label>(6)</label></formula><p>Our approximated training objective is to minimize the negative log-posterior distribution of data and noise samples,</p><formula xml:id="formula_8">J N CE (θ) = −E P d [log h(i, v)] −m·E Pn [log(1 − h(i, v ′ ))] .<label>(7)</label></formula><p>Here, P d denotes the actual data distribution. For P d , v is the feature corresponding to x i ; whereas for P n , v ′ is the feature from another image, randomly sampled according to noise distribution P n . In our model, both v and v ′ are sampled from the non-parametric memory bank V .</p><p>Computing normalizing constant Z i according to Eq. (4) is expensive. We follow <ref type="bibr" target="#b24">[25]</ref>, treating it as a constant and estimating its value via Monte Carlo approximation:</p><formula xml:id="formula_9">Z ≃ Z i ≃ nE j exp(v T j f i /τ ) = n m m k=1 exp(v T j k f i /τ ),<label>(8)</label></formula><p>where {j k } is a random subset of indices. Empirically, we find the approximation derived from initial batches sufficient to work well in practice.</p><p>NCE reduces the computational complexity from O(n) to O(1) per sample. With such drastic reduction, our experiments still yield competitive performance. Unlike typical classification settings where each class has many instances, we only have one instance per class. During each training epoch, each class is only visited once. Therefore, the learning process oscillates a lot from random sampling fluctuation. We employ the proximal optimization method <ref type="bibr" target="#b28">[29]</ref> and introduce an additional term to encourage the smoothness of the training dynamics. At current iteration t, the feature representation for data x i is computed from the network v</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proximal Regularization</head><formula xml:id="formula_10">(t) i = f θ (x i ).</formula><p>The memory bank of all the representation are stored at previous iteration V = {v (t−1) }. The loss function for a positive sample from P d is:</p><formula xml:id="formula_11">− log h(i, v (t−1) i ) + λ v (t) i − v (t−1) i 2 2 .<label>(9)</label></formula><p>As learning converges, the difference between iterations,</p><formula xml:id="formula_12">i.e. v (t) i − v (t−1) i</formula><p>, gradually vanishes, and the augmented loss is reduced to the original one. With proximal regularization, our final objective becomes: <ref type="figure" target="#fig_1">Fig. 3</ref> shows that, empirically, proximal regularization helps stabilize training, speed up convergence, and improve the learned representation, with negligible extra cost.</p><formula xml:id="formula_13">J N CE (θ) = −E P d log h(i, v (t−1) i ) − λ v (t) i − v (t−1) i 2 2 −m·E Pn log(1 − h(i, v ′(t−1) )) .<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Weighted k-Nearest Neighbor Classifier</head><p>To classify test imagex, we first compute its featuref = f θ (x), and then compare it against the embeddings of all the images in the memory bank, using the cosine similarity s i = cos(v i ,f ). The top k nearest neighbors, denoted by N k , would then be used to make the prediction via weighted voting. Specifically, the class c would get a total weight w c = i∈N k α i · 1(c i = c). Here, α i is the contributing weight of neighbor x i , which depends on the similarity as α i = exp(s i /τ ). We choose τ = 0.07 as in training and we set k = 200.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct 4 sets of experiments to evaluate our approach. The first set is on CIFAR-10 to compare our nonparametric softmax with parametric softmax. The second set is on ImageNet to compare our method with other unsupervised learning methods. The last two sets of experiments investigate two different tasks, semi-supervised learning and object detection, to show the generalization ability of our learned feature representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Parametric vs. Non-parametric Softmax</head><p>A key novelty of our approach is the non-parametric softmax function. Compared to the conventional parametric softmax, our softmax allows a non-parametric metric to transfer to supervised tasks.</p><p>We compare both parametric and non-parametric formulations on CIFAR-10 [17], a dataset with 50, 000 training instances in 10 classes. This size allows us to compute the non-parametric softmax in Eq.(2) without any approximation. We use ResNet18 as the backbone network and its output features mapped into 128-dimensional vectors.</p><p>We evaluate the classification effectiveness based on the learned feature representation. A common practice <ref type="bibr" target="#b47">[48,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b30">31]</ref> is to train an SVM on the learned feature over the training set, and to then classify test instances based on the feature extracted from the trained network. In addition, we also use nearest neighbor classifiers to assess the learned feature. The latter directly relies on the feature metric and may better reflect the quality of the representation. <ref type="table">Table 1</ref> shows top-1 classification accuracies on CI-FAR10. On the features learned with parametric softmax, we obtain accuracies of 60.3% and 63.0% with linear SVM and kNN classifiers respectively. On the features learned with non-parametric softmax, the accuracy rises to 75.4% and 80.8% for the linear and nearest neighbour classifiers, a remarkable 18% boost for the latter.</p><p>We also study the quality of NCE approximating nonparametric softmax (Sec. 3.2). The approximation is controlled by m, the number of negatives drawn for each instance. With m = 1, the accuracy with kNN drops significantly to 42.5%. As m increases, the performance improves steadily. When m = 4, 096, the accuracy approaches that at m = 49, 999 -full form evaluation without any approximation. This result provides assurance that NCE is an efficient approximation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Image Classification</head><p>We learn a feature representation on ImageNet ILSVRC <ref type="bibr" target="#b33">[34]</ref>, and compare our method with representative unsupervised learning methods.</p><p>Experimental Settings. We choose design parameters via empirical validation. In particular, we set temperature τ = 0.07 and use NCE with m = 4, 096 to balance performance and computing cost. The model is trained for 200 epochs using SGD with momentum. The batch size is 256. The learning rate is initialized to 0.03, scaled down with coefficient 0.1 every 40 epochs after the first 120 epochs. Our code is available at: http://github. com/zhirongw/lemniscate.pytorch.</p><p>Comparisons. We compare our method with a randomly initialized network (as a lower bound) and various unsupervised learning methods, including self-supervised learning <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b47">48]</ref>, adversarial learning <ref type="bibr" target="#b3">[4]</ref>, and Exemplar CNN <ref type="bibr" target="#b2">[3]</ref>. The split-brain autoencoder <ref type="bibr" target="#b47">[48]</ref> serves a strong baseline that represents the state of the art. The results of these methods are reported with AlexNet architecture <ref type="bibr" target="#b17">[18]</ref> in their original papers, except for exemplar CNN <ref type="bibr" target="#b4">[5]</ref>, whose results are reported with ResNet-101 <ref type="bibr" target="#b2">[3]</ref>. As the network architecture has a big impact on the performance, we consider a few typical architectures: AlexNet <ref type="bibr" target="#b17">[18]</ref>, VGG16 <ref type="bibr" target="#b35">[36]</ref>, ResNet-18, and ResNet-50 <ref type="bibr" target="#b9">[10]</ref>.</p><p>We evaluate the performance with two different protocols: (1) Perform linear SVM on the intermediate features from conv1 to conv5. Note that there are also corresponding layers in VGG16 and ResNet <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b9">10]</ref>. <ref type="formula" target="#formula_2">(2)</ref> Perform kNN on the output features.   2. Using nearest neighbor classification on the final 128 dimensional features, our method achieves 31.3%, 33.9%, 40.5% and 42.5% accuracies with AlexNet, VGG16, ResNet-18 and ResNet-50, not much lower than the linear classification results, demonstrating that our learned feature induces a reasonably good metric. As a comparison, for Split-brain, the accuracy drops to 8.9% with nearest neighbor classification on conv3 features, and to 11.8% after projecting the features to 128 dimensions.</p><p>3. With our method, the performance gradually increases as we examine the learned feature representation from earlier to later layers, which is generally desirable. With all other methods, the performance decreases beyond conv3 or conv4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4.</head><p>It is important to note that the features from intermediate convolutional layers can be over 10, 000 dimen- sions. Hence, for other methods, using the features from the best-performing layers can incur significant storage and computation costs. Our method produces a 128-dimensional representation at the last layer, which is very efficient to work with. The encoded features of all 1.28M images in ImageNet only take about 600 MB of storage. Exhaustive nearest neighbor search over this dataset only takes 20 ms per image on a Titan X GPU.</p><p>Feature generalization. We also study how the learned feature representations can generalize to other datasets.</p><p>With the same settings, we conduct another large-scale experiment on Places <ref type="bibr" target="#b48">[49]</ref>, a large dataset for scene classification, which contains 2.45M training images in 205 categories. In this experiment, we directly use the feature extraction networks trained on ImageNet without finetuning. Consistency of training and testing objectives. Unsupervised feature learning is difficult because the training objective is agnostic about the testing objective. A good training objective should be reflected in consistent improvement in the testing performance. We investigate the relation between the training loss and the testing accuracy across iterations. <ref type="figure" target="#fig_2">Fig. 4</ref> shows that our testing accuracy continues to improve as training proceeds, with no sign of overfitting. It also suggests that better optimization of the training objective may further improve our testing accuracy.   The embedding feature size. We study how the performance changes as we vary the embedding size from 32 to 256. <ref type="table" target="#tab_5">Table 4</ref> shows that the performance increases from 32, plateaus at 128, and appears to saturate towards 256.</p><p>Training set size. To study how our method scales with the data size, we train different representations with various proportions of ImageNet data, and evaluate the classification performance on the full labeled set using nearest neighbors. <ref type="table" target="#tab_6">Table 5</ref> shows that our feature learning method benefits from larger training sets, and the testing accuracy improves as the training set grows. This property is crucial for successful unsupervised learning, as there is no shortage of unlabeled data in the wild.</p><p>Qualitative case study. To illustrate the learned features, <ref type="figure" target="#fig_4">Figure 5</ref> shows the results of image retrieval using the learned features. The upper four rows show the best cases training set size 0.1% 1% 10% 30% 100% accuracy 3.9 10.7 23.1 31.7 40.5 where all top 10 results are in the same categories as the queries. The lower four rows show the worst cases where none of the top 10 are in the same categories. However, even for the failure cases, the retrieved images are still visually similar to the queries, a testament to the power of our unsupervised learning objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Semi-supervised Learning</head><p>We now study how the learned feature extraction network can benefit other tasks, and whether it can provide a good basis for transfer learning to other tasks. A common scenario that can benefit from unsupervised learning is when we have a large amount of data of which only a small fraction are labeled. A natural semi-supervised learning approach is to first learn from the big unlabeled data and then fine-tune the model on the small labeled data.</p><p>We randomly choose a subset of ImageNet as labeled and treat others as unlabeled. We perform the above semisupervised learning and measure the classification accuracy on the validation set. In order to compare with <ref type="bibr" target="#b18">[19]</ref>, we report the top-5 accuracy here. We compare our method with three baselines: (1) Scratch, i.e. fully supervised training on the small labeled subsets, (2) Split-brain <ref type="bibr" target="#b47">[48]</ref> for pre-training, and (3) Colorization <ref type="bibr" target="#b18">[19]</ref> for pre-training. Finetuning on the labeled subset takes 70 epochs with initial learning rate 0.01 and a decay rate of 10 every 30 epochs. We vary the proportion of labeled subset from 1% to 20% of the entire dataset. <ref type="figure" target="#fig_5">Fig. 6</ref> shows that our method significantly outperforms all other approaches, and ours is the only one outperforming supervised learning from limited labeled data. When only 1% of data is labeled, we outperform by a large 10% margin, demonstrating that our feature learned from unlabeled data is effective for task adaptation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Object Detection</head><p>To further assess the generalization capacity of the learned features, we transfer the learned networks to the new task of object detection on PASCAL VOC 2007 <ref type="bibr" target="#b5">[6]</ref>. Training object detection model from scratch is often difficult, and a prevalent practice is to pretrain the underlying CNN on ImageNet and fine-tune it for the detection task.</p><p>We experiment with Fast R-CNN <ref type="bibr" target="#b6">[7]</ref> with AlexNet and VGG16 architectures, and Faster R-CNN <ref type="bibr" target="#b31">[32]</ref> with ResNet-50. When fine-tuning Fast R-CNN, the learning rate is initialized to 0.001 and scaled down by 10 times after every 50K iterations. When fine-tuning AlexNet and VGG16, we follow the standard practice, fixing the conv1 model weights. When fine-tuning Faster R-CNN, we fix the model  weights below the 3rd type of residual blocks, only updating the layers above and freezing all batch normalization layers. We follow the standard pipeline for finetuning and do not use the rescaling method proposed in <ref type="bibr" target="#b1">[2]</ref>. We use the standard trainval set in VOC 2007 for training and testing.</p><p>We compare three settings: 1) directly training from scratch (lower bound), 2) pretraining on ImageNet in a supervised way (upper bound), and 3) pretraining on ImageNet or other data using various unsupervised methods. <ref type="table" target="#tab_8">Table 6</ref> lists detection performance in terms of mean average precision (mAP). With AlexNet and VGG16, our method achieves an mAP of 48.1% and 60.5%, on par with the state-of-the-art unsupervised methods. With Resnet-50, our method achieves an mAP of 65.4%, surpassing all existing unsupervised learning approaches. It also shows that our method scales well as the network gets deeper. There remains a significant gap of 11% to be narrowed towards mAP 76.2% from supervised pretraining.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Summary</head><p>We present an unsupervised feature learning approach by maximizing distinction between instances via a novel nonparametric softmax formulation. It is motivated by the observation that supervised learning results in apparent image similarity. Our experimental results show that our method outperforms the state-of-the-art on image classification on ImageNet and Places, with a compact 128-dimensional representation that scales well with more data and deeper networks. It also delivers competitive generalization results on semi-supervised learning and object detection tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The pipeline of our unsupervised feature learning approach. We use a backbone CNN to encode each image as a feature</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The effect of our proximal regularization. The original objective value oscillates a lot and converges very slowly, whereas the regularized objective has smoother learning dynamics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Our kNN testing accuracy on ImageNet continues to improve as the training loss decreases, demonstrating that our unsupervised learning objective captures apparent similarity which aligns well with the semantic annotation of the data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Retrieval results for example queries. The left column are queries from the validation set, while the right columns show the 10 closest instances from the training set. The upper half shows the best cases. The lower half shows the worst cases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Semi-supervised learning results on ImageNet with an increasing fraction of labeled data (x axis). Ours are consistently and significantly better. Note that the results for colorization-based pretraining are from a deeper ResNet-152 network [19].</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 shows</head><label>2</label><figDesc></figDesc><table>that: 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Top-1 classification accuracies on ImageNet.</figDesc><table>Image Classification Accuracy on Places 
method 
conv1 conv2 conv3 conv4 conv5 kNN #dim 
Random 
15.7 20.3 19.8 19.1 17.5 3.9 10K 
Data-Init [16] 21.4 26.2 27.1 26.1 24.0 -10K 
Context [2] 19.7 26.7 31.9 32.7 30.9 -10K 
Adversarial [4] 17.7 24.5 31.0 29.9 28.0 -10K 
Video [44] 20.1 28.5 29.9 29.7 27.9 -10K 
Color [47] 
22.0 28.7 31.8 31.3 29.7 -10K 
Jigsaw [27] 23.0 32.1 35.5 34.8 31.3 -10K 
SplitBrain [48] 21.3 30.7 34.0 34.1 32.5 10.8 10K 
Ours Alexnet 18.8 24.3 31.9 34.5 33.6 30.1 128 
Ours VGG16 17.6 23.1 29.5 33.8 36.3 32.8 128 
Ours Resnet18 17.8 23.0 30.3 34.2 41.3 36.7 128 
Ours Resnet50 18.1 22.3 29.7 34.1 42.1 38.7 128 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Top-1 classification accuracies on Places, based directly on features learned on ImageNet, without any fine-tuning.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3</head><label>3</label><figDesc></figDesc><table>compares the results obtained with different meth-
ods and under different evaluation policies. Again, with 
linear classifier on conv5 features, our method achieves 
competitive performance of top-1 accuracy 34.5% with 
AlexNet, and 42.1% with ResNet-50. With nearest neigh-
bors on the last layer which is much smaller than intermedi-
ate layers, we achieve an accuracy of 38.7% with ResNet-
50. These results show remarkable generalization ability of 
the representations learned using our method. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Classification performance on ImageNet with ResNet18 for different embedding feature sizes.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Classification performances trained on different amount of training set with ResNet-18.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Object detection performance on PASCAL VOC 2007 test, in terms of mean average precision (mAP), for supervised pretraining methods (marked by †), existing un- supervised methods, and our method.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unsupervised visual representation learning by context prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multi-task self-supervised visual learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.07860</idno>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">The pascal visual object classes (voc) challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Noise-contrastive estimation: A new estimation principle for unnormalized statistical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gutmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<title level="m">Distilling the knowledge in a neural network</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A fast learning algorithm for deep belief nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Teh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Learning image representations tied to egomotion from unlabeled video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Large scale metric learning from equivalence constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Koestinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hirzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wohlhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06856</idno>
		<title level="m">Datadependent initializations of convolutional neural networks</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Colorization as a proxy task for visual understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Building high-level features using large scale unsupervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing (ICASSP), 2013 IEEE International Conference on</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Convolutional deep belief networks for scalable unsupervised learning of hierarchical representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Grosse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 26th annual international conference on machine learning</title>
		<meeting>the 26th annual international conference on machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Ensemble of exemplar-svms for object detection and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV. IEEE</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning word embeddings efficiently with noise-contrastive estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hierarchical probabilistic neural network language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Morin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Aistats</title>
		<imprint>
			<publisher>Citeseer</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Unsupervised learning of visual representations by solving jigsaw puzzles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<editor>ECCV. Springer</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Representation learning by learning to count</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noroozi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Favaro</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.06734</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Proximal algorithms. Foundations and Trends R in Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.06370</idno>
		<title level="m">Learning features by watching objects move</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Context encoders: Feature learning by inpainting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krahenbuhl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Neighbourhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst.(NIPS)</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05175</idno>
		<title level="m">Prototypical networks for few-shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Robust boltzmann machines for recognition and denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Extracting and composing robust features with denoising autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-A</forename><surname>Manzagol</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25th international conference on Machine learning</title>
		<meeting>the 25th international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">An uncertain future: Forecasting from static images using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Walker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06369</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Unsupervised learning of visual representations using videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Transitive invariance for self-supervised visual representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.02901</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title level="m" type="main">Joint detection and identification feature learning for person search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Colorful image colorization. ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Split-brain autoencoders: Unsupervised learning by cross-channel prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.07813</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
