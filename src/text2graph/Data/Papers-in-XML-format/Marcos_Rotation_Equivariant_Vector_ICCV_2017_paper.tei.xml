<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Rotation equivariant vector field networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Marcos</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michele</forename><surname>Volpi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Ecole des Ponts</orgName>
								<address>
									<settlement>Paris Tech</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devis</forename><surname>Tuia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Zurich</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Rotation equivariant vector field networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In many real life problems, such as overhead (aerial or satellite) or biomedical image analysis, there are no dominant up-down or left-right relationships. For example, when detecting cars in aerial images, the object's absolute orientation is not a discriminant feature. If the absolute orientation of the image is changed, e.g. by following a different flightpath, we would expect the car detector to score the exact same values over the same cars, just in their new position on the rotated image, independently from their new orientation along the image axes. In this case, we say that the problem is rotation equivariant: rotating the input is expected to result in the same rotation in the output. On the other hand, if we were confronted with a classification setting in which we are only interested in the presence or absence of cars in the whole scene, the classification score should remain the same, no matter the absolute orientation of the input scene. In this case the problem is rotation invariant. The more general case would be rotation covariance, in which the output changes as a function of the rotation of the input, with some predefined behavior. Taking again the cars example, a rotation covariant problem would be to retrieve the absolute orientation of cars with respect to longitude and latitude: in this case, a rotation of the image should produce a change of the predicted angle. Throughout this article we will make use of the terms equivariance, invariance and covariance of a function f (·) with respect to a transformation g(·) in the following sense:</p><p>-equivariance: f (g(·)) = g(f (·)), -invariance: f (g(·)) = f (·), -covariance: f (g(·)) = g ′ (f (·)),</p><p>where g ′ (·) is a second transformation, which is itself a function of g(·). With the above definitions, equivariance and invariance are special cases of covariance. We illustrate these properties in <ref type="figure" target="#fig_0">Fig. 1</ref>.</p><p>In this paper, we propose a CNN architecture that naturally encodes these three properties: RotEqNet. In the following, we will recall how CNNs achieve translation invariance, before discussing our own proposition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Dealing with translations in CNNs</head><p>The success of CNNs is partly due to the translation equivariant nature of the convolution operation. The convolution of an image x ∈ R M ×N ×d with a filter w ∈ R m×n×d , written y = w * x, is obtained by applying the same scalar product operation over all overlapping m × n windows (unit stride) on x. If x undergoes an integer translation in the horizontal and vertical directions by (p, q) pixels, the same pixel neighborhoods in x will exist in the translated x, but again translated by (p, q) pixels. Therefore, any operation involving fixed neighborhoods such as the convolution is translation equivariant.</p><p>A crucial consequence of learning convolution weights is a drastic reduction in the number of parameters. Without the translation equivariance assumption, each local window would have a different set of weights. Forcing weights to be shared across locations, known as weight tying, reduces the number of learnable parameters proportionally to the number of pixels in the image and hardcodes translation equivariance within the model. This fact is vital for the applicability of deep neural networks to images <ref type="bibr" target="#b17">[18]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Incorporating rotation equivariance in CNNs</head><p>RotEqNet shows similar advantageous characteristics when dealing with rotations: by encoding equivariance, we are able to strongly reduce the number of parameters while keeping similar or better accuracy across different tasks.</p><p>However, applying the exact same reasoning of weight tying for rotations is not straightforward. To follow the same logic, one should apply R rotated versions of each convolutional filter, resulting in R feature maps per filter. The dimensionality of subsequent filters would therefore increase with R, strongly increasing model size and requirements for runtime memory usage.</p><p>One way of reducing the size of the model while keeping rotation equivariance would be to propagate only the maximum value occurring across R feature maps. However, deeper layers would have no information about the orientation of features at previous layers.</p><p>We propose a trade-off between these two approaches by keeping the maximum value across the R feature maps, but in the form of a 2D vector field that captures its magnitude and orientation and propagates it through all the layers of the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>Two families of approaches explicitly account for rotation invariance or equivariance: 1) those that transform the representation (image or feature maps) and 2) those that rotate the filters. RotEqNet belongs to the latter. 1) Rotating the inputs: Jaderberg et al. <ref type="bibr" target="#b13">[14]</ref> propose the Spatial Transformer layer, which learns how to crop and transform a region of the image (or a feature map) before passing it to the next layer. This transforms relevant regions into a canonical form, improving the learning process by reducing geometrical appearance variations in subsequent layers. TI-pooling <ref type="bibr" target="#b15">[16]</ref> inputs several rotated versions of a same image to the same CNN and then performs pooling across the different feature vectors at the first fully connected layer. Such scheme allows another subsequent fully connected layer to choose among rotated inputs to perform classification. Cheng et al. <ref type="bibr" target="#b4">[5]</ref> employ in every minibatch several rotated versions of the input images. Their representations after the first fully connected layer are then encouraged to be similar, forcing the CNN to learn rotation invariance. Henriques et al. <ref type="bibr" target="#b10">[11]</ref> warp the images such that the translation equivariance inherent to convolutions is transformed into rotation and scale equivariance.</p><p>On the one hand, these methods have the advantage of exploiting conventional CNN implementations, since they only act on data representations. On the other hand, they can only consider global transformations of the input images. While this is well suited for tasks such as image classification, it limits their applicability to other problems (e.g. semantic segmentation), where the local relative orientation of certain objects with respect to surroundings is what matters. Instead, RotEqNet is based on specific CNN building blocks designed to deal with local orientation information. Therefore, RotEqNet can approach diverse tasks such as classification, fully convolutional semantic segmentation, detection and regression.</p><p>It is worth mentioning that standard data augmentation strategies belong to this first family. They rely on random rotations and flips of the training samples <ref type="bibr" target="#b23">[24]</ref>: given abundant training samples and enough model capacity, a CNN might learn that different orientations should score the same by learning equivalent filters at different orientations <ref type="bibr" target="#b18">[19]</ref>. Unlike this, RotEqNet is well suited for problems with limited training samples that can profit from reduced model sizes, since the behavior with respect to rotations is hardcoded and it does not need to be learned.</p><p>2) Rotating the filters: Gens and Domingos <ref type="bibr" target="#b9">[10]</ref> tackle the problem of the exploding dimensionality (discussed in Sec. 1.2) by applying learnable pooling operations and sampling the symmetry space at each layer. This way, they avoid applying the filters exhaustively across the (high dimensional) feature maps by selectively sampling few rotations. By doing so, only the least important information is lost from layer to layer. Cohen et al. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7]</ref> use a smaller symmetry group, composed of a flipping and four 90</p><p>• rotations and perform pooling within the group. They apply it only in deeper layers, since they found that pooling in the early layers discards important information and harms the performance. Instead of explicitly defining a symmetry group, Ngiam et al. <ref type="bibr" target="#b20">[21]</ref> pool across several untied filters, thus letting the network learn the type of invariance. Sifre et al. <ref type="bibr" target="#b22">[23]</ref> use hand crafted wavelets that are separable in the roto-translational space, allowing for more efficient computations. Another approach to avoid the dimensionality explosion is to limit the depth of the network: Sohn et al. <ref type="bibr" target="#b25">[26]</ref> and Kivinen et al. <ref type="bibr" target="#b14">[15]</ref> propose such a scheme with Restricted Boltzmann Machines (RBM), while Marcos et al. <ref type="bibr" target="#b19">[20]</ref> consider supervised CNNs consisting of a single convolutional layer.</p><p>These works find a compromise between the computational resources required and the amount of orientation information kept throughout the layers, by either keeping the model shallow or accounting for a limited amount of orientations. With RotEqNet, we avoid such compromise by pooling multiple orientations and passing forward both the maximum magnitude and the orientation at which it occurred. This modification allows to build deep rotation equivariant architectures, in which deeper layers are aware of the dominant orientations. At the same time, the dimensionality of feature maps and filters is kept low by discarding information about non-maximum orientations, thus reducing memory requirements.</p><p>The most similar approaches to RotEqNet are the recently proposed Harmonic Networks (H-Nets) <ref type="bibr" target="#b28">[29]</ref> and Oriented Response Networks (ORN) <ref type="bibr" target="#b30">[31]</ref>, both of which use an enriched feature map explicitly capturing the underlying orientations. They do so by using either complex circular harmonics (H-Nets) or the full vector of oriented responses (ORN). H-Nets offer a very compact feature map, but are limited to learning filters that are a combination of circular harmonic wavelets. On the other hand, ORN allows to learn arbitrary filters, but relies on a much less compact representation of the feature maps, leading to heavier models both in terms of size and memory requirements. RotEqNet provides the best of both worlds: the compactness of the former with the flexibility of the latter. These properties make it particularly suitable to address problems characterized by limited training samples, as we will see in the experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Rotation equivariant vector field networks</head><p>We focus on achieving rotation equivariance by performing convolutions with several rotated instances of the same canonical filter (see <ref type="figure" target="#fig_1">Fig. 2</ref>). The canonical filter w is rotated at R different evenly spaced orientations.In the experiments (Sec. 4) we deal with problems requiring either full invariance, equivariance or covariance, so we use the inter-</p><formula xml:id="formula_0">val α = [0 • , 360 • ].</formula><p>However, this interval can be adapted to a known range of tilts. The output of the filter w at a specific location consists of the magnitude of the maximal activation across the orientations and the corresponding angle. If we convert this polar representation into Cartesian coordinates, each filter w produces a vector field feature map z ∈ R H×W ×2 , where the output of each location consists of two values [u, v] ∈ R 2 implicitly encoding the maximal activation in both magnitude and direction. Since the feature maps have become vector fields, from this moment on the filters must also be vector fields, as seen in the right part of <ref type="figure" target="#fig_1">Fig. 2</ref>.</p><p>The advantage of representing z in Cartesian coordinates is that the horizontal and vertical components [u, v] are orthogonal, and thus a convolution of the two vector fields can be computed on each component independently using standard convolutions (see Eq. <ref type="formula" target="#formula_5">(5)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">RotEqNet building blocks</head><p>RotEqNet requires specific building blocks to handle vectors fields as inputs and/or outputs <ref type="figure" target="#fig_1">(Fig. 2)</ref>. In the following, we present our reformulation of traditional CNN blocks to account for both vector field activations and filters. The implementation 1 is based on the MatConvNet [27] toolbox 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.1">Rotating convolution (RotConv)</head><p>Given an input image with m/2 zero-padding x ∈ R H+m/2×W +m/2×d , we apply the filter w ∈ R m×m×d at R orientations, corresponding to the angles:</p><formula xml:id="formula_1">α r = 360 R r ∀r = 1, 2 . . . R.<label>(1)</label></formula><p>Each one of these rotated versions of the canonical filters (highlighted by red squares in <ref type="figure" target="#fig_1">Fig. 2</ref>) is computed by resampling w with bilinear interpolation after rotation of α r degrees around the filter's center.</p><formula xml:id="formula_2">w r = g αr (w),<label>(2)</label></formula><p>where g α is the α degrees rotation operator. Interpolation is always required unless only rotations of multiples of 90</p><p>• are considered. In practice, this means that the rotation equivariance will only be approximate.</p><p>Since the rotation can force weights near the corners of the filter to be relocated outside of its spatial support, only the weights within a circle of diameter m pixels are used to compute the convolutions. The output tensor y ∈ R H×W ×R consists of R feature maps computed as:</p><formula xml:id="formula_3">y (r) = (x * w r ) ∀r = 1, 2 . . . R,<label>(3)</label></formula><p>where ( * ) is the convolution operator. The tensor y encodes the roto-translation output space such that rotation in the input corresponds to a translation across the feature maps. Note that only the canonical filter w is actually stored in the model. During backpropagation, gradients corresponding to each rotated filter ∇w r are aligned back to the canonical form and added:</p><formula xml:id="formula_4">∇w = r g −αr (∇w r ).<label>(4)</label></formula><p>This block can be applied on conventional CNN feature maps (left side of <ref type="figure" target="#fig_1">Fig. 2</ref>) or on vector field feature maps (right side of <ref type="figure" target="#fig_1">Fig. 2</ref>). In the second case it is computed on each component independently and the resulting 3D tensors added:</p><formula xml:id="formula_5">(z * w) = (z u * w u ) + (z v * w v ),<label>(5)</label></formula><p>where subscripts u and v denote the horizontal and vertical components.</p><p>It is important to note that the image rotation operator g α requires an additional step when w ∈ R m×m×2 is a 2D vector field. The components of w r = g αr (w) have to be computed as:</p><formula xml:id="formula_6">w r u = cos(α r )g αr (w u ) − sin(α r )g αr (w v )<label>(6)</label></formula><formula xml:id="formula_7">w r v = cos(α r )g αr (w v ) + sin(α r )g αr (w u )<label>(7)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2">Orientation pooling (OP):</head><p>Given the output 3D tensor y, the role of the orientation pooling is to convert it to a 2D vector field z ∈ R H×W ×2 . This avoids the exploding dimensionality problem by only keeping information about the maximally activating orientation of w. First, we extract a 2D map of the largest activation magnitudes, ρ ∈ R H×W , and their corresponding orientations, θ ∈ R H×W . Specifically, for activations located at [i, j]:</p><formula xml:id="formula_8">ρ[i, j] = max r y[i, j, r],<label>(8)</label></formula><formula xml:id="formula_9">θ[i, j] = 360 R arg max r y[i, j, r].<label>(9)</label></formula><p>This can be treated as a polar representation of a 2D vector field as long as ρ[i, j] ≥ 0 ∀i, j, a condition that is met when using any function on y that returns non-negative values prior to the OP. We employ the common Rectified Linear Unit (ReLu) operation, defined as ReLu(x) = max(x, 0), to ρ, as it provides non-saturating, sparse nonlinear activations offering stable training. Then, this representation can be transformed into Cartesian coordinates as:</p><formula xml:id="formula_10">u = ReLu(ρ) cos(θ) (10) v = ReLu(ρ) sin(θ)<label>(11)</label></formula><p>with u, v ∈ R H×W . The 2D vector field z is then built as:</p><formula xml:id="formula_11">z = 1 0 u + 0 1 v<label>(12)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.3">Spatial pooling (SP) for vector fields</head><p>Max-pooling is commonly used in CNNs to obtain some invariance to small deformations and reducing the size of the feature maps. This is done by downsampling the input feature map</p><formula xml:id="formula_12">x ∈ R M ×N ×d to x p ∈ R M p × N p</formula><p>×d . This operation is performed by taking the maximum value contained in each one of the C non-overlapping p × p regions of x, indexed by c. It is computed as x p [c] = max i∈c x[i], which can be expressed as:</p><formula xml:id="formula_13">y p [c] = y[j], where j = arg max i∈c y[i].<label>(13)</label></formula><p>This allows us to define a max-pooling for vector fields as:</p><formula xml:id="formula_14">z p [c] = z[j], where j = arg max i∈c ρ[i],<label>(14)</label></formula><p>where ρ is a standard scalar map containing the magnitudes of the vectors in z.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.4">Batch normalization (BN) for vector fields</head><p>BN <ref type="bibr" target="#b12">[13]</ref> normalizes every feature map in a mini-batch to zero mean and unit standard deviation. It improves convergence by training with stochastic gradient descent.</p><p>In our case, since working with vector fields of magnitude and orientation of activations, BN should only normalize magnitudes of the vectors to unit standard deviation. It would not make sense to normalize the angles, since their values are already bounded and changing their distribution would alter important information about relative and global orientations. Given a vector field feature map z and its map of magnitudes ρ, we compute batch normalization as:</p><formula xml:id="formula_15">z = z var(ρ) .<label>(15)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Computational considerations</head><p>Although RotEqNet allows for smaller models, they might require a higher count of convolutions than a comparable standard CNN. For instance, with the architecture used for MNIST-rot in Sec. 4, a standard CNN requires 4× more filters per layer to saturate performance, compared to RotEqNet. At the same time, RotEqNet requires R/4 = 4.25× (for R = 17) more convolutions. This results in RotEqNet saving 10× in model memory, 2× in data memory at a price of requiring just 1.5× more computing time. This is because, although the convolution count is higher, the number of feature maps per convolution is smaller. Less feature maps mean smaller convolution filters and the possibility to use larger mini batches, both factors contributing to a faster training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We explore the performance of RotEqNet on datasets where the orientation of the patterns of interest is arbitrary. This is very often the case in biomedical and abovehead imaging, since the orientation of the camera is usually not correlated with the patterns of interest. We apply RotEqNet to problems from these two fields, as well to MNIST-rot, a randomly rotated handwritten digit recognition benchmark. We also perform a study on the trade-off between invariance and accuracy in a synthetic patch matching problem. These case studies allow us to analyze the performance of RotEqNet in problems requiring equivariance, covariance and invariance to rotations and to analyze the effectiveness of RotEqNet to perform accurately with very small model architectures and limited training samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Invariance: MNIST-rot</head><p>MNIST-rot <ref type="bibr" target="#b16">[17]</ref> is a variant of the original MNIST digit recognition dataset, where a random rotation between 0</p><p>• and 360</p><p>• is applied to each 28×28 digit image. The training set is also considerably smaller than the standard MNIST, with 12k samples, from which 10k are used for training and 2k for validation. The test set consists of 50k samples. Since we aim at predicting the correct label independently from the rotation, this problem requires rotation invariance. Model: We test four CNN models with the same architecture, but different number of filters per layer. The largest model we used is shown in <ref type="table">Table 1</ref> and involves 100k parameters. The models are trained for 90 epochs, starting with a learning rate of 0.1 and reducing it gradually to 0.001. The weight decay is kept constant at 0.01. We use a dropout rate of 0.7 in the fully connected layer and batch normalization before every convolutional layer. The number of orientations is set to R = 17.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Test time data augmentation:</head><p>We observe an important contribution of data augmentation at test time, a technique often used with approximately invariant or equivariant CNNs <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. In particular, we input to the network several rotated versions of the same image using fixed angles between 0 • and 90</p><p>• . Rotation-based data augmentation at test time might seem counter-intuitive in a rotation invariant model, but the different rotations coupled to resampling of images and filters (cf. Sec. 3.1.1) will produce slightly different activations. The final prediction is given by the average of such scores. We report results obtained with and without this type of augmentation.</p><p>Comparison to data augmented training: In order to disentangle the contributions of data augmentation and RotEqNet, we trained the RotEqNet model and a standard CNN with the same architecture and 10× more parameters. In Tab. 4.1, we show the results for these models trained on both MNIST-rot and 10k digits from the original MNIST, with and without data augmentation. We observe how both methods complement each other.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error rate (in %) SVM <ref type="bibr" target="#b16">[17]</ref> 10.38±0.27 TIRBM <ref type="bibr" target="#b25">[26]</ref> 4.2 H-Net <ref type="bibr" target="#b28">[29]</ref> 1.69 ORN <ref type="bibr" target="#b30">[31]</ref> 1.54 TI-pooling <ref type="bibr" target="#b15">[16]</ref> 1.2 RotEqNet (Ours)   <ref type="table">Table 3</ref>. Results on MNIST and MNIST-rot using a standard CNN or RotEqNet, with and without data augmentation. Results: We first studied the behavior of RotEqNet with respect to the total number of parameters and compared it to the state-of-the-art TI-pooling <ref type="bibr" target="#b15">[16]</ref>. <ref type="figure" target="#fig_2">Figure 3</ref> shows the results for both methods trained on the training set with different model sizes. The latter was achieved by varying the number of filters per layer, keeping the same architecture.</p><p>RotEqNet requires approximately two orders of magnitude less parameters to obtain the same accuracy as TI-Pooling. We report the test error in <ref type="table" target="#tab_2">Table 2</ref>. RotEqNet obtains an error of 1.09%, a small improvement with respect to the state-of-the-art TI-pooling <ref type="bibr" target="#b15">[16]</ref>, but with almost 100× less parameters. Test-time data augmentation further reduces the error to 1.01%, thus improving significantly over TIPooling and over the more recent H-Net <ref type="bibr" target="#b28">[29]</ref> and ORN <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Equivariance: ISBI 2012 Challenge</head><p>This benchmark <ref type="bibr" target="#b0">[1]</ref> involves segmentation of neuronal structures in electron microscope (EM) stacks <ref type="bibr" target="#b2">[3]</ref>. In this problem we need to precisely locate the neuron membrane  boundaries. Therefore, a rotation of the inputs should lead to the same rotation in the output, making the ISBI 2012 problem a good candidate to study rotation equivariance. The data consist of two EM stacks of drosophila neurons, each composed of 30 images of size 512 × 512 pixels <ref type="figure" target="#fig_3">(Fig. 4a)</ref>. One stack is used for training and the other for testing. The ground truth for the training stack consists of densely annotated binary images <ref type="figure" target="#fig_3">(Fig. 4b)</ref>. The ground truth for the test stack is private and the results are to be submitted to an evaluation server <ref type="bibr" target="#b2">3</ref> . Model: We transform the original binary problem into a three class segmentation problem: 1) non-membrane, 2) central membrane pixels and 3) external membrane pixels. Pixels in the membrane but not belonging to either 2) or 3) are considered to be unlabeled <ref type="figure" target="#fig_3">(Fig. 4c</ref>). This way, we can assign a higher penalization to the non-membrane pixels next to the membrane and a lower one to those in the middle of the cells. The central membrane scores are used as the final binary prediction <ref type="figure" target="#fig_3">(Fig. 4d)</ref>.</p><formula xml:id="formula_16">Type Size Input 512 × 512 RotConv, 9 × 9, N filt. OP, 2 × 2 SP 256 × 256 × N × 2 RotConv, 9 × 9, 2N filt. OP, 2 × 2 SP 128 × 128×2N×2 RotConv 9 × 9×2N×2, 3N filt. OP, 2 × 2 SP 64 × 64×3N×2 RotConv, OP 9 × 9×3N×2,</formula><formula xml:id="formula_17">(a) (b) (c) (d)</formula><p>Since we are dealing with a dense prediction problem with spatial autocorrelation at different resolution levels, we apply three RotConv blocks with spatial pooling. We then upsample the output of each block to the size of the original image, before concatenating them and applying two more RotConv blocks. <ref type="table" target="#tab_4">Table 4</ref> shows the architecture. The parameter N is used to change the size of the model. We evaluated the results with N = 2 and with an ensemble of three models, with N = [1, 2, 3].</p><p>Comparison to data augmented training: we evaluated the RotEqNet model (N = 2) and an equivalent standard CNN with 10× more parameters on 5 held out validation images. RotEqNet seems not to profit as much from data augmentation as its standard CNN counterpart, but improves the CNN solution in all the cases considered, as illustrated in <ref type="table" target="#tab_4">Table 4</ref>  <ref type="table">Table 5</ref>. ISBI results on the validations set using a standard CNN or RotEqNet, with and without data augmentation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Rand. Thin Inf. Thin # params. CUMedVision <ref type="bibr" target="#b3">[4]</ref> 0.9768 0.9886 -IAL MC/LMC <ref type="bibr" target="#b1">[2]</ref> 0.9826 0.9894 -DIVE <ref type="bibr" target="#b8">[9]</ref> 0.9685 0.9858 5.7M PolyMtl <ref type="bibr" target="#b7">[8]</ref> 0.9689 0.9861 11M U-Net <ref type="bibr" target="#b21">[22]</ref> 0.9728 0.9866 33M RotEqNet <ref type="figure" target="#fig_1">(N = 2)</ref> 0.9599 0.9806 30k RotEqNet, 3 models 0.9712 0.9865 100k <ref type="table">Table 6</ref>. Scores on the held out test set of the ISBI 2012 Challenge.</p><p>Results: A detailed explanation on the evaluation metrics used in the challenge can be found on the ISBI 2012 challenge website <ref type="bibr" target="#b2">3</ref> , as well as in <ref type="bibr" target="#b0">[1]</ref>. The winners of the challenge were Chen et al. <ref type="bibr" target="#b3">[4]</ref>, although Beier et al. <ref type="bibr" target="#b1">[2]</ref> have the highest scores at the time of writing. These two works rely on complex post-processing pipeline. Our rotation equivariant prediction provides results comparable to other stateof-the-art methods only relying on the raw CNN softmax output <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22</ref>] (see <ref type="table">Table 6</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Covariance: car orientation estimation</head><p>Estimating car orientations from above-head imagery requires rotation covariant models. We use the dataset provided by the authors of <ref type="bibr" target="#b10">[11]</ref>, which is based on Google Map images. It is composed by 15 tiles, where cars' bounding boxes and corresponding orientations come from manual annotation. We implement our approach in similarly to <ref type="bibr" target="#b10">[11]</ref>. We crop a 48×48 square patch around every car, based on the bounding box center point. We then use these crops for both training and testing of the model. As in <ref type="bibr" target="#b10">[11]</ref>, we use the cars in the first 10 images (409 cars) for training and those in the last 5 images (209 cars) for testing. We did not use the cars whose center was nearer than 38 pixels from the image border, in order to avoid artifacts.  Model: We want to learn a covariant function with respect to rotations, since a rotation by ∆α</p><formula xml:id="formula_18">Type Size Input 48 × 48 RotConv 11 × 11, 3 filt. OP 38 × 38 × 4 × 2 RotConv 11 × 11 × 3 × 2, 6 filt. OP 28 × 28 × 6 × 2 RotConv 11 × 11 × 6 × 2, 3 filt. OP, 2 × 2 SP 9 × 9 × 3 × 2 RotConv fully 9 × 9 × 3 × 2, 1 filt. connected (FC1) 1 × 1 × 21 FC2, Hardcoded 1 × 1 × 21, 2 filt. Output 1 × 1 × 2</formula><p>• in the input image results in a change by ∆α</p><p>• in the predicted angle. In particular, we train on sine and cosine of α</p><p>• , since they are continuous with respect to ∆α</p><p>• . The network's architecture is illustrated in <ref type="table" target="#tab_6">Table 7</ref>. For the output we use a tanh non-linearity, followed by a normalization of the output vector to unit-norm. The first fully connected layer (FC1) is a RotConv block with a single filter (R = 21) not followed by by an Orientation Pooling, meaning that the subsequent feature vector has 21 dimensions instead of just one. We can expect this vector to undergo a circular shift when the input image is subject to a rotation. We hardcode the two mappings of the following layer (FC2) to [sin(360/R), sin(2 · 360/R), . . . sin(R · 360/R)] and [cos(360/R), cos(2 · 360/R), . . . cos(R · 360/R)]. This ensures that there will be no preferred orientations inherited from a biased training set. The weight decay and learning rate are 10 −2 and 5 · 10 −3 respectively, for the 80 epochs. All the filters were initialized from a normal distribution with zero mean and σ = 10 −3 . The final models correspond to the average of the weights of the last 30 epochs.</p><p>Results: <ref type="table" target="#tab_8">Table 8</ref> reports the average test error. The use of RotEqNet substantially improves the results, outperforming by more than 20% the previous state-of-the-art method <ref type="bibr" target="#b10">[11]</ref>. In <ref type="figure">Fig. 5</ref>, we show the error distribution in the test set for the hybrid model. Note how most samples, 82.7%, are predicted with less than 15</p><p>• of orientation error, while most of the contribution to the total error comes from the 6.7% of samples with errors larger than 150</p><p>• , in which the front of the car has been mistaken with the rear.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Avg. error ( • ) # params CNN <ref type="bibr" target="#b10">[11]</ref> 28.87 27k Warped-CNN <ref type="bibr" target="#b10">[11]</ref> 26   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Invariance 2: robustness in patch matching</head><p>Patch matching is widely used in many image processing and computer vision problems, such as registration, 3D reconstruction and inpainting. The aim is to find matching pairs of patches (e.g. the same features in the two different images of the same object). In this setting, the differences in orientation are often considered to be a nuisance. Although handcrafted features such as SIFT are still widely used as baselines to measure similarity, recent works have shown that learning ad-hoc features with siamese CNNs <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref> can perform substantially better.In the following, we apply RotEqNet to analyze how this problem can benefit from a tunable amount of rotation invariance.</p><p>Depending on the problem at hand, one might have a prior on how much rotation invariance is required. Although CNN-based descriptors are more robust to relative rotations between matching pairs than SIFT, they still tend to perform poorly for large angular differences <ref type="bibr" target="#b24">[25]</ref>.</p><p>To showcase how RotEqNet allows to tune the amount of rotation invariance, we trained a siamese network with three RotConv blocks, with 3, 6 and 32 filters of size 9×9 respectively, totaling 40k parameters. The last fully connected block provides 32 scalar features. We trained it on 20k samples from the Notredame dataset <ref type="bibr" target="#b27">[28]</ref> with a distance-based objective function <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b29">30]</ref>.</p><p>After training, the number of bins in the last Orientation Pooling layer can be modified, thus yielding multiple descriptors per sample. For instance, if the number of bins is set to 4, one 32-dimensional descriptor will be produced for each quadrant, thus resulting in a 128-dimensional descriptor for the patch. We analyze robustness in patch matching by increasing the rotation of the patches and the number of bins, and compare our results to those obtained by SIFT and the features from a pre-trained VGG network <ref type="bibr" target="#b24">[25]</ref>. We use patches extracted from an urban photograph that are then paired to a shifted (by one pixel) and rotated version of itself. Results in <ref type="figure">Fig.7</ref> show that RotEqNet with a single bin is much more robust to rotations than VGG and SIFT descriptors, even when the main orientation assignment is used. As a trade-off, it performs slightly worse for small rotations. However, by increasing the number of bins we can invert this tendency and improve the matching accuracy for small angles (and trade off accuracy on large rotations): using two bins (i.e. a 64-dimensional descriptor), we clearly outperform the baselines on small angles and still have 60% of correct matches for rotations around 45</p><p>• (compared to less than 10% for SIFT and VGG).  <ref type="figure">Figure 7</ref>. Matching accuracy vs. rotation applied to one of the elements in each matching pair in a synthetic dataset. RotEqNet allows to trade-off some accuracy at small rotations for more robustness by changing the number of bins in the last Orientation Pooling layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Limitations and future work</head><p>Forcing the Orientation Pooling block to choose the most activating orientation could result in exacerbating noise when there is no main orientation on either the input or the filter. This is because the arbitrarily chosen orientation can have a big impact on the output, and how it will interact with filters in the following layer, but no meaning. This problem is amplified by the use of scalar products between the vector elements of the filter and its input, which assumes that the orientation of these vectors is relevant. This issue could be improved by using a custom similarity metric between vector elements such that symmetries in the filters or the input are taken into account.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We have presented a new way of hard-coding into CNNs predefined behaviors with respect to rotations. This is achieved by applying each filter at different orientations and extracting a vector field feature map, encoding the maximum activation in terms of magnitude and angle.</p><p>Experiments on classification, segmentation, orientation estimation and matching show the suitability of this approach for solving a wide variety of problems that are inherently rotation equivariant, invariant or covariant. These results suggest that taking into account only the dominant orientations is sufficient to tackle successfully a range of problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Desirable behaviors with respect to rotation of the inputs: (left) equivariance in segmentation; (center) invariance in classification; (right) covariance in absolute orientation estimation. g45 is an operator that rotates the input image by 45 • .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Example of the first two layers of RotEqNet. Each layer learns only three canonical filters (red squares) and replicates them across six orientations. The output of the first block are three vector field maps, which are further convolved by vector field filters in the second block (OP: orientation pooling; SP: spatial pooling).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Performance of RotEqNet and TI-Pooling on MNIST-rot with respect to the number of parameters.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Example validation image (#30) of the ISBI 2012 challenge. (a) Image (190 × 130 pixels). (b) Membrane ground truth. (c) The pre-processed 3-class ground truth: black is nonmembrane, yellow is membrane center, red is membrane border and blue is non-class. (d) Probability map produced by RotEqNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>of the errors in the test set (top). Exam- ples (bottom) of correctly and incorrectly identified orientations. Ground truth arrows in green (thin) and predictions in red (thick).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Error (left y-axis, blue) vs computational time (right yaxis, red) for the number of filters considered. The vertical dashed line denotes R = 17.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Error rate on the MNIST-rot dataset trained on the train- val subset.</figDesc><table>Train on MNIST 

Train on MNIST-rot 
No augm. Augm. No augm. Augm. 
CNN 
57% 
2.3% 
4.9% 
2.2% 
RotEqNet 
20% 
1.1% 
1.4% 
1.1% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Network architecture used with ISBI 2012 challenge data. Layer parameters are in white and variables are shaded in gray.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>.2.</figDesc><table>No augm. 

Augm. 
CNN 
0.9232 
0.9572 
RotEqNet 
0.9726 
0.9790 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Architecture of the car orientation estimation net- work.Parameters are in white and variables are shaded in gray.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 8 .</head><label>8</label><figDesc>Mean error in the prediction of car orientations.to R: In order to study the sensitivity of RotEqNet to the number of angles R, we trained the model using R = 21 and tested it for different values (see Fig- ure 6). We observed relatively small changes in the test error for R &gt; 17.</figDesc><table>Sensitivity 10 

20 

30 

40 

1 

2 

3 

time [s] 
error [%] 

# rotations [R] 

1 
5 
10 
15 
20 
25 
30 
36 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Will be made available at http://github.com/di-marcos/RotEqNet 2 http://www.vlfeat.org/matconvnet</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">http://brainiac2.mit.edu/isbi challenge/</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Crowdsourcing the creation of image segmentation algorithms for connectomics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Arganda-Carreras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Turaga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">R</forename><surname>Berger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cireşan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Giusti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Gambardella</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dwivedi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Frontiers in Neuroanatomy</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">An efficient fusion move algorithm for the minimum cost lifted multicut problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Köthe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">A</forename><surname>Hamprecht</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the European Conf. on Computer Vision (ECCV)</title>
		<meeting>the European Conf. on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="715" to="730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">An integrated micro-and macroarchitectural analysis of the drosophila brain by computer-assisted serial section electron microscopy</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cardona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saalfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Preibisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pulokas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tomancak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hartenstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PLoS Biol</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page">1000502</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep contextual networks for neuronal structure segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><forename type="middle">J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Heng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Conf. on Artificial Intelligence (AAAI)</title>
		<meeting>the Conf. on Artificial Intelligence (AAAI)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">RIFD-CNN: RotationInvariant and Fisher Discriminative convolutional neural networks for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2884" to="2893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Group equivariant convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Machine Learning</title>
		<meeting>the International Conf. on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2990" to="2999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08498</idno>
		<title level="m">Steerable CNNs</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The importance of skip connections in biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Drozdzal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vorontsov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chartrand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kadoury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA)</title>
		<meeting>eeding of the Deep Learning and Data Labeling for Medical Applications Workshop (DLMIA)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="179" to="187" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deep models for brain em image segmentation: novel insights and improved performance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fakhry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">15</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep symmetry networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2537" to="2545" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Warped convolutions: Efficient invariance to spatial transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04382</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">When face recognition meets with deep learning: an evaluation of convolutional neural networks for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kittler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Christmas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hospedales</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition Workshops (CVPRW)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="142" to="150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Transformation equivariant boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Kivinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Artificial Neural Networks (ICANN)</title>
		<meeting>the International Conf. on Artificial Neural Networks (ICANN)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">TIpooling: transformation-invariant pooling for feature learning in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Savinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Buhmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An empirical evaluation of deep architectures on problems with many factors of variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Machine Learning (ICML)</title>
		<meeting>the International Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="473" to="480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Handwritten zip code recognition with multilayer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Le Cun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Matan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jacket</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Baird</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Intl. Conf. on Pattern Recognition (ICPR)</title>
		<meeting>eeding of the IEEE Intl. Conf. on Pattern Recognition (ICPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1990" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="35" to="40" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Understanding image representations by measuring their equivariance and equivalence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="991" to="999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning rotation invariant convolutional filters for texture classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Marcos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Volpi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tuia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Pattern Recognition</title>
		<meeting>the International Conf. on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Tiled convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">W</forename><surname>Koh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</title>
		<meeting>the International Conf. on Medical Image Computing and Computer-Assisted Intervention (MICCAI)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Rotation, scaling and deformation invariant scattering for texture discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mallat</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1233" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Document Analysis and Recognition (ICDAR)</title>
		<meeting>the International Conf. on Document Analysis and Recognition (ICDAR)</meeting>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning invariant representations with local transformations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conf. on Machine Learning (ICML)</title>
		<meeting>the International Conf. on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the ACM Intl. Conf. on Multimedia</title>
		<meeting>eeding of the ACM Intl. Conf. on Multimedia</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning local image descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Winder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Worrall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Garbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Turmukhambetov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.04642</idno>
		<title level="m">Harmonic networks: Deep translation and rotation equivariance</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>eeding of the IEEE Conf. on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4353" to="4361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.01833</idno>
		<title level="m">Oriented response networks</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
