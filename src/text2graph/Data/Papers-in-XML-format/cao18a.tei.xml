<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Adversarial Learning with Local Coordinate Coding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiezhang</forename><surname>Cao</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yong</forename><surname>Guo</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingyao</forename><surname>Wu</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingkui</forename><surname>Tan</surname></persName>
						</author>
						<title level="a" type="main">Adversarial Learning with Local Coordinate Coding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative adversarial networks (GANs) aim to generate realistic data from some prior distribution (e.g., Gaussian noises). However, such prior distribution is often independent of real data and thus may lose semantic information (e.g., geometric structure or content in images) of data. In practice, the semantic information might be represented by some latent distribution learned from data, which, however, is hard to be used for sampling in GANs. In this paper, rather than sampling from the pre-defined prior distribution, we propose a Local Coordinate Coding (LCC) based sampling method to improve GANs. We derive a generalization bound for LCC based GANs and prove that a small dimensional input is sufficient to achieve good generalization performance. Extensive experiments on various real-world datasets demonstrate the effectiveness of the proposed method.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Generative Adversarial Networks (GANs) <ref type="bibr" target="#b4">(Goodfellow et al., 2014)</ref> have been successfully applied to many tasks, such as video prediction <ref type="bibr" target="#b18">(Ranzato et al., 2014;</ref><ref type="bibr" target="#b15">Mathieu et al., 2016)</ref>, image translation <ref type="bibr" target="#b7">(Isola et al., 2017;</ref><ref type="bibr" target="#b9">Kim et al., 2017)</ref>, etc. Specifically, GANs learn to generate data by playing a two-player game: a generator tries to produce samples from a simple latent distribution, and a discriminator distinguishes between the generated data and real data.</p><p>Recently, many attempts have been made to improve GANs <ref type="bibr" target="#b17">(Radford et al., 2015;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b8">Karras et al., 2018)</ref>. However, existing studies suffer from two limitations. First, many studies employ some simple prior distri-bution, such as Gaussian distributions <ref type="bibr" target="#b4">(Goodfellow et al., 2014)</ref> and uniform distributions <ref type="bibr" target="#b17">(Radford et al., 2015)</ref>. However, such pre-defined prior distributions are often independent of the data distributions and these methods may produce images with distorted structures without sufficient semantic information. Although such semantic information can be represented by some latent distribution, e.g., extracting embeddings using an AutoEncoder <ref type="bibr" target="#b6">(Hinton &amp; Salakhutdinov, 2006)</ref>, how to conduct sampling from this distribution still remains an open question in GANs.</p><p>Second, the generalization ability of GANs w.r.t. the dimension of the latent distribution is unknown. In practice, we observe that the performance of GANs is sensitive to the dimension of the latent distribution. Unfortunately, it is difficult to analyze the dimensionality of the latent distribution, since the specified prior distribution is independent of the real data. Therefore, it is very necessary and important to explore a new method to study the dimension of latent distribution and its impacts on the generalization ability.</p><p>In this paper, relying on the manifold assumption on images <ref type="bibr" target="#b21">(Tenenbaum et al., 2000;</ref><ref type="bibr" target="#b19">Roweis &amp; Saul, 2000)</ref>, we propose a novel generative model using Local Coordinate Coding (LCC) <ref type="bibr" target="#b24">(Yu et al., 2009</ref>) to improve GANs in generating perceptually convincing images. First, we employ an AutoEncoder to learn embeddings lying on the latent manifold to capture the semantic information in data. Then, we develop a new LCC sampling method for training GANs by exploiting the local information on the latent manifold.</p><p>The contributions of this paper are summarized as follows.</p><p>First, we propose an LCC sampling method for GANs to capture the local information of data. With the LCC sampling, the proposed scheme, called LCC-GANs, is able to sample meaningful points from the latent manifold to generate new data.</p><p>Second, we study the generalization bound of LCC-GANs based on the Rademacher complexity of the discriminator set and the error w.r.t. the intrinsic dimensionality of the manifold. In particular, we prove that a small dimensional input is sufficient to achieve good generalization performance. Extensive experiments on real-world datasets demonstrate the superiority of the proposed method over several state-of-the-arts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Studies</head><p>Recently, Generative Adversarial Networks have shown promising performance for generating images, such as DCGANs <ref type="bibr" target="#b17">(Radford et al., 2015)</ref>, WGANs <ref type="bibr" target="#b0">(Arjovsky et al., 2017)</ref> and Progressive GANs <ref type="bibr" target="#b8">(Karras et al., 2018)</ref>. Most existing generative models seek to learn from some simple prior distribution, such as Gaussian distributions and uniform distributions, to generate samples <ref type="bibr" target="#b4">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b0">Arjovsky et al., 2017;</ref><ref type="bibr" target="#b17">Radford et al., 2015;</ref><ref type="bibr" target="#b8">Karras et al., 2018)</ref>. However, such prior distributions are independent of the data distributions, which may lose semantic information and lead to difficulties in analyzing the dimension of latent space.</p><p>Besides, some generative models do sampling via some learned posterior distribution. For example, Variational AutoEncoder (VAE) <ref type="bibr" target="#b11">(Kingma &amp; Welling, 2014)</ref>, Wasserstein AutoEncoder (WAE) <ref type="bibr" target="#b22">(Tolstikhin et al., 2018)</ref> and Adversarial AutoEncoder (AAE) <ref type="bibr" target="#b14">(Makhzani et al., 2015)</ref> enforce the posterior distribution to match the prior distribution. However, it is difficult for these methods to conduct sampling directly on the posterior distribution. Moreover, although these methods help to make inference, overly simplified distributions would also lose semantic information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Local Coordinate Coding</head><p>We first introduce some definitions about local coordinate coding which will be used to develop our proposed method.</p><p>Definition 1 (Lipschitz Smoothness <ref type="bibr" target="#b24">(Yu et al., 2009)</ref> <ref type="bibr" target="#b24">(Yu et al., 2009)</ref>) A coordinate coding is a pair (γ, C), where C ⊂ R d is a set of anchor points (bases), and γ is a map of</p><formula xml:id="formula_0">) A function f θ (x) in R d is (L x , L f )-Lipschitz smooth if f (x ) − f (x) 2 ≤ L x x − x 2 and f (x ) − f (x) − ∇f (x) T (x − x) 2 ≤ L f x − x 2 2 , where L x , L f &gt; 0. Definition 2 (Coordinate Coding</formula><formula xml:id="formula_1">x ∈ R d to [γ v (x)] v∈C ∈ R |C| such that v γ v (x) = 1. Then, the physical approximation of x ∈ R d is r(x) = v∈C γ v (x)v.</formula><p>Definition 2 indicates that any point in R d can be represented by a linear combination of a set of anchor points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Latent Manifold</head><p>High dimensional data often lie on some low dimensional manifold <ref type="bibr" target="#b21">(Tenenbaum et al., 2000;</ref><ref type="bibr" target="#b19">Roweis &amp; Saul, 2000)</ref>. Based on this manifold assumption, we can learn a manifold M embedded in the latent space R d B by some manifold learning method, such as an AutoEncoder (AE) <ref type="bibr" target="#b6">(Hinton &amp; Salakhutdinov, 2006)</ref>, to capture the semantic information of data. Given N training data {x i } N i=1 , we can use an Encoder to extract the embeddings</p><formula xml:id="formula_2">{h i } N i=1</formula><p>, where h i = Encoder(x i ). Formally, the latent manifold can be defined as follows.</p><p>Definition 3 (Latent Manifold <ref type="bibr" target="#b24">(Yu et al., 2009)</ref></p><formula xml:id="formula_3">) A subset M embedded in the latent space R d B is called a smooth manifold with a intrinsic dimension d := d M , if there ex- ists a constant c M , such that given any h ∈ M, there are d bases v 1 (h), . . . , v d (h) ∈ R d B so that ∀ h ∈ M : inf γ∈R d h − h − d j=1 γ j v j (h) 2 ≤ c M h − h 2 2 .</formula><p>where</p><formula xml:id="formula_4">γ = [γ 1 , . . . , γ d ]</formula><p>T is the local coding of a latent point h using the corresponding bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Generative Adversarial Networks</head><p>We apply the neural network distance <ref type="bibr" target="#b1">(Arora et al., 2017)</ref> to measure the similarity between two distributions. Definition 4 (Neural Network Distance <ref type="bibr" target="#b1">(Arora et al., 2017)</ref>) Let F be a set of neural networks from R d to [0, 1] and φ be a concave measure function, then for D ∈ F, the neural network distance w.r.t. φ between two distributions µ and ν can be defined as</p><formula xml:id="formula_5">d F ,φ (µ, ν)= sup D∈F E x∼µ φ(D(x)) + E x∼ν φ( D(x)) −φ c ,</formula><p>where φ c =2φ( Objective function of general GANs. Given a Generator G u and a Discriminator D v parameterized by u ∈ U and v ∈ V, where U and V are parameter spaces. Let D real be the real distribution of training samples x ∈ R d and D Gu be the distribution generated by G u . The objective function of GANs can be defined as:</p><formula xml:id="formula_6">min u∈U max v∈V E x∼D real [φ(D v (x))] + E x∼D Gu [φ(1 − D v (x))] ,</formula><p>where φ : [0, 1] → R is any monotone function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Generator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LCC Sampling</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Discriminator</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LCC Sampling</head><p>Step 2:</p><p>Construct a M-dimensional vector with .</p><p>Step 1:</p><formula xml:id="formula_7">Find -nearest neighbors ℬ = =1</formula><p>of an arbitrary basis .  <ref type="figure">Figure 2</ref>. The scheme of the proposed LCC-GANs. We use an AutoEncoder to learn the embeddings on the latent manifold from real data. Relying on LCC, we learn a set of bases such that the LCC sampling can be conducted. As a result, the proposed method is able to take the constructed LCC codings to generate new data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Adversarial Learning with LCC</head><p>In this section, we seek to improve GANs by exploiting LCC. The overall structure of the proposed method, called LCC-GANs, is illustrated in <ref type="figure">Figure 2</ref>.</p><p>As shown in <ref type="figure">Figure 2</ref>, instead of sampling from some predefined prior distribution, we seek to sample points from a learned latent manifold for training GANs. Specifically, we use an AutoEncoder (AE) to learn embeddings over a latent manifold of real data and then employ LCC to learn a set of bases to form local coordinate systems on the latent manifold. After that, we introduce LCC into GANs by approximating the generator using a linear function w.r.t. a set of codings (see Section 4.1). Relying on such approximation, we then propose an LCC based sampling method to exploit the local information of data on the latent manifold (see Section 4.3). The details of the proposed method are illustrated in following subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Generator Approximation Based on LCC</head><p>According to Definition 3, any point on the latent manifold can be approximated by a linear combination of a set of local bases. Inspired by this, if the bases are sufficiently localized, the generator of GANs can also be approximated by a linear function w.r.t. a set of codings.</p><formula xml:id="formula_8">Lemma 1 (Generator Approximation) Let (γ, C) be an arbitrary coordinate coding on R d B . Given a (L h , L G )- Lipschitz smooth generator G u (h), for all h ∈ R d B : G u v∈C γ v (h)v − v∈C γ v (h)G u (v) 2 ≤2L h h−r(h) 2 +L G v∈C |γ v (h)|· v−r(h) 2 2 ,<label>(1)</label></formula><p>where</p><formula xml:id="formula_9">r(h) = v∈C γ v (h)v.</formula><p>Given the local bases and a Lipschitz smooth generator, the generator w.r.t. the linear combination of the local bases can be approximated by the linear combination of the generator w.r.t. local bases. Since two close latent points often share the same local bases but with different weights (i.e., codings), we can change these weights for generator approximation. Therefore, the pieces of generated data can cover an entire manifold seamlessly (see <ref type="figure" target="#fig_0">Figure 1</ref>(b)).</p><p>Objective function of LCC. We minimize the right-hand term of the inequality in (1) to obtain a set of bases. Given a set of the latent points</p><formula xml:id="formula_10">{h i } N i=1</formula><p>, by assuming h ≈ r(h) <ref type="bibr" target="#b24">(Yu et al., 2009</ref>), we address the following problem:</p><formula xml:id="formula_11">min γ,C h 2L h h − r(h) 2 +L G v∈C |γ v (h)|· v − h 2 2 , s.t. v∈C γ v (h) = 1, ∀ h,<label>(2)</label></formula><p>where r(h) = v∈C γ v (h)v. In practice, we update γ and C by alternately optimizing a LASSO problem and a least-square regression problem, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Objective Function of LCC-GANs</head><p>After solving Problem (2), every latent point h ∈ R d B is close to its physical approximation r(h), i.e., h ≈ r(h), then the generator can be approximated by</p><formula xml:id="formula_12">G u (h) ≈ G u (r(h)) G w (γ(h)), h ∈ H,<label>(3)</label></formula><p>where</p><formula xml:id="formula_13">r(h) = Vγ(h), V = [v 1 , v 2 , . . . , v M ] and γ(h) = [γ 1 (h), γ 2 (h), . . . , γ M (h)] T with M = |C|.</formula><p>Here, H is the latent distribution and w ∈ W is the parameters of the generator w.r.t. u and fixed V learned from Problem (2).</p><p>Using the neural network distance, we consider the following objective function of LCC-GANs between the generated distribution and the empirical distribution:  <ref type="figure">Figure 3</ref>. The geometric views on LCC Sampling. By learning embeddings (i.e., black points) which lie on the latent manifold, we use LCC to learn a set of bases (i.e., gray points) to form a local coordinate system such that we can sample different latent points (i.e., coloured points) by LCC sampling. As a result, LCC-GANs can generate new data which have different attributes.</p><formula xml:id="formula_14">min Gw∈G d F ,φ D Gw(γ(h)) , D real , h ∈ H.<label>(4)</label></formula><p>To be more specific, Problem (4) can be rewritten as:</p><formula xml:id="formula_15">min w∈W max v∈V E x∼ D real φ(Dv(x)) + E h∼H φ Dv (Gw (γ(h))) ,</formula><p>where φ(·) is a monotone function, and</p><formula xml:id="formula_16">D v (·) = 1−D v (·).</formula><p>The detailed algorithm is shown in Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">LCC Sampling Method</head><p>To address Problem (4), one of the key issues is on how to conduct sampling from the learned latent manifold. Although the latent manifold can be learned by AutoEncoder, it is very hard to sample valid points on it to train GANs. To address this, we propose an LCC sampling method to capture the latent distribution on the learned latent manifold (see <ref type="figure">Figure 3)</ref>. The proposed sampling method contains the following two steps.</p><p>Step 1: Given a local coordinate system, we randomly select a latent point (specifically, it can be a basis), and then</p><formula xml:id="formula_17">find its d-nearest neighbors B = {v j } d j=1 . Step 2: We construct an M -dimensional vector γ(h) = [γ 1 (h), γ 2 (h), . . . , γ M (h)]</formula><p>T as the LCC coding for sampling. Here, each element of γ(h) is corresponding to the weight of the basis. To conduct local sampling, we construct the coding of the neighbors B as follows:</p><formula xml:id="formula_18">γ j (h) = z j , v j ∈ B 0 , v j / ∈ B ,</formula><p>where z j is the j-th element of z ∈ R d from the prior distribution p(z). Here, we set p(z) to be the standard Gaussian distribution N (0, I). Finally, we obtain a new latent point Vγ(h).</p><p>Based on Definition 3, the intrinsic dimensionality is determined by the number of bases in a local region. Thus, we turn the determination of intrinsic dimension into an easier problem of selecting sufficient number of local bases.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 LCC-GANs Training Method.</head><p>Initialize: Training data {xi} N i=1 ; a prior distribution p(z), where z ∈ R d ; minibatch size n. 1: Learn the latent manifold M using an AutoEncoder 2: Construct LCC bases {vi}</p><formula xml:id="formula_19">M i=1 on H by optimizing: minγ,C h 2L h h−r(h) 2 +LG v∈C |γv(h)|· v−h 2 2</formula><p>3: for number of training iterations do 4:</p><p>Do LCC Sampling to obtain a minibatch {γ(hi)} n i=1</p><p>5: Sample a minibatch {xi} n i=1 from the data distribution 6:</p><p>Update the discriminator by ascending the gradient: ∇v</p><formula xml:id="formula_20">1 n n i=1 φ(Dv(xi)) + φ((1 − Dv(Gw(γ(hi))))) 7:</formula><p>Do LCC Sampling to obtain a minibatch {γ(hi)} n i=1</p><p>8: Update the generator by descending the gradient: ∇w 1 n n i=1 φ(1 − Dv(Gw(γ(hi)))) 9: end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Theoretical Analysis</head><p>We first give some necessary notations to develop our theoretical analysis for LCC based GANs. Let {x i } N i=1 be a set of observed training samples drawn from the real distribution D real , and let D real denote the empirical distribution over</p><formula xml:id="formula_21">{x i } N i=1</formula><p>. Given a generator G u and a set of the latent</p><formula xml:id="formula_22">points {h i } r i=1 , {G u (h i )} r i=1</formula><p>denotes a set of r generated samples from the generated distribution D Gu , and D Gw is an empirical generated distribution. Motivated by <ref type="bibr" target="#b1">(Arora et al., 2017;</ref><ref type="bibr" target="#b25">Zhang et al., 2018)</ref>, we define the generalization of LCC-GANs as follows:</p><p>Definition 5 (Generalization) The neural network distance d F ,φ (·, ·) between distributions generalizes with N training samples and error , if for a learned distribution D Gu , the following holds with high probability,</p><formula xml:id="formula_23">d F ,φ D Gw , D real − inf G d F ,φ (D Gu , D real ) ≤ .</formula><p>In Definition 5, the generalization of GANs means that the population distance</p><formula xml:id="formula_24">d F ,φ (D Gu , D real ) is close to the distance d F ,φ ( D Gw , D real ).</formula><p>In theory, we hope to ob-  Theorem 2 Under the condition of Theorem 1, given an empirical distribution D real with N samples drawn from D real , the following holds with probability at least 1 − δ,</p><formula xml:id="formula_25">tain a small d F ,φ (D Gu , D real ). In practice, we can min- imize the empirical loss d F ,φ ( D Gw , D real ) to approximate d F ,φ ( D Gw , D real ). First,</formula><formula xml:id="formula_26">E H d F ,φ D G w (γ(h)) , D real ≤ inf G E H d F ,φ D Gu(h) , D real + (d M ), where (d M ) = L φ Q L h ,L G (γ, C) + 2∆,</formula><formula xml:id="formula_27">E H d F ,φ D G w , D real − inf G E H [d F ,φ (D Gu , D real )] ≤2R X (F) + 2∆ 2 N log( 1 δ ) + 2 (d M ),</formula><p>where R X (F) is the Rademacher complexity of F.</p><p>See supplementary materials for the proof.</p><p>Theorem 2 shows that the generalization error of LCCGANs can be bounded by Rademacher complexity of F and an error term (d M ). Specifically, the former term R X (F) implies that the set of discriminator should be smaller to have better generalization ability, and also be large enough to be able to identify the data distribution, which is consistent with <ref type="bibr" target="#b25">(Zhang et al., 2018)</ref>. The latter term (d M ) indicates that a small dimensional input is sufficient to achieve good generalization. In practice, every dataset has its own dimension of the latent manifold. Nevertheless, experiments show that the proposed method is able to generate perceptually convincing images with small dimensional inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We compare LCC-GANs with several state-of-the-arts, i.e., Vanilla GANs <ref type="bibr" target="#b4">(Goodfellow et al., 2014)</ref>, WGANs (Arjovsky et al., 2017) and Progressive GANs <ref type="bibr" target="#b8">(Karras et al., 2018)</ref>. Here, Vanilla GANs and Progressive GANs are used to implement our LCC-GANs. For all considered GAN methods, the inputs are sampled from a d-dimensional prior distribution, and we train the generative models to produce 64 × 64 images. All experiments are conducted on a single Nvidia Titan X GPU.</p><p>Implementation details. We implement LCC-GANs based on PyTorch. <ref type="bibr">1</ref> We follow the experimental settings in DCGANs <ref type="bibr" target="#b17">(Radford et al., 2015)</ref>. Specifically, for the optimization, we use Adam <ref type="bibr" target="#b10">(Kingma &amp; Ba, 2015)</ref> with a mini-batch size of 64 and a learning rate of 0.0002 to train the generator and the discriminator. We initialize the parameters of both the generator and the discriminator following the strategy in <ref type="bibr" target="#b5">(He et al., 2015)</ref>.</p><p>Datasets and evaluation metrics. To thoroughly evaluate the proposed method, we conduct experiments on a wide variety of benchmark datasets, including <ref type="bibr">MNIST (LeCun et al., 1998)</ref>, Oxford-102 <ref type="bibr" target="#b16">(Nilsback &amp; Zisserman, 2008)</ref>, LSUN <ref type="bibr" target="#b23">(Yu et al., 2015)</ref> and CelebA <ref type="bibr" target="#b13">(Liu et al., 2015)</ref>. For quantitative comparisons, we adopt the Inception Score (IS) <ref type="bibr" target="#b20">(Salimans et al., 2016)</ref> and Multi-Scale Structural Similarity (MS-SSIM) <ref type="bibr" target="#b8">(Karras et al., 2018)</ref> as the performance metrics, which are highly consistent with human evaluations. Inception Score measures both the single image quality and the diversity over a large number of samples (i.e., 50k). In general, a larger IS value corresponds to the better performance of the method, and a smaller MS-SSIM value corresponds to images with more diversity.</p><p>(a) Generated samples with d = 3. The yellow and red boxes denote similar generated digits "2" and "8", respectively.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Results on MNIST</head><p>In this experiment, we evaluate the performance of the proposed method on MNIST (LeCun et al., 1998), which contains handwritten digit images ranging from 0 to 9. In this small dataset, we adopt Vanilla GANs as the baseline to implement the proposed LCC-GANs. The visual comparisons are shown in <ref type="figure" target="#fig_5">Figure 4</ref>.</p><p>From <ref type="figure" target="#fig_5">Figure 4</ref>(a), given a very low dimensional input with d = 3, Vanilla GANs produce only few kinds of digits with almost the same shapes (see the yellow and red boxes in <ref type="figure" target="#fig_5">Figure 4</ref>(a)). In other words, Vanilla GANs produce images with very low diversity. In contrast, LCC-GANs with a small dimensional input d = 3 can produce digits with different styles and different orientations. Equipped with LCC, the proposed LCC-GANs effectively preserve the local information of data on the latent manifold and thus help the training of GANs.</p><p>In <ref type="figure" target="#fig_5">Figure 4</ref>(b), we increase the dimension of input to d = 5 and compare the proposed LCC-GANs with other stateof-the-art GAN methods. In this experiment, the baseline GAN methods often produce digits with obscure structure. Nevertheless, the proposed LCC-GANs significantly outperform the considered baseline methods and produce sharp images with high diversity. More critically, LCCGANs with d = 5 are able to achieve comparable or even better performance than their GAN counterparts with d = 100 (see the red box in <ref type="figure" target="#fig_5">Figure 4(b)</ref>). These results show the effectiveness of the proposed LCC-GANs when training a generative model with the local information of the latent manifold. Compared to the baseline methods, LCC-GANs only need a relatively low dimensional input to produce visually promising images. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Results on Oxford-102 Flowers</head><p>We further evaluate the proposed LCC-GANs on a larger dataset Oxford-102 which contains flower images of 102 categories. In this experiment, we adjust the input of generative models with different dimensions, i.e., d = {3, 5, 10, 30}, and adopt Vanilla GANs to implement the proposed LCC-GANs and investigate the effect of different input dimensions. The results are shown in <ref type="figure">Figure 5</ref>.</p><p>From <ref type="figure">Figure 5</ref>, we have the following observations. First, for Vanilla GANs, the performance highly depends on the input dimension. Given a small dimension, i.e., d = 3 or d = 5, Vanilla GANs often fail to produce meaningful flowers and obtain images with a blurring structure and distorted regions. In contrast, the proposed LCC-GANs are able to produce promising images with clear structure given an input with d = 5. With such a low dimensional input, LCC-GANs effectively capture the local information of the latent manifold and produce perceptually convincing images. Second, we further investigate the effect of input dimension. From <ref type="figure">Figure 5</ref>, the proposed LCC-GANs consistently outperform Vanilla GANs given the inputs of different dimensions.</p><p>Moreover, we compare the proposed LCC-GANs with several state-of-the-art GAN methods and report the results in <ref type="table" target="#tab_2">Table 1</ref>. From <ref type="table" target="#tab_2">Table 1</ref>, the proposed LCC-GANs with d = 10 significantly outperform the other baseline methods and achieve the best performance with a score of 2.71. More critically, LCC-GANs with d = 10 achieve even better performance than Vanilla GANs with d = 100, which require the input with much higher dimension.</p><p>Comparisons of different representation methods. On Oxford-102, we compare different representation methods and adopt Inception Score and MS-SSIM to evaluate the quality and diversity of the generated images, respectively. We adjust the input with different dimensions, i.e., d = {5, 10, 30, 100}, and adopt Vanilla GANs to implement LCC-GANs. The results are shown in <ref type="table" target="#tab_3">Table 2</ref>.  From <ref type="table" target="#tab_3">Table 2</ref>, LCC-GANs consistently outperform other methods with various d in both measures. These results show the effectiveness of the proposed LCC-GANs in producing perceptually promising images with higher quality and larger diversity than the considered baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results on LSUN</head><p>In this experiment, we evaluate the proposed LCC-GANs on LSUN which is a collection of natural images of indoor scenes. We train the generative models to produce images of different categories, including bedroom, classroom, conference room, etc. In this experiment, we also adopt Vanilla GANs as the baseline models to implement LCC-GANs. We show the visual comparison results in <ref type="figure" target="#fig_7">Figure 6</ref>.</p><p>From <ref type="figure" target="#fig_7">Figure 6</ref>, when we train the models using an input with a small dimension d = 10, Vanilla GANs often fail to generate clear and meaningful images. In contrast, LCCGANs significantly outperform their GAN counterparts and produce images with sharp structure and rich details. Moreover, when generating images of different scenes, LCC-GANs consistently outperform Vanilla GANs. Note that the scene images in LSUN are much more complex than the images of MNIST and Oxford-102. Therefore, training a generative model can be more difficult. However, with the help of LCC, the proposed LCC-GANs are  able to effectively capture the local common features and produce visually convincing images.</p><p>In this experiment, we also present the generated samples of Vanilla GANs with a high dimensional input d = 100. Compared to this method, LCC-GANs only require an input with d = 10 to produce even better images. In other words, this LCC sampled input effectively preserves the local information of real images on the latent manifold and thus helps the training of GANs. With the help of LCC sampling, most of the generated images show sharper structure and contain more meaningful details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Results on CelebA</head><p>In this experiment, we evaluate the proposed method on the large-scale dataset CelebA, which is composed of a set of celebrity faces. Here, Progressive GANs <ref type="bibr" target="#b8">(Karras et al., 2018</ref>) are adopted to implement LCC-GANs. We conduct comparisons and show the results in <ref type="figure" target="#fig_9">Figure 7</ref>.</p><p>Since face images often share a common face outline and only differ in detailed attributes, e.g., hair, eyes, mouth, skin features, it requires an input with a larger dimension to capture the local information. In this way, we adopt the input with a larger dimension for both Progressive GANs and the proposed LCC-GANs in the training. From <ref type="figure" target="#fig_9">Figure 7</ref>, the performance of Progressive GANs degrades severely given an input with a small dimension d = 30, compared CelebA to d = 100. However, with the help of LCC, the proposed LCC-GANs with the input of d = 30 are able to produce images of better quality than Progressive GANs with high dimensional inputs of d = 100. According to these results, LCC-GANs greatly benefit from the LCC sampling and make the training much easier than directly matching the standard Gaussian distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Demonstration of LCC Sampling</head><p>In this experiment, we investigate the effectiveness of the proposed LCC sampling method. To achieve this, we can simply fix a specific set of bases and only change the corresponding weights to produce images. Ideally, these images should be located in a local area of the latent manifold and share some common features.</p><p>We conduct experiments on LCC sampling and show the results in <ref type="table" target="#tab_4">Table 3</ref>. The second column of <ref type="table" target="#tab_4">Table 3</ref> shows the generated images sampled by LCC sampling method on different datasets. The last column is the real image with the largest similarity to the generated images. From <ref type="table" target="#tab_4">Table 3</ref>, LCC-GANs produce digits with sharp shapes and different orientations or styles (see the top row in <ref type="table" target="#tab_4">Table 3</ref>). Each generated image contains a digit "5" but with obvious individual differences. In other words, the proposed LCC sampling method is able to generate new data by effectively exploiting the local information on the latent manifold.</p><p>When synthesizing flowers and faces, we draw a similar conclusion that verifies the effectiveness of the proposed LCC sampling method. Specifically, LCC-GANs produce flowers with similar shapes but with different colors. Similarly, LCC-GANs also produce varying face images of promising quality which share some common features. These results demonstrate that the proposed LCCGANs generalize well to unseen data rather than simply memorizing the training samples. In this experiment, we adopt MS-SSIM as the evaluation measure and compare the proposed LCC-GANs with several GAN methods on four benchmark datasets. We use Vanilla GANs to implement LCC-GANs. To show the superiority of the proposed method, we set d = 30 for LCCGANs and d = 100 for the other baselines. Here, we can only report MS-SSIM because Inception Score is no longer a valid measure and may give misleading results on CelebA <ref type="bibr" target="#b2">(Barratt &amp; Sharma, 2018)</ref>. The quantitative results are shown in <ref type="table" target="#tab_5">Table 4</ref>.</p><p>From <ref type="table" target="#tab_5">Table 4</ref>, with a low dimensional input, the proposed method is able to produce images with larger or comparable diversity (smaller MS-SSIM score) than the considered baselines with high dimensional inputs on most datasets. These results show the effectiveness of the proposed LCCGANs in generating images with large diversity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>In this paper, we have proposed a novel generative model by exploiting the local information on the latent manifold of real data to improve GANs using Local Coordinate Coding (LCC). Unlike existing methods, based on a generator approximation, we have developed an LCC based sampling method to train GANs. In this way, we are able to conduct analysis on the generalization ability of GANs and theoretically prove that a small dimensional input will help to achieve good generalization. Extensive experiments on several benchmark datasets demonstrate the superiority of the proposed method over the state-of-the-art methods. Specifically, with the proposed LCC sampling, the proposed method outperforms the considered baselines by producing sharper images with higher diversity.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. A geometric view of Local Coordinate Coding. Given a set of local bases, if data lie on a manifold, a nonlinear function f (x) can be locally approximated by a linear function w.r.t. the coding. Given all bases, f (x) can be globally approximated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>1 2</head><label>1</label><figDesc>) is a constant with the given φ and D(x)=1−D(x). For simplicity, we omit the constant φ c .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>we have the following general- ization bound on D real to develop the generalization anal- ysis of LCC-GANs. Theorem 1 Suppose φ(·) is Lipschitz smooth: |φ (·)| ≤ L φ , and bounded in [−∆, ∆]. Given the coordinate coding (γ, C), an example set H in latent space and the empiri- cal distribution D real , if the generator is Lipschitz smooth, then the expected generalization error satisfies:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>and generative quality Q L h ,L G (γ, C) has an upper bound w.r.t. d M in Lemma 3 which is given in supplementary materials. See supplementary materials for the proof. Theorem 1 shows that the generalization bound for D real is related to the dimension of the latent manifold (i.e., d M ) rather than the dimension of the latent space (i.e., d B ). Based on Theorem 1 and the Rademacher complexity (Bartlett &amp; Mendelson, 2002), we then accomplish the gen- eralization bound on an unknown real distribution D real .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(b) Comparisons of different GANs with d = 5, where GANs with d = 100 are considered as the baseline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Performance comparisons of various GANs on MNIST.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>( a )</head><label>a</label><figDesc>Results of LCC-GANs with d = 10. (b) Results of Vanilla GANs with d = 10. (c) Results of Vanilla GANs with d = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Results of LCC-GANs with Vanilla GANs for different dimensions of the latent distribution on LSUN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head></head><label></label><figDesc>of LCC-GANs with d = 30. (b) Results of Progressive GANs with d = 30. (c) Results of Progressive GANs with d = 100.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Performance comparisons of LCC-GANs with Progressive GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Inception scores of various generative models on Oxford-102. For each method, we produce 50, 000 samples for testing.</figDesc><table>Samples 
Methods Vanilla GANs (d=10) WGANs (d=10) Progressive GANs (d=10) Vanilla GANs (d=100) LCC-GANs (d=10) 
Scores 
2.21 ± 0.03 
2.14 ± 0.02 
2.43 ± 0.05 
2.66 ± 0.03 
2.71 ± 0.03 

Figure 5. Results of LCC-GANs and Vanilla GANs on Oxford-
102. Top: Vanilla GANs. Bottom: LCC-GANs. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Inception-Score (IS) and MS-SSIM on Oxford-102.</figDesc><table>Methods 
d = 5 
d = 10 
d = 30 
d = 100 
IS 
SSIM 
IS 
SSIM 
IS 
SSIM 
IS 
SSIM 
Vanilla GANs 2.03 0.205 2.37 0.180 2.57 0.166 2.66 0.160 
VAE 
2.14 0.203 2.38 0.185 2.54 0.163 2.68 0.162 
Sparse Coding 2.44 0.197 2.63 0.179 2.68 0.157 2.72 0.153 
LCC Coding 
2.57 0.188 2.71 0.163 2.83 0.153 2.75 0.147 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Generated images from LCC sampling on MNIST, Oxford-102 and CelebA. The last column shows the most simi- lar images in training set to the generated samples on the left.</figDesc><table>Datasets 
Generated Samples 
Nearest Real data 

MNIST 

Oxford-102 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 .</head><label>4</label><figDesc>MS-SSIM on different datasets. Here, d = 100 is for all baselines and d = 30 for LCC-GANs.</figDesc><table>Methods 
MNIST Oxford-102 LSUN CelebA 
Vanilla GANs 
0.242 
0.160 
0.224 
0.337 
WGANs 
0.251 
0.157 
0.237 
0.324 
Progressive GANs 
0.239 
0.151 
0.213 
0.308 
LCC-GANs 
0.224 
0.153 
0.203 
0.305 

6.6. More Quantitative Results 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">PyTorch is from http://pytorch.org/.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (GANs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">A note on the inception score</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Barratt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sharma</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.01973</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Rademacher and gaussian complexities: Risk bounds and structural results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">L</forename><surname>Bartlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="463" to="482" />
			<date type="published" when="2002-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Reducing the dimensionality of data with neural networks. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">313</biblScope>
			<biblScope unit="page" from="504" to="507" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image-toimage translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kim</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3730" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Makhzani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frey</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05644</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">B. Adversarial autoencoders. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep Multi-scale Video Prediction beyond Mean Square Error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Automated flower classification over a large number of classes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-E</forename><surname>Nilsback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Indian Conference on Computer Vision, Graphics and Image Processing</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="722" to="729" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Video (language) Modeling: a Baseline for Generative Models of Natural Videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6604</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Nonlinear dimensionality reduction by locally linear embedding. science</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Improved techniques for training gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A global geometric framework for nonlinear dimensionality reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>De Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Langford</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2319" to="2323" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Wasserstein auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Olivier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schoelkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Construction of a large-scale image dataset using deep learning with humans in the loop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1506.03365</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Nonlinear learning using local coordinate coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2223" to="2231" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On the discrimination-generalization tradeoff in GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
