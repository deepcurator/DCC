<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dropout Inference in Bayesian Neural Networks with Alpha-divergences</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
						</author>
						<title level="a" type="main">Dropout Inference in Bayesian Neural Networks with Alpha-divergences</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>To obtain uncertainty estimates with real-world Bayesian deep learning models, practical inference approximations are needed. Dropout variational inference (VI) for example has been used for machine vision and medical applications, but VI can severely underestimates model uncertainty. Alpha-divergences are alternative divergences to VI's KL objective, which are able to avoid VI's uncertainty underestimation. But these are hard to use in practice: existing techniques can only use Gaussian approximating distributions, and require existing models to be changed radically, thus are of limited use for practitioners. We propose a re-parametrisation of the alpha-divergence objectives, deriving a simple inference technique which, together with dropout, can be easily implemented with existing models by simply changing the loss of the model. We demonstrate improved uncertainty estimates and accuracy compared to VI in dropout networks. We study our model's epistemic uncertainty far away from the data using adversarial images, showing that these can be distinguished from non-adversarial images by examining our model's uncertainty.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning models have been used to obtain state-ofthe-art results on many tasks <ref type="bibr" target="#b46">Szegedy et al., 2014;</ref><ref type="bibr" target="#b45">Sutskever et al., 2014;</ref><ref type="bibr" target="#b44">Sundermeyer et al., 2012;</ref><ref type="bibr" target="#b34">Mikolov et al., 2010;</ref><ref type="bibr" target="#b21">Kalchbrenner &amp; Blunsom, 2013)</ref>, and in many pipelines these models have replaced the more traditional Bayesian probabilistic models <ref type="bibr" target="#b42">(Sennrich et al., 2016)</ref>. But unlike deep learning models, Bayesian probabilistic models can capture parameter uncertainty and its induced effects over predictions, capturing the models' ignorance about the world, and able to convey their increased uncertainty on out-of-data examples. This 1 University of Cambridge, UK 2 The Alan Turing Institute, UK. Correspondence to: Yingzhen Li &lt;yl494@cam.ac.uk&gt;.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proceedings of the 34</head><p>th International Conference on Machine Learning, Sydney, <ref type="bibr">Australia, PMLR 70, 2017</ref><ref type="bibr">. Copyright 2017</ref> by the author(s). information can be used, for example, to identify when a vision model is given an adversarial image (studied below), or to tackle many problems in AI safety <ref type="bibr" target="#b1">(Amodei et al., 2016)</ref>. With model uncertainty at hand, applications as farreaching as safety in self-driving cars can be explored, using models which can propagate their uncertainty up the decision making pipeline <ref type="bibr" target="#b10">(Gal, 2016)</ref>. With deterministic deep learning models this invaluable uncertainty information is often lost.</p><p>Bayesian deep learning -an approach to combining Bayesian probability theory together with deep learningallows us to use state-of-the-art models and at the same time obtain model uncertainty <ref type="bibr" target="#b10">(Gal, 2016;</ref><ref type="bibr" target="#b11">Gal &amp; Ghahramani, 2016a)</ref>. Originating in the 90s <ref type="bibr" target="#b38">(Neal, 1995;</ref><ref type="bibr" target="#b32">MacKay, 1992;</ref><ref type="bibr" target="#b8">Denker &amp; LeCun, 1991)</ref>, Bayesian neural networks (BNNs) in particular have started gaining in popularity again <ref type="bibr" target="#b14">(Graves, 2011;</ref><ref type="bibr" target="#b4">Blundell et al., 2015;</ref><ref type="bibr">HernandezLobato &amp; Adams, 2015)</ref>. BNNs are standard neural networks (NNs) with prior probability distributions placed over their weights. Given observed data, inference is then performed to find what are the more likely and less likely weights to explain the data. But as easy it is to formulate BNNs, is as difficult to perform inference in them. Many approximations have been proposed over the years <ref type="bibr" target="#b8">(Denker &amp; LeCun, 1991;</ref><ref type="bibr" target="#b38">Neal, 1995;</ref><ref type="bibr" target="#b14">Graves, 2011;</ref><ref type="bibr" target="#b4">Blundell et al., 2015;</ref><ref type="bibr" target="#b16">Hernandez-Lobato &amp; Adams, 2015;</ref><ref type="bibr" target="#b17">Hernández-Lobato et al., 2016)</ref>, some more practical and some less practical. A practical approximation for inference in Bayesian neural networks should be able to scale well to large data and complex models (such as convolutional neural networks (CNNs) <ref type="bibr" target="#b41">(Rumelhart et al., 1985;</ref><ref type="bibr" target="#b28">LeCun et al., 1989)</ref>). Much more importantly perhaps, it would be impractical to change existing model architectures that have been well studied, and it is often impractical to work with complex and cumbersome techniques which are difficult to explain to non-experts. Many existing approaches to obtain model confidence often do not scale to complex models or large amounts of data, and require us to develop new models for existing tasks for which we already have well performing tools <ref type="bibr" target="#b10">(Gal, 2016)</ref>.</p><p>One possible solution for practical inference in BNNs is variational inference (VI) <ref type="bibr" target="#b20">(Jordan et al., 1999)</ref>, a ubiquitous technique for approximate inference. Dropout variational distributions in particular (a mixture of two Gaussians with small standard deviations, and with one component fixed at zero) can be used to obtain a practical inference technique <ref type="bibr" target="#b12">(Gal &amp; Ghahramani, 2016b)</ref>. These have been used for machine vision and medical applications <ref type="bibr" target="#b22">(Kendall &amp; Cipolla, 2016;</ref><ref type="bibr" target="#b23">Kendall et al., 2015;</ref><ref type="bibr" target="#b2">Angermueller &amp; Stegle, 2015;</ref><ref type="bibr" target="#b50">Yang et al., 2016)</ref>. Dropout variational inference can be implemented by adding dropout layers <ref type="bibr" target="#b43">Srivastava et al., 2014)</ref> before every weight layer in the NN model. Inference is then carried out by Monte Carlo (MC) integration over the variational distribution, in practice implemented by simulating stochastic forward passes through the model at test time (referred to as MC dropout). Although dropout VI is a practical technique for approximate inference, it also has some major limitations. Dropout VI can severely underestimate model uncertainty <ref type="bibr">(Gal, 2016, Section 3.3</ref>.2) -a property many VI methods share <ref type="bibr" target="#b47">(Turner &amp; Sahani, 2011)</ref>. This can lead to devastating results in applications that must rely on good uncertainty estimates such as AI safety applications.</p><p>Alternative objectives to VI's objective are therefore needed.</p><p>Black-box α-divergence minimisation <ref type="bibr" target="#b35">Minka, 2005</ref>) is a class of approximate inference methods extending on VI, approximating EP's energy function <ref type="bibr" target="#b36">(Minka, 2001)</ref> as well as the Hellinger distance <ref type="bibr" target="#b15">(Hellinger, 1909)</ref>. These were proposed as a solution to some of the difficulties encountered with VI. However, the main difficulty with α-divergences is that the divergences are hard to use in practice. Existing inference techniques only use Gaussian approximating distributions, with the density over the approximation having to be evaluated explicitly many times. The objective offers a limited intuitive interpretation which is difficult to explain to non-experts, and of limited use for engineers <ref type="bibr">(Gal, 2016, Section 2.2.2)</ref>. Perhaps more importantly, current α-divergence inference techniques require existing models and code-bases to be changed radically to perform inference in the Bayesian counterpart to these models. To implement a complex CNN structure with the inference and code of , for example, one would be required to re-implement many already-implemented software tools.</p><p>In this paper we propose a re-parametrisation of the induced α-divergence objectives, and by relying on some mild assumptions (justified below), derive a simple approximate inference technique which can easily be implemented with existing models. Further, we rely on the dropout approximate variational distribution and demonstrate how inference can be done in a practical way -requiring us to only change the loss of the NN, L(θ), and to perform multiple stochastic forward passes at training time. Precisely, given l(·, ·) some standard NN loss such as cross entropy or the Euclidean loss, and {f</p><formula xml:id="formula_0">ω k (x n )} K k=1</formula><p>a set of K stochastic dropout network outputs on input x n with randomly masked weights ω k , our proposed objective is:</p><formula xml:id="formula_1">L(θ) = − 1 α n log-sum-exp −αl(y n , f ω k (x n )) + L 2 (θ)</formula><p>with α a real number, θ the set of network weights to be optimised, and an L 2 regulariser over θ. By selecting α = 1 this objective directly optimises the per-point predictive log-likelihood, while picking α → 0 would focus on increasing the training accuracy, recovering VI.</p><p>Specific choices of α will result in improved uncertainty estimates (and accuracy) compared to VI in dropout BNNs, without slowing convergence time. We demonstrate this through a myriad of applications, including an assessment of fully connected NNs in regression and classification, and an assessment of Bayesian CNNs. Finally, we study the uncertainty estimates resulting from our approximate inference technique. We show that our models' uncertainty increases on adversarial images generated from the MNIST dataset, suggesting that these lie outside of the training data distribution. This in practice allows us to tell-apart such adversarial images from non-adversarial images by examining epistemic model uncertainty.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>We review background in Bayesian neural networks and approximate variational inference. In the next section we discuss α-divergences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Bayesian Neural Networks</head><p>Given training inputs X = {x 1 , . . . , x N } and their corresponding outputs Y = {y 1 , . . . , y N }, in parametric Bayesian regression we would like to infer a distribution over parameters ω of a function y = f ω (x) that could have generated the outputs. Following the Bayesian approach, to find parameters that could have generated our data, we put some prior distribution over the space of parameters p 0 (ω). This distribution captures our prior belief as to which parameters are likely to have generated our outputs before observing any data. We further need to define a probability distribution over the outputs given the inputs p(y|x, ω). For classification tasks we assume a softmax likelihood,</p><formula xml:id="formula_2">p y|x, ω = Softmax (f ω (x))</formula><p>or a Gaussian likelihood for regression. Given a dataset X, Y, we then look for the posterior distribution over the space of parameters: p(ω|X, Y). This distribution captures how likely the function parameters are, given our observed data. With it we can predict an output for a new input point x * by integrating</p><formula xml:id="formula_3">p(y * |x * , X, Y) = p(y * |x * , ω)p(ω|X, Y)dω. (1)</formula><p>One way to define a distribution over a parametric set of functions is to place a prior distribution over a neural net-</p><formula xml:id="formula_4">work's weights ω = {W i } L i=1</formula><p>, resulting in a Bayesian NN <ref type="bibr" target="#b32">(MacKay, 1992;</ref><ref type="bibr" target="#b38">Neal, 1995)</ref>. Given weight matrices W i and bias vectors b i for layer i, we often place standard matrix Gaussian prior distributions over the weight matrices, p 0 (W i ) = N (W i ; 0, I) and often assume a point estimate for the bias vectors for simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Approximate Variational Inference in Bayesian</head><p>Neural Networks</p><p>In approximate inference, we are interested in finding the distribution of weight matrices (parametrising our functions) that have generated our data. This is the posterior over the weights given our observables X, Y: p(ω|X, Y), which is not tractable in general. Existing approaches to approximate this posterior are through variational inference (as was done in <ref type="bibr" target="#b18">Hinton &amp; Van Camp (1993)</ref>; <ref type="bibr" target="#b3">Barber &amp; Bishop (1998)</ref>; <ref type="bibr" target="#b14">Graves (2011);</ref><ref type="bibr" target="#b4">Blundell et al. (2015)</ref>). We need to define an approximating variational distribution q θ (ω) (parametrised by variational parameters θ), and then minimise w.r.t. θ the KL divergence <ref type="bibr" target="#b26">(Kullback &amp; Leibler, 1951;</ref><ref type="bibr" target="#b25">Kullback, 1959)</ref> between the approximating distribution and the full posterior:</p><formula xml:id="formula_5">KL q θ (ω)||p(ω|X, Y) ∝ − q θ (ω) log p(Y|X, ω)dω + KL(q θ (ω)||p 0 (ω)) = − N i=1 q θ (ω) log p(y i |f ω (x i ))dω + KL(q θ (ω)||p 0 (ω)),</formula><p>where A ∝ B is slightly abused here to denote equality up to an additive constant (w.r.t. variational parameters θ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Dropout Approximate Inference</head><p>Given a (deterministic) neural network, stochastic regularisation techniques in the model (such as dropout <ref type="bibr" target="#b43">Srivastava et al., 2014)</ref>) can be interpreted as variational Bayesian approximations in a Bayesian NN with the same network structure <ref type="bibr" target="#b12">(Gal &amp; Ghahramani, 2016b)</ref>. This is because applying a stochastic regularisation technique is equivalent to multiplying the NN weight matrices M i by some random noise i (with a new noise realisation for each data point). The resulting stochastic weight matrices W i = i M i can be seen as draws from the approximate posterior over the BNN weights, replacing the deterministic NN's weight matrices M i . Our set of variational parameters is then the set of</p><formula xml:id="formula_6">matrices θ = {M i } L i=1</formula><p>. For example, dropout can be seen as an approximation to Bayesian NN inference with dropout approximating distributions, where the rows of the matrices W i distribute according to a mixture of two Gaussians with small variances and the mean of one of the Gaussians fixed at zero. The uncertainty in the weights induces prediction uncertainty by marginalising over the approximate posterior using Monte Carlo integration:</p><formula xml:id="formula_7">p(y = c|x, X, Y) = p(y = c|x, ω)p(ω|X, Y)dω ≈ p(y = c|x, ω)q θ (ω)dω ≈ 1 K K k=1 p(y = c|x, ω k ) with ω k ∼ q θ (ω)</formula><p>, where q θ (ω) is the Dropout distribution <ref type="bibr" target="#b10">(Gal, 2016)</ref>. Given its popularity, we concentrate on the dropout stochastic regularisation technique throughout the rest of the paper, although any other stochastic regularisation technique could be used instead (such as multiplicative Gaussian noise <ref type="bibr" target="#b43">(Srivastava et al., 2014)</ref> or dropConnect <ref type="bibr" target="#b49">(Wan et al., 2013)</ref>).</p><p>Dropout VI is an example of practical approximate inference, but it also underestimates model uncertainty <ref type="bibr">(Gal, 2016, Section 3.3.2)</ref>. This is because minimising the KL divergence between q(ω) and p(ω|X, Y) penalises q(ω) for placing probability mass where p(ω|X, Y) has no mass, but does not penalise q(ω) for not placing probability mass at locations where p(ω|X, Y) does have mass. We next discuss α-divergences as an alternative to the VI objective.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Black-box α-divergence minimisation</head><p>In this section we provide a brief review of the black box alpha (BB-α, Hernández-Lobato et al. <ref type="formula" target="#formula_13">(2016)</ref>) method upon which the main derivation in this paper is based. Consider approximating the following distribution:</p><formula xml:id="formula_8">p(ω) = 1 Z p 0 (ω) n f n (ω).</formula><p>In Bayesian neural networks context, these factors f n (ω) represent the likelihood terms p(y n |x n , ω), Z = p(Y|X), and the approximation target p(ω) is the exact posterior p(ω|X, Y). Popular methods of approximate inference include variational inference (VI) <ref type="bibr" target="#b20">(Jordan et al., 1999)</ref> and expectation propagation (EP) <ref type="bibr" target="#b36">(Minka, 2001)</ref>, where these two algorithms are special cases of power EP <ref type="bibr" target="#b37">(Minka, 2004</ref>) that minimises Amari's α-divergence <ref type="bibr" target="#b0">(Amari, 1985)</ref> D α [p||q] in a local way:</p><formula xml:id="formula_9">D α [p||q] = 1 α(1 − α) 1 − p(ω) α q(ω) 1−α dω .</formula><p>We provide details of α-divergences and local approximation methods in the appendix, and in the rest of the paper we consider three special cases in this rich family:</p><p>1. Exclusive KL divergence:</p><formula xml:id="formula_10">D 0 [p||q] = KL[q||p] = E q log q(ω) p(ω) ;</formula><p>2. Hellinger distance:</p><formula xml:id="formula_11">D 0.5 [p||q] = 4Hel 2 [q||p] = 2 p(ω) − q(ω) 2 dω;</formula><p>3. Inclusive KL divergence:</p><formula xml:id="formula_12">D 1 [p||q] = KL[p||q] = E p log p(ω) q(ω) .</formula><p>Since α = 0 is used in VI and α = 1.0 is used in EP, in later sections we will also refer to these alpha settings as the VI value, Hellinger value, and EP value, respectively.</p><p>Power-EP, though providing a generic variational framework, does not scale with big data. It maintains approximating factors attached to every likelihood term f n (ω), resulting in space complexity O(N ) for the posterior approximation which is clearly undesirable. The recently proposed stochastic EP <ref type="bibr" target="#b31">(Li et al., 2015)</ref> and BB-α (Hernández-Lobato et al., 2016) inference methods reduce this memory overhead to O(1) by sharing these approximating factors. Moreover, optimisation in BB-α is done by descending the so called BB-α energy function, where Monte Carlo (MC) methods and automatic differentiation are also deployed to allow fast prototyping.</p><p>BB-α has been successfully applied to Bayesian neural networks for regression, classification  and model-based reinforcement learning <ref type="bibr" target="#b9">(Depeweg et al., 2016)</ref>. They all found that using α = 0 often returns better approximations than the VI case. The reasons for the worse results of VI are two fold. From the perspective of inference, due to the zero-forcing behaviour of exclusive KL discussed before, VI often fits to a local mode of the exact posterior and is over-confident in prediction. On hyper-parameter learning point of view, as the variational lower-bound is used as a (biased) approximation to the maximum likelihood objective, the learned model could be biased towards over-simplified cases <ref type="bibr" target="#b47">(Turner &amp; Sahani, 2011)</ref>. These problems could potentially be addressed by using α-divergences. For example, inclusive KL encourages the coverage of the support set (referred as mass-covering), and when used in local divergence minimisation <ref type="bibr" target="#b35">(Minka, 2005)</ref>, it can fit an approximation to a mode of p(ω) with better estimates of uncertainty. Moreover the BB-α energy provides a better approximation to the marginal likelihood as well, meaning that the learned model will be less biased and thus fitting the data distribution better . Hellinger distance seems to provide a good balance between zero-forcing and masscovering, and empirically it has been found to achieve the best performance.</p><p>Given the success of α-divergence methods, it is a natural idea to extend these algorithms to other classes of approximations such as dropout. However this task is non-trivial. First, the original formulation of BB-α energy is an ad hoc adaptation of power-EP energy (see appendix), which applies to exponential family q distributions only. Second, the energy function offers a limited intuitive interpretation to non-experts, thus of limited use for practitioners. Third and most importantly, a naive implementation of BB-α using dropout would bring in a prohibitive computational burden. To see this, we first review the BB-α energy function in the general case  given α = 0:</p><formula xml:id="formula_13">L α (q) = − 1 α n log E q f n (ω)p 0 (ω) 1 N q(ω) 1 N α .<label>(2)</label></formula><p>One could verify that this is the same energy function as presented in  by considering q an exponential family distribution. In practice (2) might be intractable, hence an MC approximation is introduced:</p><formula xml:id="formula_14">L MC α (q) = − 1 α n log 1 K k f n ( ω k )p 0 ( ω k ) 1 N q( ω k ) 1 N α (3) with ω k ∼ q(ω)</formula><p>. This is a biased approximation as the expectation in (2) is computed before taking the logarithm. But empirically <ref type="bibr" target="#b17">Hernández-Lobato et al. (2016)</ref> showed that the bias introduced by the MC approximation is often dominated by the variance of the samples, meaning that the effect of the bias is negligible. When α → 0 it returns the variational free energy (the VI objective) The original paper (Hernández-Lobato et al., 2016) proposed a naive implementation which directly evaluates the MC estimation (3) with samples ω k ∼ q(ω). However as discussed before, dropout implicitly samples different masked weight matrices ω ∼ q for different data points. This indicates that the naive approach, when applied to dropout approximation, would gather all these samples for all M datapoints in a mini-batch (i.e. M K sets of neural network weight matrices in total), which brings prohibitive cost if the network is wide and deep. Interestingly, the minimisation of the variational free energy (α = 0) with the dropout approximation can be computed very efficiently. The main reason for this success is due to the additive structure of the variational free energy: no evaluation of q density is required if the "regulariser" KL[q||p 0 ] can be computed/approximated efficiently. In the following section we propose an improved version of BB-α energy to allow applications with dropout and other flexible approximation structures.</p><formula xml:id="formula_15">L 0 (q) = L VFE (q) = KL[q||p 0 ] − n E q [log f n (ω)] ,<label>(</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">A New Reparameterisation of BB-α Energy</head><p>We propose a reparamterisation of the BB-α energy to reduce the computational overhead. First we denoteq(ω) as a free-form "cavity distribution" (see appendix), and write the approximate posterior q as</p><formula xml:id="formula_16">q(ω) = 1 Z qq (ω) q(ω) p 0 (ω) α N −α ,<label>(5)</label></formula><p>where we assume Z q &lt; +∞ is the normalising constant to ensure q a valid distribution. When α/N → 0, the unnormalised density in (5) converges toq(ω) for every ω, and Z q → 1 by the assumption of Z q &lt; +∞ (Van Erven &amp; Harremoës, 2014). Hence q →q when α/N → 0, and this happens for example when we choose α → 0, or N → +∞ as well as when α grows sub-linearly to N . Now we rewrite the BB-alpha energy in terms ofq:</p><formula xml:id="formula_17">L α (q) = − 1 α n log 1 Z qq (ω) q(ω) p 0 (ω) α N −α 1− α N p 0 (ω) α N f n (ω) α dω = − 1 α n q(ω)f n (ω) α − 1 − α N log Z q = N α 1 − α N log q(ω) q(ω) p 0 (ω) α N −α dω − 1 α n log Eq [f n (ω) α ] = R β [q||p 0 ] − 1 α n log Eq [f n (ω) α ] , β = N N − α , where R β [q||p 0 ] represents the Rényi divergence (Rényi (1961), see appendix) of order β. Furthermore, provided R β [q||p 0 ] &lt; +∞ (which holds when assuming Z q &lt; +∞), we have R β [q||p 0 ] → KL[q||p 0 ] = KL[q||p 0 ] as α N → 0.</formula><p>This means that for a constant α that scales sublinearly with N , in large data settings we can further approximate the BB-α energy as</p><formula xml:id="formula_18">L α (q) ≈L α (q) = KL[q||p 0 ] − 1 α n log E q [f n (ω) α ] .</formula><p>Note that here we also use the fact that now q ≈q. Critically, the proposed reparameterisation is continuous in α, and by taking α → 0 the variational free-energy (4) is recovered.</p><p>Given a loss function l(·, ·), e.g. l 2 loss in regression or cross entropy in classification, we can define the (un-</p><formula xml:id="formula_19">normalised) likelihood term f n (ω) ∝ p(y n |x n , ω) ∝ exp[−l(y n , f ω (x n ))</formula><p>], e.g. see <ref type="bibr" target="#b29">(LeCun et al., 2006)</ref> 1 . Swapping f n (ω) for this last expression, and approximating the expectation over q using Monte Carlo sampling, we obtain our proposed minimisation objective:</p><formula xml:id="formula_20">L MC α (q) = KL[q||p 0 ] + const (6) − 1 α n log-sum-exp[−αl(y n , f ω k (x n ))]</formula><p>with log-sum-exp being the log-sum-exp operator over K samples from the approximate posterior ω k ∼ q(ω). This objective function also approximates the marginal likelihood. Therefore, compared to the original formulation (2), the improved version (6) is considerably simpler (both to implement and to understand), has a similar form to standard objective functions used in deep learning research, yet remains an approximate Bayesian inference algorithm.</p><p>To gain some intuitive understanding of this objective, we observe what it reduces to for different α and K settings. By selecting α = 1 the per-point predictive log-likelihood log E q [p(y n |x n , ω)] is directly optimised. On the other hand, picking the VI value (α → 0) would focus on increasing the training accuracy E q [log p(y n |x n , ω)]. The Hellinger value could be used to achieve a balance between reducing training error and improving predictive likelihood, which has been found to be desirable <ref type="bibr" target="#b9">Depeweg et al., 2016)</ref>. Lastly, for K = 1 the log-sum-exp disappears, the α's cancel out, and the original (stochastic) VI objective is recovered.</p><p>In summary, our proposal modifies the loss function by multiplying it by α and then performing log-sum-exp with a sum over multiple stochastic forward passes sampled from the BNN approximate posterior. The remaining KLdivergence term (between q and the prior p) can often be approximated. It can be viewed as a regulariser added to the objective function, and reduces to L 2 -norm regulariser for certain popular q choices <ref type="bibr" target="#b10">(Gal, 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Dropout BB-α</head><p>We now provide a concrete example where the approximate distribution is defined by dropout. With dropout VI, MC samples are used to approximate the expectation w.r.t. q, which in practice is implemented as performing stochastic forward passes through the dropout network -i.e. given an input x, the input is fed through the network and a new dropout mask is sampled and applied at each dropout layer. This gives a stochastic output -a sample from the dropout network on the input x. A similar approximation is used in our case as well, where to implement the MC sampling in eq. <ref type="formula" target="#formula_15">(6)</ref> we perform multiple stochastic forward passes through the network.</p><p>Recall the neural network f ω (x) is parameterised by the variable ω. In classification, cross entropy is often used as the loss function l(y, f ω (x)) = −y T log p ω (x), where the label y n is a one-hot binary vector, and the network output p ω (x n ) = Softmax(f ω (x n )) encodes the probability vector of class assignments. Applying the re-formulated BB-α energy (6) with a Bayesian equivalent of the network, we arrive at the objective functioñ</p><formula xml:id="formula_21">L MC α (q) = i p i ||M i || 2 2 − 1 α n y T n log 1 K k (p ω k (x n )) α = 1 α n l y n , 1 K k p ω k (x n ) α + i L 2 (M i ) with {p ω k (x n )} K k=1</formula><p>being K stochastic network outputs on input x n , p i equals to one minus the dropout rate of the ith layer, and the L 2 regularization terms coming from an approximation to the KL-divergence <ref type="bibr" target="#b10">(Gal, 2016)</ref>. I.e. we raise network probability outputs to the power α and average them as an input to the standard cross entropy loss. Taking α = 1 can be viewed as training the neural network with an adjusted "power" loss, regularized by an L 2 norm. Implementing this induced loss with Keras <ref type="bibr" target="#b7">(Chollet, 2015)</ref> is as simple as a few lines of Python. A code snippet is given in <ref type="figure">Figure 1</ref>, with more details in the appendix.</p><p>In regression problems, the loss function is defined as l(y, f ω (x)) = τ 2 ||y − f ω (x)|| 2 2 and the likelihood term can be interpreted as y ∼ N (y; f ω (x), τ −1 I). Plugging this into the energy function returns the following objectivẽ</p><formula xml:id="formula_22">L MC α (q) = − 1 α n log-sum-exp − ατ 2 ||y n − f ω k (x n )|| 2 2 + N D 2 log τ + i p i ||M i || 2 2 ,<label>(7)</label></formula><p>with {f</p><formula xml:id="formula_23">ω k (x n )} K k=1</formula><p>being K stochastic forward passes on input x n . Again, this is reminiscent of the l 2 objective in standard deep learning, and can be implemented by simply passing the input through the dropout network multiple times, collecting the stochastic outputs, and feeding the set of outputs through our new BB-alpha loss function.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We test the reparameterised BB-α on Bayesian NNs with the dropout approximation. We assess the proposed inference in regression and classification tasks on standard benchmarking datasets, comparing different values of α. This last experiment leads us to propose a technique that could be used to identify adversarial image attacks. In the appendix we further provide a study of run time trade-off. def softmax_cross_ent_with_mc_logits(alpha):</p><p>def loss(y_true, mc_logits): # mc_logits: MC samples of shape MxKxD mc_log_softmax = mc_logits \ -K.max(mc_logits, axis=2, keepdims=True) mc_log_softmax = mc_log_softmax -\ logsumexp(mc_log_softmax, 2) mc_ll = K.sum(y_true * mc_log_softmax,-1) return -1./alpha * (logsumexp(alpha * \ mc_ll, 1) + K.log(1.0 / K_mc)) return loss <ref type="figure">Figure 1</ref>. Code snippet for our induced classification loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Regression</head><p>The first experiment considers Bayesian neural network regression with approximate posterior induced by dropout. We use benchmark UCI datasets 2 that have been tested in related literature. The model is a single-layer neural network with 50 ReLU units for all datasets except for Protein and Year, which use 100 units. We consider α ∈ {0.0, 0.5, 1.0} in order to examine the effect of masscovering/zero-forcing behaviour in dropout. MC approximation with K = 10 samples is also deployed to compute the energy function. Other initialisation settings are largely taken from .</p><p>We summarise the test negative log-likelihood (LL) and RMSE with standard error (across different random splits, the lower the better) for selected datasets in <ref type="figure" target="#fig_1">Figure 2</ref> and 3, respectively. The full results are provided in the appendix. Although optimal α may vary for different datasets, using non-VI values has significantly improved the test-LL performances, while remaining comparable in test error metric. In particular, α = 0.5 produced overall good results for both test LL and RMSE, which is consistent with previous findings. We also compare with a BNN with a Gaussian approximation (VI-G) , a BNN with HMC, and a sparse Gaussian process model with 50 inducing points . In test-LL metric our best dropout model out-performs the Gaussian approximation method on almost all datasets, and for some datasets is on par with HMC which is the current gold standard for Bayesian neural works, and with the GP model that is known to be superior in regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Classification</head><p>We further experiment with a classification task, comparing the accuracy of the various α values on the MNIST benchmark <ref type="bibr" target="#b27">(LeCun &amp; Cortes, 1998)</ref>. We assessed a fully connect NN with 2 hidden layers and 100 units in each layer. We used dropout probability 0.5 and α ∈ {0, 0.5, 1}. Again, we use K = 10 samples at training time for all α values,  and K test = 100 samples at test time. We use weight decay 10 −6 , which is equivalent to prior lengthscale l 2 = 0.1 <ref type="bibr" target="#b12">(Gal &amp; Ghahramani, 2016b)</ref>. We repeat each experiment three times and plot mean and standard error. Test RMSE as well as test log likelihood are given in <ref type="figure" target="#fig_4">Figure 4</ref>. As can be seen, Hellinger value α = 0.5 gives best test RMSE, with test log likelihood matching that of the EP value α = 1. The VI value α = 0 under-performs according to both metrics.</p><p>We next assess a convolutional neural network model (CNN). For this experiment we use the standard CNN example given in <ref type="bibr" target="#b7">(Chollet, 2015)</ref> with 32 convolution filters, 100 hidden units at the top layer, and dropout probability 0.5 before each fully-connected layer. Other settings are as before. Average test accuracy and test log likelihood are given in <ref type="figure" target="#fig_5">Figure 5</ref>. In this case, VI value α = 0 seems to supersede the EP value α = 1, and performs similarly to the Hellinger value α = 0.5 according to both metrics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Detecting Adversarial Examples</head><p>The third set of experiments considers adversarial attacks on dropout-trained Bayesian neural networks. We test the hypothesis that certain techniques for generating adversarial examples will give images that lie outside of the image  manifold, i.e. far from the data distribution (note though that there exist techniques that will guarantee the images staying near the data manifold, by minimising the perturbation used to construct the adversarial example). By assessing the BNN uncertainty, we should see increased uncertainty for adversarial images if they indeed lie outside of the training data distribution. The tested models are fully connected networks with 3 hidden layers of 1000 units trained using dropout rate 0.5 and different alpha values. These models are also compared to a benchmark MLP with the same architecture but trained by maximum likelihood. The adversarial examples are generated on MNIST test data that is normalised to be in the range [0, 1]. For the dropout trained networks we perform MC dropout at test time with K test = 10 MC samples.</p><p>The first attack in consideration is the Fast Gradient Sign (FGS) method <ref type="bibr" target="#b13">(Goodfellow et al., 2014)</ref>. This is an untargeted attack, which attempts to reduces the maximum value of the predicted class label probability</p><formula xml:id="formula_24">x adv = x − η · sgn(∇ x max y log p(y|x)).</formula><p>We use the single gradient step FGS implemented in Cleverhans <ref type="bibr" target="#b39">(Papernot et al., 2016)</ref> with the stepsize η varied   between 0.0 and 0.5. The left panel in <ref type="figure" target="#fig_6">Figure 6</ref> demonstrates the classification accuracy on adversarial examples, which shows that the dropout networks, especially the one trained with α = 1.0, are significantly more robust to adversarial attacks compared to the deterministic NN. For example, for η = 0.1 the adversarial samples still visually close to the original class, and the BNN trained with α = 0.0 achieves an accuracy level almost 3 times higher than the MLP and around 20% higher than the VI-trained version. More interestingly, the test data examples and adversarial images can be told-apart by investigating the uncertainty representation of the dropout models. In the right panel of <ref type="figure" target="#fig_6">Figure 6</ref> we depict the predictive entropy computed on the neural network output probability vector, and show example corresponding adversarial images below the axis for each corresponding stepsize. Clearly the deterministic NN model produces over-confident predictions on adversarial samples, e.g. it predicts the wrong label very confidently even when the input is still visually close to digit "7" (η = 0.2). While dropout models, though producing wrong labels, are very uncertain about their predictions. This uncertainty keeps increasing as we move away from the data manifold. Hence the dropout networks are much more immunised from noise-corrupted inputs, as they can be detected using uncertainty estimates in this example.</p><p>The second attack we consider is a targeted version of FGS <ref type="bibr" target="#b13">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b6">Carlini &amp; Wagner, 2016)</ref>, which maximises the predictive probability of a selected class instead. As an example, we fix class 0 as the target and apply the iterative gradient-base attack to all non-zero digits in test data. At step t, the adversarial output is computed as where the stepsize η is fixed at 0.01 in this case. Results are presented in the left panel of <ref type="figure" target="#fig_7">Figure 7</ref>, and again dropout trained models are more robust to this attack compared with the MLP. Similarly these adversarial examples could be detected by the Bayesian neural networks' uncertainty, by examining the predictive entropy. By visually inspecting the generated adversarial examples in the right panel of <ref type="figure" target="#fig_7">Figure 7</ref>, it is clear that the MLP overconfidently classifies a digit 7 to class 0. On the other hand, the dropout models are still fairly uncertain about their predictions even after 40 gradient steps. More interestingly, running this iterative attack on dropout models produces a smooth interpolation between different digits, and when the model is confident on predicting the target class, the corresponding adversarial images are visually close to digit zero.</p><p>These initial results suggest that assessing the epistemic uncertainty of classification models can be used as a viable technique to identify adversarial examples. We would note though that we used this experiment to demonstrate our techniques' uncertainty estimates, and much more research is needed to solve the difficulties faced with adversarial inputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We presented a practical extension of the BB-alpha objective which allows us to use the technique with dropout approximating distributions. The technique often supersedes existing approximate inference techniques (even sparse Gaussian processes), and is easy to implement. A code snippet for our induced loss is given in the appendix.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>the number of samples K → 1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Negative test-LL results for Bayesian NN regression. Figure 3. Test RMSE results for Bayesian NN regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 2. Negative test-LL results for Bayesian NN regression. Figure 3. Test RMSE results for Bayesian NN regression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>(a) NN test accuracy (b) NN test log likelihood</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. MNIST test accuracy and test log likelihood for a fully connected NN in a classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. MNIST test accuracy and test log likelihood for a convolutional neural network in a classification task.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Un-targeted attack: classification accuracy results as a function of perturbation stepsize. The adversarial examples are shown for (from top to bottom) MLP and BNN trained with dropout and α = 0.0, 0.5, 1.0.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Targeted attack: classification accuracy results (on both original and target class) as a function of the number of iterative gradient steps. Note the log scale x-axis in the left panel.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>adv</head><label></label><figDesc>+ η · sgn(∇ x log p(y target |x t−1 adv )),</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">fn(ω) does not need to be a normalised density of yn unless one would like to optimise the associated hyper parameters.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">http://archive.ics.uci.edu/ml/datasets. html</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Rich Turner, Nicolas Papernot, and the reviewers for comments. YL thanks the Schlumberger Foundation FFTF fellowship for supporting her PhD study.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Amari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Shun-ichi. Differential-Geometrical Methods in Statistic</title>
		<imprint>
			<date type="published" when="1985" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dario</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Steinhardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christiano</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Paul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Mane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.06565</idno>
		<title level="m">Concrete problems in ai safety</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multi-task deep neural network to predict CpG methylation profiles from low-coverage sequencing data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Angermueller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Stegle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS MLCB workshop</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ensemble learning in Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">NATO ASI SERIES F COMPUTER AND SYSTEMS SCIENCES</title>
		<imprint>
			<biblScope unit="volume">168</biblScope>
			<biblScope unit="page" from="215" to="238" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Julien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daan</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep gaussian processes for regression using approximate expectation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><forename type="middle">D</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turner</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning (ICML)</title>
		<meeting>The 33rd International Conference on Machine Learning (ICML)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Wagner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.04644</idno>
		<title level="m">Towards evaluating the robustness of neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francois</forename><surname>Chollet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Keras</surname></persName>
		</author>
		<ptr target="https://github.com/fchollet/keras" />
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Transforming neural-net output levels to probability distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 3. Citeseer</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Learning and policy search in stochastic dynamical systems with bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Depeweg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Finale</forename><surname>Doshi-Velez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Steffen</forename><surname>Udluft</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07127</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Uncertainty in Deep Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
		<respStmt>
			<orgName>University of Cambridge</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Bayesian convolutional neural networks with Bernoulli approximate variational inference. ICLR workshop track</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dropout as a Bayesian approximation: Representing model uncertainty in deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yarin</forename><surname>Gal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zoubin</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathon</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical variational inference for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2348" to="2356" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Neue begründung der theorie quadratischer formen von unendlichvielen veränderlichen</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ernst</forename><surname>Hellinger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal für die reine und angewandte Mathematik</title>
		<imprint>
			<biblScope unit="volume">136</biblScope>
			<biblScope unit="page" from="210" to="271" />
			<date type="published" when="1909" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of Bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jose</forename><surname>Hernandez-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ryan</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Black-box alpha divergence minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thang</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of The 33rd International Conference on Machine Learning</title>
		<meeting>The 33rd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1511" to="1520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Keeping the neural networks simple by minimizing the description length of the weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Camp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Drew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1993" />
			<biblScope unit="page" from="5" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nitish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">An introduction to variational methods for graphical models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zoubin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tommi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saul</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">K</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="183" to="233" />
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Recurrent continuous translation models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Phil</forename><surname>Blunsom</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Modelling uncertainty in deep learning for camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE International Conference on Robotics and Automation (ICRA)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4762" to="4769" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Bayesian segnet: Model uncertainty in deep convolutional encoder-decoder architectures for scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.02680</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Information theory and statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1959" />
			<publisher>John Wiley &amp; Sons</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">On information and sufficiency. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Solomon</forename><surname>Kullback</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">A</forename><surname>Leibler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="page" from="79" to="86" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Corinna</forename><surname>Cortes</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Backpropagation applied to handwritten zip code recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Boser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bernhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><forename type="middle">S</forename><surname>Denker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Donnie</forename><surname>Henderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wayne</forename><surname>Hubbard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="541" to="551" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sumit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Raia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ranzato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">A tutorial on energy-based learning. Predicting structured data</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">0</biblScope>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Rényi divergence variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Stochastic expectation propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingzhen</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">José</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Turner</forename><surname>Miguel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Richard</forename><forename type="middle">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A practical Bayesian framework for backpropagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">Jc</forename><surname>Mackay</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="448" to="472" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Dropout Inference in Bayesian Neural Networks with Alpha-divergences</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Recurrent neural network based language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomáš</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Karafiát</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Martin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Burget</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lukáš</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Černockỳ</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Khudanpur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sanjeev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Eleventh Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Divergence measures and message passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Minka</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Expectation propagation for approximate Bayesian inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Uncertainty in Artificial Intelligence (UAI)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Minka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Power</surname></persName>
		</author>
		<idno>MSR-TR-2004-149</idno>
		<imprint>
			<date type="published" when="2004" />
			<pubPlace>Microsoft Research</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">Technical Report</note>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Bayesian learning for neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Radford</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1995" />
		</imprint>
		<respStmt>
			<orgName>University of Toronto</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sheatsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Reuben</forename><surname>Feinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.00768</idno>
		<title level="m">an adversarial machine learning library</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">On measures of entropy and information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alfréd</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Fourth Berkeley symposium on mathematical statistics and probability</title>
		<imprint>
			<date type="published" when="1961" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Learning internal representations by error propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Williams</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ronald</forename><forename type="middle">J</forename></persName>
		</author>
		<imprint>
			<date type="published" when="1985" />
			<publisher>DTIC Document</publisher>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Edinburgh neural machine translation systems for wmt 16</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rico</forename><surname>Sennrich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barry</forename><surname>Haddow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexandra</forename><surname>Birch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the First Conference on Machine Translation</title>
		<meeting>the First Conference on Machine Translation<address><addrLine>Berlin, Germany</addrLine></address></meeting>
		<imprint>
			<publisher>Association for Computational Linguistics</publisher>
			<date type="published" when="2016-08" />
			<biblScope unit="page" from="371" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Dropout: A simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">LSTM neural networks for language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Sundermeyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ralf</forename><surname>Schlüter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hermann</forename><surname>Ney</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">INTERSPEECH</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ilya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">V</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">Going deeper with convolutions. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Two problems with variational expectation maximisation for time-series models. Inference and Estimation in Probabilistic Time-Series Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Turner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sahani</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Rényi divergence and Kullback-Leibler divergence. Information Theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Van Erven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Harremoës</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="3797" to="3820" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML-13</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiao</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Kwitt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Niethammer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02504</idno>
		<title level="m">Fast predictive image registration</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
