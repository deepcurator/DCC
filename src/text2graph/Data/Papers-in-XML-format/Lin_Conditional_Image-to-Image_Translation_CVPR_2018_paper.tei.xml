<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Image-to-Image translation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxin</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yingce</forename><surname>Xia</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Qin</surname></persName>
							<email>taoqin@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhibo</forename><surname>Chen</surname></persName>
							<email>chenzhibo@ustc.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Science</orgName>
								<address>
									<country>Technology of China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tie-Yan</forename><surname>Liu</surname></persName>
							<email>tie-yan.liu@microsoft.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Microsoft Research Asia</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Image-to-Image translation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Image-to-image translation covers a large variety of computer vision problems, including image stylization <ref type="bibr" target="#b3">[4]</ref>, segmentation <ref type="bibr" target="#b12">[13]</ref> and saliency detection <ref type="bibr" target="#b4">[5]</ref>. It aims at learning a mapping that can convert an image from a source domain to a target domain, while preserving the main presentations of the input images. For example, in the aforementioned three tasks, an input image might be converted to a portrait similar to Van Gogh's styles, a heat map splitted into different regions, or a pencil sketch, while the edges and outlines remain unchanged. Since it is usually hard to collect a large amount of parallel data for such tasks, unsupervised learning algorithms have been widely adopted. Particularly, the generative adversarial networks (GAN) <ref type="bibr" target="#b5">[6]</ref> and dual learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b20">21]</ref> are extensively studied in image-to-image translations. <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref> tackle imageto-image translation by the aforementioned two techniques, where the GANs are used to ensure the generated images belonging to the target domain, and dual learning can help improve image qualities by minimizing reconstruction loss.</p><p>An implicit assumption of image-to-image translation is that an image contains two kinds of features <ref type="bibr" target="#b0">1</ref> : domainindependent features, which are preserved during the translation (i.e., the edges of face, eyes, nose and mouse while translating a man' face to a woman' face), and domainspecific features, which are changed during the translation (i.e., the color and style of the hair for face image translation). Image-to-Image translation aims at transferring images from the source domain to the target domain by preserving domain-independent features while replacing domain-specific features.</p><p>While it is not difficult for existing image-to-image translation methods to convert an image from a source domain to a target domain, it is not easy for them to control or manipulate the style in fine granularity of the generated image in the target domain. Consider the gender transform problem studied in <ref type="bibr" target="#b8">[9]</ref>, which is to translate a man's photo to a woman's. Can we translate Hillary's photo to a man' photo with the hair style and color of Trump? DiscoGAN <ref type="bibr" target="#b8">[9]</ref> can indeed output a woman's photo given a man's photo as input, but cannot control the hair style or color of the output image. DualGAN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b24">25]</ref> cannot implement this kind of fine-granularity control neither. To fulfill such a blank in image translation, we propose the concept of conditional image-to-image translation, which can specify domainspecific features in the target domain, carried by another input image from the target domain. An example of conditional image-to-image translation is shown in <ref type="figure" target="#fig_0">Figure 1</ref>, in which we want to convert Hillary's photo to a man's photo. As shown in the figure, with an addition man's photo as input, we can control the translated image (e.g., the hair color and style).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Problem Setup</head><p>We first define some notations. Suppose there are two image domains D A and D B . Following the implicit assumption, an image x A ∈ D A can be represented as The problem of conditional image-to-image translation from domain D A to D B is as follows: Taken an image x A ∈ D A as input and an image x B ∈ D B as conditional input, outputs an image x AB in domain D B that keeping the domain-independent features of x A and combining the domain-specific features carried in x B , i.e.,</p><formula xml:id="formula_0">x A = x i A ⊕ x</formula><formula xml:id="formula_1">x AB = G A→B (x A , x B ) = x i A ⊕ x s B ,<label>(1)</label></formula><p>where G A→B denotes the translation function. Similarly, we have the reverse conditional translation</p><formula xml:id="formula_2">x BA = G B→A (x B , x A ) = x i B ⊕ x s A .<label>(2)</label></formula><p>For simplicity, we call G A→B the forward translation and G B→A the reverse translation. In this work we study how to learn such two translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Our Results</head><p>There are three main challenges in solving the conditional image translation problem. The first one is how to extract the domain-independent and domain-specific features for a given image. The second is how to merge the features from two different domains into a natural image in the target domain. The third one is that there is no parallel data for us to learn such the mappings.</p><p>To tackle these challenges, we propose the conditional dual-GAN (briefly, cd-GAN), which can leverage the strengths of both GAN and dual learning. Under such a framework, the mappings of two directions, G A→B and G B→A , are jointly learned. The model of cd-GAN follows the encoder-decoder based framework: the encoder is used to extract the domain-independent and domain-specific features and the decoder is to merge the two kinds of features to generate images. We chose GAN and dual learning due to the following considerations: (1) The dual learning framework can help learn to extract and merge the domain-specific and domain-independent features by minimizing carefully designed reconstruction errors, including reconstruction errors of the whole image, the domainindependent features, and the domain-specific features. <ref type="formula" target="#formula_2">(2)</ref> GAN can ensure that the generated images well mimic the natural images in the target domain. (3) Both dual learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b24">25]</ref> and GAN <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b0">1]</ref> work well under unsupervised settings.</p><p>We carry out experiments on different tasks, including face-to-face translation, edge-to-shoe translation, and edgeto-handbag translation. The results demonstrate that our network can effectively translate image with conditional information and robust to various applications.</p><p>Our main contributions lie in two folds: (1) We define a new problem, conditional image-to-image translation, which is a more general framework than conventional image translation. (2) We propose the cd-GAN algorithm to solve the problem in an end-to-end way.</p><p>The remaining parts are organized follows. We introduce related work in Section 2 and present the details of cd-GAN in Section 3, including network architecture and the training algorithm. Then we report experimental results in Section 4 and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Image generation has been widely explored in recent years. Models based on variational autoencoder (VAE) <ref type="bibr" target="#b10">[11]</ref> aim to improve the quality and efficiency of image generation by learning an inference network. GANs <ref type="bibr" target="#b5">[6]</ref> were firstly proposed to generate images from random variables by a two-player minimax game. Researchers have been exploited the capability of GANs for various image generation tasks. <ref type="bibr" target="#b0">[1]</ref> proposed to synthesize images at multiple resolutions with a Laplacian pyramid of adversarial generators and discriminators, and can condition on class labels for controllable generation. <ref type="bibr" target="#b18">[19]</ref> introduced a class of deep convolutional generative networks (DCGANs) for high-quality image generation and unsupervised image classification tasks.</p><p>Instead of learning to generate image samples from scratch (i.e., random vectors), the basic idea of image-toimage translation is to learn a parametric translation function that transforms an input image in a source domain to an image in a target domain. <ref type="bibr" target="#b12">[13]</ref> proposed a fully convolutional network (FCN) for image-to-segmentation translation. Pix2pix <ref type="bibr" target="#b7">[8]</ref> extended the basic FCN framework to other image-to-image translation tasks, including label-tostreet scene and aerial-to-map. Meanwhile, pix2pix utilized adversarial training technique to ensure high-level domain similarity of the translation results.</p><p>The image-to-image models mentioned above require paired training data between the source and target domains. There is another line of works studying unpaired domain translation. Based on adversarial training, <ref type="bibr" target="#b2">[3]</ref> and <ref type="bibr" target="#b1">[2]</ref> proposed algorithms to jointly learn to map latent space to data space and project the data space back to latent space. <ref type="bibr" target="#b19">[20]</ref> presented a domain transfer network (DTN) for unsupervised cross-domain image generation employing a compound loss function including multiclass adversarial loss and f -constancy component, which could generate convincing novel images of previously unseen entities and preserve their identity. <ref type="bibr" target="#b6">[7]</ref> developed a dual learning mechanism which can enable a neural machine translation system to automatically learn from unlabeled data through a dual learning game. Following the idea of dual learning, DualGAN <ref type="bibr" target="#b21">[22]</ref>, DiscoGAN <ref type="bibr" target="#b8">[9]</ref> and CycleGAN <ref type="bibr" target="#b24">[25]</ref> were proposed to tackle the unpaired image translation problem by training two cross domain transfer GANs at the same time. <ref type="bibr" target="#b14">[15]</ref> proposed to utilize dual learning for semantic image segmentation. <ref type="bibr" target="#b13">[14]</ref> further proposed a conditional CycleGAN for face super-resolution by adding facial attributes obtained from human annotation. However, collecting a large amount of such human annotated data can be hard and expensive.</p><p>In this work, we study a new setting of image-to-image translation, in which we hope to control the generated images in fine granularity with unpaired data. We call such a new problem conditional image-to-image translation. <ref type="figure" target="#fig_2">Figure 2</ref> shows the overall architecture of the proposed model, in which the left part is an encoder-decoder based framework for image translation and the right part includes additional components introduced to train the encoder and decoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conditional Dual GAN</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The Encoder-Decoder Framework</head><p>As shown in the figure, there are two encoders e A and e B and two decoders g A and g B .</p><p>The encoders serve as feature extractors, which take an image as input and output the two kinds of features, domainindependent features and domain-specific features, with the corresponding modules in the encoders. In particular, given two images x A and x B , we have</p><formula xml:id="formula_3">(x i A , x s A ) = e A (x A ); (x i B , x s B ) = e B (x B ).<label>(3)</label></formula><p>If only looking at the encoder, there is no difference between the two kinds of features. It is the remaining parts of the overall model and the training process that differentiate the two kinds of features. More details are discussed in Section 3.3.</p><p>The decoders serve as generators, which take as inputs the domain-independent features from the image in the source domain and the domain-specific features from the image in the target domain and output a generated image in the target domain. That is,</p><formula xml:id="formula_4">x AB = g B (x i A , x s B ); x BA = g A (x i B , x s A ).<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training Algorithm</head><p>We leverage dual learning techniques and the GAN techniques to train the encoders and decoders. The optimization process is shown in the right part of <ref type="figure" target="#fig_2">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">GAN loss</head><p>To ensure the generated x AB and x BA are in the corresponding domains, we employ two discriminators d A and d B to differentiate the real images and synthetic ones. d A (or d B ) takes an image as input and outputs a probability indicating how likely the input is a natural image from domain D A (or D B ). The objective function is</p><formula xml:id="formula_5">ℓ GAN = log(d A (x A )) + log(1 − d A (x BA )) + log(d B (x B )) + log(1 − d B (x AB )).<label>(5)</label></formula><p>The goal of the encoders and decoders e A , e B , g A , g B is to generate images as similar to natural images and fool the discriminators d A and d B , i.e., they try to minimize ℓ GAN . The goal of d A and d B is to differentiate generated images from natural images, i.e., they try to maximize ℓ GAN .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Dual learning loss</head><p>The key idea of dual learning is to improve the performance of a model by minimizing the reconstruction error. To reconstruct the two imagesx A andx B , as shown in <ref type="figure" target="#fig_2">Figure 2</ref>, we first extract the two kinds of features of the generated images: and then reconstruct images as follows:</p><formula xml:id="formula_6">(x i A ,x s B ) = e B (x AB ); (x i B ,x s A ) = e A (x BA ),<label>(6)</label></formula><formula xml:id="formula_7">x A = g A (x i A , x s A );x B = g B (x i B , x s B ).<label>(7)</label></formula><p>We evaluate the reconstruction quality from three aspects: the image level reconstruction error ℓ im dual , the reconstruction error ℓ di dual of the domain-independent features, and the reconstruction error ℓ ds dual of the domain-specific features as follows:</p><formula xml:id="formula_8">ℓ im dual (x A , x B ) = x A −x A 2 + x B −x B 2 , (8) ℓ di dual (x A , x B ) = x i A −x i A 2 + x i B −x i B 2 , (9) ℓ ds dual (x A , x B ) = x s A −x s A 2 + x s B −x s B 2 .<label>(10)</label></formula><p>Compared with the existing dual learning approaches <ref type="bibr" target="#b21">[22]</ref> which only consider the image level reconstruction error, our method considers more aspects and therefore is expected to achieve better accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Overall training process</head><p>Since the discriminators only impact the GAN loss ℓ GAN , we only use this loss to compute the gradients and update d A and d B . In contrast, the encoders and decoders impact all the 4 losses (i.e., the GAN loss and three reconstruction errors), we use all the 4 objectives to compute gradients and update models for them. Note that since the 4 objectives are of different magnitudes, their gradients may vary a lot in terms of magnitudes. To smooth the training process, we normalize the gradients so that their magnitudes are comparable across 4 losses. We summarize the training process in Algorithm 1. </p><formula xml:id="formula_9">d A ← Opt(d A , (1/K)∇ d A K k=1 ℓ GAN (x A,k , x B,k )), d B ← Opt(d B , (1/K)∇ d B K k=1 ℓ GAN (x A,k , x B,k )); 5:</formula><p>For each Θ ∈ {e A , e B , g A , g B }, compute the gradients</p><formula xml:id="formula_10">∆ GAN = (1/K)∇ Θ K k=1 ℓ GAN (x A,k , x B,k ), ∆ im = (1/K)∇ Θ K k=1 ℓ im dual (x A,k , x B,k ), ∆ di = (1/K)∇ Θ K k=1 ℓ di dual (x A,k , x B,k ), ∆ ds = (1/K)∇ Θ K k=1 ℓ ds dual (x A,k , x B,k ),</formula><p>normalize the four gradients to make their magnitudes comparable, sum them to obtain ∆, and Θ → Opt(Θ, ∆). 6: Repeat step 2 to step 6 until convergence In Algorithm 1, the choice of optimizers Opt(·, ·) is quite flexible, whose two inputs are the parameters to be opti-mized and the corresponding gradients. One can choose different optimizers (e.g. Adam <ref type="bibr" target="#b9">[10]</ref>, or nesterov gradient descend <ref type="bibr" target="#b17">[18]</ref>) for different tasks, depending on common practice for specific tasks and personal preferences. Besides, the e A , e B , g A , g B , d A , d B might refer to either the models themselves, or their parameters, depending on the context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Discussions</head><p>Our proposed framework can learn to separate the domain-independent features and domain-specific features. In <ref type="figure" target="#fig_2">Figure 2</ref>, consider the path of x A → e A → x i A → g B → x AB . Note that after training we ensure that x AB is an image in domain D B and the features x DualGAN <ref type="bibr" target="#b21">[22]</ref>, DiscoGAN <ref type="bibr" target="#b8">[9]</ref> and CycleGAN <ref type="bibr" target="#b24">[25]</ref> can be treated as simplified versions of our cd-GAN, by removing the domain-specific features. For example, in Cycle-GAN, given an x A ∈ D A , any x AB ∈ D B is a legal translation, no matter what x B ∈ D B is. In our work, we require that the generated images should match the inputs from two domains, which is more difficult.</p><p>Furthermore, cd-GAN works for both symmetric translations and asymmetric translations. In symmetric translations, both directions of translations need conditional inputs (illustrated in <ref type="figure" target="#fig_0">Figure 1(a)</ref>). In asymmetric translations, only one direction of translation needs a conditional image as input (illustrated in <ref type="figure" target="#fig_0">Figure 1(b)</ref>). That is, the translation from bag to edge does not need another edge image as input; even given an additional edge image as the conditional input, it does not change or help to control the translation result.</p><p>For asymmetric translations, we only need to slightly modify objectives for cd-GAN training. Suppose the translation direction of G B→A does not need conditional input. Then we do not need to reconstruct the domainspecific features x s A . Accordingly, we modify the error of domain-specific features as follows, and other 3 losses do not change. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct a set of experiments to test the proposed model. We first describe experimental settings, and then report results for both symmetric translations and asymmetric translations. Finally we study individual components and loss functions of the proposed model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Settings</head><p>For all experiments, the networks take images of 64 × 64 resolution as inputs. The encoders e A and e B start with 3 convolutional layers, each convolutional layer followed by leaky rectified linear units (Leaky ReLU) <ref type="bibr" target="#b15">[16]</ref>. Then the network is splitted into two branches: in one branch, a convolutional layer is attached to extract domain-independent features; in the other branch, two fully-connected layers are attached to extract domain-specific features. Decoder networks g A and g B contain 4 deconvolutional layers with Re-LU units <ref type="bibr" target="#b16">[17]</ref>, except for the last layer using tanh activation function. The discriminators d A and d B consist of 4 convolution layers, two fully-connected layers. Each layer is followed by Leaky ReLU units except for the last layer using sigmoid activation function. Details (e.g., number and size of filters, number of nodes in fully-connected layers) can be found in the supplementary document.</p><p>We use Adam <ref type="bibr" target="#b9">[10]</ref> as the optimization algorithm with learning rate 0.0002. Batch normalization is applied to all convolution layers and deconvolution layers except for the first and last ones. Minibatch size is fixed as 200 for all the tasks.</p><p>We implement three related baselines for comparison.</p><p>1. DualGAN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b24">25]</ref>. DualGAN was primitively proposed for unconditional image-to-image translation which does not require conditional input. Similar to our cd-GAN, DualGAN trains two translation models jointly.</p><p>2. DualGAN-c. In order to enable DualGAN to utilize conditional input, we design a network as DualGANc. The main difference between DualGAN and DualGAN-c is that DualGAN-c translates the target outputs as Eqn. <ref type="bibr" target="#b2">(3,</ref><ref type="bibr" target="#b3">4)</ref>, and reconstructs inputs asx A = g A (e B (x AB )) andx B = g B (e A (x BA )).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GAN-c.</head><p>To verify the effectiveness of dual learning, we remove the dual learning losses of cd-GAN during training and obtain GAN-c.</p><p>For symmetric translations, we carry out experiments on men-to-women face translations. We use the CelebA dataset <ref type="bibr" target="#b11">[12]</ref>, which consists of 84434 men's images (denoted as domain D A ) and 118165 women's images (denoted as domain D B ). We randomly choose 4732 men's images and 6379 women's images for testing, and use the rest for training. In this task, the domain-independent features are organs (e.g., eyes, nose, mouse, ? and domainspecific features refer to hair-style, beard, the usage of lipstick. For asymmetric translations, we work on edges-toshoes and edges-to-bags translations with datasets used in <ref type="bibr" target="#b22">[23]</ref> and <ref type="bibr" target="#b23">[24]</ref> respectively. In these two tasks, the domainindependent features are edges and domain-specific features are colors, textures, etc. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The translation results of face-to-face, edges-to-bags and edges-to-shoes are shown in <ref type="figure" target="#fig_6">Figure 3</ref>-5 respectively.</p><p>For men-to-women translations, from <ref type="figure" target="#fig_6">Figure 3</ref>(a), we have several observations. (1) DualGAN can indeed generate woman's photo, but its results are purely based on the men's photos, since it does not take the conditional images as inputs. <ref type="formula" target="#formula_2">(2)</ref> Although taking the conditional image as input, DualGAN-c fails to integrate the information (e.g., style) from the conditional input into its translation output. (3) For GAN-c, sometimes its translation result is not relevant to the original source-domain input, e.g., the 4-th row <ref type="figure" target="#fig_6">Figure 3(a)</ref>. This is because in training it is required to generate a target-domain image, but its output is not required to be similar (in certain aspects) to the original input. <ref type="formula" target="#formula_4">(4)</ref> cd-GAN works best among all the models by preserving domain-independent features from the source-domain input  In 6-th column of 5-th row, the hair-style of the generated image is the most similar to the conditional input.</p><p>We can get similar observations for women-to-men translations as shown in <ref type="figure" target="#fig_6">Figure 3(b)</ref>, especially for the domain-specific features such as hair style and beard.</p><p>From <ref type="figure" target="#fig_7">Figure 4</ref> and 5, we find that cd-GAN can well leverage the domain-specific information carried in the conditional inputs and control the generated target-domain images accordingly. DualGAN, DuanGAN-c and GAN-c do not effectively utilize the conditional inputs.</p><p>One important characteristic of conditional image-toimage translation model is that it can generate diverse target-domain images for a fixed source-domain image, only if different target-domain images are provided as inputs. To verify such this ability of cd-GAN, we conduct t- wo experiments: (1) for each woman's photo, we work on women-to-men translations with different man's photos as conditional inputs; (2) for each edge of a bag, we work on edges-to-bags translations with different bags as conditional inputs. The results are shown in <ref type="figure" target="#fig_9">Figure 6</ref>. <ref type="figure" target="#fig_9">Figure 6(b)</ref> shows that cd-GAN can fulfill edges with the colors and textures provided by the conditional inputs. Besides, cd-GAN also achieves reasonable improvements on most face translations: The domain-independent features like woman's facial outline, orientations and expressions are preserved, while the women specific features like hair-style and the usage the lipstick are replaced with men's. An example is the second row of <ref type="figure" target="#fig_9">Figure 6(a)</ref>, where pointed chins, serious expressions and looking forward are preserved in the generated images. The hairstyles (bald v.s. short hair) and the beard (no beard v.s. short beard) are reflected by the corresponding men's. Similar translations of the other images can also be found. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Component Study</head><p>In this sub section, we study other possible design choices for the model architecture in <ref type="figure" target="#fig_2">Figure 2</ref> and losses used in training. We compare cd-GAN with other four models as follows:</p><p>• cd-GAN-rec. The inputs are reconstructed aŝ</p><formula xml:id="formula_11">x A = g A (x i A ,x s A );x B = g B (x i B ,x s B )<label>(12)</label></formula><p>instead of Eqn. <ref type="formula" target="#formula_7">(7)</ref>. That is, the connection from x • cd-GAN-nof. Both domain-specific and domainindependent feature reconstruction losses, i.e., Eqn. <ref type="formula" target="#formula_1">(10)</ref> and Eqn. <ref type="bibr" target="#b8">(9)</ref>, are removed from dual learning losses.</p><p>• cd-GAN-nos. The domain-specific feature reconstruction loss, i.e., Eqn. <ref type="formula" target="#formula_1">(10)</ref>, is removed from dual learning losses.</p><p>• cd-GAN-noi. The domain-independent feature reconstruction loss, i.e., Eqn. <ref type="formula" target="#formula_1">(10)</ref> is removed from dual learning losses.</p><p>The comparison experiments are conducted on the edgesto-handbags task. The results are shown in <ref type="figure" target="#fig_10">Figure 7</ref>. Our cd-GAN outperforms the other four candidate models with better color schemes. Failure of cd-GAN-rec demonstrates the necessity of "skip connections" (i.e., the connections from x s A to g A and from x s B to g B ) for image reconstruction. Since the domain-specific feature level and image level reconstruction losses have implicitly put constrains on domain-specific feature to some extent, the results produced by cd-GAN-noi are closest to results of cd-GAN among the four candidate models.</p><p>So far, we have shown the translation results of cd-GAN generated from the combination domain-specific features and domain-independent features. One may be interested in what we really learn in the two kinds of features. Here we try to understand them by generating translation results using each kind of features separately:</p><p>• We generate an image using the domain-specific features only:</p><formula xml:id="formula_12">x A=0 AB = g B (x i A = 0, x s B ),</formula><p>in which we set the domain-independent features to 0.</p><p>• We generate an image using the domain-independent features only:</p><formula xml:id="formula_13">x B=0 AB = g B (x i A , x s B = 0),</formula><p>in which we set the domain-specific features to 0.</p><p>The results are shown in <ref type="figure">Figure 8</ref>. As we can see, the image x A=0 AB has similar style to x B , which indicates that our cd-GAN can indeed extract domain-specific features. While x B=0 AB already loses conditional information of x B , it still preserves main shape of x A , which demonstrates that cd-GAN indeed extracts domain-independent features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">User Study</head><p>We have conducted user study to compare domainspecific features similarity between generated images and conditional images. Total 17 subjects (10 males, 7 females, age range 20 − 35) from different backgrounds are asked to make comparison of 32 sets of images. We show the subjects source image, conditional image, our result and results from other methods. Then each subject selects generated image most similar to conditional image. The result of user study shows that our model obviously outperforms other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions and Future Work</head><p>In this paper, we have studied the problem of conditional image-to-image translation, in which we translate an image from a source domain to a target domain conditioned on another target-domain image as input. We have proposed a <ref type="figure">Figure 8</ref>. Images generated using only domain-independent features or domain-specific features. new model based on GANs and dual learning. The model can well leverage the conditional inputs to control and diversify the translation results. Experiments on two settings (symmetric translations and asymmetric translations) and three tasks (face-to-face, edges-to-shoes and edges-tohandbags translations) have demonstrated the effectiveness of the proposed model.</p><p>There are multiple aspects to explore for conditional image translation. First, we will apply the proposed model to more image translation tasks. Second, it is interesting to design better models for this translation problem. Third, the problem of conditional translations may be extend to other applications, such as conditional video translations and conditional text translations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgement</head><p>This work was supported in part by the National Key Research and Development Program of China under Grant No.2016YFC0801001, NSFC under Grant 61571413, 61632001, 61390514, and Intel ICRI MNC.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Conditional image-to-image translation. (a) Conditional women-to-men photo translation. (b) Conditional edges-tohandbags translation. The purple arrow represents translation flow and the green arrow represents the conditional information flow.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>A</head><label></label><figDesc>'s are domain-specific features, and ⊕ is the op- erator that can merge the two kinds of features into a com- plete image. Similarly, for an image x B ∈ D B , we have x B = x i B ⊕ x s B . Take the images in Figure 1 as exam- ples: (1) If the two domains are man's and woman's pho- tos, the domain-independent features are individual facial organs like eyes and mouths and the domain-specific fea- tures are beard and hair style. (2) If the two domains are real bags and the edges of bags, the domain-independent features are exactly the edges of bags themselves, and the domain-specific are the colors and textures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Architecture of the proposed conditional dual GAN (cd-GAN).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Algorithm 1 cd-GAN training process Require: Training images {x A,i } m i=1 ⊂ D A , {x B,j } m j=1 ⊂ D B , batch size K, optimizer Opt(·, ·); 1: Randomly initialize e A , e B , g A , g B , d A and d B . 2: Randomly sample a minibatch of images and prepare the data pairs S = {(x A,k , x B,k )} K k=1 . 3: For any data pair (x A,k , x B,k ) ∈ S, generate condition- al translations by Eqn.(3,4), and reconstruct the images by Eqn.(6,7); 4: Update the discriminators as follows:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>try to inherent the features that are independent to domain D A . Given that x i A is domain inde- pendent, it is x s B that carries information about domain D B . Thus, x s B is domain-specific features. Similarly, we can see that x s A is domain-specific and x i B is domain-independent.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Conditional face-to-face translation. (a) Results of conditional men→women translation. (b) Results of conditional women→men translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Results of conditional edges→handbags translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Results of conditional edges→shoes translation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Our cd-GAN model can produce diverse results with different conditional images. (a) Results of women→men translation with two different men's images as conditional inputs. (b) Results of edges→handbags translation with two different handbags as conditional inputs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results produced by different connections and losses of cd-GANs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head></head><label></label><figDesc>A in the right box of Figure 2 is replaced by the connection fromx s A to g A , and the connection from x s B to g B in the right box of Figure 2 is replaced by the connection fromx s B to g B .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_12"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. The result of user study.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Note that the two kinds of features are relative concepts, and domainspecific features in one task might be domain-independent features in another task, depending on what domains one focuses on in the task.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<title level="m">Adversarial feature learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Adversarially learned inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2423" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Dual learning for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="820" to="828" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Imageto-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to discover cross-domain relations with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05192</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Conditional cyclegan for attribute guided face image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-K</forename><surname>Tang</surname></persName>
		</author>
		<idno>arX- iv:1705.09966</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep dual learning for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A method of solving a convex programming problem with convergence rate o (1/k2)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nesterov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Soviet Mathematics Doklady</title>
		<imprint>
			<date type="published" when="1983" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="372" to="376" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Unsupervised crossdomain image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Taigman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Polyak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02200</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Dual supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017-08" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="6" to="11" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Dualgan: Unsupervised dual learning for image-to-image translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="192" to="199" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Generative visual manipulation on the natural image manifold</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="597" to="613" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unpaired imageto-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10593</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
