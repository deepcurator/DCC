<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:46+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning Answer Embeddings for Visual Question Answering</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hexiang</forename><surname>Hu</surname></persName>
							<email>hexiang.frank.hu@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">U. of Southern California</orgName>
								<orgName type="department" key="dep2">U. of Southern California</orgName>
								<orgName type="department" key="dep3">U. of Southern California</orgName>
								<address>
									<settlement>Los Angeles, Los Angeles, Los Angeles</settlement>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei-Lun</forename><surname>Chao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">U. of Southern California</orgName>
								<orgName type="department" key="dep2">U. of Southern California</orgName>
								<orgName type="department" key="dep3">U. of Southern California</orgName>
								<address>
									<settlement>Los Angeles, Los Angeles, Los Angeles</settlement>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sha</forename><surname>Fei</surname></persName>
							<email>feisha@usc.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">U. of Southern California</orgName>
								<orgName type="department" key="dep2">U. of Southern California</orgName>
								<orgName type="department" key="dep3">U. of Southern California</orgName>
								<address>
									<settlement>Los Angeles, Los Angeles, Los Angeles</settlement>
									<region>CA, CA, CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning Answer Embeddings for Visual Question Answering</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose a novel probabilistic model for visual question answering (Visual QA </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Visual question answering (Visual QA) has made significant progress in the last few years. More than 10 datasets have been released for the task <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b24">25]</ref>, together with a number of learning models that have been narrowing the gap between the human's performance and the machine's <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>In this task, the machine is presented with an image and a related question and needs to output a correct answer. There * Equal contributions Q1: Where is the ball?</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Q2: Who is holding the bat?</head><p>The little boy.</p><p>In the basket.</p><p>In the air.</p><p>The man in the white uniform.</p><p>The player.</p><p>The woman.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Image + Question Embedding</head><p>Joint Embedding Space Answer Embedding <ref type="figure">Figure 1</ref>. Conceptual diagram of our approach. We learn two embedding functions to transform image question pair (i, q) and (possible) answer a into a joint embedding space. The distance (by inner products) between the embedded (i, q) and a is then measured and the closest a (in red) would be selected as the output answer.</p><p>are several ways of "outputting", though. One way is to ask the machine to generate a piece of free-form texts <ref type="bibr" target="#b7">[8]</ref>. However, this often requires humans to decide whether the answer is correct or not. Thus, scaling this type of evaluation to assess a large amount of data (on a large number of models) is challenging.</p><p>Automatic evaluation procedures have the advantage of scaling up. There are two major paradigms. One is to use multiple-choice based Visual QA <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>. In this setup, for each pair of image and question, a correct answer is mixed with a set of incorrect answers and the learner optimizes to select the correct one. While popular, it is difficult to design good incorrect answers without bias such that learners are not able to exploit <ref type="bibr" target="#b4">[5]</ref>. Several recent papers <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b12">13]</ref> have shown that even when the image or the question is missing, the correct answer can still be identified (using the incidental statistics, i.e., bias, in the data).</p><p>The other paradigm that is amenable to automatic evaluation revises the pool of possible answers to be the same for any pair of image and question <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b2">3]</ref>, i.e., open-ended Visual QA. In particular, the pool is composed of most frequent K answers in the training dataset. This has the advantage of framing the task as a multi-way classifier that outputs one of the K categories, with the image and the question as the input to the classifier.</p><p>However, while alleviating the bias of introducing incorrect answers that are image and question specific, the openend Visual QA approaches also suffer from several prob-lems. First, treating the answers as independent categories (as entailed by the multi-way classification) removes the semantic relationship between answers. For example, the answers of "running" and "jogging" (to the question "what is the woman in the picture doing?") are semantically close, so one would naturally infer the corresponding images are visually similar. However, treating "running" and "jogging" as independent categories "choice i" and "choice j" would not automatically regularize the learner to ensure the classifier's outputs of visually similar images and semantically similar questions to be semantically close. In other words, we would desire the outputs of the Visual QA model express semantic proximities aligned with visual and semantic proximities at the inputs. Such alignment will put a strong prior on what the models can learn and prevent them from exploiting biases in the datasets, thus become more robust.</p><p>Secondly, Visual QA models learned on one dataset do not transfer to another dataset unless the two datasets share the same space of top K answers-if there is a difference between the two spaces (for example, as "trivial" as changing the frequency order of the answers), the classifier will make a substantial number of errors. This is particularly alarming unless we construct a system a prior to map one set of answers to another set, we are likely to have very poor transfer across datasets and would have to train a new Visual QA model whenever we encounter a new dataset. In fact, for two popular Visual QA datasets, about 10% answers are shared and of top-K answers (where K &lt; 10, 000), only 50% answers are shared. We refer readers to section 4.5 and <ref type="table" target="#tab_7">Table 6</ref> for more results.</p><p>In this paper, we propose a new learning model to address these challenges. Our main idea is to learn also an embedding of the answers. Together with the (joint embedding) features of image and question in some spaces, the answer embeddings parameterize a probabilistic model describing how the answers are similar to the image and question pair. We learn the embeddings for the answers as well as the images and the questions to maximize the correct answers' likelihood. The learned model thus aligns the semantic similarity of answers with the visual/semantic similarity of the image and question pair. Furthermore, the learned model can also embed any unseen answers, thus can generalize from one dataset to another one. <ref type="figure">Fig. 1</ref> illustrates the main idea of our approach.</p><p>Our method needs to learn embeddings of hundreds and thousands of answers. Thus to optimize our probabilistic model, we overcome the challenge by introducing a computationally efficient way of adaptively sampling negative examples in a minibatch.</p><p>Our model also has the computational advantage that for each pair of image and question, we only need to compute the joint embedding of image and question for once, irrespective of how many candidate answers one has to examine. On the other end, models such as <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7]</ref> learn a joint embedding of the triplet (image, question and answer) needs to compute embeddings at the linear order of the number of candidate answers. When the number of candidate answers need to be large (to obtain better coverage), such models do not scale up easily.</p><p>While our approach is motivated by addressing challenges in open-end Visual QA, the proposed approach trivially includes multiple-choice based Visual QA as a special case and is thus equally applicable. We extensively evaluated our approach on several existing datasets, including Visual7W <ref type="bibr" target="#b31">[32]</ref>, VQA2 <ref type="bibr" target="#b8">[9]</ref>, and Visual Genome <ref type="bibr" target="#b15">[16]</ref>. We show the gain in performance by our approach over the existing approaches that are based on multi-way classification. We also show the effectiveness of our approach in transferring models trained on one dataset to another. To our best knowledge, we are likely the first to examine the challenging issue of transferability in the open-end Visual QA task <ref type="bibr" target="#b0">1</ref> . The rest of the paper is organized as follows. Section 3.1 introduces the notation and problem setup. Section 3.2 presents our proposed methods. Section 4 shows our empirical results on multiple Visual QA datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Visual QA</head><p>In open-end Visual QA, one popular framework of algorithms is to learn a joint image-question embedding and perform multi-way classification (for predicting top-frequency answers) on top <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b17">18]</ref>. Though such methods naturally limited themselves to answer questions within a fixed (usually small) vocabulary, this framework has been shown to outperform other methods that dedicate for freeform answer generation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b13">14]</ref>. Different from this line of research, in the multiple-choice setting, algorithms are usually designed to learn a scoring function with the image, question, and answer triplets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>. Such methods can take the advantage of answer semantics but fail to scale up inferencing along the increasing number of answer candidates. Comparing to all previous approaches, our proposed framework leverages the advantages of both worlds, capable of taking the answer semantic into account while remaining efficient. Please refer to section 3.6 for detailed discussion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Aligned Embeddings</head><p>The idea of learning and aligning embeddings has been explored in visual recognition <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b19">20]</ref>, in which the image and label embeddings are learned. Our work extends it to Visual QA 2 for parameterizing and learning a novel proba- <ref type="bibr" target="#b0">1</ref> Our work focuses on the transferability across datasets with different question and answer spaces. We leave visual transferability (e.g., by domain adaptation) as future work. <ref type="bibr" target="#b1">2</ref> We replace the image embedding with a joint one for image-question pairs, and investigate more complicated models for the answer embedding. bilistic model. We further propose an efficient optimization technique to handle a large number of candidate answers (e.g., more than 201,000 in Visual Genome <ref type="bibr" target="#b15">[16]</ref>), a situation rarely encountered in visual recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Methods</head><p>In what follows, we describe our approach in detail. We start by describing a general setup for Visual QA and introducing necessary notations. We then introduce the main idea, followed by detailed descriptions of the method and important steps to scale the method to handling hundreds of thousands negative samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Setup and Notations</head><p>In the Visual QA task, the machine is given an image i and a question q, and is asked to generate an answer a. In this work, we focus on the open-ended setting where a is a member of a set A. This set of candidate answers is intuitively "the universe of all possible answers". However, in practice, it is approximated by the top K most frequent correct answers in a training set <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>, plus all the incorrect answers in the dataset (if any). Another popular setting is multiple-choice based. For each pair of (i, q), the set A is different (this set is either automatically generated <ref type="bibr" target="#b4">[5]</ref> or manually generated <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b0">1]</ref>). Without loss of generality, however, we use A to represent both. Whenever necessary, we clarify the special handling we would need for (i, q) specific candidate set.</p><p>We distinguish two subsets in A with respect to a pair (i, q): C and D = A − C. The set C contains all the correct answers for (i, q)-it could be a singleton or in some cases, contains multiple semantically similar answers to the correct answer (e.g., "policeman" to "police officer"), depending on the datasets. The set D contains all the incorrect (or undesired) answers.</p><p>A training dataset is thus denoted by a set of N distinctive triplets D = {(i n , q n , C n )} when only the correct answers are given, or D = {(i n , q n , A n = C n ∪ D n )} when both the correct and incorrect answers are given.</p><p>Note that by i, q or a, we refers to their "raw" formats (an image in pixel values, and a question or an answer in its textual forms).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Main Idea</head><p>Our main idea is motivated by two deficiencies in the current approaches for open-ended Visual QA <ref type="bibr" target="#b0">[1]</ref>. In those methods, it is common to construct a K-way classifier so that for each (i, q), the classifier outputs k that corresponds to the correct answer (i.e., the k-th element in A is the correct answer).</p><p>However, this classification paradigm cannot capture all the information encoded in the dataset for us to derive better models. First, by equating two different answers a k and a l with the ordinal numbers k and l, we lose the semantic kinship between the two. If there are two triplets (i m , q m , a k ∈ C m ) and (i n , q n , a l ∈ C n ) having similar visual appearance between i m and i n and similar semantic meaning between q m and q n , we would expect a k and a l to have some degrees of semantic similarity. In a classification framework, such expectation cannot be fulfilled as the assignment of ordinal numbers k or l to either a k or a l can be arbitrary such that the difference between k and l does not preserve the similarity between a k and a l . However, observing such similarity at both the inputs to the classifier and the outputs of the classifier is beneficial and adds robustness to learning.</p><p>The second flaw with the multi-way classification framework is that it does not lend itself to generalize across two datasets with little or no overlapping in the candidate answer sets A. Unless there is a prior defined mapping between the two sets, the classifier trained on one dataset is not applicable to the other dataset.</p><p>We propose a new approach to overcome those deficiencies. The key idea is to learn embeddings of all the data. The embedding functions, when properly parameterized and learned, will preserve similarity and will generalize to unseen answers (in the training data).</p><p>Embeddings We first define a joint embedding function f θ (i, q) to generate the joint embedding of the pair i and q. We also define an embedding function g φ (a) to generate the embedding of an answer a. We will postpone to later to explain why we do not learn a function that generates the joint embedding of the triplet.</p><p>The embedding functions are parameterized by θ and φ, respectively. In this work, we use deep learning models such as multi-layer perceptron (MLP) and Stacked Attention Network (SAN) <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref> (after removing the classifier at the last layer). In principle, any representation network can be used-our focus is on how to use the embeddings.</p><p>Probabilistic Model of Compatibility (PMC) Given a triplet (i n , q n , a ∈ C n ) where a is a correct answer, we define the following probabilistic model</p><formula xml:id="formula_0">p(a|i n , q n ) = exp(f θ (i n , q n ) ⊤ g φ (a)) a ′ ∈A exp(f θ (i n , q n ) ⊤ g φ (a ′ ))<label>(1)</label></formula><p>Discriminative Learning with Weighted Likelihood Given the probabilistic model, it is natural to learn the parameters to maximize its likelihood. In our work, we have found the following weighted likelihood is more effective</p><formula xml:id="formula_1">ℓ = − N n a∈Cn d∈A α(a, d) log P (d|i n , q n ),<label>(2)</label></formula><p>where the weighting function α(a, d) measures how much the answer d could contribute to the objective function. A nature design is</p><formula xml:id="formula_2">α(a, d) = I[a = d],<label>(3)</label></formula><p>where I[·] is the binary indicator function, taking value of 1 if the condition is true and 0 if false. In this case, the objective function reduces to the standard cross-entropy loss if C n is a singleton. However, in section 3.4, we discuss several different designs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Large-scale Stochastic Optimization</head><p>The optimization of eq. <ref type="formula" target="#formula_1">(2)</ref> is very challenging on real Visual QA datasets. There, the size of A can be as large as hundreds of thousands <ref type="bibr" target="#b2">3</ref> . Thus computing the normalization term of the probability model is a daunting task.</p><p>We use a minibatch based stochastic gradient descent procedure to optimize the weighted likelihood. Specifically, we choose B triplets randomly from D (the training dataset defined in section 3.1) and compute the gradient of the weighted likelihood.</p><p>Within a minibatch</p><formula xml:id="formula_3">(i b , q b , C b ) or (i b , q b , C b ∪D b ) for b = 1, 2, · · · B,</formula><p>we construct a minibatched-universe</p><formula xml:id="formula_4">A B = N b=1 (C b D b )<label>(4)</label></formula><p>Namely, all the possible answers in the minibatch are used. However, this "mini-universe" might not be a representative sampling of the true "universe" A. Thus, we augment it with negative sampling. First we compute the set</p><formula xml:id="formula_5">A B = A − A B<label>(5)</label></formula><p>and sample M samples from this set. These samples (denoted as A o ) are mixed with A B to increase the exposure to incorrect answers (i.e. negative samples) encountered by the triplets in a minibatch. In short, we use A 0 A B in lieu of A in computing the posterior probability p(a|i, q) and the likelihood.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Defining the Weighting Function α</head><p>We can take advantage of the weighting function α(a, d) to incorporate external or prior semantic knowledge. For example, α(a, d) can depend on semantic similiarity scores between a and d. Using the WUPS score <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b18">19]</ref>, we define the following rule <ref type="bibr" target="#b2">3</ref> In the Visual Genome dataset <ref type="bibr" target="#b15">[16]</ref>, for example, we have more than 201,000 possible answers.</p><formula xml:id="formula_6">α(a, d) = 1 if WUPS(a, d) &gt; λ, 0 otherwise,<label>(6)</label></formula><p>where λ is a threshold (e.g., 0.9 as in <ref type="bibr" target="#b18">[19]</ref>). α(a, d) can also be used to scale triplets with a lot of semantic similar answers in C (for instance, "apple", "green apple", "small apple" or "big apple" are good answers to "what is on the table?"):</p><formula xml:id="formula_7">α(a, d) = I[a = d] |C|<label>(7)</label></formula><p>such that each of these similar answers only contributes to a fraction of the likelihood to the objective function. The idea of eq. <ref type="formula" target="#formula_7">(7)</ref> has been exploited in several recent work <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b14">15]</ref> to boost the performance on VQA <ref type="bibr" target="#b2">[3]</ref> and VQA2 <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Prediction</head><p>During testing, given the learned f θ and g φ , for the open-ended setting we can apply the following decision rule</p><formula xml:id="formula_8">a * = arg max a∈A f θ (i, q) ⊤ g φ (a),<label>(8)</label></formula><p>to identify the answer to the pair (i, q).</p><p>Note that we have the freedom to choose A again: it can be the same as the "universe of answers" constructed for the training (i.e., the collection of most frequent answers), or a union with all the answers in the validation or testing set. The flexibility is afforded here by using the embedding function g φ to embed any texts. Note that in existing openended Visual QA, the set A is constrained to the most frequent answers, reflecting the limitation of using multi-way classification as a framework for Visual QA tasks.</p><p>This decision rule readily extends to the multiple-choice setting, where we just need to set A to include the correct answer and the incorrect answers in each testing triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Comparison to Existing Algorithms</head><p>Most existing Visual QA algorithms (most working on the open-ended setting on VQA <ref type="bibr" target="#b2">[3]</ref> and VQA2 <ref type="bibr" target="#b8">[9]</ref>) train a multi-way classifier on top of the f θ embedding. The number of classes are set to 1,000 for VQA <ref type="bibr" target="#b6">[7]</ref> and around 3,000 for VQA2 <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b14">15]</ref> of the top-frequency correct answers. These top-frequent answers cover over 90% of the training and 88% of the training and validation examples. Those training examples whose correct answers are not in the top-K frequent ones are simply disregarded during training.</p><p>There are some algorithms also learning a tri-variable compatability function h(i, q, a) <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref>. And the correct answer is inferred by identify a * such that h(i, q, a * ) is the highest. This type of learning is particularly suitable for multiple-choice based Visual QA. Since the number of candidate answers is small, enumerating all possible a is feasible.</p><p>However, for open-ended Visual QA tasks, the number of possible answers is very large-computing the function h() for every one of them is costly. Note that our decision rule relies on computing f θ (i, q)</p><p>⊤ g φ (a), a factorized form of the more generic function h <ref type="figure">(i, q, a)</ref>. However, precisely due to this factorization, we only need to compute f θ (i, q) just once for every pair (i, q). For g φ (a), as long as the model is sufficiently simple, enumerating over many possible a is less demanding than what a generic (and more complex) function h(i, q, a) requires. Indeed, in practice we only need to compute g φ (a) once for any possible a 4 . See section 4.6 for details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We validate our approach on several Visual QA datasets. We start by describing these datasets and the empirical setups. We then report our results. The proposed approach performs very well. It outperforms the corresponding multiway classification-based approaches where the answers are modeled as independent ordinal numbers. Moreover, it outperforms those approaches in transferring models learned on one dataset to another one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We apply the proposed approach to four datasets. <ref type="table" target="#tab_0">Table 1</ref> summarizes their characteristics.</p><p>VQA2 <ref type="bibr" target="#b8">[9]</ref>. The dataset uses images from MSCOCO <ref type="bibr" target="#b16">[17]</ref> with the same training/validation/testing splits and constructs triplets (i n , q n , C n ) of image (i n ), question (q n ), and correct answers (C n ) respectively. On average, 6 questions are generated for each image, and each (i n , q n ) pair is answered by 10 human annotators (i.e, |C n | = 10). The most frequent one is selected as the single correct answer t n .</p><p>Visual7W Telling (Visual7W) <ref type="bibr" target="#b31">[32]</ref> and V7W <ref type="bibr" target="#b4">[5]</ref>. Visual7W uses 47,300 images from MSCOCO <ref type="bibr" target="#b16">[17]</ref> and contains 139,868 (i n , q n , C n , D n ) tuples. The set of correct answers C n is a singleton, containing only one answer. Each has 3 incorrect answers generated by humans (i.e., |D n | = 3). Humans are encouraged to start questions with the 6W words; i.e., "who", "where", "how", "when", "why", and "what". V7W is a revised version of Visual7W, which has <ref type="bibr" target="#b3">4</ref> The answer embedding g(a) for all possible answers (say 100,000) can be pre-computed. At inference we only need to compute the embedding f (i, q) once for an (i, q) pair and perform 100,000 inner products. In contrast, methods like <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref> need to compute h(i, q, a) for 100,000 times. Even if such a function is parameterized with a simple MLP, the computation is much more intensive than an inner product when one has to perform 100,000 times. a more carefully designed set of incorrect answers to prevent machines from ignoring the image, or question or both to exploit the bias in the datasets <ref type="bibr" target="#b4">[5]</ref>. In this dataset, each (i n , q n , C n ) triplet is associated with 6 auto-generated incorrect answers.</p><p>Visual Genome (VG) <ref type="bibr" target="#b15">[16]</ref> and qaVG <ref type="bibr" target="#b4">[5]</ref>. qaVG <ref type="bibr" target="#b4">[5]</ref> is a multiple-choice Visual QA dataset derived from VG <ref type="bibr" target="#b15">[16]</ref>. VG contains 101,174 images from MSCOCO <ref type="bibr" target="#b16">[17]</ref> and has 1,445,322 (i n , q n , C n ) triplets. The set of correct answers C n is a singleton. On average an image is coupled with 14 question-answer pairs. qaVG augments each (i n , q n , C n ) triplet with 6 auto-generated incorrect answers. The dataset is divided into 50%, 20%, and 30% for training, validation, and testing-each portion is a "superset" of the corresponding one in Visual7W or V7W. We train our model on VG <ref type="bibr" target="#b15">[16]</ref> and evaluate it on qaVG <ref type="bibr" target="#b4">[5]</ref>.</p><p>Answer Coverage within Each Dataset. In <ref type="table" target="#tab_1">Table 2</ref>, We show the number of unique answers in each dataset on each split, together with the portions of question and answer pairs covered by the top-K frequent correct answers from the training set. We observe that the qaVG contains the largest number of answers, followed by Visual7W and VQA2. In terms of coverage, we see that the distribution of answers on VQA2 is the most skewed: over 88% of training and validation triplets are covered by the top-1000 frequent answers. On the other hand, Visual7W and qaVG needs more than top-5000 frequent answers to achieve a similar coverage. Thus, a prior, Visual7W and qaVG are "harder" datasets, where a multi-way classification-based open-ended Visual QA model will not perform well unless the number of categories is significantly higher (say ≫ 5000) in order to be able to encounter less frequent answers in the test portion of the dataset-the answers just have a long-tail distribution.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>Our Model. We use two different models to parameterize the embedding function f θ (i, q) in our experimentsMulti-layer Perceptron <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref> (MLP) and Stacked Attention Network <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b14">15]</ref>  <ref type="figure">(SAN)</ref>. For both models, we first represent each token in the question by the 300-dimensional GloVe vector <ref type="bibr" target="#b20">[21]</ref>, and use the ResNet-152 <ref type="bibr" target="#b10">[11]</ref> to extract the visual features following the exact setting of <ref type="bibr" target="#b14">[15]</ref>. Detailed specifications of each model are as follows.</p><p>• Multi-layer Perceptron (MLP): We represent an image by the 2,048-dimensional vector form the top layer of the ResNet-152 pre-trained on ImageNet <ref type="bibr" target="#b22">[23]</ref>, and a question by the average of the GloVe vectors after a linear transformation followed by tanh non-linearity and dropout. We then concatenate the two features (in total 2,348 dimension), and feed them into a one-layer MLP (4,096 hidden nodes and intermediate dropout), with the output dimensionality of 1,024.</p><p>• Stacked Attention Network (SAN): We represent an image by the 14 × 14 × 2048-dimensional tensor, extracted from the second last layer of the ResNet-152 pre-trained on ImageNet <ref type="bibr" target="#b22">[23]</ref>. See <ref type="bibr" target="#b17">[18]</ref> for details. On the other hand, we represent a question by a one layer bidirectional LSTM over GloVe word embeddings. Image and question features are then inputed into the SAN structure for fusion. Specifically, we follow a very similar network architecture presented in <ref type="bibr" target="#b14">[15]</ref>, with the output dimensionality of 1,024.</p><p>For parameterizing the answering embedding function g φ (a), we adopt two architectures: 1) Utilizing a one-layer MLP on average GloVe embeddings of answer sequences, with the output dimensionality of 1,024. 2) Utilizing a twolayer bidirectional LSTM (bi-LSTM) on top of GloVE embeddings of answer sequences. We use MLP for computing answer embedding by default. We denote method with bi-LSTM answer embedding with a postfix ⋆ (e.g. SAN⋆). Please refer to our Suppl. Material for more details about architectures and optimization.</p><p>In the following, we denote our factorized model applying PMC for optimization as fPMC (cf. eq (1)). We consider variants of fPMC with different architectures (e.g. MLP, SAN) for computing f θ (i, q) and g φ (a), named as fPMC(MLP), fPMC(SAN) and fPMC(SAN⋆).</p><p>Competing Methods. We compare our model to multiway classification-based (CLS) models which take either MLP or SAN as f θ . We denote them as CLS(MLP) or CLS(SAN). We set the number of output classes for CLS model to be top-3,000 frequent training answers for VQA2, and top-5,000 for Visual7W and VG. This is a common setup for open-ended Visual QA <ref type="bibr" target="#b0">[1]</ref>.</p><p>Meanwhile, we also re-implement approaches that learn a scoring function h(i, q, a) with its input as (i n , q n , C n ) triplets <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref>. As such methods are initially designed for multiple-choice datasets, the calibration between positive and negative samples needs to be carefully tuned. It is challenging to adapt to 'open-end' settings where the number of negative answers scaled up <ref type="bibr" target="#b4">5</ref> . Therefore, we adapt them to also utilize our PMC framework for training, which optimize stochastic multi-class cross-entropy with negative answers sampling. We name such methods as uPMC (unfactorized PMC) and call its variants as uPMC(MLP) and <ref type="bibr" target="#b4">5</ref> See the Suppl. Material for details LSTM <ref type="bibr" target="#b31">[32]</ref> 55.6 ---MLP <ref type="bibr" target="#b4">[5]</ref> 65.7 52.0 -58.5</p><p>MLP <ref type="bibr" target="#b12">[13]</ref> 67.1 ---C+LSTM <ref type="bibr" target="#b8">[9]</ref> --54.1 -MCB <ref type="bibr" target="#b8">[9]</ref> 62. uPMC(SAN). We also compare to reported results from other state-of-the-art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Evaluation Metrics</head><p>The evaluation metric for each dataset is different. For VQA2, the standard metric is to compare the selected answer a * of a (i, q) pair to the ten corresponding human annotated answers C = {s 1 , · · · , s 10 }. The performance on such an (i, q) pair is set as follows</p><formula xml:id="formula_9">acc(a * , C) = max 1, l I[a * = s l ] 3 .<label>(9)</label></formula><p>We report the average performance over examples in the validation split and test split. For Visaul7W (or V7W), the performance is measured by the portion of correct answers selected by the Visual QA model from the candidate answer set. The chance for random guess is 25% (or 14.3%). For VG, we focus on the multiple choice evaluation (on qaVG). We follow the settings proposed by <ref type="bibr" target="#b4">[5]</ref> and measure multiple choice accuracy. The chance for random guess is 14.3%. <ref type="table" target="#tab_2">Table 3</ref> gives a comprehensive evaluation for most state-of-the-art approaches on four different settings over VQA2(test-dev), Visual7W, V7W and qaVG <ref type="bibr" target="#b5">6</ref> . Among all those settings, our proposed fPMC model outperform the Comparing to other state-of-the-art methods, we show competitive performance against most of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on Individual Visual QA Datasets</head><p>In <ref type="table" target="#tab_2">Table 3</ref>, note that there are differences in the experimental setups in many of the comparison to state-of-the-art methods. For instance, MLP <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b4">5]</ref> used either better text embedding or more advanced visual feature, which benefits their result on Visual7W significantly. Under the same configuration, our model has obtained improvement. Besides, most of the state-of-the-art methods on VQA2 fall into the category of classification model that accommodates specific Visual QA settings. They usually explore better architectures for extracting rich visual information <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b1">2]</ref>, or better fusion mechanisms across multiple modalities <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b29">30]</ref>. We notice that our proposed PMC model is orthogonal to all those recent advances in multi-modal fusion and neural architectures. More advanced deep learning models can be adapted into our framework as f θ (i, q) (e.g. fPMC(MFH)) to achieve superior performance across different settings. This is particularly exemplified by the dominance of SAN over the vanilla MLP model. We leave this for future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>Importance of Negative Sampling Our approach is probabilistic, demanding to compute a proper probability over the space of all possible answers. (In contrast, classification-based models limit their output spaces to a pre-determined number, at the risk of not being able to handle unseen answers).</p><p>In section 3.3, we describe a large-scale optimization technique that allows us to approximate the likelihood by performing negative sampling. Within each mini-batch, we create a mini-universe of all possible answers as the union of all the correct answers (i.e., A B ). Additionally, we randomly sample M answers from the union of all answers outside of the mini-batch, creating "an other world" of all possible answers A o . The A o provides richer negative samples to A B and is important to the performance of our model, as shown in <ref type="table" target="#tab_4">Table 4</ref>.</p><p>We further conducted detailed analysis on the effects of negative sample sizes as shown in <ref type="figure" target="#fig_0">Fig. 2</ref>. With the number of negative samples increasing from 0 to 3,000 for each mini-batch, we observe a increasing trend from the validation accuracy. A significant performance boost is obtained comparing methods with a small number of negative samples to no additional negative samples. The gain then becomes marginal after A o is greater than 2,000. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Transfer Learning Across Datasets</head><p>One important advantage of our method is to be able to cope with unseen answers in the training dataset. This is in stark contrast to multi-way classification based models which will have to skip on those answers as the output categories are selected as top-K most frequent answers from the training dataset.</p><p>Thus, classification based models for Visual QA are not amenable to transfer across datasets where there is a large gap between different spaces of answers. <ref type="table" target="#tab_7">Table 6</ref> illustrates the severity by computing the number of common answers across datasets. On average, about 7% to 10% of the unique answers are shared across datasets. If we restrict the number of answers to consider to top 1,000, about 50% to 65% answers are shared. However, top 1000 most frequent answers are in general not enough to cover all the questions in any dataset. Hence, we arrive at the unexciting observation-we can transfer but we can only answer a few questions! In <ref type="table" target="#tab_6">Table 5</ref>, we report our results of transferring learned Visual QA model from one dataset (row) to another one (column). For VQA2, we evaluate the open-end accuracy using top-3000 frequent answer candidates on validation set. We evaluate multiple-choice accuracy on the test set of Visual7W and qaVG.  The classification models (CLS) clearly fall behind the performance of our method (uPMC and fPMC)-the red upper arrows signify improvement. In some pairs the improvement is significant (e.g., from 42.8% to 54.8% when transferring from Visual7W to qaVG). Furthermore, we noticed that fPMC outperforms uPMC in all transfer settings.</p><p>However, VQA2 seems a particular difficult dataset to be transferred to, from either V7W or qaVG. The improvement from CLS to fPMC is generally small. This is because VQA2 contains a large number of Yes/No answers. For such answers, learning embeddings is not advantageous as there are little semantic meanings to extract from them. We perform another study by removing those answers (and associated questions) from VQA2 and report the transfer learning results in <ref type="table" target="#tab_8">Table 7</ref>. In general, both CLS and fPMC transfer better. Moreover, fPMC improves over CLS by a larger margin than that in <ref type="table" target="#tab_6">Table 5</ref>.</p><p>To gain a deeper understanding towards which component brings the advantage in transfer learning, we performed additional experiments to analyze the difference on seen/unseen answers. At the same time, we include a t-SNE visualization to access the quality of our answer embeddings. We conclude that learned answer embeddings can better capture semantic and syntactic similarities among answers. See the Suppl. Material for details on both analysis.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Inference Efficiency</head><p>Next we study the inference efficiency of the proposed fPMC, uPMC (i.e., triplet based approaches <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b23">24]</ref> with PMC) models with the CLS model. For fair comparison, we use the one-hidden-layer MLP model for all approaches, keep |C| = 1000 and mini-batch size to be 128 (uPMC based approach is memory consuming. More candidates <ref type="table">Table 8</ref>. Efficiency study among CLS(MLP), uPMC(MLP) and fPMC(MLP). The reported numbers are the average inference time of a mini-batch of 128 ( |C| = 1000).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>CLS(MLP) uPMC(MLP) fPMC(MLP) Time (ms) 22.01 367.62 22.14 require reducing the mini-batch size). We evaluate models on the VQA2 validation set (∼2200 mini-batches) and report the (average) mini-batch inference time. <ref type="figure" target="#fig_1">Fig. 3</ref> and <ref type="table">Table 8</ref> show that fPMC(MLP) obtains similar performance to CLS(MLP), with at least 10 times faster than uPMC(MLP). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>We propose a novel approach of learning answer embeddings for the visual question answering (Visual QA) task. The main idea is to learn embedding functions to capture the semantic relationship among answers, instead of treating them as independent categories as in multi-way classification-based models. Besides improving Visual QA results on single datasets, another significant advantage of our approach is to enable better model transfer. The empirical studies on several datasets have validated our approach.</p><p>Our approach is also "modular" in the sense that it can exploit any joint modeling of images and texts (in this case, the questions). An important future direction is to discover stronger multi-modal modeling for this purpose.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Detailed analysis on the size of negative sampling to fPMC(MLP) and fPMC(SAN) at each mini-batch. The reported number is the accuracy on VQA2 (val).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Inference time Vs. Mini-batch index. fPMC(MLP) and CLS(MLP) model are 10x faster than uPMC(MLP) (use PyTorch v0.2.0 + Titan XP + Cuda 8 + Cudnnv5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Summary statistics of Visual QA datasets.</figDesc><table>Dataset 
# of Images # of (i, q, C) triplets (|C|, |D|) 
Name 
train val test train val 
test per tuple 
VQA2 [9] 83K 41K 81K 443K 214K 447K (10, 0) 
Visual7W [32] 14K 5K 8K 69K 28K 42K (1, 3) 
V7W [5] 
14K 5K 8K 69K 28K 42K (1, 6) 
qaVG [5] 
49K 19K 29K 727K 283K 433K (1, 6) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 .</head><label>2</label><figDesc>The answer coverage of each dataset. # of unique answers triplets covered by top K =</figDesc><table>Dataset 
train/val/test/All 
1,000 3,000 
5,000 
VQA2 
22K/13K/ -/29K 
88% 93% 
96% 
Visual7W 63K/31K/43K/108K 57% 68% 
71% 
VG 
119K/57K/79K/201K 61% 72% 
76% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Results (%) on Visual QA with different settings: open- ended (Top-K) and multiple-choice (MC) based for different datasets. The omitted ones are due to their missing in the cor- responding work.</figDesc><table>Visual7W V7W 
VQA2 
qaVG 
Method 
MC [32] MC [5] Top-3k [9] MC [5] 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 .</head><label>4</label><figDesc>The effect of negative sampling (M = 3, 000) on fPMC. The number is the accuracy in each question type on VQA2 (val).</figDesc><table>Method Mini-Universe Y/N Number Other All 
MLP 
A B 
70.1 33.0 
38.7 49.8 
SAN 
78.2 37.1 
45.7 56.7 
MLP 
A o A B 
76.6 36.1 
43.9 55.2 
SAN 
79.0 38.0 
51.3 60.0 

corresponding classification model by a noticeable margin. 
Meanwhile, fPMC outperforms uPMC over all settings. 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>The Effect of Incorporating Semantic Knowledge in Weighted Likelihood In section 3.2, we have introduced the weighting function α(a, d) to measure how much an in- correct answer d should contribute to the overall objective function. In particular, this weighting function can be used to incorporate prior semantic knowledge about the relation- ship between a correct answer a and an incorrect answer d. We report the details in the Suppl. Material.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Results of cross-dataset transfer using either classification-based models or our models (PMC) for Visual QA. (f θ = SAN) Visual7W VQA2 qaVG CLS uPMC fPMC fPMC⋆ CLS uPMC fPMC fPMC⋆ CLS fPMC fPMC fPMC⋆ Visual7W 53.7 65.3 65.6 66.0 ↑ 19.1 18.5 19.8 ↑ 19.1 42.8 52.2 54.8 ↑ 54.</figDesc><table>3 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 .</head><label>6</label><figDesc>The # of common answers across datasets (training set)</figDesc><table>Top-K most frequent answers 
Total # of 
Dataset 
1K 3K 5K 10K all unique answers 
VQA2, Visual7W 451 1,262 2,015 3,585 10K 
137K 
VQA2, qaVG 495 1,328 2,057 3,643 11K 
149K 
Visual7W, qaVG 657 1,890 3,070 5,683 27K 
201K 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 7 .</head><label>7</label><figDesc>Transferring is improved on the VQA2 dataset without Yes/No answers (and the corresponding questions) (f θ = SAN).</figDesc><table>Dataset 
CLS uPMC fPMC fPMC⋆ 
Visual7W 31.7 
29.5 
33.1 ↑ 
32.0 
qaVG 
42.6 
39.3 
43.0 
43.4 ↑ 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">The omitted ones are due to their missing in the corresponding work. In fact, most existing work only focuses on one or two datasets.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vqa</surname></persName>
		</author>
		<title level="m">Visual question answering. IJCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bottom-up and top-down attention for image captioning and vqa</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Vqa: Visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Being negative but constructively: Lessons learnt from creating better visual question answering datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NAACL</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Devise: A deep visual-semantic embedding model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are you talking to a machine? dataset and methods for multilingual image question</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2296" to="2304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making the v in vqa matter: Elevating the role of image understanding in visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Gupta</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.03865</idno>
		<title level="m">Survey of visual question answering: Datasets and techniques</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple loss function for improving the convergence and accuracy of visual question answering models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Visual question answering: Datasets, algorithms, and future challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
	<note>Computer Vision and Image Understanding</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Show, ask, attend, and answer: A strong baseline for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elqursh</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03162</idno>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A multi-world approach to question answering about real-world scenes based on uncertain input</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Zero-shot learning by convex combination of semantic embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Frome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Exploring models and data for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="page" from="211" to="252" />
		</imprint>
	</monogr>
	<note>ImageNet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visual question answering: A survey of methods and datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V D</forename><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">163</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="21" to="40" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Verbs semantics and lexical selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Palmer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACL</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Ask, attend and answer: Exploring question-guided spatial attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Beyond bilinear: Generalized multi-modal factorized highorder pooling for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chenchao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03619</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-modal factorized bilinear pooling with co-attention learning for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jianping</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dacheng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
