<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yue</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bin</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><forename type="middle">Wang</forename><surname>Kliss</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moe</forename><forename type="middle">;</forename></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Software</orgName>
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software Beijing Key Laboratory for Industrial Big Data System and Application</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">HashGAN: Deep Learning to Hash with Pair Conditional Wasserstein GAN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0">  </div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In the big data era, large-scale and high-dimensional media data has been pervasive in search engines and social networks. To guarantee retrieval quality and computation efficiency, approximate nearest neighbors (ANN) search has attracted increasing attention. Parallel to the traditional indexing methods <ref type="bibr" target="#b20">[21]</ref>, another advantageous solution is hashing methods <ref type="bibr" target="#b36">[37]</ref>, which transform high-dimensional media data into compact binary codes and generate similar binary codes for similar data items. This paper will focus on the learning to hash methods <ref type="bibr" target="#b36">[37]</ref> that build data-dependent hash encoding schemes for efficient image retrieval, which have shown better performance than data-independent hashing methods, e.g. Locality-Sensitive Hashing (LSH) <ref type="bibr" target="#b11">[12]</ref>.</p><p>Many learning to hash methods have been proposed to enable efficient ANN search by Hamming ranking of compact binary hash codes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b40">41]</ref>. Recently, * Corresponding author: M. Long (mingsheng@tsinghua.edu.cn).</p><p>deep learning to hash methods <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref> have shown that deep neural networks can enable end-toend representation learning and hash coding with nonlinear hash functions. These deep learning to hash methods have shown state-of-the-art results on many datasets. In particular, it proves crucial to jointly learn similarity-preserving representations and control quantization error of converting continuous representations to binary codes <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b5">6]</ref>.</p><p>However, the encouraging performance comes only by large-scale image data where sufficient supervised information is available in the forms of pointwise labels or pairwise similarity. In many image retrieval applications, the supervised information available may be insufficient, especially for new domains. And it is usually costly or even prohibitive to annotate sufficient training data for deep learning to hash. Subject to the scarcity of similarity information, existing deep learning to hash methods <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6]</ref> may overfit the training images and result in substantial loss of retrieval quality. This paper presents HashGAN, a novel deep architecture for deep learning to hash, which learns compact binary hash codes from both real images and large-scale synthesized images. We propose a novel Pair Conditional Wasserstein GAN (PC-WGAN), which synthesizes discriminative and diverse images by conditioning on the pairwise similarity information. To the best of our knowledge, PC-WGAN is the first GAN that enables image synthesis by incorporating pairwise similarity information. Well-specified loss functions including cosine cross-entropy loss and cosine quantization loss are proposed for similarity-preserving learning and quantization error control. The proposed HashGAN can be trained end-to-end by back-propagation in a minimax optimization mechanism. Extensive experiments demonstrate that HashGAN can generate high-quality binary hash codes and yield state-of-the-art multimedia retrieval performance on three datasets, NUS-WIDE, CIFAR-10, and MS-COCO.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Hashing Methods. Existing hashing methods <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41]</ref> consist of unsupervised and supervised hashing. Please refer to <ref type="bibr" target="#b36">[37]</ref> for a comprehensive survey.</p><p>Unsupervised hashing methods learn hash functions that encode data to binary codes by training from unlabeled data <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b25">26]</ref>. Supervised hashing further explores supervised information (e.g. pairwise similarity or relevance feedback) to generate compact hash codes <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b33">34]</ref>. Recently, deep learning to hash methods yield breakthrough results on image retrieval datasets by blending the power of deep learning <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20]</ref>. In particular, DHN <ref type="bibr" target="#b41">[42]</ref> is the first end-to-end framework that jointly preserves pairwise similarity and controls the quantization error. HashNet <ref type="bibr" target="#b5">[6]</ref> improves DHN by balancing the positive and negative pairs in training data to trade of precision vs. recall, and by continuation technique for lower quantization error, which obtains state-of-the-art performance on several benchmark datasets.</p><p>Generative Models. Generative Adversarial Networks (GANs) <ref type="bibr" target="#b13">[14]</ref> are powerful models for generating images in a minimax game mechanism without requiring supervised information. State-of-the-art unsupervised generative models for image synthesis include Deep Convolutional GANs (DCGANs) <ref type="bibr" target="#b30">[31]</ref> and Wasserstein GANs (WGANs) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b14">15]</ref>. Recently, a more powerful family of generative models synthesize images with GANs by further conditioning on supervised information (e.g., class labels or text descriptions) <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b31">32]</ref>. Auxiliary Classifier GAN (AC-GAN) <ref type="bibr" target="#b28">[29]</ref> is the state-of-the-art solution to integrate supervised information by feeding it into the generator and adding a loss function to account for the supervised information in the discriminator.</p><p>Existing supervised generative models only incorporate pointwise supervised information, e.g. class labels or text descriptions. However, in many real retrieval applications, we only have pairwise similarity information for training hashing models <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b5">6]</ref>. Deep Semantic Hashing (DSH) <ref type="bibr" target="#b29">[30]</ref> is the first hashing method that explores GANs for image synthesis, but it can only incorporate pointwise side information (class labels) which is often unavailable in online image retrieval applications. Different from previous methods, we propose a new HashGAN architecture, which consists of a specifically-designed Pair Conditional Wasserstein GAN (PC-WGAN) that enables incorporation of pairwise similarity information for generating diverse synthetic images, and a deep hashing network trained with both real and synthetic images to generate nearly lossless hash codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">HashGAN</head><p>In similarity retrieval systems, we are given N training points</p><formula xml:id="formula_0">X = {x i } N i=1</formula><p>, where some pairs of points x i and x j are given with pairwise similarity labels s ij , where s ij = 1 if x i and x j are similar while s ij = 0 if x i and x j are dissimilar. The goal of deep learning to hash is to learn nonlinear hash function F : x → h ∈ {−1, 1}</p><p>K from input space to Hamming space {−1, 1} K using deep neural networks, which encodes each point x into compact K-bit hash code h = F (x) such that the similarity information S between the given pairs can be preserved in the compact hash codes. In supervised hashing, the similarity pairs S = {s ij } can be constructed from semantic labels of data points or relevance feedback from click-through data in online search systems. This paper presents HashGAN, a deep learning to hash architecture with a novel Pair Conditional Wasserstein GAN (PC-WGAN) specifically designed for generative learning from images with pairwise similarity information. <ref type="figure" target="#fig_0">Figure 1</ref> shows the architecture of HashGAN, which consists of two main components. (1) A pair conditional Wasserstein GAN (PC-WGAN), which takes as inputs the training images and pairwise similarity and jointly learns a generator G and a discriminator D: the generator G accepts as input the concatenation of a random noise u and an embedding vector v that encodes the similarity information to synthesize nearly real images; the discriminator D tries to distinguish the real and synthetic images using the adversarial loss. (2) A hash encoder F , which generates compact binary hash codes h for all images in a Bayesian learning framework: the framework jointly preserves the similarity information of both the real and synthetic images by a cosine cross-entropy loss and controls the quantization error by a cosine quantization loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Pair Conditional WGAN</head><p>The training strategy of generative adversarial networks (GANs) <ref type="bibr" target="#b13">[14]</ref> defines a minimax game between two competing networks: a generator network G that captures underlying data distribution of real images for synthesizing images, and a discriminator network D that distinguishes the real images from synthetic images. Specifically, the generator G accepts as input a random noise u, which is sampled from some simple noise distribution such as uniform distribution or spherical Gaussian distribution, and synthesizes a fake imagex = G(u); the discriminator D takes as inputs either a real image x or a synthetic imagex and must distinguish them by minimizing the classification error of probabilities D(x) and D(x). To tackle the training difficulty of GANs, <ref type="bibr" target="#b14">[15]</ref> proposes an improved training strategy of Wasserstein GAN (WGAN) <ref type="bibr" target="#b0">[1]</ref>, which trains the discriminator through the Wasserstein distance that is continuous everywhere and differentiable almost everywhere, and proposes to enforce a differentiable Lipschitz constraint with gradient penalty as</p><formula xml:id="formula_1">min D L D = Ẽ x∼Pg [D (x)] − E x∼Pr [D (x)] + γ Ê x∼Px ( ∇xD(x) 2 − 1) 2 ,<label>(1)</label></formula><p>where γ is the penalty coefficient typically set as γ = 10, P r is the real data distribution, P g is the generator distribution implicitly defined byx = G(u), and Px is implicitly defined as sampling uniformly along straight lines between pairs of points sampled from the real data distribution P r (1) A pair conditional Wasserstein GAN (PC-WGAN), which takes as inputs the training images and pairwise similarity and jointly learns a generator G and a discriminator D: the generator G accepts as input the concatenation of a random noise u and an embedding vector v that encodes the similarity information to synthesize nearly real images; the discriminator D tries to distinguish the real and synthetic images using the adversarial loss. (2) A hash encoder F , which generates compact binary hash codes h for all images in a Bayesian learning framework: the framework jointly preserves the similarity information of both the real and synthetic images by a cosine cross-entropy loss and minimizes the quantization error by a cosine quantization loss. Best viewed in color.</p><p>and the generator distribution P g . In the minimax game, the generator is trained to maximize probability of classifying synthetic images as real, which is equivalent to minimizing</p><formula xml:id="formula_2">min G L G = E u∼Pu [−D (G(u))] ,<label>(2)</label></formula><p>where u is a random noise sampled from some simple noise distribution P u . The goal of the generator is to maximally fool the discriminator with nearly real synthesized images. This improved WGAN <ref type="bibr" target="#b14">[15]</ref> performs better than standard WGAN and enables stable and efficient training of various GAN architectures with almost no hyper-parameter tuning. While WGAN with stabilized training strategy <ref type="bibr" target="#b14">[15]</ref> can synthesize good images from random noises, it cannot take the advantage of useful side information. Class conditional synthesis can significantly improve the quality of generated samples <ref type="bibr" target="#b28">[29]</ref>, by supplying both the generator and discriminator with class labels to produce class conditional samples <ref type="bibr" target="#b26">[27]</ref>. The auxiliary classifier GAN (AC-GAN) <ref type="bibr" target="#b28">[29]</ref> is the state-of-the-art solution to integrate side information, which enables conditioning by feeding the supervised information into the generator and adding a new loss function with the supervised information in the discriminator. The generator synthesizes images from the inputs combined of supervised information and random noise, and the discriminator jointly distinguishes different classes as well as real from synthetic.</p><p>A major disadvantage of AC-GAN is that it can only be conditioned on point-wise supervised information, such as class labels or feature vectors from counterpart modalities. However, in deep learning to hash, we only have data X =</p><formula xml:id="formula_3">{x i } N i=1</formula><p>with pairwise similarity information S = {s ij }. A naive solution to applying AC-GAN on data with pairwise information is that for each image x i , use each row s i· ∈ R N of the similarity matrix S as the supervised information. Unfortunately, this solution is infeasible since the number N of training points is usually larger than several thousands in deep learning to hash, whereas AC-GAN with such highdimensional inputs cannot be trained successfully. Hence, it still remains an open problem how to enable GANs and WGANs conditioned on pairwise supervised information.</p><p>In this paper, we propose Pair Conditional WGAN (PC-WGAN), a new extension of WGAN to learn from data with pairwise supervised information {X , S}. At first, we reduce the high-dimension of pointwise supervised information s i· by a similarity embedding approach, which embeds the similarity information s i· associated with each image to a lowdimensional vector v i ∈ R V . The similarity embedding can be attained by minimizing the following reconstruction loss</p><formula xml:id="formula_4">min vi 0| N i=1 L V = sij ∈S s ij − v T i v j 2 ,<label>(3)</label></formula><p>where L V is the similarity embedding loss, and the nonnegative constraints are imposed to make the latent embeddings consistent with prior supervised information, which is given as nonnegative similarity labels {s ij }. Since s ij ≈ v T i v j , each embedding vector v i can approximately represent the similarity information of each point x i with V -dimension, which is low-dimensional and can be fed as inputs to GANs.</p><p>In PC-WGAN, each generated point has a corresponding embedding vector v i ∼ P v in addition to the random noise u i ∈ R V . The generator uses both embedding vector and random noise to generate every image asx i = G(v i , u i ). The discriminator should give two probability distributions: one over the synthetic vs real as D (x) and D (x) for binary classification, and another over the similar vs dissimilar in all image pairs as C (x i , x j ) and 1−C (x i , x j ) for pairwise classification. More specifically, the discriminator network (except the last classifier layer) is shared between D and C. Denote byz and z the last-layer activations of network C for pairwise classification then C (</p><formula xml:id="formula_5">x i , x j ) = 1 1+exp(−z T i zj ) .</formula><p>The overall loss for training discriminator of PC-WGAN is</p><formula xml:id="formula_6">min D,C L D,C = Ẽ x∼Pg [D (x)] − E x∼Pr [D (x)] + γ Ê x∼Px ( ∇xD(x) 2 − 1) 2 − sij ∈S s ij log C (x i , x j ) − sij ∈S (1 − s ij ) log (1 − C (x i , x j )),<label>(4)</label></formula><p>where the third and fourth rows are the cross-entropy loss between probability C (x i , x j ) and pairwise similarity s ij . In the minimax game, the generator is trained to maximize probabilities of synthetic being real as well as similar being dissimilar or vice versa, which is equivalent to minimizing</p><formula xml:id="formula_7">min G,V L G,V = E v∼Pv u∼Pu [−D (G (v, u))] + sij ∈S s ij − v T i v j 2 − sij ∈S s ij log C (x i , x j ) − sij ∈S (1 − s ij ) log (1 − C (x i , x j )),<label>(5)</label></formula><p>and note thatx i = G(v i , u i ). The goal of the generator is to maximally fool the discriminator with synthetic images generated from the similarity embedding and random noise.</p><p>In applications, the size of training data with similarity information is remarkably smaller than the size of complete unlabeled data. We enable PC-WGAN to learn from both labeled data and unlabeled data to synthesize high-quality images by further using zero embedding vector v j = 0 for each unlabeled image x j / ∈ X . The generator distribution P g changes to G(v i , u i ) ∪ G(0, u j ), and P r becomes distributions of both supervised and unsupervised real images. Though both P g and P r are changed due to unlabeled data, the PC-WGAN objectives in (4) and (5) remain unchanged.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Deep Learning to Hash</head><p>The PC-WGAN trained on images with pairwise similarity information can generate high-quality synthetic images, which can be used to boost the performance of deep learning to hash over images with insufficient similarity labels.</p><p>In this paper, we propose a hash encoder network F , which generates compact hash codes for both synthetic and real images in a Bayesian framework. The hash encoder consists of three components: (1) a deep convolutional network (CNN) for learning deep compact codes h i = F (x i ) for each input imagex i , wherex i can be a real image x with similarity information or a synthetic imagex generated by PC-WGAN with similarity information; (2) a cosine crossentropy loss for similarity-preserving hash learning; and (3) a cosine quantization loss for controlling quantization error.</p><p>Given training data X = {x i } N i=1 and synthetic images X = {x j } M j=1 , we can expand the training data to X ∪X and the similarity labels to S = {s ij }</p><note type="other">N +M i,j=1 for deep hashing. The logarithm Maximum a Posteriori (MAP) estimation of hash codes H</note><formula xml:id="formula_8">= [h 1 , . . . , h N +M ] given S and X ∪X is log P (H|S) ∝ log P (S|H) P (H) = s ij ∈S wij log P (sij|hi, hj) + N +M i=1</formula><p>log P (hi), <ref type="bibr" target="#b5">(6)</ref> where</p><formula xml:id="formula_9">P (S|H) = sij ∈S [P (s ij |h i , h j )]</formula><p>wij is weighted likelihood function, and w ij is the weight for each training pair (x i ,x j , s ij ), which tackles the data imbalance problem by weighting the training pairs according to the importance of misclassifying that pair <ref type="bibr" target="#b7">[8]</ref>. Since each similarity label in S can only be s ij = 1 or s ij = 0, to account for the data imbalance between similar and dissimilar pairs, we propose</p><formula xml:id="formula_10">w ij = |S| / |S 1 | , s ij = 1 |S| / |S 0 | , s ij = 0<label>(7)</label></formula><p>where S 1 = {s ij ∈ S : s ij = 1} is the set of similar pairs and S 0 = {s ij ∈ S : s ij = 0} is the set of dissimilar pairs. For each pair, P (s ij |h i , h j ) is the conditional probability of similarity label s ij given a pair of hash codes h i and h j , which can be naturally defined as pairwise logistic function,</p><formula xml:id="formula_11">P (sij|hi, hj) = σ (cos (hi, hj)) , sij = 1 1 − σ (cos (hi, hj)) , sij = 0 = σ(cos (hi, hj)) s ij (1 − σ (cos (hi, hj))) 1−s ij<label>(8)</label></formula><p>where σ (h) = 1/(1 + e −αh ) is adaptive sigmoid function. Similar to logistic regression, we can see that the smaller the Hamming distance dist H (h i , h j ) is, the larger the cosine similarity cos (h i , h j ) as well as the conditional probability P (1|h i , h j ) will be, implying that the image pairx i and x j should be classified as similar; otherwise, the larger the conditional probability P (0|h i , h j ) will be, implying that the image pairx i andx j should be classified as dissimilar. Hence, Equation <ref type="formula" target="#formula_11">(8)</ref> is a reasonable extension of the logistic regression classifier to the pairwise classification scenario, which is optimal for binary similarity labels s ij ∈ {0, 1}.</p><p>Since discrete optimization of Equation <ref type="formula">(6)</ref> with binary constraints h i ∈ {−1, 1} K is very challenging, continuous relaxation h i ∈ R K is applied to the binary constraints for ease of optimization, as adopted by most hashing methods <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b41">42]</ref>. To control the quantization error h i −sgn(h i ) of continuous relaxation and close the gap between Hamming distance and cosine distance for learning high-quality hash codes, we propose a novel bimodal Gaussian prior for h i as</p><formula xml:id="formula_12">P (h i ) = 1 2ǫ exp − 1 ǫ |h i | h i − 1 √ K 2 2 ,<label>(9)</label></formula><p>where ǫ is the diversity parameter of bimodal Gaussian distribution, and 1 ∈ R K is the vector of ones with norm √ K. By taking Equations <ref type="formula" target="#formula_11">(8)</ref> and <ref type="formula" target="#formula_12">(9)</ref> into the MAP estimation in Equation <ref type="formula">(6)</ref>, we obtain the optimization problem of the hash encoder F for learning compact hash codes as follows</p><formula xml:id="formula_13">min F L F = sij ∈S w ij log (1 + exp (α cos (h i , h j ))) − sij ∈S w ij s ij α cos (h i , h j ) − β N +M i=1 cos (|h i | , 1),<label>(10)</label></formula><p>β is the parameter to balance the weight between the cosine cross-entropy loss in the first and second rows of Eq. (10) and the cosine quantization loss in the third row of (10).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">HashGAN Optimization</head><p>This paper establishes deep learning to hash for images with pairwise similarity information, which constitutes two key components: Pair Conditional Wasserstein GAN (PC-WGAN) for generating nearly real images and Deep Hash Encoder for generating compact hash codes for each image. The overall optimization problem is a unified integration of the PC-WGAN objective in Equations <ref type="formula" target="#formula_6">(4) (5)</ref> and the Deep Hash Encoder objective in Equation <ref type="bibr" target="#b9">(10)</ref>. As the proposed HashGAN architecture is a variant of GANs, the two-player minimax game mechanism is adopted for the optimization. The optimization problems for discriminator D, generator G and hash encoder F are respectively computed as follows</p><formula xml:id="formula_14">min D,C L F + λL D,C , min G,V L F + λL G,V , min F L F ,<label>(11)</label></formula><p>where λ is a parameter to trade of the importance of deep hash encoder and PC-WGAN. The network parameters can be efficiently optimized through standard back-propagation using automatic differentiation techniques by TensorFlow.</p><p>Finally, we obtain hash code for each image by simple binarization h ← sgn(h), where sgn(·) is the sign function. Through the minimax optimization in Equation (11), we can synthesize nearly real images with pairwise information by the proposed PC-WGAN, and generate nearly lossless hash codes by similarity-preserving learning and quantization error minimization from both real and synthetic images. It is worth noting that, we can alleviate the difficulty in learning with insufficient supervised information by using both real and synthetic data for deep learning to hash, which yields higher quality hash codes for improved search performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the efficacy of the proposed HashGAN approach with state-of-the-art shallow and deep hashing methods on three benchmark datasets. Codes and configurations will be available at: https://github.com/thuml. <ref type="bibr" target="#b6">[7]</ref> is a public image dataset which contains 269,648 images in the 81 ground truth categories. We follow similar experimental protocols in <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6]</ref>, and randomly sample 5,000 images as the query points, with the remaining images used as the database and randomly sample 10,000 images from the database as the training points.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup NUS-WIDE</head><p>CIFAR-10 is a public dataset with 60,000 images in 10 classes. We follow protocol in <ref type="bibr" target="#b4">[5]</ref> to randomly select 100 images per class as the query set, 500 images per class as the training set, and the rest images are used as the database.</p><p>MS-COCO <ref type="bibr" target="#b22">[23]</ref> is a widely-used image dataset for image recognition, segmentation and captioning. The current release contains 82,783 training images and 40,504 validation images, where each image is labeled by some of the 80 semantic concepts. We randomly sample 5,000 images as query points, with the rest used as the database, and randomly sample 10,000 images from the database for training.</p><p>Following standard evaluation protocol as previous work <ref type="bibr" target="#b39">[40,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b5">6]</ref>, the similarity information for hash function learning and for ground-truth evaluation is constructed from image labels: if two images i and j share at least one label, they are similar and s ij = 1, otherwise they are dissimilar and s ij = 0. Though we use the ground truth image labels to construct the similarity information, the proposed Hash-GAN can learn compact binary hash codes when only the similarity information is available, which is more general than many label-information based hashing methods <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b2">3]</ref>.</p><p>We compare retrieval performance of HashGAN with eight state-of-the-art hashing methods, including supervised shallow hashing methods BRE <ref type="bibr" target="#b18">[19]</ref>, ITQ-CCA <ref type="bibr" target="#b12">[13]</ref>, KSH <ref type="bibr" target="#b24">[25]</ref>, SDH <ref type="bibr" target="#b33">[34]</ref>, and supervised deep hashing methods CNNH <ref type="bibr" target="#b39">[40]</ref>, DNNH <ref type="bibr" target="#b19">[20]</ref>, DHN <ref type="bibr" target="#b41">[42]</ref> and HashNet <ref type="bibr" target="#b5">[6]</ref>. We evaluate retrieval quality based on four standard evaluation metrics: Mean Average Precision (MAP), Precision-Recall  <ref type="bibr" target="#b33">[34]</ref> 0 curves (PR), Precision curves within Hamming distance 2 (P@H≤2), and Precision curves with respect to the numbers of top returned samples (P@N). For direct comparison to published results, all methods use identical training and test sets. We follow HashNet <ref type="bibr" target="#b5">[6]</ref> and DHN <ref type="bibr" target="#b41">[42]</ref> and adopt MAP@5000 for NUS-WIDE dataset, MAP@5000 for MS-COCO dataset, and MAP@54000 for CIFAR-10 dataset.</p><p>For shallow hashing methods, we use as image features the 4096-dimensional DeCAF 7 features <ref type="bibr" target="#b8">[9]</ref>. For deep hashing methods, we use as input the original images, and adopt AlexNet <ref type="bibr" target="#b17">[18]</ref> as the backbone architecture. We follow <ref type="bibr" target="#b14">[15]</ref> and adopt a four-layer ResNet <ref type="bibr" target="#b15">[16]</ref> architecture for the discriminator and generator in HashGAN, which is proved to generate high quality images with 64 × 64 pixels. We adopt AlexNet <ref type="bibr" target="#b17">[18]</ref> as the hash encoder, fine-tune all layers but the last one copied from the pre-trained AlexNet. As the last layer is trained from scratch, we set its learning rate to be 10 times that of the lower layers. We use mini-batch stochastic gradient descent (SGD) with 0.9 momentum as the solver, and cross-validate the learning rate from 10 −5 to 10 −2 with a multiplicative step-size 10 1 2 . We fix the mini-batch size of images as 256 and the weight decay parameter as 0.0005. We cross-validate the dimension of the embedding vector v, and observe that fixing this hyper-parameter as 32 is enough to achieve satisfiable results. Also, HashGAN is not sensitive to different dimensions given that the dimension of v is large enough, e.g. 32. We select the parameters of all comparison methods through cross-validation on training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The MAP results of all methods are demonstrated in Table 1, which shows that the proposed HashGAN substantially outperforms all the comparison methods by large margins. Specifically, compared to SDH, the best shallow hashing method with deep features as input, HashGAN achieves absolute increases of 11.0%, 19.5% and 15.9% in average MAP on NUS-WIDE, CIFAR-10, and MS-COCO respectively. HashGAN outperforms HashNet, the state-of-the-art deep hashing method, by large margins of 3.9%, 5.3% and 0.9% in average MAP on the three datasets, respectively.</p><p>The MAP results reveal several interesting insights. (1) Shallow hashing methods cannot learn discriminative deep representations and compact hash codes through end-to-end framework, which explains the fact that they are surpassed by deep hashing methods. (2) Deep hashing methods DHN and HashNet learn less lossy hash codes by jointly preserving similarity information and controlling the quantization error, which significantly outperform pioneering methods CNNH and DNNH without reducing the quantization error.</p><p>The proposed HashGAN improves substantially from the state-of-the-art HashNet by two important perspectives: (1) HashGAN integrates a novel Pair Conditional Wasserstein GAN (PC-WGAN) to synthesize nearly real images as training data, which substantially increases the diversity of training data and alleviates the technical difficulty of insufficient supervised information in many real applications. (2) HashGAN adopts new cosine cross-entropy loss and cosine quantization loss, which can approximate the Hamming distance more accurately to learn nearly lossless hash codes.</p><p>The performance in Precision within Hamming radius 2 (P@H≤2) is very important for efficient image retrieval since such Hamming ranking only requires O(1) time cost for each query, which enables really fast pruning. As shown in <ref type="figure">Figures 2(a), 3(a) and 4(a)</ref>, HashGAN achieves the highest P@H≤2 results on all three benchmark datasets using different numbers of bits. This validates that HashGAN can learn compacter hash codes than all comparison methods to establish more efficient and accurate Hamming ranking. When using longer hash codes, the Hamming space will become higher-dimensional and more sparse such that fewer data points will fall in the Hamming ball within radius 2. This explains why most existing hashing methods perform worse in terms of P@H≤2 criterion with longer hash codes. precision at lower recall levels or smaller number of top samples. This is very desirable for precision-first retrieval, which is widely implemented in practical retrieval systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis 4.3.1 Ablation Study</head><p>We investigate four variants of HashGAN: (1) HashGAN-B is the HashGAN variant without binarization (h ← sgn(h) is not performed), which may serve as the upper bound of retrieval performance; (2) HashGAN-Q is the HashGAN variant without using the proposed quantization loss <ref type="bibr" target="#b9">(10)</ref>, in other words β = 0; (3) HashGAN-C is the HashGAN variant by replacing the cosine cross-entropy loss in (10) with the widely-used inner-product cross-entropy loss <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6]</ref>; (4) HashGAN-G is the HashGAN variant by removing the proposed Pair Conditional Wasserstein GAN (PC-WGAN), i.e. only the hash encoder is trained to generate hash codes and λ = 0. The MAP results with respect to different code lengths on three benchmark datasets are reported in <ref type="table">Table 2</ref>.</p><p>Pair Conditional Wasserstein GAN. <ref type="table">Table 2</ref> shows that HashGAN significantly outperforms HashGAN-G by large margins of 2.5%, 3.8% and 1.8% in average MAP on three datasets, respectively. Without generating high-quality and nearly real synthetic images using PC-WGAN, the diversity of training images for deep learning to hash may be limited, which will lead to overfitting when the pairwise similarity information is insufficient and to worse search performance. The proposed PC-WGAN turns out to be the most important underpinning, which helps HashGAN achieve the state-ofthe-art retrieval performance in various evaluation metrics. Besides, we feed the images generated by PC-WGAN to the best baselines DHN <ref type="bibr" target="#b41">[42]</ref> and HashNet <ref type="bibr" target="#b5">[6]</ref>, which can also outperform the traditional methods by 2.0% on average.</p><p>Cosine Cross-Entropy Loss. <ref type="table">Table 2</ref> shows that Hash-GAN outperforms HashGAN-C by 2.0%, 2.5% and 1.9% in average MAP on the three datasets. HashGAN-C uses the widely-used inner-product cross-entropy loss <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6]</ref> which achieves state-of-the-art results on previous retrieval tasks. In real search engines, cosine similarity is widely used to mitigate the diversity of vector lengths and improve retrieval quality, while it has not been integrated with cross-entropy loss for supervised hash learning <ref type="bibr" target="#b36">[37]</ref>. We propose a novel cosine cross-entropy loss (10) based on cosine similarity, which can better approximate the Hamming distance and preserve the similarity information of training image pairs.</p><p>Cosine Quantization Loss. By jointly optimizing the cosine cross-entropy loss and the cosine quantization loss over deep representations of both real and synthetic images, HashGAN incurs small average MAP decreases of 3.0%, 2.0%, and 2.6% when binarizing continuous representations of HashGAN-B. In contrast, without optimizing the cosine quantization loss (10), HashGAN-Q suffers from very large MAP decreases of 4.7%, 3.8%, and 5.3%, and substantially underperforms HashGAN. These results in <ref type="table">Table 2</ref> validate that the cosine quantization loss (10) can effectively reduce the binarization error and yield nearly lossless hash coding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Visualization Study</head><p>Visualization of Hash Codes by t-SNE. <ref type="figure" target="#fig_3">Figure 5</ref> shows the t-SNE visualizations <ref type="bibr" target="#b34">[35]</ref> of the hash codes learned by the proposed HashGAN approach and the best deep hashing method HashNet <ref type="bibr" target="#b5">[6]</ref> on the CIFAR-10 dataset. We observe that the hash codes learned by HashGAN exhibit clear discriminative structures where the hash codes in different categories are well separated, while the hash codes generated by HashNet exhibit relative vague structures. This validates that by introducing the novel pair conditional WGAN into deep hashing, the hash codes generated through the proposed HashGAN are more discriminative than that generated by HashNet, enabling more accurate image retrieval. (left) and real images randomly selected from the dataset (right). We can observe that the synthetic images are nearly real and semantically relevant to each class but are much more diverse, which can improve the quality of hash codes. <ref type="figure">Figure 7</ref> illustrates synthetic (left) and real (right) image samples on NUS-WIDE, which is a multi-label (81 labels) dataset with high-resolution images and is more difficult to generate high-quality images. In both cases, HashGAN can generate plausible images to improve retrieval performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper tackles deep learning to hash with insufficient similarity information by image synthesis from generative models. The proposed HashGAN can synthesize nearly real images conditioned on the pairwise similarity information, with more diverse synthesized images to improve the quality of compact binary hash codes. Extensive empirical results demonstrate that HashGAN can yield state-of-the-art multimedia retrieval performance on standard benchmarks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. HashGAN for deep learning to hash with a new Pair Conditional Wasserstein GAN (PC-WGAN). The architecture of the proposed HashGAN consists of two main components. (1) A pair conditional Wasserstein GAN (PC-WGAN), which takes as inputs the training images and pairwise similarity and jointly learns a generator G and a discriminator D: the generator G accepts as input the concatenation of a random noise u and an embedding vector v that encodes the similarity information to synthesize nearly real images; the discriminator D tries to distinguish the real and synthetic images using the adversarial loss. (2) A hash encoder F , which generates compact binary hash codes h for all images in a Bayesian learning framework: the framework jointly preserves the similarity information of both the real and synthetic images by a cosine cross-entropy loss and minimizes the quantization error by a cosine quantization loss. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .Figure 3 .Figure 4 .</head><label>234</label><figDesc>Figure 2. The experimental results of HashGAN and comparison methods on the NUS-WIDE dataset under three evaluation metrics.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. The t-SNE visualizations of the hash codes respectively learned by HashGAN and HashNet on the CIFAR-10 dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .Figure 7 .</head><label>67</label><figDesc>Figure 6. Visualization of image examples on CIFAR-10.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Mean Average Precision (MAP) of Hamming Ranking for Different Number of Bits on the Three Image Datasets</figDesc><table>Method 
NUS-WIDE 
CIFAR-10 
MS-COCO 
16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits 16 bits 32 bits 48 bits 64 bits 
ITQ-CCA [13] 0.460 0.405 0.373 0.347 0.354 0.414 0.449 0.462 0.566 0.562 0.530 0.502 
BRE [19] 
0.503 0.529 0.548 0.555 0.370 0.438 0.468 0.491 0.592 0.622 0.630 0.634 
KSH [25] 
0.551 0.582 0.612 0.635 0.524 0.558 0.567 0.569 0.521 0.534 0.534 0.536 
SDH </table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Wasserstein generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, NSW, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2017-06-11" />
			<biblScope unit="page" from="214" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Collective deep quantization for efficient cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3974" to="3980" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep visualsemantic quantization for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep visual-semantic hashing for cross-modal retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGKDD</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1445" to="1454" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep quantization network for efficient image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. AAAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Hashnet: Deep learning to hash by continuation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Nus-wide: A real-world web image database from national university of singapore</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-T</forename><surname>Zheng</surname></persName>
		</author>
		<editor>ICMR. ACM</editor>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Maximum likelihood in cost-sensitive learning: Model specification, approximations, and upper bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Dmochowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sajda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Parra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2010-12" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep hashing for compact binary codes learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">Erin</forename><surname>Liong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2475" to="2483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast search in hamming space with multi-index hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Punjani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Similarity search in high dimensions via hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gionis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Motwani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">VLDB</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="518" to="529" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Iterative quantization: A procrustean approach to learning binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
	<note>Improved training of wasserstein gans. CoRR, abs/1704.00028, 2017. 2, 3</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning to hash with binary reconstructive embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Simultaneous feature learning and hash coding with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Contentbased multimedia information retrieval: State of the art and challenges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Djeraba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jain</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Multimedia Comput. Commun. Appl</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="19" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Feature learning based deep supervised hashing with pairwise labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Deep supervised hashing for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2064" to="2072" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Supervised hashing with kernels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-G</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
		<editor>CVPR. IEEE</editor>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Hashing with graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML. ACM</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Minimal loss hashing for compact binary codes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="353" to="360" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Conditional image synthesis with auxiliary classifier gans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep semantic hashing with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</title>
		<meeting>the 40th International ACM SIGIR Conference on Research and Development in Information Retrieval</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="225" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Logeswaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<title level="m">Generative adversarial text to image synthesis. ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Learning a nonlinear embedding by preserving class neighbourhood structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AISTATS</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="412" to="419" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Supervised discrete hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H. Tao</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR. IEEE</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Visualizing highdimensional data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2579" to="2605" />
			<date type="published" when="2008-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for large-scale search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="2393" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A survey on learning to hash</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">T</forename><surname>Shen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">40</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="769" to="790" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Supervised quantization for similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G.-J</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2018" to="2026" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Spectral hashing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Supervised hashing for image retrieval via image representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<publisher>AAAI</publisher>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Supervised hashing with latent factor models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Guo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGIR</title>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Deep hashing network for efficient similarity retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. AAAI</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
