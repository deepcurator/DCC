<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Modulation Module for Multi-task Learning with Applications in Image Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiangyun</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haoxiang</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">AIBee</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaohui</forename><surname>Shen</surname></persName>
							<affiliation key="aff2">
								<orgName type="department">Bytedance AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<affiliation key="aff3">
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">EECS Department</orgName>
								<orgName type="institution">Northwestern University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Modulation Module for Multi-task Learning with Applications in Image Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Multi-task learning has been widely adopted in many computer vision tasks to improve overall computation efficiency or boost the performance of individual tasks, under the assumption that those tasks are correlated and complementary to each other. However, the relationships between the tasks are complicated in practice, especially when the number of involved tasks scales up. When two tasks are of weak relevance, they may compete or even distract each other during joint training of shared parameters, and as a consequence undermine the learning of all the tasks. This will raise destructive interference which decreases learning efficiency of shared parameters and lead to low quality loss local optimum w.r.t. shared parameters. To address the this problem, we propose a general modulation module, which can be inserted into any convolutional neural network architecture, to encourage the coupling and feature sharing of relevant tasks while disentangling the learning of irrelevant tasks with minor parameters addition. Equipped with this module, gradient directions from different tasks can be enforced to be consistent for those shared parameters, which benefits multi-task joint training. The module is end-to-end learnable without ad-hoc design for specific tasks, and can naturally handle many tasks at the same time. We apply our approach on two retrieval tasks, face retrieval on the CelebA dataset <ref type="bibr" target="#b11">[12]</ref> and product retrieval on the UT-Zappos50K dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref>, and demonstrate its advantage over other multi-task learning methods in both accuracy and storage efficiency.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Multi-task learning aims to improve learning efficiency and boost the performance of individual tasks by jointly learning multiple tasks at the same time. With the recent prevalence of deep learning-based approaches in various computer vision tasks, multitask learning is often implemented as parameter sharing in certain intermediate layers in a unified convolutional neural network architecture <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b18">19]</ref>. However, such feature sharing only works when the tasks are correlated and complementary to each other. When two tasks are irrelevant, they may provide competing or even contradicting gradient directions during feature learning. For example, learning to predict face attributes</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Smile</head><p>Open Mouth Young <ref type="figure">Fig. 1</ref>. Conflicting training signals in multi-task learning: when jointly learning discriminative features for multiple face attributes, some samples may introduce conflicting training signals in updating shared model parameters, such as "Smile" vs. "Young".</p><p>of "Open Mouth" and "Young" can lead to discrepant gradient directions for the examples in <ref type="figure">Figure 1</ref>. Because the network is supervised to produce nearby embeddings in one task but faraway embeddings in the other task, the shared parameters get conflicting training signals. It is analogous to the destructive interference problem in Physics where two waves of equal frequency and opposite phases cancel each other. It would make the joint training much more difficult and negatively impact the performance of all the tasks. Although this problem is rarely identified in the literature, many of the existing methods are in fact designed to mitigate destructive interference in multi-task learning. For example, in the popular multi-branch neural network architecture and its variants, the task-specific branches are designed carefully with the prior knowledge regarding the relationships of certain tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b19">20]</ref>. By doing this, people expect less conflicting training signals to the shared parameters. Nevertheless, it is difficult to generalize those specific designs to other tasks where the relationships may vary, or to scale up to more tasks such as classifying more than 20 facial attributes at the same time, where the task relationships become more complicated and less well studied.</p><p>To overcome these limitations, we propose a novel modulation module, which can be inserted into arbitrary network architecture and learned through end-to-end training. It can encourage correlated tasks to share more features, and at the same time disentangle the feature learning of irrelevant tasks. In back-propagation of the training signals, it modulates the gradient directions from different tasks to be more consistent for those shared parameters; in the feed-forward pass, it modulates the features towards taskspecific feature spaces. Since it does not require prior knowledge of the relationships of the tasks, it can be applied to various multi-task learning problems, and handle many tasks at the same time. One related work is <ref type="bibr" target="#b23">[24]</ref> which try to increase model capacity without a proportional increase in computation.</p><p>To validate the effectiveness of the proposed approach, we apply the modulation module in a neural network to learn the feature embedding of multiple attributes, and evaluate the learned feature representations on diverse retrieval tasks. In particular, we first propose a joint training framework with several embedded modulation modules for the learning of multiple face attributes, and evaluate the attribute-specific face retrieval results on the CelebA dataset. In addition, we provide thorough analysis on the task relationships and the capability of the proposed module in promoting correlated tasks while decoupling unrelated tasks. Experimental results show that the advantage of our approach is more significant with more tasks involved, showing its generalization capability to larger-scale multi-task learning problems. Compared with existing multitask learning methods, the proposed module learns improved task-specific features and supports a compact model for scalability. We further apply the proposed approach in product retrieval on the UT-Zappos50K dataset, and demonstrate its superiority over other state-of-the-art methods.</p><p>Overall, the contributions of this work are four-fold:</p><p>-We address the destructive interference problem of unrelated tasks in multi-task learning, which is rarely discussed in previous work. -We propose a novel modulation module that is general and end-to-end learnable, to adaptively couple correlated tasks while decoupling unrelated ones during feature learning. -With minor task-specific overhead, our method supports scalable multi-task learning without manually grouping of tasks. -We apply the module to the feature learning of multiple attributes, and demonstrate its effectiveness on retrieval tasks, especially on large-scale problems (e.g., as many as 20 attributes are jointly learned).</p><p>2 Related Work</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Multi-task learning</head><p>It has been observed in many prior works that jointly learning of multiple correlated tasks can help improve the performance of each of them, for example, learning face detection with face alignment <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>, learning object detection with segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>, and learning semantic segmentation with depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. While these works mainly study what related tasks can be jointly learned in order to mutually benefit each other, we instead investigate a proper joint training scheme given any tasks without assumption on their relationships. A number of research efforts have been devoted to exploiting the correlations among related tasks for joint training. For example, Jou et al. <ref type="bibr" target="#b7">[8]</ref> propose the Deep Cross Residual Learning to introduce the cross-residuals connections as a form of network regularization for better network generalization. Misra et al. <ref type="bibr" target="#b13">[14]</ref> propose the Cross-stitch Networks to combine the activations from multiple task-specific networks for better joint training. Kokkinos et al. <ref type="bibr" target="#b8">[9]</ref> propose UberNet to jointly learn low-, mid-, and high-level vision tasks by branching out task-specific paths from different stages in a deep CNN.</p><p>Most multi-task learning frameworks, if not all, involve parameters shared across tasks and task-specific parameters. In joint learning beyond similar tasks, it is desirable to automatically discover what and how to share between tasks. Recent works along this line include Lu et al. <ref type="bibr" target="#b12">[13]</ref>, who propose to automatically discover a neural network design to group similar tasks together; Yang et al. <ref type="bibr" target="#b31">[32]</ref>, who model this problem as tensor factorization to learn how to share knowledge across tasks; and Veit et al. <ref type="bibr" target="#b25">[26]</ref>, who propose to share all neural network layers but masking the final image features differently conditioned on the attributes/tasks.</p><p>Compared to these existing works, in this paper, we explicitly identify the problem of destructive interference and propose a metric to quantify it. Our observation further confirms its correlation to the quality of learned features. Moreover, our proposed module is end-to-end learnable and flexible to be inserted anywhere into an existing network architecture. Hence, our method can further enhance the structure learned with the algorithm from Lu et al. <ref type="bibr" target="#b12">[13]</ref> to improve its suboptimal within-group branches. When compared with the tensor factorization by Yang et al. <ref type="bibr" target="#b31">[32]</ref>, our module is lightweight, easy to train, and with a small and accountable overhead to include additional tasks. Condition similar networks <ref type="bibr" target="#b25">[26]</ref> shares this desirable scalability feature with our method in storage efficiency. However, as they do not account for the destructive interference problem in layers other than the final feature layer, we empirically observe that their method does not scale-up well in accuracy for many tasks (See Section 4.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Image Retrieval</head><p>In this work, we evaluate our method with applications on image retrieval. Image retrieval has been widely studied in computer vision <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>. We do not study the efficiency problem in image retrieval as in many prior works <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16]</ref>. Instead, we focus on learning discriminative task-specific image features for accurate retrieval.</p><p>Essentially, our method is related to how discriminative image features can be extracted. In the era of deep learning, feature extraction is a very important and fundamental research direction. From the early pioneering AlexNet <ref type="bibr" target="#b9">[10]</ref> to recent seminal ResNet <ref type="bibr" target="#b4">[5]</ref> and DenseNet <ref type="bibr" target="#b5">[6]</ref>, the effectiveness and efficiency of neural networks have been largely improved. This line of research focuses on designing better neural network architectures, which is independent of our method. By design, our algorithm can potentially benefit from better backbone architectures.</p><p>Another important related research area is metric learning <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b22">23]</ref>, which mostly focuses on designing an optimization objective to find a metric to maximize the inter-class distance while minimizing the intra-class distance. They are often equivalent to learning a discriminative subspace or feature embedding. Some of them have been introduced into deep learning as the loss function for better feature learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b2">3]</ref>. Our method is by design agnostic to the loss function, and we can potentially benefit from more sophisticated loss functions to learn more discriminative image feature for all tasks. In our experiment, we use triplet loss <ref type="bibr" target="#b21">[22]</ref> due to its simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Our Method</head><p>In this section, we first identify the destructive interference problem in sharing features for multi-task learning and then present the technical details of our modulation module to resolve this problem.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Destructive interference</head><p>Despite that a multi-task neural network can have many variants which involve the learning of different task combinations, the fundamental technique is to share intermedi-  <ref type="table">Table 1</ref>. Accuracy and UCR Comparison on three face attribute-based retrieval tasks (See Section 4.1 for details): the comparison empirically support our analysis of the destructive interference problem and the assumption that reasonable task-specific modulation parameters can be learned from data ate network parameters for different tasks, and jointly train with all supervision signals from different tasks by gradient descent methods. One issue raised from this common scheme is that two irrelevant or weakly relevant tasks may drag gradients propagated from different tasks in conflicting or even opposite directions. Thus, learning the shared parameters can suffer from the well-known destructive interference problem. Formally, we denote θ as the parameters of a neural network F over different tasks, I as its input, and f = F (I|θ) as its output. The update of θ follows its gradient:</p><formula xml:id="formula_0">∇θ = ∂L ∂f ∂f ∂θ ,<label>(1)</label></formula><p>where L is the loss function. In multi-task learning, θ will be updated by gradients from different tasks. Essentially, ∂L ∂f directs the learning of θ. In common cases, a discriminative loss generally encourages f i and f j to be similar for images I i and I j from the same class. However, the relationship of I i and I j can change in multi-task learning, even flip in different tasks. When training all these tasks, the update directions of θ may be conflicting, which is the namely destructive interference problem.</p><p>More specifically, given a mini-batch of training samples from task t and t ′ , ∇θ = ∇θ t + ∇θ t ′ , where ∇θ t/t ′ denotes gradients from samples of task t/t ′ . Gradients from two tasks are negatively impacting the learning of each other, when</p><formula xml:id="formula_1">A t,t ′ = sign( ∇θ t , ∇θ t ′ ) = −1.<label>(2)</label></formula><p>The destructive interference hinders the learning of the shared parameters and essentially leads to low quality loss local optimum w.r.t. shared parameters.</p><p>Empirical Evidence We validate our assumption through a toy experiment on jointly learning of multiple attribute-based face retrieval tasks. More details on the experimental settings can be found in Section 4.1.</p><p>Intuitively, the attribute smile is related to attribute open mouth but irrelevant to attribute young <ref type="bibr" target="#b4">5</ref> . As shown in <ref type="table">Table 1</ref>, when we share all the parameters of the neural network across different tasks, the results degrade when jointly training the tasks compared with training three independent task-specific networks. The degradation when jointly training smile and young is much more significant than the one when jointly training smile and open mouth. That is because there are always some conflicting gradients from some training samples even if two tasks are correlated, and apparently when the two tasks are with weak relevance, the conflicts become more frequent, making the joint training ineffective.</p><p>To further understand how the learning leads to the above results, we follow Equation 2 to quantitatively estimate the compatibility of task pairs by looking at the ratio of mini-batches with A t,t ′ &gt; 0 in one training epoch. So we define this ratio as Update Compliance Ratio(UCR) which measures the consistence of two tasks. The larger the UCR is, the more consistent the two tasks are in joint training. As shown in <ref type="table">Table 1</ref>, in joint learning of smile and open mouth we observe higher compatibility compared with joint learning of smile and young, which explains the accuracy discrepancy from (b) to (c) in <ref type="table">Table 1</ref>. Comparing (e) with (b) and (c), the accuracy improvement is accompanied with UCR improvement which explains how the proposed module improves the overall performance. With our proposed method introduced as following, we observe increased UCR for both task pairs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">A Modulation Module</head><p>Most multi-task learning frameworks involve task-specific parameters and shared parameters. Here we introduce a modulation module as a generic framework to add taskspecific parameters and link it to alleviation of destructive interference.</p><p>More specifically, we propose to modulate the feature maps with task-specific projection matrix W t for task t. As illustrated in <ref type="figure">Figure 2</ref>, this module maintains the feature map size to keep it compatible with layers downwards in the network architecture. Following we will discuss how this design affects the back-propagation and feed-forward pass.</p><p>Back-propagation In back-propagation, destructive interference happens when gradients from two tasks t and t ′ over the shared parameters θ have components in conflicting directions, i.e., ∇θ t , ∇θ t ′ &lt; 0. It can be simply derived that the proposed modulation over feature maps is equivalent to modulating shared parameters with task-specific masks M t/t ′ . With the proposed modulation, the update to θ is now M t ∇θ t +M t ′ ∇θ t ′ . Since the task-specific masks/projection matrices are learnable, we observe that the training process will naturally mitigate the destructive interference by reducing the average across-task gradient angles M t ∇θ t , M t ′ ∇θ t ′ , which is observed to result in better local optimum of shared parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Feed-Forward Pass</head><note type="other">Given feature map x with size M × N × C and the modulation projection matrix W, we have</note><formula xml:id="formula_2">x ′ = W t × x,<label>(3)</label></formula><p>which is the input to the next layer. A full projection matrix would require W t of size M N C × M N C, which is infeasible in practice and the modulation would degenerate to completely separated branches with a full project matrix. Therefore, we firstly simplify the W t to have shared elements within each channel. Formally, W = {w i,j }, {i, j} ∈ {1, . . . , C}</p><formula xml:id="formula_3">x ′ mni = C j=1 x mnj * w i,j ,<label>(4)</label></formula><p>where x ′ mni , x mni and w ij denote elements from input, output feature maps and W t respectively. We ignore the subscription t for simplicity. Here W is in fact a channelwise projection matrix.</p><p>We can further reduce the computation by simplifying the W t to be a channel-wise scaling vector W t with size C as illustrated in <ref type="figure">Figure 2</ref>.</p><p>Formally, W = {w c }, c ∈ {1, . . . , C}.</p><formula xml:id="formula_4">x ′ mnc = x mnc * w c ,<label>(5)</label></formula><p>where x ′ mnc and x mnc denotes elements from input and output feature maps respectively.</p><p>Compared with the channel-wise scaling vector design, we observe empirically the overall improvement from the channel-wise projection matrix design is marginal, hence we will mainly discuss and evaluate the simpler channel-wise scaling vector option. This module can be easily implemented by adding task specific linear transformations as shown in <ref type="figure" target="#fig_0">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Training</head><p>The modulation parameters W t are learned together with the neural network parameters through back-propagation. In this paper, we use triplet loss <ref type="bibr" target="#b21">[22]</ref> as the objective for optimization. More specifically, given a set of triplets from different tasks</p><formula xml:id="formula_5">(I a , I p , I n , t) ∈ T, L = T [ f a − f p 2 + α − f a − f n 2 )] + (6) f a,p,n = F (I a,p,n |θ, W t ))<label>(7)</label></formula><p>, where α is the expected distance margin between positive pair and negative pair, I a is the anchor sample, I p is the positive sample, I n is the negative sample and t is the task. When training the Neural Network with a discriminative loss, we argue that by introducing the Modulation module into the neural network, it will learn to leverage the additional knobs to decouple unrelated tasks and couple related ones to minimize the training loss. In the toy experiment shown in <ref type="table">Table 1</ref>, we primarily show that our method can surpass fully independent learning. The reduced ratios of conflicting minibatches in training as shown in <ref type="table">Table 1</ref>  We further empirically validate this assumption by introducing an additional regularization loss to encode human prior knowledge on the tasks' relevancy. We assume the learned W for smile would be more similar to the one for open mouth compared with the one for young. We regularize the pairs of relevant tasks to have similar task-specific Ws with</p><formula xml:id="formula_6">L a = max(0, W i − W j 2 + β − W i − W k 2 )<label>(8)</label></formula><p>, where β is the expected margin, i, j, k denotes three tasks, and task pair (i, j) is considered more relevant compared to task pair (i, k). L a is weighted by a hyper-parameter λ and combined with the above triplet loss over samples in training.</p><p>As shown in <ref type="table">Table 1</ref>, the accuracy of our method augmented with this regularization loss is better but the gap is only marginal. This suggests that without encoding prior knowledge through the loss, the learned Ws may implicitly capture task relationships in a similar way. On the other hand, it is impractical to manually define all pairwise relationships when the number of tasks scales up, hence we ignore this regularization loss in our large-scale experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In the experiments, we evaluate the performance of our approach on the face retrieval and product retrieval tasks.  <ref type="table">Table 2</ref>. Our Basic Neural Network Architecture: Conv-Pool-ResnetBlock stands for a 3 × 3 conv-layer followed by a stride 2 pooling layer and a standard residual block consist of 2 3 × 3 conv-layers.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Setup</head><p>In both retrieval settings, we define a task as retrieval based on a certain attribute of either face or product. Both datasets have the per-image annotation for each attribute.</p><p>To quantitatively evaluate the methods under the retrieval setting, we randomly sample image triplets from their testing sets as our benchmarks. Each triplet consists of an anchor sample I a , a positive sample I p , and a negative sample I n . Given a triplet, we retrieve one sample from I p and I n with I a and consider it a success if I p is preferred. In our method, we extract discriminative features with the proposed network and measure image pair distance by their euclidean distance of features. The accuracy metric is the ratio of successfully retrieved triplets. Unless stated otherwise, we use the neural network architecture in <ref type="table">Table 2</ref> for our method, our re-implementation of other state-of-the-art methods, and our baseline methods.</p><p>We add the proposed Modulation modules to all layers from block4 to the final layer and use ADAGRAD <ref type="bibr" target="#b0">[1]</ref> for optimization in training with learning rate 0.01. We uniformly initialize the parameters in all added modules to be 1. We use the batch size of 180 for 20 tasks and 168 for 7 tasks joint training. In each mini-batch, we evenly sample triplets for all tasks. Our method generally converges after 40 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Face Retrieval</head><p>Dataset We use Celeb-A dataset <ref type="bibr" target="#b11">[12]</ref> for the face retrieval experiment. Celeb-A consists of more than 200,000 face images with binary annotations on 40 face attributes related to age, expression, decoration, etc. We select 20 attributes more related to face appearance and ignore attributes around decoration such as eyeglasses and hat for our experiments. We also report the results on 40 attributes to verify the effectiveness on 40 attributes.</p><p>We randomly sampled 30000 triplets for training and 10000 triplets for testing for each task. Our basic network architecture is shown in <ref type="table">Table 2</ref>. We augment it by inserting our gradient modulation modules and train from scratch.   <ref type="table">Table 4</ref>. Comparison of UCR between different tasks on joint training of seven face attributes with our method (red) and the fully shared network baseline (black): we quantitatively demonstrate the mitigation of destructive interference with our method.</p><p>Results We report our evaluation of the following methods in <ref type="table" target="#tab_4">Table 3</ref>:</p><p>-Ours: we insert the proposed Modulation modules to the block4, block5, and fc layers to the network in <ref type="table">Table 2</ref> and jointly train it with all training triplets from 20 tasks; -Conditional Similarity Network (CSN) from Veit et al. <ref type="bibr" target="#b25">[26]</ref>: we follow the opensourced implementation from the authors to replace the network architecture with ours and jointly train it with all training triplets from 20 tasks; -Independent Task-specific Network(ITN): in this strong baseline we train 20 taskspecific neural networks with training triplets from each task independently; -Single Fully-shared Network(FSN): we train one network with all training triplets.</p><p>-Independent Branch 256(IB-256): based on shared parameters, we add task-specific branch with feature size 256. -Independent Branch 25(IB-25): based on shared parameters, we add task-specific branch with feature size 25. -Only-mask: our network is pretrained from the independent branch model, the shared parameters are fixed and only the module parameters are learned.  <ref type="table">Table 5</ref>. Ablation Study of our method: with more layers modulated by the proposed method, performance generally improves; channel-wise projection module is marginally better than the default channel-wise scaling vector design.</p><p>Single Fully-shared network and CSN severely suffer from the destructive interference as shown in <ref type="table" target="#tab_4">Table 3</ref>. Note when jointly training only 7 tasks, CSN performs much better than the fully-shared network and similarly to fully shared network with additional parameters as shown in <ref type="table">Table 5</ref>. However, it does not scale up to handle as many as 20 tasks. Since the majority of the parameters are naively shared across tasks until the last layer, CSN still suffers from destructive interference.</p><p>We then compare our methods with Independent Branch methods. Independent Branch methods naively add task specific branches above the shared parameters. The branching for IB-25 and IB256 begins at the end of the baseline model in <ref type="table">Table 2</ref>, i.e., different attributes have different branches after the FC layer. As illustrated in <ref type="table" target="#tab_4">Table 3</ref>, our method clearly outperforms them with much fewer task-specific parameters. Regarding the number of additional parameters, we observe that to approximate accuracy of our method, this baseline needs about 1.3M task-specific parameters, which is 100 times of ours. The comparison indicates that our module is more efficient in leveraging additional parameters budget.  <ref type="table">Table 6</ref>. Accuracy Comparison on joint training of 4 product retrieval tasks on UT-Zappos50k: our method significantly outperforms others.</p><p>Compared with the independently trained task-specific networks, our method achieves slightly better average accuracy with almost 20 times fewer parameters. Notably, our method achieves obvious improvement for both face shape related attributes (chubby, double chin) and all three beard related attributes (goatee, mustache, sideburns), which demonstrates that the proposed method does not only decouple unrelated tasks but also adaptively couples related tasks to improve their learning. We show some example retrieval results in <ref type="figure">Figure 4</ref>.</p><p>We reported the Update Compliance Ratio(UCR) comparison in <ref type="table">Table 4</ref>. Our method significantly improves the UCR in the joint training for all task pairs. This indicates that the proposed module is effective in alleviating the destructive interference by leading the gradients over shared parameters from different tasks to be more consistent.</p><p>To further validate that the source of improvement is from better shared parameters instead of simply additional task specific parameters. We keep our shared parameters fixed as the ones trained with the strong baseline IB-256 and only make the modulation modules trainable. As reported in the last column in <ref type="table" target="#tab_4">Table 3</ref>, the results are not as good as our full pipeline, which suggests that the proposed modules improved the learning of shared parameters. To validate the effectiveness of our method on 40 attributes, we evaluate our method on 40 attributes and obtain average 85.75% which is significant better than 78.22% of our baseline IB-25 which has same network complexity but with independent branches. Ablation Study In <ref type="table">Table 5</ref>, we evaluate how the performance evolves when we insert more Modulation modules into the network. By adding proposed modules to all layers after blockN , N = 5, 4, 3, 2, we observe that the performance generally increases with more layers modulated. This is well-aligned with our intuition that with gradients modulated in more layers, the destructive inference problem gets solved better. Because early layers in the neural networks generally learn primitive filters <ref type="bibr" target="#b35">[36]</ref> shared across a broad spectrum of tasks, shared parameters may not suffer from conflicting updates. Hence the performance improvement saturates eventually.</p><p>We also experiment with channel-wise projection matrix instead of channel-wise scaling vector in the proposed modules as introduced in Section 3.2. We observe marginal improvement with the more complicated module, as shown in the last row of <ref type="table">Table 5</ref>. This suggests that potentially with more parameters being modulated, the overall performance improves at the cost of additional task-specific parameters. It also shows that the proposed channel-wise scaling vector design is a cost-effective choice.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Product Retrieval</head><p>Dataset We use UT-Zappos50K dataset <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b34">35]</ref> for the product retrieval experiment. UT-Zappos50K is a large shoe dataset consisting of more than 50,000 catalog images collected from the web. The datasets are richly annotated and we can retrieve shoes based on their type, suggested gender, height of their heel, and the closing mechanism. We jointly learn these 4 tasks in our experiment. We follow the same training, validation, and testing set splits as Veit et al. <ref type="bibr" target="#b25">[26]</ref> to sample triplets.</p><p>Results As shown in <ref type="table">Table 6</ref>, our method is significantly better than all other competing methods. Because CSN manually initializes the 1-dimensional mask for each attribute to be non-overlapping, their method does not exploit their correlation well when two tasks are correlated. We argue that naively sharing features for all tasks may hinder the further improvement of CSN due to gradient discrepancy among different tasks. In our method, proposed modules are inserted in the network and the correlation of different tasks are effectively exploited. Especially for heel task, our method obtains a nearly 3 point gain over CSN. Note that because our network architecture is much simpler than the one used by Veit et al. <ref type="bibr" target="#b25">[26]</ref> and does not pre-train on ImageNet. The numbers are generally not compatible to those reported in their paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Discussion</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">General applicability</head><p>In this paper, we mainly discuss multi-task learning with application in image retrieval in which each task has similar network structure and loss functions. By design the proposed module is not limited to a specific loss and should be applicable to handle different tasks and different loss functions.</p><p>In general multi-task learning, each task may have its specifically designed network architecture and own loss, such as face detection and face alignment <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b36">37]</ref>, learning object detection and segmentation <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b1">2]</ref>, learning semantic segmentation and depth estimation <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>. The signals from different tasks could be explicitly conflicting as well and lead to severe destructive interference especially when the number of jointly learned tasks scale up. When such severe destructive interference happens, the proposed module could be added to modulate the update directions as well as task-specific features. We leave it as our future work to validate this assumption through experiments.  <ref type="figure">Fig. 4</ref>. Example face retrieval results in two tasks: using models jointly trained for 20 face attributes with CSN and our method respectively. Some incorrectly ranked faces are highlighted in red.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Speed and Memory size Trade-off</head><p>Similar to a multi-branch architecture and arguably most multi-task learning frameworks, our method shares the problem of runtime speed and memory size trade-off in inference. One can either choose to keep all task-specific feature maps in memory to finish all the predictions in a single pass or iteratively feed-forward through the network from the shared feature maps to keep a tight memory foot-print. However, we should highlight that our method can achieve better accuracy with a more compact model in storage. Either a single pass inference or iterative inference could be feasible with our method. Since most computations happen in the early stage in inference, with the proposed modules, our method only added 15% overhead in feed-forward time. The feature maps after block4 are much smaller than the ones in the early stages, so the increased memory footprint would be sustainable for 20 tasks too.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>In this paper, we propose a Modulation module for multi-task learning. We identify the destructive interference problem in joint learning of unrelated tasks and propose to quantify it with Update Compliance Ratio. The proposed modules alleviate this problem by modulating the directions of gradients in back-propagation and help extract better task-specific features by exploiting related tasks. Extensive experiments on CelebA dataset and UT-Zappos50K dataset verify the effectiveness and advantage of our approach over other multi-task learning methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Structure of the proposed Modulation Module which adapts features via learned weights with respect to each task. This module can be inserted between any layers and maintain the network structure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Fig. 2. A neural network fully modulated by our proposed modules: in testing, the network takes inputs as the image and task label to extract discriminative image features for the specified task.</figDesc><table>conv 
conv 
fc 

... 

Task Label 

conv 
module 
module 
module 
module 

smile Acc. open mouth Acc. young Acc. smile / young UCR smile /open-mouth UCR 

smile + young + open mouth(a) 
84.71% 
74.73 % 
71.6% 
-
-

smile + young(b) 
83.85% 
-
74.71% 
22.1% 
-

smile + open mouth(c) 
91.72% 
92.65% 
-
-
43.71% 

Three Independent Networks(d) 
93.32% 
94.40% 
84.90% 
-
-

With Proposed Modulation(e) 
94.03% 
95.31% 
86.20% 
50.63% 
52.77% 

With Proposed Modulation + Reg(f) 94.94% 
95.58% 
87.75% 
-
-

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>also validate our design. The learned W * capture the relationship of tasks implicitly. We obtained W s , W y and W o for smile, young, open-mouth respectively. Then the element-wise difference between W s and W o ,∇W s,o , and the difference between W s and W y , ∇W s,y , are obtained to measure their relevancy. The mean and variance of ∇W s,o is 0.18 and 0.03 while the mean and variance of ∇W s,y is 0.24 and 0.047.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Pool-ResnetBlock 73 × 73 × 64 block3 Conv-Pool-ResnetBlock 35 × 35 × 128 block4 Conv-Pool-ResnetBlock 16 × 16 × 128 block5 Conv-Pool-ResnetBlock 7 × 7 × 128</figDesc><table>Name 
Operation 
Output Size 

conv1 
3 × 3 convolution 
148 × 148 × 32 
block2 Conv-fc 
Fully-Connected 
256 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Accuracy</figDesc><table>comparison on the joint training of 20 face attributes: with far fewer param-
eters, our method achieves best mean accuracy over the 20 tasks compared with the competing 
methods. 

smile 
ovalface 
shadow 
bald 
arc-eyebrows big-lips 
big-nose 
smile 
-
51.56/48.47 67.70/26.33 67.82/32.30 52.32/45.40 54.83/49.49 58.72/45.25 
ovalface 51.56/48.47 
-
67.36/26.94 64.99/35.29 57.86/50.13 57.74/49.32 54.98/46.64 
shadow 
67.70/26.33 67.36/26.94 
-
91.67/30.54 66.87/26.48 72.51/28.25 69.90/29.99 
bald 
67.82/32.30 64.99/35.29 91.67/30.54 
-
61.74/31.67 67.60/36.22 72.66/41.04 
arc-eyebrows 52.32/45.40 57.86/50.13 66.87/26.48 61.74/31.67 
-
58.86/51.13 50.34/41.43 
big-lips 
54.83/46.49 57.74/49.32 72.51/28.25 67.70/36.22 58.86/51.13 
-
55.20/46.84 
big-nose 58.72/45.25 54.98/46.64 69.90/29.99 72.66/41.04 50.34/41.43 55.20/46.84 
-

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">Here the attribute refers to its estimation from a given face image.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements This work was supported in part by National Science Foundation grant IIS-1217302, IIS-1619078, the Army Research Office ARO W911NF-16-1-0138, and Adobe Collaboration Funding.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Mask r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Product quantization for nearest neighbor search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="page" from="117" to="128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep cross residual learning for multitask visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM on Multimedia Conference</title>
		<meeting>the 2016 ACM on Multimedia Conference</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="998" to="1007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Ubernet: Training a universal convolutional neural network for low-, mid-, and high-level vision using diverse datasets and limited memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep learning of binary hash codes for fast image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Hsiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">S</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR) Workshops</title>
		<imprint>
			<date type="published" when="2015-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep learning face attributes in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Computer Vision (ICCV)</title>
		<meeting>International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fully-adaptive feature sharing in multi-task networks with applications in person attribute classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Javidi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Joint semantic segmentation and depth estimation with deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mousavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pirsiavash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Košecká</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3D Vision (3DV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="611" to="619" />
		</imprint>
	</monogr>
	<note>Fourth International Conference on</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale image retrieval with compressed fisher vectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sánchez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Poirier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3384" to="3391" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Hyperface: A deep multi-task learning framework for face detection, landmark localization, pose estimation, and gender recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">M</forename><surname>Patel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.01249</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">An all-in-one convolutional neural network for face analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Automatic Face &amp; Gesture Recognition (FG 2017</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="17" to="24" />
		</imprint>
	</monogr>
	<note>12th IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dex: Deep expectation of apparent age from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Rothe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="10" to="15" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Boostexter: A boosting-based system for text categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="135" to="168" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="41" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shazeer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mirhoseini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maziarz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.06538</idno>
		<title level="m">Outrageously large neural networks: The sparsely-gated mixture-of-experts layer</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Object retrieval and localization with spatially-constrained similarity measure and k-nn re-ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Avidan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2012 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3013" to="3020" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Conditional similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deep learning for contentbased image retrieval: A comprehensive study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C H</forename><surname>Hoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="157" to="166" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Semi-supervised hashing for scalable image retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2010 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3424" to="3431" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2800" to="2809" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Distance metric learning with application to clustering with side-information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="521" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Deep multi-task representation learning: A tensor factorisation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">M</forename><surname>Hospedales</surname></persName>
		</author>
		<idno>CoRR abs/1605.06391</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04710</idno>
		<title level="m">Multi-task convolutional neural network for face recognition</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Fine-grained visual comparisons with local learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Semantic jitter: Dense supervision for visual comparisons via synthetic images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Joint face detection and alignment using multitask cascaded convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1499" to="1503" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
