<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:20+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">License Plate Detection and Recognition in Unconstrained Scenarios</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sérgio</forename><forename type="middle">Montazzolli</forename><surname>Silva</surname></persName>
							<email>smsilva@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul Porto Alegre</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cláudio</forename><surname>Rosito</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul Porto Alegre</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jung</forename></persName>
							<email>crjung@inf.ufrgs.br</email>
							<affiliation key="aff0">
								<orgName type="department">Institute of Informatics</orgName>
								<orgName type="institution">Federal University of Rio Grande do Sul Porto Alegre</orgName>
								<address>
									<country key="BR">Brazil</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">License Plate Detection and Recognition in Unconstrained Scenarios</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>License Plate · Deep learning · Convolutional Neural Net- works</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Despite the large number of both commercial and academic methods for Automatic License Plate Recognition (ALPR), most existing approaches are focused on a specific license plate (LP) region (e.g. European, US, Brazilian, Taiwanese, etc.), and frequently explore datasets containing approximately frontal images. This work proposes a complete ALPR system focusing on unconstrained capture scenarios, where the LP might be considerably distorted due to oblique views. Our main contribution is the introduction of a novel Convolutional Neural Network (CNN) capable of detecting and rectifying multiple distorted license plates in a single image, which are fed to an Optical Character Recognition (OCR) method to obtain the final result. As an additional contribution, we also present manual annotations for a challenging set of LP images from different regions and acquisition conditions. Our experimental results indicate that the proposed method, without any parameter adaptation or fine tuning for a specific scenario, performs similarly to state-of-the-art commercial systems in traditional scenarios, and outperforms both academic and commercial approaches in challenging ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Several traffic-related applications, such as detection of stolen vehicles, toll control and parking lot access validation involve vehicle identification, which is performed by Automatic License Plate Recognition (ALPR) systems. The recent advances in Parallel Processing and Deep Learning (DL) have contributed to improve many computer vision tasks, such as Object Detection/Recognition and Optical Character Recognition (OCR), which clearly benefit ALPR systems. In fact, deep Convolutional Neural Networks (CNNs) have been the leading machine learning technique applied for vehicle and license plate (LP) detection <ref type="bibr" target="#b21">[18,</ref><ref type="bibr" target="#b32">28,</ref><ref type="bibr" target="#b22">19,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b35">31,</ref><ref type="bibr" target="#b20">17]</ref>. Along with academic papers, several commercial ALPR systems have been also exploring DL methods. They are usually allocated in huge data-centers and work through web-services, being able to process thousands to millions of images per day and be constantly improved. As examples of these systems, we can mention Sighthound (https://www.sighthound.com/), the commercial version of OpenALPR (http://www.openalpr.com/) and Amazon Rekognition (https://aws.amazon.com/rekognition/). Despite the advances in the state-of-the-art, most ALPR systems assume a mostly frontal view of the vehicle and LP, which is common in applications such as toll monitoring and parking lot validation, for instance. However, more relaxed image acquisition scenarios (e.g. a law enforcement agent walking with a mobile camera or smartphone) might lead to oblique views in which the LP might be highly distorted yet still readable, as illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, and for which even state-of-the-art commercial systems struggle.</p><p>In this work we propose a complete ALPR system that performs well over a variety of scenarios and camera setups. Our main contribution is the introduction of a novel network capable of detecting the LP in many different camera poses and estimate its distortion, allowing a rectification process before OCR. An additional contribution is the massive use of synthetically warped versions of real images for augmenting the training dataset, allowing the network to be trained from scratch using less than 200 manually labeled images. The proposed network and data augmentation scheme also led to a flexible ALPR system that was able to successfully detect and recognize LPs in independent test datasets using the same system parametrization.</p><p>We also generalized an existing OCR approach develpoed for Brazilian LPs <ref type="bibr" target="#b32">[28]</ref>. Basically, we re-trained their OCR network using a new training set composed by a mixture of real and artificially generated data using font-types similar to the target regions. As a result, the re-trained network became much more robust for detection and classification of real characters in the original Brazilian scenario, but also for European and Taiwanese LPs, achieving very high precision and recall rates. All the annotated data used for this work is publicly available 1 , and the reference images can be obtained by downloading the Cars Dataset <ref type="bibr" target="#b19">[16]</ref>, the SSIG Database <ref type="bibr" target="#b7">[6]</ref>, and the AOLP dataset <ref type="bibr" target="#b11">[10]</ref>.</p><p>The remainder of this work is organized as follows. In Section 2 we briefly review related approaches toward ALPR. Details of the proposed method are given in Section 3, where we describe the LP detection and unwarping network, as well as the data augmentation process used to train our models. The overall evaluation and final results are presented in Section 4. Finally, Section 5 summarizes our conclusions and gives perspectives for some future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Work</head><p>ALPR is the task of finding and recognizing license plates in images. It is commonly broken into four subtasks that form a sequential pipeline: vehicle detection, license plate detection, character segmentation and character recognition. For simplicity, we refer to the combination of the last two subtasks as OCR.</p><p>Many different ALPR systems or related subtasks have been proposed in the past, typically using image binarization or gray-scale analysis to find candidate proposals (e.g. LPs and characters), followed by handcrafted feature extraction methods and classical machine learning classifiers <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b5">4]</ref>. With the rise of DL, the state-of-the-art started moving to another direction, and nowadays many works employ CNNs due to its high accuracy for generic object detection and recognition <ref type="bibr" target="#b26">[23,</ref><ref type="bibr" target="#b27">24,</ref><ref type="bibr" target="#b24">21,</ref><ref type="bibr" target="#b29">25,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b12">11]</ref>.</p><p>Related to ALPR are Scene Text Spotting (STS) and number reading in the wild (e.g. from Google Street View images <ref type="bibr" target="#b25">[22]</ref>) problems, which goals are to find and read text/numbers in natural scenes. Although ALPR could be seen as a particular case of STS, the two problems present particular characteristics: in ALPR, we need to learn characters and numbers (without much font variability) with no semantic information, while STS is focused on textual information containing high font variability, and possibly exploring lexical and semantic information, as in <ref type="bibr" target="#b34">[30]</ref>. Number reading does not present semantic information, but dealing only with digits is simpler than the ALPR context, since it avoids common digit/letter confusions such as B-8, D-0, 1-I, 5-S, for instance.</p><p>As the main contribution of this work is a novel LP detection network, we start this section by reviewing DL-based approaches for this specific subtask, as well as a few STS methods that can handle distorted text and could be used for LP detection. Next, we move to complete ALPR DL-based systems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">License Plate Detection</head><p>The success of YOLO networks <ref type="bibr" target="#b26">[23,</ref><ref type="bibr" target="#b27">24]</ref> inspired many recent works, targeting real-time performance for LP detection <ref type="bibr" target="#b32">[28,</ref><ref type="bibr" target="#b10">9,</ref><ref type="bibr" target="#b35">31,</ref><ref type="bibr" target="#b20">17]</ref>. A slightly modified version of the YOLO <ref type="bibr" target="#b26">[23]</ref> and YOLOv2 <ref type="bibr" target="#b27">[24]</ref> networks were used by Hsu et al. <ref type="bibr" target="#b10">[9]</ref>, where the authors enlarged the networks output granularity to improve the number of detections, and set the probabilities for two classes (LP and background). Their network achieved a good compromise between precision and recall, but the paper lacks a detailed evaluation over the bounding boxes extracted. Moreover, it is known that YOLO networks struggle to detect small sized objects, thus further evaluations over scenarios where the car is far from the camera is needed.</p><p>In <ref type="bibr" target="#b35">[31]</ref>, a setup of two YOLO-based networks was trained with the goal of detecting rotated LPs. The first network is used to find a region containing the LP, called "attention model", and the second network captures a rotated rectangular bounding-box of the LP. Nonetheless, they considered only on-plane rotations, and not more complex deformations caused by oblique camera views, such as the ones illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>. Also, as they do not present a complete ALPR system, it is difficult to evaluate how well an OCR method would perform on the detected regions.</p><p>License plate detectors using sliding window approaches or candidate filtering coupled with CNNs can also be found in the literature <ref type="bibr" target="#b3">[3,</ref><ref type="bibr" target="#b2">2,</ref><ref type="bibr" target="#b31">27]</ref>. However, they tend to be computationally inefficient as a result of not sharing calculations like in modern meta-architectures for object detection such as YOLO, SSD <ref type="bibr" target="#b24">[21]</ref> and Faster R-CNN <ref type="bibr" target="#b29">[25]</ref>.</p><p>Although Scene Text Spotting (STS) methods focus mostly on large font variations and lexical/semantic information, but it is worth mentioning a few approaches that deal with rotated/distorted text and could be explored for LP detection in oblique views. Jaderberg and colleagues <ref type="bibr" target="#b16">[13]</ref> presented a CNN-based approach for text recognition in natural scenes using an entirely synthetic dataset to train the model. Despite the good results, they strongly rely on N-grams, which are not applicable to ALPR. Gupta et al. <ref type="bibr" target="#b8">[7]</ref> also explored synthetic dataset by realistically pasting text into real images, focusing mostly on text localization. The output is a rotated bounding box with around the text, which finds limitations for off-plane rotations common in ALPR scenarios.</p><p>More recently, Wang et al. <ref type="bibr" target="#b33">[29]</ref> presented an approach to detect text in a variety of geometric positions, called Instance Transformation Network (ITN). It is basically a composition of three CNNs: a backbone network to compute features, a transformation network to infer affine parameters where supposedly exists text in the feature map, and a final classification network whose input is built by sampling features according to the affine parameters. Although this approach can (in theory) handle off-plane rotations, it is not able to correctly infer the transformation that actually maps the text region to a rectangle, since there is no physical (or clear psychological) bounding region around the text that should map to a rectangle in an undistorted view. In ALPR, the LP is rectangular and planar by construction, and we explore this information to regress the transformation parameters, as detailed in Section 3.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Complete ALPR Methods</head><p>The works of Silva and Jung <ref type="bibr" target="#b32">[28]</ref> and Laroca et al. <ref type="bibr" target="#b20">[17]</ref> presented complete ALPR systems based on a series of modified YOLO networks. Two distinct networks were used in <ref type="bibr" target="#b32">[28]</ref>, one to jointly detect cars and LPs, and another to perform OCR. A total of five networks were used in <ref type="bibr" target="#b20">[17]</ref>, basically one for each ALPR subtask, being two for character recognition. Both reported real-time systems, but they are focused only on Brazilian license plates and were not trained to capture distortion, only frontal and nearly rectangular LPs.</p><p>Selmi et al. <ref type="bibr" target="#b31">[27]</ref> used a series of pre-processing approaches based on morphological operators, Gaussian filtering, edge detection and geometry analysis to find LP candidates and characters. Then, two distinct CNNs were used to (i) classify a set of LP candidates per image into one single positive sample; and (ii) to recognize the segmented characters. The method handles a single LP per image, and according to the authors, distorted LPs and poor illumination conditions can compromise the performance.</p><p>Li et al. <ref type="bibr" target="#b22">[19]</ref> presented a network based on Faster R-CNN <ref type="bibr" target="#b29">[25]</ref>. Shortly, a Region Proposal Network is assigned to find candidate LP regions, whose corresponding feature maps are cropped by a RoI Pooling layer. Then, these candidates are fed into the final part of the network, which computes the probability of being/not being an LP, and performs OCR through a Recurrent Neural Network. Despite promising, the evaluation presented by the authors shows a lack of performance in most challenging scenarios containing oblique LPs.</p><p>Commercial systems are good reference points to the state-of-the-art. Although they usually provide only partial (or none) information about their architecture, we still can use them as black boxes to evaluate the final output. As mentioned in Section 1, examples are Sighthound, OpenALPR (which is an official NVIDIA partner in the Metropolis platform 2 ) and Amazon Rekognition (a general-purpose AI engine including a text detection and recognition module that can be used for LP recognition, as informed by the company).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Proposed Method</head><p>The proposed approach is composed by three main steps: vehicle detection, LP detection and OCR, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given an input image, the first module detects vehicles in the scene. Within each detection region, the proposed Warped Planar Object Detection Network (WPOD-NET) searches for LPs and regresses one affine transformation per detection, allowing a rectification of the LP area to a rectangle resembling a frontal view. These positive and rectified detections are fed to an OCR Network for final character recognition.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Vehicle Detection</head><p>Since vehicles are one of the underlying objects present in many classical detection and recognition datasets, such as PASCAL-VOC <ref type="bibr" target="#b6">[5]</ref>, ImageNet <ref type="bibr" target="#b30">[26]</ref>, and COCO <ref type="bibr" target="#b23">[20]</ref>, we decided to not train a detector from scratch, and instead chose a known model to perform vehicle detection considering a few criteria. On one hand, a high recall rate is desired, since any miss detected vehicle having a visible LP leads directly to an overall LP miss detection. On the other hand, high precision is also desirable to keep running times low, as each falsely detected vehicle must be verified by WPOD-NET. Based on these considerations, we decided to use the YOLOv2 network due to its fast execution (around 70 FPS) and good precision and recall compromise (76.8% mAP over the PASCAL-VOC dataset). We did not perform any change or refinement to YOLOv2, just used the network as a black box, merging the outputs related to vehicles (i.e. cars and buses), and ignoring the other classes.</p><p>The positive detections are then resized before being fed to WPOD-NET. As a rule of thumb, larger input images allow the detection of smaller objects but increase the computational cost <ref type="bibr" target="#b14">[12]</ref>. In roughly frontal/rear views, the ratio between the LP size and the vehicle bounding box (BB) is high. However, this ratio tends to be much smaller for oblique/lateral views, since the vehicle BB tends to be larger and more elongated. Hence, oblique views should be resized to a larger dimension than frontal ones to keep the LP region still recognizable.</p><p>Although 3D pose estimation methods such as <ref type="bibr" target="#b36">[32]</ref> might be used to determine the resize scale, this work presents a simple and fast procedure based on the aspect ratio of the vehicle BB. When it is close to one, a smaller dimension can be used, and it must be increased as the aspect ratio gets larger. More precisely, the resizing factor f sc is given by</p><formula xml:id="formula_0">f sc = 1 min{W v , H v } min D min max(W v , H v ) min(W v , H v ) , D max ,<label>(1)</label></formula><p>where W v and H v are the width and height of the vehicle BB, respectively. Note that </p><formula xml:id="formula_1">D min ≤ f sc min(W v , H v ) ≤ D max ,</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">License Plate Detection and Unwarping</head><p>License plates are intrinsically rectangular and planar objects, which are attached to vehicles for identification purposes. To take advantage of its shape, we proposed a novel CNN called Warped Planar Object Detection Network. This network learns to detect LPs in a variety of different distortions, and regresses coefficients of an affine transformation that "unwarps" the distorted LP into a rectangular shape resembling a frontal view. Although a planar perspective projection could be learned instead of the affine transform, the division involved in the perspective transformation might generate small values in the denominator, and hence leading to numerical instabilities. The WPOD-NET was developed using insights from YOLO, SSD and Spatial Transformer Networks (STN) <ref type="bibr" target="#b17">[14]</ref>. YOLO and SSD perform fast multiple object detection and recognition at once, but they do not take spatial transformations into account, generating only rectangular bounding boxes for every detection. On the opposite, STN can be used for detecting non-rectangular regions, however it cannot handle multiple transformations at the same time, performing only a single spatial transformation over the entire input. The detection process using WPOD-NET is illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. Initially, the network is fed by the resized output of the vehicle detection module. The feedforwarding results in an 8-channel feature map that encodes object/non-object probabilities and affine transformation parameters. To extract the warped LP, let us first consider an imaginary square of fixed size around the center of a cell (m, n). If the object probability for this cell is above a given detection threshold, part of the regressed parameters is used to build an affine matrix that transforms the fictional square into an LP region. Thus, we can easily unwarp the LP into a horizontally and vertically aligned object.</p><p>Network Architecture The proposed architecture has a total of 21 convolutional layers, where 14 are inside residual blocks <ref type="bibr" target="#b9">[8]</ref>. The size of all convolutional filters is fixed in 3 × 3. ReLU activations are used throughout the entire network, except in the detection block. There are 4 max pooling layers of size 2 × 2 and stride 2 that reduces the input dimensionality by a factor of 16. Finally, the detection block has two parallel convolutional layers: (i) one for inferring the probability, activated by a softmax function, and (ii) another for regressing the affine parameters, without activation (or, equivalently, using the identity F (x) = x as the activation function). </p><formula xml:id="formula_2">T mn (q) = max(v 3 , 0) v 4 v 5 max(v 6 , 0) q + v 7 v 8 ,<label>(2)</label></formula><p>where the max function used for v 3 and v 6 was adopted to ensure that the diagonal is positive (avoiding undesired mirroring or excessive rotations). To match the network output resolution, the points p i are re-scaled by the inverse of the network stride, and re-centered according to each point (m, n) in the feature map. This is accomplished by applying a normalization function</p><formula xml:id="formula_3">A mn (p) = 1 α 1 N s p − n m ,<label>(3)</label></formula><p>where α is a scaling constant that represents the side of the fictional square. We set α = 7.75, which is the mean point between the maximum and minimum LP dimensions in the augmented training data divided by the network stride. Assuming that there is an object (LP) at cell (m, n), the first part of the loss function considers the error between a warped version of the canonical square and the normalized annotated points of the LP, given by</p><formula xml:id="formula_4">f affine (m, n) = 4 i=1 T mn (q i ) − A mn (p i ) 1 .<label>(4)</label></formula><p>The second part of the loss function handles the probability of having/not having an object at (m, n). It is similar to the SSD confidence loss <ref type="bibr" target="#b24">[21]</ref>, and basically is the sum of two log-loss functions</p><formula xml:id="formula_5">f probs (m, n) = logloss(I obj , v 1 ) + logloss(1 − I obj , v 2 ),<label>(5)</label></formula><p>where I obj is the object indicator function that returns 1 if there is an object at point (m, n) or 0 otherwise, and logloss(y, p) = −y log(p). An object is considered inside a point (m, n) if its rectangular bounding box presents an IoU larger than a threshold γ obj (set empirically to 0.3) w.r.t. another bounding box of the same size and centered at (m, n).</p><p>The final loss function is given by a combination of the terms defined in Eqs. <ref type="formula" target="#formula_4">(4)</ref> and <ref type="formula" target="#formula_5">(5)</ref>:  Given the reduced number of annotated images in the training dataset, the use of data augmentation is crucial. The following augmentation transforms are used:</p><formula xml:id="formula_6">loss = M m=1 N n=1 [I obj f affine (m, n) + f probs (m, n)].<label>(6</label></formula><p>-Rectification: the entire image is rectified based on the LP annotation, assuming that the LP lies on a plane; -Aspect-ratio: the LP aspect-ratio is randomly set in the interval <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b5">4]</ref> to accommodate sizes from different regions;</p><p>-Centering: the LP center becomes the image center; -Scaling: the LP is scaled so its width matches a value between 40px and 208px (set experimentally based on the readability of the LPs). This range is used to define the value of α used in Eq. <ref type="formula" target="#formula_3">(3)</ref>; -Rotation: a 3D rotation with randomly chosen angles is performed, to account for a wide range of camera setups; -Mirroring: 50% chance; -Translation: random translation to move the LP from the center of the image, limited to a square of 208 × 208 pixels around the center; -Cropping: considering the LP center before the translation, we crop a 208 × 208 region around it; -Colorspace: slight modifications in the HSV colorspace; -Annotation: the locations of the four LP corners are adjusted by applying the same spatial transformations used to augment the input image.</p><p>From the chosen set of transformations mentioned above, a great variety of augmented test images with very distinct visual characteristics can be obtained from a single manually labeled sample. For example, <ref type="figure" target="#fig_6">Fig. 6</ref> shows 20 different augmentation samples obtained from the same image. We trained the network with 100k iterations of mini-batches of size 32 using the ADAM optimizer <ref type="bibr" target="#b18">[15]</ref>. The learning rate was set to 0.001 with parameters β 1 = 0.9 and β 2 = 0.999. The mini-batches were generated by randomly choosing and augmenting samples from the training set, resulting in new input tensors of size 32 × 208 × 208 × 3 at every iteration.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">OCR</head><p>The character segmentation and recognition over the rectified LP is performed using a modified YOLO network, with the same architecture presented in <ref type="bibr" target="#b32">[28]</ref>. However, the training dataset was considerably enlarged in this work by using synthetic and augmented data to cope with LP characteristics of different regions around the world (Europe, United States and Brazil) <ref type="bibr" target="#b3">3</ref> . The artificially created data consist of pasting a string of seven characters onto a textured background and then performing random transformations, such as rotation, translation, noise, and blur. Some generated samples and a short overview of the pipeline for synthetic data generation are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. As shown in Section 4, the use of synthetic data helped to greatly improve the network generalization, so that the exact same network performs well for LPs of different regions around the world.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Evaluation Datasets</head><p>One of our goals is to develop a technique that performs well in a variety of unconstrained scenarios, but that should also work well in controlled ones (such as mostly frontal views). Therefore, we chose four datasets available online, namely OpenALPR (BR and EU) <ref type="bibr" target="#b5">4</ref> , SSIG and AOLP (RP), which cover many different situations, as summarized in the first part of <ref type="table" target="#tab_1">Table 1</ref>. We consider three distinct variables: LP angle (frontal and oblique), distance from vehicles to the camera (close, intermediate and far), and the region where the pictures were taken. The more challenging dataset currently used in terms of LP distortion is the AOLP Road Patrol (RP) subset, which tries to simulate the case where a camera is installed in a patrolling vehicle or hand-held by a person. In terms of distance from the camera to the vehicles, the SSIG dataset appears to be the most challenging one. It is composed of high-resolution images, allowing that LPs from distant vehicles might still be readable. None of them present LPs from multiple (simultaneous) vehicles at once.</p><p>Although all these databases together cover numerous situations, to the best of our knowledge there is a lack of more general-purpose dataset with challenging images in the literature. Thus, an additional contribution of this work is the manual annotation of a new set of 102 images (named as CD-HARD) selected from the Cars Dataset, covering a variety of challenging situations. We selected mostly images with strong LP distortion but still readable for humans. Some of these images (crops around the LP region) are shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, which was used to motivate the problem tackled in this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experimental Results</head><p>This section covers the experimental analysis of our full ALPR system, as well as comparisons with other state-of-the-art methods and commercial systems. Unfortunately, most academic ALPR papers focus on specific scenarios (e.g. single country or region, environment conditions, camera position, etc.). As a result, there are many scattered datasets available in the literature, each one evaluated by a subset of methods. Moreover, many papers are focused only on LP detection or character segmentation, which limits even more the comparison possibilities for the full ALPR pipeline. In this work, we used four independent datasets to evaluate the accuracy of the proposed method in different scenarios and region layouts. We also show comparisons with commercial products and papers that present full ALPR systems.</p><p>The proposed approach presents three networks in the pipeline, for which we empirically set the following acceptance thresholds: 0.5 for vehicle (YOLOv2) and LP (WPOD-NET) detection, and 0.4 for character detection and recognition (OCR-NET). Also, it is worth noticing that characters "I" and "1" are identical for Brazilian LPs. Hence, they were considered as a single class in the evaluation of the OpenALPR BR and SSIG datasets. No other heuristic or post-processing was applied to the results produced by the OCR module.</p><p>We evaluate the system in terms of the percentage of correctly recognized LPs, where an LP is considered correct if all characters were correctly recognized, and no additional characters were detected. It is important to note that the exact same networks were applied to all datasets: no specific training procedure was used to tune the networks for a given type of LP (e.g. European or Taiwanese). The only slight modification performed in the pipeline was for the AOLP Road Patrol dataset. In this dataset, the vehicles are very close to the camera (causing the vehicle detector to fail in several cases), so that we directly applied the LP detector (WPOD-NET) to the input images.</p><p>To show the benefits of including fully synthetic data in the OCR-NET training procedure, we evaluated our system using two sets training data: (i) real augmented data plus artificially generated ones; and (ii) only real augmented data. These two versions are denoted by "Ours" and "Ours (no artf.)", respectively, in <ref type="table" target="#tab_2">Table 2</ref>. As can be observed, the addition of fully synthetic data improved the accuracy in all tested datasets (with a gain ≈ 5% for the AOLP RP dataset). Moreover, to highlight the improvements of rectifying the detection bounding box, we also present the results of using a regular non-rectified bounding box, identified as "Ours (unrect.)" in <ref type="table" target="#tab_2">Table 2</ref>. As expected, the results do not vary much in the mostly frontal datasets (being even slightly better for ALPR-EU), but there was a considerable accuracy drop in datasets with challenging oblique LPs (AOLP-RP and the proposed CD-HARD). <ref type="table" target="#tab_2">Table 2</ref> also shows the results of competitive (commercial and academic) systems, indicating that our system achieved recognition rates comparable to commercial ones in databases representing more controlled scenarios, where the LPs are mostly frontal (OpenALPR EU and BR, and SSIG). More precisely, it was the second best method in both OpenALPR datasets, and top one in SSIG. In the challenging scenarios (AOLP RP and the proposed CD-HARD dataset), however, our system outperformed all compared approaches by a significant margin (over 7% accuracy gain when compared to the second best result).</p><p>It is worth mentioning that the works of Li et al. <ref type="bibr" target="#b21">[18,</ref><ref type="bibr" target="#b22">19]</ref>, Hsu et al. <ref type="bibr" target="#b11">[10]</ref> and Laroca et al. <ref type="bibr" target="#b20">[17]</ref> are focused on a single region or dataset. By outperforming them, we demonstrate a strong generalization capacity. It is also important to note that the full LP recognition rate for the most challenging datasets (AOLP-RP and CD-HARD) was higher than directly applying the OCR module to the annotated rectangular LP bounding boxes (79.21% for AOLP-RP and 53.85% for CD-HARD). This gain is due to the unwarping allowed by WPOD-NET, which greatly helps the OCR task when the LP is strongly distorted. To illus-trate this behavior, we show in <ref type="figure">Fig. 8</ref> the detected and unwarped LPs for the images in <ref type="figure" target="#fig_0">Fig. 1</ref>, as well as the final recognition result produced by OCR-NET. The detection score of the top right LP was below the acceptance threshold, illustrating a false negative example.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ZCAA30</head><p>GNO6BGV C24JBH ACAC1350 MXH4622 AURI318 J2II3 06U564 VZ60MLB KK4504 HBDD1111</p><p>*missed* <ref type="figure">Fig. 8</ref>: Detected/unwarped LPs from images in <ref type="figure" target="#fig_0">Fig. 1</ref> and final ALPR results.</p><p>The proposed WPOD-NET was implemented using TensorFlow framework, while the initial YOLOv2 vehicle detection and OCR-NET were created and executed using the DarkNet framework. A Python wrapper was used to integrate the two frameworks. The hardware used for our experiments was an Intel Xeon processor, with 12Gb of RAM and an NVIDIA Titan X GPU. With that configuration, we were able to run the full ALPR system with an average of 5 FPS (considering all datasets). This time is highly dependent of the number of vehicles detected in the input image. Hence, incrementing the vehicle detection threshold will result in higher FPS, but lower recall rates.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions and Future Work</head><p>In this work, we presented a complete deep learning ALPR system for unconstrained scenarios. Our results indicate that the proposed approach outperforms existing methods by far in challenging datasets, containing LPs captured at strongly oblique views while keeping good results in more controlled datasets.</p><p>The main contribution of this work is the introduction of a novel network that allows the detection and unwarping of distorted LPs by generating an affine transformation matrix per detection cell. This step alleviates the burden of the OCR network, as it needed to handle less distortion.</p><p>As an additional contribution, we presented a new challenging dataset for evaluating ALPR systems in captures with mostly oblique LPs. The annotations for the dataset will be made publicly available so that the dataset might be used as a new challenging LP benchmark.</p><p>For future work, we want to extend our solution to detect motorcycle LPs. This poses new challenges due to differences in aspect ratio and layout. Moreover, we intend to explore the obtained affine transformations for automatic camera calibration problem in traffic surveillance scenarios.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Examples of challenging oblique license plates present in the proposed evaluation dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Illustration of the proposed pipeline.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 :</head><label>3</label><figDesc>Fig. 3: Fully convolutional detection of planar objects (cropped for better visualization).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: Detailed WPOD-NET architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>)</head><label></label><figDesc>Training Details For training the proposed WPOD-NET, we created a dataset with 196 images, being 105 from the Cars Dataset, 40 from the SSIG Dataset (training subset), and 51 from the AOLP dataset (LE subset). For each image, we manually annotated the 4 corners of the LP in the picture (sometimes more than one). The selected images from the Cars Dataset include mostly European LPs, but there are many from the USA as well as other LP types. Images from SSIG and AOLP contain Brazilian and Taiwanese LPs, respectively. A few annotated samples are shown in Fig. 5.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Examples of the annotated LPs in the training dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Different augmentations for the same sample. The red quadrilateral represents the transformed LP annotation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Fig. 7 :</head><label>7</label><figDesc>Fig. 7: Artificial LP samples with the proposed generation pipeline (bottom).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>so that D min and D max delimit the range for the smallest dimension of the resized BB. Based on experiments and trying to keep a good compromise between accuracy and running times, we selected D min = 288 and D max = 608.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 :</head><label>1</label><figDesc>Evaluation datasets.</figDesc><table>Database (subset) 
LP angle 
Vehicle Dist. #images Region 

OpenALPR 
5 (EU) 
mostly frontal 
close 
104 
Europe 
OpenALPR (BR) 
mostly frontal 
close 
108 
Brazil 
SSIG (test-set) 
mostly frontal 
medium,far 
804 
Brazil 
AOLP (Road Patrol) 
frontal + oblique 
close 
611 
Taiwan 
Proposed (CD-HARD) mostly oblique close,medium,far 
102 
Various 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Full ALPR results for all 5 datasets.*OpenALPR struggled to understand the "Q" letter in Taiwanese LPs. **In [10] the authors provided an estimative, and not the real evaluation.</figDesc><table>OpenALPR 
SSIG 
AOLP 
Proposed Average 
EU 
BR 
Test 
RP 
CD-HARD 

Ours 
93.52% 91.23% 88.56% 98.36% 
75.00% 
89.33% 
Ours (no artf.) 
92.59% 88.60% 84.58% 93.29% 
73.08% 
86.43% 
Ours (unrect.) 
94.44% 90.35% 87.81% 84.61% 
57.69% 
82.98% 
Commercial systems 
OpenALPR 
96.30% 85.96% 87.44% 69.72%* 
67.31% 
81.35% 
Sighthound 
83.33% 94.73% 81.46% 83.47% 
45.19% 
77.64% 
Amazon Rekog. 
69.44% 83.33% 31.21% 68.25% 
30.77% 
56.60% 
Literature 
Laroca et al. [17] 
-
-
85.45% 
-
-
-
Li et al. [18] 
-
-
-
88.38% 
-
-
Li et al. [19] 
-
-
-
83.63% 
-
-
Hsu et al. [10] 
-
-
-
85.70%** 
-
-

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Available at http://www.inf.ufrgs.br/ ∼ crjung/alpr-datasets.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">NVIDIA platform for video analysis in smart cities (https://www.nvidia.com/en-us/ autonomous-machines/intelligent-video-analytics-platform/).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We also used Taiwanese LPs, but could not find information in English about the font type used by this country in order to include in the artificial data generation.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">Available at https://github.com/openalpr/benchmarks.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank the funding agencies CAPES and CNPq, as well as NVIDIA Corporation for donating a Titan X Pascal GPU.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">License Plate Recognition From Still Images and Video Sequences: A Survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">N</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Anagnostopoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Psoroulas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Loumos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kayafas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="377" to="391" />
			<date type="published" when="2008-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<idno type="doi">10.1109/TITS.2008.922938</idno>
		<ptr target="http://ieeexplore.ieee.org/document/4518951/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Segmentation-and AnnotationFree License Plate Recognition With Deep Localization and Failure Identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bulan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kozitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ramesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shreve</surname></persName>
		</author>
		<idno type="doi">10.1109/TITS.2016.2639020</idno>
		<ptr target="https://doi.org/10.1109/TITS.2016.2639020" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2351" to="2363" />
			<date type="published" when="2017-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Convolutional neural networks for license plate detection in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Delmar</forename><surname>Kurpiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Minetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nassu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2017-09" />
			<biblScope unit="page" from="3395" to="3399" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<idno type="doi">10.1109/ICIP.2017.8296912</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8296912/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Automatic License Plate Recognition (ALPR): A State-of-the-Art Review</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ibrahim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shehata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Badawy</surname></persName>
		</author>
		<idno type="doi">10.1109/TCSVT.2012.2203741</idno>
		<ptr target="http://ieeexplore.ieee.org/document/6213519/" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="311" to="325" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Benchmark for license plate character segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gonalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P G</forename><surname>Da Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<ptr target="http://www.ssig.dcc.ufmg.br/wp-content/uploads/2016/11/JEI-2016-Benchmark.pdf" />
	</analytic>
	<monogr>
		<title level="j">Journal of Electronic Imaging</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="5" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Residual Learning for Image Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2016.90</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2016.90" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Robust license plate detection in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ambikapathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">L</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">P</forename><surname>Su</surname></persName>
		</author>
		<idno type="doi">10.1109/AVSS.2017.8078493</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8078493/" />
	</analytic>
	<monogr>
		<title level="m">14th IEEE International Conference on Advanced Video and Signal Based Surveillance (AVSS)</title>
		<imprint>
			<date type="published" when="2017-08" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Application-Oriented License Plate Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Hsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Z</forename><surname>Chung</surname></persName>
		</author>
		<idno type="doi">10.1109/TVT.2012.2226218</idno>
		<ptr target="https://doi.org/10.1109/TVT.2012.2226218" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Vehicular Technology</title>
		<imprint>
			<biblScope unit="volume">62</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="552" to="561" />
			<date type="published" when="2013-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Densely Connected Convolutional Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017-07" />
			<biblScope unit="page" from="2261" to="2269" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<idno type="doi">10.1109/CVPR.2017.243</idno>
		<ptr target="http://arxiv.org/abs/1608.06993http://ieeexplore.ieee.org/document/8099726/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="3296" to="3297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<idno type="doi">10.1109/CVPR.2017.351</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8099834/" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Synthetic Data and Artificial Neural Networks for Natural Scene Text Recognition. NIPS, Conference on Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1" to="10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Cortes, C., Lawrence, N.D., Lee, D.D., Sugiyama, M., Garnett, R.</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2015" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>CoRR abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d object representations for finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">4th International IEEE Workshop on 3D Representation and Recognition</title>
		<meeting><address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3" to="13" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">A robust real-time automatic license plate recognition based on the YOLO detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Laroca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Severo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Zanlorensi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Gonçalves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">R</forename><surname>Schwartz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Menotti</surname></persName>
		</author>
		<idno>CoRR abs/1802.09567</idno>
		<ptr target="http://arxiv.org/abs/1802.09567" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.05610</idno>
		<ptr target="http://arxiv.org/abs/1601.05610" />
		<title level="m">Reading Car License Plates Using Deep Convolutional Neural Networks and LSTMs</title>
		<imprint>
			<date type="published" when="2016-01" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Towards end-to-end car license plates detection and recognition with deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno>CoRR abs/1709.08828</idno>
		<ptr target="http://arxiv.org/abs/1709.08828" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV</title>
		<editor>Fleet, D., Pajdla, T., Schiele, B., Tuytelaars, T.</editor>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<idno type="doi">10.1007/978-3-319-46448-02</idno>
		<ptr target="https://doi.org/10.1007/978-3-319-46448-02" />
		<title level="m">SSD: Single Shot MultiBox Detector</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS workshop on deep learning and unsupervised feature learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2011</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">You Only Look Once: Unified, Real-Time Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2016.91</idno>
		<ptr target="http://ieeexplore.ieee.org/document/7780460/" />
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, Faster, Stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="6517" to="6525" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title/>
		<idno type="doi">10.1109/CVPR.2017.690</idno>
		<ptr target="http://arxiv.org/abs/1612.08242http://ieeexplore.ieee.org/document/8100173/" />
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards RealTime Object Detection with Region Proposal Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="doi">10.1109/TPAMI.2016.2577031</idno>
		<ptr target="https://doi.org/10.1109/TPAMI.2016.2577031" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1137" to="1149" />
			<date type="published" when="2017-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="doi">10.1007/s11263-015-0816-y</idno>
		<ptr target="https://doi.org/10.1007/s11263-015-0816-y" />
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep Learning System for Automatic License Plate Detection and Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Selmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ben Halima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Alimi</surname></persName>
		</author>
		<idno type="doi">10.1109/ICDAR.2017.187</idno>
		<ptr target="http://ieeexplore.ieee.org/document/8270118/" />
	</analytic>
	<monogr>
		<title level="m">2017 14th IAPR International Conference on Document Analysis and Recognition (ICDAR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017-11" />
			<biblScope unit="page" from="1132" to="1138" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Real-time brazilian license plate detection and recognition using deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Jung</surname></persName>
		</author>
		<idno type="doi">10.1109/SIBGRAPI.2017.14</idno>
		<ptr target="https://doi.org/10.1109/SIBGRAPI.2017.14" />
	</analytic>
	<monogr>
		<title level="m">2017 30th SIBGRAPI Conference on Graphics, Patterns and Images (SIBGRAPI)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="page" from="55" to="62" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Geometry-Aware Scene Text Detection with Instance Transformation Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="1381" to="1389" />
		</imprint>
	</monogr>
	<note>Salt Lake City</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Scene text recognition using similarity and a lexicon with sparse belief propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Weinman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Hanson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions</title>
		<imprint>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1733" to="1746" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">A New CNN-Based Method for Multi-Directional Car License Plate Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ahmad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="doi">10.1109/TITS.2017.2784093</idno>
		<ptr target="https://doi.org/10.1109/TITS.2017.2784093" />
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Intelligent Transportation Systems</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="507" to="517" />
			<date type="published" when="2018-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Sparse representation for 3d shape estimation: A convex relaxation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Leonardos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Daniilidis</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="1648" to="1661" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
