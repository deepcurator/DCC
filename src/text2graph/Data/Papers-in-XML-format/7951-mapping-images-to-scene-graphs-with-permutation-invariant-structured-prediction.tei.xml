<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:44+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roei</forename><surname>Herzig</surname></persName>
							<email>roeiherzig@mail.tau.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Moshiko</forename><surname>Raboh</surname></persName>
							<email>mosheraboh@mail.tau.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gal</forename><surname>Chechik</surname></persName>
							<email>gal.chechik@biu.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Berant</surname></persName>
							<email>joberant@cs.tau.ac.il</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Globerson</surname></persName>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Tel Aviv University</orgName>
								<orgName type="institution" key="instit2">Tel Aviv University</orgName>
								<orgName type="institution" key="instit3">Ilan University</orgName>
								<address>
									<country>NVIDIA Research</country>
								</address>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">Tel Aviv University</orgName>
								<orgName type="institution" key="instit2">Tel Aviv University</orgName>
								<address>
									<postCode>AI2</postCode>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Mapping Images to Scene Graphs with Permutation-Invariant Structured Prediction</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Machine understanding of complex images is a key goal of artificial intelligence. One challenge underlying this task is that visual scenes contain multiple interrelated objects, and that global context plays an important role in interpreting the scene. A natural modeling framework for capturing such effects is structured prediction, which optimizes over complex labels, while modeling within-label interactions. However, it is unclear what principles should guide the design of a structured prediction model that utilizes the power of deep learning components. Here we propose a design principle for such architectures that follows from a natural requirement of permutation invariance. We prove a necessary and sufficient characterization for architectures that follow this invariance, and discuss its implication on model design. Finally, we show that the resulting model achieves new state-of-the-art results on the Visual Genome scene-graph labeling benchmark, outperforming all recent approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Understanding the semantics of a complex visual scene is a fundamental problem in machine perception. It often requires recognizing multiple objects in a scene, together with their spatial and functional relations. The set of objects and relations is sometimes represented as a graph, connecting objects (nodes) with their relations (edges) and is known as a scene graph <ref type="figure">(Figure 1</ref>). Scene graphs provide a compact representation of the semantics of an image, and can be useful for semantic-level interpretation and reasoning about a visual scene <ref type="bibr" target="#b10">[11]</ref>. Scene-graph prediction is the problem of inferring the joint set of objects and their relations in a visual scene.</p><p>Since objects and relations are inter-dependent (e.g., a person and chair are more likely to be in relation "sitting on" than "eating"), a scene graph predictor should capture this dependence in order to improve prediction accuracy. This goal is a special case of a more general problem, namely, inferring multiple inter-dependent labels, which is the research focus of the field of structured prediction. Structured prediction has attracted considerable attention because it applies to many learning problems and poses <ref type="bibr">Figure 1</ref>: An image and its scene graph from the Visual Genome dataset <ref type="bibr" target="#b14">[15]</ref>. The scene graph captures the entities in the image (nodes, blue circles) like dog and their relations (edges, red circles) like hat, on, dog .</p><p>unique theoretical and algorithmic challenges [e.g., see <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b27">28]</ref>. It is therefore a natural approach for predicting scene graphs from images.</p><p>Structured prediction models typically define a score function s(x, y) that quantifies how well a label assignment y is compatible with an input x. In the case of understanding complex visual scenes, x is an image, and y is a complex label containing the labels of objects detected in an image and the labels of their relations. In this setup, the inference task amounts to finding the label that maximizes the compatibility score y * = arg max y s(x, y). This score-based approach separates a scoring component -implemented by a parametric model, from an optimization component -aimed at finding a label that maximizes that score. Unfortunately, for a general scoring function s(·), the space of possible label assignments grows exponentially with input size. For instance, for scene graphs the set of possible object label assignments is too large even for relatively simple images, since the vocabulary of candidate objects may contain thousands of objects. As a result, inferring the label assignment that maximizes a scoring function is computationally hard in the general case.</p><p>An alternative approach to score-based methods is to map an input x to a structured output y with a "black box" neural network, without explicitly defining a score function. This raises a natural question: what is the right architecture for such a network? Here we take an axiomatic approach and argue that one important property such networks should satisfy is invariance to a particular type of input permutation. We then prove that this invariance is equivalent to imposing certain structural constraints on the architecture of the network, and describe architectures that satisfy these constraints.</p><p>To evaluate our approach, we first demonstrate on a synthetic dataset that respecting permutation invariance is important, because models that violate this invariance need more training data, despite having a comparable model size. Then, we tackle the problem of scene graph generation. We describe a model that satisfies the permutation invariance property, and show that it achieves state-of-the-art results on the competitive Visual Genome benchmark <ref type="bibr" target="#b14">[15]</ref>, demonstrating the power of our new design principle.</p><p>In summary, the novel contributions of this paper are: a) Deriving sufficient and necessary conditions for graph-permutation invariance in deep structured prediction architectures. b) Empirically demonstrating the benefit of graph-permutation invariance. c) Developing a state-of-the-art model for scene graph prediction on a large dataset of complex visual scenes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Structured Prediction</head><p>Scored-based methods in structured prediction define a function s(x, y) that quantifies the degree to which y is compatible with x, and infer a label by maximizing s(x, y) [e.g., see <ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b27">28]</ref>. Most score functions previously used decompose as a sum over simpler functions, s(x, y) = i f i (x, y), making it possible to optimize max y f i (x, y) efficiently. This local maximization forms the basic building block of algorithms for approximately maximizing s(x, y). One way to decompose the score function is to restrict each f i (x, y) to depend only on a small subset of the y variables.</p><p>The renewed interest in deep learning led to efforts to integrate deep networks with structured prediction, including modeling the f i functions as deep networks. In this context, the most widelyused score functions are singleton f i (y i , x) and pairwise f ij (y i , y j , x). The early work taking this approach used a two-stage architecture, learning the local scores independently of the structured prediction goal <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8]</ref>. Later studies considered end-to-end architectures where the inference algorithm is part of the computation graph <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b32">33]</ref>. Recent studies go beyond pairwise scores, also modelling global factors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>.</p><p>Score-based methods provide several advantages. First, they allow intuitive specification of local dependencies between labels and how these translate to global dependencies. Second, for linear score functions, the learning problem has natural convex surrogates <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b27">28]</ref>. Third, inference in large label spaces is sometimes possible via exact algorithms or empirically accurate approximations. However, with the advent of deep scoring functions s(x, y; w), learning is no longer convex. Thus, it is worthwhile to rethink the architecture of structured prediction models, and consider models that map inputs x to outputs y directly without explicitly maximizing a score function. We would like these models to enjoy the expressivity and predictive power of neural networks, while maintaining the ability to specify local dependencies between labels in a flexible manner. In the next section, we present such an approach and consider a natural question: what should be the properties of a deep neural network used for structured prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Permutation-Invariant Structured Prediction</head><p>In what follows we define the permutation-invariance property for structured prediction models, and argue that permutation invariance is a natural principle for designing their architecture.</p><p>We first introduce our notation. We focus on structures with pairwise interactions, because they are simpler in terms of notation and are sufficient for describing the structure in many problems. We denote a structured label by y = [y 1 , . . . , y n ]. In a score-based approach, the score is defined via a set of singleton scores f i (y i , x) and pairwise scores f ij (y i , y j , x), where the overall score s(x, y) is the sum of these scores. For brevity, we denote f ij = f ij (y i , y j , x) and f i = f i (y i , x). An inference algorithm takes as input the local scores f i , f ij and outputs an assignment that maximizes s(x, y). We can thus view inference as a black-box that takes node-dependent and edge-dependent inputs (i.e., the scores f i , f ij ) and returns a label y, even without an explicit score function s(x, y). While numerous inference algorithms exist for this setup, including belief propagation (BP) and mean field, here we develop a framework for a deep labeling algorithm (we avoid the term "inference" since the algorithm does not explicitly maximize a score function). Such an algorithm will be a black-box, taking the f functions as input and the labels y 1 , . . . , y n as output. We next ask what architecture such an algorithm should have.</p><p>We follow with several definitions. A graph labeling function F : (V, E) → Y is a function whose input is an ordered set of node features V = [z 1 , . . . , z n ] and an ordered set of edge features E = [z 1,2 . . . , z i,j , . . . , z n,n−1 ]. For example, z i can be the array of values f i , and z i,j can be the table of values f i,j . Assume z i ∈ R d and z i,j ∈ R e . The output of F is a set of node labels y = [y 1 , . . . , y n ]. Thus, algorithms such as BP are graph labeling functions. However, graph labeling functions do not necessarily maximize a score function. We denote the joint set of node features and edge features by z (i.e., a set of n + n(n − 1) = n 2 vectors). In Section 3.1 we discuss extensions to this case where only a subset of the edges is available.</p><p>A natural requirement is that the function F produces the same result when given the same features, up to a permutation of the input. For example, consider a label space with three variables y 1 , y 2 , y 3 , and assume that F takes as input z = (z 1 , z 2 , z 3 , z 12 , z 13 , z 23 ) = (f 1 , f 2 , f 3 , f 12 , f 13 , f 23 ), and outputs a label y = (y * 1 , y * 2 , y * 3 ). When F is given an input that is permuted in a consistent way, say, z = (f 2 , f 1 , f 3 , f 21 , f 23 , f 13 ), this defines exactly the same input. Hence, the output should still be y = (y * 2 , y * 1 , y * 3 ). Most inference algorithms, including BP and mean field, satisfy this symmetry requirement by design, but this property is not guaranteed in general in a deep model. Here, our goal is to design a deep learning black-box, and hence we wish to guarantee invariance to input permutations. A black-box that violates this invariance "wastes" capacity on learning it at training time, which increases sample complexity, as shown in Sec. 5.1. We proceed to formally define the permutation invariance property. Definition 1. Let z be a set of node features and edge features, and let σ be a permutation of {1, . . . , n}. We define σ(z) to be a new set of node and edge features given by</p><formula xml:id="formula_0">[σ(z)] i = z σ(i) and [σ(z)] i,j = z σ(i),σ(j) .</formula><p>We also use the notation σ([y 1 , . . . , y n ]) = [y σ(1) , . . . , y σ(n) ] for permuting the labels. Namely, σ applied to a set of labels yields the same labels, only permuted by σ. Be aware that applying σ to the input features is different from permuting labels, because edge input features must permuted in a way that is consistent with permuting node input features. We now provide our key definition of a function whose output is invariant to permutations of the input. See <ref type="figure" target="#fig_0">Figure 2</ref> (left). Definition 2. A graph labeling function F is said to be graph-permutation invariant (GPI), if for all permutations σ of {1, . . . , n} and for all z it satisfies: F(σ(z)) = σ(F(z)).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Characterizing Permutation Invariance</head><p>Motivated by the above discussion, we ask: what structure is necessary and sufficient to guarantee that F is GPI? Note that a function F takes as input an ordered set z. Therefore its output on z could certainly differ from its output on σ(z). To achieve permutation invariance, F should contain certain symmetries. For instance, one permutation invariant architecture could be to define y i = g(z i ) for any function g, but this architecture is too restrictive and does not cover all permutation invariant functions. Theorem 1 below provides a complete characterization (see <ref type="figure" target="#fig_0">Figure 2</ref> for the corresponding architecture). Intuitively, the architecture in Theorem 1 is such that it can aggregate information from the entire graph, and do so in a permutation invariant manner. Theorem 1. Let F be a graph labeling function. Then F is graph-permutation invariant if and only if there exist functions α, ρ, φ such that for all k = 1, . . . , n:</p><formula xml:id="formula_1">[F(z)] k = ρ   z k , n i=1 α   z i , j =i φ(z i , z i,j , z j )     ,<label>(1)</label></formula><formula xml:id="formula_2">where φ : R 2d+e → R L , α : R d+L → R W and ρ : R W +d → R.</formula><p>Proof. First, we show that any F satisfying the conditions of Theorem 1 is GPI. Namely, for any</p><formula xml:id="formula_3">permutation σ, [F(σ(z))] k = [F(z)] σ(k) .</formula><p>To see this, write [F(σ(z))] k using Eq. 1 and Definition 1:</p><formula xml:id="formula_4">[F(σ(z))] k = ρ(z σ(k) , i α(z σ(i) , j =i φ(z σ(i) , z σ(i),σ(j) , z σ(j) ))).<label>(2)</label></formula><p>The second argument of ρ above is invariant under σ, because it is a sum over nodes and their neighbors, which is invariant under permutation. Thus Eq. 2 is equal to:</p><formula xml:id="formula_5">ρ(z σ(k) , i α(z i , j =i φ(z i , z i,j , z j ))) = [F(z)] σ(k)</formula><p>where equality follows from Eq. 1. We thus proved that Eq. 1 implies graph permutation invariance.</p><p>Next, we prove that any given GPI function F 0 can be expressed as a function F in Eq. 1. Namely, we show how to define φ, α and ρ that can implement F 0 . Note that in this direction of the proof the function F 0 is a black-box. Namely, we only know that it is GPI, but do not assume anything else about its implementation.</p><p>The key idea is to construct φ, α such that the second argument of ρ in Eq. 1 contains the information about all the graph features z. Then, the function ρ corresponds to an application of F 0 to this representation, followed by extracting the label y k . To simplify notation assume edge features are scalar (e = 1). The extension to vectors is simple, but involves more indexing.</p><p>We assume WLOG that the black-box function F 0 is a function only of the pairwise features z i,j (otherwise, we can always augment the pairwise features with the singleton features). Since z i,j ∈ R we use a matrix R n,n to denote all the pairwise features.</p><p>Finally, we assume that our implementation of F 0 will take additional node features z k such that no two nodes have the same feature (i.e., the features identify the node).</p><p>Our goal is thus to show that there exist functions α, φ, ρ such that the function in Eq. 2 applied to Z yields the same labels as F 0 (Z).</p><p>Let H be a hash function with L buckets mapping node features z i to an index (bucket). Assume that H is perfect (this can be achieved for a large enough L). Define φ to map the pairwise features to a vector of size L. Let 1 [j] be a one-hot vector of dimension R L , with one in the j th coordinate. Recall that we consider scalar z i,j so that φ is indeed in R L , and define φ as:</p><formula xml:id="formula_6">φ(z i , z i,j , z j ) = 1 [H(z j )] z i,</formula><note type="other">j , i.e., φ "stores" z i,j in the unique bucket for node j. Let s i = zi,j ∈E φ(z i , z i,j , z j ) be the second argument of α in Eq. 1 (s i ∈ R L ). Then, since all z j are distinct, s i stores all the pairwise features for neighbors of i in unique positions within its L coordinates. Since s i (H(z k )) contains the feature z i,k whereas s j (H(z k )) contains the feature z j,k , we cannot simply sum the s i , since we would lose the information of which edges the features originated from. Instead, we define α to map s i to R</note><p>L×L such that each feature is mapped to a distinct location. Formally:</p><formula xml:id="formula_7">α(z i , s i ) = 1 [H(z i )] s T i .<label>(3)</label></formula><p>α outputs a matrix that is all zeros except for the features corresponding to node i that are stored in row H(z i ). The matrix M = i α(z i , s i ) (namely, the second argument of ρ in Eq. 1) is a matrix with all the edge features in the graph including the graph structure.</p><p>To complete the construction we set ρ to have the same outcome as F 0 . We first discard rows and columns in M that do not correspond to original nodes (reducing M to dimension n × n). Then, we use the reduced matrix as the input z to the black-box F 0 .</p><p>Assume for simplicity that M does not need to be contracted (this merely introduces another indexing step). Then M corresponds to the original matrix Z of pairwise features, with both rows and columns permuted according to H. We will thus use M as input to the function F 0 . Since F 0 is GPI, this means that the label for node k will be given by</p><formula xml:id="formula_8">F 0 (M ) in position H(z k ). Thus we set ρ(z k , M ) = [F 0 (M )] H(z k )</formula><p>, and by the argument above this equals [F 0 (Z)] k , implying that the above α, φ and ρ indeed implement F 0 .</p><p>Extension to general graphs So far, we discussed complete graphs, where edges correspond to valid feature pairs. However, many graphs of interest might be incomplete. For example, an n-variable chain graph in sequence labeling has only n − 1 edges. For such graphs, the input to F would not contain all z i,j pairs but rather only features corresponding to valid edges of the graph, and we are only interested in invariances that preserve the graph structure, namely, the automorphisms of the graph. Thus, the desired invariance is that σ(F(z)) = F(σ(z)), where σ is not an arbitrary permutation but an automorphism. It is easy to see that a simple variant of Theorem 1 holds in this case. All we need to do is replace in Eq. 2 the sum j =i with j∈N (i) , where N (i) are the neighbors of node i in the graph. The arguments are then similar to the proof above.</p><p>Implications of Theorem 1 Our result has interesting implications for deep structured prediction. First, it highlights that the fact that the architecture "collects" information from all different edges of the graph, in an invariant fashion via the α, φ functions. Specifically, the functions φ (after summation) aggregate all the features around a given node, and then α (after summation) can collect them. Thus, these functions can provide a summary of the entire graph that is sufficient for downstream algorithms. This is different from one round of message passing algorithms which would not be sufficient for collecting global graph information. Note that the dimensions of φ, α may need to be large to aggregate all graph information (e.g., by hashing all the features as in the proof of Theorem 1), but the architecture itself can be shallow.</p><p>Second, the architecture is parallelizable, as all φ functions can be applied simultaneously. This is in contrast to recurrent models <ref type="bibr" target="#b31">[32]</ref> which are harder to parallelize and are thus slower in practice.</p><p>Finally, the theorem suggests several common architectural structures that can be used within GPI. We briefly mention two of these. 1) Attention: Attention is a powerful component in deep learning architectures <ref type="bibr" target="#b0">[1]</ref>, but most inference algorithms do not use attention. Intuitively, in attention each node i aggregates features of neighbors through a weighted sum, where the weight is a function of the neighbor's relevance. For example, the label of an entity in an image may depend more strongly on entities that are spatially closer. Attention can be naturally implemented in our GPI characterization, and we provide a full derivation for this implementation in the appendix. It plays a key role in our scene graph model described below. 2) RNNs: Because GPI functions are closed under composition, for any GPI function F we can run F iteratively by providing the output of one step of F as part of the input to the next step and maintain GPI. This results in a recurrent architecture, which we use in our scene graph model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Related Work</head><p>The concept of architectural invariance was recently proposed in DEEPSETS <ref type="bibr" target="#b30">[31]</ref>. The invariance we consider is much less restrictive: the architecture does not need to be invariant to all permutations of singleton and pairwise features, just those consistent with a graph re-labeling. This characterization results in a substantially different set of possible architectures.</p><p>Deep structured prediction. There has been significant recent interest in extending deep learning to structured prediction tasks. Much of this work has been on semantic segmentation, where convolutional networks <ref type="bibr" target="#b26">[27]</ref> became a standard approach for obtaining "singleton scores" and various approaches were proposed for adding structure on top. Most of these approaches used variants of message passing algorithms, unrolled into a computation graph <ref type="bibr" target="#b28">[29]</ref>. Some studies parameterized parts of the message passing algorithm and learned its parameters <ref type="bibr" target="#b17">[18]</ref>. Recently, gradient descent has also been used for maximizing score functions <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b9">10]</ref>. An alternative to deep structured prediction is greedy decoding, inferring each label at a time based on previous labels. This approach has been popular in sequence-based applications (e.g., parsing <ref type="bibr" target="#b4">[5]</ref>), relying on the sequential structure of the input, where BiLSTMs are effectively applied. Another related line of work is applying deep learning to graph-based problems, such as TSP <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b12">13]</ref>. Clearly, the notion of graph invariance is important in these, as highlighted in <ref type="bibr" target="#b8">[9]</ref>. They however do not specify a general architecture that satisfies invariance as we do here, and in fact focus on message passing architectures, which we strictly generalize. Furthermore, our focus is on the more general problem of structured prediction, rather than specific graph-based optimization problems.</p><p>Scene graph prediction. Extracting scene graphs from images provides a semantic representation that can later be used for reasoning, question answering, and image retrieval <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25]</ref>. It is at the forefront of machine vision research, integrating challenges like object detection, action recognition and detection of human-object interactions <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b23">24]</ref>. Prior work on scene graph predictions used neural message passing algorithms <ref type="bibr" target="#b28">[29]</ref> as well as prior knowledge in the form of word embeddings <ref type="bibr" target="#b18">[19]</ref>. Other work suggested to predict graphs directly from pixels in an end-to-end manner <ref type="bibr" target="#b20">[21]</ref>. NeuralMotif <ref type="bibr" target="#b31">[32]</ref>, currently the state-of-the-art model for scene graph prediction on Visual Genome, employs an RNN that provides global context by sequentially reading the independent predictions for each entity and relation and then refines those predictions. The NEURALMOTIF model maintains GPI by fixing the order in which the RNN reads its inputs and thus only a single order is allowed. However, this fixed order is not guaranteed to be optimal.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Experimental Evaluation</head><p>We empirically evaluate the benefit of GPI architectures. First, using a synthetic graph-labeling task, and then for the problem of mapping images to scene graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Synthetic Graph Labeling</head><p>We start with studying GPI on a synthetic problem, defined as follows. An input graph G = (V, E) is given, where each node i ∈ V is assigned to one of K sets. The set for node i is denoted by </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Γ(i).</head><p>The goal is to compute for each node the number of neighbors that belong to the same set. Namely, the label of a node is</p><formula xml:id="formula_9">y i = j∈N (i) 1[Γ(i) = Γ(j)].</formula><p>We generated random graphs with 10 nodes (larger graphs produced similar results) by sampling each edge independently and uniformly, and sampling Γ(i) for every node uniformly from {1, . . . , K}. The node features z i ∈ {0, 1} K are one-hot vectors of Γ(i) and the edge features z i,j ∈ {0, 1} indicate whether ij ∈ E. We compare two standard non-GPI architectures and one GPI architecture: (a) A GPI-architecture for graph prediction, described in detail in Section 5.2. We used the basic version without attention and RNN. (b) LSTM: We replace φ(·) and α(·), which perform aggregation in Theorem 1 with two LSTMs with a state size of 200 that read their input in random order. (c) A fully-connected (FC) feed-forward network with 2 hidden layers of 1000 nodes each. The input to the fully connected model is a concatenation of all node and pairwise features. The output is all node predictions. The focus of the experiment is to study sample complexity. Therefore, for a fair comparison, we use the same number of parameters for all models. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Scene-Graph Classification</head><p>We evaluate the GPI approach on the motivating task of this paper, inferring scene graphs from images ( <ref type="figure">Figure 1)</ref>. In this problem, the input is an image annotated with a set of bounding boxes for the entities in the image. <ref type="bibr" target="#b1">2</ref> The goal is to label each bounding box with the correct entity category and every pair of entities with their relation, such that they form a coherent scene graph.</p><p>We begin by describing our Scene Graph Predictor (SGP) model. We aim to predict two types of variables. The first is entity variables [y 1 , . . . , y n ] for all bounding boxes. Each y i can take one of L values (e.g., "dog", "man"). The second is relation variables [y n+1 , . . . , y n 2 ] for every pair of bounding boxes. Each such y j can take one of R values (e.g., "on", "near"). Our graph connects variables that are expected to be inter-related. It contains two types of edges: 1) entity-entity edge connecting every two entity variables (y i and y j for 1 ≤ i = j ≤ n. 2) entity-relation edges connecting every relation variable y k (where k &gt; n) to its two entity variables. Thus, our graph is not a complete graph and our goal is to design an architecture that will be invariant to any automorphism of the graph, such as permutations of the entity variables.</p><p>For the input features z, we used the features learned by the baseline model from <ref type="bibr" target="#b31">[32]</ref>. <ref type="bibr" target="#b2">3</ref> Specifically, the entity features z i included (1) The confidence probabilities of all entities for y i as learned by the baseline model. Similarly, the relation features z j ∈ R R contained the probabilities of relation entities for the relation j. For the Linguistic model, these features were extended to include word embedding of the most probable class. For entity-entity pairwise features z i,j , we use the relation probability for each pair.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Constrained Evaluation</head><p>Unconstrained Evaluation SGCls PredCls SGCls PredCls R@50 R@100 R@50 R@100 R@50 R@100 R@50 R@100 <ref type="bibr">Lu et al., 2016 [19]</ref> 11.  <ref type="table">Table 1</ref>: Test set results for graph-constrained evaluation (i.e., the returned triplets must be consistent with a scene graph) and for unconstrained evaluation (triplets need not be consistent with a scene graph).</p><p>Because the output of SGP are probability distributions over entities and relations, we use them as an the input z to SGP, once again in a recurrent manner and maintain GPI.</p><p>We next describe the main components of the GPI architecture. First, we focus on the parts that output the entity labels. φ ent is the network that integrates features for two entity variables y i and y j . It simply takes z i , z j and z i,j as input, and outputs a vector of dimension n 1 . Next, the network α ent takes as input the outputs of φ ent for all neighbors of an entity, and uses the attention mechanism described above to output a vector of dimension n 2 . Finally, the ρ ent network takes these n 2 dimensional vectors and outputs L logits predicting the entity value. The ρ rel network takes as input the α ent representation of the two entities, as well as z i,j and transforms the output into R logits. See appendix for specific network architectures.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Experimental Setup and Results</head><p>Dataset. We evaluated our approach on Visual Genome (VG) <ref type="bibr" target="#b14">[15]</ref>, a dataset with 108,077 images annotated with bounding boxes, entities and relations. On average, images have 12 entities and 7 relations per image. For a proper comparison with previous results <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>, we used the data from <ref type="bibr" target="#b28">[29]</ref>, including the train and test splits. For evaluation, we used the same 150 entities and 50 relations as in <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32]</ref>. To tune hyper-parameters, we also split the training data into two by randomly selecting 5K examples, resulting in a final 70K/5K/32K split for train/validation/test sets.</p><p>Training. All networks were trained using Adam <ref type="bibr" target="#b13">[14]</ref> with batch size 20. Hyperparameter values below were chosen based on the validation set. The SGP loss function was the sum of cross-entropy losses over all entities and relations in the image. In the loss, we penalized entities 4 times more strongly than relations, and penalized negative relations 10 times more weakly than positive relations.</p><p>Evaluation. In <ref type="bibr" target="#b28">[29]</ref> three different evaluation settings were considered. Here we focus on two of these: (1) SGCls: Given ground-truth bounding boxes for entities, predict all entity categories and relations categories. (2) PredCls: Given bounding boxes annotated with entity labels, predict all relations. Following <ref type="bibr" target="#b18">[19]</ref>, we used Recall@K as the evaluation metric. It measures the fraction of correct ground-truth triplets that appear within the K most confident triplets proposed by the model. Two evaluation protocols are used in the literature differing in whether they enforce graph constraints over model predictions. The first graph-constrained protocol requires that the top-K triplets assign one consistent class per entity and relation. The second unconstrained protocol does not enforce any such constraints. We report results on both protocols, following <ref type="bibr" target="#b31">[32]</ref>.</p><p>Models and baselines. We compare four variants of our GPI approach with the reported results of four baselines that are currently the state-of-the-art on various scene graph prediction problems (all models use the same data split and pre-processing as <ref type="bibr" target="#b28">[29]</ref>): 1) LU ET AL., 2016 <ref type="bibr" target="#b18">[19]</ref>: This work leverages word embeddings to fine-tune the likelihood of predicted relations. 2) XU ET AL, 2017 <ref type="bibr" target="#b28">[29]</ref>: This model passes messages between entities and relations, and iteratively refines the feature map used for prediction. 3) NEWELL &amp; DENG, 2017 <ref type="bibr" target="#b20">[21]</ref>: The PIXEL2GRAPH model uses associative embeddings <ref type="bibr" target="#b21">[22]</ref> to produce a full graph from the image. 4) YANG ET AL., 2018 <ref type="bibr" target="#b29">[30]</ref>: The GRAPH R-CNN model uses object-relation regularities to sparsify and reason over scene graphs. 5) ZELLERS ET AL., 2017 <ref type="bibr" target="#b31">[32]</ref>: The NEURALMOTIF method encodes global context for capturing high-order motifs in scene graphs, and the BASELINE outputs the entities and relations distributions without using the global context. The following variants of GPI were compared: 1) GPI: NO ATTENTION: Our GPI model, but with no attention mechanism. Instead, following Theorem 1, we simply sum the features. 2) GPI: NEIGHBORATTENTION: Our GPI model, with attention over neighbors features. 3) GPI: LINGUISTIC: Same as GPI: NEIGHBORATTENTION but also concatenating the word embedding vector, as described above.</p><p>Results. <ref type="table">Table 1</ref> shows recall@50 and recall@100 for three variants of our approach, and compared with five baselines. All GPI variants performs well, with LINGUISTIC outperforming all baselines for SGCls and being comparable to the state-of-the-art model for PredCls. Note that PredCl is an easier task, which makes less use of the structure, hence it is not surprising that GPI achieves similar accuracy to <ref type="bibr" target="#b31">[32]</ref>. <ref type="figure" target="#fig_4">Figure 4</ref> illustrates the model behavior. Predicting isolated labels with z i (4c) mislabels several entities, but these are corrected at the final output (4d). <ref type="figure" target="#fig_4">Figure 4e</ref> shows that the system learned to attend more to nearby entities (the window and building are closer to the tree), and 4f shows that stronger attention is learned for the class bird, presumably because it is usually more informative than common classes like tree.</p><p>Implementation details. The φ and α networks were each implemented as a single fully-connected (FC) layer with a 500-dimensional outputs. ρ was implemented as a FC network with 3 500-dimensional hidden layers, with one 150-dimensional output for the entity probabilities, and one 51-dimensional output for relation probabilities. The attention mechanism was implemented as a network like to φ and α, receiving the same inputs, but using the output scores for the attention . The full code is available at https://github.com/shikorab/SceneGraph</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>We presented a deep learning approach to structured prediction, which constrains the architecture to be invariant to structurally identical inputs. As in score-based methods, our approach relies on pairwise features, capable of describing inter-label correlations, and thus inheriting the intuitive aspect of score-based approaches. However, instead of maximizing a score function (which leads to computationally-hard inference), we directly produce an output that is invariant to equivalent representations of the pairwise terms.</p><p>This axiomatic approach to model architecture can be extended in many ways. For image labeling, geometric invariances (shift or rotation) may be desired. In other cases, invariance to feature permutations may be desirable. We leave the derivation of the corresponding architectures to future work. Finally, there may be cases where the invariant structure is unknown and should be discovered from data, which is related to work on lifting graphical models <ref type="bibr" target="#b3">[4]</ref>. It would be interesting to explore algorithms that discover and use such symmetries for deep structured prediction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Left: Graph permutation invariance. A graph labeling function F is graph permutation invariant (GPI) if permuting the node features maintains the output. Right: a schematic representation of the GPI architecture in Theorem 1. Singleton features zi are omitted for simplicity. (a) First, the features zi,j are processed element-wise by φ. (b) Features are summed to create a vector si, which is concatenated with zi. (c) A representation of the entire graph is created by applying α n times and summing the created vector. (d) The graph representation is then finally processed by ρ together with z k .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Accuracy as a function of sample size for graph labeling. Right is a zoomed in version of left.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 ,</head><label>3</label><figDesc>Figure 3, shows the results, demonstrating that GPI requires far fewer samples to converge to the correct solution. This illustrates the advantage of an architecture with the correct inductive bias for the problem.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>( 2 )</head><label>2</label><figDesc>Bounding box information given as (left, bottom, width, height); (3) The number of smaller entities (also bigger); (4) The number of entities to the left, right, above and below. (5) The number of entities with higher and with lower confidence; (6) For the linguistic model only: word embedding of the most probable class. Word vectors were learned with GLOVE from the ground-truth captions of Visual Genome.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a) An input image with bounding boxes from VG. (b) The ground-truth scene graph. (c) The Baseline fails to recognize some entities (tail and tree) and relations (in front of instead of looking at). (d) GPI:LINGUISTIC fixes most incorrect LP predictions. (e) Window is the most significant neighbor of Tree. (f) The entity bird receives substantial attention, while Tree and building are less informative.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">For simplicity, we focus on the task where boxes are given. 3 The baseline does not use any LSTM or context, and is thus unrelated to the main contribution of [32].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by the ISF Centers of Excellence grant, and by the Yandex Initiative in Machine Learning. Work by GC was performed while at Google Brain Research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">End-to-end learning for structured prediction energy networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Belanger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bishan</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Mccallum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="429" to="439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Irwan</forename><surname>Bello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hieu</forename><surname>Pham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Quoc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Samy</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.09940</idno>
		<title level="m">Neural combinatorial optimization with reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Automorphism groups of graphical models and lifted variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hung</forename><forename type="middle">Hai</forename><surname>Bui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tuyen</forename><forename type="middle">N</forename><surname>Huynh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Riedel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI&apos;13</title>
		<meeting>the Twenty-Ninth Conference on Uncertainty in Artificial Intelligence, UAI&apos;13</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="132" to="141" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danqi</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP)</title>
		<meeting>the 2014 conference on empirical methods in natural language processing (EMNLP)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Second International Conference on Learning Representations</title>
		<meeting>the Second International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning deep structured models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">G</forename><surname>Liang Chieh Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alan</forename><forename type="middle">L</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clement</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Camille</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="1915" to="1929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Neural message passing for quantum chemistry</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Samuel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Patrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><forename type="middle">E</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep value networks learn to evaluate and iteratively refine structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Gygli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anelia</forename><surname>Angelova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning</title>
		<editor>Doina Precup and Yee Whye Teh</editor>
		<meeting>the 34th International Conference on Machine Learning<address><addrLine>Sydney, Australia</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="1341" to="1351" />
		</imprint>
	</monogr>
	<note>International Convention Centre</note>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Image generation from scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Agrim</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1804.01622</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3668" to="3678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning combinatorial optimization algorithms over graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elias</forename><surname>Khalil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanjun</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuyu</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bistra</forename><surname>Dilkina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6351" to="6361" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oliver</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Justin</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kenji</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joshua</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephanie</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li-Jia</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="32" to="73" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Conditional random fields: Probabilistic models for segmenting and labeling sequence data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lafferty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mccallum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th International Conference on Machine Learning</title>
		<meeting>the 18th International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="282" to="289" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">On support relations and semantic scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wentong</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">Ying</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hanno</forename><surname>Ackermann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bodo</forename><surname>Rosenhahn</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05834</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deeply learning the messages in message passing inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guosheng</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chunhua</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anton</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="361" to="369" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Visual relationship detection with language priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ranjay</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei-Fei</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="852" to="869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning efficiently with approximate inference via dual losses</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Meshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sontag</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jaakkola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Globerson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Machine Learning</title>
		<meeting>the 27th International Conference on Machine Learning<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="783" to="790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Pixels to graphs by associative embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="1172" to="1180" />
		</imprint>
	</monogr>
	<note>to appear</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Associative embedding: End-to-end learning for joint detection and grouping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alejandro</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiao</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jia</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Inform. Process. Syst</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2274" to="2284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">An effective neural network model for graphbased dependency parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenzhe</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baobao</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 53rd Annual Meeting of the Association for Computationa Linguistics</title>
		<meeting>the 53rd Annual Meeting of the Association for Computationa Linguistics</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="313" to="322" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive imagelanguage cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bryan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">M</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julia</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1946" to="1955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Discovering objects and their relations from entangled scene representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Battaglia</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05068</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Fully connected deep structured networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Urtasun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>ArXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="640" to="651" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Max margin Markov networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guestrin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>S. Thrun, L. Saul, and B. Schölkopf</editor>
		<meeting><address><addrLine>Cambridge, MA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2004" />
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Scene Graph Generation by Iterative Message Passing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Danfei</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuke</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conf. Comput. Vision Pattern Recognition</title>
		<meeting>Conf. Comput. Vision Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3097" to="3106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Graph R-CNN for scene graph generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianwei</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiasen</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devi</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conf. Comput. Vision</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="690" to="706" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2017" />
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="page" from="3394" to="3404" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rowan</forename><surname>Zellers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Yatskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Thomson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yejin</forename><surname>Choi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.06640</idno>
		<idno>abs/1711.06640</idno>
		<title level="m">Neural motifs: Scene graph parsing with global context</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sadeep</forename><surname>Jayasumana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernardino</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vibhav</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhizhong</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dalong</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philip Hs</forename><surname>Torr</surname></persName>
		</author>
		<title level="m">Conditional random fields as recurrent neural networks. In Proceedings of the IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1529" to="1537" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
