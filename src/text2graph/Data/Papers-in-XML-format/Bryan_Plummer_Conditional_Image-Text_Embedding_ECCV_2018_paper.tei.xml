<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:17+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Image-Text Embedding Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bryan</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
							<email>bplumme2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paige</forename><surname>Kordas</surname></persName>
							<email>pkordas2@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hadi Kiapour</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuai</forename><surname>Zheng</surname></persName>
							<email>shuzheng@ebay.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robinson</forename><surname>Piramuthu</surname></persName>
							<email>rpiramuthu@ebay.com</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Svetlana</forename><surname>Lazebnik</surname></persName>
							<email>slazebni@illinois.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ebay</forename><forename type="middle">Inc</forename></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Illinois at Urbana-Champaign †</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Image-Text Embedding Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Natural language grounding</term>
					<term>phrase localization</term>
					<term>embed- ding methods</term>
					<term>conditional models</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. This paper presents an approach for grounding phrases in images which jointly learns multiple text-conditioned embeddings in a single end-to-end model. In order to differentiate text phrases into semantically distinct subspaces, we propose a concept weight branch that automatically assigns phrases to embeddings, whereas prior works predefine such assignments. Our proposed solution simplifies the representation requirements for individual embeddings and allows the underrepresented concepts to take advantage of the shared representations before feeding them into concept-specific layers. Comprehensive experiments verify the effectiveness of our approach across three phrase grounding datasets, Flickr30K Entities, ReferIt Game, and Visual Genome, where we obtain a (resp.) 4%, 3%, and 4% improvement in grounding performance over a strong region-phrase embedding baseline 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Phrase grounding attempts to localize a given natural language phrase in an image. This constituent task has applications to image captioning <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b33">34]</ref>, image retrieval <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b25">26]</ref>, and visual question answering <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b6">7]</ref>. Research on phrase grounding has been spurred by the release of several datasets, some of which primarily contain relatively short phrases <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b17">18]</ref>, while others contain longer queries, including entire sentences that can provide rich context <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b21">22]</ref>. The difference in query length compounds the already challenging problem of generalizing to any (including never before seen) natural language input. Despite this, much of the recent attention has focused on learning a single embedding model between image regions and phrases <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b20">21]</ref>.</p><p>In this paper, we propose a Conditional Image-Text Embedding (CITE) network that jointly learns different embeddings for subsets of phrases ( <ref type="figure">Figure 1</ref>). This enables our model to train separate embeddings for phrases that share a concept. Each conditional embedding can learn a representation specific to a Our CITE model separates phrases into different groups and learns conditional embeddings for these groups in a single end-to-end model. Assignments of phrases to embeddings can either be pre-defined (e.g. by separating phrases into distinct concepts like people or clothing), or can be jointly learned with the embeddings using the concept weight branch. Similarly colored blocks refer to layers of the same type, with purple blocks representing fully connected layers. Best viewed in color subset of phrases while also taking advantage of weights that are shared across phrases. This is especially important for smaller groups of phrases that would be prone to overfitting if we were to train separate embeddings for them. In contrast to similar approaches that manually determine how to group concepts <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref>, we use a concept weight branch, trained jointly with the rest of the network, to do a soft assignment of phrases to learned embeddings automatically. The concept weight branch can be thought of producing a unique embedding for each region-phrase pair based on a phrase-specific linear combination of individual conditional embeddings. By training multiple embeddings our model also reduces variance akin to an ensemble of networks, but with far fewer parameters and lower computational cost. Our idea of conditional embeddings was directly inspired by the conditional similarity networks of Veit et al. <ref type="bibr" target="#b29">[30]</ref>, although that work does not deal with cross-modal data and does not attempt to automatically assign different input items to different similarity subspaces. An earlier precursor of the idea of conditional similarity metrics can be found in <ref type="bibr" target="#b1">[2]</ref>. Our work is also similar in spirit to Zhang et al . <ref type="bibr" target="#b36">[37]</ref>, who produced a linear classifier used to discriminate between image regions based on the textual input.</p><p>Our primary focus is on improving methods of associating individual image regions with individual phrases. Orthogonal to this goal, other works have focused on performing global inference for multiple phrases in a sentence and multiple regions in an image. Wang et al . <ref type="bibr" target="#b32">[33]</ref> modeled the pronoun relationships between phrases and forced each phrase prediction associated with a caption to be assigned to a different region. Chen et al . <ref type="bibr" target="#b2">[3]</ref> also took into account the predictions made by other phrases when localizing phrases and incorporated bounding box regression to improve their region proposals. In their follow-up work <ref type="bibr" target="#b3">[4]</ref>, they introduced a region proposal network for phrases effectively reproducing the full Faster RCNN detection pipeline <ref type="bibr" target="#b26">[27]</ref>. Yu et al . <ref type="bibr" target="#b35">[36]</ref> took into account the visual similarity of objects in a single image when providing context for their predictions. Plummer et al . <ref type="bibr" target="#b23">[24]</ref> performed global inference using a wide range of image-language constraints derived from attributes, verbs, prepositions, and pronouns. Yeh et al . <ref type="bibr" target="#b34">[35]</ref> used a word prior in combination with segmentation masks, geometric features, and detection scores to select a region from all possible bounding boxes in an image. Many of these modifications could be used in combination with our approach to further improve performance.</p><p>The contributions of our paper are summarized below:</p><p>-By conditioning the embedding used by our model on the input phrase we simplify the representation requirements for each embedding, leading to a more generalizable model. -We introduce a concept weight branch which enables our embedding assignments to be learned jointly with the image-text model. -We introduce several improvements to the Similarity Network of Wang et al . <ref type="bibr" target="#b31">[32]</ref> boosting the baseline model's localization performance by 3.5% over the original paper. -We perform extensive experiments over three datasets, Flickr30K Entities <ref type="bibr" target="#b24">[25]</ref>, ReferIt Game <ref type="bibr" target="#b14">[15]</ref>, and Visual Genome <ref type="bibr" target="#b17">[18]</ref>, where we report a (resp.) 4%, 3% and 4% improvement in phrase grounding performance over the baseline.</p><p>We begin Section 2.1 by describing the image-text Similarity Network <ref type="bibr" target="#b31">[32]</ref> that we use as our baseline model. Section 2.2 describes our text-conditioned embedding model. Section 2.3 discusses three methods of assigning phrases to the trained embeddings. Lastly, Section 3 contains detailed experimental results and analysis of our proposed approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Our Approach</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Image-Text Similarity Network</head><p>Given an image and a phrase, our goal is to select the most likely location of the phrase from a set of region proposals. To accomplish this, we build upon the image-text similarity network introduced in Wang et al . <ref type="bibr" target="#b31">[32]</ref>. The image and text branches of this network each have two fully connected layers with batch normalization <ref type="bibr" target="#b10">[11]</ref> and ReLUs. The final outputs of these branches are L2 normalized before performing an element-wise product between the image and text representations. This representation is then fed into a triplet of fully connected layers using batch normalization and ReLUs. This is analogous to using the CITE model in <ref type="figure">Figure 1</ref> with a single conditional embedding.</p><p>The training objective for this network is a logistic regression loss computed over phrases P , the image regions R, and labels Y . The label y ij for the ith input phrase and jth region is +1 where they match and −1 otherwise. Since this is a supervised learning approach, matching pairs of phrases and regions need to be provided in the annotations of each dataset. After producing some score x ij measuring the affinity between the image region and text features using our network, the loss is given by</p><formula xml:id="formula_0">L sim (P, R, Y ) = ij log(1 + exp (−y ij x ij )).<label>(1)</label></formula><p>In this formulation, it is easy to consider multiple regions for a given phrase as positive examples and to use a variable number of region proposals per image. This is in contrast to competing methods which score regions with softmax with a cross entropy loss over a set number of proposals per image (e.g. <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b2">3]</ref>).</p><p>Sampling phrase-region training pairs. Following Wang et al . <ref type="bibr" target="#b31">[32]</ref>, we consider any regions with at least 0.6 intersection over union (IOU) with the ground truth box for a given phrase as a positive example. Negative examples are randomly sampled from regions of the same image with less than 0.3 IOU with the ground truth box. We select twice the number of negative regions as we have positive regions for a phrase. If too few negative regions occur for an image-phrase pair, then the negative example threshold is raised to 0.4 IOU.</p><p>Features. We represent phrases using the HGLMM fisher vector encoding <ref type="bibr" target="#b16">[17]</ref> of word2vec <ref type="bibr" target="#b22">[23]</ref> PCA reduced down to 6,000 dimensions. We generate region proposals using Edge Boxes <ref type="bibr" target="#b37">[38]</ref>. Similarly to most state-of-the-art methods on our target datasets, we represent image regions using a Fast RCNN network <ref type="bibr" target="#b7">[8]</ref> finetuned on the union of PASCAL 2007 and 2012 trainval sets <ref type="bibr" target="#b4">[5]</ref>. The only exception is the experiment reported in <ref type="table" target="#tab_1">Table 1</ref>(d), where we fine-tune the Fast RCNN parameters (corresponding to the VGG16 box in <ref type="figure">Figure 1</ref>) on the Flickr30K Entities dataset.</p><p>Spatial location. Following <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b35">36]</ref>, we experiment with concatenating bounding box location features to our region representation. This way our model can learn to bias predictions for phrases based on their location (e.g. that sky typically occurs in the top part of an image). For Flickr30K Entities we encode this spatial information as defined in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref>   <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, experiments on the ReferIt Game dataset encode the spatial information as an 8-dimensional feature vector [x min , y min , x max , y max , x center , y center , w, h]. For Visual Genome we adopt the same method of encoding spatial location as used for the ReferIt Game dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Conditional Image-Text Network</head><p>Inspired by Veit et al . <ref type="bibr" target="#b29">[30]</ref>, we modify the image-text similarity model of the previous section to learn a set of conditional or concept embedding layers de- <ref type="figure">Figure 1</ref>. These are K parallel fully connected layers each with output dimensionality M . The outputs of these layers, in the form of a matrix of size M × K, are fed into the embedding fusion layer, together with a K-dimensional concept weight vector U , which can be produced by several methods, as discussed in Section 2.3. The fusion layer simply performs a matrixvector product, i.e., F = CU . This is followed by another fully connected layer representing the final classifier (i.e., the layer's output dimension is 1).</p><formula xml:id="formula_1">noted C 1 , . . . C K in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Embedding Assignment</head><p>This section describes three possible methods for producing the concept weight vector U for combining the conditional embeddings as introduced in Section 2.2.</p><p>Coarse categories. The Flickr30K Entities dataset comes with hand-constructed dictionaries that group phrases into eight coarse categories: people, clothing, body parts, animals, vehicles, instruments, scene, other. We use these dictionaries to map phrases to binary concept vectors representing their group membership. This is analogous to the approach of Veit et al . <ref type="bibr" target="#b29">[30]</ref>, which defines the concepts based on meta-data labels. Both the remaining approaches base their assignments on the training data rather than a hand-defined category label.</p><p>Nearest cluster center. A simple method of creating concept weights is to perform K-means clustering on the text features of the queries in the test set. Each cluster center becomes its own concept to learn. The concept weights U are then encoded as one-hot cluster membership vectors which we found to work better than alternatives such as similarity of a sample to each cluster center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept weight branch.</head><p>Creating a predefined set of concepts to learn, either using dictionaries or K-means clustering, produces concepts that don't necessarily have anything to do with the difficulty or ease in localizing the phrases within them. An alternative is to let the model decide which concepts to learn. With this in mind, we feed the raw text features into a separate branch of the network consisting of two fully connected layers with batch normalization and a ReLU between them, followed by a softmax layer to ensure the output sums to 1 (denoted as the concept weight branch in <ref type="figure">Figure 1</ref>). The output of the softmax is then used as the concept weights U . This can be seen as analogous to using soft attention <ref type="bibr" target="#b33">[34]</ref> on the text features to select concepts for the final representation of a phrase. We use L1 regularization on the output of the last fully connected layer before being fed into the softmax to promote sparsity in our assignments. The training objective for our full CITE model then becomes</p><formula xml:id="formula_2">L CIT E = L sim (P, R, Y ) + λ φ 1 ,<label>(2)</label></formula><p>where φ are the inputs to the softmax layer and λ is a parameter controlling the importance of the regularization term. Note that we do not enforce diversity of assignments between different phrases, so it is possible that all phrases attend to a single embedding. However, we do not see this actually occur in practice.</p><p>We also tried to use entropy minimization rather then L1 regularization for our concept weight branch as well as hard attention instead of soft attention, but found all worked similarly in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Datasets and Protocols</head><p>We evaluate the performance of our phrase-region grounding model on three datasets: Flickr30K Entities <ref type="bibr" target="#b24">[25]</ref>, ReferIt Game <ref type="bibr" target="#b14">[15]</ref>, and Visual Genome <ref type="bibr" target="#b17">[18]</ref>. The metric we report is the proportion of correctly localized phrases in the test set. Consistent with prior work, a 0.5 IOU between the best-predicted box for a phrase and its ground truth is required for a phrase to be considered successfully localized. Similarly to <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b3">4]</ref>, for phrases associated with multiple bounding boxes, the phrase is represented as the union of its boxes. Training procedure. We begin training our models with Adam <ref type="bibr" target="#b15">[16]</ref>. After every epoch, we evaluate our model on the validation set. After it hasn't improved performance for 5 epochs, we fine-tune our model with stochastic gradient descent at 1/10th the learning rate and the same stopping criteria. We report test set performance for the model that performed best on the validation set.</p><p>Comparative evaluation. In addition to comparing to previously published numbers of state-of-the-art approaches on each dataset, we systematically evaluate the following baselines and variants of our model:</p><p>-Similarity Network. Our first baseline is given by our own implementation of the model from Wang et al . <ref type="bibr" target="#b31">[32]</ref>, trained using the procedure described above. Phrases are pre-processed using stop word removal rather than partof-speech filtering as done in the original paper. This change, together with a more careful tuning of the training settings, leads to a 2.5% improvement in performance over the reported results in <ref type="bibr" target="#b31">[32]</ref>. The model is further enhanced by using the spatial location features (Section 2.1), resulting in a total improvement of 3.5%. -Individual Coarse Category Similarity Networks. We train multiple Similarity Networks on different subsets of the data created according to the coarse category assignments as described in Section 2.3. -Individual K-means Similarity Networks. We train multiple Similarity</p><p>Networks on different subsets of the data created according to the nearest cluster center assignments as described in Section 2. Phrases are matched to embeddings using nearest cluster center assignments. -CITE, Learned. Our full model with the concept weight branch used to automatically produce concept weights as described in Section 2.3. 43.89 GroundeR <ref type="bibr" target="#b27">[28]</ref> 47.81 MCB <ref type="bibr" target="#b6">[7]</ref> 48.69 RtP <ref type="bibr" target="#b24">[25]</ref> 50.89 Similarity Network <ref type="bibr" target="#b31">[32]</ref> 51.05 IGOP <ref type="bibr" target="#b34">[35]</ref> 53.97 SPC <ref type="bibr" target="#b23">[24]</ref> 55.49 MCB + Reg + Spatial <ref type="bibr" target="#b2">[3]</ref> 51.01 MNN + Reg + Spatial <ref type="bibr" target="#b2">[3]</ref> 55 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Flickr30K Entities</head><p>We use the same splits as Plummer et al . <ref type="bibr" target="#b24">[25]</ref>, which separates the images into 29,783 for training, 1,000 for testing, and 1,000 for validation. Models are trained with a batch size of 200 (128 if necessary to fit into GPU memory) and learning rate of 5e-5. We set λ = 5e-5 in Eq. (2). We use the top 200 Edge Box proposals per image and embedding dimension M = 256 unless stated otherwise.</p><p>Grounding Results. <ref type="table" target="#tab_1">Table 1</ref> compares overall localization accuracies for a number of methods. The numbers for our Similarity Network baseline are reported in <ref type="table" target="#tab_1">Table 1</ref>(b), and as stated above, they are better than the published numbers from <ref type="bibr" target="#b31">[32]</ref>.  <ref type="table" target="#tab_1">Table 1</ref>(c) that going from 200 to 500 bounding box proposals provides a small boost in localization accuracy. This results in our best performance using PASCAL-tuned features which is 3% better than the prior work reported in <ref type="table" target="#tab_1">Table 1</ref>(a) and 4.5% better than the Similarity Network. We also note that the time to test an image-phrase pair is almost unaffected using our approach (the CITE, Learned, K=4 model performs inference on 200 Edge Boxes at 0.182 seconds per pair using a NVIDIA Titan X GPU with our implementation) compared with the baseline Similarity Network (0.171 seconds per pair). Finally, <ref type="table" target="#tab_1">Table 1(d)</ref> gives results for models whose visual features were fine-tuned for localization on the Flickr30K Entities dataset. Our model still obtains a 1.5% improvement over the approach of Chen et al . <ref type="bibr" target="#b3">[4]</ref>, which used bounding box regression as well as a region proposal network. In principle, we could also incorporate these techniques to further improve the model. <ref type="table" target="#tab_4">Table 2</ref> breaks down localization accuracy by coarse category. Of particular note are our results on the challenging body part category, which are typically small and represent only 3.5% of the phrases in the test set, improving over the next best model as well as the Similarity Network trained on just body part phrases by 10% when using Flickr30K-tuned features. We also see a substantial improvement in the vehicles and other categories, seeing a 5-9% improvement over the previous state-of-the-art. The only category where we perform worse are phrases referring to scenes, which commonly cover the majority (or entire) image. Here, incorporating a bias towards selecting larger proposals, as in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>, can lead to significant improvements.</p><p>Parameter Selection. In addition to reporting the localization performance, we also provide some insight into the effect of different parameter choices and what information our model is capturing. In <ref type="figure">Figure 2</ref> we show how the number K of learned embeddings affects performance. Using our concept weight branch consistently outperforms K-means cluster assignments. <ref type="table">Table 3</ref> shows how the embedding dimensionality M affects performance. Here we see that reducing the output dimension from 256 to 64 (i.e., by 1/4th) leads to a minor (1%) decrease in performance. This result is particularly noteworthy as the CITE network with K = 4, M = 64 has 4 million parameters compared the 14 million the baseline Similarity Network has with M = 256 while still maintaining a  <ref type="figure">Fig. 2</ref>. Effect of the number of learned embeddings (K) on Flickr30K Entities localization accuracy using PASCAL-tuned features 3% improvement in performance. We also experimented with different ways of altering the Similarity Network to have the same number of parameters to ours at similar points (e.g. increasing the last fully connected layer to be K times larger or adding K additional layers), but found they performed comparably to the baseline Similarity Network (i.e. their performance was about 4% worse than our approach). In addition to experiments on how many layers to use and the size of each layer, we also explored the effect the number of Edge Boxes has on performance in <ref type="table">Table 4</ref>. In contrast to some prior work which performed best using 200 candidates (e.g. <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b23">24]</ref>), our model's increased discriminate power enables us to still be able to obtain a benefit from using up to 500 proposals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Concept Weight Branch Examination.</head><p>To analyze what our model is learning, <ref type="figure" target="#fig_2">Figure 3</ref> shows the means and standard deviations of the weights over the different embeddings broken down by coarse categories. Interestingly, people end up being split between two embeddings. We find that people phrases tend to be split by plural vs. singular. <ref type="table">Table 5</ref> gives a closer look at the conditional embeddings by listing the ten phrases with the highest weight for each embedding. While most phrases give the first embedding little weight, it appears to provide the most benefit for finding very specific references to people rather than generic terms (e.g. little curly hair girl instead of girl itself). These patterns generally <ref type="table">Table 3</ref>. Localization accuracy with different embedding sizes using the CITE, Learned, K = 4 model on Flickr30K Entities with PASCAL-tuned features. Embedding size refers to M , the output dimensionality of layers P1 and the conditional embeddings in <ref type="figure">Figure 1</ref>. The remaining fully connected layers' output dimensions (excluding those that are part of the VGG16 network) are four times the embedding size hold through multiple runs of the model, indicating they are important concepts to learn for the task.</p><p>Qualitative Results. <ref type="figure" target="#fig_3">Figure 4</ref> gives a look into areas where our model could be improved. Of the phrases that occur at least 100 times in the test set, the lowest performing phrases are street and people at (resp.) 60% and 64% accuracy. The highest performing of these common phrases is man at 81% accuracy, which also happens to be the most common phrase with 1065 instances in the test set. In the top-left example of <ref type="figure" target="#fig_3">Figure 4</ref>, the word people, which is not correctly localized, refers to partially visible background pedestrians. Analyzing the saliency of a phrase in the context of the whole caption may lead to treating these phrases differently. Global inference constraints, for example, a requirement that predictions for a man and a woman must be different, would be useful for the top-center example. Performing pronoun resolution, as attempted in <ref type="bibr" target="#b23">[24]</ref>, would help in the top-right example. In the test set, the pronoun one is correctly localized around 36% of the time, whereas the blond woman is correctly localized 81% of the time. Having an understanding of relationships between entities may help in cases such as the bottom-left example of <ref type="figure" target="#fig_3">Figure 4</ref>, where the extent of the table could be refined by knowing that the groceries are "on" it. Our model also performs relatively poorly on phrases referring to classic "stuff" categories, as shown in the bottom-center and bottom-right examples. The water and street phrases in these examples are only partly localized. Using pixel-level predictions may help to recover the full extent of these types of phrases since the parts of the images they refer to are relatively homogeneous.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">ReferIt Game</head><p>We use the same splits as Hu et al . <ref type="bibr" target="#b9">[10]</ref>, which consist of 10,000 images combined for training and validation with the remaining 10,000 images for testing. Models are trained with a batch size of 128, learning rate of 5e-4, and λ = 5e-4 in Eq. (2). We generate 500 Edge Box proposals per image.</p><p>Results. <ref type="table" target="#tab_6">Table 6</ref> reports the localization accuracy across the ReferIt Game test set. The first line of <ref type="table" target="#tab_6">Table 6</ref>(b) shows that our model using the nearest cluster center assignments results in a 2.5% improvement over the baseline Similarity Network. Using our concept weight branch in order to learn assignments yields an additional small improvement.</p><p>We note that we do not outperform the approach of Yeh et al . <ref type="bibr" target="#b34">[35]</ref> on this dataset. This can likely be attributed to the failures of Edge Boxes to produce adequate proposals on the ReferIt Game dataset. Oracle performance using the top 500 proposals is 93% on Flickr30K Entities, while it is only 86% on this dataset. As a result, the specialized bounding box methods used by Yeh et al . as</p><p>A woman painting on the sidewalk of a busy street as people walk by her.</p><p>A man with a hat and a woman with a black top are walking on a grass field.</p><p>Two blond females in public, one handing out fliers and the other holding a bunch of multicolored balloons.</p><p>A woman puts new groceries on the table.</p><p>A lady by the water is grasping a black pot.</p><p>A bicyclist with a backpack rides down a suburban street. well as Chen et al . <ref type="bibr" target="#b2">[3]</ref> may play a larger role here. Our model would also likely benefit from these improved bounding boxes.</p><p>As with the Flickr30K Entities dataset, we show the effect of the number K of embeddings on localization performance in <ref type="figure">Figure 5</ref>. While the concept weight branch provides a small performance improvement across many different choices of K, when K = 2 the clustering assignments actually perform a little better. However, this behavior is atypical in our experiments across all three datasets, and may simply be due to the small size of the ReferIt Game training data, as it has far fewer ground truth phrase-region pairs to train our models with.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Visual Genome</head><p>We use the same splits as Zhang et al . <ref type="bibr" target="#b36">[37]</ref>, consisting of 77,398 images for training and 5,000 each for testing and validation. Models are trained with a learning rate of 5e-5, and λ = 5e-4 in Eq. (2). We generate 500 Edge Box proposals per image, and use a batch size of 128. Results. <ref type="table">Table 7</ref> reports the localization accuracy across the Visual Genome dataset. <ref type="table">Table 7</ref>(a) lists published numbers from several recent methods. The current state of the art performance belongs to Zhang et al . <ref type="bibr" target="#b36">[37]</ref>, who finetuned visual features on this dataset and created a cleaner set during training by pruning ambiguous phrases. We did not perform either fine-tuning or phrase pruning, so the most comparable reference number for our methods is their 17.5% accuracy without these steps. The baseline accuracies for our Similarity Network with and without spatial features are given in the last two lines of <ref type="table">Table 7</ref>(a). We can see that including the spatial features gives only a small improvement. This is likely due to the denser annotations in this dataset as compared to Flickr30K Entities. For example, a phrase like a man in Flickr30K Entities would typically refer to a relatively large region towards the center since background instances are commonly not mentioned in an image-level caption. However, entities in Visual Genome include both foreground and background instances.</p><p>In the first line of <ref type="table">Table 7</ref>(b), we see our K-means model is 3.5% better than the Similarity Network baseline, and over 6% better than the 17.5% accuracy of <ref type="bibr" target="#b36">[37]</ref>. According to the second line of <ref type="table">Table 7</ref>(b), using the concept weight branch obtains a further improvement. In fact, our full model with pre-trained PASCAL features has better performance than <ref type="bibr" target="#b36">[37]</ref> with fine-tuned features.</p><p>As with the other two datasets, <ref type="figure" target="#fig_4">Figure 6</ref> reports performance as a function of the number of learned embeddings. Echoing most of the earlier results, we see a consistent improvement for the learned embeddings over the K-means ones. The large size of this dataset (&gt; 250,000 instances in the test set) helps to reinforce the significance of our results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>This paper introduced a method of learning a set of conditional embeddings and phrase-to-embedding assignments in a single end-to-end network. The ef- <ref type="table">Table 7</ref>. Phrase localization performance on Visual Genome. (a) Published results and our Similarity Network baselines. APP refers to ambiguous phrase pruning (see <ref type="bibr" target="#b36">[37]</ref> for details). (b) Our best-performing conditional models Method Accuracy (a) State-of-the-art Densecap <ref type="bibr" target="#b12">[13]</ref> 10.1 SCRC <ref type="bibr" target="#b9">[10]</ref> 11.0 DBNet <ref type="bibr" target="#b36">[37]</ref> 17.5 DBNet (with APP) <ref type="bibr" target="#b36">[37]</ref> 21.2 DBNet (with APP, V. Genome-tuned Features) <ref type="bibr" target="#b36">[37]</ref>   fectiveness of our approach was demonstrated on three popular and challenging phrase-to-region grounding datasets. In future work, our model could be further improved by including a term to enforce that distinct concepts are being learned by each embedding. Our experiments focused on localizing individual phrases to a fixed set of category-independent region proposals. As such, our absolute accuracies could be further improved by incorporating a number of orthogonal techniques used in competing work. By jointly predicting multiple phrases in an image our model could take advantage of relationships between multiple entities (e.g. <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>). Including bounding box regression and a region proposal network as done in <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b3">4]</ref> would also likely lead to a better model. In fact, tying the regression parameters to a specific concept embedding may further improve performance since it would simplify our prediction task as a result of needing to learn parameters for just the phrases assigned to that embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Fig. 1. Our CITE model separates phrases into different groups and learns conditional embeddings for these groups in a single end-to-end model. Assignments of phrases to embeddings can either be pre-defined (e.g. by separating phrases into distinct concepts like people or clothing), or can be jointly learned with the embeddings using the concept weight branch. Similarly colored blocks refer to layers of the same type, with purple blocks representing fully connected layers. Best viewed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>3. -CITE, Coarse Categories. No concept weight branch. Phrases are as- signed according to their coarse category. -CITE, Random. No concept weight branch. Phrases are randomly assigned to an embedding. At test time, phrases seen during training keep their as- signments, while new phrases are randomly assigned. -CITE, K-means. No concept weight branch.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. The mean weight for each embedding (left) along with the standard deviation of those weights (right) broken down by coarse category for the Flickr30K Entities dataset using Flickr30K-tuned features</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Examples demonstrating some common failure cases on the Flickr30K Entities dataset. See Section 3.2 for discussion</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Effect of the number of learned embeddings on performance on the Visual Genome with models trained on 1/3 of the available training data</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>for this dataset. For an image of height H and width W and a box with height h and width w is encoded as [x min /W, y min /H, x max /W, y max /H, wh/W H]. For a fair comparison to prior work</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Phrase localization performance on the Flickr30k Entities test set.</figDesc><table>(a) State-
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 (</head><label>1</label><figDesc>c) reports results for variants of conditional embed- ding models. From the first two lines, we can see that learning embeddings from subsets of the data without any shared weights leads to only a small improvement (≤ 1%) over the Similarity Network baseline. The third line of Table 1(c) reports that separating phrases by manually defined high-level concepts only leads to a 1% improvement even when weights are shared across embeddings.</figDesc><table>This is likely 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Comparison of phrase grounding performance over coarse categories on the Flickr30K Entities dataset. Our models were tested with 500 Edge Box proposals</figDesc><table>People 
Cloth-Body Anim-Vehi-Instru-Scene Other 
ing Parts als 
cles ments 
PASCAL-tuned Features 
GroundeR [28] 
61.00 38.12 10.33 62.55 68.75 36.42 58.18 29.08 
RtP [25] 
64.73 46.88 17.21 65.83 68.75 37.65 51.39 31.77 
IGOP [35] 
68.71 56.83 19.50 70.07 73.75 39.50 60.38 32.45 
MCB + Reg + Spatial [3] 
62.75 43.67 14.91 65.44 65.25 24.74 64.10 34.62 
MNN + Reg + Spatial [3] 
67.38 47.57 20.11 73.75 72.44 29.34 63.68 37.88 
CITE, Learned, K = 4 + Spatial 73.20 52.34 30.59 76.25 75.75 48.15 55.64 42.83 
Flickr30K-tuned Features 
PGN + QRN + Spatial [4] 
75.05 55.90 20.27 73.36 68.95 45.68 65.27 38.80 
CITE, Learned, K = 4 + Spatial 75.95 58.50 30.78 77.03 79.25 48.15 58.78 43.24 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 6 .</head><label>6</label><figDesc>Localization performance on the ReferIt Game test set. (a) Published results and our Similarity Network baseline. (b) Our best-performing conditional models</figDesc><table>Method 
Accuracy 
(a) State-of-the-art 
SCRC [10] 
17.93 
GroundeR + Spatial [28] 
26.93 
MCB + Reg + Spatial [3] 
26.54 
CGRE [21] 
31.85 
MNN + Reg + Spatial [3] 
32.21 
IGOP [35] 
34.70 
Similarity Network + Spatial 
31.26 
(b) Conditional Models + Spatial 
CITE, K-Means, K = 2 
34.01 
CITE, Learned, K = 12 
34.13 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Code: https://github.com/BryanPlummer/cite</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Performance on this task can be further improved by taking into account the predictions made for other phrases in the same sentence [24, 33, 3, 4], with the best result using Pascal-tuned features of 57.53% achieved by Chen et al . [3] and 65.14% using Flickr30K-tuned features [4].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements: This material is based upon work supported in part by the National Science Foundation under Grants No. 1563727 and 1718221, Amazon Research Award, AWS Machine Learning Research Award, and Google Research Award.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Similarity metrics for categorization: from monolithic to category specific</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<publisher>ICCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">MSRC: Multimodal spatial regression with semantic context for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page">ICMR</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Query-guided regression network with context policy for phrase grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kovvuri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">The PASCAL Visual Object Classes Challenge 2012 (VOC2012) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">From captions to visual concepts and back</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Iandola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Platt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">ICCV</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep image retrieval: Learning global representations for image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Larlus</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Natural language object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">ICML</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image retrieval using scene graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Referitgame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>EMNLP</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference for Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Associating neural word embeddings with deep image representations using fisher vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Klein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wolf</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Visual genome: Connecting language and vision using crowdsourced dense image annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Krishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kravitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalantidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Shamma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Attention correctness in neural image captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>AAAI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Referring expression generation and comprehension via attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Phrase localization and visual relationship detection with comprehensive image-language cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Cnn image retrieval learns from bow: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Radenovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chum</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>NIPS</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Solving visual madlibs with multiple cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>BMVC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Conditional similarity networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Learning deep structure-preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Learning two-branch neural networks for image-text matching tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03470</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">ICML</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Interpretable and globally optimal prediction for textual grounding using image concepts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Yeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hwu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N</forename><surname>Do</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Discriminative bimodal networks for visual localization and detection with natural language queries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
