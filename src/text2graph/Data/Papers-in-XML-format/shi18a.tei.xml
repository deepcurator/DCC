<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Spectral Approach to Gradient Estimation for Implicit Distributions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxin</forename><surname>Shi</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shengyang</forename><surname>Sun</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Zhu</surname></persName>
						</author>
						<title level="a" type="main">A Spectral Approach to Gradient Estimation for Implicit Distributions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Recently there have been increasing interests in learning and inference with implicit distributions (i.e., distributions without tractable densities). To this end, we develop a gradient estimator for implicit distributions based on Stein's identity and a spectral decomposition of kernel operators, where the eigenfunctions are approximated by the Nyström method. Unlike the previous works that only provide estimates at the sample points, our approach directly estimates the gradient function, thus allows for a simple and principled out-ofsample extension. We provide theoretical results on the error bound of the estimator and discuss the bias-variance tradeoff in practice. The effectiveness of our method is demonstrated by applications to gradient-free Hamiltonian Monte Carlo and variational inference with implicit distributions. Finally, we discuss the intuition behind the estimator by drawing connections between the Nyström method and kernel PCA, which indicates that the estimator can automatically adapt to the geometry of the underlying distribution.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently there have been increasing interests in learning and inference with implicit distributions, i.e., distributions defined by a sampling process but without tractable densities. Popular examples include Generative adversarial networks (GAN) <ref type="bibr" target="#b12">(Goodfellow et al., 2014;</ref><ref type="bibr" target="#b28">Mohamed &amp; Lakshminarayanan, 2016)</ref>. Compared to the explicit likelihoods (e.g., Gaussian) in other deep generative models such as variational autoencoders (VAE) <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013)</ref>, implicit distributions are shown able to capture the complex data manifold that lies in a high dimensional space, leading to more realistic samples generated by GAN than other models. Besides, as the constraint of requiring an explicit density is removed, implicit distributions are treated as more flexible variants of variational families for approximate inference <ref type="bibr" target="#b34">(Ranganath et al., 2016;</ref><ref type="bibr" target="#b23">Liu &amp; Feng, 2016;</ref><ref type="bibr" target="#b27">Mescheder et al., 2017;</ref><ref type="bibr" target="#b46">Tran et al., 2017;</ref><ref type="bibr" target="#b17">Huszár, 2017;</ref><ref type="bibr" target="#b22">Li &amp; Turner, 2018;</ref><ref type="bibr" target="#b40">Shi et al., 2018)</ref>.</p><p>Despite that it is appealing to use flexible implicit distributions, which capture complex correlations and manifold structures, deploying them in practical scenes is still challenging. This is because most learning and inference algorithms require optimizing some divergences between two distributions, which often rely on evaluating the densities of them. However, the density of an implicit distribution is intractable and we only have access to its samples. Previous works have explored two directions to solve the problem. One is to first approximate the optimization objective with the samples and then use the approximation to guide the learning procedure. Many works in this direction are based on the fact that the density ratio between two distributions can be estimated from their samples, by a probabilistic classifier (also known as the discriminator) trained in an adversarial game <ref type="bibr" target="#b10">(Donahue et al., 2016;</ref><ref type="bibr" target="#b11">Dumoulin et al., 2016;</ref><ref type="bibr" target="#b27">Mescheder et al., 2017;</ref><ref type="bibr" target="#b46">Tran et al., 2017;</ref><ref type="bibr" target="#b17">Huszár, 2017)</ref>, or by kernel-based estimators <ref type="bibr" target="#b40">(Shi et al., 2018)</ref>. The other direction is to estimate the gradients instead of the objective. <ref type="bibr" target="#b22">Li &amp; Turner (2018)</ref> propose the Stein gradient estimator for the log density of an implicit distribution. It is based on a ridge regression that inverts a generalized version of Stein's identity <ref type="bibr" target="#b13">(Gorham &amp; Mackey, 2015;</ref>. As argued in their work, this approach is more direct and avoids probable arbitrarily diverse gradients provided by the approximate objective. In this paper, we focus on the latter direction.</p><p>Though the Stein gradient estimator has been shown to be a fast and easy way to obtain gradient estimates for implicit models. It is still limited by its simple formulation, i.e., the unjustified choice of both the test function (the kernel feature mapping) and the regularization scheme (the Frobeniusnorm regularization used in ridge regression). The problem has deeper implications. For instance, no theoretical results have been established for the estimator. Moreover, there is no principled way to obtain gradient estimates at positions out of the sample points.</p><p>In this paper, we develop a novel gradient estimator for implicit distributions, which is called the Spectral Stein Gradient Estimator (SSGE). To approximate the gradient function of the log density (i.e., ∇ x log q(x)), SSGE expands it in terms of the eigenfunctions of a kernel-based operator. These eigenfunctions are orthogonal with respect to the underlying distribution. By setting the test functions in the Stein's identity to these eigenfunctions, we can take advantage of their orthogonality to obtain a simple solution. The eigenfunctions in the solution are then approximated by the Nyström method <ref type="bibr" target="#b31">(Nyström, 1930;</ref><ref type="bibr" target="#b1">Baker, 1997;</ref><ref type="bibr" target="#b50">Williams &amp; Seeger, 2001)</ref>. Unlike the Stein gradient estimator <ref type="bibr" target="#b22">(Li &amp; Turner, 2018)</ref>, our approach allows for a direct and principled out-of-sample extension. Moreover, we provide theoretical analysis on the error bound of SSGE and discuss the bias-variance tradeoff in practice. We also discuss its effectiveness in reducing the curse of dimensionality by drawing connections to kernel PCA.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Background</head><p>In this section we briefly introduce the Nyström method and the Stein gradient estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Nyström Method</head><p>The Nyström method originates as a method for approximating the solution of Fredholm integral equations of the second kind <ref type="bibr" target="#b31">(Nyström, 1930;</ref><ref type="bibr" target="#b1">Baker, 1997)</ref>. It was used by <ref type="bibr" target="#b50">Williams &amp; Seeger (2001)</ref> for estimating extensions of eigenvectors in Gaussian process regression. Specifically, the following equation for finding the eigenfunctions {ψ j } j≥1 , ψ j ∈ L 2 (X , q) 1 of the covariance kernel k(x, y) w.r.t. the probability measure q is considered:</p><formula xml:id="formula_0">k(x, y)ψ(y)q(y)dy = µψ(x).<label>(1)</label></formula><p>And there is a constraint that the eigenfunctions {ψ j } j≥1 are orthonormal under q:</p><formula xml:id="formula_1">ψ i (x)ψ j (x)q(x)dx = δ ij ,<label>(2)</label></formula><formula xml:id="formula_2">where δ ij = 1[i = j].</formula><p>Approximating the left side of eq. (1) with its unbiased Monte Carlo estimate using i.i.d. samples {x 1 , . . . , x M } from q and applying the equation to these samples, we obtain</p><formula xml:id="formula_3">1 M Kψ ≈ µψ,<label>(3)</label></formula><p>where K is the Gram matrix:</p><formula xml:id="formula_4">K ij = k(x i , x j ), and ψ = ψ(x 1 ), . . . , ψ(x M )</formula><p>. This is an eigenvalue problem for K. We compute the eigenvectors u 1 , . . . , u J with the J 1 L 2 (X , q) denotes the space of all square-integrable functions w.r.t. q. largest eigenvalues λ 1 ≥ · · · ≥ λ J for K. Now we have the solutions of eq. (3) by comparing against Ku j = λ j u j :</p><formula xml:id="formula_5">ψ j (x m ) ≈ √ M u jm , m = 1, . . . , M,<label>(4)</label></formula><formula xml:id="formula_6">µ j ≈ λ j M .<label>(5)</label></formula><p>Note that the scaling factor in eq. (4) is due to the empirical constraint translated from eq. <ref type="formula" target="#formula_1">(2)</ref>: <ref type="bibr" target="#b1">Baker (1997)</ref> shows that for a fixed kernel k, λj M converges to µ j in the limit as M → ∞.</p><formula xml:id="formula_7">1 M M m=1 ψ i (x m )ψ j (x m ) ≈ δ ij .</formula><p>Plugging these solutions back into eq. (1), we get the Nyström formula for approximating the value of the jth eigenfunction at any point x:</p><formula xml:id="formula_8">ψ j (x) ≈ψ j (x) = √ M λ j M m=1 u jm k(x, x m ).<label>(6)</label></formula><p>The Nyström method has been shown to be a thread linking many dimension reduction methods such as kernel PCA <ref type="bibr" target="#b37">(Schölkopf et al., 1998)</ref>, multidimensional scaling (MDS) <ref type="bibr" target="#b6">(Borg &amp; Groenen, 2005)</ref>, local linear embedding (LLE) <ref type="bibr" target="#b36">(Roweis &amp; Saul, 2000)</ref>, Laplacian eigenmaps <ref type="bibr" target="#b2">(Belkin &amp; Niyogi, 2003)</ref>, and spectral clustering <ref type="bibr" target="#b47">(Weiss, 1999)</ref>, unifying their out-of-sample extensions <ref type="bibr" target="#b3">(Bengio et al., 2004a;</ref><ref type="bibr" target="#b7">Burges et al., 2010)</ref>. We will discuss these connections later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Stein's Identity and Stein Gradient Estimator</head><p>Recent developments in Stein discrepancy and its kernelized extensions <ref type="bibr" target="#b13">(Gorham &amp; Mackey, 2015;</ref><ref type="bibr" target="#b8">Chwialkowski et al., 2016;</ref><ref type="bibr" target="#b24">Liu &amp; Wang, 2016)</ref> have renewed the interests in Stein's method, which is a classic tool in statistics. Central to these works is an equation that generalizes the original Stein's identity <ref type="bibr" target="#b43">(Stein, 1981)</ref>, shown in the following theorem.</p><p>Theorem 1 <ref type="bibr" target="#b13">(Gorham &amp; Mackey 2015;</ref>. Assume that q(x) is a continuous differentiable probability density supported on</p><formula xml:id="formula_9">X ⊂ R d . h : X → R d is a smooth vector- valued function h(x) = [h 1 (x), h 2 (x), . . . , h d (x)] , and ∀i ∈ 1, . . . , d , h i is in the Stein class of q, i.e., x∈X ∇ x (h i (x)q(x)) dx = 0.<label>(7)</label></formula><p>Then the following identity holds:</p><formula xml:id="formula_10">E q [h(x)∇ x log q(x) + ∇ x h(x)] = 0.<label>(8)</label></formula><p>The condition (7) can be easily checked using integration by parts or divergence theorem. Specifically, when X = R d , eq. <ref type="formula" target="#formula_9">(7)</ref> holds if lim x →∞ h(x)q(x) = 0; when X is a compact subset of R d with piecewise smooth boundary ∂X , eq. (7) holds if h(x)q(x) = 0, ∀x ∈ ∂X . Here h(x) is called the test function. We can check that for a RBF kernel k, and for any fixed x, k(x, ·) and k(·, x) are in the Stein class of continuous differentiable densities supported on R d .</p><p>Because the expectation in eq. (8) can be approximated by Monte Carlo estimates, the identity connects ∇ x log q(x) and the samples from q. Inspired by this, <ref type="bibr" target="#b22">Li &amp; Turner (2018)</ref> propose the Stein gradient estimator, which inverts eq. <ref type="formula" target="#formula_10">(8)</ref> to obtain estimates of ∇ x log q(x) at the sample points. Below we briefly review their method. Specifically, consider M i.i.d. samples x 1:M drawn from q(x). We define two matrices</p><formula xml:id="formula_11">H = h(x 1 ), · · · , h(x M ) ∈ R d ×M and G = ∇ x 1 log q(x 1 ), · · · , ∇ x M log q(x M ) ∈ R M ×d . Monte Carlo sampling with eq. (8) shows that − 1 M HG ≈ ∇ x h,<label>(9)</label></formula><p>where</p><formula xml:id="formula_12">∇ x h = 1 M M m=1 ∇ x m h(x m ) ∈ R d ×d , ∇ x m h(x m ) = [∇ x m h 1 (x m ), . . . , ∇ x m h d (x m )]</formula><p>. Equation (9) inspires the following ridge regression problem:</p><formula xml:id="formula_13">argmin G∈R M ×d ∇ x h + 1 M HĜ 2 F + η M 2 Ĝ 2 F ,</formula><p>where · 2 F denotes the Frobenius norm of a matrix and η &gt; 0 is the regularization coefficient. It has an analytic solution that</p><formula xml:id="formula_14">G Stein = −M (K + ηI) −1 H ∇ x h,<label>(10)</label></formula><p>where K = H H. By noticing that K ij = h(x i ) h(x j ) and applying the kernel trick, we have</p><formula xml:id="formula_15">K ij = k(x i , x j ), where k : R d × R d → R is a positive defi- nite kernel. Similarly we can show (H ∇ x h) ij = 1 M M m=1 ∇ x m j k(x i , x m ).</formula><p>With the kernel trick, the above derivation implicitly set the test function to be the feature mapping h :</p><formula xml:id="formula_16">R d → H, h(x) = k(x, ·)</formula><p>, where H is the Reproducing Kernel Hilbert Space (RKHS) induced by k.</p><p>Though introducing the kernel trick enhances the expressiveness of the Stein gradient estimator, the choice of the test function is not well justified. It is unclear whether eq. <ref type="formula" target="#formula_10">(8)</ref> holds when the test function is set to the kernel feature mapping h(x) = k(x, ·), which maps from R d to H instead of the common R d . <ref type="bibr" target="#b22">Li &amp; Turner (2018)</ref> show that their approach is equivalent to minimizing a regularized version of the V-statistics of kernel Stein discrepancy , which has proved to be effective in testing goodness-of-fit. However, it is still unclear whether the test power is sufficient due to the added Frobenius-norm regularization term. Besides, eq. (10) only gives the gradient estimates at the sample points. For out-of-sample prediction at a test point, two choices are proposed in <ref type="bibr" target="#b22">Li &amp; Turner (2018)</ref>: one is by adding the test point to the sample points and re-compute eq. (10), which could be computationally demanding. We will refer to this out-of-sample extension as Stein + . The other choice is to refit a parametric estimator (a linear combination of RBF kernels), which is cheaper but less accurate. Both approaches assume that the test points are also sampled from q. However, they are unjustified when the assumption is not satisfied. Since the latter is an approximation to the former, we will only compare with Stein + in the experiments for out-of-sample predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section we derive a gradient estimator for implicit distributions called the Spectral Stein Gradient Estimator (SSGE). Unlike the previous Stein gradient estimator that only provides estimates at the sample points, SSGE directly estimates the gradient function and thus allows simple and principled out-of-sample predictions. We also provide theoretical analysis on the error bound of SSGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Spectral Stein Gradient Estimator</head><p>To begin with, let</p><formula xml:id="formula_17">x denote a d-dimensional vector in R d . Consider an implicit distribution q(x) supported on X ⊂ R d , from which we observe M i.i.d. samples x 1:M .</formula><p>We denote the target gradient function to estimate by g :</p><formula xml:id="formula_18">X → R d : g(x) = ∇ x log q(x). The ith component of the gradient is g i (x) = ∇ xi log q(x). We assume g 1 , . . . , g d ∈ L</formula><p>2 (X , q). As introduced in section 2.1, {ψ j } j≥1 form an orthonormal basis of L 2 (X , q). So we can expand g i (x) into the following spectral series:</p><formula xml:id="formula_19">g i (x) = ∞ j=1 β ij ψ j (x).<label>(11)</label></formula><p>Below we will show how to estimate the coefficients β ij . According to Theorem 1, we have the following proposition. Proposition 1. If k(·, ·) has continuous second order partial derivatives, and both k(x, ·) and k(·, x) are in the Stein class of q, the following set of equations hold true:</p><formula xml:id="formula_20">E q [ψ j (x)g(x) + ∇ x ψ j (x)] = 0, j = 1, 2 . . . , ∞. (12)</formula><p>Proof. We only need to prove that ψ j (x) is in the Stein class of q. See Appendix A for details.</p><p>Substituting eq. <ref type="formula" target="#formula_0">(11)</ref> into eq. (12) and using the orthonormality of {ψ j } j≥1 , we can show that</p><formula xml:id="formula_21">β ij = −E q ∇ xi ψ j (x).</formula><p>To estimate β ij , we need an approximation of ∇ xi ψ j (x). The key observation is that derivatives can be taken w.r.t. both sides of eq. (1):</p><formula xml:id="formula_22">µ j ∇ xi ψ j (x) = ∇ xi k(x, y)ψ j (y)q(y)dy = ∇ xi k(x, y)ψ j (y)q(y)dy.<label>(13)</label></formula><p>Monte-Carlo sampling with eq. (13), we have an estimate of ∇ xi ψ j (x):</p><formula xml:id="formula_23">∇ xi ψ j (x) ≈ 1 µ j M M m=1 ∇ xi k(x, x m )ψ j (x m ).<label>(14)</label></formula><p>Substituting eqs. <ref type="formula" target="#formula_5">(4)</ref> and <ref type="formula" target="#formula_6">(5)</ref> into eq. <ref type="formula" target="#formula_0">(14)</ref> and comparing with eq. (6), we can shoŵ</p><formula xml:id="formula_24">∇ xi ψ j (x) ≈ ∇ xiψj (x).<label>(15)</label></formula><p>Perhaps surprisingly, Equation <ref type="formula" target="#formula_0">(15)</ref> indicates that ∇ xiψj (x) is a good approximation to ∇ xi ψ j (x) 2 . In fact, as we shall see in Theorem 2, the error introduced by Nyström approximation is negligible with high probability as M → ∞. Now truncating the series expansion to the first J terms and plugging in the Nyström approximations of {ψ j } J j=1 , we get our estimator:</p><formula xml:id="formula_25">g i (x) = J j=1β ijψj (x),<label>(16)</label></formula><formula xml:id="formula_26">β ij = − 1 M M m=1 ∇ xiψj (x m ),<label>(17)</label></formula><p>whereψ j is the Nyström approximation of ψ j as in Section 2.1. We use RBF kernels in all experiments.</p><p>Computational Cost The spectral gradient estimatorĝ i (x) depends onβ ij andψ j (x). To compute them, the computational bottleneck lies in computing the Gram matrix and its eigendecomposition, which have complexity O(M 2 d) and O(M 3 ), respectively. Therefore the computational cost of constructing the estimator is</p><formula xml:id="formula_27">O(M 3 +M 2 d). For prediction, given x ∈ R d , evaluatingĝ i (x) has cost O(M (d + J))</formula><p>. In comparison, the Stein gradient estimator directly approximates gradients at the sample points, which involves computing the Gram matrix and its inverse. Thus the overall complexity is also O(M 3 + M 2 d). Note that SSGE only requires the J largest eigenvalues and corresponding eigenvectors, efficient algorithms <ref type="bibr" target="#b32">(Parlett &amp; Scott, 1979</ref>) might be applied to further reduce its complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Theoretical Results</head><p>Following the derivation in Section 3.1, we analyze the theoretical properties of the resulting estimator in eqs. <ref type="formula" target="#formula_0">(16)</ref> and (17). To be clear, we formally restate the assumptions that have been made in the derivation. Assumption 1. k(x, ·) and k(·, x) are in the Stein class of q.</p><formula xml:id="formula_28">Assumption 2. g i (x) ∈ L 2 (X , q), i = 1, . . . , d, i.e., g i (x) 2 q(x) dx = ∞ j=1 β 2 ij ≤ C &lt; ∞.</formula><p>2 This does not hold for general functions.</p><formula xml:id="formula_29">Assumption 3. µ 1 &gt; µ 2 &gt; · · · &gt; µ J &gt; 0.</formula><p>Note that Assumption 1 holds for RBF kernels. Assumption 2 is necessary for g i (x)'s being possible to be expanded into the spectral series. We need Assumption 3 since our derivation is based on several well-studied bounds of Nyström approximation, i.e., Lemmas 4 and 5 in Appendix B <ref type="bibr" target="#b41">(Sinha &amp; Belkin, 2009;</ref><ref type="bibr" target="#b19">Izbicki et al., 2014)</ref>. Note that when this assumption does not hold, we could proceed as in <ref type="bibr" target="#b35">Rosasco et al. (2010)</ref> (Theorem 12) and derive the error bound in a similar way.</p><p>Theorem 2 (Error Bound of SSGE, proof in Appendix B).</p><p>Given the above assumptions, the error</p><formula xml:id="formula_30">|ĝ i (x) − g i (x)| 2 q(x) dx is bounded by J 2 O p 1 M + O p C µ J ∆ 2 J M + JO p C µ J ∆ 2 J M + g i 2 H O(µ J ),<label>(18)</label></formula><p>where ∆ J = min 1≤j≤J |µ j − µ j+1 |, O p is the Big O notation in probability.</p><p>The first three terms in eq. <ref type="formula" target="#formula_0">(18)</ref> are the sample errors caused by the Nyström approximation, which we call the estimation error. It is negligible with high probability as M → ∞. The last term is caused by the bias introduced by the truncation, which we call the approximation error. From the bound we can observe a tradeoff between the estimation error and the approximation error. As an illustration, one may set J to be as large as possible to reduce the magnitude of µ J and thus reduce the approximation error, but it will increase the estimation error at a rate of O p</p><formula xml:id="formula_31">J 2 µ J .</formula><p>For RBF kernels and their corresponding RKHS, smoother target functions tend to have smaller g i 2 H , and thus a tighter bound. In general, this indicates that choosing the appropriate kernel which is suitable to the target gradient function can improve the performance of the gradient estimator (by leading to a smaller g i 2 H ). Hyperparameter Selection When RBF kernels are used, SSGE has two free parameters: The kernel bandwidth σ, and the number of eigenfunctions (J) used in the estimate. For σ, we use the median heuristic, i.e., we set it to be the median of pairwise distances between all samples, which turns out to work well in all experiments. Below we discuss the criterion for selecting J, which is usually harder.</p><p>As the performance of the gradient estimator directly influences the task where it is used. The optimal choice for tuning J is to apply cross-validation on the specific task. However, since J is a discrete parameter, one has to manually set a continuous interval and bin it so that the commonly used black-box hyperparameter-search methods (e.g., Bayesian optimization) are applicable. However, this approach does not take the magnitude of eigenvalues into consideration. Observing this, we propose that, instead of directly tuning J, we could tune a thresholdr for the percentage of remaining eigenvalues:</p><formula xml:id="formula_32">J = argmax J r J , s.t. r J = J j=1 λ j M j =1 λ j , r J ≤r.</formula><p>Note that searchingr may still not be easy due to the nonsmooth validation surface. But in experiments we found thatr values in [0.95, 0.99] usually work well.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Gradient Estimation for Entropy</head><p>Above we have derived a gradient estimator for the log density of an implicit distribution. Now we discuss a useful extension of it. Consider the situation where we need to optimize the entropy H(q) = −E q log q of an implicit distribution q φ (x) w.r.t. its parameters φ. When x is continuous and can be reparameterized <ref type="bibr" target="#b21">(Kingma &amp; Welling, 2013)</ref>, e.g., x = f ( ; φ), ∼ N (0, I), it can be shown (see Appendix C) that</p><formula xml:id="formula_33">∇ φ H(q) = −E ∇ x log q φ (f ( ; φ))∇ φ f ( ; φ),<label>(19)</label></formula><p>where ∇ x log q φ (f ( ; φ)) can be easily estimated by SSGE. As we shall see in experiments, eq. (19) can be used for variational inference with implicit distributions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Related Work</head><p>Our work is closely related to other works on implicit distributions. Apart from the density-ratio based approaches introduced in Section 1, we discuss two more directions.</p><p>Nonparametric Inference Nonparametric variational inference (VI) methods such as PMD <ref type="bibr" target="#b9">(Dai et al., 2016)</ref> and SVGD <ref type="bibr" target="#b24">(Liu &amp; Wang, 2016)</ref> remove the need of parametric families, they keep a set of particles and gradually adjust them towards the true posterior. These particles can be viewed as samples from an implicit distribution. Instead of directly computing gradients in the sample space like us, SVGD performs functional gradient descent to transform the implicit distribution towards the true posterior. Though elegant, SVGD is limited to KL-divergence based VI problems, while our approach is generally applicable wherever gradient estimates are needed for the log density of implicit distributions.</p><p>Kernel Exponential Families and Score Matching Previous to our work, the problem of estimating gradient functions of intractable log densities has been worked on by <ref type="bibr" target="#b44">Strathmann et al. (2015)</ref>; <ref type="bibr" target="#b45">Sutherland et al. (2018)</ref>. They identified the problem when developing Hamiltonian Monte Carlo (HMC) under the settings where higher-order information of the target distribution is unavailable. To address it, a kernel exponential family <ref type="bibr" target="#b42">(Sriperumbudur et al., 2017)</ref> is fit to samples along the Markov Chain trajectory by score matching <ref type="bibr" target="#b18">(Hyvärinen, 2005)</ref>, and then serves as a surrogate of the target distribution to provide gradient estimates for HMC. We will compare to them in Section 5.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate the proposed approach on both toy problems and real-world examples. The latter includes applications of SSGE to two widely used inference methods: Hamiltonian Monte Carlo and variational inference. Code is available at https://github.com/thjashin/ spectral-stein-grad. Implementations are based on ZhuSuan <ref type="bibr" target="#b39">(Shi et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Toy Experiment</head><p>As a simple example, we experiment with estimating the gradient function of a 1-D standard Gaussian distribution. The target log density is log q(x) = − 1 2 log 2π − 1 2 x 2 , and the true gradient function is ∇ x log q(x) = −x. We draw M = 100 i.i.d. samples from q for use in the estimation. In <ref type="figure" target="#fig_0">Figure 1</ref> we plot the gradients estimates produced by the Stein gradient estimator, its out-of-sample extension (Stein + ) (see Section 2.2), and our approach (SSGE). Since the original Stein estimator only gives estimates at the sample points, we plot them as individual points (in red). For the regularization coefficient η in eq. (10), we searched it in {0.001, 0.01, 0.1, 1, 10, 100} and plot the best result at η = 0.1 3 . For SSGE, we set J = 6. We can see that despite all three estimators produce rather good approximation where the samples are taken densely (e.g., in [−2, 2]), the gradient function estimated by SSGE is notably better at the places where samples are less dense.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Gradient-free Hamiltonian Monte Carlo</head><p>In this experiment we investigate the usefulness of SSGE in constructing a gradient-free HMC sampler. We follow the settings in <ref type="bibr" target="#b38">Sejdinovic et al. (2014)</ref>; <ref type="bibr" target="#b44">Strathmann et al. (2015)</ref> and consider a Gaussian Process classification problem on the UCI Glass dataset. The goal is to infer the posterior over hyperparameters under a fully Bayesian treatment. Specifically, consider a Gaussian process whose joint distribution is over latent variables f , labels y, and hyperparameters θ:</p><formula xml:id="formula_34">p(f , y, θ) = p(θ)p(f |θ)p(y|f ),<label>(20)</label></formula><p>where f |θ ∼ N (0, K θ ). K θ is the Gram matrix formed by the data points</p><formula xml:id="formula_35">x 1:N ∈ R D : (K θ ) ij = exp − D d=1 |x i,d − x j,d | 2 2 2 d ,<label>(21)</label></formula><p>where we define θ d = log 2 d . The problem to consider is a binary classification between window and non-window glasses, so the likelihood is given by a logistic classifier: p(y i |f i ) = 1 1+exp(−yifi) , y i ∈ {−1, 1}. The posterior over θ is highly nonlinear, as shown in <ref type="figure" target="#fig_1">Figure 2a</ref>. As pointed out in previous works <ref type="bibr" target="#b29">(Murray &amp; Adams, 2010;</ref><ref type="bibr" target="#b38">Sejdinovic et al., 2014)</ref>, sampling from the posterior of θ is challenging, e.g., Gibbs sampling often gets stuck due to p(θ|f , y) is very sharp. One way to address this problem is the pseudomarginal MCMC <ref type="bibr" target="#b0">(Andrieu et al., 2009</ref>), through which a markov chain can be simulated to directly sample from p(θ|y). Since the likelihood p(y|θ) is intractable, pseudomarginal MCMC replaces it with an unbiased Monte Carlo estimate using importance sampling:</p><formula xml:id="formula_36">p(y|θ) = 1 K K i=1 p(y|f i )p(f i |θ) q(f i ) , f 1:K ∼ q(f ). (22)</formula><p>In practice q(f ) is chosen to be the Laplace approximation of p(f |y, θ). As for any pseudo-marginal MCMC scheme, the gradient information of the posterior is not available and HMC is not suitable. We have to resort to gradientfree MCMC methods. As mentioned in Section 4, kernel adaptive Metropolis samplers <ref type="bibr" target="#b38">(Sejdinovic et al., 2014)</ref> were developed and then extended to kernel HMC (KMC) <ref type="bibr" target="#b44">(Strathmann et al., 2015)</ref>. In this experiment we compare the performance of KMC and HMC with gradients estimated by SSGE and Stein + .</p><p>To begin, we run 20 randomly initialized adaptiveMetropolis samplers for 30k iterations, with the first 10k samples discarded. We then keep every 400-th sample in each of the chains, and combine them to get 1k samples. These samples are treated as the ground truth. Similar to <ref type="bibr" target="#b45">Sutherland et al. (2018)</ref>, our experiment assumes the idealized scenario where a burn-in period for collecting a sufficient number of samples has completed. This is to remove all the other factors that could have an effect on the comparison of acceptance ratios, which then only depends on the accuracy of the gradient estimation of potentials, i.e., −∇ θ log p(θ|y). So we fit all three estimators on a random subset of M = 200 of these samples and repeated 10 times.</p><p>For each fitted estimator, we start from a random initial point from the posterior sketch and run HMC samplers with the gradient estimates for 5k iterations. To be fair in comparing the acceptance ratios, no adaptation of HMC parameters can be used. So we randomly uses between 1 and 10 leapfrog steps of size chosen uniformly in [0.01, 0.1], and a standard Gaussian momentum. The kernel bandwidths (σ) in all three estimators are determined by median heuristics (i.e., set to the median of the pairwise distances between the M data points). For Stein + , η = 0.001. For SSGE,r = 0.95.</p><p>The average acceptance ratios over 10 runs are plotted in <ref type="figure" target="#fig_1">Figure 2b</ref>. We can see that SSGE clearly outperforms Stein + and is even better than the KMC algorithm, which is specially designed as a gradient-free HMC algorithm. Though KMC is carefully designed, its gradients are estimated by first fitting a kernel exponential family as a surrogate and then taking derivatives through it, while SSGE is arguably more direct.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Variational Inference with Implicit Distributions</head><p>As introduced in Section 1, there have been increasing interests in constructing flexible variational posteriors with implicit distributions. Specifically, for a latent-variable model p(z, x) where x and z denote observed and latent variables, respectively, Variational Inference (VI) approximates the posterior p(z|x) by maximizing the following evidence lower bound (ELBO):</p><formula xml:id="formula_37">L(x; φ) = E q φ (z) log p(z, x) − E q φ (z) log q φ (z),<label>(23)</label></formula><p>where q φ (z) is called the variational distribution. The second term of eq. <ref type="formula" target="#formula_1">(23)</ref> is the entropy of q, which is intractable for implicit distributions. As shown in Section 3.3, SSGE can be used here for estimating gradients of the entropy term, thus allowing VI with implicit distributions. Below we conduct experiments on two examples: Bayesian Neural Networks (BNN) and Variational Autoencoders (VAE).  Note that the original Stein gradient estimator can also be used here. In experiments, we find that despite lack of theoretical evidences, the performance of a well-tuned Stein gradient estimator is very close to SSGE when no out-ofsample predictions are required. As the emphasis of this paper is on SSGE, we only focus on verifying the accuracy of SSGE below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.1.">BAYESIAN NEURAL NETWORKS</head><p>We evaluate the predictive ability of BNNs with implicit variational posteriors trained by SSGE. To visually access the quality of uncertainty, we choose a 1-D regression problem <ref type="bibr" target="#b15">(Hernández-Lobato &amp; Adams, 2015;</ref><ref type="bibr" target="#b26">Louizos &amp; Welling, 2016)</ref>. Specifically, 20 inputs are randomly sampled from <ref type="bibr">[−4, 4]</ref>, then the target value y is computed with y = x 3 + n , n ∼ N (0, 9). We use a BNN with 1 hidden layer and 20 units to model the normalized inputs and targets. We also set the variance of the observation noise to the true value. We compare SSGE with implicit posteriors, Hamiltonian Monte Carlo (HMC) <ref type="bibr" target="#b30">(Neal et al., 2011)</ref> and Bayes-by-backprop (BBB) <ref type="bibr" target="#b5">(Blundell et al., 2015)</ref>. To better demonstrate SSGE's gradient estimation effect, we also test SSGE with a factorized posterior, in comparison to BBB.</p><p>We keep 20 chains and run 100k iterations for HMC. All other methods are trained with 100 samples for 20k iterations using Adam optimizer <ref type="bibr" target="#b20">(Kingma &amp; Ba, 2014)</ref>. For SSGE, we set J = 100. The implicit posteriors we use for weights in both layers are standard normal distributions transformed by fully connected networks with one hidden layer of 100 units.</p><p>As shown in <ref type="figure" target="#fig_3">Fig. 3</ref>, HMC, as the golden standard, smoothly fits the training data and outputs sensible uncertainty estimation. HMC not only produces large uncertainty outside the data region, its predictive variance also varies even in regions with training points. This kind of interpolation behavior is hard to be captured by factorized Gaussian posteriors, as shown in <ref type="figure" target="#fig_3">Fig. 3c and 3d</ref>. SSGE with implicit posteriors also has big predictive variances beyond training points, which implies that the BNN trained by SSGE is not overfitting, although it underestimates the uncertainty in the middle region. Also, we observe that SSGE can have similar interpolation behaviors as HMC (see the rightmost two points in <ref type="figure" target="#fig_3">Fig. 3b)</ref>. Besides, SSGE with a factorized posterior has a similar prediction with BBB. Given the network and the variational posterior are both the same, we can attribute this similarity to accurate gradient estimation by SSGE.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.2.">VARIATIONAL AUTOENCODERS</head><p>From the above example we see that SSGE enables variational posteriors parameterized by implicit distributions. To demonstrate that it scales to larger models and datasets, we adopt the settings in <ref type="bibr" target="#b40">Shi et al. (2018)</ref> and train a deep convolutional VAE with implicit variational posteriors (Implicit VAE for short) on the CelebA dataset. As in their work, the latent dimension is 32, and the network structure of the decoder is chosen to be the same as DCGAN <ref type="bibr" target="#b33">(Radford et al., 2015)</ref>. The observation likelihoods are Gaussian distributions with trainable data-independent variances. The implicit posterior is a deep convolutional net symmetric to the decoder, with Gaussian noises injected into hidden layers. Full details of the model structures can be found in <ref type="bibr" target="#b40">Shi et al. (2018)</ref>.</p><p>To examine how accurate the gradient estimates provided by SSGE are, we conduct experiments under three different settings: a plain VAE with normal variational posteriors, an Implicit VAE trained with the entropy term removed from the ELBO, and an Implicit VAE using SSGE's gradient estimates for the entropy. For SSGE, we set M = 100, and r = 0.99. In <ref type="figure" target="#fig_4">Figures 4a to 4c</ref> we show samples randomly generated from the trained models. We can see that without the entropy term, the Implicit VAE tends to overfit and produces visually bad generations, while if we retain the entropy term and use SSGE to estimate its gradients, the Implicit VAE can produce realistic samples. To quantitatively measure the sample quality, we compare the Fréchet Inception Distance (FID) <ref type="bibr" target="#b16">(Heusel et al., 2017)</ref> between real data and random generations from the models. The results are shown in <ref type="figure" target="#fig_4">Figure 4d</ref>. We can see that the Implicit VAE trained by SSGE converges faster and produces samples with slightly better quality than the plain VAE. This is probably due to that implicit posteriors are less likely to overfit <ref type="bibr" target="#b40">(Shi et al., 2018)</ref>, and SSGE gives accurate gradients for optimizing them. Besides the CelebA experiments, we also tested the models on MNIST dataset and evaluated the test log likelihoods. See Appendix D for details.  <ref type="bibr" target="#b16">(Heusel et al., 2017)</ref> between random generated images and real images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Discussion</head><p>Connection to Kernel PCA As mentioned in Section 2.1, the Nyström approximation is closely related to Kernel PCA <ref type="bibr" target="#b37">(Schölkopf et al., 1998)</ref> (KPCA), which is a powerful method for nonlinear dimension reduction. In KPCA, the input data is first projected to a (usually high-dimensional) feature space, where PCA is then applied. The operations in the feature space are handled by the kernel trick. We briefly review the method below.</p><p>Given a positive definite kernel k : X × X → R, we denote the induced RKHS by H and its corresponding feature map by φ : X → H. Let the data to be analyzed be</p><formula xml:id="formula_38">{x i } M i=1 , x i ∈ X .</formula><p>To simplify the derivation, we first assume the data to be centered in the feature space. Then the covariance matrix is formed as</p><formula xml:id="formula_39">C = 1 M M i=1 φ(x i )φ(x i )</formula><p>. In general, PCA requires to solve the following eigenvalue problem 4 :</p><formula xml:id="formula_40">Cv = µv.<label>(24)</label></formula><p>A key observation of KPCA is that the eigenvectors lie in the span of the feature vectors, since from eq. (24) we have</p><formula xml:id="formula_41">v = 1 µM M i=1 φ(x i ) v φ(x i ) = M i=1 α i φ(x i ).<label>(25)</label></formula><p>Here we use α = [α 1 , . . . , α M ] to represent the coefficients. This implies that instead of directly dealing with eq. <ref type="formula" target="#formula_1">(24)</ref> we can consider a set of n projected equations: φ(x i ) Cv = µφ(x i ) v, i = 1, . . . , M . Plugging eq. (25) here and replacing φ(x i ) φ(x j ) with k(x i , x j ) (the kernel trick), we get 1 M KKα = µKα, which turns out an eigenvalue problem for K: 1 M Kα = µα. Note that this is exactly the same eigenvalue problem solved in eq. (3). As above, we denote by u 1 , . . . , u J the eigenvectors of K that correspond to the J largest eigenvalues λ 1 ≥ · · · ≥ λ J , and we have µ j = λj M . To determine the αs, we set the eigenvectors v to have unit lengths: v v = α Kα = λα α = 1. So the <ref type="bibr">4</ref> We reuse some notations from the above sections (e.g., the eigenvalue µ), and as we shall see, they are closely related.</p><p>αs should be normalized to have length . It was pointed out by <ref type="bibr" target="#b48">Williams &amp; Seeger (2000)</ref> to be equivalent to using the well understood Nyström approximation. We can see this by noticing that each component of ξ(x) is identical to eq. (6) up to a scaling factor:</p><formula xml:id="formula_42">ξ j (x) = α j k x = λ j Mψ j (x).<label>(26)</label></formula><p>Looking back at eq. (16), we can see that SSGE estimates the gradients by a linear estimator with KPCA embeddings as input features. As KPCA embeddings are known to automatically adapt to the geometry of the samples, given a suitable kernel is chosen, it can reduce the curse of dimensionality when the estimator is applied to high dimensional spaces, which helps explain the effectiveness of SSGE.</p><p>Connection to Manifold-modeling Dimension Reduction Methods It has been pointed out in previous works <ref type="bibr" target="#b49">(Williams, 2001;</ref><ref type="bibr" target="#b14">Ham et al., 2004;</ref><ref type="bibr" target="#b3">Bengio et al., 2004a;</ref> that many successful manifold-modeling dimension reduction methods (e.g., MDS, LLE, Laplacian eigenmaps, and Spectral clustering) can be viewed as KPCA with different ways of constructing data-dependent kernels. We believe it is a promising direction to learn a better kernel from a dataset of samples that could improve the manifold modeling behavior of KPCA embeddings, thus further improving the gradient estimator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusion</head><p>We propose the Spectral Stein Gradient Estimator (SSGE) for implicit distributions. Unlike previous methods, SSGE directly estimates the gradient function and thus has a principled out-of-sample extension. Future work may include learning kernels or eigenfunctions in the estimator, as indicated by the error bound as well as the connection to dimension reduction methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Gradient estimates of the log density of N (0, 1).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) Dimensions 2 and 7 of the marginal hyperparameter posterior of Gaussian Process classification on the UCI Glass dataset; (b) The average acceptance ratios of gradient-free HMC using SSGE, KMC, and Stein + .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Prediction results for the 1-D regression experiment. Spectral-Implicit and Spectral-Factorized represent using SSGE to perform VI with implicit posteriors and factorized Gaussian posteriors, respectively. Shaded areas represent 3 times standard deviation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a)-(c) CelebA samples generated by VAE, Implicit VAE trained without the entropy term, and Implicit VAE trained by SSGE; (d) Fréchet Inception Distances (FID) (Heusel et al., 2017) between random generated images and real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>For a new data point x, KPCA computes the embedding ξ(x) (the projection onto the first J eigenvectors) as ξ(x) = [φ(x) v 1 , . . . , φ(x) v J ] = [α 1 k x , . . . , α J k x ] , where k x = [k(x, x 1 ), . . . , k(x, x M )]</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>1 Dept. of Comp. Sci. &amp; Tech., BNRist Center, State Key Lab for Intell. Tech. &amp; Sys., THBI Lab, Tsinghua University 2 Dept. of Comp. Sci., University of Toronto. Correspondence to: Jiaxin Shi &lt;shijx15@mails.tsinghua.edu.cn&gt;, Jun Zhu &lt;dc- szj@tsinghua.edu.cn&gt;.</figDesc><table>Proceedings of the 35 
th International Conference on Machine 
Learning, Stockholm, Sweden, PMLR 80, 2018. Copyright 2018 
by the author(s). 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">The criterion for selecting η is unclear in Li &amp; Turner (2018).</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank anonymous reviewers for insightful feedbacks, and thank the meta-reviewer and Chang Liu for comments on improving Theorem 1. This work was supported by NSFC Projects (Nos. 61620106010, 61621136008, 61332007), Beijing NSF Project (No. L172037), Tiangong Institute for Intelligent Computing, NVIDIA NVAIL Program, Siemens and Intel.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">The pseudo-marginal approach for efficient monte carlo computations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">O</forename><surname>Roberts</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Annals of Statistics</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="697" to="725" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">The Numerical Treatment of Integral Equations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">T</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1997" />
			<publisher>Clarendon Press</publisher>
			<pubPlace>Oxford</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Laplacian eigenmaps for dimensionality reduction and data representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Niyogi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1373" to="1396" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning eigenfunctions links spectral embedding and kernel PCA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouimet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2197" to="2219" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Out-of-sample extensions for LLE, Isomap, MDS, eigenmaps, and spectral clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Paiement</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Delalleau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">L</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ouimet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="177" to="184" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Weight uncertainty in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cornebise</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1613" to="1622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Modern multidimensional scaling: Theory and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Borg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">J</forename><surname>Groenen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<publisher>Springer Science &amp; Business Media</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Dimension reduction: A guided tour. Foundations and Trends R in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="275" to="365" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A kernel test of goodness of fit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chwialkowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2606" to="2615" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Provable bayesian inference via particle mirror descent</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="985" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Adversarial feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Darrell</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09782</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Belghazi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lamb</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Mastropietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00704</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Adversarially learned inference. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Measuring sample quality with stein&apos;s method</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gorham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mackey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="226" to="234" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">A kernel view of the dimensionality reduction of manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mika</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page">47</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Probabilistic backpropagation for scalable learning of bayesian neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1861" to="1869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Gans trained by a two time-scale update rule converge to a nash equilibrium</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Heusel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ramsauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nessler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Klambauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.08500</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08235</idno>
		<title level="m">Variational inference using implicit distributions</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Estimation of non-normalized statistical models by score matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="695" to="709" />
			<date type="published" when="2005-04" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">High-dimensional density ratio estimation with extensions to approximate likelihood computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Izbicki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schafer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="420" to="429" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<title level="m">A method for stochastic optimization</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Gradient estimators for implicit models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">E</forename><surname>Turner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Feng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00081</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Stein variational gradient descent: A general purpose bayesian inference algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances In Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2370" to="2378" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">A kernelized stein discrepancy for goodness-of-fit tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jordan</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="276" to="284" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Structured and efficient variational deep learning with matrix gaussian posteriors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Louizos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1708" to="1716" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adversarial variational bayes: Unifying variational autoencoders and generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2391" to="2400" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lakshminarayanan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.03483</idno>
		<title level="m">Learning in implicit generative models</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Slice sampling covariance hyperparameters of latent gaussian models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="1732" to="1740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Mcmc using hamiltonian dynamics. Handbook of Markov Chain Monte Carlo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Über die praktische auflösung von integralgleichungen mit anwendungen auf randwertaufgaben</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Nyström</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Acta Mathematica</title>
		<imprint>
			<biblScope unit="volume">54</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="185" to="204" />
			<date type="published" when="1930" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">The lanczos algorithm with selective orthogonalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">N</forename><surname>Parlett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Scott</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematics of computation</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">145</biblScope>
			<biblScope unit="page" from="217" to="238" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Operator variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Altosaar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="496" to="504" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">On learning with integral operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Rosasco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">D</forename><surname>Vito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="905" to="934" />
			<date type="published" when="2010-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Nonlinear dimensionality reduction by locally linear embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">290</biblScope>
			<biblScope unit="issue">5500</biblScope>
			<biblScope unit="page" from="2323" to="2326" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Nonlinear component analysis as a kernel eigenvalue problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1299" to="1319" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Kernel adaptive metropolis-hastings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">L</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Andrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1665" to="1673" />
		</imprint>
	</monogr>
	<note>What is an rkhs?</note>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhusuan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.05870</idno>
		<title level="m">A library for Bayesian deep learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Kernel implicit variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Semi-supervised learning using sparse eigenfunction bases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Belkin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1687" to="1695" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Density estimation in infinite dimensional exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hyvärinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kumar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1830" to="1888" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Estimation of the mean of a multivariate normal distribution. The Annals of Statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Stein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="page" from="1135" to="1151" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Gradient-free hamiltonian monte carlo with efficient kernel exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Livingstone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Szabo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="955" to="963" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Efficient and principled score estimation with nystrm kernel exponential families</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sutherland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arbel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page" from="652" to="660" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Hierarchical implicit models and likelihood-free variational inference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranganath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5529" to="5539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Segmentation using eigenvectors: a unifying view</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The proceedings of the seventh IEEE international conference on computer vision</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="1999" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="975" to="982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">The effect of the input density distribution on kernel-based classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On a connection between kernel PCA and metric multidimensional scaling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="675" to="681" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Using the Nyström method to speed up kernel machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Seeger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="page" from="682" to="688" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
