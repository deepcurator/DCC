<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">PointCNN</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rui</forename><surname>Bu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingchao</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Baoquan</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Shandong University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">PointCNN</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>CCS CONCEPTS • Computing methodologies → Shape representations</term>
					<term>Neu- ral networks</term>
					<term>Shape analysis</term>
					<term>KEYWORDS Convolutional Neural Network, Sparse Data, Irregular Domains</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ABSTRACT</head><p>We present a simple and general framework for feature learning from point cloud. The key to the success of CNNs is the convolution operator that is capable of leveraging spatially-local correlation in data represented densely in grids (e.g. images). However, point cloud are irregular and unordered, thus a direct convolving of kernels against the features associated with the points will result in deserting the shape information while being variant to the orders.</p><p>To address these problems, we propose to learn a X-transformation from the input points, and then use it to simultaneously weight the input features associated with the points and permute them into latent potentially canonical order, before the element-wise product and sum operations are applied. The proposed method is a generalization of typical CNNs into learning features from point cloud, thus we call it PointCNN. Experiments show that PointCNN achieves on par or better performance than state-of-the-art methods on multiple challenging benchmark datasets and tasks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">INTRODUCTION</head><p>Spatially-local correlation is an ubiquitous property of various types of data that is independent of the data representation. The convolution operator has shown to be quite effective in exploiting such correlation if the data is represented in regular domains, such as images, and has been the key to the success of CNNs for a variety of tasks <ref type="bibr" target="#b20">[LeCun et al. 2015]</ref>.</p><p>For data that is inherently of less dimension than the ambient space, such as surfaces in 3D space, or line sketches in 2D, it can be more effective if the data is represented as point cloud in the ambient space, rather than a dense grid of the entire space. Not only that, 3D point cloud probably is the most common raw output from 3D sensors, and is becoming more accessible. However, point cloud is irregular and unordered, rendering convolution operator ill-suited for leveraging spatially-local correlation in the data.</p><p>We illustrate the problems and challenges of applying convolution on point cloud with <ref type="figure" target="#fig_0">Figure 1</ref>. Suppose the unordered set of the C dimensional input features are the same F = { f a , f b , f c , f d } in all the cases in <ref type="figure" target="#fig_0">Figure 1</ref>, and we have one convolution kernel K = [k α , k β , k γ , k δ ] T in shape 4 × C. In (i), by following canonical order given by the regular grid structure, the features in the local 2 × 2 patch can be casted into  <ref type="bibr">(ii, iii, and iv)</ref>. In regular grids, each grid cell is associated with a feature. In point cloud, the points are sampled from local neighborhoods, in analogy to local patches in regular grids, and each point is associated with a feature, an order index, as well as its coordinates. However, the lack of regular grids poses the challenge of sorting the points into canonical orders.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conv(·, ·)</head><p>is simply an element-wise product followed by a sum 1 . In (ii), (iii), and (iv), the points are sampled from local neighborhoods, thus can be in arbitrary orders. By following orders as illustrated in the figure, the input feature set F can be casted into [f a , f b , f c , f d ] T in (ii) and (iii), and [f c , f a , f b , f d ] T in (iv). Based on this, if the convolution operator is directly applied, the output features for the three cases could be computed as:</p><formula xml:id="formula_0">f ii = Conv(K, [f a , f b , f c , f d ] T ), f iii = Conv(K, [f a , f b , f c , f d ] T ), f iv = Conv(K, [f c , f a , f b , f d ] T ).<label>(1)</label></formula><p>Note that f ii ≡ f iii holds for all cases, while f iii f iv holds for most cases. Now, it is clear that a direct convolving results in deserting the shape information (i.e., f ii ≡ f iii ) while being variant to the orders (i.e., f iii f iv ).</p><p>In this paper, we propose to learn a K ×K X-transformation from the coordinates of K input points (p 1 , p 2 , ..., p K ) with multilayer perceptron <ref type="bibr" target="#b33">[Rumelhart et al. 1986</ref>], i.e., X = MLP(p 1 , p 2 , ..., p K ). then use it to simultaneously weight and permute the input features, and finally apply the typical convolution on the transformed features. We call the process X-Conv, and it is the basic building block for our PointCNN. The X-Conv for (ii), (iii), and (iv) in <ref type="figure" target="#fig_0">Figure 1</ref> can be depicted as:</p><formula xml:id="formula_1">f ii = Conv(K, X ii × [f a , f b , f c , f d ] T ), f iii = Conv(K, X iii × [f a , f b , f c , f d ] T ), f iv = Conv(K, X iv × [f c , f a , f b , f d ] T ),<label>(2)</label></formula><p>where the Xs are 4×4 matrices, as K = 4 in <ref type="figure" target="#fig_0">Figure 1</ref>. Note that since X ii and X iii are learnt from points in different shapes, they can be different to weight the input features accordingly, thus achieve f ii f iii . For X iii and X iv , if they are learnt to satisfy X iii = X iv × Π, where Π is the permutation matrix for permuting <ref type="bibr">(c, a, b, d)</ref> into <ref type="bibr">(a, b, c, d)</ref>, then f iii ≡ f iv can be achieved. From the analysis of the example in <ref type="figure" target="#fig_0">Figure 1</ref>, it is clear that, with ideal X-transformations, X-Conv is capable of taking the point shapes into consideration, while being independent of point orders.</p><p>In practice, we found that the learnt X-transformations are far from ideal, especially in terms of the permutation equivalence aspect. Nevertheless, PointCNN built with X-Conv is still significantly better than a direct application of typical convolution on point cloud, and on par or better than state-of-the-art non-convolutional neural networks designed for consuming point cloud data, such as PointNet++ <ref type="bibr" target="#b29">[Qi et al. 2017b]</ref>.</p><p>We explain the detail of X-Conv, as well as PointCNN architectures in Section 3. We show our results on multiple challenging benchmark datasets and tasks in Section 4, together with ablation experiments and visualizations for better understanding of PointCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">RELATED WORK</head><p>Feature Learning from Regular Domains. CNNs were originally designed for taking advantage of spatially-local correlation in images <ref type="bibr" target="#b21">[LeCun et al. 1998</ref>], which are represented as pixels in 2D regular grids. Since spatially-local correlation is not only a property of 2D images, but also that of many general data, CNNs have brought about breakthroughs in processing images, video, speech and audio <ref type="bibr" target="#b20">[LeCun et al. 2015]</ref>. There has been work in extending CNNs to higher dimensional regular domains, such as 3D voxels <ref type="bibr" target="#b39">[Wu et al. 2015b]</ref>. However, as both the input and convolution kernels are of higher dimensions, the amount of both computation and memory inflates dramatically. Octree based approaches have been proposed to save computation by skipping convolution in empty space <ref type="bibr" target="#b31">[Riegler et al. 2017;</ref><ref type="bibr" target="#b37">Wang et al. 2017]</ref>. The activations are kept sparse in ] to retain sparsity in convolved layers. Nevertheless, the kernels themselves are still dense and of high dimension in these approaches. Sparse kernels are proposed in , but this approach cannot be applied recursively for learning hierarchical features. Compared with these methods, PointCNN is sparse in both input representation and convolution kernels.</p><p>Feature Learning from Irregular Domains. Stimulated by the rapid advances and demands in 3D sensing, there has been quite a few recent developments in feature learning from 3D point cloud. PointNet <ref type="bibr" target="#b27">[Qi et al. 2017a]</ref> and Deep Sets <ref type="bibr" target="#b44">[Zaheer et al. 2017]</ref> proposed to achieve input order invariance by the use of symmetric function over inputs. <ref type="bibr">PointNet++ [Qi et al. 2017b</ref>] applies PointNet hierarchically for better capturing of local structures. In contrast, PointCNN uses X-Conv in the local feature extraction stage, while PointNet is max-pooling based, and is shown to perform better on various tasks. Besides point cloud, sparse data in irregular domains can be represented as graphs, or meshes, and a line of work have been proposed for feature learning from such representations <ref type="bibr" target="#b24">[Maron et al. 2017;</ref><ref type="bibr" target="#b25">Monti et al. 2017;</ref><ref type="bibr" target="#b41">Yi et al. 2017a]</ref>. We refer the interested reader to <ref type="bibr" target="#b2">[Bronstein et al. 2017</ref>] for a comprehensive survey of work along these directions.</p><p>--</p><p>Figure 2: Hierarchical convolution from regular grids (upper) and point cloud (lower). In regular grids, convolution operator is recursively applied on local grid patches, which often reduces the spatial resolution of the grids (4 × 4 → 3 × 3 → 2 × 2), while increases their channel number (visualized by the dots' thickness). Similarly, in point cloud, XConv is recursively applied to "project", or "aggregate", information from neighborhoods into less and less representative points (9 → 5 → 2), but each with richer information (again, visualized by the dots' thickness).</p><p>Invariance vs. Equivariance. While symmetric function based approaches ( <ref type="bibr" target="#b7">[Dieleman et al. 2016;</ref><ref type="bibr" target="#b27">Qi et al. 2017a;</ref><ref type="bibr" target="#b30">Ravanbakhsh et al. 2016;</ref><ref type="bibr" target="#b44">Zaheer et al. 2017]</ref>) have theoretical guarantee in terms of achieving order invariance, they come with a price of throwing away information. Hinton et al. proposed a line of pioneering work for addressing this problem through equivariance, rather than invariance <ref type="bibr" target="#b13">[Hinton et al. 2011;</ref><ref type="bibr" target="#b34">Sabour et al. 2017]</ref>. The Xtransformations in our formulation, ideally, are capable of realizing equivariance, and are demonstrated to be effective in practice. We also found similarity between PointCNN and Spatial Transformer Networks (STNs) <ref type="bibr" target="#b15">[Jaderberg et al. 2015]</ref>, in the sense that both of them provided a mechanism to "transform" input into latent canonical forms for being further processed. And similar to STNs, though there is no explicit loss or constraint in PointCNN for enforcing the canonicalization, in practice, it turns out that the networks find their ways to leverage the mechanism for learning better. Note that, in PointCNN, the X-transformation is supposed to serve for both weighting and permutation, thus is modelled as a general matrix. This is different than that in <ref type="bibr" target="#b5">[Cruz et al. 2017]</ref>, where a permutation matrix is the desired output, and is approximated by a doubly stochastic matrix.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">PointCNN</head><p>The hierarchical application of convolution operator is essential to CNNs for learning hierarchical representation. PointCNN shares the same design, and generalizes it to point cloud. In this section, we firstly introduce hierarchical convolution in PointCNN, in analogy to that image CNNs, then explain the core X-Conv operator in detail, and finally present PointCNN architectures for classification and segmentation tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Hierarchical Convolution</head><p>Before we introduce the hierarchical convolution in PointCNN, we briefly go through that in image CNNs, with the illustration of <ref type="figure">Figure 2</ref> upper. The input to image CNNs is a feature map F 1 in shape R 1 × R 1 × C 1 , where R 1 is the spatial resolution, and C 1 is the feature channel depth. The convolution of kernels K in shape K × K × C 1 × C 2 against local patches in shape K × K × C 1 from F 1 yields another feature map F 2 in shape R 2 × R 2 × C 2 . Note that in <ref type="figure">Figure 2</ref> upper, R 1 = 4, K = 2, and R 2 = 3. Compared with F 1 , F 2 often is of lower resolution (R 2 &lt; R 1 ) and deeper channels (C 2 &gt; C 1 ), and encodes higher level information. This process is recursively applied, producing feature maps in less and less spatial resolution (4 × 4 → 3 × 3 → 2 × 2 in <ref type="figure">Figure 2</ref> upper), but deeper and deeper channels (visualized by thicker and thicker dots in <ref type="figure">Figure 2</ref> upper).</p><p>The input to PointCNN is</p><formula xml:id="formula_3">F 1 = {(p 1,i , f 1,i ) : i = 1, 2, ..., N 1 }, i.e., a set of points {p 1,i : p 1,i ∈ R D }, each associated with a feature { f 1,i : f 1,i ∈ R C 1 }.</formula><p>Following the hierarchical construction of image CNNs, we would like to apply X-Conv on F 1 and get a higher level representation</p><formula xml:id="formula_4">F 2 = {(p 2,i , f 2,i ) : f 2,i ∈ R C 2 , i = 1, 2, ..., N 2 },</formula><p>where {p 2,i } is a set representative points of {p 1,i }, i.e., N 2 &lt; N 1 , and C 2 &gt; C 1 , so F 2 is of less resolution and deeper feature channels than F 1 . When the X-Conv process of turning F 1 into F 2 is recursively applied, the input points with features are "projected", or "aggregated", into less and less points (9 → 5 → 2 in <ref type="figure">Figure 2</ref> lower), but each with richer and richer features (visualized by thicker and thicker dots in <ref type="figure">Figure 2</ref> lower).</p><p>Note that {p 2,i } is not necessarily a subset of {p 1,i }. The representative points can be at arbitrary locations in the space whichever are beneficial for the information "projection" or "aggregation". In our implementation, {p 2,i } is simply a random down-sampling of {p 1,i } for classification tasks, and farthest point sampling for segmentation tasks, as segmentation tasks are more demanding on a uniform point distribution. We suspect some special points which have shown promising performance in geometric processing, such as Deep Ponits <ref type="bibr" target="#b38">[Wu et al. 2015a]</ref>, could fit in here as well. However, we leave the exploration of better representative points generation methods as future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">X-Conv Operator</head><p>X-Conv is the core operator for turning F 1 into F 2 . To leverage spatially-local correlation, similar to convolution in image CNNs, X-Conv works with local regions. Since the output features are supposed to be associated with the representative points {p 2,i }, X-Conv takes their neighborhood points in {p 1,i }, as well as the associated features, as input to convolve with.</p><p>For simplicity, we denote a representative point in {p 2,i } as p, and its K neighbors in {p 1,i } as N, thus the X-Conv input for this specific p is S = {(p i , f i ) : p i ∈ N}. Note that S is an unordered set. Without loss of generality, S can be casted into a</p><formula xml:id="formula_5">K × D matrix P = (p 1 , p 2 , ..., p K ) T , and a K × C 1 matrix F = (f 1 , f 2 , ..., f K ) T . The trainable parameters of X-Conv is a K × (C 1 + C δ ) × C 2 tensor K.</formula><p>With these inputs, we would like to compute feature F p , which is the input features "projected", or "aggregated" into the representative point p. We depict the X-Conv operator in Algorithm 1, or maybe</p><formula xml:id="formula_6">ALGORITHM 1: X-Conv Operator Input : K, p, P, F Output : F p ▷ Features "projected", or "aggregated", to p 1: P ′ ← P − p ▷ Move P to local coordinate system of p 2: F δ ← MLP δ (P ′ ) ▷ Individually lift each point into C δ dim. space 3: F * ← [F δ , F] ▷ Concatenate F δ and F, F * is a K × (C δ + C 1 ) matrix 4: X ← MLP(P ′ )</formula><p>▷ Learn the K × K X-transformation matrix 5: F X ← X × F * ▷ Weight and permute F * with the learnt X 6: F p ← Conv(K, F X ) ▷ Finally, typical convolution between K and F X more concisely, it can be summarized as:</p><formula xml:id="formula_7">F p = X−Conv(K, p, P, F) = Conv(K, MLP(P − p) × [MLP δ (P − p), F]),<label>(3)</label></formula><p>where MLP δ (·) is a multilayer perceptron applied individually on each point, same to that in PointNet. Note that all the operations involved in building X-Conv, i.e., Conv(·, ·), MLP(·), matrix multiplication (·)×(·), and MLP δ (·), are differentiable. In this case, clearly, X-Conv is differentiable, thus can be plugged into neural network for training by back propagation. In our implementation, K nearest neighbor search is applied for extracting the K neighboring points. This assumes a more or less uniform distribution of input points. For point cloud with nonuniform point distribution, a radius search can be applied first, and then randomly sample K points out of the radius search results.</p><p>Note that trainable kernel</p><formula xml:id="formula_8">K of X-Conv is a K × (C 1 + C δ ) × C 2 tensor.</formula><p>The trainable parameter number is proportional to the number of neighboring points K, instead of being quadratic in image CNNs, or cubic in 3D CNNs. In this sense, we consider our PointCNN sparse in both the input representation and kernels, and it saves both memory and computation. The sparse kernels enables the coupling of long range information without dramatic growth of trainable parameter numbers.</p><p>Since Line 4-6 of Algorithm 1 have been covered in the Introduction, here we explain the rationale behind Line 1-3 of Algorithm 1 in detail. Since X-Conv is designed to work on local point regions, the output should not be dependent on the absolute position of p and its neighboring points, but on their relative positions, thus we build local coordinate systems at the representative points and the neighboring points are translated to center around the origins, i.e., P ′ ← P − p (Line 1 of Algorithm 1). Note that one point may be in the neighborhood of multiple representative points, for example, p 1,1 is neighboring to both p 2,1 and p 2,2 in <ref type="figure">Figure 3</ref> a and b, thus one point can be at different relative positions in local coordinate systems of different representative points.</p><p>It is the local coordinates of neighboring points, together with their associated features, that defines the output features. In other word, besides the associated features, the local coordinates themselves are part of the input features as well. However, the local coordinates are of quite different dimensionality and representation than the associated features. We first lift the coordinates into an higher dimensional and more abstract representation (F δ ← MLP δ (P ′ ), Line 2 of Algorithm 1), and then combine it with the associated features (F * ← [F δ , F], Line 3 of Algorithm 1) for being further processed <ref type="figure">(Figure 3 c)</ref>.</p><formula xml:id="formula_9">( #,# , #,# ) ',# ',' ',' ',# ',' ',# [ , ( #,# − ',# ), #,# ] a b c ( #,# − ',' , #,# ) [ , ( #,# − ',# ), #,# ] ( #,# − ',# , #,# )</formula><p>Figure 3: The process for converting point coordinates to features. The neighboring points of representative points are transformed to local coordinate systems of the representative points (a and b). Then the local coordinates of each point are individually lifted into features, and combined with the associated features (c).</p><p>The lifting of coordinates into features is through a point-wise MLP δ (·), which is the same as that in PointNet and PointNet++. However, the lifted features are not processed by a symmetric function in PointCNN. Instead, they are weighted and potentially permuted, together with the associated features, by the learnt Xtransformation. Note that, unlike MLP δ (·), MLP(·) is applied on the entire neighboring point coordinates. Thus the resulting X is dependent on the order of the points, and this is desired, as X is supposed to permute F * according to the input points, thus it has to be aware of the specific input order.</p><p>One nice property of X-Conv is that it handles point cloud with or without additional features in a quite uniform fashion. For input point cloud without any additional features, i.e., F is empty, the first X-Conv layer uses only F δ .</p><p>Note that, in theory, X-transformation can be applied on either the features, or the kernels. We opt to apply it on the features, in which way, the follow up operation is a standard Conv operation -an operation that is highly optimized by popular deep learning frameworks. Otherwise, it will result in a convolution between features and the kernels "spawned" by X, which is not common, thus probably not fully optimized.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">PointCNN Architectures</head><p>From <ref type="figure">Figure 2</ref>, we can see that the convolution layers in image CNNs and X-Conv layers in PointCNN only differs in two aspects: the way the local regions are extracted (K × K patches in image CNNs vs. K neighboring points around representative points.) and the way the information from local regions is learnt (Conv in image CNNs vs. X-Conv). Otherwise, there is no much difference in assembling a deep network with the X-Conv layers than that with convolution layers in image CNNs.</p><p>In <ref type="figure" target="#fig_1">Figure 4</ref> (a), we show a simple PointCNN, with two X-Conv layers that gradually turn the input points (with or without features) into less representation points, but each with richer feature. After the second X-Conv layer, there is only one representative point left, and it received information from all the points from the previous layer. In PointCNN, we can roughly define the receptive field of each representative point as the ratio K/N , where K is the neighboring point number, and N is the point number in the previous layer. With this definition, the only one left point "sees" all the points from previous layer, thus has receptive field 1.0 -it has a global view of the entire shape, thus its features are informative for semantic understanding of the shape. We can add some fully connected layers on top of the last X-Conv layer output followed by a loss for training the network. Note that the number of training samples for the top X-Conv layers of PointCNN in <ref type="figure" target="#fig_1">Figure 4</ref> (a) drops rapidly, making it inefficient to train the top X-Conv layers thoroughly. To address this problem, we propose the PointCNN in <ref type="figure" target="#fig_1">Figure 4</ref> (b), where more representative points are kept in the X-Conv layers. However, we want to maintain the depth of the network, while keeping the receptive field growth rate, such that the deeper representative points "see" larger and larger portion of the entire shape. We achieve this goal by employing the dilated convolution idea from image CNNs into PointCNN. Instead of always taking the K neighboring points as input, we may uniformly sample K input points from K × D neighboring points, where D is the dilation rate. In this case, the receptive field increases from K/N to (K ×D)/N , without the increase of the actual neighboring point number, nor the kernel size.</p><p>In the second X-Conv layer of PointCNN in <ref type="figure" target="#fig_1">Figure 4</ref> (b), dilation rate D = 2 is used, thus all the four remaining representative points "see" the entire shape, and all of them are suitable for making predictions. Note that, in this way, we can train the top X-Conv layers more thoroughly, as much more connections are involved in the network, compared with that in PointCNN of <ref type="figure" target="#fig_1">Figure 4</ref> (a). In testing time, the output from the multiple representative points are averaged right before the so f tmax to stabilize the prediction. This design is quite similar to that of Network in Network <ref type="bibr">[Lin et</ref>   <ref type="bibr" target="#b39">[Wu et al. 2015b]</ref> and ScanNet <ref type="bibr" target="#b6">[Dai et al. 2017]</ref>.</p><p>2014]. PointCNN in the denser style <ref type="figure" target="#fig_1">(Figure 4 (b)</ref>) is the one we used for classification tasks.</p><p>For segmentation tasks, high resolution pointwise output is required, and this can be realized by building PointCNN following Conv-DeConv <ref type="bibr" target="#b26">[Noh et al. 2015]</ref> architecture, where the DeConv part is responsible of propagating global information into high resolution predictions (see <ref type="figure" target="#fig_1">Figure 4</ref> (c)). Note that both the "Conv" and "DecConv" in PointCNN segmentation network are the same X-Conv operator. For "DeConv" layers, the only difference with the "Conv" layers is that there are more points, but less feature channels, in the output than that in the input. And the higher resolution points for the "DeConv" layers are forwarded from earlier "Conv" layers, following the design of U-Net <ref type="bibr" target="#b32">[Ronneberger et al. 2015]</ref>.</p><p>ELU <ref type="bibr" target="#b4">[Clevert et al. 2016</ref>] is the nonlinear activation function used in PointCNN, as we found it is more stable and performs slightly better than ReLU <ref type="bibr" target="#b9">[Glorot et al. 2011]</ref>. Batch normalization <ref type="bibr" target="#b14">[Ioffe and Szegedy 2015]</ref> is applied on P ′ , F p and the fully connected layer outputs (except for that of the last fully connected layer) for reducing internal covariate shift. It is important to note that batch normalization should not be applied in MLP δ and MLP, since F * and X, especially X, are supposed to be quite specific for a particular representative point. For the Conv in Line 6 of Algorithm 1, separable convolution <ref type="bibr" target="#b3">[Chollet 2016</ref>] is used for reducing parameter number and computation than that of typical convolution. We use ADAM optimizer <ref type="bibr" target="#b16">[Kingma and Ba 2014]</ref> with initial learning rate 0.01 for the training of PointCNN.</p><p>Dropout is applied before the last fully connected layer for reducing over-fitting. We also employed the "subvolume supervision" idea from  for addressing over-fitting problem. In the last X-Conv layers, the receptive field is set to be less than 1, such that only a partial information is "seen" by the representative points in the last X-Conv layers. The network is pushed to learn harder from the partial information at training time, and performs better in testing time.</p><p>In this paper, PointCNN is demonstrated with simple feed forward networks on classification tasks, and simple feed forward layers plus skip-links in segmentation network. However, since the interface X-Conv exposed to its input and output layers is quite similar to that of Conv, we think many advanced neural network techniques from image CNNs can be adopted to work with X-Conv, e.g., recurrent PointCNN. We leave the exploration along these directions as future work.</p><p>Data augmentation. For the training of the parameters in XConv, clearly, it is not beneficial if the neighboring points are always the same set in the same order for a specific representative point. To improve the generalizability, we propose to randomly sample and shuffle the input points, such that both the neighboring point sets and order can be different from batch to batch. To train a model that takes N points as input, N (N , (N /8) 2 ) points are used for the training, where N denotes Gaussian distribution. We found this strategy is crucial for the training of PointCNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">EXPERIMENTS 4.1 Datasets and Tasks</head><p>We conducted extensive evaluation of PointCNN for classification task on six benchmark datasets, and segmentation task on three benchmark datasets. The PointCNN architecture details for the tasks on these datasets can be found in the Appendix A.1. Before we dive into the experimental results, we introduce the datasets:</p><p>• Object datasets: ModelNet40 <ref type="bibr" target="#b39">[Wu et al. 2015b]</ref> and ShapeNet Parts <ref type="bibr" target="#b40">[Yi et al. 2016</ref>].</p><p>-ModelNet40 is composed of 12, 311 3D mesh models from 40 categories, with a 9, 843/2, 468 training/testing split. Both the gravity and "facing" directions of the models are aligned in the dataset. However, the "facing" direction is ignored by random horizontal rotations to better approximate the scenarios in real world applications. We use the point cloud conversion of ModelNet40 provided by <ref type="bibr" target="#b27">[Qi et al. 2017a</ref>] as our input, where 2, 048 points are sampled from each mesh, and we further sample N (10, 24, 128 2 ) points to train a model for testing with 1, 024 points on the classification task. -ShapeNet Parts contains 16, 880 models (14, 006/2, 874 training/testing split) from 16 shape categories, each annotated with 2 to 6 parts and there are 50 different parts in total. Each point sampled from the models is associated with a part label. The task is to predict the part label for each point, thus a segmentation task, and can be treated as a dense point-wise classification problem. The category label for each model is given, and can be used for trimming irrelevant predictions, same as that in . N (2048, 256 2 ) points are sampled from each point cloud to train a model for testing with 2, 048 input points on the segmentation task. Each testing point cloud is sampled mutliple times to make sure all the points are evaluated at least r (r = 10 in our experiments) times at testing time.</p><p>• Indoor scene datasets: S3DIS <ref type="bibr">[Armeni et al. 2016</ref>  <ref type="bibr" target="#b40">[Yi et al. 2016]</ref> in part averaged IoU (%), S3DIS <ref type="bibr">[Armeni et al. 2016]</ref> in mean IoU (%), and ScanNet <ref type="bibr" target="#b6">[Dai et al. 2017]</ref> in per voxel accuracy (%).</p><p>are mostly made by 3D modeling tools, S3DIS and ScanNet are from real scans of indoor environments.</p><p>-S3DIS contains 3D scans from Matterport scanners in 6 areas including 271 rooms. Each point with RGB features in the scan is annotated with one of the semantic labels from 13 categories.</p><p>The task is segmentation The data is firstly split by room, and then the rooms are sliced into 1.5m by 1.5m blocks, with 0.3m padding on each side. The sliced blocks are handled in the same way as the object point clouds in ShapeNet Parts. The points in the pading areas serve as context of the internal points, and themselves are not linked to loss in the training phase, nor used for prediction in the testing phase. -ScanNet contains 1, 513 scanned and reconstructed indoor scenes, with 1, 201/312 scenes for training/testing in semantic voxel labeling of 17 categories. We firstly prepare data in the same way as that of S3DIS to train a segmentation model, and the segmentation results on testing data are then converted into semantic voxel labeling, as that in <ref type="bibr" target="#b29">[Qi et al. 2017b]</ref>, for a fair comparison with previous methods. The 9, 305/2, 606 training/testing object instances from the 17 categories in ScaneNet are also used for evaluating classification task. Note that ScanNet comes with RGB information for each point. However, they are not used in previous methods. To make fair comparisons, we do not use them either.</p><p>• 2D sketch datasets: TU-Berlin <ref type="bibr" target="#b8">[Eitz et al. 2012]</ref> and Quick Draw <ref type="bibr" target="#b12">[Ha and Eck 2017]</ref>. Similar to surfaces in 3D space, line sketches in 2D are inherently of less dimension than the ambient space, and can be represented as point cloud, thus we consider 2D sketches good arena for evaluating neural networks that are designed to consume point cloud data. TU-Berlin has sketches from 250 categories, with 80 sketches from each category, where 2/3 are used for training and the rest 1/3 for testing. Quick Draw is the largest available sketch dataset, with sketches from 345 categories, each with 70, 000/2, 500 training/testing samples. We sample N (512, 64 2 ) points from the sketch stokes to train a model for testing with 512 points on sketch classification task.</p><p>• Image datasets: MNIST and CIFAR10. MNIST and CIFAR10 are widely used for sanity check of image CNNs. Since PointCNN is a generalization of CNNs, we would like to evaluate PointCNN on the point cloud representation of MNIST and CIFAR10. For MNIST, we randomly sample 160 foreground pixels and convert them into point cloud representation, with the gray-scale pixel value as the input feature. For CIFAR10, we randomly sampled Method TU-Berlin Quick Draw Sketch-a-Net  77.95 -AlexNet <ref type="bibr" target="#b18">[Krizhevsky et al. 2012]</ref> 68.60 -PointNet++ <ref type="bibr" target="#b29">[Qi et al. 2017b]</ref> 66.53 51.58 PointCNN 67.72 56.75 <ref type="table">Table 3</ref>: Accuracy (%) comparisons on Tu-Berlin <ref type="bibr" target="#b8">[Eitz et al. 2012]</ref> and Quick Draw <ref type="bibr" target="#b12">[Ha and Eck 2017]</ref> classification.</p><p>512 pixels out of the 32 × 32 pixels for converting into point cloud with RGB features. Note that there is "shape" information in the MNIST point cloud, sine the point cloud follow the digits' structure, but this is not the case for the CIFAR10 point cloud, where the points are mostly the same blob for all the data samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Classification and Segmentation Results</head><p>Classification results. Classification task is generally considered as the touchstone for the evaluation of a neural network, as a neural network which shows strong performance on classification task can often be adapted to achieve strong performance on other tasks. We evaluate PointCNN on the classification of ModelNet40 and ScanNet 3D objects, and TU-Berlin and Quick Draw 2D sketches.</p><p>Since ModelNet40, ScanNet and TU-Berlin datasets are rather small, strong over-fitting is observed if the receptive field of the last X-Conv layers is set to 1. To address this problem, we set the receptive field of the last X-Conv layers to 1/3 for PointCNN of ModelNet40 and ScanNet, 5/8 for that of TU-Berlin and Quick Draw.</p><p>We summarize our results on ModelNet40 and ScanNet in <ref type="table">Table 1</ref>, in comparison with several methods that are designed to consume these data in different representations with different core operators. Compare with the other representations, point cloud probably is the most direct output of 3D sensing. The consistent better performance of PointCNN on these two datasets makes PointCNN a natural choice for endowing 3D sensing pipeline with recognition capability.</p><p>2 An earlier version of this work participated ShapeNet Parts segmentation challenge at ICCV 2017 and was briefly described in the technical report . The major reasons for why it performs less well are three folds: 1. it did not assume the aligned "facing" directions, thus added random horizontal rotations to the data, which made the task harder; 2. random sampling was used for generating the representative points, rather than farthest point sampling; 3. it was using ReLU for activation, and batch normalization was used in MLP δ and MLP.  PointCNN results on the classification task of TU-Berlin and Quick Draw sketches are presented in <ref type="table">Table 3</ref>, where we compare it with the competitive PointNet++, as well as image CNN based methods. PointCNN is better than PointNet++ on both of the two datasets, and the advantage is more prominent on Quick Draw (25M data samples), which is significantly larger than TUBerlin (0.02M data samples). On TU-Berlin dataset, while the performance of PointCNN is comparable with the widely used generic AlexNet <ref type="bibr" target="#b18">[Krizhevsky et al. 2012]</ref> image CNN, there is still a big gap with the specialized Sketch-a-Net  image CNN. It is interesting to study whether the specialized designs in Sketch-aNet can be adopted into PointCNN for improving its performance on the sketch datasets.</p><formula xml:id="formula_10">PointCNN w/o X w/o X (wider) w/o X (deeper) Core Layers X-Conv ×4 Conv × 4 Conv × 4 Conv × 6 # Parameter 0.45M 0.23M 0.49M 0.4M</formula><p>Segmentation results. Segmentation is a more challenging task than classification, as it requires a finer understanding of the data. We evaluate PointCNN on the segmentation of ShapeNet Parts, S3DIS and ScanNet datasets, and summarize the results in <ref type="table">Table 2</ref>.</p><p>Note that the part averaged IoU metric for ShapeNet Parts is the one used for ICCV 2017 ShapeNet Parts segmentation challenge . It is a weighted average of per category IoU, with the number of shapes in each category as the weights, and the per category IoU is computed for each category first by averaging across all parts on all shapes with the certain category label. Compared with mean IoU, the part averaged IoU puts more emphasis on the correct prediction of small parts.</p><p>From <ref type="table">Table 2</ref>, we can see that PointCNN is better than all the comparison methods, including SSCN ], SegCloud <ref type="bibr" target="#b36">[Tchapmi et al. 2017]</ref> and SPGraph <ref type="bibr" target="#b19">[Landrieu and Simonovsky 2017]</ref>, which are specialized networks designed for segmentation tasks with very competitive performance. Note that the performance of PointCNN on S3DIS is on par with other methods, even if the RGB features are not used, which is a strong indication that PointCNN can effectively leverage "shape" information in point cloud. And the additional performance brought by using RGB features shows that PointCNN can benefit from extra features than the point coordinates to learn even better (from 54.1% to 62.74%, 8.64% improvement).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Ablation Experiments and Visualizations</head><p>Ablation test of the core X-Conv operator. To verify the effectiveness of the core X-Conv operator, we propose PointCNN w/o X as a baseline method, where Line 4-6 of Algorithm 1 is replaced by F p ← Conv(K, F * ), i.e., the input features are convolved without the transformation of the learnt X. This is all and the only difference between PointCNN and PointCNN w/o X. Compared with PointCNN, the baseline has less trainable parameters, and is more "shallow" due to the removal of MLP(·) in Line 4 of Algorithm 1. To make the comparison fair, based on the baseline, we further propose PointCNN w/o X (wider) and w/o X(deeper). As indicated by their names, they are wider/deeper than PointCNN w/o X, and have approximately same amount of parameters as PointCNN. The model depth of PointCNN w/o X(deeper) also compensate the depth decrease introduced by the removal of MLP(·) from PointCNN.</p><p>The comparison results are summarized in <ref type="table" target="#tab_3">Table 4</ref>. Clearly, PointCNN outperforms the proposed variants with a significant margin, and the gap between PointCNN and PointCNN w/o X is neither due to model parameter number, nor model depth. While the widening and deepening of PointCNN w/o X bring extra parameters (and model depth) and increase accuracy on training time, they do not generalized well to testing data. In contrast, the extra parameters in MLP(·) of PointCNN introduced for learning X turned out to be an high-return investment. With these comparisons, we can conclude that X-Conv is the key to the performance of PointCNN.</p><p>Note that after the removal MLP(·), there is still MLP δ (·) (Line 2 of Algorithm 1, the same module used in PointNet) in the network. Remember that we use separable convolution <ref type="bibr" target="#b3">[Chollet 2016]</ref> for Conv(·, ·). In this case, if the depthwise filters are learnt to be some constants, then the separable convolution approximates the mean function, which is symmetric, thus the results can be made independent of the input point order, similar to the max-pooling in PointNet. We suspect this might be the reason why the performance of PointCNNs w/o X is on par with PointNet.</p><p>Stress test on small number of input points. We evaluate PointCNN performance with different number of input points on ModelNet40 classification task, in comparison with PointNet and PointNet++, and summarize the results in <ref type="table">Table 6</ref>. Note that the performance of PointCNN at 512 (90.7% accuracy) input points is on par with PointNet++ with 1, 024 input points. Moreover, when input point number is further reduced into 64 (88.3% accuracy) and 32 (84.4% accuracy), PointCNN outperforms PointNet/PointNet++ with a 3.9% and 18.3% gap respectively. In such settings, the inference runs at 0.6ms and 0.3ms per sample on NVidia GTX 1080 GPU 3 , making PointCNN quite promising for real time recognition applications with low resolution point cloud input, such as autonomous driving.</p><p>How about if we per-order the neighboring points? PointCNN is designed to consume unordered points, and it is up to the network for permuting the points into a latent potentially canonical order. One interesting question to ask is that whether PointCNN can benefit from some sort of pre-ordering. We verified this hypothesis by sorting the extracted neighboring points according to their coordinates, one axis by one axis. We found that while this strategy sometimes brings a small amount of gain on the training dataset, it is often harmful for the performance on testing dataset, even through the same pre-ordering is applied in both training and testing dataset.</p><p>T-SNE visualization of X-Conv features. Note that each representative point, with its neighboring points in a particular order, has a corresponding F * and F X in R K ×C , where C = C δ + C 1 . For the same representative point, if its neighboring points in different orders are feed into the network, we get a set of F * and F X , and we denote them as F * and F X . Similarly, we define the set of F * in PointCNN w/o X as Clearly, F * can be quite scattering in the R K ×C space, since different of input point orders will result in a different F * . On the other hand, if the learnt X can perfectly canonize F * , F X is supposed to stay at a canonical point in the space.</p><p>To verify this, we show T-SNE visualization of F o , F * and F X of 15 randomly picking representative points from ModelNet40 dataset in <ref type="figure">Figure 5</ref>, each with one color, and same color in the sub-figures. Note that F o is quite "blended", which indicates that the features from different representative points are not discriminative to each other ( <ref type="figure">Figure 5 (a)</ref>). F * while being better than F o , is still "fuzzy" <ref type="figure">(Figure 5 (b)</ref>). In <ref type="figure">(Figure 5 (c)</ref>), F * are "concentrated" by X, and the features of each representative points become highly discriminative from each other. Note that even though the "concentration" is far from reaching a point, the improvement is significant, and this visualization explains the extraordinary performance of PointCNN in feature learning.</p><p>Distance visualization of features at different hierarchies. Hierarchical feature representation is an important feature of PointCNN. In image CNNs, such hierarchies can be visualized from the 2D grid kernels. However, due to the lack of grid structure, the kernels of PointCNN cannot be observed in such a way. Instead, we opt to visualize PointCNN features at different hierarchies by examining the feature distances of representative points at different hierarchies.</p><p>In <ref type="figure" target="#fig_3">Figure 7</ref>, each point is associated with the feature from a certain hierarchy, and the points with an arrow indication are set as the query points, the feature distance between these points and all the rest points are computed. The points with distances less than 40% of the maximal distances are highlighted with blue, and the darker the color, the more similar the points are to the query points. It is clear that while the low level features of PointCNN capture local geometry, the high level features faithfully encode semantic information.</p><p>Model size, memory usage and timing. We implement PointCNN in tensorflow <ref type="bibr" target="#b0">[Abadi et al. 2015]</ref>. The actual model size, memory usage and timing depends on the model complexity. Here we report the statistics for two typical settings. The PointCNN for classification with 1024 input points has 0.5M parameters, and can run on NVidia GTX 1080 GPU (8GB GPU memory) with batch size 200, at 0.9/0.23 second per batch at training/inference stage. The PointCNN for segmentation with 2048 input points has 4.5M parameters, and can run on NVidia Tesla P100 GPU (16GB GPU memory) with batch size 32, at 0.44/0.41 second per batch at training/inference stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Discussions and Future Work</head><p>Better understanding of X-transformation. While X-Conv is designed aiming at addressing the problems in applying convolution directly on point cloud, and it is demonstrated to outperform stateof-the-art methods, the rigorous theoretical rationale behind this operation, especially when it is composited into a deep neural network, remains barely understood. We proposed probably the most straight forward way of learning the X-transformations as general matrices simply with MLP. While the general matrices are capable of realizing weighting and permutation, it is not clear whether it is the minimalist for our goal. Actually, we found that PointCNN can exhibit strong over-fitting on some small datasets, such as ModelNet40 and TU-Berlin sketches, even though the parameter number in PointCNN is rather small compared with alternative methods. It seems that while powerful representation capability is brought by X-transformations, extra degrees of freedom smuggled in. It is extremely intriguing to study whether there are some structural constraints in X, rather than being a general matrix, and to propose specific methods for learning it.</p><p>From Line 1 of Algorithm 1 (P ′ ← P − p), clearly, X-Conv is independent of translation. However, it is not clear whether XConv is capable of achieving equivariance on the rigid rotation of the local points. Or if it is capable, whether the learnt degree of equivariance on the rigid rotation is up to different tasks, i.e., the network can learn to ignore local rigid rotations if they are not useful to solve the task, and capture them if they are necessary.</p><p>PointCNN for shape analysis. We have demonstrated the feature learning effectiveness of PointCNN for classification and segmentation tasks. We suspect the PointCNN learnt features might outperform hand-crafted features in various shape analysis tasks, such as shape key point matching, registration, and retrieval, as is the case with CNN features for various image tasks. Note that in PointNet and PointNet++, the learnt features are features of regions, whereas in PointCNN, the learnt features are "projected", or "aggregated" at the specific representative points, thus PointCNN might outperform at tasks where the locations matter more.</p><p>Fully convolutional PointCNN. The way we process point cloud in different scales, such as S3DIS and ScanNet, is sub-optimal. Following the fully convolutional idea in processing images in different sizes, PointCNN should be applied in a fully convolutional way for extracting features from point cloud in different scales. The block slicing we currently used is a brutal approximation of the fully convolutional approach. We leave the extension of PointCNN into supporting fully convolutional X-Conv, which would require highly efficient point cloud indexing and memory management, as future work.</p><p>PointCNN or CNN? Since X-Conv is a generalization of Conv, ideally, PointCNN is supposed to perform as good as, if not better than, CNN, if the underlying data is the same but only represented differently. To verify this, we evaluate PointCNN on the point cloud representation of MNIST and CIFAR10. We show the our PointCNN classification results, in comparison with PointNet++, as well as image CNNs in <ref type="table">Table 5</ref>. For MNIST data, PointCNN achieved the best performance out of the comparison methods, which indicates that PointCNN performs quite well in learning the digits' shape information. For CIFAR10 data, where there is mostly no "shape" information, PointCNN has to learn mostly from the spatially-local correlation in the RGB features, and it performed reasonably well on this task, though there is a large gap between PointCNN and the mainstream image CNNs. Note that PointNet++ performs no better than random choice on CIFAR10. We suspect the reason is that, in PointNet++, the RGB features, after being process by the max-pooling, become in-discriminative. Together with the lack of "shape" information, PointNet++ failed completely on this task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>MNIST (%) CIFAR10 (%) LeNet <ref type="bibr" target="#b21">[LeCun et al. 1998]</ref> 99.20 84.07 Network in Network <ref type="bibr" target="#b23">[Lin et al. 2014]</ref> 99.53 91.20 PointNet++ <ref type="bibr" target="#b27">[Qi et al. 2017a]</ref> 99.49 10.0 PointCNN 99.54 76.69 <ref type="table">Table 5</ref>: Classification accuracies on MNIST and CIFAR10.</p><p>From the CIFAR10 experiment, we can conclude that CNNs are still the choice over PointCNN for general images for now, before the ideal PointCNN, if it exists, is developed. Note that this is not contradictory to the fact that PointCNN outperforms MVCNN <ref type="bibr" target="#b35">[Su et al. 2015]</ref> on ModelNet40. MVCNN project 3D meshes into multiview images, and then apply image CNNs, while PointCNN directly operates on the points sampled from the meshes, but not the image pixel points. It seems that the sparser the data is, the more prominent the advantage of PointCNN can be observed.</p><p>It is interesting to study the principle criteria for making the choice of CNN+dense representation vs. PointCNN+point cloud representation. Meanwhile, some seemly dense data might be represented sparsely. For example, videos are commonly represented as dense 3D volume, which might be an overkill since usually only a small portion of pixels are non-stationary in the frames. PointCNN+sparse, but irregular, represented videos seems to be an interesting direction.</p><p>The combination of PointCNN and CNNs. Due to the rapid advancement of 3D sensor, more and more data will be captured with point cloud and images hand in hand. In such case, processing point cloud and images with PointCNN and CNNs independently, and then merge them for the final inference might not be the optimal solution. It is interesting to study how to combine PointCNN and CNNs to jointly process paired point cloud and images, probably at early convolution stages.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">CONCLUSION</head><p>We proposed PointCNN, which is a generalization of CNN into leveraging spatially-local correlation from data represented as point cloud. We demonstrated its strong performance on multiple challenging benchmark datasets and tasks. The core of PointCNN is the X-Conv operator that weights and permutes input points and features before they are process by a typical convolution.</p><p>As point cloud data is becoming more accessible, we envision it is of great importance to develop methods that can effectively leverage spatially-local correlation from such data, and our method is just a starting point in the important undertaking. We open source our code at https://github.com/yangyanli/PointCNN for encouraging future developments. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3394-3404.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A APPENDIX A.1 PointCNN Model Zoo</head><p>In <ref type="figure" target="#fig_4">Figure 8</ref>, we list the PointCNNs used for classification and segmentation tasks on multiple benchmark datasets. PointCNNs are easy to implement, setup, and tune. Larger C are used for layers with more abstract/semantic information, such as the top layers in classification networks, and middle layers in "Conv-DeConv" segmentation networks. To relax the memory demand, smaller Ks are used at layers with large number of representative points, such as bottom layers of classification networks, and top and bottom layers of segmentation networks. Deeper PointCNN with larger receptive field in the last X-Conv layer are used for larger or harder datasets. The skip-links, together with the dilation parameter D, make it easy to fuse information from different scales (receptive fields), as illustrated in (d) and (e), which is essential for segmentation tasks.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Convolution input from regular grids (i) and point cloud (ii, iii, and iv). In regular grids, each grid cell is associated with a feature. In point cloud, the points are sampled from local neighborhoods, in analogy to local patches in regular grids, and each point is associated with a feature, an order index, as well as its coordinates. However, the lack of regular grids poses the challenge of sorting the points into canonical orders.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: PointCNN architecture for classification (a and b) and segmentation (c), where N and C denote the output representative point number and feature dimensionality, K is the neighboring point number for each representative point, and D is the X-Conv dilation rate.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: T-SNE visualization of features in PointCNN w/o X (a), and before (b) and after (c) X-transformation in PointCNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Distance visualization of features at different hierarchies.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: PointCNN model zoo, where (a) is used for ModelNet40 and ScanNet classification, (b) is used for TU-Berlin sketch classification, (c) is used for Quick Draw sketch classification, (d) is used for ScanNet and S3DIS segmentation, and (e) is used for ShapeNet Parts segmentation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>al.</figDesc><table>Method 

Input 
Core Operator 
ModelNet40 ScanNet 
MVCNN [Su et al. 2015] 
Images 
2D Conv 
90.1 
-
FPNN [Li et al. 2016] 
3D Dist. Field 
1D Conv 
87.5 
-
Vol. CNN [Qi et al. 2016] 
Voxels 
3D Conv 
89.9 
74.9 
O-CNN [Wang et al. 2017] 
Octree Voxels 
Sparse 3D Conv 
90.6 
-
PointNet [Qi et al. 2017a] 
Point Cloud 
Pointwise MLP 
89.2 
-
PointNet++ [Qi et al. 2017b] Point Cloud Multiscale Pointwise MLP 
90.7 
76.1 
PointCNN 
Point Cloud 
X-Conv 
91.7 
77.9 
Table 1: Comparisons of classification accuracy (%) on ModelNet40 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Ablation test of PointCNN variants on ModelNet40 classification. X-Conv is the key to PointCNN performance.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Actually, this is a special instance of convolution -a convolution that is applied at one spatial location. For simplicity, we call it convolution as well.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Averaged from 0.12 and 0.06 second per batch with batch size 200.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ACKNOWLEDGMENTS</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Vincent Vanhoucke, Vijay Vasudevan, Fernanda Viégas, Oriol Vinyals, Pete Warden</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sanjay</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rafal</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manjunath</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josh</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Mané</surname></persName>
		</author>
		<ptr target="https://www.tensorflow.org/Softwareavailablefromtensorflow.org" />
	</analytic>
	<monogr>
		<title level="m">TensorFlow: Large-Scale Machine Learning on Heterogeneous Systems</title>
		<meeting><address><addrLine>Rajat Monga, Sherry Moore, Derek Murray, Chris Olah, Mike Schuster, Jonathon Shlens, Benoit Steiner, Ilya Sutskever, Kunal Talwar, Paul Tucker; Martin Wattenberg, Martin Wicke, Yuan Yu, and Xiaoqiang Zheng</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Ioannis Brilakis, Martin Fischer, and Silvio Savarese. 2016. 3d semantic parsing of large-scale indoor spaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ozan</forename><surname>Sener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Helen</forename><surname>Amir R Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jiang</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="1534" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Geometric deep learning: going beyond euclidean data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Michael M Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Magazine</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="18" to="42" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">François</forename><surname>Chollet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02357</idno>
		<title level="m">Xception: Deep Learning with Depthwise Separable Convolutions</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Djork-Arné</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">DeepPermNet: Visual Permutation Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rodrigo</forename><forename type="middle">Santa</forename><surname>Cruz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anoop</forename><surname>Cherian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">ScanNet: Richly-annotated 3D Reconstructions of Indoor Scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angel</forename><forename type="middle">X</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maciej</forename><surname>Halber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Nießner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Exploiting Cyclic Symmetry in Convolutional Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>De Fauw</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=3045390.3045590" />
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 33rd International Conference on International Conference on Machine Learning</title>
		<meeting>the 33rd International Conference on International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="page" from="1889" to="1898" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">How Do Humans Sketch Objects?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mathias</forename><surname>Eitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1" to="44" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep Sparse Rectifier Neural Networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xavier</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antoine</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (Proceedings of Machine Learning Research)</title>
		<editor>, Geoffrey Gordon, David Dunson, and Miroslav DudÃŋk</editor>
		<meeting><address><addrLine>Fort Lauderdale, FL, USA</addrLine></address></meeting>
		<imprint>
			<publisher>PMLR</publisher>
			<date type="published" when="2011" />
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="page" from="315" to="323" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.10275</idno>
		<title level="m">3D Semantic Segmentation with Submanifold Sparse Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurens</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01307</idno>
		<title level="m">Submanifold Sparse Convolutional Networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Douglas</forename><surname>Eck</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03477</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">A Neural Representation of Sketch Drawings. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Transforming autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Geoffrey E Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sida D</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A Method for Stochastic Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Diederik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Escape from Cells: Deep Kd-Networks for The Recognition of 3D Point Cloud Models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Large-scale Point Cloud Semantic Segmentation with Superpoint Graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Loïc</forename><surname>Landrieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Simonovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.09869</idno>
		<ptr target="http://arxiv.org/abs/1711.09869" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="page" from="436" to="444" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proc. IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">FPNN: Field probing neural networks for 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangyan</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sören</forename><surname>Pirk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="307" to="315" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Convolutional Neural Networks on Surfaces via Seamless Toric Covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Meirav</forename><surname>Haggai Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noam</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Miri</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ersin</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaron</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lipman</surname></persName>
		</author>
		<idno type="doi">10.1145/3072959.3073616</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073616" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Geometric deep learning on graphs and manifolds using mixture model CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Federico</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Davide</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Masci</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning Deconvolution Network for Semantic Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hyeonwoo</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seunghoon</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bohyung</forename><surname>Han</surname></persName>
		</author>
		<idno type="doi">10.1109/ICCV.2015.178</idno>
		<ptr target="https://doi.org/10.1109/ICCV.2015.178" />
	</analytic>
	<monogr>
		<title level="m">ICCV (ICCV &apos;15)</title>
		<meeting><address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1520" to="1528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">PointNet: Deep Learning on Point Sets for 3D Classification and Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaichun</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2017.16</idno>
		<idno>CVPR. 77-85</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.16" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Volumetric and multi-view CNNs for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Angela</forename><surname>Nießner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5648" to="5656" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">PointNet++: Deep Hierarchical Feature Learning on Point Sets in a Metric Space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Charles R Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><forename type="middle">J</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guibas</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5105" to="5114" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04500</idno>
		<title level="m">Deep learning with sets and point clouds</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Octnet: Learning deep 3d representations at high resolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gernot</forename><surname>Riegler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Osman Ulusoys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">U-Net: Convolutional Networks for Biomedical Image Segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Olaf</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Brox</surname></persName>
		</author>
		<editor>MICCAI, Nassir Navab, Joachim Hornegger, William M. Wells, and Alejandro F. Frangi</editor>
		<imprint>
			<date type="published" when="2015" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="234" to="241" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
		<ptr target="http://dl.acm.org/citation.cfm?id=104279.104293" />
		<title level="m">Parallel Distributed Processing: Explorations in the Microstructure of Cognition</title>
		<meeting><address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1986" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="318" to="362" />
		</imprint>
	</monogr>
	<note>Chapter Learning Internal Representations by Error Propagation</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dynamic routing between capsules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sara</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicholas</forename><surname>Frosst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3859" to="3869" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
	<note>Evangelos Kalogerakis, and Erik Learned-Miller</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">SEGCloud: Semantic Segmentation of 3D Point Clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Tchapmi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iro</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Armeni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">O-CNN: Octree-based Convolutional Neural Networks for 3D Shape Analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng-Shuai</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu-Xiao</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chun-Yu</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xin</forename><surname>Tong</surname></persName>
		</author>
		<idno type="doi">10.1145/3072959.3073608</idno>
		<ptr target="https://doi.org/10.1145/3072959.3073608" />
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
	<note>Article</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Deep Points Consolidation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shihao</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hui</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Minglun</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthias</forename><surname>Zwicker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Cohen-Or</surname></persName>
		</author>
		<idno type="doi">10.1145/2816795.2818073</idno>
		<ptr target="https://doi.org/10.1145/2816795.2818073" />
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page">176</biblScope>
			<date type="published" when="2015-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhirong</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuran</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Linguang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoou</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianxiong</forename><surname>Xiao</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A Scalable Active Framework for Region Annotation in 3D Shape Collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladimir</forename><forename type="middle">G</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Duygu</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I-Chao</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyan</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixing</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alla</forename><surname>Sheffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="doi">10.1145/2980179.2980238</idno>
		<ptr target="https://doi.org/10.1145/2980179.2980238" />
	</analytic>
	<monogr>
		<title level="j">ToG</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page">210</biblScope>
			<date type="published" when="2016-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">SyncSpecCNN: Synchronized spectral CNN for 3d shape segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xingwen</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leonidas</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="doi">10.1109/CVPR.2017.697</idno>
		<idno>CVPR. 6584-6592</idno>
		<ptr target="https://doi.org/10.1109/CVPR.2017.697" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manolis</forename><surname>Savva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haibin</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Benjamin</forename><surname>Graham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roman</forename><surname>Klokov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.06104</idno>
		<title level="m">Large-Scale 3D Shape Reconstruction and Segmentation from ShapeNet Core55</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qian</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yongxin</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Feng</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi-Zhe</forename><surname>Song</surname></persName>
		</author>
		<idno type="doi">10.1007/s11263-016-0932-3</idno>
		<ptr target="https://doi.org/10.1007/s11263-016-0932-3" />
		<title level="m">Tao Xiang, and Timothy M. Hospedales. 2017. Sketch-a-Net: A Deep Neural Network That Beats Humans. IJCV</title>
		<imprint>
			<date type="published" when="2017-05" />
			<biblScope unit="volume">122</biblScope>
			<biblScope unit="page" from="411" to="425" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manzil</forename><surname>Zaheer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Satwik</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siamak</forename><surname>Ravanbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barnabas</forename><surname>Poczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">J</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Smola</surname></persName>
		</author>
		<title level="m">Deep Sets</title>
		<editor>NIPS, I. Guyon, U. V</editor>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wallach</surname></persName>
		</author>
		<editor>Fergus, S. Vishwanathan, and R. Garnett</editor>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
