<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Reasoning about Fine-grained Attribute Phrases using Reference Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jong-Chyi</forename><surname>Su</surname></persName>
							<email>jcsu@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chenyun</forename><surname>Wu</surname></persName>
							<email>chenyun@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huaizu</forename><surname>Jiang</surname></persName>
							<email>hzjiang@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Subhransu</forename><surname>Maji</surname></persName>
							<email>smaji@cs.umass.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Massachusetts</orgName>
								<address>
									<settlement>Amherst</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Reasoning about Fine-grained Attribute Phrases using Reference Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We present a framework for learning to describe finegrained visual differences between instances using attribute phrases. </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Attribute-based representations have been used for describing instances within a basic-level category as they often share a set of high-level properties. These attributes serve as basis for human-centric tasks such as retrieval and categorization <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b35">36]</ref>, and for generalization to new categories based on a description of their attributes <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b24">25]</ref>. However, most prior work has relied on a fixed set of attributes designed by experts. This limits their scalability to new domains since collecting expert annotations are expensive, and results in models that are less robust to noisy open-ended descriptions provided by a non-expert user. * Authors contributed equally  <ref type="figure" target="#fig_0">Figure 1</ref>. Left: Each annotation in our dataset consists of five pairs of attribute phrases. Right: A reference game played between a speaker who describes an attribute of an image within a pair and a listener whose goal is to pick the right one.</p><p>Instead of discrete attributes this work investigates the use of attribute phrases for describing instances. Attribute phrases are short sentences that describe a unique semantic visual property of an object (e.g., "red and white color", "wing near the top"). Like captions, they can describe properties in a compositional manner, but are typically shorter and only capture a single aspect. Like attributes, they are modular, and can be combined in different ways to describe instances within a category. Their compositionality allows the expression of large number of properties in a compact manner. For example, colors of objects, or their parts, can be expressed by combining color terms (e.g., "red and white", "green and blue", etc.). A collection of these phrases constitutes the semantic space of describable attributes and can be used as a basis for communication between a human and computer for various tasks.</p><p>We begin by collecting a dataset of attribute phrases by asking annotators to describe five visual differences between random pairs of airplanes from the OID airplane dataset <ref type="bibr" target="#b43">[44]</ref>. Each difference is of the form "P 1 vs. P 2 " with phrases P 1 and P 2 corresponding to the properties of the left and right image respectively ( <ref type="figure" target="#fig_0">Figure 1</ref>). By collecting multiple properties at a time we obtain a diverse set of describable attributes. Moreover, phrases collected in a contrastive manner reveal attributes that are better suited for fine-grained discrimination. The two phrases in a comparison describe the same underlying attribute (e.g., round nose and pointy nose both describe the shape), and reflect an axis of comparison in the underlying semantic space. We then analyze the ability of automatic methods to generate these attribute phrases using the collected dataset. In particular we learn to generate descriptions and ground them in images in the context of a reference game (RG) between a speaker S and a listener L <ref type="figure" target="#fig_0">(Figure 1</ref>). S is provided with a pair of images {I 1 ,I 2 } and produces a visual difference of the form P 1 (or "P 1 vs. P 2 "). L's goal is to identify which of the two images corresponds to P 1 . Reference games have been widely used to collect datasets describing objects within a scene. This work employs the framework to generate and reason about compositional language-based attributes for fine-grained visual categorization.</p><p>Our experiments show that a speaker trained to describe visual differences displays remarkable pragmatic behavior allowing a neural listener to rank the correct image with 91.4% top-5 accuracy in the RG compared with 80.6% of a speaker trained to generate captions non-contrastively. We also investigate a family of pragmatic speakers who generate descriptions by jointly reasoning about the listener's ability to interpret them, based on the work of Andreas and Klein <ref type="bibr" target="#b2">[3]</ref>. Contrastively trained pragmatic speakers offer significant benefits (on average 7% higher top-5 accuracy in RG across listeners) over simple pragmatic speakers. The resulting speakers can be used to generate attribute-based explanations for differences between two categories. Moreover, given a set of attribute phrases, the score of an image with respect to each phrase according to a listener provides a natural embedding of the image into the space of semantic attributes. On the task of image classification on the FGVC aircraft dataset <ref type="bibr" target="#b29">[30]</ref> this representation outperforms existing attribute-based representations by 20% accuracy.</p><p>In summary, we show that reasoning about attribute phrases via reference games is a practical way of discovering and grounding describable attributes for fine-grained categories. We validate our approach on a dataset of 6,286 images with 9,400 pairs for a total of 47,000 phrases (Section 3). We systematically evaluate various speakers and listeners using the RG and human studies (Section 4.1-4.2), investigate the effectiveness of attribute phrases on various recognition tasks (Section 4.3-4.6), and conclude in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Attribute-based representations. Attributes have been widely used in the computer vision as an intermediate, interpretable representation for high-level recognition. They often represent properties that can be shared across categories, e.g., both a car and bicycle have wheels, or within a subordinate category, e.g., birds can be described by the shape of their beak. Due to their semantic nature they have been used for learning interpretable classifiers <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>, attribute-based retrieval systems <ref type="bibr" target="#b5">[6]</ref>, as high-level priors for unseen categories for zero-shot learning <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b17">18]</ref>, and as a means for communication in an interactive recognition system <ref type="bibr" target="#b22">[23]</ref>.</p><p>A different line of work has explored the question of discovering task-specific attributes. Berg et al. <ref type="bibr" target="#b4">[ 5]</ref> discover attributes by mining frequent n-grams in captions. Parikh and Grauman <ref type="bibr" target="#b34">[35]</ref> ask annotators to name directions that maximally separate the data according to some underlying features. Other approaches <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b1">2]</ref> have mined phrases from online text repositories to discover commonsense knowledge about properties of categories (e.g., cars have doors). For a detailed description of the above methods see this recent survey <ref type="bibr" target="#b28">[29]</ref>.</p><p>The interface for collecting attribute phrases is based on our earlier work <ref type="bibr" target="#b27">[28]</ref>, which showed that annotations collected in a pairwise manner could be analyzed to discover a lexicon of parts and attributes. This work extends the prior work in a several ways. We (a) consider the complete problem of generating and interpreting attribute phrases on a significantly larger dataset, (b) systematically evaluate speaker and listener models on the data, and (c) show their utility in various recognition tasks.</p><p>Referring expression comprehension and generation. Modern captioning systems <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b45">46]</ref> produce descriptions by using encoder-decoder architectures, typically consisting of a convolutional network for encoding an image and a recurrent network for decoding a sentence. A criticism of these tasks is that captions in existing datasets (e.g., MS COCO dataset <ref type="bibr" target="#b25">[26]</ref>) can be generated by identifying the dominant categories and relying on a language model. State-of-the-art systems are often matched by simple nearest-neighbor retrieval approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b49">50]</ref>. Visual question-answering systems <ref type="bibr" target="#b3">[4]</ref> face a similar issue that most questions can be answered by relying on commonsense knowledge (e.g., the sky is often blue). Some recent attempts have been made to address these issues <ref type="bibr" target="#b18">[19]</ref>.</p><p>Tasks where annotators are asked to describe an object in an image such that another can correctly identify it provides a way to collect context-sensitive captions <ref type="bibr" target="#b19">[20]</ref>. These tasks have been widely studied in the linguistics community in an area called pragmatics (see Grice's maxims <ref type="bibr" target="#b12">[13]</ref>). Much prior work in computer vision has focused on generating referring expressions to distinguish an object within an image <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b47">48,</ref><ref type="bibr" target="#b48">49,</ref><ref type="bibr" target="#b26">27]</ref>. More recently, referring expression generation have been extended to interactive dialogue systems <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8]</ref>. In contrast, our work aims to collect and generate referring expressions for fine-grained discrimination between instances.</p><p>For the task of fine-grained recognition, the work of Reed et al. <ref type="bibr" target="#b36">[37]</ref> is the most related to ours. They ask annotators on Amazon Mechanical Turk to describe properties of birds and flowers, and use the data to train models of images and text. They show the utility of such models for zeroshot recognition where a description of a novel category is provided as supervision, and for text-based image retrieval. Another recent work <ref type="bibr" target="#b44">[45]</ref> showed that referring expressions for images within a set can be generated simply by enforcing separation of image probabilities during decoding using beam search. However, their model was trained on context agnostic captions. Our work takes a different approach. First, we collect attribute phrases in a contrastive manner that encourages pragmatic behavior among annotators. Second, we ask annotators to provide multiple attribute descriptions, which as we described earlier, allows modular reuse across instances, and serves as an intermediate representation for various tasks. Attribute phrases capture the spectrum between basic attributes and detailed captions. Like "visual phrases" <ref type="bibr" target="#b38">[39]</ref> they capture frequently occurring relations between basic attributes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The framework used to collect our dataset is described in Section 3.1. Various speaker and listener models are described in Section 3.2 and Section 3.3 respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A dataset of attribute phrases</head><p>We rely on human annotators to discover the space of descriptive attributes. Our annotations are collected on images from the OID aircraft dataset <ref type="bibr" target="#b43">[44]</ref>. The annotations are organized into 4700 image pairs (1851 images) in training set, 2350 pairs (1730 images) in validation set, and 2350 pairs (2705 images) in test set. Each pair is chosen by picking two different images uniformly at random within the provided split in the OID aircraft dataset.</p><p>Annotators from Amazon Mechanical Turk are asked to describe five properties in the form "P 1 vs. P 2 ", each corresponding to a different aspect of the objects in the left and the right image respectively. We also provide some examples as guidance to the annotators. The interface shown in <ref type="figure">Figure 2</ref> is lightweight and allows rapid deployment compared to existing approaches for collecting attribute annotations where an expert decides the set and semantics of attributes ahead of time. However, the resulting annotations are noisier and reflect the diversity of open-ended languagebased descriptions. A second pass over the data is done to check for consistency, after which about 15% of the description pairs were discarded.   <ref type="figure">Figure 2</ref>. The annotation interface used to collect five different attribute phrase pairs adapted from <ref type="bibr" target="#b27">[28]</ref>. Amazon mechanical turkers were paid $0.12 for annotating three pairs.</p><p>are qualitatively different from image captions which are typically longer and more grammatical. However, each annotation provides five different attribute pairs.</p><p>The OID dataset also comes with a set of expert-designed attributes. A comparison with OID attributes shows that attribute phrases capture novel properties that describe the relative arrangement of parts (e.g., "door above the wing", "wing on top"), color combinations, relative sizes, shape, and number of parts (e.g., "big nose", "more windows", etc.) Section 4.4 shows a visualization of the space of attribute phrases. Section 4.3 provides a direct comparison of OID attributes and those derived from our data on the task of FGVC-aircraft variant classification <ref type="bibr" target="#b29">[30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Speaker models</head><p>A speaker maps visual inputs to attribute phrases. We consider two speakers; a simple speaker (SS) that takes a single image as input and produces a description, and a discerning speaker (DS) that takes two images as input and produces a single (or a pair of) description(s).</p><p>Both our speaker models are based on the show-and-tell model <ref type="bibr" target="#b45">[46]</ref> developed for image captioning. Images are encoded using a convolutional network and decoded into a sentence using a recurrent network over words. We use one-hot encoding for 730 words with frequency greater than 5 in the training set. We consider fc7 layer outputs of the VGG-16 network <ref type="bibr" target="#b40">[41]</ref> plus two fully-connected layers with ReLU units <ref type="bibr" target="#b33">[34]</ref> on top as the image feature, and a LSTM model <ref type="bibr" target="#b13">[14]</ref> with 2048 hidden units to generate the sentences. The image feature is fed into the LSTM not only as the initial input, but also in each state input together with word embeddings. This led to an improved speaker in our experiments. For the discerning speaker, we concatenate two image features as input to the LSTM. At test time we apply beam search with beam size 10 and get 10 output descriptions from each image (pair). Although the discerning speaker is trained to generate phrase pairs, we can simply take the first (or second) half of the pair and evaluate it in the same way as a simple speaker.</p><p>We also consider a pragmatic speaker that generates contrastive captions by reasoning about the listener's ability to pick the correct image based on the description. Andreas and Klein <ref type="bibr" target="#b2">[3]</ref> proposed a simple strategy to do so by reranking descriptions of an image based on a weighted combination of (a) fluency -the score assigned by the speaker, and (b) accuracy -the score assigned by the listener on the referred image. Various pragmatic speakers are possible based on the choice of speakers and listeners. The details are described in Section 4.2.</p><p>Optimization details: Our implementation is based on Tensorflow <ref type="bibr" target="#b0">[1]</ref>. The descriptions are truncated at length 14 when training the LSTM. The VGG-16 network is initialized with weights pre-trained on ImageNet dataset <ref type="bibr" target="#b23">[24]</ref>. We first fix the VGG-16 weights and train the rest of the network, using Adam optimizer <ref type="bibr" target="#b20">[21]</ref> with initial learning rate 0.001, β 1 =0 .7, β 2 =0 .999 and ✏ =1 .0 × 10 −8 .W e have batch normalization <ref type="bibr" target="#b15">[16]</ref> in fully connected layers after VGG-16, and drop out with rate 0.7 in LSTM. We use batch size 64 for 40000 steps (∼28 epochs). Second, we fine tune the whole network with initial learning rate modified to 5 × 10 −6 , batch size 32 for another 40000 steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Listener models</head><p>A listener interprets a single (or a pair of) attribute phrase(s), and picks an image within a pair by measuring the similarity between the phrase(s) and images in a common embedded space. Once again we consider two listeners: a simple listener (SL) that interprets a single phrase, and a discerning listener (DL) that interprets a phrase pair.</p><p>The simple listener models the score of the image I 1 within a pair (I 1 ,I 2 ) for a phrase P as:</p><formula xml:id="formula_0">p(I 1 |P )=σ(φ(I 1 ) T ✓(P),φ(I 2 ) T ✓(P)).</formula><p>Here φ and ✓ are embeddings of the image and the phrase respectively, and σ is the softmax function σ(x, y)= exp(x)/(exp(x)+exp(y)). Similarly, a discerning listener models the score of an image by comparing it with an embedding of the phrase pair ✓([P 1 vs. P 2 ]). A simple way to construct a discerning listener from a simple listener is by averaging the predictions from the left and right phrases, i.e., p(I|[P 1 vs.</p><formula xml:id="formula_1">P 2 ]) = (p(I|P 1 )+p(I|P 2 )) /2.</formula><p>We follow the setup of the speaker to embed phrases and use the final state of a LSTM with 1024 hidden nodes as the phrase embedding. The vocabulary of words is kept identical. For image features, once again we use the fc7 layer of the VGG-16 network and add a fully-connected layer with 1024 units and ReLU activation. The parameters are learned by minimizing the cross-entropy loss.</p><p>We also evaluate two variants of the simple listener, SL r and SL, based on whether it is trained on non-contrastive data (I 1 ,I 2 ,P 1 ) where I 2 is a random image within the training set, or the contrastive data where I 2 is the other image in the annotation pair.</p><p>Optimization details: We first fix the VGG-16 network and use Adam optimizer with initial learning rate =0.001, β 1 = 0.7, batch size = 32 for 2000 steps (4000 steps for SL r model), then fine-tune the entire model with a learning rate 1 × 10 −5 for another 7000-10000 steps.</p><p>Human listener. We also consider human annotators to perform the task of the listener in the RG. For each generated phrase that describes one image out of an image pair, we let three users to pick which image out of the pair the phrase is referring to. However, unlike (most) human speakers, neural speakers can produce irrelevant descriptions. Thus, in addition to the choice of left and right image, users have the option to say "not sure" when the description is ambiguous. If two or more users out of three picked the same image, we say the human listener is certain about the choice, otherwise we say the human listener is uncertain. The interface is shown in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Results</head><p>We evaluate various listeners and speakers on the dataset we collected in terms of their accuracy in the RG in Section 4.1 and Section 4.2 respectively. We then evaluate their effectiveness on a fine-grained classification task in Section 4.3, visualize the space of attribute phrases discovered from the data in Section 4.4, for text-based image retrieval in Section 4.5, and for generating visual explanations for differences between categories in Section 4.6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Evaluating listeners</head><p>We first evaluate various listeners on human-generated phrases. For simple listeners, each annotation provides ten different reference tasks (I 1 ,I 2 , P) →{0,1} corresponding to five different left and right attribute phrases. Each task is evaluated independently and accuracy is measured as the fraction of correct references made by the listener. Similarly, discerning listeners are evaluated by replacing P with "P 1 vs. P 2 " or "P 2 vs. P 1 ".</p><p>Accuracy using human speakers. The results are shown in <ref type="table">Table 1</ref>. Training on contrastive data improves the accuracy of the simple listener slightly from 84.2% (SL r ) to 86.3% (SL) on the test set. Discerning listeners see both phrases at once and naturally perform better. There is almost no difference between a discerning listener that combines two simple listeners by averaging their predictions (2×SL), and one that interprets the two phrases at once (DL). The results indicate that on our dataset the listener's task is relatively easy and contrastive data does not provide any significant benefits. As a reference the accuracy of a human listener is close to 100% on human-generated phrases. Are the top attributes more salient? As annotators are asked to describe five different attributes they might pick ones that are more salient first. We evaluate this hypothesis by measuring the accuracy of the listener (SL) on phrases as a function of the position of the annotation in the interface ranging from one for the top attribute to five for the last one.</p><p>The results are shown in <ref type="table">Table 2</ref>. The accuracy decreases monotonically from one to five suggesting that the first attribute phrase is easier for the listener to discriminate. We are uncertain if this is because the attributes near the top are more discriminative, or because the listener is better at interpreting these as they are likely to be more frequent in the training data. Nevertheless, attribute saliency is a signal we did not model explicitly and may be used to train better speakers and listeners (e.g., see Turakhia and Parikh <ref type="bibr" target="#b41">[42]</ref>  <ref type="table">Table 2</ref>. Accuracy (%) of the simple listener (SL) on RG using human-generated attribute phrases at positions one through five across the validation and test set. The accuracy decreases monotonically from one to five suggesting that the top attribute phrases are easier to discriminate.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluating speakers</head><p>We use simple listeners, SL and SL r , and the human listener to evaluate speakers. As described in Section 3.2 we use beam search to generate 10 descriptions for each image pair and evaluate them individually using various listeners. The discerning speaker generates phrase pairs but we simply take the first and second half separated by "vs.", a special word in the vocabulary, and evaluate it using a simple listener (that sees only one phrase). If the word "vs." is missing in the generated output we simply consider the entire sentence as the P 1 . Only 1 out of 23500 phrase pairs did not contain the "vs." token.</p><p>For evaluation with humans we collect three independent annotations on a subset of 100 image pairs (with 10 descriptions each) out of the full test set. The listeners are considered to be correct when the probability of the correct image is greater than half. For human listener, we report the accuracy of when there is a majority agreement on the correct image, i.e., when two or more users picked the correct image. For direct comparison with the simple speaker models, we also report the human listener accuracy when they are allowed to guess. This is the sum of earlier accuracy, and half of the cases when there is no majority agreement. Human annotators are uncertain when the generated descriptions are not fluent or when they are not discriminative. Therefore, a better human accuracy reflects speaker quality both in terms of fluency and discriminativeness. Some examples of the generated attribute phrases using various speakers are shown in <ref type="figure" target="#fig_1">Figure 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy of various speakers and listeners.</head><p>Results on the full test set (Test) and the human-evaluated subset (Test*) are shown in <ref type="table" target="#tab_5">Table 3</ref>. The accuracy of discerning speaker exceeds that of simple speaker by more than 10% no matter which listener to use. This result suggests that data collected contrastively using our annotation task allows direct training of speaker models that show remarkable context-sensitive behavior. Somewhat surprisingly we also see that the simple listeners are more accurate than the human listener when evaluated on descriptions generated by our speaker models. This is because humans tend to be more cautious in the reference game. For example, simple listeners will accept yellowish grass being referred to as "concrete" compared to green grass, but humans tend to view it as an unclear reference.  Does pragmatics help? Given that our discerning speaker can generate highly accurate contrastive descriptions, we investigate if additional benefits can be achieved if the speaker jointly reasons about the listener's ability to interpret the descriptions. We employ the pragmatic speaker model of Andreas and Klein <ref type="bibr" target="#b2">[3]</ref> where a simple speaker generates descriptions that are reranked by a simple listener using a weighted combination of speaker and listener scores. In particular, we rerank the output 10 sentences from speakers by the probabilities from simple listeners. We combine the listener probability p l and speaker beamsearch probability p s as p = p , and pick the optimal λ on a validation set annotated by a human listener. We found that the optimal λ is close to 0, so we decided to use p l only for reranking on test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Accuracy (%)</head><note type="other">SL</note><p>In <ref type="table">Table 4</ref>, we report the accuracy of top k sentences (k = 1, 5, 7) of the human listener and the results after reranking on the Test* set. When using the listener score from SL r the average accuracy of the top five generated descriptions after reranking improves dramatically from 64.2% to 82.6% for the simple speaker. The accuracy of the discerning speaker also improves to 90%. This suggests that better pragmatics can be achieved if both the speaker and listener are trained in a contrastive manner. Surprisingly the contrastively-trained simple listener SL is less effective at reranking than SL r . We believe this is because the SL overfits on the human speaker descriptions and is less effective when used with neural speakers. <ref type="figure" target="#fig_3">Figure 4</ref> shows an example pair and the output of different speakers. Simple speaker suffers from generating descriptions that are true to the target image, but fail to differentiate two images. Discerning speaker can mostly avoid this mistake. Reranking by listeners can move better sentences to the top and improves the quality of top sentences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Human listener accuracy (%)</head><p>Reranker  <ref type="table">Table 4</ref>. Accuracy of pragmatic speakers with human listeners on the Test* set. After generating the descriptions by the speaker model (either SS or DS), we use the listener model (SLr or SL) to rerank them. We report the accuracy based on human listener from the user study. We report both the accuracy when there is majority agreement, and accuracy with guessing (in brackets). Pragmatic speakers are strictly better than non-pragmatic ones.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Fine-grained classification with attributes</head><p>We compare the effectiveness of attribute phrases to existing attributes in the OID dataset on the task of fine-grained classification on the FGVC aircraft dataset <ref type="bibr" target="#b29">[30]</ref>. The OID dataset is designed with attributes in mind and has long-tail distribution over aircraft variants with 2728 models, while the FGVC dataset is designed for fine-grained classification task with 100 variants each with 100 images. Both datasets are based on the images from the airliners.net website and have a few overlapping images. We exclude the 169 images from the FGVC test set that appear in the OID training+validation set in our evaluation.</p><p>There are 49 attributes in the OID dataset organized into 14 categories. We exclude three attributes -two referring to the airline label and model, most of which have only one training examples per category, and another that is rare. We then trained linear classifiers to predict each attribute using the fc7 layer feature of the VGG-16 network. Using the same features and trained classifiers, we construct a 46 dimensional embedding of the FGVC images into the space of OID attributes. The attribute classifiers based on the VGG-16 network features are fairly accurate (66% mean AP across attributes) and outperforms the Fisher vector baseline included in the OID dataset paper.</p><p>For the attribute phrase embeddings, we first obtain the K most frequent ones in our training set. Given an image I, we compute the score φ(I) T ✓(P) for each phrase P from a listener as the embedding. For a fair comparison the image features are kept identical to the OID attribute classifiers. We also explore an opponent attribute space, where instead of top phrases we consider the top phrase pairs. Phrase pairs represent an axis of comparison, e.g., "small vs. medium", or "red and blue vs. red and white", and are better suited for describing relative attributes. We use the discerning listener for the embedding on the opponent attribute space.    <ref type="figure" target="#fig_4">Figure 5</ref> shows a comparison of OID attributes and attribute phrases for various listeners and number of attributes. For the same number of attributes as the OID dataset, attribute phrases are 12% better. With 300 attributes the accuracy improves to 32%, about 20% better than OID. These results indicate that attribute phrases provide a better coverage of the space of discriminative directions. The two simple listeners perform equally well and the opponent attribute space does not offer any additional benefits.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>SS:</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visualizing the space of descriptive attributes</head><p>We visualize the space of the 500 most frequent phrases in the training set using the embedding of the simple listener model projected from 1024 dimensions to 2 using t-SNE <ref type="bibr" target="#b42">[43]</ref> in <ref type="figure">Figure 6</ref>. Various semantically related phrases are clustered into groups. The cluster on the top right reflects color combinations; Phrases such as "less windows" and "small plane" are nearby (bottom right). Visualizations of the learned embeddings of images φ(I) and opponent attribute phrases ✓([P 1 vs. P 2 ]) are provided in the supplementary material.  <ref type="figure">Figure 6</ref>. Visualization of the 500 most frequent descriptions. Each attribute is embedded into a 1024 dimensional space using the simple listener SL and projected into two dimensions using t-SNE <ref type="bibr" target="#b42">[43]</ref>. (Best viewed digitally with zoom.)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Image retrieval with descriptive attributes</head><p>The listeners also allows us to retrieve an image given one or more attribute phrases. Given a phrase P we rank the images in the test set by the listener scores φ(I) T ✓(P). <ref type="figure" target="#fig_6">Figure 7</ref> shows some query phrases and the 18 most similar images retrieved from the test set. These results were obtained by simply concatenating all the query phrases to obtain a single phrase. More sophisticated schemes for combining scores from individual phrase predictions are likely to improve results <ref type="bibr" target="#b39">[40]</ref>. Our model can retrieve images with multiple attribute phrases well even though the composition of phrases does not appear in the training set. For example, "red and blue" only shows five times in total of 47, 000 phrases in the training set, "pointy nose" and "on the runway" are never seen in a single phrase together.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Generating attribute explanations</head><p>The pairwise reasoning of a speaker can be extended to analyze an instance within a set by aggregating speaker utterances across all pairs that include the target. Similarly one can describe differences between two sets by considering all pairs of instances across the two sets. We use this to generate attribute-based explanations for visual differences between two categories. We select two categories A, B from FGVC aircraft dataset and randomly choose ten images from each category. For each image pair (I 1 ∈ A, I 2 ∈ B), we generate ten phrase pairs using our discerning speaker. We then sort unique phrases primarily by their image frequency (number of images from target category described by the given description minus that from the opposite category), and when tied secondarily by their phrase frequency (number of occurrences of the phrase in target category minus that in the opposite category.) The top ten attribute phrases for the two categories for an example pair of categories are shown in <ref type="figure" target="#fig_7">Figure 8</ref>. The algorithm reveals several discriminative attributes between two such as "engine under wings" for 747-400, and "stabilizer on top of tail" for ATR-42.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We analyzed attribute phrases that emerge when annotators describe visual differences between instances within a subordinate category (airplanes), and showed that speakers and listeners trained on this data can be used for various human-centric tasks such as text-based retrieval and attribute-based explanations of visual differences between unseen categories. Our experiments indicate that pragmatic speakers that combine listeners and speakers are effective on the reference game <ref type="bibr" target="#b2">[3]</ref>, and speakers trained on contrastive data offers significant additional benefits. We also showed that attribute phrases are modular and can be used to embed images into an interpretable semantic space. The resulting attribute phrases are highly discriminative and outperform existing attributes on FGVC aircraft dataset on the fine-grained classification task.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1 shows an example of our dataset (more examples are in the supplementary material). Annotations describe the shapes of parts (nose, wings and tail), relative sizes, orientation, colors, types of engines, etc. Most descriptions are short with an average length of 2.4 words on each side, although about 4.3% of them have more than 4 words. These</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>GroundFigure 3 .</head><label>3</label><figDesc>Figure 3. Example output of simple speaker SS and discerning speaker DS. Simple speaker takes the left image in the green box as input, while the discerning speaker takes both images as input. In brackets are the probabilities according to the speaker.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. An example output of various speakers. Given the image pair, we use SS and DS to generate descriptions of the top left image. Outputs from SS and DS are listed in the order of probabilities from speaker beam search. Outputs of SS+SLr and DS+SLr are reranked by SLr. Green checks mean human listener picks correct image with certain, while question marks mean human listener is uncertain which image is referred to. The results indicate that DS is better than SS, and reranking using listeners improves the quality of top sentences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Classification accuracy on FGVC aircraft dataset using the 46 dimensional OID attributes and varying number of attribute phrases. See Section 4.3 for details.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Top 18 images ranked by the listener for various attribute phrases as queries (shown on top). We rank the images by the scores from the simple listener on the concatenation of the attribute phrases. The images are ordered from top to bottom, left to right.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Top 10 discriminative attribute phrases for pairs of categories from FGVC aircraft dataset. Descriptions are generated by the discerning speaker for each pair of images in the first and second category. The phrases sorted by the occurrence frequency provides an attribute-based explanation of the visual difference between two categories.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Instructions :</head><label>Instructions</label><figDesc>Annotate each one of the three tasks Press Next to move to the next pair and Submit once done. If the images do not display, your browser may not support this interface. Try the latest Chrome, Safari or Firefox browsers.</figDesc><table>Click here to see example answers before you start. 

VS 

List 5 differences between the two images 

1. 
VS 

2. 
VS 

3. 
VS 

4. 
VS 

5. 
VS 

pair 1 of 3 

Previous Next 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>).</figDesc><table>12345 
Val 91.3 86.6 84.1 82.5 82.3 
Test 92.3 87.4 85.9 84.0 81.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Test Test *</figDesc><table>r 
SL 
Human 
Top Test 

 *  

Test 
Test 

 *  

SS 

1 
84.0 79.8 83.0 81.7 68.0 (77.0) 
5 
80.0 79.2 78.0 80.6 64.2 (74.1) 
10 
78.0 78.9 76.6 80.0 61.6 (72.4) 

DS 

1 
94.0 92.8 92.0 92.8 82.0 (88.5) 
5 
91.2 90.3 91.2 91.4 80.2 (86.7) 
10 
88.6 88.8 90.0 90.5 77.9 (85.0) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Accuracy in the RG using different speakers and listeners. Test represents the full test set consisting of 2350 image pairs. Test * represents a subset of 100 test set image pairs for which we collected human listener results. For the human listener, we report the accuracy when there is a majority agreement, and accuracy with guessing (in brackets). DS is significantly better at generating discriminative attribute phrases than SS.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement: This research was supported in part by the NSF grants 1617917 and 1661259, and a faculty gift from Facebook. The experiments were performed using equipment obtained under a grant from the Collaborative R&amp;D Fund managed by the Massachusetts Tech Collaborative and GPUs donated by NVIDIA.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Evaluation of output embeddings for fine-grained image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Walter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
		<title level="m">Reasoning About Pragmatics with Neural Listeners and Speakers. Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Automatic attribute discovery and characterization from noisy web data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shih</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Describing clothing by semantic attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girod</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kottur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yadav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M F</forename><surname>Moura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Visual dialog</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Guesswhat?! visual object discovery through multi-modal dialogu</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Vries</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Strub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chandar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Pietquin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Devlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1505.04467</idno>
		<title level="m">Zitnick. Exploring nearest neighbor approaches for image captioning</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Attribute-centric recognition for cross-category generalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Logic and conversation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">P</forename><surname>Grice</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1975" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<title level="m">Natural language object retrieval. Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Segment-phrase table for semantic segmentation, visual entailment and paraphrasing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Izadinia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hajishirzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Zero-shot recognition with unreliable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">ReferItGame: Referring to objects in photographs of natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kazemzadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ordonez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Matten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods in Natural Language Processing (EMNLP)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">ADAM: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Unifying visual-semantic embeddings with multimodal neural language models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics (TACL)</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">WhittleSearch: Image search with relative attribute feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kovashka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Attributebased classification for zero-shot visual object categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nickisch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Harmeling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="453" to="465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Comprehension-guided referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Discovering a lexicon of parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Workshop on Parts and Attributes, European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A taxonomy of part and attribute discovery techniques</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Attributes</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="247" to="268" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1306.5151</idno>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Generation and comprehension of unambiguous object descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Camburu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Generating expressions that refer to visible objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van Deemter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Modeling context between objects for referring expression understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">K</forename><surname>Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">I</forename><surname>Morariu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Interactively building a discriminative vocabulary of nameable attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Relative attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning deep representations of fine-grained visual descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">VISKE: Visual knowledge extraction and question answering by visual verification of relation phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Kumar Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Multi-attribute spaces: Calibration for attribute fusion and similarity search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Scheirer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">N</forename><surname>Belhumeur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Attribute dominance: What pops out?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Turakhia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visualizing data using tsne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">7</biblScope>
			<date type="published" when="2008-11" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Understanding objects in detail with fine-grained attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tsogkas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Saphra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Context-aware captions from context-agnostic supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<idno>2017. 3</idno>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Show and tell: A neural image caption generator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Similarity comparisons for interactive finegrained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Horn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Modeling context in referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Poirson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">A joint speakerlistener-reinforcer model for referring expressions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Tan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">L</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Yin and Yang: Balancing and answering binary visual questions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
