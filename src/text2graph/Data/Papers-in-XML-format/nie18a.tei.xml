<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:22+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weili</forename><surname>Nie</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ankit</forename><forename type="middle">B</forename><surname>Patel</surname></persName>
						</author>
						<title level="a" type="main">A Theoretical Explanation for Perplexing Behaviors of Backpropagation-based Visualizations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Backpropagation-based visualizations have been proposed to interpret convolutional neural networks (CNNs), however a theory is missing to justify their behaviors: Guided backpropagation (GBP) and deconvolutional network (DeconvNet) generate more human-interpretable but less classsensitive visualizations than saliency map. Motivated by this, we develop a theoretical explanation revealing that GBP and DeconvNet are essentially doing (partial) image recovery which is unrelated to the network decisions. Specifically, our analysis shows that the backward ReLU introduced by GBP and DeconvNet, and the local connections in CNNs are the two main causes of compelling visualizations. Extensive experiments are provided that support the theoretical analysis.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Driven by massive data and computational resources, modern convolutional neural networks (CNNs) and other network architectures have achieved many outstanding results, such as image recognition <ref type="bibr" target="#b9">(Krizhevsky et al., 2012)</ref>, neural machine translation <ref type="bibr" target="#b21">(Sutskever et al., 2014)</ref>, and playing Go games <ref type="bibr" target="#b17">(Silver et al., 2016)</ref>, etc. Despite their extensive applications, these neural networks are always considered as black boxes. Interpretability used to be for its own sake; now, due to safety-critical applications such as self-driving cars and tumor diagnosis, it is no longer satisfying to have a black box that is unaccountable for its decisions. The demand for explainable artificial intelligence (XAI) <ref type="bibr" target="#b4">(Gunning, 2017)</ref> -human interpretable explanations of model decisions -has driven the development of visualization techniques, including image synthesis via activation maximization <ref type="bibr" target="#b19">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b6">Johnson et al., 2016;</ref><ref type="bibr" target="#b12">Nguyen et al., 2016)</ref> and backpropagation-based visualizations <ref type="bibr" target="#b19">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b24">Zeiler &amp; Fergus, 2014;</ref><ref type="bibr" target="#b20">Springenberg et al., 2014;</ref><ref type="bibr" target="#b16">Shrikumar et al., 2017;</ref><ref type="bibr" target="#b7">Kindermans et al., 2017)</ref>.</p><p>The basic idea of backpropagation-based visualizations is to highlight class-relevant pixels by propagating the network output back to the input image space. The intensity changes of these pixels have the most significant impact on network decisions. Specifically, <ref type="bibr" target="#b19">(Simonyan et al., 2013)</ref> visualizes the spatial support of a given class in a given image, i.e. saliency map, by using the true gradient which masks out negative entries of bottom data via the forward ReLU. Despite its simplicity, the results of saliency map are normally very noisy which makes the interpretation difficult. <ref type="bibr" target="#b24">(Zeiler &amp; Fergus, 2014)</ref> visualize the reverse mapping from feature activities back to the input pixel space with the deconvolutional network (DeconvNet) method. The basic idea of DeconvNet is to mask out negative entries of the top gradients by resorting to the backward ReLU. <ref type="bibr" target="#b20">(Springenberg et al., 2014)</ref> proposed the Guided Backpropagation (GBP) method which combines the above two methods: by considering both the forward and backward ReLUs, it masks out the values for which either top gradients or bottom data are negative and produces sharper visualizations. More recently, DeepLift <ref type="bibr" target="#b16">(Shrikumar et al., 2017)</ref> and PatternNet <ref type="bibr" target="#b7">(Kindermans et al., 2017)</ref> have been proposed to further improve the visual quality of backpropagation-based methods.</p><p>This class of backpropagation-based visualizations, in particular GBP and DeconvNet, has attracted a lot of attention in both the deep learning community and other fields <ref type="bibr" target="#b22">(Szegedy et al., 2013;</ref><ref type="bibr" target="#b15">Selvaraju et al., 2016;</ref><ref type="bibr" target="#b2">Fong &amp; Vedaldi, 2017;</ref><ref type="bibr" target="#b8">Kraus et al., 2016)</ref>. Despite their good visual quality, the question of how they are actually related to the decision-making has remained largely unexplored. Do the pretty visualizations actually tell us reliably about what the network is doing internally? Our experiments have confirmed previous observations <ref type="bibr" target="#b11">(Mahendran &amp; Vedaldi, 2016;</ref><ref type="bibr" target="#b15">Selvaraju et al., 2016;</ref><ref type="bibr" target="#b14">Samek et al., 2017)</ref> that saliency map is indeed very sensitive to the change of class labels, while GBP and DeconvNet, though their visualization results are much cleaner than saliency map, remain almost the same given different class labels. It seems that the visual quality improvement of backpropagation-based methods is sacrificing the ability of highlighting important pixels to a specific output class. In this sense, GBP and DeconvNet may be unreliable in interpreting how deep neural networks make classification decisions.</p><p>The most commonly used explanation for these visualizations is to approximate the neural networks with a linear function <ref type="bibr" target="#b19">(Simonyan et al., 2013;</ref><ref type="bibr" target="#b7">Kindermans et al., 2017)</ref>, where the derivative of output with respect to input image is just the weight vector of the model. In such sense, the backpropagation-based methods can be regarded as visualizing the learned weights. But apparently the approximate linear model is too simplistic to reflect the highly nonlinear property of deep neural networks. For example, GBP and DeconvNet essentially apply the same algorithm as saliency map, but treat ReLU, the nonlinear activation, differently. The linear model explanation thus cannot answer questions regarding why GBP and DeconvNet outperform saliency map in terms of visual quality whereas they are less class-sensitive than saliency map, as both of them reduce to saliency map in a linear model. Therefore, we need a more complex model, which should at least capture the impact of both forward ReLU and backward ReLU, to better understand what the main causes of their visually compelling results are and what information, if not the classification decisions, we can extract from these visualizations.</p><p>Our contributions. We provide a theoretical explanation for why GBP and DeconvNet generate more humaninterpretable but less class-sensitive visualizations than saliency map. Specifically, our analysis reveals that GBP and DeconvNet are essentially doing (partial) image recovery instead of highlighting class-relevant pixels or visualizing the learned weights, which means in principle they are unrelated to the decision-making of neural networks. We also find that it is the backward ReLU introduced by either GBP or DeconvNet, together with the local connections in CNNs that results in crisp visualizations. In particular, we explain how DeconvNet also relies on the max-pooling to recover the input. Finally, we do extensive experiments to support our theory and further reveal more detailed properties of these backpropagation-based visualizations 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Backpropagation-based Visualizations</head><p>In this section, we first give formal definitions of backpropagation-based visualizations: saliency map, DeconvNet and GBP, and then compare their empirical behaviors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Formal Definitions</head><p>The key difference of backpropagation-based methods is the way they propagate the output score back through the 1 Code is available at https://github.com/weilinie/BackpropVis </p><formula xml:id="formula_0">y i (l) o i (l) T i (l) R i<label>(</label></formula><formula xml:id="formula_1">b,i (t) I R (l) i t</formula><p>Therefore, the formal definition of backpropagation-based methods for propagating the output score back through the i-th ReLU activation in the l-th layer is</p><formula xml:id="formula_2">T (l) i =          σ (l) f,i R (l) i for saliency map σ (l) b,i R (l) i for DeconvNet σ (l) f,i σ (l) b,i R (l) i</formula><p>for GBP which can be further uniformly formulated as</p><formula xml:id="formula_3">T (l) i = h R (l) i ∂g y (l) i ∂y (l) i (1)</formula><p>where the two functions h(·) and g(·) are defined as h(t) = t for saliency map σ(t) for DeconvNet and GBP g(t) = t for DeconvNet σ(t) for saliency map and GBP . Backpropagation-based visualizations for the trained VGG-16 net given an input "tabby". From top row to the last row, it is saliency map, DeconvNet and GBP, where "max" refers to computing the (modified) gradient for the maximum class logit and the number, say "482", refers to computing the (modified) gradient for the 482-th logit. These numbers are randomly chosen for generality. Best viewed in the electronic version.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Empirical Observations</head><p>To be a good visualization method, a clean and visually human-interpretable result is very desirable. More importantly, it should also reveal how the neural networks make decisions. Based on this, we provide the empirical behaviors of the backpropagation-based visualizations for a pretrained VGG-16 net <ref type="bibr" target="#b18">(Simonyan &amp; Zisserman, 2014)</ref> in Figure 2. Without loss of generality, the visualizations are obtained by choosing one of the class logits (i.e. the unnormalized class probability output right before the softmax function) as the output score to be taken derivative with respect to the input image.</p><p>For the visual quality, saliency map is very noisy while DeconvNet and GBP produce human-interpretable visualizations with a subtle difference: DeconvNet unexpectedly produces some kind of texture-like pattern, and GBP is cleaner with some background information filtered out. For the class-sensitivity, saliency map changes greatly for different class logits while DeconvNet and GBP are almost invariant to which class logit we choose. This, together with more experiments, suggests that after introducing the backward ReLU, both DeconvNet and GBP modify the true gradient in a way that they create much cleaner results but their functionality as an indicator of important pixels to a specific class has disappeared. In the next section, we will explain these empirical behaviors and discuss the reason why GBP and DeconvNet differ greatly from saliency map.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Theoretical Explanations</head><p>We first analyze the backpropagation-based methods in a three-layer CNN with random Gaussian weights, which is then extended to more complicated models such as CNNs with max-pooling and deep CNNs. Besides, we also investigate their behaviors in well-trained CNNs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">A Random Three-Layer CNN</head><p>Consider a three-layer CNN, consisting of an input layer and a convolutional hidden layer, followed by a ReLU activation function and a fully connected layer of which its output is called class logits. Formally, let x ∈ R d be a normalized input image with dimension d and x = 1, and let W ∈ R p×N be N convolutional filters where each column w (i) denotes the i-th filter with size p. Note that here we use vectors to represent images and filters for simplicity, and the analysis also works for the more practical two-dimensional case. Then, we let Y ∈ R p×J be J image patches extracted from x, and each column y (j) with size p is generated by a linear function</p><formula xml:id="formula_4">y (j) = D j x where D j 0 p×(j−1)b I p×p 0 p×(d−(j−1)b−p)</formula><p>with b being the stride size 2 . For example, given a filter with size 3 and stride 1, the resulting j-th patch y (j) is made of the j-th to (j + 2)-th consecutive pixels. The weights in the fullyconnected layer can be represented by V ∈ R N J×K with K being the number of output logits. Therefore, the k-th logit is represented by</p><formula xml:id="formula_5">f k (x) = N i=1 J j=1 V qij ,k σ(w (i)T y (j) )<label>(3)</label></formula><p>where the index q ij denotes the ((i − 1)J + j)-th entry in every column vector of weight matrix V .</p><p>Assume every entry of V and W is sampled from an</p><formula xml:id="formula_6">i.i.d. Gaussian distribution N (0, c 2 ).</formula><p>The following lemma provides the formula for backpropagation-based visualizations in a random three-layer CNN. Note that the norm of the final results will be in the range of [0, 1] as we apply the normalization during visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Lemma 1. The backpropagation-based visualizations for the k-th logit in a random three-layer CNN is formalized as</head><formula xml:id="formula_7">s k (x) = 1 Z k J j=1 D j T N i=1 h(V qij ,k )w (i,j)<label>(4)</label></formula><p>where Z k is the normalization coefficient to ensure</p><formula xml:id="formula_8">s k (x) ∈ [0, 1], h(·)</formula><p>is given by Eq. (2) and</p><formula xml:id="formula_9">w (i,j) = w (i)</formula><p>for DeconvNet</p><formula xml:id="formula_10">w (i) I w (i)T y (j)</formula><p>for saliency map and GBP </p><formula xml:id="formula_11">s GBP k (x) ≈ x<label>(5)</label></formula><p>Proof. See Appendix B.</p><p>The above theorem shows that after introducing the backward ReLU, the input image can be approximately recovered by GBP in a random three-layer CNN, regardless of the class label. However, according to the linear model explanation, backpropagation-based methods are visualizing learned weights, which should be random noise as they are all sampled from i.i.d Gaussians. Obviously, it is inconsistent with the actual behavior of GBP.</p><p>As the approximation in Eq. <ref type="formula" target="#formula_0">(5)</ref> builds on an assumption that the number of filters N is sufficiently large, a key question is: How many filters are needed to guarantee an accurate recovery? From <ref type="bibr" target="#b10">(Lugosi &amp; Mendelson, 2017)</ref>, we can set N =Õ( p ǫ 2 ) such that with high probability</p><formula xml:id="formula_12">1 N N i=1w (i,j) − E[w (i,j) ] &lt; ǫ,</formula><p>where p denotes the filter size andÕ(·) hides some other factors. As an upper bound, it reveals that the number of convolutional filters needed heavily depends on the filter size p. As the filter size intrinsically determined by the local connections in CNNs is usually small, we could use a mild number of convolutional filters to recover the input image. For example, given a filter size 3 × 3 × 3, we need at most O(10 3 ) filters to achieve an estimation error ǫ less than 0.1. This strongly suggests that GBP visualizations are human-interpretable in most of the CNNs, and thus the local connections property is another key factor underlying crisp visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.2.">SALIENCY MAP AND DECONVNET</head><p>Here we show the behaviors of saliency map and DeconvNet in a random three-layer CNN are largely different from GBP. Theorem 2. In a random three-layer CNN, if the number of filters N is sufficiently large, saliency map and DeconvNet are approximated as Gaussian random variables satisfying</p><formula xml:id="formula_13">s Sal k (x), s Deconv k (x) ∼ N (0, I) Proof. See Appendix C.</formula><p>The above theorem shows that both saliency map and DeconvNet visualizations will yield random noise, conveying little information about the input image and class logits. For saliency map, it is easily understood since saliency map represents the true gradient of the class logit, which heavily depends on the weights. For DeconvNet, although its behavior appears similar to saliency map in this simplistic scenario, we will show later on that it behaves more similarly to GBP, in particular with the existence of max-pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Extensions to More Realistic Models</head><p>In this section, we extend our analysis of a simple random three-layer CNN to other more realistic cases, including the max-pooling, deeper nets and trained weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1.">CNNS WITH MAX-POOLING</head><p>If we add a max-pooling layer between the ReLU and the fully-connected layer, the k-th logit becomes</p><formula xml:id="formula_14">f k (x) = N i=1 J j=1 Vq ij ,k δ(σ(w (i)T y (j) ))</formula><p>where δ(·) denotes the max-pooling, which successively selects the maximum value in a fixed-size pooling window, and the new indexq ij is the down-sampled version of q ij . Then the backpropagation-based visualizations for the k-th logit can be formulated as</p><formula xml:id="formula_15">s k (x) = 1 Z k J j=1 D j T N i=1 h(δ ′ (o ij )Vq ij ,k )w (i,j)<label>(6)</label></formula><p>where o ij σ(w (i)T y (j) ) is the output of each ReLU activation and δ ′ (o ij ) denotes the derivative of δ(·) evaluated at o ij , which is</p><formula xml:id="formula_16">δ ′ (o ij ) = 1 if o ij is chosen by max-pooling 0 otherwise</formula><p>Since o ij ≥ 0 with equality holds for w (i)T y (j) ≤ 0, given a proper pooling window size, it is highly possible that o ij is chosen by the max-pooling if and only if w (i)T y (j) &gt; 0. It means with high probability, Eq. (6) is approximated as</p><formula xml:id="formula_17">s k (x) ≈ 1 Z k J j=1 D j T N i=1 h(Vq ij ,k )w (i,j) I(w (i)T y (j) )<label>(7)</label></formula><p>For saliency map and GBP, we knoww (i,j) I(w (i)T y (j) ) = w (i,j) and thus Eq. <ref type="formula" target="#formula_0">(7)</ref> is further reduced to Eq. (4), which means the behaviors of saliency map and GBP remain the same after introducing the max-pooling. However, with high probability, DeconvNet at the k-th logit becomes</p><formula xml:id="formula_18">s Deconv k (x) ≈ 1 Z k J j=1 D j T N i=1 σ(Vq ij ,k )w (i) I(w (i)T y (j) )</formula><p>which is exactly the form of GBP in Eq. (4). Therefore, adding the max-pooling makes the DeconvNet behave like GBP -doing nothing but image recovery. This also explains and extends the previous intuitive claims in <ref type="bibr" target="#b14">(Samek et al., 2017;</ref><ref type="bibr" target="#b13">Odena et al., 2016</ref>) that the image-specific information in DeconvNet comes from the max-pooling.</p><p>Note that that the approximation from Eq. (6) to Eq. <ref type="formula" target="#formula_0">(7)</ref> in DeconvNet with the max-pooling is essentially different from the approximations used in GBP. For GBP, the approximate gap can be made arbitrarily small by increasing the hidden layer size N , leading to a perfect recovery of the input. However, for DeconvNet, given any pooling window size, there might always exist at least one of the following two contradictory cases: it is possible that a ij is chosen by the max-pooling if w (i)T y (j) ≤ 0, and also possible that a ij is not chosen if w (i)T y (j) &gt; 0. This makes DeconvNet (with max-pooling), in theory, never recover input perfectly, which might explain why the unusual texture-like artifacts appear in the DeconvNet visualizations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2.">DEEP CNNS</head><p>The analysis for a three-layer CNN can be generalized to the multi-layer (or deeper) case. For clarity, we formulate the k-th logit of an L-layer deep CNN in a matrix form:</p><formula xml:id="formula_19">f k (x) = Γ (L)T k σ Γ (L−1)T · · · σ Γ (1)T x</formula><p>where Γ (l) ∈ R d l ×d l+1 denotes either the convolutional or fully-connected operator matrix in the l-th layer and Γ (L) k is the k-th column of Γ (L) . Denote by o (l) the output of ReLU activations in the l-th layer, i.e.</p><formula xml:id="formula_20">o (l) = σ Γ (l)T o (l−1) , ∀l ∈ {1, · · · , L−1} with o (0)</formula><p>x. Then backpropagation-based visualizations at the k-th logit in an L-layer deep CNN can be formulated as</p><formula xml:id="formula_21">s k (x) = 1 Z k ∂õ (1) ∂x · h(V (1) ·,k ) (a) = 1 Z k J j=1 D j T N i=1 h(V (1) qij ,k )w (i,j)<label>(8)</label></formula><formula xml:id="formula_22">with ∀l ∈ {1, · · · , L − 1}, V (l) ·,k = ∂õ (l+1) ∂o (l) · h ∂õ (l+2) ∂o (l+1) · · · h ∂õ (L−1) ∂o (L−2) h Γ (L) k</formula><p>where in (a) we rewrite s k (x) in an expanded form,õ</p><formula xml:id="formula_23">(l) g Γ (l)T o (l−1) , w (i)</formula><p>is the i-th filter encoded in Γ (1) and N is the number of filters in the first convolutional layer. Also, h(·), g(·) andw <ref type="bibr">(i,j)</ref> are defined in Eq. <ref type="formula" target="#formula_0">(2)</ref> and Lemma 1.</p><p>First, the approximate property ofV</p><p>·,k in the random deep CNN is given in the following proposition.   <ref type="formula" target="#formula_7">4)</ref>, which means the analysis of backpropagation-based visualizations in a shallow threelayer CNN also applies to the deep CNN case. Therefore, the behaviors of these visualizations will barely change when increasing the depth of neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3.">CNNS WITH TRAINED WEIGHTS</head><p>The previous analysis for random CNNs does not apply to the trained case directly since the weights here may not be i.i.d. Gaussian distributed. For saliency map, which uses the true gradient, the trained weights are likely to impose a stronger bias towards some specific subset of the input pixels, and so they can highlight class-relevant pixels rather than producing random noise. For GBP and DeconvNet, the analysis is a little more involved.</p><p>On the one hand, the trained weights w (i) will only lie in a small subspace of the whole image patch space which will create some "dead zones", as illustrated in <ref type="figure" target="#fig_3">Figure 3 (a)</ref>. That is, all image patches lying in the "dead zone" will be filtered out by the forward ReLU. For example, it is well-known that the trained weights in the first convolutional layer are Gabor-like filters to detect the image patches containing edges <ref type="bibr" target="#b23">(Yosinski et al., 2014;</ref><ref type="bibr" target="#b24">Zeiler &amp; Fergus, 2014)</ref>. That is, image patches without edges will probably be filtered out by the first convolutional layer. Also, the higher convolutional layers keep filtering out more image patches with certain patterns (e.g. <ref type="figure" target="#fig_6">Figure 9</ref>). See the supplementary material for a comparison between GBP and a linear edge detector.</p><p>On the other hand, as shown in <ref type="figure" target="#fig_3">Figure 3 (b)</ref> and (c), the histograms of weights connected to the respective one of any two different neurons in the first fully connected layer (called "fc1") of the trained VGG-16 net are very similar to each other. Approximately, they form two very similar Gaussians with a small standard deviation, which means the (modified) gradients at any two different neurons in the layer "fc1" with respect to the input image are almost the same. Namely,</p><formula xml:id="formula_25">∂õ (fc1)</formula><p>∂x in Eq. <ref type="formula" target="#formula_0">(8)</ref> for GBP and DeconvNet (with max-pooling) satisfies</p><formula xml:id="formula_26">∂õ (fc1) m ∂x ≈ F conv (x), ∀m ∈ {1, · · · , M } whereõ (fc1)</formula><p>m is the m-th entry ofõ (fc1) and F conv (·) :</p><formula xml:id="formula_27">R d → R</formula><p>d denotes the (normalized) overall filtering effect of the convolutional layers and M is the number of neurons in the layer "fc1". Thus, Eq. <ref type="formula" target="#formula_0">(8)</ref> for GBP and DeconvNet (with max-pooling) in the trained CNN can be approximated as</p><formula xml:id="formula_28">s k (x) = 1 Z k ∂õ (fc1) ∂x · h(V (fc1) ·,k ) = 1 Z k M m=1 ∂õ (fc1) m ∂x · h(V (fc1) m,k ) (a) ≈ F conv (x)<label>(9)</label></formula><p>where (a) follows from setting the normalization coefficient to be</p><formula xml:id="formula_29">Z k = 1 M m=1 h(V (fc1) m,k ) .</formula><p>It shows that GBP and DeconvNet (with max-pooling) in a trained CNN are actually doing the partial image recovery, where the trained weights control which image patch could form an active path to the class logit. More importantly, this filtering process is not class sensitive (e.g. the edge detector). In the end, only these "active" image patches are combined in the first fully connected layer to form the final visualization results. As the right side of (9) does not depend on k, it illustrates why the GBP and DecovNet visualizations in the trained VGG are not class-sensitive.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>To verify our theoretical analysis, we conduct a series of experiments on a three-layer CNN, a three-layer fully- <ref type="figure">Figure 4</ref>. Backpropagation-based visualizations in a random threelayer CNN (top row) and a random three-layer FCN (bottom row) given the input image "tabby". From left to right, each column represents GBP, DeconvNet and saliency map, respectively. Only GBP visualization in the CNN is human-interpretable.</p><formula xml:id="formula_30">GBP-CNN Deconv-CNN Sal-CNN GBP-FCN Deconv-FCN Sal-FCN</formula><p>connected network (FCN) and a VGG-16 net. For a random network, their weights are all sampled from the truncated Gaussians with a zero-mean and standard deviation 0.1. Unless stated otherwise, the input is the image "tabby" from the ImageNet dataset <ref type="bibr" target="#b0">(Deng et al., 2009</ref>) with size 224×224×3. See the supplementary materials for more results on other images and other neural network such as ResNet <ref type="bibr" target="#b5">(He et al., 2016)</ref>. In the three-layer CNN, the filter size is 7 × 7 × 3, the number of filters is N = 256, and the stride is 2. In the three-layer FCN, the hidden layer size is set to N h = 4096. By default, the backpropagation-based visualizations are calculated with respect to the maximum class logit. <ref type="figure">Figure 4</ref> shows the backpropagation-based visualizations on a random three-layer CNN and a random three-layer FCN, respectively. We can see only GBP in the CNN can produce a human-interpretable visualization, while DeconvNet and saliency map in the CNN get random noise, which verifies our theoretical analysis in the section 3.1. In contrast, as local connections do not exist in the FCN and the input size (e.g. 224 × 224 × 3) is extremely large, all the backpropagation-based methods (including GBP) in the FCN generate random noise. Particularly for GBP, the number of hidden neurons N h = 4096 is still not large enough to recover the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Impact of Local Connections</head><p>To further highlight the impact of local connections in the visual quality of GBP, we vary the number of filters N in the CNN and the number of hidden neurons N h in the FCN, respectively, while keep other parameters fixed. The results are given in <ref type="figure">Figure 5</ref>. Note that in the FCN, we have downsampled the input image to be of size 64 × 64 × 3 due to computational limitations. We can see that as the number of filters N increases (resp. the hidden layer size N h ), the vi- sual quality of GBP in the CNN (resp. in the FCN) becomes better. Interestingly, even by setting N h = 70000, which is definitely unrealistic, the FCN cannot achieve a comparable performance to the CNN with N = 64. Therefore, it confirms that the local connections in the CNN really contribute to the good visual quality of GBP.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Impact of Max-Pooling and Network Depth</head><p>To show the impact of the max-pooling in backpropagationbased visualizations, we then add a max-pooling layer in the above random three-layer CNN while keeping other parameters fixed, and the results are given in <ref type="figure">Figure 6</ref> (top row). As compared with the visualizations in <ref type="figure">Figure 4</ref> (top row), neither GBP or saliency map is impacted by the max-pooling, whereas the DeconvNet visualization has now become human interpretable instead of being the random noise as before. It confirms that the max-pooling is critical in helping DeconvNet produce human-interpretable visualizations via image recovery, as predicted by our theoretical analysis in the section 3.2.1.</p><p>To show the impact of network depth, we also apply backpropagation-based visualizations in a random VGG-16 net, which also includes the max-pooling but is much deeper than the three-layer CNN. <ref type="figure">Figure 6</ref> (bottom row) shows that only saliency map generates random noise while both GBP and DeconvNet could produce human-interpretable visualizations. Though there are subtle visual differences between the top row and bottom row of <ref type="figure">Figure 6</ref>, the behaviors of backpropagation-based methods are basically unchanged after increasing the network depth. In addition, both GBP and DeconvNet reconstruct every fine-grained detail of the input image in the random VGG , which is different from the trained VGG in <ref type="figure" target="#fig_1">Figure 2</ref> where only those "active" image patches are preserved.</p><p>GBP-pool Deconv-pool Sal-pool GBP-VGG Deconv-VGG Sal-VGG <ref type="figure">Figure 6</ref>. Backpropagation-based visualizations given the input image "tabby" in a random three layer CNN with the max-pooling (top row) and in a random VGG-16 net (bottom row). Now DeconvNet visualization also becomes human-interpretable. <ref type="figure">Figure 7</ref>. Average l2 distance statistics. For each input, we randomly choose two class logits to get corresponding visualizations and calculate their l2 distances. The above is an average l2 distance by using 10K images of the ImageNet for each backpropagationbased method in both random and trained VGG-16 net.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Average l 2 Distance Statistics</head><p>To quantitatively describe how backpropagation-based visualizations change with respect to different class logits, we also provide the average l 2 distance statistics as shown in <ref type="figure">Figure 7</ref>. Our results are obtained by first calculating the l 2 distance of two visualization results given two different class logits for each input image and then taking an average of those l 2 distances based on 10K images from the ImageNet test set. The process is repeated for all backpropagationbased methods in both random and trained cases. As we can see, the average l 2 distance of saliency map is much larger than that of both GBP and DeconvNet in either a random VGG or a trained VGG, which clearly demonstrates that saliency map is class-sensitive but GBP and DeconvNet are not. Interestingly, in the trained VGG-16 net, the average l 2 distance of DeconvNet is slightly larger than that of GBP. It shows that the class insensitivity is exchanged for further improvement of visual quality. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Adversarial Attack on VGG</head><p>Adversarial attack provides another way of directly testing whether visualizations are class-sensitive or doing image recovery. The class-sensitive visualizations should change drastically as both the predicted class label and ReLU states of intermediate layers have changed, while the visualizations doing image recovery should change little as only a tiny adversarial perturbation is added into the input image.</p><p>In this experiment, we first generate an adversarial example "busby" via the fast gradient sign method (FGSM) <ref type="bibr" target="#b3">(Goodfellow et al., 2014)</ref> by feeding the image "panda" into the pretrained VGG-16 net. Next, we apply the backpropagationbased visualizations to the original image "panda" and its adversary "busby" in the trained VGG-16 net. As shown in <ref type="figure" target="#fig_5">Figure 8</ref>, the saliency map visualization changes significantly whereas the GBP and DeconvNet visualizations remain almost unchanged after replacing "panda" by its adversary "busby". Therefore, it further confirms that saliency map is class-sensitive in that it highlights important pixels in making classification decisions. However, GBP and DeconvNet are doing nothing but (partial) image recovery.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">VGG with Partly Trained Weights</head><p>There exist some differences for backpropagation-based visualizations, GBP and DeconvNet in particular, between the random and trained cases. We take GBP as an example here to investigate the contributions of different layers in the trained VGG-16 net to these visual differences.</p><p>First, to isolate the impact of later layers, we load the trained weights up to a given layer and leave later layers randomly initialized. As shown in <ref type="figure" target="#fig_6">Figure 9</ref> (top row), from "Conv1-1*" to "Conv5-1*" GBP keeps filtering out more image patches as the number of trained convolutional layers increases. However, from "Conv5-1*" to "FC3*" (i.e., the fully-trained case) GBP behaves almost the same, no matter weights in the dense layers are random or trained. Therefore, it is the trained weights in the convolutional layers rather than those in the dense layers that account for filtering out image patches. Also, it further confirms that GBP is class-insensitive. Furthermore, to reveal the impact of each layer, we load the trained weights for the whole VGG-16 net except for a given layer which is randomly initialized instead. The results are shown in <ref type="figure" target="#fig_6">Figure 9</ref> (bottom row). We can see that the GBP visualization is blurry for "Conv1-1 ⋄ ", clean with much background information for "Conv3-1 ⋄ " and clean without background information for "Conv5-1 ⋄ ", respectively. It means that the earlier convolutional layer has more important impact in the GBP visualization than the later convolutional layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a theoretical explanation for backpropagation-based visualizations, where we started from a random three-layer CNN and later generalized it to more realistic cases. We showed that unlike saliency map, both GBP and DeconvNet are essentially doing (partial) image recovery, which verified their class-insensitive properties. We revealed that it is the backward ReLU, used by both GBP and DeconvNet, along with the local connections in CNNs, that is responsible for human-interpretable visualizations. We also explained how DeconvNet also relies on the max-pooling to recover the input. Our analysis was supported by extensive experiments. Finally, we hope our analysis can provide useful insights into developing better visualization methods for deep neural networks. A future direction is to understand how the GBP visualizations in the trained CNNs filter out image patches layer by layer.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Illustration of how backpropagation-based methods propagate back through the i-th nonlinear activation in the l-th layer with input y (l) i and output o (l) i , where T (l) i denotes the (modified) gradient after passing through the activation and R (l) i denotes the top gradient before the activation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2</head><label>2</label><figDesc>Figure 2. Backpropagation-based visualizations for the trained VGG-16 net given an input "tabby". From top row to the last row, it is saliency map, DeconvNet and GBP, where "max" refers to computing the (modified) gradient for the maximum class logit and the number, say "482", refers to computing the (modified) gradient for the 482-th logit. These numbers are randomly chosen for generality. Best viewed in the electronic version.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. (a) shows a two-dimensional toy example where w (i) 's are all in a cone (the orange area) and all the y (j) 's in another cone (the grey area) called "dead zone" will be filtered out by the ReLU. (b) and (c) show the histograms of all weights connected to the 26-th activation and the 44-th one, respectively, in the layer "fc1" of the trained VGG-16 net. Note that we randomly picked up two activations (i.e. 26 and 44 here) for comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>N 70000 Figure 5 .</head><label>700005</label><figDesc>Figure 5. GBP visualizations given the input image "tabby" in a three-layer CNN (top row) by varying the number of filters N and in a three-layer FCN (bottom row) by varying the number of hidden neurons N h .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Top row: the image "panda" and its backpropagationbased visualizations. Bottom row: the adversarial example misclassified as "busby" and its backpropagation-based visualizations. Both experiments are applied in the trained VGG-16 net.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Top row: load trained weights up to the indexed layer and leave the later layers to be randomly initialized (marked by star sign). Bottom row: load trained weights except for the indexed layer is randomly initialized instead (marked by diamond sign).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Proof. See Appendix A. Next, we can analyze the different behaviors of these backpropagation-based methods case by case. 3.1.1. GUIDED BACKPROPAGATION First, the behavior of GBP is given as follows. Theorem 1. In a random three-layer CNN, if the number of filters N is sufficiently large, GBP at the k-th logit can be approximated as</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Here we assume a VALID padding method implicitly, and other padding methods do not impact our analysis.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Thanks to the anonymous reviewers for useful comments. WN, YZ and AB were supported by IARPA via DoI/IBC contract D16PC00003.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
	<note>CVPR 2009. IEEE Conference on</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Inverting visual representations with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4829" to="4837" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interpretable explanations of black boxes by meaningful perturbation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="3429" to="3437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Defense Advanced Research Projects Agency (DARPA), nd Web</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gunning</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>Explainable artificial intelligence (xai)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-J</forename><surname>Kindermans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">T</forename><surname>Schütt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dähne</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.05598</idno>
		<title level="m">Patternnet and patternlrp-improving the interpretability of neural networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Classifying and segmenting microscopy images with deep multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Z</forename><surname>Kraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">J</forename><surname>Frey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="52" to="59" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Sub-gaussian estimators of the mean of a random vector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lugosi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mendelson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.00482</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Salient deconvolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="120" to="135" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthesizing the preferred inputs for neurons in neural networks via deep generator networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Clune</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3387" to="3395" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deconvolution</surname></persName>
		</author>
		<idno>doi: 10.23915/ distill.00003</idno>
		<ptr target="http://distill.pub/2016/deconv-checkerboard" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Evaluating the visualization of what a deep neural network has learned</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Samek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Binder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Montavon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lapuschkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-R</forename><surname>Müller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="page" from="2660" to="2673" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Selvaraju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Das</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vedantam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Grad-Cam</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.02391</idno>
		<title level="m">Why did you say that? visual explanations from deep networks via gradient-based localization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Learning important features through propagating activation differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<title level="m">Very deep convolutional networks for large-scale image recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6034</idno>
		<title level="m">Deep inside convolutional networks: Visualising image classification models and saliency maps</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Striving for simplicity: The all convolutional net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6806</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3104" to="3112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fergus</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">How transferable are features in deep neural networks?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3320" to="3328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Visualizing and understanding convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="818" to="833" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
