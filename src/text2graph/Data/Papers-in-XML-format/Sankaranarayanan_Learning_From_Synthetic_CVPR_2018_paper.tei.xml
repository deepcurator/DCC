<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Swami</forename><surname>Sankaranarayanan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yogesh</forename><surname>Balaji</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arpit</forename><surname>Jain</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GE Global Research</orgName>
								<address>
									<settlement>Niskayuna</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ser</forename><forename type="middle">Nam</forename><surname>Lim</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">GE Global Research</orgName>
								<address>
									<settlement>Niskayuna</settlement>
									<region>NY</region>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">GE Venture</orgName>
								<orgName type="institution">Avitas Systems</orgName>
								<address>
									<settlement>Boston</settlement>
									<region>MA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rama</forename><surname>Chellappa</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">UMIACS</orgName>
								<orgName type="institution" key="instit2">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning from Synthetic Data: Addressing Domain Shift for Semantic Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Convolutional Neural Networks (DCNNs) have revolutionalized the field of computer vision, achieving the best performance in a multitude of tasks such as image classification <ref type="bibr" target="#b11">[12]</ref>, semantic segmentation <ref type="bibr" target="#b19">[20]</ref>, visual question answering <ref type="bibr" target="#b22">[23]</ref> etc. This strong performance can be attributed to the availability of abundant labeled training data. While annotating data is relatively easier for certain tasks like image classification, they can be extremely laborious and time-consuming for others. Semantic segmentation is one such task that requires great human effort as it involves obtaining dense pixel-level labels. The annotation time for obtaining pixel-wise labels for a single image from * First two authors contributed equally  <ref type="figure" target="#fig_1">Figure 1</ref>: Characterization of Domain Shift and effect of the proposed approach in reducing the same the CITYSCAPES dataset is about 1 hr., highlighting the level of difficulty ( <ref type="bibr" target="#b3">[4]</ref>, <ref type="bibr" target="#b25">[26]</ref>). The other challenge lies in collecting the data: While natural images are easier to obtain, there are certain domains like medical imaging where collecting data and finding experts to precisely label them can also be very expensive.</p><p>One promising approach that addresses the above issues is the utility of synthetically generated data for training. However, models trained on the synthetic data fail to perform well on real datasets owing to the presence of domain gap between the datasets. Domain adaptation encompasses the class of techniques that address this domain shift prob-lem. Hence, the focus of this paper is in developing domain adaptation algorithms for semantic segmentation. Specifically, we focus on the hard case of the problem where no labels from the target domain are available. This class of techniques is commonly referred to as Unsupervised Domain Adaptation.</p><p>Traditional approaches for domain adaptation involve minimizing some measure of distance between the source and the target distributions. Two commonly used measures are Maximum Mean Discrepancy (MMD) ( <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b20">[21]</ref>  <ref type="bibr" target="#b21">[22]</ref>), and learning the distance metric using DCNNs as done in Adversarial approaches ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b29">[30]</ref>). Both approaches have had good success in the classification problems; however, as pointed out in <ref type="bibr" target="#b31">[32]</ref>, their performance improvement does not translate well to the semantic segmentation problem. This motivates the need for developing new domain adaptation techniques tailored to semantic segmentation.</p><p>The method we present in this work falls in the category of aligning domains using an adversarial framework. Among the recent techniques that address this problem, FCN in the wild <ref type="bibr" target="#b13">[14]</ref> is the only approach that uses an adversarial framework. However, unlike <ref type="bibr" target="#b13">[14]</ref> where a discriminator operates directly on the feature space, we project the features to the image space using a generator and the discriminator operates on this projected image space. Adversarial losses are then derived from the discriminator. We observed that applying adversarial losses in this projected image space achieved a significant performance improvement as compared to applying such losses directly in the feature space (ref. <ref type="table" target="#tab_4">Table 4</ref>).</p><p>The main contribution of this work is that we propose a technique that employs generative models to align the source and target distributions in the feature space. We first project the intermediate feature representations obtained using a DCNN to the image space by training a reconstruction module using a combination of L 1 and adversarial losses. We then impose the domain alignment constraint by forcing the network to learn features such that source features produce target-like images when passed to the reconstruction module and vice versa. This is accomplished by employing a series of adversarial losses. As training progresses, the generation quality gradually improves, while at the same time, the features become more domain invariant.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Fully Convolutional Networks (FCN) by Shelhamer et al <ref type="bibr" target="#b19">[20]</ref> signified a paradigm shift in how to fully exploit the representational power of CNNs for the semantic pixel labeling tasks. While performance has been steadily improving for popular benchmarks such as PASCAL VOC <ref type="bibr" target="#b5">[6]</ref> and MS-COCO <ref type="bibr" target="#b17">[18]</ref>, they do not address the challenges of domain shift within the context of semantic segmentation. Domain adaptation has been widely explored in computer vision primarily for the classification task. Some of the earlier approaches involved using feature reweighting techniques <ref type="bibr" target="#b4">[5]</ref>, or constructing intermediate representations using manifolds ( <ref type="bibr" target="#b10">[11]</ref>, <ref type="bibr" target="#b9">[10]</ref>). Since the advent of deep neural networks, emphasis has been shifted to learning domain invariant features in an end-to-end fashion. A standard framework for deep domain adaptation involves minimizing a measure of domain discrepancy along with the task being solved. Some approaches use Maximum Mean Discrepancy and its kernel variants for this task ( <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b21">[22]</ref>), while others use adversarial approaches ( <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b1">[2]</ref>, <ref type="bibr" target="#b27">[28]</ref>). We focus on adversarial approaches since they are more related to our work. Revgrad <ref type="bibr" target="#b6">[7]</ref> performs domain adaptation by applying adversarial losses in the feature space, while PixelDA <ref type="bibr" target="#b1">[2]</ref> and CoGAN <ref type="bibr" target="#b18">[19]</ref> operate in the pixel space. While these techniques perform adaptation for the classification task, there are very few approaches aimed at semantic segmentation. To the best of our knowledge, <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b31">[32]</ref> are the only two approaches that address this problem. FCN in the wild <ref type="bibr" target="#b13">[14]</ref> proposes two alignment strategies -(1) global alignment which is an extension to the domain adversarial training proposed by <ref type="bibr" target="#b6">[7]</ref> to the segmentation problem and (2) local alignment which aligns class specific statistics by formulating it as a multiple instance learning problem. Curriculum domain adaptation <ref type="bibr" target="#b31">[32]</ref> on the other hand proposes curriculum-style learning approach where the easy task of estimating global label distributions over images and local distributions over landmark superpixels is learnt first. The segmentation network is then trained so that the target label distribution follow these inferred label properties.</p><p>One possible direction to address the domain adaptation problem is to employ style transfer or cross domain mapping networks to stylize the source domain images as target and train the segmentation models in this stylized space. Hence, we discuss some recent work related to the style transfer and unpaired image translation tasks. The popular work of Gatys et al. <ref type="bibr" target="#b7">[8]</ref> introduced an optimization scheme involving backpropagation for performing content preserving style transfer, while Johnson et al. <ref type="bibr" target="#b14">[15]</ref> proposed a feedforward method for the same. CycleGAN <ref type="bibr" target="#b32">[33]</ref> performs unpaired image-to-image translation by employing adversarial losses and cycle consistency losses. In our experiments, we compare our approach to some of these style-transfer based data augmentation schemes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we provide a formal treatment of the proposed approach and explain in detail our iterative optimization procedure. Let X ∈ R M ×N ×C be an arbitrary input image (with C channels) and Y ∈ R M ×N be the corresponding label map. Given an input X, we denote the output of a CNN asŶ ∈ R M ×N ×Nc , where N c is the number of classes.Ŷ (i, j) ∈ R Nc is a vector representing the class probability distribution at pixel location (i, j) output by the CNN. The source(s) or target (t) domains are denoted by a superscript such as X s or X t . First, we provide an input-output description of the different network blocks in our pipeline. Next, we describe separately the treatment of source and target data, followed by a description of the different loss functions and the corresponding update steps. Finally, we motivate the design choices involved in the discriminator (D) architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Description of network blocks</head><p>Our training procedure involves alternatively optimizing the following network blocks:</p><p>(a) The base network, whose architecture is similar to a pre-trained model such as VGG-16, is split into two parts: the embedding denoted by F and the pixel-wise classifier denoted by C. The output of C is a label map up-sampled to the same size as the input of F .</p><p>(b) The generator network (G) takes as input the learned embedding and reconstructs the RGB image.</p><p>(c) The discriminator network (D) performs two different tasks given an input: (a) It classifies the input as real or fake in a domain consistent manner (b) It performs a pixelwise labeling task similar to the C network. Note that (b) is active only for source data since target data does not have any labels during training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Treatment of source and target data</head><p>Given a source image and label pair {X s , Y s } as input, we begin by extracting a feature representation using the F network. The classifier C takes the embedding F (X s ) as input and produces an image-sized label mapŶ s . The generator G reconstructs the source input X s conditioned on the embedding. Following recent successful works on image generation, we do not explicitly concatenate the generator input with a random noise vector but instead use dropout layers throughout the G network. As shown in <ref type="figure" target="#fig_2">Figure 3</ref>, D performs two tasks: (1) Distinguishing the real source input and generated source image as source-real/source-fake (2) producing a pixel-wise label map of the generated source image. Given a target input X t , the generator network G takes the target embedding from F as input and reconstructs the target image. Similar to the previous case, D is trained to distinguish between real target data (target-real) and the generated target images from G (target-fake). However, different from the previous case, D performs only a single task i.e. it classifies the target input as target-real/target-fake. Since the target data does not have any labels during training, the classifier network C is not active when the system is presented with target inputs. <ref type="figure" target="#fig_2">Fig. 3</ref> shows various losses used in our method. We begin by describing these losses, and then describe our iterative optimization approach.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Iterative optimization</head><p>The different adversarial losses used to train our models are shown in <ref type="table">Table.</ref> 1. In addition to these adversarial losses, we use the following losses: (1) L seg and L auxpixel-wise cross entropy loss used in standard segmentation networks such as in FCN and (2) L rec -L 1 loss between input and reconstructed images.</p><p>The directions of flow of information across different network blocks are listed in <ref type="figure" target="#fig_0">Figure 2</ref>. In each iteration, a randomly sampled triplet (X s , Y s , X t ) is provided to the system. Then, the network blocks are updated iteratively in the following order: </p><formula xml:id="formula_0">t adv,D . The overall loss L D is given by L D = L s adv,D + L t adv,D + L s aux .</formula><p>(2) G-update: In this step, the generator is updated using a combination of an adversarial loss</p><formula xml:id="formula_1">L s adv,G + L t adv,G</formula><p>intended to fool D and a reconstruction loss L rec . The adversarial loss encourages realistic output from the generator. The pixelwise L 1 loss is crucial to ensure image fidelity between the generator outputs and the corresponding input images. The overall generator loss is given as:</p><formula xml:id="formula_2">L G = L s adv,G + L t adv,G + L s rec + L t rec .</formula><p>(3) F-update: The update to the F network is the critical aspect of our framework where the notion of domain Classify fake target input as real source (src-real) <ref type="table">Table 1</ref>: Within-domain and Cross-domain adversarial losses that are used to update our networks during training. G and D networks are updated using only the within-domain losses while F is updated only using the cross domain loss. All these adversarial losses originate from the D network. L adv,X implies that the gradients from the loss function L are used to update X only, while the other networks are held fixed.</p><p>shift is captured. The parameters of F are updated using a combination of several loss terms: <ref type="table">Table 1</ref>, the adversarial loss terms used to update F account for the domain adaptation. More specifically, the iterative updates described here can be considered as a min-max game between the F and the G-D networks. During the D update step discussed earlier, the adversarial loss branch of D learns to classify the input images as real or fake in a domain consistent manner. To update F , we use the gradients from D that lead to a reversal in domain classification, i.e. for source embeddings, we use gradients from D corresponding to classifying those embeddings as from target domain (L s adv,F ) and for target embeddings, we use gradients from D corresponding to classifying those embeddings as from source domain (L t adv,F ). Note that, this is similar to the min-max game between the G-D pair, except in this case, the competition is between classifying the generated image as from source/target domains instead of them being real/fake.</p><formula xml:id="formula_3">L F = L seg + α L s aux + β (L s adv,F +L t adv,F ). As illustrated in</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Motivating design choice of D</head><p>• In traditional GANs that are derived from the DC-GAN <ref type="bibr" target="#b24">[25]</ref> implementations, the output of the discriminator is a single scalar indicating the probability of the input being fake or drawn from an underlying data distribution. Recent works on image generation have utilized the idea of Patch discriminator in which the output is a two dimensional feature map where each pixel carries a real/fake probability. This results in significant improvement in the visual quality of their generator reconstructions. We extend this idea to our setting by using a variant of the Patch discriminator, where each pixel in the output map indicates real/fake probabilities across source and target domains hence resulting in four classes per pixel: src-real, src-fake, tgt-real, tgt-fake.</p><p>• In general, GANs are hard to train on tasks which involve realistic images of a larger scale. One promising approach to training stable generative models with the GAN framework is the Auxiliary Classifier GAN (AC-GAN) approach by Odena et al. where they show that by conditioning G during training and adding an auxiliary classification loss to D, they can realize a more stable GAN training and even generate large scale images. Inspired by their results on image classification, we extend their idea to the segmentation problem by employing an auxiliary pixel-wise labeling loss to the D network.</p><p>Both these components prove crucial to our performance.</p><p>The ablation study performed in Section 5.3 shows the effect of the above design choices on the final performance. Specific details about the architectures of these network blocks can be found in the supplementary material.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments and Results</head><p>In this section, we provide a quantitative evaluation of our method by performing experiments on benchmark datasets. We consider two challenging synthetic datasets available for semantic segmentation: SYNTHIA and GTA-5. SYNTHIA <ref type="bibr" target="#b26">[27]</ref> is a large dataset of photo-realistic frames rendered from a virtual city with precise pixellevel semantic annotations. Following previous works ( <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>), we use the SYNTHIA-RAND-CITYSCAPES subset that contains 9400 images with annotations that are compatible with cityscapes. GTA-5 is another large-scale dataset containing 24966 labeled images. The dataset was curated by Richter et al. <ref type="bibr" target="#b25">[26]</ref> and is generated by extracting frames from the computer game Grand Theft Auto V.</p><p>We used CITYSCAPES <ref type="bibr" target="#b3">[4]</ref> as our real dataset. This dataset contains urban street images collected from a moving vehicle captured in 50 cities around Germany and neighboring countries. The dataset comes with 5000 annotated images split into three sets -2975 images in the train set, 500 images in the val set and 1595 images in the test set. In all our experiments, for training our models we used labeled SYNTHIA or GTA-5 dataset as our source domain and unlabeled CITYSCAPES train set as our target domain. We compared the proposed approach with the only two contemporary methods that address this problem: FCN in the wild <ref type="bibr" target="#b13">[14]</ref> and Curriculum Domain adaptation <ref type="bibr" target="#b31">[32]</ref>. Following these approaches, we designate the 500 images from CITYSCAPES val as our test set.</p><p>Architecture In all our experiments, we used FCN-8s as our base network. The weights of this network were initialized with the weights of the VGG-16 <ref type="bibr" target="#b28">[29]</ref> model trained on Imagenet <ref type="bibr" target="#b16">[17]</ref>.</p><p>Implementation details In all our experiments, images were resized and cropped to 1024 × 512. We trained our model for 100, 000 iterations using Adam solver <ref type="bibr" target="#b15">[16]</ref> with a batch size of 1. Learning rate of 10 −5 was used for F and C networks, and 2 × 10 −4 for G and D networks. While evaluating on CITYSCAPES dataset whose images and ground truth annotations are of size 2048 × 1024, we first produce our predictions on the 1024 × 512 sized image and then upsample our predictions by a factor of 2 to get the final label map, which is used for evaluation. Our training codes and additional results are publicly available. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">SYNTHIA -&gt; CITYSCAPES</head><p>In this experiment, we use the SYNTHIA dataset as our source domain, and CITYSCAPES as our target domain. We randomly pick 100 images from the 9400 labeled images of SYNTHIA dataset and use it for validation purposes, the rest of the images are used for training. We use the unlabeled images corresponding to the CITYSCAPES train set for training our model. In order to ensure fairness of experimental results, we followed the exact evaluation protocol as specified by the previous works ( <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>): The 16 common classes between SYNTHIA and CITYSCAPES are chosen used as our labels. The predictions corresponding to the other classes are treated as belonging to void class, and not backpropagated during training. The 16 classes are: sky, building, road, sidewalk, fence, vegetation, pole, car, traffic sign, person, bicycle, motorcycle, traffic light, bus, wall, and rider. <ref type="table">Table 2a</ref> reports the performance of our method in comparison with <ref type="bibr" target="#b13">[14]</ref> and <ref type="bibr" target="#b31">[32]</ref>. The source-only model which corresponds to the no adaptation case i.e. training only using the source domain data achieves a mean IOU of 26.8. The target-only values denote the performance obtained by a model trained using CITYSCAPES train set (supervised training), and they serve as a crude upper bound to the domain adaptation performance. These values were included to put in perspective the performance gains obtained by the proposed approach. We observe that our method achieves a mean IOU of 36.1, thereby improving the baseline by 9.3 points, thus resulting in a higher performance improvement compared to other reported methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">GTA5 -&gt; CITYSCAPES</head><p>In this experiment, we adapt from the GTA-5 dataset to the CITYSAPES dataset. We randomly pick 1000 images from the 24966 labeled images of GTA-5 dataset and use it for validation purpose and use the rest of the images for   training. We use the unlabeled images corresponding to the CITYSCAPES train set for training our model. In order to ensure fairness of experimental results, we followed the exact evaluation protocol as specified by the previous works ( <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b31">[32]</ref>): we use 19 common classes between GTA-5 and CITYSCAPES as our labels. The results of this experiment are reported in <ref type="table">Table.</ref> 2b. Similar to the previous experiment, our baseline performance (29.6) is higher than the performance reported in <ref type="bibr" target="#b13">[14]</ref>, due to difference in network architecture and experimental settings. On top of this, the proposed approach yields an improvement of 7.5 points to obtain a mIOU of 37.1. This performance gain is higher than that achieved by the other compared approaches.</p><p>Note regarding different baselines: The baseline numbers reported by us do not match with the ones reported in <ref type="bibr" target="#b31">[32]</ref> and <ref type="bibr" target="#b13">[14]</ref> due to different experimental settings (this mismatch was also reported in <ref type="bibr" target="#b31">[32]</ref>). However, we would like to point out that we improve over a stronger baseline compared to the other two methods in both our adaptation experiments. In addition, <ref type="bibr" target="#b31">[32]</ref> uses additional data from PASCAL-CONTEXT <ref type="bibr" target="#b23">[24]</ref> dataset to obtain the superpixel segmentation. In contrast, our approach is a single stage end-to-end learning framework that does not use any additional data and yet obtains better performance improvement.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussion</head><p>In this section, we perform several exploratory studies to give more insight into the functionality and effectiveness of the proposed approach. similar to the previous section, all the evaluation results are reported on the CITYSCAPES val set, unless specified otherwise. We denote this set as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Effect of Image Size</head><p>The datasets considered in this paper consists of images of large resolution which is atleast twice larger than the most commonly used Segmentation benchmarks for CNNs i.e. PASCAL VOC (500×300) and MSCOCO (640×480). In this setting, it is instructive to understand the effect of image size on the performance of our algorithm both from a quantitative and computational perspective. <ref type="table" target="#tab_3">Table  3</ref> presents the results of our approach applied over three different image sizes along with the training and evaluation times. It should be noted that the Curriculum DA approach <ref type="bibr" target="#b31">[32]</ref> used a resolution of 640×320. By comparing with our main results in <ref type="table">Table 2a</ref>, we see that our approach provides a higher relative performance improvement over a similar baseline. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Comparison with direct style transfer</head><p>Generative methods for style transfer have achieved a great amount of success in the recent past. A simple approach to performing domain adaptation is to use such approaches as a data augmentation method: transfer the images from the source domain to target domain and use the provided source ground truth to train a classifier on the combined source and target data. In order to compare the proposed approach with this direct data augmentation procedure, we used a state of the art generative approach (Cycle-GAN <ref type="bibr" target="#b32">[33]</ref>) to transfer images from source domain to target domain. From our experiment, using generative approaches solely as a data augmentation method provides only a relatively small improvement over the source-only baseline and clearly suboptimal compared to the proposed approach. However, as shown in a recent approach by Hoffman et al. <ref type="bibr" target="#b12">[13]</ref>, such cross domain transfer can be performed by a careful training procedure. The results obtained by the proposed approach is comparable or better then <ref type="bibr" target="#b12">[13]</ref>. Combining both approaches to produce a much stronger domain adaptation technique for segmentation is under progress.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Component-wise ablation</head><p>In this experiment, we show how each component in our loss function affects the final performance. We consider the following cases: (a) Ours(full): the full implementation of our approach (b) Ours w/o auxiliary pixel-wise loss: Here, the output of the D network is a single branch classifying the input as real/fake. This corresponds to α = 0 in the F -update step. Note that, setting both α and β as zero corresponds to the source-only setting in our experiments. Setting only β = 0 does not improve over the source-only baseline as there is no cross domain adversarial loss. (c) Ours w/o Patch discriminator: Instead of using the D network as a Patch discriminator, we used a regular GAN-like discriminator where the output is a 4-D probability vector that the input image belongs to one of the four classes -srcreal, src-fake, tgt-real and tgt-fake. (d) Feature space based D: In this setting, we remove the G-D networks and apply an adversarial loss directly on the embedding. This is similar to the global alignment setting in the FCN-in-the-wild approach <ref type="bibr" target="#b13">[14]</ref>.</p><p>The mean IoU results on the test set are shown in <ref type="table">Table.</ref> 4. It can be observed that each component is very important to obtain the full improvement in performance. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Cross Domain Retrieval</head><p>A crucial aspect of domain adaptation is in finding good measures of domain discrepancy that provide a good illustration of the domain shift. While there exist several classical measures such as A-distance <ref type="bibr" target="#b0">[1]</ref> and M M D <ref type="bibr" target="#b8">[9]</ref> for the case of image classification, the extension of such measures for a pixel-wise problem such as semantic segmentation is non-trivial. In this section, we devise a simple experiment in order to illustrate how the proposed approach brings source and target distributions closer in the learnt embedding space. We start with the last layer of the F network, which we label as the embedding layer, whose output is a spatial feature map. We perform an average pooling to reduce this spatial map to a 4096 dimensional feature descriptor for each input image.</p><p>We begin the cross domain retrieval task by choosing a pool of N = N src +N tgt images from the combined source and target training set. Let X denote these set of images and F X denote the set of the feature descriptors computed for X. Then, we choose two query sets, one consisting of source images (S) and the other consisting of target images (T ), each disjoint with X. Let the corresponding feature sets be denoted as Q S and Q T . We retrieve k-NN lists for each item in the query set from the combined feature set F X . For each query point in Q S , we count the number of target samples retrieved in the corresponding k-NN list. |A k | indicates the average number of target samples retrieved over the entire source query set Q S . For each query point in Q T , we count the number of source samples retrieved in the corresponding k-NN list. |B k | indicates the average number of source samples retrieved over the entire target query set Q T . We used cosine similarity as a metric to compute the k-NN lists. If more target samples are retrieved for a source query point (and vice-versa), it suggests that source and target distributions are aligned well in the feature space. <ref type="figure">Figure 4</ref>: Illustration of Domain Adaptation achieved by the proposed approach. The plot compares the average number of retrieved sampled for the cross domain retrieval task described in Section 5.4 between the source-only model and the model adapted using the proposed approach. Target → Source implies that the query set used belongs to target domain (Q T ) and items queried for from the set X belong to the source domain and vice-versa for Source → Target. In general, the values plotted on the y-axis corresponds to the number of samples retrieved from the set X that belong to the opposite domain as to that of the query set.</p><formula xml:id="formula_4">(a) Target → Source, |B k | (vs) k (b) Source → Target, |A k | (vs) k</formula><p>For this experiment, the sizes of query sets and the feature set F X are as follows: N src = N tgt = 1000, |Q S | = 1000, |Q T | = 1000. The mean average precision (mAP) was computed across the entire query sets for the respective cross domain tasks. <ref type="figure">Figure 4</ref> shows the plot of the quantities |A k | <ref type="figure">(Fig.4b)</ref> and |B k | <ref type="figure">(Fig.4a)</ref> for a range of values of k. It can be observed from the plots in both the tasks that for any given rank k, the number of cross domain samples retrieved by the adapted model is higher than the sourceonly model. This effect becomes more clear as k increases. This observation is supported by better mAP values for the adapted model as shown in <ref type="figure">Figure 4</ref>. While this by itself is not a sufficient condition for better segmentation performance, however this along with the results from <ref type="table">Table 2</ref> imply that the proposed approach performs domain adaptation in a meaningful manner. Owing to the difficulty in visualizing the mapping learned for segmentation tasks, a cross domain retrieval experiment can be seen as a reasonable measure of how domain gap is reduced in the feature space.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">Generalization to unseen domains</head><p>A desirable characteristic of any domain adaptation algorithm is domain generalization i.e. improving performance over domains that are not seen during training. To test the generalization capability of the proposed approach, we test the model trained for the SYNTHIA → CITYSCAPES setting on the CamVid dataset <ref type="bibr" target="#b2">[3]</ref>. We choose to evaluate our models on the 10 common classes among the three datasets. <ref type="table" target="#tab_5">Table 5</ref> shows the mean IoU values computed for the source-only baseline and the adapted model. The proposed approach yields a raw improvement of 8.3 points in performance which is a significant improvement considering the fact that CamVid images are not seen by the adapted model during training. This experiment showcases the ability of the proposed approach to learn domain invariant representations in a generalized manner. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>In this paper, we have addressed the problem of performing semantic segmentation across different domains. In particular, we have considered a very hard case where abundant supervisory information is available for synthetic data (source) but no such information is available for real data (target). We proposed a joint adversarial approach that transfers the information of the target distribution to the learned embedding using a generator-discriminator pair. We have shown the superiority of our approach over existing methods that address this problem using experiments on two large scale datasets thus demonstrating the generality and scalability of our training procedure. Furthermore, our approach has no extra computational overhead during evaluation, which is a critical aspect when deploying such methods in practice. As future work, we would like to extend this approach to explicitly incorporate geometric constraints accounting for perspective variations and to adapt over temporal inputs such as videos across different domains.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>The Authors acknowledge support of the following organisations for sponsoring this work: (1) Avitas Systems, a GE venture (2) MURI from the Army Research Office under the Grant No. W911NF-17-1-0304. This is part of the collaboration between US DOD, UK MOD and UK Engineering and Physical Research Council (EPSRC) under the Multidisciplinary University Research Initiative.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The directions of data flow solid arrows during the forward pass and gradient flow dotted arrows during the backward pass of our iterative update procedure. Solid blocks indicate that the block is frozen during that update step while dotted block indicate that it is being updated. Red denoted source information and Blue denotes target information.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( 1 )</head><label>1</label><figDesc>D-update: For source inputs, D is updated using a combination of within-domain adversarial loss L s adv,D and auxiliary classification loss L s aux . For target inputs, it is updated using only the adversarial loss L</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: During training, the F and C networks are trained jointly with the adversarial framework(G-D pair). F is updated using a combination of supervised loss and an adversarial component. In the bottom right, we show the test time usage. Only the F and C network blocks are used. There is no additional overhead during evaluation compared to the base model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 :</head><label>2</label><figDesc>Results of Semantic Segmentation by adapting from (a) SYTNHIA to CITYSCAPES and (b) GTA-5 to CITYSCAPES. We compare with two approaches that use two different base networks. To obtain a fair idea about our performance gain, we compare with the Curriculum DA approach that uses the same base network as ours. The Target-only training procedure is the same for both the settings since in both cases the target domain is CITYSCAPES. However, the results in (a) are reported over the 16 common classes while the results in (b) are reported over all the 19 classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Mean IoU values and computation times across different image size on the SYNTHIA → CITYSCAPES setting. The numbers in bold indicate the absolute improve- ment in performance over the Source-only baseline. The re- ported training and evaluation times are for the proposed ap- proach and are averaged over training and evaluation runs.</figDesc><table>Image size 
512 × 256 
640 × 320 
1024 × 512 
mIOU-Source-only 
21.5 
23.2 
26.8 
mIOU-Ours 
31.3 (+9.8) 34.5 (+11.3) 36.1 (+9.3) 
Train time (per image) 
1.5s 
2.1s 
2.9s 
Eval time (per image) 
0.16s 
0.19s 
0.3s 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Ablation study showing the effect of each compo- nent on the final performance of our approach on the SYN- THIA → CITYSCAPES setting</figDesc><table>Method 
mean IoU 
Source-only 
22.2 
Feature space based D 
25.3 
Ours w/o Patch Discriminator 
28.3 
Ours w/o auxiliary loss (α = 0) 
29.2 
Ours 
34.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Mean IoU segmentation performance measured on a third unseen domain (CamVid dataset) for the models cor- responding to the SYNTHIA → CITYSCAPES setting</figDesc><table>Method 
mean IoU 
Source-only 
36.1 
Ours 
44.4 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Training code: https://goo.gl/3Jsu2s</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Analysis of representations for domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="137" to="144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Unsupervised pixel-level domain adaptation with generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Bousmalis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Krishnan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.05424</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Semantic object classes in video: A high-definition ground truth database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Fauqueur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition Letters</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="88" to="97" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daume</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 45th Annual Meeting of the Association of Computational Linguistics</title>
		<meeting>the 45th Annual Meeting of the Association of Computational Linguistics</meeting>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Int. J. Comput. Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation by backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.7495</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno>abs/1508.06576</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Daml: Domain adaptation metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Geng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2980" to="2989" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2012 IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2066" to="2073" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Domain adaptation for object recognition: An unsupervised approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2011 International Conference on Computer Vision, ICCV &apos;11</title>
		<meeting>the 2011 International Conference on Computer Vision, ICCV &apos;11</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1711.03213</idno>
		<title level="m">Cycada: Cycle-consistent adversarial domain adaptation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Fcns in the wild: Pixel-level adversarial and constraint-based adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1612.02649</idno>
		<imprint>
			<date type="published" when="2005" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision -ECCV 2016 -14th European Conference</title>
		<meeting><address><addrLine>Amsterdam, The Netherlands</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="694" to="711" />
		</imprint>
	</monogr>
	<note>Proceedings, Part II</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. CoRR, abs/1412</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6980</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y.</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Coupled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tuzel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="469" to="477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning</title>
		<meeting>the 32nd International Conference on Machine Learning</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="97" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<idno>abs/1602.04433</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 29: Annual Conference on Neural Information Processing Systems</title>
		<meeting><address><addrLine>Barcelona, Spain</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016-12-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Playing for data: Ground truth from computer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Richter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vineet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<editor>B. Leibe, J. Matas, N. Sebe, and M. Welling</editor>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9906</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">The synthia dataset: A large collection of synthetic images for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sellart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Materzynska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3234" to="3243" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sankaranarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01705</idno>
		<title level="m">Generate to adapt: Aligning domains using generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition. CoRR, abs/1409.1556</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1702.05464</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Curriculum domain adaptation for semantic segmentation of urban scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Unpaired image-to-image translation using cycle-consistent adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2017-10" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
