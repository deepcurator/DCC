<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-End Learning of Motion Representation for Video Understanding</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lijie</forename><surname>Fan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenbing</forename><surname>Huang</surname></persName>
							<email>hwenbing@126.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chuang</forename><surname>Gan</surname></persName>
							<email>ganchuang1990@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="department">MIT-Watson Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Ermon</surname></persName>
							<email>ermon@cs.stanford.edu</email>
							<affiliation key="aff3">
								<orgName type="department">Department of Computer Science</orgName>
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Boqing</forename><surname>Gong</surname></persName>
							<email>boqinggo@outlook.com</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junzhou</forename><surname>Huang</surname></persName>
							<email>jzhuang@uta.edu</email>
							<affiliation key="aff0">
								<orgName type="department">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-End Learning of Motion Representation for Video Understanding</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Despite the recent success of end-to-end learned representations, hand-crafted   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep learning and especially Convolutional Neural Networks (CNNs) have revolutionized image-based tasks, e.g., image classification <ref type="bibr" target="#b15">[16]</ref> and objective detection <ref type="bibr" target="#b31">[32]</ref>. However, the progress on video analysis is still far from satisfactory, reflecting the difficulty associated with learning representations for spatiotemporal data. We believe that the major obstacle is that the distinctive motion cues in videos demand some new network designs, which are yet to be found and tested.</p><p>While there have been some attempts <ref type="bibr" target="#b35">[36]</ref> to learn features by convolution operations over both spatial and temporal dimensions, optical flow is still widely and effectively used for video analysis <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b28">29]</ref>. The optical flow, as the name implies, captures the displacements of pixels between two consecutive frames <ref type="bibr" target="#b41">[42]</ref>. Thus, applying the optical flow to the video understanding tasks enables one to model the motion cues explicitly, conveniently, but inefficiently. It is often computationally expensive to estimate the optical flows. A currently successful example of applying optical flow to video understanding is the twostream model <ref type="bibr" target="#b32">[33]</ref>, where a CNN is trained on the optical flow data to learn action patterns. Various extensions of the two-stream model have been proposed and achieved state-ofthe-art results on serval tasks including action recognition <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b39">40]</ref> and action detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b28">29]</ref>.</p><p>Despite the remarkable performance, current optical-flowbased approaches have notable drawbacks:</p><p>• Training is a two-stage pipeline. In the first stage, the optical flow for every two consecutive frames is extracted via the optimization-based method (e.g. TV-L1 <ref type="bibr" target="#b41">[42]</ref>). In the second stage, a CNN is trained on the extracted flow data. These two stages are separated and the information (e.g. gradients) from the second stage cannot be used to adjust the process of the first stage.</p><p>• Optical flow extraction is expensive in space and time. The extracted optical flow has to be written to the disk for both the training and testing. For the UCF-101 dataset <ref type="bibr" target="#b33">[34]</ref> which contains about 10 thousands videos, extracting optical flows for all data via the TV-L1 method takes one GPU-day, and storing them costs more than one TeraByte of storage for the original fields as floats (often a linear JPEG normalization is required to save storage cost <ref type="bibr" target="#b32">[33]</ref>).</p><p>To tackle the above mentioned problems, we propose a novel neural network design for learning optical-flow like features in an end-to-end manner. This network, named TVNet, is obtained by imitating and unfolding the iterative optimization process of TV-L1 <ref type="bibr" target="#b41">[42]</ref>. In particular, we formulate the iterations in the TV-L1 method as customized layers of a neural network. As a result, our TVNet is well-grounded and can be directly used without additional training by any groundtruth optical flows.</p><p>Furthermore, our TVNet is end-to-end trainable, and can therefore be naturally connected with a tasks-specific network (e.g. action classification network) to form a "deeper" end-to-end trainable architecture. As a result, it is not necessary to pre-compute or store the optical-flow features anymore.</p><p>Finally, by performing the end-to-end learning, it is possible to fine-tune the weights of the TVNet that is initialized as a standard optical flow feature extractor. This allows us to discover richer and task-specific features (compared to the original optical flow) and thus to deliver better performance.</p><p>To verify the effectiveness of the proposed architecture, we perform experimental comparisons between the proposed TVNet and several competing methods on two action recognition benchmarks (HMDB51 <ref type="bibr" target="#b23">[24]</ref> and UCF101 <ref type="bibr" target="#b33">[34]</ref>).</p><p>To sum up, this paper makes the following contributions:</p><p>• We develop a novel neural network to learn motions from videos by unfolding the iterations of the TV-L1 method to customized neural layers. The network, dubbed TVNet, is well-initialized and end-to-end trainable.</p><p>• Despite being initialized as a specific TV-L1 architecture, the proposed TVNet can be further fine-tuned to learn richer and more task-oriented features than the standard optical flow.</p><p>• Our TVNet achieves better accuracies than other action representation condunterparts (e.g., TV-L1 <ref type="bibr" target="#b41">[42]</ref>, FlowNet2.0 <ref type="bibr" target="#b17">[18]</ref>) and 3D Convnets <ref type="bibr" target="#b35">[36]</ref> on the two action recognition benchmarks, i.e.,72.6% on HMDB51 and 95.4% on UCF101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video understanding, such as action recognition and action similarity detection, has attracted a lot of research attention in the past decades. Different from static image understanding, video understanding requires more reliable motion features to reflect the dynamic changes occurring in videos. Laptev et al. <ref type="bibr" target="#b24">[25]</ref> proposed a spatio-temporal interest points (STIPs) method by extending Harris corner detectors to 3-dimensional space to capture motion. Similarly, the 3D extensions of SIFT and HOG have also been investigated <ref type="bibr" target="#b6">[7]</ref> and <ref type="bibr" target="#b21">[22]</ref>, respectively. Wang et al. <ref type="bibr" target="#b36">[37]</ref> proposed improved Dense Trajectories (iDT), where the descriptors were obtained by tracking densely sampled points and describing the volume around the tracklets by histograms of optical flow (HOF) and motion boundary histograms (MBH). Despite its stat-of-the-art performances, IDT is computationally expensive and becomes intractable on large-scale video dataset.</p><p>Motivated by the promising results of deep networks on image understanding tasks, there have also been a number of attempts to develop deep architectures to learn motion features for video understanding <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b42">43,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b11">12]</ref>. The leading approaches fall into two broad categories. The first one is to learn appearance and motion jointly by extending 2D convolutional layers to 3D counterparts <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b19">20]</ref>, including recently proposed I3D <ref type="bibr" target="#b5">[6]</ref> and P3D <ref type="bibr" target="#b29">[30]</ref>. However, modeling motion information through 3D convolutional filters is computationally expensive, and large-scale training videos are needed for desired performance <ref type="bibr" target="#b5">[6]</ref>. The other category of work is based on two-stream networks <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b39">40,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b25">26]</ref>. This line of approaches trains two networks, one using the appearance (i.e., RGB) data and the other one using hand-crafted motion features such as optical flow to represent motion patterns. In contrast, in our method, the motion descriptor is learned with a trainable neural network rather than hand-crafted. As a consequence, our optical-flow-like motion features can be jointly learned and fine-tuned using a task-specific network. Additionally, we do not need to store and read the optical flow from disk, leading to significant computational gains.</p><p>A recent research topic is to estimate optical flow by CNNs <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b3">4]</ref>. These approaches cast the optical flow estimation as an optimization problem with respect to the CNN parameters. A natural idea is to combine the flow CNN with the task-specific network to formulate an end-to-end model (see for example in <ref type="bibr" target="#b44">[45]</ref>). Nevertheless, an obvious issue of applying the flow nets is that they require thousands of hundreds of groundtrue flow images to train the parameters of the flow network to produce meaningful optical flows (see <ref type="bibr" target="#b7">[8]</ref>). For real applications, it is costly to obtain the labeled flow data. In contrast, our network is well initialized as a particular TV-L1 method and is able to achieve desired performance even in its initial form (without fine-tuning).</p><p>Recently, Ng et al. <ref type="bibr" target="#b26">[27]</ref> proposed to train a single stream convolutional neural network to jointly estimate optical flow and recognize actions, which is most relevant to our work. To capture the motion feature, they formulated FlowNet <ref type="bibr" target="#b10">[11]</ref> to learn the optical flow from synthetic ground truth data.</p><p>Though the results are promising, the approach still lags behind the state of the arts in terms of accuracy compared to traditional approaches. This is due to the well known gap between synthetic and real videos. Contrastly, our network is formulated by unfolding the TV-L1 method that has been applied successfully to action recognition and we do not rely on the groundtruth of optical flow for training. Thus, our network combines the strengths of both TV-L1 and deep learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Notations and background</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Notations</head><p>A video sequence can be written as a function of three arguments, I t (x, y), where x, y index the spatial dimensions t is for the time dimension. Denote by Ω all the coordinates of the pixels in a frame. The function value I t (x, y) corresponds to the pixel brightness at position x = (x, y) ∈ Ω in the t-th video frame. A point x may move from time to time across the video frames, and the optical flow is to track such displacement between adjacent frames. We denote by</p><formula xml:id="formula_0">u t (x) = (u t 1 (x), u t 2 (x)</formula><p>) the displacement of the point x from time t to the next frame t + 1. We omit the superscript t and/or argument x from u t (x) when no ambiguity is caused.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">The TV-L1 method</head><p>Among the existing approaches to estimating optical flows, the TV-L1 method <ref type="bibr" target="#b41">[42]</ref> is especially appealing for its good balance between efficiency and accuracy. We review it in detail in this subsection to make the paper self-contained. The design of our TV-Net (cf. Section 4) is directly motivated by the optimization procedure of TV-L1.</p><p>The main formulation of TV-L1 is as follows,</p><formula xml:id="formula_1">min u(x),x∈Ω x∈Ω (|∇u 1 (x)| + |∇u 2 (x)|) + λ|ρ(u(x))|,<label>(1)</label></formula><p>where the first term |∇u 1 | + |∇u 2 | accounts for the smoothness condition, while the second term ρ(u) corresponds to the famous brightness constancy assumption <ref type="bibr" target="#b41">[42]</ref>. In particular, the brightness of a point x is assumed to remain the same after it shifts to a slightly different location in the next frame, i.e.,</p><formula xml:id="formula_2">I 0 (x + u) ≈ I 1 (x). Accordingly, ρ(u) = I 1 (x + u) − I 0 (x)</formula><p>is defined in order to penalize the brightness difference in the second term. Since the function I 1 (x + u) is highly non-linear with respect to u, Zach et al. <ref type="bibr" target="#b41">[42]</ref> approximate the brightness difference ρ(u) by the Taylor expansion at an initial displacement u 0 , leading to</p><formula xml:id="formula_3">ρ(u) ≈ ∇I 1 (x + u 0 )(u − u 0 ) + I 1 (x + u 0 ) − I 0 (x).</formula><p>The above gives a first-order approximation to the original problem and linearizes it to an easier form. Furthermore, the authors introduce an auxiliary variable v to enable a convex relaxation of the original problem min {u,v} x∈Ω</p><formula xml:id="formula_4">(|∇u 1 | + |∇u 2 |) + 1 2θ |u − v| 2 + λ|ρ(v)|,<label>(2)</label></formula><p>Algorithm 1 The TV-L1 method for optical flow extraction. Hyper-parameters: λ, θ, τ, ǫ, N warps , N iters Input:</p><formula xml:id="formula_5">I 0 , I 1 , u 0 p 1 = [p 11 , p 12 ] = [0, 0]; p 2 = [p 21 , p 22 ] = [0, 0]; for w = 1 to N warps do Warp I 1 (x + u 0 ), ∇I 1 (x + u 0 ) by interpolation; ρ(u) = ∇I 1 (x + u 0 )(u − u 0 ) + I 1 (x + u 0 ) − I 0 (x), n = 0; while n &lt; N iters and stopping_criterion &gt; ǫ do v =    λθ∇I 1 ρ(u) &lt; −λθ|∇I 1 | 2 , −λθ∇I 1 ρ(u) &gt; λθ|∇I 1 | 2 , −ρ(u) ∇I1 |∇I1| 2 otherwise, where ∇I 1 represents ∇I 1 (x + u 0 ) for short; u d = v + θdiv(p d ), d = 1, 2; p d = p d +τ /θ∇u d 1+τ /θ|∇u d | , d = 1, 2; n = n + 1; end while end for</formula><p>in which a very small θ can force u and v to be equal at the minimum. This objective is minimized by alternatively updating u and v. The details of the optimization process are presented in Algorithm 1, where the variables p 1 and p 2 are the dual flow vector fields.</p><p>Understanding Algorithm 1. The core computation challenge in the algorithm is on the pixel-wise computations of the gradients (i.e., ∇I 1 and ∇u d ), divergence (i.e., div(p)), and warping (i.e., I 1 and ∇I 1 ). The details of the numerical estimations are provided as below.</p><p>• Gradient-1. The gradient of the image I 1 is computed by central difference:</p><formula xml:id="formula_6">∂I 1 (i, j) ∂x = I1(i+1,j)−I1(i−1,j) 2 1 &lt; i &lt; W, 0 otherwise.<label>(3)</label></formula><p>We can similarly compute</p><formula xml:id="formula_7">∂ ∂y I 1 (i, j) along the j index.</formula><p>• Gradient-2. The gradient of each component of the flow u is computed via the forward difference:</p><formula xml:id="formula_8">∂u d (i, j) ∂x = u d (i + 1, j) − u d (i, j) 1 ≤ i &lt; W, 0 otherwise,<label>(4)</label></formula><p>where d ∈ {1, 2}. Also, ∂ ∂y u d (i, j) can be similarly computed by taking the difference on the j index.</p><p>• Divergence. The divergence of the dual variables p is computed via the backward difference: </p><formula xml:id="formula_9">div(p d )(i, j) =    p d1 (i, j) − p d1 (i − 1, j) 1 &lt; i &lt; W, p d1 (i, j) i = 1, −p d1 (i − 1, j) i = W. +    p d2 (i, j) − p d2 (i, j − 1) 1 &lt; j &lt; H, p d2 (i, j) j = 1, −p d2 (i, j − 1) j = H.<label>(5)</label></formula><p>Another pixel-wise estimation is the brightness I 1 (x + u 0 ) . It is often obtained by warping the frame I 1 along the initial flow field u 0 using the bicubic interpolation. Multi-scale TV-L1. Since the Taylor expansion is applied to linearize the brightness difference, the initial flow field u 0 should be close to the real field u to ensure a small approximation error. To achieve this, the approximation field u 0 is derived by a multi-scale scheme in a coarse-to-fine manner. To be specific, at the coarsest level, u 0 is initialized as the zero vectors and the corresponding output of Algorithm 1 is applied as the initialization of the next level 1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">TVNets</head><p>This section presents the main contribution of this paper, i.e., the formulation of TVNet. The central idea is to imitate the iterative process in TV-L1 and meanwhile unfold the iterations into a layer-to-layer transformations, in the same spirit as the neural networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Network design</head><p>We now revisit Algorithm 1 and convert its key components to a neural network. First, the iterations in Algorithm 1 can be unfolded as a fixed-size feed-forward network if we <ref type="bibr" target="#b0">1</ref>  <ref type="figure" target="#fig_0">Figure 1</ref> in the supplementary material demonstrates the framework with three-scale optimization.</p><p>fix the number of the iterations within the while-loop to be N iters (see <ref type="figure" target="#fig_1">Figure 2)</ref>. Second, each iteration (i.e. layer) is continuous and is almost everywhere smooth with respect to the input variables. Such property ensures that the gradients can be back-propagated through each layer, giving rise to an end-to-end trainable system.</p><p>Converting Algorithm 1 into a neural network involves efficiency and numerical stability considerations. To this end, we modify Algorithm 1 by replacing the computations of the gradients and divergence Eq. <ref type="formula" target="#formula_6">(3)</ref>- <ref type="formula" target="#formula_9">(5)</ref> with specific convolutions, performing warping with bilinear interpolation, and stabilizing the division calculations with a small threshold. We provide the details below.</p><p>Convolutional computation. The most tedious part in Algorithm 1 is the pixel-wise computation of Eq. (3)-(5). We propose to perform all these calculations with specific convolutional layers. We define the following kernels,</p><formula xml:id="formula_10">w c = [0.5, 0, −0.5], w f = w b = [−1, 1].<label>(6)</label></formula><p>Thus, for the pixels in the valid area (1 &lt; i &lt; W ), Eq. (3)-(4) can be equivalently written as ∂ ∂x</p><formula xml:id="formula_11">I 1 = I 1 * w c ,<label>(7)</label></formula><formula xml:id="formula_12">∂ ∂x u d = u d * w f ,<label>(8)</label></formula><p>where * defines the convolution operation. Eq. (6) only describes the kernels along the x axis. We transpose them to obtain the kernels along the y axis.</p><p>The divergence in Eq. <ref type="formula" target="#formula_9">(5)</ref> is computed by a backward difference, but the convolution is computed in a forward direction. To rewrite Eq.(5) in convolution form, we need to first shift the pixels of p d1 right (and shift p d2 down) by one pixel and pad the first column of p d1 (and the first row of p d2 ) with zeros, leading top d1 (andp d2 ). Then, Eq.(5) can be transformed to Bilinear-interpolation-based warping. The original TV-L1 method uses bicubic interpolation for the warping process. Here, for efficiency reasons, we adopt bilinear interpolation instead. Note that the bilinear interpolation has been applied successfully in previous works such as the spatial transformer network <ref type="bibr" target="#b18">[19]</ref> and the optical flow extraction method <ref type="bibr" target="#b10">[11]</ref>. We denote by I w 1 = I 1 (x + u 0 ) the warping. Then, we compute</p><formula xml:id="formula_13">div(p d ) =p d1 * w b +p d2 * w T b ,<label>(9)</label></formula><formula xml:id="formula_14">I w 1 (i, j) = H n W m I 1 (m, n) max(0, 1 − |i + u 1 − m|) max(0, 1 − |j + u 2 − n|),<label>(10)</label></formula><note type="other">where u 1 and u 2 are respectively the horizontal and vertical flow values of u 0 at position (i, j). We follow the details in [19] and derive the partial gradients for Eq. (10) with respect to u 0 as the bilinear interpolation is continuous and piecewise smooth.</note><p>Numerical stabilization. We need to take care of the division in Algorithm 1, i.e., v = −ρ(u) ∇I1 |∇I1| 2 . The operation is ill-defined when the denominator is equal to zero. To avoid this issue, the original TV-L1 method checks whether the value of |∇I 1 | 2 is bigger than a small constant; if not, the algorithm will set the denominator to be this small constant. Here, we utilize a soft non-zero transformation by rewriting the update of v as v = −ρ(u) ∇I1 |∇I1| 2 +ε , where a small value ε &gt; 0 is added to the denominator. This transformation is more efficient as we do not need to explicitly check the value of |∇I 1 | 2 at each step. Another division computation in Algorithm 1 is</p><formula xml:id="formula_15">p d = p d +τ /θ∇u d 1+τ /θ|∇u d | .</formula><p>At first glance, this division is safe since the denominator is guaranteed to be larger than 1. However, as we will see later, its gradients contain division computations where the denominators can be zero. Thus, we apply the soft transformation by adding a small value ε &gt; 0 to the denominator, namely,</p><formula xml:id="formula_16">p d = p d + τ /θ∇u d 1 + τ /θ ∇u 2 d1 + ∇u 2 d2 + ε .</formula><p>(11) The gradient of p d with respect to ∇u d1 is in this form</p><formula xml:id="formula_17">∂ ∂∇u d1 p d = a − b ∇u 2 d1 + ∇u 2 d2 + ε ,<label>(12)</label></formula><p>where a and b are well-defined variables (the details are provided in the supplementary material). In practice, both ∇u d1 and ∇u d2 are often equal to zero within the still area of the image (e.g., the background). As such, the computation of the gradients would encounter a division by zero if the positive term ε was not added in Eq. <ref type="bibr" target="#b11">(12)</ref>. Multi-scale version. The multi-scale TVNet is formulated by directly unfolding the multi-scale version of TV-L1. A higher scale takes as input the up-sampled output of its immediate lower scale. There are multiple warps at each scale and each warp consists of multiple iterations. Hence, the total number of iterations of the multi-scale TVNets are N scales × N warps × N iters .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Going beyond TV-L1</head><p>In the previous section, we have transformed the TV-L1 algorithm to a feed-forward network. However, such network is parameter-free and not learnable. To formulate a more expressive network, we relax certain variables in TV-L1 to be trainable parameters. Relaxing the variables render TVNet not equivalent to TV-L1 any more. However, it allows the network to learn more complex, task-specific feature extractors by end-to-end training.</p><p>The first variable we relax is the initialization optical field u 0 . In TV-L1, u 0 is set to be zero. However, from the optimization prospective, zero initialization is not necessarily the best choice; making u 0 trainable will enable us to automatically determine a better initialization for the optimization. We also propose to relax the convolutional filters in Eq. <ref type="formula" target="#formula_11">(7)</ref>- <ref type="bibr" target="#b8">(9)</ref>. The original convolutions are used to derive the (numerical) gradients and divergences. Allowing the convolutional filters to be trainable parameters will enable them to discover more complex patterns in a data-driven way. We will demonstrate the benefit of the trainable version compared to the original architecture in our experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Multi-task Loss</head><p>As discussed before, our TVNet can be concatenated to any task-specific networks (e.g., the BN-Inception net for action classification <ref type="bibr" target="#b39">[40]</ref>) to perform end-to-end action recogntion without the need of explicitly extracting the optical flow data, as illustrated in <ref type="figure" target="#fig_1">Figure 2 (c)</ref>. Because of the end-to-end structure, the parameters of TVNet can be fine-tuned by back-propagating gradients of the task-specific loss. Additionally, since the original TV-L1 method is developed to minimize the energy function in Eq. (1), we can also use this function as an additional loss function to force it to produce meaningful optical-flow-like features. To this end, we formulate a multi-task loss as</p><formula xml:id="formula_18">L = L c + λL f .<label>(13)</label></formula><p>Here L c is the action classification loss (e.g. the cross entropy), L f is defined in Eq. <ref type="formula" target="#formula_1">(1)</ref> where the exact computation other than the Tailor approximation is applied to compute ρ(u(x)), and λ is a hyper-parameter to trade-off these two losses. We set λ = 0.1 in all our experiments and find that it works well across all of them. Note that it is tractable to compute the gradients of L f as it has been translated to convolutions and the bilinear interpolation (see § 4.1).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>This section performs experimental evaluations to verify the effectiveness of the proposed TVNet. We first carry out a complete comparison between TVNets of various structures with the TV-L1 method regarding the optimization efficiency. Then, we compare the performance of TVNets with state-of- the-art methods on the task of action recognition 2 . The three hyper-parameters, N scales , N warps and N iters determine the structure of the TVNet. For convenience, we denote the TVNet with particular values of the hyperparameters as TVNet-N scales -N warps -N iters . We denote the architecture as TVNet-N iters for short when both N scales and N warps are fixed to be 1. For the TV-L1 method, the hyper-parameters are fixed as N scales = N warps = 5 and N iters = 50 in all experiments unless otherwise specified. Our methods are implemented by the Tensorflow platform <ref type="bibr" target="#b0">[1]</ref>. Unless otherwise specified, all experiments were performed on 8 Tesla P40 GPUs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Comparison with TV-L1</head><p>Initialized as a particular TV-L1 method, the parameters of TVNet can be further finetuned as discussed in Section 4.2. Therefore, it is interesting to evaluate how much the training process can improve the final performance. For this purpose, we compare the estimation errors between TVNet and TV-L1 on the optical flow dataset, i.e., the MiddleBurry dataset <ref type="bibr" target="#b1">[2]</ref>.</p><p>Dataset. The MiddleBurry dataset <ref type="bibr" target="#b1">[2]</ref> is a widely-used benchmark for evaluating different optical flow extraction methods. Here we only perform evaluation on the training set as we are merely concerned about the training efficiency of TVNets. For the training set, only 8 image pairs are provided with the ground-true optical flow.</p><p>Implementation details. The estimation errors are measured via the average End-Point Error (EPE) defined by</p><formula xml:id="formula_19">EP E . = 1 N N i=1 (u 1,i − u gt 1,i ) 2 + (u 2,i − u gt 2,i ) 2 ,<label>(14)</label></formula><p>where (u 1,i , u 2,i ) and (u gt 1,i , u gt 2,i ) are the predicted and ground-true flow fields, respectively. For the training of TVNets, we adopt the EPE (Eq. <ref type="formula" target="#formula_1">(14)</ref>) as the loss function, and apply the batch gradient decent method with the learning rate and max-iteration being 0.05 and 3000, respectively. Several structures, i.e., TVNet-10, TVNet-30, TVNet-50, <ref type="bibr" target="#b1">2</ref> We also provide additional experimental evaluations on action similarity labeling in the supplementary material.  <ref type="figure">Figure 3</ref>. Examples of flow fields from TV-L1 and TVNet-50 estimated on MiddleBurry. With training, TVNet-50 is able to extract finer details than TV-L1 does. <ref type="table">Table 2</ref>. The execution speed of different flow extraction methods. Only one gpu is used for the evaluations. As no ground-truth is given on UCF101, we apply the term ρ(u) (Eq. <ref type="formula" target="#formula_1">(1)</ref>) instead of End-Point-Error to measure the optical flow error. TVNet-50 achieves the fastest speed among all the methods. Theoretically, since TVNet-50 has a much smaller number of iterations than TV-L1 (i.e. 50 v.s. 1250), the speed of TVNet-50 should be more than 100 times faster than TV-L1. However, due to the different implementations of TV-L1 and TVNet, the real computational reduction of TVNet is not so big. As the TVNet-50 is implemented by Tensorflow, we can easily perform parallel flow extraction with TVNet-50 by enlarging the batch size of input images (e.g., setting batch = 10); as such, the FPS will be further improved to 60.  <ref type="table">Table 3</ref>. Classification accuracy of various motion descriptors on HMDB51 (split 1) and UCF101 (split 1).The top part shows the results of current best action representation methods; the middle part reports the accuracies of the four baselines; the bottom part presents the performance of our models. TVNet-50 achieves the best results on both datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Methods FPS Flow Errors</head><p>Methods HMDB51 UCF101 C3D <ref type="bibr" target="#b35">[36]</ref> -82.3% ActionFlowNet <ref type="bibr" target="#b26">[27]</ref> 56 TVNet-3-10, TVNet-1-3-10, and their counterparts of TV-L1 are implemented to compare the difference between different network designs.</p><p>Results. We have performed one-to-one comparisons between TVNets and TV-L1 on MiddleBurry in <ref type="table" target="#tab_0">Table 1</ref>. Given the same architecture, TVNet without training achieves close performance to TV-L1. This is not surprising since TVNet and TV-L1 are almost the same except the way of interpolation (bilinear vs. bicubic). To further evaluate the effect of training u 0 , we conduct additional experiments and report the results in <ref type="table" target="#tab_0">Table 1</ref>. Clearly, making u 0 trainable in TVNets can indeed reduce the End-Point Error. With training both u 0 and the convolution filters, all TVNets except TVNet-10 achieve lower errors than TV-L1-5-5-50, even though the number of iterations in TVNets (not more than 50) are much smaller than that of TV-L1-5-5-50 (up to 1250). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Action recognition</head><p>Dataset. Our experiments are conducted on two popular action recognition datasets, namely the UCF101 <ref type="bibr" target="#b33">[34]</ref> and the HMDB51 <ref type="bibr" target="#b23">[24]</ref> datasets. The UCF101 dataset contains 13320 videos of 101 action classes. The HMDB51 dataset consists of 6766 videos from 51 action categories.</p><p>Implementation details. As discussed before, our TVNets can be concatenated by a classification network to formulate an end-to-end model to perform action recognition. We apply the BN-Inception network <ref type="bibr" target="#b39">[40]</ref> as the classification model in our experiments due to its effectiveness. The BN-Inception network is pretrained by the cross-modality skill introduced in <ref type="bibr" target="#b38">[39]</ref> for initialization.</p><p>We sample a stack of 6 consecutive images from each video and extract 5 flow frames for every consecutive pair. The resulting stack of optical flows are fed to the BNInception network for prediction. To train the end-to-end model, we set the mini-batch size of the sampled stacks to 128 and the momentum to 0.9. The learning rate was initialized to 0.005. The maximum number of learning iterations for the UCF101 and the HMDB51 datasets was chosen as 18000 and 7000, respectively. We decreased the learning rates by a factor of 10 after the 10000th and 16000th iterations for the UCF101 experiment, and after 4000th and 6000th iterations for the HMDB51 case. We only implement TVNet-50 in this experiment. To prevent overfitting, we also carry out the corner cropping and scale jittering <ref type="bibr" target="#b39">[40]</ref>; the learning rate for TVNets is further divided by 255.</p><p>For the testing, stacks of flow fields are extracted from the Method HMDB51 UCF101 ST-ResNet <ref type="bibr" target="#b8">[9]</ref> 66.4% 93.4% ST-ResNet + IDT <ref type="bibr" target="#b8">[9]</ref> 70.3% 94.6% TSN <ref type="bibr" target="#b39">[40]</ref> 68.5% 94.0% KVMF <ref type="bibr" target="#b43">[44]</ref> 63.3% 93.1% TDD <ref type="bibr" target="#b37">[38]</ref> 65.9% 91.5% C3D (3 nets) <ref type="bibr" target="#b35">[36]</ref> -90.4% Two-Stream Fusion <ref type="bibr" target="#b9">[10]</ref> 65.4% 92.5% Two-Stream (VGG16) <ref type="bibr" target="#b2">[3]</ref> 58.5% 91.4% Two-Stream+LSTM <ref type="bibr" target="#b27">[28]</ref> -88.6% Two-Stream Model <ref type="bibr" target="#b32">[33]</ref> 59 center and four corners of a video. We sample 25 stacks from each location (i.e., center and corners), followed by flipping them horizontally to enlarge the testing samples. All the sampled snippets (250 in total) are fed to BN-Inception <ref type="bibr" target="#b39">[40]</ref> and their outputs are averaged for prediction.</p><p>Baselines. Beside the TV-L1 method, we carry out other three widely-used flow extraction baselines including DIS-Fast <ref type="bibr" target="#b22">[23]</ref>, DeepFlow <ref type="bibr" target="#b40">[41]</ref> and FlowNet2.0 <ref type="bibr" target="#b17">[18]</ref>. For FlowNet2.0, we use the pretrained model by the KITTI dataset. For all baselines, we compute the optical flow beforehand and store the flow fields as JPEG images by linear compression. All methods share the same training setting and classification network for fair comparison.</p><p>Computational efficiency comparisons. We have added thorough computational comparison between TVNet, TV-L1, DIS-Fast, Deepflow, and Flownet2.0 in <ref type="table">Table 2</ref>. To do so, we randomly choose one testing video from the UCF101 dataset, and compute the optical flow for every two consecutive frames. The average running time (excluding I/O times) for TVNet-50, TV-L1, DIS-Fast, DeepFlow and FlowNet2.0 are summarized in <ref type="table">Table 2</ref>. The results verify the advantages of TVNets regarding high number of Frames-per-Second (FPS), low optical flow error, end-to-end trainable property, and small number of model parameters. Flownet2.0 performs more accurately than TV-L1 on the optical flow datasets (e.g. MiddleBurry) as reported by <ref type="bibr" target="#b17">[18]</ref>. However, for the action datasets, TV-L1 and our TVNet obtain lower flow error than Flownet2.0 according to <ref type="table">Table 2</ref>.</p><p>Classification accuracy comparisons. <ref type="table">Table 3</ref> presents action recognition accuracies of TVNet-50 compared with the four baselines and current best action representation methods. Clearly, TVNet-50 outperforms all compared methods on both datasets. Compared to TV-L1, the improvement of TVNet-50 on UCF101 is not big; however, our TVNet-50 is computationally advantageous over TV-L1 because it only employs one scale and one warp, while TV-L1 adopts five scales and five warps. Even when we freeze its parameters, TVNet-50 still achieves better results than DIS-Fast, DeepFlow and FlowNet2.0; as our TVNet is initialized as a special TV-L1, the initial structure is sufficient to perform  <ref type="figure">Figure 4</ref>. Illustrations of the motion patterns obtained by TV-L1 and TVNet-50 on the UCF101 dataset. From the first to the last column, we display the image-pair (first image only), the motion features by TV-L1, TVNet-50 without training and with training, respectively. Interestingly, with training, TVNet-50 generates more abstractive motion features than TV-L1 and its non-trained version. These features not only automatically remove the movement of the background (see the "punch" example), but also capture the outline of the moving objects.</p><p>promisingly. The TVNet is trained with the multi-task loss given by Eq. <ref type="bibr" target="#b12">(13)</ref>. To verify the effect of the flow loss term, i.e. L f , we train a new model only with the classification loss. <ref type="table">Table 3</ref> shows that such setting decreases the accuracy.</p><p>Flownet2.0 can also be jointly finetuned for action classification. This is done in ActionFlowNet <ref type="bibr" target="#b26">[27]</ref> but the results, as incorporated in <ref type="table">Table 3</ref>, are worse than ours. This is probably because TVNet has much fewer parameters than Flownet2.0, making the training more efficient and less prone to overfitting. For the UCF101 dataset, the TVNet outperforms C3D <ref type="bibr" target="#b35">[36]</ref> by more than 2%. The C3D method applied 3-dimensional convolutions to learn spatiotemporal features. In contrast to this implicit modeling, in our model, the motion pattern is extracted by TVNet explicitly. We also visualize the outputs by TV-L1 and TVNets in <ref type="figure">Figure 4</ref>.</p><p>Comparison with other state-of-the-arts. To compare with state-of-the-art methods, we apply several practical tricks to our TVNet-50, as suggested by previous works <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b39">40]</ref>. First, we perform the two-stream combination trick <ref type="bibr" target="#b32">[33]</ref> by additionally training a spatial network on RGB images. We use the BN-Inception network as the spatial network and apply the same experimental setting as those in <ref type="bibr" target="#b39">[40]</ref> for the training. At testing, we combine the predictions of the spatial and temporal networks with a fixed weight (i.e., 1:2). Second, to take the long-term temporal awareness into account, we perform the temporal pooling of 3 sampled segments for each video during training as suggested by <ref type="bibr" target="#b39">[40]</ref>. <ref type="table" target="#tab_4">Table 4</ref> summarizes the classification accuracy of TVNets compared with the state-of-the-art approaches over all three splits of the UCF101 and the HMDB51 dataset. The improvements achieved by TVNets are quite substantial compared to the original two-stream method <ref type="bibr" target="#b32">[33]</ref> (6.5% on UCF101 and 11.6% on HMDB51). Such significant gains are achieved as a result of employing better models (i.e., BN-Inception net) and also considering end-to-end motion mining.</p><p>The TSN method <ref type="bibr" target="#b39">[40]</ref> is actually a two-stream model with TV-L1 inputs. TSN shares the same classification network and experimental setups as our TVNets. As shown in <ref type="table">Table 3</ref>, our TVNets outperform TSN on both action datasets (e.g. 71.6% vs. 68.5% on HMDB51), verifying the effectiveness of TVNets for the two-stream models.</p><p>Combining CNN models with trajectory-based handcrafted IDT features <ref type="bibr" target="#b36">[37]</ref> can improve the final performances <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b8">9]</ref>. Hence, we averaged the L2-normalized SVM scores of FV-encoded IDT features (i.e., HOG, HOF and MBH) with the L2-normalized video predictions (before the loss layer) of our methods. <ref type="table" target="#tab_4">Table 4</ref> summarizes the results and indicates that there is still room for improvement. Our 95.4% on the UCF101 and 72.6% on the HMDB51 remarkably outperform all the compared methods.</p><p>A recent state-of-the-art result is obtained by I3D <ref type="bibr" target="#b5">[6]</ref>, achieving 97.9% on UCF101 and 80.2% on HMDB51. However, the I3D method improves the performance by using a large amount of additional training data. It is unfair to compare their results with ours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we propose a novel end-to-end motion representation learning framework, named as TVNet. Particularly, we formulate the TV-L1 approach as a neural network, which takes as input stacked frames and outputs optical-flow-like motion features. Experimental results on two video understanding tasks demonstrate its superior performances over the existing motion representation learning approaches. In the future, we will explore more large-scale video understanding tasks to examine the benefits of the end-to-end motion learning method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Visualization results of optical-flow-like motion features by TV1 [42], TVNet (without training) and TVNet (with training).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (a) Illustration of the process for unfolding TV-L1 to TVNet. For TV-L1, we illustrate each iteration of Algorithm 1. We reformulate the bicubic warping, gradient and divergence computations in TV-L1 to bilinear warping and convolution operations in TVNet. (b) The end-to-end model for action recognition.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>where w T b denotes the transposition of w b . We then refine the boundary points for the outputs of Eq. (7)-(9) to meet the boundary condition in Eq. (3)-(5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig- ure 3 displays the visualization of the optical flow between TV-L1-5-5-50 and TVNet-50. Another interesting observa- tion is from the comparison between TVNet-30, TVNet-50, TVNet-3-10 and TVNet-1-3-10. It is observed TVNet-30 and TVNet-50 finally outperform TVNet-3-10 and TVNet- 1-3-10 after training, implying that the flat structure (i.e. N scales = N warps = 1) is somehow easier to train. For the remaining experiments below, we will only compare the per- formance between TVNet-50 and TV-L1-5-5-50, and denote TV-L1-5-5-50 as TV-L1 for similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>The average EPEs on MiddleBurry. "Training ulution filters are trained. After training, TVNet-50 outperforms TV-L1 significantly although TV-L1 has a much larger number of optimization iterations (i.e., 1250).</figDesc><table>0 " means 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 .</head><label>4</label><figDesc>Mean classification accuracy of the state-of-the-arts on HMDB51 and UCF101.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">A database and evaluation methodology for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Scharstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lewis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">92</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="31" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Revisiting the effectiveness of off-theshelf temporal modeling approaches for large-scale video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1708.03805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3034" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<biblScope unit="page" from="4724" to="4733" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Mosift: Recognizing human actions in surveillance videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hauptmann</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2758" to="2766" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="3468" to="3476" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1933" to="1941" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Häusser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazırbaş</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<title level="m">Flownet: Learning optical flow with convolutional networks. ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Webly-supervised video recognition by mutually voting for relevant web images and web video frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="849" to="866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Devnet: A deep event network for multimedia event detection and evidence recounting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">You lead, we exceed: Labor-free video concept learning by jointly exploiting web videos and images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Going deeper into action recognition: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Herath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Harandi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image and Vision Computing (IVC)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01925</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="221" to="231" />
			<date type="published" when="2013-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1725" to="1732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klaser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="276" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fast optical flow using dense inverse search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kroeger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="471" to="488" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Hmdb: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Int. Conference on Computer Vision (ICCV)</title>
		<meeting>Int. Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2556" to="2563" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">On space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="issue">2-3</biblScope>
			<biblScope unit="page" from="107" to="123" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Attention clusters: Purely attention based local feature integration for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVPR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Actionflownet: Learning motion representation for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.03052</idno>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Multi-region two-stream r-cnn for action detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="744" to="759" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5534" to="5542" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Optical flow estimation using a spatial pyramid network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.00850</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Advances in Neural Information Processing Systems (NIPS)</title>
		<meeting>Advances in Neural Information essing Systems (NIPS)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Ucf101: A dataset of 101 human actions classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1212.0402</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning to extract motion from videos in convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="412" to="428" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4489" to="4497" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3551" to="3558" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with trajectory-pooled deep-convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4305" to="4314" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Towards good practices for very deep two-stream convnets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1507.02159</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Temporal segment networks: towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. European Conference on Computer Vision (ECCV)</title>
		<meeting>European Conference on Computer Vision (ECCV)</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="20" to="36" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Realtime action recognition with enhanced motion vector cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<monogr>
		<title level="m" type="main">Hidden two-stream convolutional networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Newsam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00389</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
