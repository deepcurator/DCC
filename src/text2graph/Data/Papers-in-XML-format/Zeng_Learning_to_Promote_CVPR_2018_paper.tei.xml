<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Learning to Promote Saliency Detectors</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yu</forename><surname>Zeng</surname></persName>
							<email>zengyu@mail.dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huchuan</forename><surname>Lu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihe</forename><surname>Zhang</surname></persName>
							<email>zhanglihe@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyang</forename><surname>Feng</surname></persName>
							<email>mengyangfeng@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ali</forename><surname>Borji</surname></persName>
							<email>aliborji@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Central Florida</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Learning to Promote Saliency Detectors</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Detecting salient objects or regions of an image, i.e. saliency detection, is useful for many computer vision tasks. As a preprocessing step, saliency detection is appealing for many practical applications, such as content-ware video compression <ref type="bibr" target="#b36">[37]</ref>, image resizing <ref type="bibr" target="#b1">[2]</ref>, and image retrieval <ref type="bibr" target="#b9">[10]</ref>. A plethora of saliency models have been proposed in the past two decades to locate conspicuous image regions <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5]</ref>. Although much effort has been devoted and significant progress has been made, saliency detection remains a challenging open problem.</p><p>Conventional saliency detection methods usually utilize low-level features and heuristic priors which are not robust enough to discover salient objects in complex scenes, neither are capable of capturing semantic objects. Deep neural</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Signs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Persons</head><p>Salient Background <ref type="figure">Figure 1</ref>. Images and the corresponding feature maps from the last convolution layer of VGG16 <ref type="bibr" target="#b24">[25]</ref>. The small binary mask in each image indicates the salient object of this image.</p><p>networks (DNNs) have been used to remedy the drawbacks of conventional methods. They can learn high-level semantic features from training samples, thus are more effective in locating semantically salient regions, yielding more accurate results in complex scenes. DNNs usually need to be trained on a large dataset, while training data for saliency detection is very limited. This issue is generally solved by pre-training on a large dataset for other tasks, such as image classification, which easily leads to several problems. First, saliency detection is an imagespecific task, and labels should be assigned to pixels depending on the image content. However, features produced by pre-trained feature extractors are supposed to work for all images. For example, signs and persons are salient objects in the first column of <ref type="figure">Figure 1</ref> , while they belong to the background in the second column. However, the regions of signs and persons are indiscriminately highlighted in the feature maps in the two columns. With this kind of feature extractor, the prediction model might be enforced to learn to map similar features into opposite labels, which is difficult for small training dataset. Second, categories and appearance of salient objects vary from image to image, while small training data is not enough to capture the diversity. For example, the six salient objects shown in <ref type="figure">Figure 1</ref> come from six different categories and differ wildly in their appearance. Consequently, it might be hard to learn a unified detector to handle all varieties of salient objects.</p><p>Considering the large diversity of salient objects, we avoid training a deep neural network (DNN) that directly maps images into labels. Instead, we train a DNN as an embedding function to map pixels and the attributes of the salient/background regions into a metric space. The attributes of the salient/background regions are mapped as anchors in the metric space. Then, a nearest neighbor (NN) classifier is constructed in this space, which assigns each pixel with the label of its nearest anchor. As a nonparametric model, the NN classifier can adapt well to new data and handle the diversity of salient objects. Additionally, since the classification task is performed by the NN classifier, the goal of the DNN is turned to learning a general mapping from the attributes of the salient/background regions to anchors in the embedding space. Compared with directly learning to detect diverse salient objects, this would be easier for the network to learn on limited data.</p><p>Concretely, we show the pipeline of our proposed method in <ref type="figure" target="#fig_0">Figure 2</ref>. During training, the DNN is provided with the true salient and background regions, of which the label of a few randomly selected pixels are flipped, to produce anchors. The output of the NN classifier constitutes a saliency map. The DNN can be trained end-to-end supervised by the loss between this saliency map and the ground truth. When testing on an image, the saliency map of each image is obtained as in training, but using approximate salient/background regions detected by an existing method. Although the approximate salient/background region is not completely correct, it is often with similar attributes to the true salient/background region. Thus, the corresponding embedding vectors (i.e. anchors) would be close to the ones of the true salient/background regions. Further, to produce better results, we propose an iterative testing scheme. The result of the NN classifier is utilized to revise anchors, yielding increasingly more accurate results.</p><p>Our method can be viewed as a zero-shot learning problem, in which the approximate salient/background regions detected by an existing method provide attributes for unseen salient objects, and the model learns from the training data to learn an image-specific classifier from the attributes to classify pixels of this image. Extensive experiments on five data sets show that our method can significantly improve accuracy of existing methods and compares favorably against state-of-the-art approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related works</head><p>Generally, saliency detection methods can be categorized into two streams: top-down and bottom-up saliency. Since our work addresses bottom-up saliency, here we mainly review recent works on bottom-up saliency, meanwhile shortly mention top-down saliency. We also explore the relation between our proposed method and top-down saliency.</p><p>Bottom-up (BU) saliency is stimuli-driven, where saliency is derived from contrast among visual stimuli. Conventional bottom-up saliency detection methods often utilize low-level features and heuristic priors. Jiang et al. <ref type="bibr" target="#b11">[12]</ref> formulate saliency detection via an absorbing Markov chain on an image graph model, where saliency of each region is defined as its absorbed time from boundary nodes. Yang et al. <ref type="bibr" target="#b31">[32]</ref> rank the similarity of the image regions with foreground cues or background cues via graphbased manifold ranking. Since the conventional methods are not robust in complex scenes neither capable of capturing semantic objects, deep neural networks (DNNs) are introduced to overcome these drawbacks. Li et al. <ref type="bibr" target="#b15">[16]</ref> train CNNs with fully connected layers to predict saliency value of each superpixel, and to enhance the spatial coherence of their saliency results using a refinement method. Li et al. <ref type="bibr" target="#b17">[18]</ref> propose a FCN trained under the multi-task learning framework for saliency detection. Zhang et al. <ref type="bibr" target="#b33">[34]</ref> present a generic framework to aggregate multi-level convolutional features for saliency detection. Although the proposed method is also based on DNNs, the main difference between ours and these methods is that they learn a general model that directly maps images to labels, while our method learns a general embedding function as well as an image-specific NN classifier.</p><p>Top-down (TD) saliency aims at finding salient regions specified by a task, and is usually formulated as a supervised learning problem. Yang and Yang <ref type="bibr" target="#b32">[33]</ref> propose a supervised top-down saliency model that jointly learns a Conditional Random Field (CRF) and a discriminative dictionary. Gao et al. <ref type="bibr" target="#b8">[9]</ref> introduced a top-down saliency algorithm by selecting discriminant features from a pre-defined filter bank.</p><p>Integration of TD and BU saliency has been exploited by some methods. For instance, Borji <ref type="bibr" target="#b2">[3]</ref> combines lowlevel features and saliency maps of previous bottom-up models with top-down cognitive visual features to predict fixations. Tong et al. <ref type="bibr" target="#b25">[26]</ref> proposed a top-down learning approach where the algorithm is bootstrapped with training samples generated using a bottom-up model to exploit the strengths of both bottom-up contrast-based saliency models and top-down learning methods. Our method also can be viewed as an integration of TD and BU saliency. Although both our method and the method of Tong et al. <ref type="bibr" target="#b25">[26]</ref> formulate the problem as top-down saliency detection specified by initial saliency maps, there are certain difference between the two. First, Tong's method trains a strong model via boostrap learning with training samples generated by a weak model. In contrast, our method maps pixels and the approximate salient/background regions into a learned metric space, which is related to zero-shot learning. Second, thanks to deep learning, our method is capable of capturing semantically salient regions and does well on complex . Classification results of all pixels constitute a saliency map (i), of which loss between the ground truth is used to supervise the network. During testing, the anchors are firstly produced according to an initial saliency map, here (e) is the initial saliency map. Given anchors, the nearest neighbor classifier can produce a new saliency map (i), which is utilized to revise the initial map as in Eqn.3. Then the revised map is used to produce new approximation to the anchors. Iterating the testing process would result in an increasingly more accurate result.</p><p>scenes, while Tong's method uses hand-crafted features and heuristic priors, which are less robust, Third, our method produces pixel-level results, while Tong's method computes saliency value of each image region to assemble a saliency map, which tends to be coarser.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">The Proposed Method</head><p>Our method consists of three components: 1) a DNN as an embedding function i.e. the anchor network, that maps pixels and regions of the input image into a learned metric space, 2) a nearest neighbor (NN) classifier in the embedding space learned specifically for this image to classify its pixels, and 3) an iterative testing scheme that utilizes the result of the NN classifier to revise anchors, yielding increasingly more accurate results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">The anchor network</head><p>Let x mn denote a pixel of an image X m . Each image consists a salient and a background region, i.e. X m = C m1 ∪ C m2 . Each pixel of an image either belongs to salient or background regions, denoted as n ∈ C mk , k = 1, 2, respectively. We use an embedding function modeled by a DNN φ with parameter θ, to map each pixel to a vector in a D-dimensional space:</p><formula xml:id="formula_0">φ mn = φ(x mn ; θ),<label>(1)</label></formula><p>where φ mn is the embedding vector to the corresponding pixel x mn .</p><p>The salient or background region C mk is also mapped into vectors in D-dimensional metric space by a DNN ψ with parameter η:</p><formula xml:id="formula_1">µ mk = ψ(C mk ; η),<label>(2)</label></formula><p>in which µ mk is the mapping of the salient or background region, i.e. anchors. We assume that in the embedding space, all pixels of an image cluster around the corresponding anchors of this image. Then a nearest neighbor classifier can be built specifically for this image by classifying each pixel according to its nearest anchor. The probability of a pixel x mn of image X m belonging to C mk can be given by the softmax over its distance to the anchors:</p><formula xml:id="formula_2">p(C mk |x mn ) = exp{−d(φ mn , µ mk )} j exp{−d(φ mn , µ mj )} ,<label>(3)</label></formula><p>where φ mn and µ mk are the vectors of pixel x mn and the salient / background anchor given by Eqn.1 and 2. d(·) denotes Euclidean distance. The CNN embeddings can be trained using a gradientbased optimization algorithm through maximizing the log likelihood with respect to θ and η on the training set:</p><formula xml:id="formula_3">L = m,n t mn log p(C m1 |x mn )+(1−t mn ) log p(C m2 |x mn ),<label>(4)</label></formula><p>where t mn is the label of pixel x mn . t mn = 1 when x mn ∈ C 1 , i.e. salient and t mn = 0 when x mn ∈ C 2 , i.e. background.</p><p>In practice, the ground-truth will not be available during testing, and the anchors are produced according to a prior saliency map, which is inaccurate. Therefore, to match training and testing conditions, during training we randomly flip the label of each pixel with probability p when producing the anchors using Eqn.2. In addition, this random flipping also increases diversity of training samples, thus helping reduce overfitting. We explain the training process of the anchor network in Alg.1. Here, L m denotes the log likelihood on the image X m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1:</head><p>Training the anchor network.</p><p>Input : Training set {(X m , t m )}, in which t mn = 1 indicates x mn ∈ C m1 , and t mn = 0 otherwise. Output: CNN embedding φ(·; θ) and ψ(·; η) 1 for training iterations do <ref type="bibr" target="#b1">2</ref> Sample a pair of training image and ground truth map (X m , t m ) from the training set. Randomly flip the elements in t m with probability p.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>Compute the embedding vector φ mn of each pixel given by Eqn.1 and produce anchors µ mk as in Eqn.2.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Compute gradient of log likelihood L m on this image with respect to θ and η. Update θ and η according to ∇ θ,η L m using a gradient based optimization method. 7 end</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Iterative testing scheme</head><p>In the testing phase, since the ground-truth is unknown, it is not possible to obtain precise salient and background regions to produce anchors as in the training time. Therefore, we produce anchors using approximate salient/background regionsĈ mk selected according to the saliency map Y (0) m of an existing method. An iterative testing scheme is proposed to gradually revise the anchors using the result of the NN classifier.</p><p>In the t-th iteration (t &gt; 0), the anchors are generated according to salient/background regionĈ mk selected by the prior saliency map Y (t) m . Given the anchors, we use the nearest neighbor classifier as in Eqn.3 to compute the probability of each pixel belonging to salient regions, i.e. saliency value, constructing another saliency map Z (t) m . Then, the prior saliency map is updated with</p><formula xml:id="formula_4">Y (t+1) m = t t + 1 Y (t) m + 1 t + 1 Z (t) m ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">Y (t+1) m</formula><p>is the prior saliency map which will be used for selecting salient and background regions in the next iteration. This means that the prior map is updated to a weighted sum of itself and the new result. After the first iteration, the prior map is completely replaced by the new result. The weight of the new result decreases with iterating, which insures stability of the iteration process. The testing algorithm of the proposed method is shown in Alg.2. <ref type="figure" target="#fig_4">Figure 3</ref> shows the process of the initial maps being promoted by the proposed method. Although the initial saliency map may not precisely separate the foreground and background, it can often partially separate them, and thus can provide information regarding categories and appearance of salient objects in the image. For instance, in the first image of <ref type="figure" target="#fig_4">Figure 3</ref>, though only a small part of the foreground is highlighted, the initial map can tell us that the foreground may be a gorilla, and the background contains a piece of green. Then, its selected foreground / background regions should be similar to the true foreground / background regions, leading to the corresponding anchors close to the true ones in the learned metric space. Thereby the nearest neighbor classification given by Eqn.3 can produce a good result. As the iterations progress, the approximate anchors gradually approach to the true ones, which would result in a better result. This, in turn could provide an increasingly accurate approximation to the anchors, and thus a more accurate result. As shown in <ref type="figure" target="#fig_4">Figure 3</ref>, the initial maps are not appealing, while the modified maps by our method look much better. Input : The input image X, the initial saliency map Y (0) , the number of iterations T . Output: The promoted saliency map Y (T ) .</p><p>1 Compute the embedding vector φ n of each pixel x n of X. for t ∈ {1, ..., T } do 2</p><p>Select the approximate salientĈ 1 and background regionĈ 2 according to Y (t) .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3</head><p>Produce the approximate anchor</p><formula xml:id="formula_6">µ k = ψ(Ĉ k ; η), k = 1, 2. 4</formula><p>Compute saliency value of each pixel according to</p><p>Eqn.3 to constitute another saliency map Z (t) m .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Update the prior saliency map:</p><formula xml:id="formula_7">Y (t+1) ← t t+1 Y (t) + 1 t+1 Z (t) 6 end</formula><p>It is known that DNNs, which typically consist of many parameters, have to be trained on large datasets to obtain good performance. For tasks where training data is scarce, such as saliency detection, revising a DNN that has been pre-trained on image classification datasets is the most viable option. Therefore, we also adopt a pre-trained DNN for our purpose rather than training a DNN from scratch. We modify the VGG16 <ref type="bibr" target="#b24">[25]</ref> network, pre-trained on the Im- ageNet <ref type="bibr" target="#b6">[7]</ref> dataset, into the pixel embedding φ(·, θ) and region embedding ψ(·, η). Since the DNN serves as an embedding instead of a classifier in the proposed method, we remove all the fully connected layers of VGG, and only retain its feature extractor component (VGG feature extractor). The VGG feature extractor consists of 5 convolution blocks, each of which contains several convolution and nonlinear layers, as well as a pooling layer. We show the network architecture and the overall structure of the proposed method in <ref type="figure" target="#fig_0">Figure 2</ref>, and describe the details in the next two subsections. In the figures and the text of this section, nonlinearity layers and batch-normalization layers are omit to avoid clutter. The combination of a convolution/fully connected layer, a batch-normalization layer and a ReLU nonlinear Convolution layers are referred to as a convolution/fully connected layers in this section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pixel embedding</head><p>Although effective in extracting hierarchical features, VGG feature extractor makes the feature maps smaller than the input image. This is not desirable for our method, because in order to map each pixel of the input image to a vector in the learned metric space, the embedding CNN should produce feature maps of the same resolution as the input image. We adopt two strategies to obtain larger feature maps: 1) remove the pooling layers of the last two convolution blocks and use dilated convolutions in these blocks to maintain receptive filed of the convolution filters, and 2) append a subpixel convolution layer after each convolution block of the VGG feature extractor to upsample the feature maps of each convolution blocks to the input image size. Subpixel convolution is an upsampling strategy originally proposed in <ref type="bibr" target="#b23">[24]</ref> for image super-resolution. To produce a C-channel tensor of N times the input size, the subpixel convolution firstly performs convolution on the feature map to get a N 2 × C-channel tensor of the input size. Then, the elements of the N 2 × C-channel tensor are rearranged into a C-channel output tensor of N times the size of the input tensor.</p><p>Five C-channel feature maps can be produced though adding a subpixel convolution layer after each of the five convolution blocks. Then the five C-channel feature maps are cascaded into a 5C-channel feature map. Directly using the features of this 5C-channel feature map to represent each pixel is not the best option since features of different convolution blocks are in different ranges. To solve this, we add two extra convolution layers after the subpixel convolution layers, to convert the 5C-channel feature maps into a D-channel feature map, in which each pixel corresponds to a D-dimensional vector. In our implementation, we set C to 64 and D to 512.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Region embedding</head><p>For simplicity, we let the pixel embedding φ(·, θ) and the the region embedding ψ(·, η) share the common feature extractor and subpixel convolution upsample layers. New layers are append after the subpixel convolution layers to map the 5C-channel feature map of an image region to a D-dimensional vector.</p><p>As shown in <ref type="figure" target="#fig_5">Figure 4</ref>, we consider two different structure of the region embedding: Conv-based and FC-based region embedding. In the Conv-based region embedding, the 5C-channel feature map of an image region is passed into convolution layers, resulting in a D-channel feature map. Then the D-dimensional embedding vector is given by averaging the D-channel feature map over pixels. The FCbased region embedding uses fully connected layers to map the average over pixels of the 5C-channel feature map into a D-dimensional vector.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We apply our method to five benchmark datasets to evaluate its performance. Details of these datasets are as fol-lows.</p><p>ECSSD <ref type="bibr" target="#b30">[31]</ref> contains 1000 natural images with multiple objects of different sizes. Some of the images come from the challenging Berkeley-300 dataset.</p><p>PASCAL-S <ref type="bibr" target="#b18">[19]</ref> stems from the validation set of PAS-CAL VOC2010 <ref type="bibr" target="#b7">[8]</ref> segmentation challenge and contains 850 natural images.</p><p>HKU-IS <ref type="bibr" target="#b15">[16]</ref> has 4447 images with high-quality pixelwise annotations. Images in this dataset are chosen to include multiple disconnected objects or objects touching the image boundary.</p><p>SOD <ref type="bibr" target="#b30">[31]</ref> has 300 images, and was originally designed for image segmentation. Pixel-wise annotations of salient objects were generated by <ref type="bibr" target="#b12">[13]</ref>. This dataset is challenging since many images contain multiple objects either with low contrast or touching the image boundary.</p><p>DUTS <ref type="bibr" target="#b26">[27]</ref> is a large scale dataset containing 10533 training images and 5019 test images. All the training images are collected from the ImageNet DET training/val sets <ref type="bibr" target="#b6">[7]</ref>, while test images are collected from the ImageNet DET test set and the SUN dataset <ref type="bibr" target="#b29">[30]</ref>. Accurate pixel-level ground truths are provided.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation metrics</head><p>We employ Precision-Recall curve, F-measure curve, Fmeasure score and MAE score to quantitatively evaluate the performance of the proposed method and compare with other methods.</p><p>The precision of a binary map is defined as the ratio of the number of salient pixels it correctly labels, to all salient pixels in this binary map. The recall value is the ratio of the number of correctly labeled salient pixels to all salient pixels in the ground-truth map:</p><formula xml:id="formula_8">precision = |T S ∩ DS| |DS| , recall = |T S ∩ DS| |T S| ,<label>(6)</label></formula><p>in which T S denotes true salient pixels, DS denotes detected salient pixels by the binary map, and | · | denotes cardinality of a set. The F-measure, denoted as F β , is an overall performance indicator computed by the weighted harmonic of precision and recall:</p><formula xml:id="formula_9">F β = (1 + β 2 ) · precision · recall β 2 · precision + recall ,<label>(7)</label></formula><p>where β 2 is set to 0.3 as suggested in <ref type="bibr" target="#b0">[1]</ref> to emphasize the precision.</p><p>Given a saliency map whose intensities are in the range of 0 and 1, a series of binary maps can be produced by thresholding the saliency map with different values in <ref type="bibr">[0,</ref><ref type="bibr" target="#b0">1]</ref>. Precision and recall values of these binary maps can be computed according to Eqn. 6. F-measure can be computed according to Eqn. 7. Plotting the (precision, recall) pairs of all the binary maps results in the precision-recall curve, and plotting the (F-measure, threshold) pairs results in the F-measure curve.</p><p>Also as suggested in <ref type="bibr" target="#b0">[1]</ref>, we use twice the mean value of the saliency maps as the threshold to generate binary maps for computing the F-measure. Notice that some works have reported slightly different F-measures using different thresholds. But as far as we know, twice the mean value is the most commonly used threshold.</p><p>As complementary to PR curves, mean absolute error (MAE) is used to quantitatively measure the average difference between the saliency map S and the ground truth map G:</p><formula xml:id="formula_10">MAE = 1 H H i=1 |S i − G i |.</formula><p>MAE indicates how similar a saliency map is compared to the ground truth. It is widely used in different pixel-level prediction tasks such as semantic segmentation and image cropping <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Implementation details</head><p>Our method is implemented in Python with the PyTorch 1 toolbox. We train and test our model on a PC with a 3.6GHz CPU, 32GB RAM and a GTX 1080 GPU. We train our model on the training set of DUTS dataset. As in <ref type="bibr" target="#b19">[20]</ref>, we augment the training data by horizontal flipping and cropping the images to reduce overfitting. The probability p of randomly flipping ground truth when producing anchors during training is set to 0.05. We compare two type of region embedding in Sec.4.4, and adopt the Conv-based one in other experiments. Adam <ref type="bibr" target="#b13">[14]</ref> optimization method is used for training our model. Learning rate is set to 1e-3. We do not use a validation set, and train our model until its training loss converges. The training process takes almost 16 hours and converges after around 300k iterations with mini-batch of size 1.</p><p>When comparing performance with other methods, the number of iterations T in the iterative testing scheme (Alg. 2) is set to 1. We discuss the effect of larger T values in Sec.4.4. When testing, the proposed method runs at about 15 fps with 256 256 resolution on our computer with a 3.6GHz CPU and a GTX 1080 GPU. We release our code for future comparisons <ref type="bibr" target="#b22">23</ref> .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation studies</head><p>Quantitative comparison between the two types of region embedding is shown in  we can see that the performance of FC-based and Convbased region embedding is comparable. The FC-based region embedding yields relatively larger F-measure, while Conv-based region embedding is more superior in terms of MAE.</p><p>We show the effect of the proposed iterative approximation scheme in <ref type="figure" target="#fig_6">Figure 5</ref>. As shown in <ref type="figure" target="#fig_6">Figure 5</ref>, the first iteration improve the F-measure and decrease MAE most significantly. The improvement slows down with iterations, and saturates gradually.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Performance</head><p>We choose 13 state-of-the-art methods as baselines, including 8 deep learning based methods (Amulet <ref type="bibr" target="#b33">[34]</ref>, SRM <ref type="bibr" target="#b28">[29]</ref>, UCF <ref type="bibr" target="#b34">[35]</ref>, DHS <ref type="bibr" target="#b19">[20]</ref>, NLDF <ref type="bibr" target="#b20">[21]</ref>, ELD <ref type="bibr" target="#b14">[15]</ref>, RFCN <ref type="bibr" target="#b27">[28]</ref>, DSS <ref type="bibr" target="#b10">[11]</ref>), and 5 conventional contenders (BSCA <ref type="bibr" target="#b22">[23]</ref>, DRFI <ref type="bibr" target="#b12">[13]</ref>, wCO <ref type="bibr" target="#b35">[36]</ref>, DSR <ref type="bibr" target="#b16">[17]</ref>, BL <ref type="bibr" target="#b25">[26]</ref>). We apply our method to promote the performance of each baseline method, by using its predicted saliency maps to generate initial anchors in Eqn.3. <ref type="figure" target="#fig_7">Figure 6</ref> shows the PR curves of the baseline methods and the one promoted by our method. <ref type="table">Table 2</ref> shows the F-measure and MAE scores of 8 deep learning based methods and the corresponding promoted results. The quantified improvements in F-measure and MAE of applying our method to conventional methods are shown in <ref type="table">Table 3</ref>. As shown in <ref type="figure" target="#fig_7">Figure 6</ref>, <ref type="table">Table 2, and  Table 3</ref>, our method drastically promotes all the baseline methods.</p><p>Based on our results, we make several fundamental ob- servations:</p><p>1. Our proposed method decreases the MAE of SRM, the best-performing method to date, by 15.3% on HKU-IS dataset and 14.2% on ECSSD dataset.</p><p>2. Although our method is based on deep learning, it also performs well when applied to conventional methods. For instance, our method decreases the MAE of DRFI by around 50% on both ECSSD and HKU-IS datasets. Our method does not rely on any specific choice of the initial map, and generalizes well across different baseline methods.  <ref type="table">Table 3</ref>. Comparison in terms of F-measure (the larger the better) and MAE (the smaller the better) score of our method against the conventional methods. The best and the second best methods are in red and green respectively. BS: the baseline; Ours: the promoted result of applying our method on the baseline.</p><p>3. Notice that the results shown here are obtained by iterating Alg. 2 only once for fast testing speed. As shown in Sec.4.4, better results can be achieved through iterating Alg. 2 more times. <ref type="figure" target="#fig_8">Figure 7</ref> shows a visual comparison of saliency maps produced by some state-of-the-art methods and the promoted ones by our method. It can be seen that the saliency maps produced by our methods highlight salient regions that are missed by the baselines. Further, our method can suppress the background regions that are wrongly labeled as salient by the baseline methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel learning method to promote existing salient object detection methods. Extensive experiments on five benchmark datasets show that our method can significantly improve accuracy of existing methods and compares favorably against state-of-the-arts. </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The pipeline of the proposed method. The input image (a) is first passed through our revised VGG network, resulting in an 512 channel feature map (b) of the same size as the input image. Each pixel is mapped to vectors e.g., (g) and (h) in the learned metric space (j). Salient and background regions is also mapped to vectors i.e. anchors in the learned metric space. For instance, (e) and (f) are salient and background anchors of this image respectively. During training, the salient and background pixels for producing anchors are selected using a randomly flipped ground truth ((d) and (e) in the figure), see Sec.3.1. An nearest neighbor classifier is built that classifies each pixel based on its distance to the anchors (see Eqn.3). Classification results of all pixels constitute a saliency map (i), of which loss between the ground truth is used to supervise the network. During testing, the anchors are firstly produced according to an initial saliency map, here (e) is the initial saliency map. Given anchors, the nearest neighbor classifier can produce a new saliency map (i), which is utilized to revise the initial map as in Eqn.3. Then the revised map is used to produce new approximation to the anchors. Iterating the testing process would result in an increasingly more accurate result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Algorithm 2 :</head><label>2</label><figDesc>Testing algorithm of the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. The process of the initial maps being promoted by the proposed method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Two different structures of the region embedding. The Σ symbol denotes averaging over pixels of the region. Top and bottom streams indicates Conv-based and FC-based region embedding respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Quantitative effect evaluated on ECSSD dataset in terms of F-measure and MAE of the proposed iterative testing scheme. Different lines represents the effect of applying the proposed method on different algorithms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. PR curves and F-measure curves of our method and the the state-of-the-art methods.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visual comparison of the algorithms promoted by our method against the baseline algorithms. Input: input images; GT: ground truth maps; A plus sign denotes the algorithm promoted by our method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>From this comparison</figDesc><table>Baseline 

UCF 
RFCN 
ELD 

Methods 
F β 
MAE 
F β 
MAE 
F β 
MAE 
BS 
0.8394 
0.0776 
0.8337 
0.1069 
0.8098 
0.0789 
FC 
0.8902 
0.0575 
0.8941 
0.0628 
0.8820 
0.0592 
Conv 
0.8805 
0.0560 
0.8885 
0.0570 
0.8689 
0.0577 

Table 1. Comparison in terms of F-measure (the larger the better) 
and MAE (the smaller the better) between two types of region em-
bedding evaluated on ECSSD dataset. The best and the second 
best methods are in red and green respectively. BS: baseline; FC: 
baseline promoted by the proposed method with FC-based region 
embedding; Conv: baseline promoted by the proposed method 
with Conv-based region embedding. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Table 2. Comparison in terms of F-measure (the larger the better) and MAE (the smaller the better) score of our method against other deep learning based methods. The best and the second best methods are in red and green respectively. BS: the baseline; Ours: the promoted result of applying our method on the baseline.</figDesc><table>ECSSD 

HKU-IS 
PASCALS 
DUTS-Test 
SOD 

Methods 
F β 
MAE 
F β 
MAE 
F β 
MAE 
F β 
MAE 
F β 
MAE 

Amulet 
BS 
0.8691 
0.0590 
0.8388 
0.0521 
0.7677 
0.0982 
0.6755 
0.0851 
0.7546 
0.1407 
Ours 
0.8963 
0.0509 
0.8772 
0.0446 
0.7985 
0.0920 
0.7281 
0.0828 
0.7769 
0.1336 

SRM 
BS 
0.8921 
0.0542 
0.8739 
0.0458 
0.8007 
0.0850 
0.7570 
0.0587 
0.8004 
0.1265 
Ours 
0.9151 
0.0465 
0.9042 
0.0388 
0.8240 
0.0810 
0.8023 
0.0558 
0.8036 
0.1170 

UCF 
BS 
0.8394 
0.0776 
0.8076 
0.0740 
0.7056 
0.1262 
0.6288 
0.1173 
0.6989 
0.1640 
Ours 
0.8805 
0.0560 
0.8530 
0.0546 
0.7703 
0.1044 
0.6911 
0.1051 
0.7520 
0.1470 

DHS 
BS 
0.8716 
0.0588 
0.8550 
0.0525 
0.7787 
0.0937 
0.7242 
0.0670 
0.7736 
0.1278 
Ours 
0.9058 
0.0482 
0.8923 
0.0421 
0.8155 
0.0859 
0.7822 
0.0610 
0.7925 
0.1216 

NLDF 
BS 
0.8781 
0.0626 
0.8735 
0.0477 
0.7787 
0.0990 
0.7426 
0.0650 
0.7906 
0.1242 
Ours 
0.9046 
0.0523 
0.8986 
0.0413 
0.8121 
0.0905 
0.7867 
0.0612 
0.8058 
0.1206 

ELD 
BS 
0.8098 
0.0789 
0.7694 
0.0736 
0.7179 
0.1227 
0.6277 
0.0923 
0.7115 
0.1545 
Ours 
0.8689 
0.0577 
0.8443 
0.0511 
0.7694 
0.1022 
0.7043 
0.0805 
0.7606 
0.1384 

RFCN 
BS 
0.8337 
0.1069 
0.8349 
0.0889 
0.7511 
0.1323 
0.7135 
0.0901 
0.7425 
0.1696 
Ours 
0.8885 
0.0570 
0.8831 
0.0437 
0.7968 
0.0946 
0.7688 
0.0666 
0.7856 
0.1323 

DSS 
BS 
0.8728 
0.0617 
0.8557 
0.0501 
0.7733 
0.1031 
0.7202 
0.0648 
0.7867 
0.1262 
Ours 
0.9075 
0.0492 
0.8995 
0.0394 
0.8117 
0.0906 
0.7867 
0.0588 
0.8061 
0.1187 

ECSSD 
HKU-IS 
PASCALS 
DUTS-Test 
SOD 

Methods 
F β 
MAE 
F β 
MAE 
F β 
MAE 
F β 
MAE 
F β 
MAE 

BSCA 
BS 
0.7046 
0.1821 
0.6544 
0.1747 
0.6005 
0.2228 
0.4995 
0.1961 
0.5835 
0.2516 
Ours 
0.7823 
0.1043 
0.7386 
0.1075 
0.6690 
0.1654 
0.5533 
0.1711 
0.6634 
0.2001 

DRFI 
BS 
0.7329 
0.1642 
0.7218 
0.1444 
0.6181 
0.2065 
0.5406 
0.1746 
0.6343 
0.2240 
Ours 
0.8136 
0.0872 
0.8061 
0.0722 
0.6943 
0.1443 
0.5895 
0.1457 
0.7069 
0.1686 

wCO 
BS 
0.6763 
0.1711 
0.6769 
0.1423 
0.5998 
0.2018 
0.5058 
0.1531 
0.5987 
0.2293 
Ours 
0.7792 
0.1084 
0.7765 
0.0883 
0.6844 
0.1551 
0.5932 
0.1365 
0.6732 
0.1878 

DSR 
BS 
0.6617 
0.1783 
0.6773 
0.1421 
0.5574 
0.2148 
0.5182 
0.1454 
0.5962 
0.2344 
Ours 
0.7993 
0.1018 
0.7992 
0.0798 
0.6806 
0.1570 
0.6353 
0.1201 
0.6916 
0.1834 

BL 
BS 
0.6838 
0.2159 
0.6597 
0.2070 
0.5742 
0.2487 
0.4896 
0.2379 
0.5797 
0.2669 
Ours 
0.7445 
0.1255 
0.7066 
0.1255 
0.6397 
0.1788 
0.5074 
0.2007 
0.6354 
0.2053 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/pytorch 2 http://ice.dlut.edu.cn/lu/ 3 https://github.com/zengxianyu/lps</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgment</head><p>This work was supported by the Natural Science Foundation of China under Grant 61725202, 61472060 and 61371157. In addition, the authors would like to thank Lijun Wang, Hongshuang Zhang and Yunhua Zhang for their help.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection. In Computer vision and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Susstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ieee conference on</title>
		<imprint>
			<biblScope unit="page" from="1597" to="1604" />
			<date type="published" when="2009" />
			<publisher>IEEE</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Saliency detection for contentaware image resizing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Süsstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">16th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1005" to="1008" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Boosting bottom-up and top-down visual features for saliency estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Salient object detection: A benchmark</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="5706" to="5722" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">State-of-the-art in visual attention modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="page" from="185" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Quantitative analysis of human-model agreement in visual saliency modeling: A comparative study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">N</forename><surname>Sihite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="55" to="69" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminant saliency, the detection of suspicious coincidences, and applications to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="989" to="1005" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Visual-textual joint relevance learning for tag-based social image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z.-J</forename><surname>Zha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="363" to="376" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deeply supervised salient object detection with short connections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR), 2017 IEEE Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="5300" to="5309" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Saliency detection via absorbing markov chain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1665" to="1672" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Salient object detection: A discriminative regional feature integration approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2083" to="2090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep saliency with encoded low level distance map and high level features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-W</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="660" to="668" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Visual saliency based on multiscale deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5455" to="5463" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Saliency detection via dense and sparse reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2976" to="2983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deepsaliency: Multi-task deep neural network model for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3919" to="3930" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">The secrets of salient object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="280" to="287" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="678" to="686" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Non-local deep features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Achkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eichel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P.-M</forename><surname>Jodoin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="733" to="740" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Saliency detection via cellular automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="110" to="119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Real-time single image and video super-resolution using an efficient sub-pixel convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bishop</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rueckert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1874" to="1883" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Salient object detection via bootstrap learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1884" to="1892" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning to detect salient objects with image-level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Conf. Comp. Vis. Patt. Recogn</title>
		<meeting>IEEE Conf. Comp. Vis. Patt. Recogn</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="136" to="145" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="825" to="841" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A stagewise refinement model for detecting salient objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Hierarchical saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1155" to="1162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Saliency detection via graph-based manifold ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3166" to="3173" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Top-down visual saliency via joint crf and dictionary learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2296" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Amulet: Aggregating multi-level convolutional features for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning uncertain convolutional features for accurate saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Saliency optimization from robust background detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2814" to="2821" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Content-aware compression using saliency-driven image retargeting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mangold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">20th IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1845" to="1849" />
		</imprint>
	</monogr>
	<note>Image Processing (ICIP)</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
