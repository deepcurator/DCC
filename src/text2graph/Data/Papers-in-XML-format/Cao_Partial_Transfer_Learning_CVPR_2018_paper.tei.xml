<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Partial Transfer Learning with Selective Adversarial Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhangjie</forename><surname>Cao</surname></persName>
							<email>caozhangjie14@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mingsheng</forename><surname>Long</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianmin</forename><surname>Wang</surname></persName>
							<email>jimwang@tsinghua.edu.cn</email>
							<affiliation key="aff1">
								<orgName type="laboratory">National Engineering Laboratory for Big Data Software</orgName>
								<orgName type="institution">University of California</orgName>
								<address>
									<settlement>Berkeley, Berkeley</settlement>
									<region>CA</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
							<email>jordan@cs.berkeley.edu</email>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">♯ † KLiss</orgName>
								<orgName type="department" key="dep2">MOE; School of Software</orgName>
								<orgName type="institution">Tsinghua University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Partial Transfer Learning with Selective Adversarial Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep networks have significantly improved the state of the art for a wide variety of machine learning problems and applications. At the moment, these impressive gains in performance come only when massive amounts of labeled data are available. Since manual labeling of sufficient training data for diverse application domains on-the-fly is often prohibitive, for problems short of labeled data, there is strong motivation to establishing effective algorithms to reduce the labeling consumption, typically by leveraging off-the-shelf labeled data from a different but related source domain. This promising transfer learning paradigm, however, suffers from the shift in data distributions across different domains, which * Corresponding author: M. Long (mingsheng@tsinghua.edu.cn).</p><p>poses a major obstacle in adapting classification models to target tasks <ref type="bibr" target="#b22">[23]</ref>.</p><p>Existing transfer learning methods assume shared label space and different feature distributions across the source and target domains. These methods bridge different domains by learning domain-invariant feature representations without using target labels, and the classifier learned from source domain can be directly applied to target domain. Recent studies have revealed that deep networks can learn more transferable features for transfer learning <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b32">33]</ref>, by disentangling explanatory factors of variations behind domains. The latest advances have been achieved by embedding transfer learning in the pipeline of deep feature learning to extract domain-invariant deep representations <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18]</ref>.</p><p>In the presence of big data, we can readily access largescale labeled datasets such as ImageNet-1K. Thus, a natural ambition is to directly transfer both the representation and classification models from large-scale dataset to our target dataset, such as Caltech-256, which are usually small-scale and with unknown categories at training and testing time. From big data viewpoint, we can assume that the large-scale dataset is diverse enough to subsume all categories of the small-scale dataset. Thus, we introduce a novel partial transfer learning problem, assuming that the target label space is a subspace of the source label space. It is a prerequisite of open set domain adaptation <ref type="bibr" target="#b2">[3]</ref>. As shown in <ref type="figure" target="#fig_0">Figure 1</ref>, partial transfer learning problem is more general and challenging than standard transfer learning, since outlier source classes ("sofa") will result in negative transfer when discriminating the target classes ("soccer-ball" and "binoculars"). Negative transfer is the phenomenon that a transfer learner performs even worse than a supervised classifier trained solely on the source domain, which is the key challenge of transfer learning <ref type="bibr" target="#b22">[23]</ref>. Thus, matching the whole source and target domains as previous methods is not an effective solution to this new partial transfer learning scenario. This paper presents Selective Adversarial Networks (SAN), which largely extends the ability of deep adversarial adaptation <ref type="bibr" target="#b6">[7]</ref> to address partial transfer learning from large- scale domains to small-scale domains. SAN aligns the distributions of source and target data in the shared label space and more importantly, selects out the source data in the outlier source classes. A key improvement over previous methods is the capability to simultaneously promote positive transfer of relevant data and alleviate negative transfer of irrelevant data, which can be trained in an end-to-end framework. Experiments show that our models exceed state-of-the-art results for deep transfer learning on public datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Transfer learning <ref type="bibr" target="#b22">[23]</ref> bridges different domains or tasks to mitigate the burden of manual labeling for machine learning <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b31">32]</ref>, computer vision <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b13">14]</ref> and natural language processing <ref type="bibr" target="#b3">[4]</ref>. The main technical difficulty of transfer learning is to formally reduce the distribution discrepancy across different domains. Deep networks can learn abstract representations that disentangle different explanatory factors of variations behind data <ref type="bibr" target="#b1">[2]</ref> and manifest invariant factors underlying different populations that transfer well from original tasks to similar novel tasks <ref type="bibr" target="#b32">[33]</ref>. Thus deep networks have been explored for transfer learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b13">14]</ref>, multimodal and multi-task learning <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b19">20]</ref>, where significant performance gains have been witnessed relative to prior shallow transfer learning methods.</p><p>However, recent advances show that deep networks can learn abstract feature representations that can only reduce, but not remove, the cross-domain discrepancy <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>, resulting in unbounded risk for target tasks <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b0">1]</ref>. Some recent work bridges deep learning and domain adaptation <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18]</ref>, which extends deep convolutional networks (CNNs) to domain adaptation by adding adaptation layers through which the mean embeddings of distributions are matched <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18]</ref>, or by adding a subnetwork as domain discriminator while the deep features are learned to confuse the discriminator in a domain-adversarial training paradigm <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref>. While performance was significantly improved, these state of the art methods may be restricted by the assumption that the source and target domains share the same label space. This assumption is violated in partial transfer learning, which transfers both representation and classification models from existing large-scale domains to unknown small-scale domains. To our knowledge, this is the first work that addresses partial transfer learning in adversarial networks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Partial Transfer Learning</head><p>In this paper, we propose partial transfer learning, a novel transfer learning paradigm where the target domain label space C t is a subspace of the source domain label space C s i.e. C t ⊂ C s . This new paradigm finds wide applications in practice, as we usually need to transfer a model from a large-scale dataset (e.g. ImageNet) to a small-scale dataset (e.g. Caltech-256). Similar to standard transfer learning, in partial transfer learning we are also provided with a source</p><formula xml:id="formula_0">domain D s = {(x i , y i )} ns i=1 of n s labeled examples associ- ated with |C s | classes and a target domain D t = {x i } ns+nt i=ns+1</formula><p>of n t unlabeled examples associated with |C t | classes, but differently, we have |C s | &gt; |C t | in partial transfer learning. The source domain and target domain are sampled from probability distributions p and q respectively. In standard transfer learning, we have p = q; and in partial transfer learning, we further have p Ct = q, where p Ct denotes the distribution of the source domain labeled data belonging to label space C t . The goal of this paper is to design a deep neural network that enables learning of transfer features f = G f (x) and adaptive classifier y = G y (f ) to bridge the cross-domain discrepancy, such that the target risk Pr (x,y)∼q [G y (G f (x)) = y] is minimized by leveraging the source domain supervision.</p><p>In standard transfer learning, one of the main challenges is that the target domain has no labeled data and thus the source classifier G y trained on source domain D s cannot be directly applied to target domain D t due to the distribution discrepancy of p = q. In partial transfer learning, another more difficult challenge is that we even do not know which part of the source domain label space C s is shared with the target domain label space C t because C t is not accessible during training, which results in two technical difficulties. On one hand, the source domain labeled data belonging to outlier label space C s \C t will cause negative transfer effect to the overall transfer performance. Existing deep transfer learning methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b17">18]</ref> generally assume source domain and target domain have the same label space and match the whole distributions p and q, which are prone to negative transfer since the source and target label spaces are different and thus cannot be matched in principle. Thus, how to eliminate or at least decrease the influence of the source labeled data in outlier label space C s \C t is the key to alleviating negative transfer. On the other hand, reducing the distribution discrepancy between p Ct and q is crucial to enabling knowledge transfer in the shared label space C t . These challenges should be tackled by filtering out the negative influence of unrelated part of source domain and at the same time enabling effective transfer learning between related part of source domain and target domain.</p><p>We propose a novel selective adversarial network to enable partial transfer learning by addressing two challenges.</p><p>(1) Circumvent negative transfer by filtering out the unrelated source labeled data belonging to the outlier label space C s \C t . (2) Promote positive transfer by maximally matching the data distributions p Ct and q in the shared label space C t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Domain Adversarial Network</head><p>Domain adversarial networks have been successfully applied to transfer learning <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b30">31]</ref> by extracting transferable features that can reduce the distribution shift between the source domain and the target domain. The adversarial learning procedure is a two-player game, where the first player is the domain discriminator G d trained to distinguish the source domain from the target domain, and the second player is the feature extractor G f fine-tuned simultaneously to confuse the domain discriminator.</p><p>To extract domain-invariant features f , the parameters θ f of feature extractor G f are learned by maximizing the loss of domain discriminator G d , while the parameters θ d of domain discriminator G d are learned by minimizing the loss of the domain discriminator. In addition, the loss of label predictor G y is also minimized. The objective of domain adversarial network <ref type="bibr" target="#b6">[7]</ref> is the following functional:</p><formula xml:id="formula_1">C 0 (θ f , θ y , θ d ) = 1 n s xi∈Ds L y (G y (G f (x i )) , y i ) − λ n s + n t xi∈Ds∪Dt L d (G d (G f (x i )) , d i ) (1)</formula><p>where λ is a trade-off parameter between the two objectives that shape the features during learning. After training convergence, the parametersθ f ,θ y ,θ d will deliver a saddle point of the functional <ref type="formula">(1)</ref>:</p><formula xml:id="formula_2">(θ f ,θ y ) = arg min θ f ,θy C 0 (θ f , θ y , θ d ) , (θ d ) = arg max θ d C 0 (θ f , θ y , θ d ) .</formula><p>(2) Domain adversarial networks are particularly effective for standard transfer learning where the source domain label space and target domain label space are the same, C s = C t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Selective Adversarial Network</head><p>In partial transfer learning, the target domain label space is a subset of the source domain label space, C t ⊂ C s . Thus, matching the whole source domain distribution p and target domain distribution q will result in negative transfer caused by the outlier label space C s \C t . The larger the outlier label space C s \C t compared to the target label space C t , the severer the negative transfer effect will be. To combat negative transfer, we should find a way to select out the outlier source classes as well as the associated source labeled data in C s \C t when performing domain adversarial adaptation.</p><p>To match the source and target domains of different label spaces C s = C t , we need to split the domain discriminator</p><formula xml:id="formula_3">G d in Equation (1) into |C s | class-wise domain discrimina- tors G k d , k = 1, .</formula><p>. . , |C s |, each is responsible for matching the source and target domain data associated with label k, as shown in <ref type="figure" target="#fig_1">Figure 2</ref>. Since the target label space C t is inaccessible during training while the target domain data are fully unlabeled, it is not easy to decide which domain discriminator G k d is responsible for each target data point. Fortunately, we observe that the output of the label predictorŷ i = G y (x i ) to each data point x i is a probability distribution over the source label space C s . This distribution well characterizes the probability of assigning x i to each of the |C s | classes. Therefore, it is natural to useŷ i as the probability to assign each data point</p><formula xml:id="formula_4">x i to the |C s | domain discriminators G k d , k = 1, . . . , |C s |.</formula><p>The assignment of each point x i to different discriminators can be implemented by a probability-weighted domain discriminator loss for all</p><formula xml:id="formula_5">|C s | domain discriminators G k d , k = 1, . . . , |C s | as follows, L ′ d = 1 n s + n t |Cs| k=1 xi∈Ds∪Dtŷ k i L k d G k d (G f (x i )) , d i , (3) where G k d is the k-th domain discriminator while L k d</formula><p>is its cross-entropy loss, and d i is the domain label of point x i . Compared with the single-discriminator domain adversarial network in Equation <ref type="formula">(1)</ref>, the proposed multi-discriminator domain adversarial network enables fine-grained adaptation where each data point x i is matched only by those relevant y f</p><formula xml:id="formula_6">G d G d CNN x G d GRL @L y @θ y − @L d @θ f @L d @θ d L y L d 1 2 K L d d d L d L d d 1 2 K 1 2 K @L f @θ f back-propagation</formula><p>Gf Gy domain discriminators according to its probabilityŷ i . This fine-grained adaptation may introduce three benefits. (1) It avoids the hard assignment of each point to only one domain discriminator, which tends to be inaccurate for target domain data. (2) It circumvents negative transfer since each point is only aligned to one or several most relevant classes, while the irrelevant classes are filtered out by the probabilityweighted domain discriminator loss. (3) The probabilityweighted domain discriminator loss puts different losses to different domain discriminators, which naturally learns multiple domain discriminators with different parameters θ k d ; these domain discriminators with different parameters can promote positive transfer for each instance.</p><formula xml:id="formula_7">@L y @θ f</formula><p>Besides the instance-level weighting mechanism described above, we introduce another class-level weighting method to further remove the negative influence of outlier source classes C s \C t and the associated source data. We observe that only the domain discriminators responsible for the target classes C t are effective for promoting positive transfer, while the other discriminators responsible for the outlier source classes C s \C t only introduce noises and deteriorate the positive transfer between the source domain and the target domain in the shared label space C t . Therefore, we need to down-weight the domain discriminators responsible for the outlier source classes, which can be implemented by class-level weighting of these domain discriminators. Since target data are not likely to belong to the outlier source classes, their probabilities y k i , k ∈ C s \C t are also sufficiently small. Thus, we can down-weight the domain discriminators responsible for the outlier source classes as follows, Although the multiple domain discriminators introduced in Equation (4) can selectively transfer relevant knowledge to target domain by decreasing the negative influence of outlier source classes C s \C t and by effectively transferring knowledge of shared label space C t , it highly depends on the probabilityŷ i = G y (x i ). Thus, we further refine the label predictor G y by exploiting the entropy minimization principle <ref type="bibr" target="#b9">[10]</ref> which encourages low-density separation between classes. This criterion is implemented by minimizing the entropy E over probabilityŷ</p><formula xml:id="formula_8">L d = 1 n s + n t |Cs| k=1 1 n t xi∈Dtŷ k i ×   xi∈(Ds∪Dt)ŷ k i L k d G k d (G f (x i )) , d i     ,<label>(4)</label></formula><formula xml:id="formula_9">k i on target domain D t as E = 1 n t xi∈Dt H (G y (G f (x i )))<label>(5)</label></formula><p>where H(·) is the conditional-entropy loss functional</p><formula xml:id="formula_10">H (G y (G f (x i ))) = − |Cs| k=1ŷ k i logŷ k i .</formula><p>By minimizing the entropy functional (5), the label predictor G y (x i ) can directly access target unlabeled data and will amend itself to pass through the target low-density regions to give more accurate probabilityŷ i with minimal prediction uncertainty.</p><p>Integrating all things together, the final objective of the proposed Selective Adversarial Network (SAN) is</p><formula xml:id="formula_11">C θ f , θy, θ k d | |Cs| k=1 = 1 ns x i ∈Ds Ly (Gy (G f (xi)), yi) + 1 nt x i ∈D t H (Gy (G f (xi))) − 1 ns + nt |Cs| k=1     1 nt x i ∈D tŷ k i   ×   x i ∈Ds∪D tŷ k i L k d G k d (G f (xi)) , di    <label>(6)</label></formula><p>where λ is a hyper-parameter that trade-offs the two objectives in the unified optimization problem. The optimization problem is to find the network parametersθ f ,θ y and θ</p><formula xml:id="formula_12">k d (k = 1, 2, ..., |C s |) that satisfy (θ f ,θ y ) = arg min θ f ,θy C θ f , θ y , θ k d | |Cs| k=1 , (θ 1 d , ...,θ |Cs| d ) = arg max θ 1 d ,...,θ |Cs | d C θ f , θ y , θ k d | |Cs| k=1 .<label>(7)</label></formula><p>The selective adversarial network (SAN) enables partial transfer learning, which simultaneously circumvents negative transfer by filtering out outlier source classes C s \C t , and promotes positive transfer by maximally matching the data distributions p Ct and q in the shared label space C t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We conduct experiments on three benchmark datasets to evaluate the efficacy of our approach against several state-ofthe-art deep transfer learning methods. Codes and datasets will be available at: https://github.com/thuml.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Setup</head><p>The evaluation is conducted on three public datasets: Office-31, Caltech-Office and ImageNet-Caltech.</p><p>Office We compare the performance of SAN with state of the art transfer learning and deep learning methods: Convolutional Neural Network (AlexNet <ref type="bibr" target="#b14">[15]</ref>), Deep Adaptation Network (DAN) <ref type="bibr" target="#b15">[16]</ref>, Reverse Gradient (RevGrad) <ref type="bibr" target="#b6">[7]</ref>, Residual Transfer Networks (RTN) <ref type="bibr" target="#b17">[18]</ref>, and Adversarial Discriminative Domain Adaptation (ADDA) <ref type="bibr" target="#b28">[29]</ref>. DAN learns transferable features by embedding deep features of multiple task-specific layers to reproducing kernel Hilbert spaces (RKHSs) and matching different distributions optimally using multi-kernel MMD. RevGrad improves domain adaptation by making the source and target domains indistinguishable for a discriminative domain classifier via an adversarial training paradigm. RTN jointly learns transferable features and adapts different source and target classifiers via deep residual learning <ref type="bibr" target="#b12">[13]</ref>. ADDA combines discriminative modeling, untied weight sharing, and a GAN loss to yield much better results than RevGrad. All prior methods do not address partial transfer learning where the target label space is a subspace of the source label space. To test SAN on different base-networks, we also compare different methods on VGG-16 <ref type="bibr" target="#b26">[27]</ref>. To go deeper with the efficacy of selective mechanism and entropy minimization, we perform ablation study by evaluating two variants of SAN: (1) SAN-selective is the variant without selective mechanism, which has the same model complexity as AlexNet; (2) SAN-entropy is the variant without entropy minimization, which has the same model complexity as SAN.</p><p>We follow standard protocols and use all labeled source data and all unlabeled target data for unsupervised transfer learning <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16]</ref>. We compare average classification accuracy of each transfer task using three random experiments. For MMD-based methods (DAN and RTN), we use Gaussian kernel with bandwidth b set to median pairwise squared distances on training data, i.e. median heuristic <ref type="bibr" target="#b10">[11]</ref>. For all methods, we perform standard cross-validation on labeled source data to select their hyper-parameters.</p><p>We implement all deep methods based on the Caffe deeplearning framework, and fine-tune from Caffe-provided models of AlexNet <ref type="bibr" target="#b14">[15]</ref> pre-trained on ImageNet. We add a bottleneck layer between the f c7 and f c8 layers as RevGrad <ref type="bibr" target="#b6">[7]</ref> except for the task ImageNet 1000 → Caltech 84 since the pre-trained model is trained on ImageNet dataset and it can fully exploit the advantage of pre-trained model with the original f c7 and f c8 layer. For SAN, we fine-tune all the feature layers and train the bottleneck layer, the classifier layer and the adversarial networks. Since these new layers and networks are trained from scratch, we set their learning rate to be 10 times that of the other layers. We use minibatch stochastic gradient descent (SGD) with momentum of 0.9 and the learning rate annealing strategy implemented in RevGrad <ref type="bibr" target="#b6">[7]</ref>: the learning rate is adjusted during SGD using the following formula:</p><formula xml:id="formula_13">η p = η0 (1+αp)</formula><p>β , where p is the training progress linearly changing from 0 to 1, η 0 = 0.001, α = 10 and β = 0.75, which is optimized for low error on the source domain. As SAN can work stably across different transfer tasks, the penalty of adversarial networks is increased from 0 to 1 gradually as RevGrad <ref type="bibr" target="#b6">[7]</ref>. All the hyper-parameters of the learning rate and penalty strategies are selected through standard cross-validation on the labeled source data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Results</head><p>The classification results on the six tasks of Office-31, the three tasks of Caltech-Office and the two tasks of ImageNetCaltech are shown in <ref type="table" target="#tab_1">Table 1</ref> and 2. The SAN model outperforms all comparison methods on all the tasks. In particular, SAN substantially improves the accuracy by huge margins on tasks with small source domain and small target domain, e.g. A 31 → W 10 , A 31 → D 10, and tasks with large source domain and small target domain, e.g. C 31 → W 10. And it achieves considerable accuracy gains on tasks with large-scale source domain and target domain, e.g. I 1000 → C 84. These results suggest that SAN can learn transferable features for partial transfer learning in all the tasks under the setting where the target label space is a subspace of the source label space.</p><p>The results reveal several interesting observations. (1) Previous deep transfer learning methods including those based on adversarial-network like RevGrad and those based on MMD like DAN perform worse than standard AlexNet, which demonstrates the influence of negative transfer effect. These methods try to transfer knowledge from all classes of source domain to target domain but there are classes in source domain that do not exist in the target domain, a.k.a. outlier source data. Fooling the adversarial network to match the distribution of outlier source data and target data will make the classifier more likely to classify target data in these outlier classes, which is prone to negative transfer. Thus these previous methods perform even worse than standard AlexNet. However, SAN outperforms them by large margins, indicating that SAN can effectively avoid negative transfer by eliminating the outlier source classes irrelevant to target domain. (2) RTN performs better than AlexNet because it executes entropy minimization criterion which can avoid the impact of outlier source data to some degree. But comparing RTN with SAN-selective which only has entropy minimization loss, we observe that SAN-selective outperforms RTN in most tasks, demonstrating that RTN also suffers from negative transfer effect and even the residual branch of RTN cannot learn the large discrepancy between source and target domain. (3) ADDA first learns a discriminative representation using the labels in the source domain and then a separate encoding that maps the target data to the same space using an asymmetric mapping learned through a domain-adversarial loss. By combining discriminative modeling, untied weight sharing, and a GAN loss, ADDA yields much better results than RevGrad and RTN. SAN outperforms ADDA in all the tasks, proving that our selective adversarial mechanism can jointly promote positive transfer from relevant source domain data to target domain and circumvent negative transfer from outlier source domain data to target domain. As a reference, the Upper Bound performance is achieved by manually removing the outlier classes (not in the target domain) from the source domain. We apply this to Office-31 dataset.</p><p>As shown in <ref type="table" target="#tab_1">Table 1</ref>, our SAN performs 6.71% worse than the upper bound while best baseline ADDA 12.56% worse.</p><p>We go deeper into different modules of SAN by comparing the results of SAN variants in <ref type="table" target="#tab_1">Tables 1 and 2.</ref> (1) SAN outperforms SAN-selective, proving that using selective adversarial mechanism can selectively transfer knowledge from source data to target data. It can successfully select the source data belonging to the classes shared with target classes by the corresponding domain discriminators.</p><p>(2) SAN outperforms SAN-entropy especially in tasks where source and target domains have very large distribution gap in terms of the different numbers of classes, e.g. I 1000 → C 84. Entropy minimization can effectively decrease the probability of predicting each point to irrelevant classes especially when there are a large number of irrelevant classes, which can in turn boost the performance of the selective adversarial mechanism. This explains the improvement from SAN-entropy to SAN.</p><p>By going even deeper with convolutions, the very deep convolutional networks have made breakthroughs in achieving new state of the art results in ImageNet Large-Scale Visual Recognition Challenge <ref type="bibr" target="#b23">[24]</ref>. Although the transferability of AlexNet features has been extensively quantified <ref type="bibr" target="#b32">[33]</ref>, it remains unclear whether very deep neural networks can learn more transferable features and how the feature transferability may change with the depths of very deep networks. In this paper, we approach this goal by evaluating the all methods based on VGG-16 network <ref type="bibr" target="#b26">[27]</ref>. From <ref type="table" target="#tab_2">Table 3</ref>, we can observe that SAN outperforms all the other methods on VGG-16 network, which demonstrates that SAN can generalize to different base networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Analysis</head><p>Accuracy for Different Numbers of Target Classes: We investigate a wider spectrum of partial transfer learning by varying the number of target classes. <ref type="figure" target="#fig_3">Figure 3(a)</ref> shows that when the number of target classes decreases, the performance of RevGrad degrades quickly, meaning that negative transfer becomes severer when the domain gap is enlarged. The performance of SAN degenerates when the number of target classes decreases from 31 to 20, where negative transfer problem arises but the transfer problem itself is still hard; the performance of SAN increases when the number of target classes decreases from 20 to 10, where the transfer problem itself becomes easier. The margin that SAN outperforms RevGrad becomes larger when the number of target classes decreases. SAN also outperforms RevGrad in standard transfer learning setting when the number of target classes is 31.</p><p>Convergence Performance: We examine the convergence of SAN by studying the test error through training process. As shown in <ref type="figure" target="#fig_3">Figure 3(b)</ref>, the test errors of DAN and RevGrad are increasing due to negative transfer. RTN converges very fast depending on the entropy minimization, but converges to a higher test error than SAN. SAN converges fast and stably to a lowest test error, meaning it can be trained efficiently and stably to enable positive transfer and alleviate negative transfer simultaneously.</p><p>Feature Visualization: We visualize the t-SNE embeddings <ref type="bibr" target="#b4">[5]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>This paper presented a novel selective adversarial network approach to partial transfer learning. Unlike previous adversarial adaptation methods that match the whole source and target domains based on the shared label space assumption, the proposed approach simultaneously circumvents negative transfer by selecting out the outlier source classes and promotes positive transfer by maximally matching the data distributions in the shared label space. Our approach successfully tackles partial transfer learning where source label space subsumes target label space, which is testified by extensive experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The partial transfer learning problem, where source label space subsumes target label space (should be prone to negative transfer).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. The architecture of the proposed Selective Adversarial Networks (SAN) for partial transfer learning, where f is the extracted deep features,ŷ is the predicted data label, andd is the predicted domain label; G f is the feature extractor, Gy and Ly are the label predictor and its loss, G k d and L k d are the domain discriminator and its loss; GRL stands for Gradient Reversal Layer. The blue part shows the class-wise adversarial networks (|Cs| in total) designed in this paper. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>the class-level weight for class k, which is small for the outlier source classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Empirical analysis of SAN: (a) transfer performance vs. #target labels, and (b) test error on target domain vs. #iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 .Figure 5 .</head><label>45</label><figDesc>Figure 4. The t-SNE visualization of DAN, RevGrad, RTN, and SAN with class information (10 classes).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>of the bottleneck representations by DAN, RevGrad, RTN and SAN on transfer task A 31 → W 10 in Figures 4(a)-4(d) (with class information) and Figures 5(a)- 5(d) (with domain information). We randomly select five classes in the source domain not shared with target domain and five classes shared with target domain. We can make intu- itive observations. (1) Figure 4(a) shows that the bottleneck features are mixed together, meaning that DAN cannot dis- criminate both source and target data very well; Figure 5(a) shows that the target data are aligned to all source classes including those outlier ones, which embodies the negative transfer issue. (2) Figures 4(b)-4(c) show that both RevGrad and RTN discriminate the source domain well but the fea- tures of most target data are very close to source data even to the wrong source classes; Figures 5(b)-5(c) further indicate that both RevGrad and RTN tend to draw target data close to all source classes even to those not existing in target domain. Thus, their performance on target data degenerates due to negative transfer. (3) Figures 4(d) and 5(d) demonstrate that SAN can discriminate different classes in both source and target while the target data are close to the right source classes, while the outlier source classes cannot influence the target classes. These results demonstrate the efficacy of both selective adversarial adaptation and entropy minimization.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>-31 [26] is a standard benchmark for domain adap- tation in computer vision, consisting of 4,652 images and 31 categories collected from three distinct domains: Amazon (A), which contains images downloaded from amazon.com, Webcam (W) and DSLR (D), which contain images taken by web camera and digital SLR camera with different settings, respectively. We denote the three domains with 31 categories as A 31, W 31 and D 31. Then we use the ten categories shared by Office-31 and Caltech-256 and select images of these ten categories in each domain of Office-31 as target domains, denoted as A 10, W 10 and D 10. We evaluate all methods across six transfer tasks A 31 → W 10, D 31 → W 10, W 31 → D 10, A 31 → D 10, D 31 → A 10 and W 31 → A 10. These tasks represent the performance on the setting where both source and target domains have small number of classes. Caltech-Office [9] is built by using Caltech-256 (C 256) [12] as source domain and the three domains in Of- fice 31 as target domains. We use the ten categories shared by Caltech-256 and Office-31 and select images of these ten categories in each domain of Office-31 as target domains [9, 17, 28]. Denoting source domains as C 256, we can build 3 transfer tasks: C 256 → W 10, C 256 → A 10 and C 256 → D 10. This setting aims to test the performance of different methods on the task setting where source domain has much more classes than the target domain. ImageNet-Caltech is built from ImageNet-1K [25] dataset containing 1000 classes and Caltech-256 containing 256 classes. They share 84 common classes, thus we form two transfer learning tasks: ImageNet 1000 → Caltech 84 and Caltech 256 → ImageNet 84. To prevent the effect of the pre-trained model on ImageNet, we use ImageNet validation set when ImageNet is used as target domain and ImageNet training set when ImageNet is used as source do- main. This setting represents the performance on tasks with large number of classes in both source and target domains.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Classification Accuracy (%) of Partial Transfer Learning Tasks on Office-31 (AlexNet as Base Network)→ W 10 D 31 → W 10 W 31 → D 10 A 31 → D 10 D 31 → A 10 W 31 → A 10 Avg</figDesc><table>Method 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 3 .</head><label>3</label><figDesc>Classification Accuracy (%) of Partial Transfer Learning Tasks on Office-31 (VGG-16 as Base Network)</figDesc><table>Method 
A 31→W 10 D 31→W 10 W 31→D 10 A 31→D 10 D 31→A 10 W 31→A 10 
Avg 
VGG [27] 
60.34 
97.97 
99.36 
76.43 
72.96 
79.12 
81.03 
DAN [16] 
58.78 
85.86 
92.78 
54.76 
55.42 
67.29 
69.15 
RevGrad [7] 
50.85 
95.23 
94.27 
57.96 
51.77 
62.32 
68.73 
RTN [18] 
69.35 
98.42 
99.59 
75.43 
81.45 
82.98 
84.54 
ADDA [29] 
72.85 
98.42 
99.59 
77.96 
84.77 
85.32 
86.49 
SAN 
83.39 
99.32 
100.00 
90.70 
87.16 
91.85 
92.07 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Acknowledgements</head></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A theory of learning from different domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ben-David</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crammer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kulesza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Pereira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Vaughan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">MLJ</title>
		<imprint>
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="151" to="175" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Open set domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">P</forename><surname>Busto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="3" to="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Natural language processing (almost) from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Karlen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kuksa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMLR</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="2493" to="2537" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Decaf: A deep convolutional activation feature for generic visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Domain transfer multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="465" to="479" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Domainadversarial training of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ganin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ustinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ajakan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Germain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Laviolette</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marchand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Domain adaptation for large-scale sentiment classification: A deep learning approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bordes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Geodesic flow kernel for unsupervised domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Semi-supervised learning by entropy minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Grandvalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="529" to="536" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Optimal kernel choice for large-scale two-sample tests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sriperumbudur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sejdinovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Strathmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Balakrishnan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pontil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fukumizu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Caltech-256 object category dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Griffin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Holub</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
		<respStmt>
			<orgName>California Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">LSDA: Large scale detection through adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning transferable features with deep adaptation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Transfer feature learning with joint distribution adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Unsupervised domain adaptation with residual transfer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Domain adaptation: Learning bounds and algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mansour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rostamizadeh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">COLT</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multimodal deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ngiam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Learning and transferring mid-level image representations using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Domain adaptation via transfer component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">W</forename><surname>Tsang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Kwok</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TNNLS</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="199" to="210" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">A survey on transfer learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDE</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1345" to="1359" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>ImageNet Large Scale Visual Recognition Challenge</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">ImageNet Large Scale Visual Recognition Challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Adapting visual category models to new domains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Kulis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556v6</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Return of frustratingly easy domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Adversarial discriminative domain adaptation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Deep domain confusion: Maximizing for domain invariance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Simultaneous deep transfer across domains and tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Tzeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Flexible transfer learning under support and model shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schneider</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">How transferable are features in deep neural networks? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lipson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Domain adaptation under target and conditional shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Muandet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
