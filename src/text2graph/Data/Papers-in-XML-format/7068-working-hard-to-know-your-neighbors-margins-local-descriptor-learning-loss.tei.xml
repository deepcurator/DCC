<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:41+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasiya</forename><surname>Mishchuk</surname></persName>
							<email>anastasiya.mishchuk@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="laboratory">Szkocka Research Group</orgName>
								<address>
									<country key="UA">Ukraine</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
							<email>mishkdmy@cmp.felk.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Recognition Group, CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenović</surname></persName>
							<email>filip.radenovic@cmp.felk.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Recognition Group, CTU in Prague</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiři</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<affiliation key="aff1">
								<orgName type="laboratory">Visual Recognition Group, CTU in Prague</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Working hard to know your neighbor&apos;s margins: Local descriptor learning loss</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We introduce a loss for metric learning, which is inspired by the Lowe's matching criterion for SIFT. We show that the proposed loss, that maximizes the distance between the closest positive and closest negative example in the batch, is better than complex regularization methods; it works well for both shallow and deep convolution network architectures. Applying the novel loss to the L2Net CNN architecture results in a compact descriptor named HardNet. It has the same dimensionality as SIFT <ref type="formula">(128)</ref> and shows state-of-art performance in wide baseline stereo, patch verification and instance retrieval benchmarks.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>best of our knowledge, no work in local descriptor learning fully mimics such strategy as the learning objective.</p><p>Simonyan and Zisserman <ref type="bibr" target="#b19">[20]</ref> proposed a simple filter plus pooling scheme learned with convex optimization to replace the hand-crafted filters and poolings in SIFT. Han et al. <ref type="bibr" target="#b13">[14]</ref> proposed a twostage siamese architecture -for embedding and for two-patch similarity. The latter network improved matching performance, but prevented the use of fast approximate nearest neighbor algorithms like kd-tree <ref type="bibr" target="#b20">[21]</ref>. Zagoruyko and Komodakis <ref type="bibr" target="#b14">[15]</ref> have independently presented similar siamese-based method which explored different convolutional architectures. Simo-Serra et al <ref type="bibr" target="#b21">[22]</ref> harnessed hardnegative mining with a relative shallow architecture that exploited pair-based similarity.</p><p>The three following papers have most closedly followed the classical SIFT matching scheme. Balntas et al <ref type="bibr" target="#b22">[23]</ref> used a triplet margin loss and a triplet distance loss, with random sampling of the patch triplets. They show the superiority of the triplet-based architecture over a pair based. Although, unlike SIFT matching or our work, they sampled negatives randomly. Choy et al <ref type="bibr" target="#b6">[7]</ref> calculate the distance matrix for mining positive as well as negative examples, followed by pairwise contrastive loss.</p><p>Tian et al <ref type="bibr" target="#b23">[24]</ref> use n matching pairs in batch for generating n 2 − n negative samples and require that the distance to the ground truth matchings is minimum in each row and column. No other constraint on the distance or distance ratio is enforced. Instead, they propose a penalty for the correlation of the descriptor dimensions and adopt deep supervision <ref type="bibr" target="#b24">[25]</ref> by using intermediate feature maps for matching. Given the state-of-art performance, we have adopted the L2Net <ref type="bibr" target="#b23">[24]</ref> architecture as base for our descriptor. We show that it is possible to learn even more powerful descriptor with significantly simpler learning objective without need of the two auxiliary loss terms.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Batch of input patches</head><p>Descriptors Distance matrix = ( , ) Final triplet (one of n in batch) <ref type="bibr" target="#b3">4</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>2</head><p>Figure 1: Proposed sampling procedure. First, patches are described by the current network, then a distance matrix is calculated. The closest non-matching descriptor -shown in red -is selected for each a i and p i patch from positive pair (green) respectively. Finally, among two negative candidates the hardest one is chosen. All operations are done in a single forward pass.</p><p>3 The proposed descriptor</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Sampling and loss</head><p>Our learning objective mimics SIFT matching criterion. The process is shown in <ref type="figure">Figure 1</ref>. First, a batch X = (A i , P i ) i=1..n of matching local patches is generated, where A stands for the anchor and P for the positive. The patches A i and P i correspond to the same point on 3D surface. We make sure that in batch X , there is exactly one pair originating from a given 3D point.</p><p>Second, the 2n patches in X are passed through the network shown in <ref type="figure">Figure 2</ref>.</p><formula xml:id="formula_0">L2 pairwise distance matrix D = cdist(a, p), where, d(a i , p j ) = 2 − 2a i p j , i = 1.</formula><p>.n, j = 1..n of size n × n is calculated, where a i and p j denote the descriptors of patches A i and P j respectively.</p><p>Next, for each matching pair a i and p i the closest non-matching descriptors i.e. the 2 nd nearest neighbor, are found respectively: a i -anchor descriptor, p i -positive descriptor, p jmin -closest non-matching descriptor to a i , where j min = arg min j=1..n,j =i d(a i , p j ), a kmin -closest non-matching descriptor to p i where k min = arg min k=1..n,k =i d(a k , p i ).</p><p>Then from each quadruplet of descriptors (a i , p i , p jmin , a kmin ), a triplet is formed:</p><formula xml:id="formula_1">(a i , p i , p jmin ), if d(a i , p jmin ) &lt; d(a kmin , p i ) and (p i , a i , a kmin ) otherwise.</formula><p>Our goal is to minimize the distance between the matching descriptor and closest non-matching descriptor. These n triplet distances are fed into the triplet margin loss:</p><formula xml:id="formula_2">L = 1 n i=1,n max (0, 1 + d(a i , p i ) − min (d(a i , p jmin ), d(a kmin , p i )))<label>(1)</label></formula><p>where</p><formula xml:id="formula_3">min (d(a i , p jmin ), d(a kmin , p i )</formula><p>is pre-computed during the triplet construction.</p><p>The distance matrix calculation is done on GPU and the only overhead compared to the random triplet sampling is the distance matrix calculation and calculating the minimum over rows and columns.</p><p>Moreover, compared to usual learning with triplets, our scheme needs only two-stream CNN, not three, which results in 30% less memory consumption and computations.</p><p>Unlike in <ref type="bibr" target="#b23">[24]</ref>, neither deep supervision for intermediate layers is used, nor a constraint on the correlation of descriptor dimensions. We experienced no significant over-fitting.  <ref type="figure">Figure 2</ref>: The architecture of our network, adopted from L2Net <ref type="bibr" target="#b23">[24]</ref>. Each convolutional layer is followed by batch normalization and ReLU, except the last one. Dropout regularization is used before the last convolution layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Model architecture</head><p>The HardNet architecture, <ref type="figure">Figure 2</ref>, is identical to L2Net <ref type="bibr" target="#b23">[24]</ref>. Padding with zeros is applied to all convolutional layers, to preserve the spatial size, except to the final one. There are no pooling layers, since we found that they decrease performance of the descriptor. That is why the spatial size is reduced by strided convolutions. Batch normalization <ref type="bibr" target="#b25">[26]</ref> layer followed by ReLU <ref type="bibr" target="#b26">[27]</ref> non-linearity is added after each layer, except the last one. Dropout <ref type="bibr" target="#b27">[28]</ref> regularization with 0.1 dropout rate is applied before the last convolution layer. The output of the network is L2 normalized to produce 128-D descriptor with unit-length. Grayscale input patches with size 32 × 32 pixels are normalized by subtracting the per-patch mean and dividing by the per-patch standard deviation.</p><p>Optimization is done by stochastic gradient descent with learning rate of 0.1, momentum of 0.9 and weight decay of 0.0001. Learning rate was linearly decayed to zero within 10 epochs for the most of the experiments in this paper. Training is done with PyTorch library <ref type="bibr" target="#b28">[29]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Model training</head><p>UBC Phototour <ref type="bibr" target="#b2">[3]</ref>, also known as Brown dataset.  <ref type="table" target="#tab_3">Table 1</ref>. Proposed descriptor outperforms competitors, with training augmentation, or without it. We haven't included results on multiscale patch sampling or so called "center-surrounding" architecture for two reasons. First, architectural choices are beyond the scope of current paper. Second, it was already shown in <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> that "center-surrounding" consistently improves results on Brown dataset for different descriptors, while hurts matching performance on other, more realistic setups, e.g., on Oxford-Affine <ref type="bibr" target="#b30">[31]</ref> dataset.</p><p>In the rest of paper we use descriptor trained on Liberty sequence, which is a common practice, to allow a fair comparison. TFeat <ref type="bibr" target="#b22">[23]</ref> and L2Net <ref type="bibr" target="#b23">[24]</ref> use the same dataset for training. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Exploring the batch size influence</head><p>We study the influence of mini-batch size on the final descriptor performance. It is known that small mini-batches are beneficial to faster convergence and better generalization <ref type="bibr" target="#b31">[32]</ref>, while large batches allow better GPU utilization. Our loss function design should benefit from seeing more hard negative patches to learn to distinguish them from true positive patches. We report the results for batch sizes 16, 64, 128, 512, 1024, 2048. We trained the model described in Section 3.2 using Liberty sequence of Brown dataset. Results are shown in <ref type="figure" target="#fig_0">Figure 3</ref>. As expected, model performance improves with increasing the mini-batch size, as more examples are seen to get harder negatives. Although, increasing batch size to more than 512 does not bring significant benefit.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical evaluation</head><p>Recently, Balntas et al. <ref type="bibr" target="#b22">[23]</ref> showed that good performance on patch verification task on Brown dataset does not always mean good performance in the nearest neighbor setup and vice versa. Therefore, we have extensively evaluated learned descriptors on real-world tasks like two view matching and image retrieval.</p><p>We have selected RootSIFT <ref type="bibr" target="#b9">[10]</ref>, TFeat-M* <ref type="bibr" target="#b22">[23]</ref>, and L2Net <ref type="bibr" target="#b23">[24]</ref> for direct comparison with our descriptor, as they show the best results on a variety of datasets.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Patch descriptor evaluation</head><p>HPatches <ref type="bibr" target="#b17">[18]</ref> is a recent dataset for local patch descriptor evaluation. It consists of 116 sequences of 6 images. The dataset is split into two parts: viewpoint -59 sequences with significant viewpoint change and illumination -57 sequences with significant illumination change, both natural and artificial. Keypoints are detected by DoG, Hessian and Harris detectors in the reference image and reprojected to the rest of the images in each sequence with 3 levels of geometric noise: Easy, Hard, and Tough variants. The HPatches benchmark defines three tasks: patch correspondence verification, image matching and small-scale patch retrieval. We refer the reader to the HPatches paper <ref type="bibr" target="#b17">[18]</ref> for a detailed protocol for each task.</p><p>Results are shown in <ref type="figure" target="#fig_2">Figure 5</ref>. L2Net and HardNet have shown similar performance on the patch verification task with a small advantage of HardNet. On the matching task, even the non-augmented version of HardNet outperforms the augmented version of L2Net+ by a noticeable margin. The difference is larger in the TOUGH and HARD setups. Illumination sequences are more challenging than the geometric ones, for all the descriptors. We have trained network with TFeat architecture, but with proposed loss function -it is denoted as HardTFeat. It outperforms original version in matching and retrieval, while being on par with it on patch verification task.</p><p>In patch retrieval, relative performance of the descriptors is similar to the matching problem: HardNet beats L2Net+.   We also ran another patch retrieval experiment, varying the number of distractors (non-matching patches) in the retrieval dataset. The results are shown in <ref type="figure" target="#fig_1">Figure 4</ref>. TFeat descriptor performance, which is comparable to L2Net in the presence of low number distractors, degrades quickly as the size of the the database grows. At about 10,000 its performance drops below SIFT. This experiment explains why TFeat performs relatively poorly on the Oxford5k <ref type="bibr" target="#b32">[33]</ref> and Paris6k <ref type="bibr" target="#b33">[34]</ref> benchmarks, which contain around 12M and 15M distractors, respectively, see Section 4.4 for more details. Performance of the HardNet decreases slightly for both augmented and plain version and the difference in mAP to other descriptors grows with the increasing complexity of the task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Ablation study</head><p>For better understanding of the significance of the sampling strategy and the loss function, we conduct experiments summarized in <ref type="table" target="#tab_5">Table 2</ref>. We train our HardNet model (architecture is exactly the same as L2Net model), change one parameter at a time and evaluate its impact.</p><p>The following sampling strategies are compared: random, the proposed "hardest-in-batch", and the "classical" hard negative mining, i.e. selecting in each epoch the closest negatives from the full training set. The following loss functions are tested: softmin on distances, triplet margin with margin m = 1, contrastive with margins m = 1, m = 2. The last is the maximum possible distance for unit-normed descriptors. Mean mAP on HPatches Matching task is shown in <ref type="table" target="#tab_5">Table 2</ref>.</p><p>The proposed "hardest-in-batch" clearly outperforms all the other sampling strategies for all loss functions and it is the main reason for HardNet's good performance. The random sampling and "classical" hard negative mining led to huge overfit, when training loss was high, but test performance was low and varied several times from run to run. This behavior was observed with all loss function. Similar results for random sampling were reported in <ref type="bibr" target="#b23">[24]</ref>. The poor results of hard negative mining ("hardest-in-the-training-set") are surprising. We guess that this is due to dataset label noise, the mined "hard negatives" are actually positives. Visual inspection confirms this. We were able to get reasonable results with random and hard negative mining sampling only with additional correlation penalty on descriptor channels (CPR), as proposed in <ref type="bibr" target="#b23">[24]</ref>.</p><p>Regarding the loss functions, softmin gave the most stable results across all sampling strategies, but it is marginally outperformed by contrastive and triplet margin loss for our strategy. One possible explanation is that the triplet margin loss and contrastive loss with a large margin have constant non-zero derivative w.r.t to both positive and negative samples, see <ref type="figure" target="#fig_3">Figure 6</ref>. In the case of contrastive loss with a small margin, many negative examples are not used in the optimization (zero derivatives), while the softmin derivatives become small, once the distance to the positive example is smaller than to the negative one.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Wide baseline stereo</head><p>To validate descriptor generalization and their ability to operate in extreme conditions, we tested them on the W1BS dataset <ref type="bibr" target="#b3">[4]</ref>. It consists of 40 image pairs with one particular extreme change between the images:</p><p>Appearance (A): difference in appearance due to seasonal or weather change, occlusions, etc;</p><p>Geometry (G): difference in scale, camera and object position;</p><p>Illumination (L): significant difference in intensity, wavelength of light source;</p><p>Sensor (S): difference in sensor data (IR, MRI).</p><p>Moreover, local features in W1BS dataset are detected with MSER <ref type="bibr" target="#b34">[35]</ref>, Hessian-Affine <ref type="bibr" target="#b10">[11]</ref> (in implementation from <ref type="bibr" target="#b35">[36]</ref>) and FOCI <ref type="bibr" target="#b36">[37]</ref> detectors. They fire on different local structures than DoG. Note that DoG patches were used for the training of the descriptors. Another significant difference to the HPatches setup is the absence of the geometrical noise: all patches are perfectly reprojected to the target image in pair. The testing protocol is the same as for the HPatches matching task.</p><p>Results are shown in <ref type="figure" target="#fig_4">Figure 7</ref>. HardNet and L2Net perform comparably, former is performing better on images with geometrical and appearance changes, while latter works a bit better in map2photo and visible-vs-infrared pairs. Both outperform SIFT, but only by a small margin. However, considering the significant amount of the domain shift, descriptors perform very well, while TFeat loses badly to SIFT. HardTFeat significantly outperforms the original TFeat descriptor on the W1BS dataset, showing the superiority of the proposed loss.</p><p>Good performance on patch matching and verification task does not automatically lead to the better performance in practice, e.g. to more images registered. Therefore we also compared descriptor on wide baseline stereo setup with two metric: number of successfully matched image pairs and average number of inliers per matched pair, following the matcher comparison protocol from <ref type="bibr" target="#b3">[4]</ref>. The only change to the original protocol is that first fast matching steps with ORB detector and descriptor were removed, as we are comparing "SIFT-replacement" descriptors.</p><p>The results are shown in <ref type="table" target="#tab_6">Table 3</ref>. Results on Edge Foci (EF) <ref type="bibr" target="#b36">[37]</ref>, Extreme view <ref type="bibr" target="#b37">[38]</ref> and Oxford Affine <ref type="bibr" target="#b10">[11]</ref> datasets are saturated and all the descriptors are good enough for matching all image pairs. HardNet has an a slight advantage in a number of inliers per image. The rest of datasets: SymB <ref type="bibr" target="#b38">[39]</ref>, GDB <ref type="bibr" target="#b39">[40]</ref>, WxBS <ref type="bibr" target="#b3">[4]</ref> and LTLL <ref type="bibr" target="#b40">[41]</ref> have one thing in common: image pairs are or from different domain than photo (e.g. drawing to drawing) or cross-domain (e.g., drawing to photo). Here HardNet outperforms learned descriptors and is on-par with hand-crafted RootSIFT. We would like to note that HardNet was not learned to match in different domain, nor cross-domain scenario, therefore such results show the generalization ability. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Image retrieval</head><p>We evaluate our method, and compare against the related ones, on the practical application of image retrieval with local features. Standard image retrieval datasets are used for the evaluation, i.e., Oxford5k <ref type="bibr" target="#b32">[33]</ref> and Paris6k <ref type="bibr" target="#b33">[34]</ref> datasets. Both datasets contain a set of images (5062 for Oxford5k and 6300 for Paris6k) depicting 11 different landmarks together with distractors. For each of the 11 landmarks there are 5 different query regions defined by a bounding box, constituting 55 query regions per dataset. The performance is reported as mean average precision (mAP) <ref type="bibr" target="#b32">[33]</ref>.</p><p>In the first experiment, for each image in the dataset, multi-scale Hessian-affine features <ref type="bibr" target="#b30">[31]</ref> are extracted. Exactly the same features are described by ours and all related methods, each of them producing a 128-D descriptor per feature. Then, k-means with approximate nearest neighbor <ref type="bibr" target="#b20">[21]</ref> is used to learn a 1 million visual vocabulary on an independent dataset, that is, when evaluating on Oxford5k, the vocabulary is learned with descriptors of Paris6k and vice versa. All descriptors of testing dataset are assigned to the corresponding vocabulary, so finally, an image is represented by the histogram of visual word occurrences, i.e., the bag-of-words (BoW) <ref type="bibr" target="#b0">[1]</ref> representation, and an inverted file is used for an efficient search. Additionally, spatial verification (SV) <ref type="bibr" target="#b32">[33]</ref>, and standard query expansion (QE) <ref type="bibr" target="#b33">[34]</ref> are used to re-rank and refine the search results. Comparison with the related work on patch description is presented in <ref type="table" target="#tab_7">Table 4</ref>. HardNet+ and L2Net+ perform comparably across both datasets and all settings, with slightly better performance of HardNet+ on average across  all results (average mAP 69.5 vs. 69.1). RootSIFT, which was the best performing descriptor in image retrieval for a long time, falls behind with average mAP 66.0 across all results.</p><p>We also trained HardNet++ version -with all available training data at the moment: union of Brown and HPatches datasets, instead of just Liberty sequence from Brown for the HardNet+. It shows the benefits of having more training data and is performing best for all setups.</p><p>Finally, we compare our descriptor with the state-of-the-art image retrieval approaches that use local features. For fairness, all methods presented in <ref type="table" target="#tab_8">Table 5</ref> use the same local feature detector as described before, learn the vocabulary on an independent dataset, and use spatial verification (SV) and query expansion (QE). In our case (HardNet++-HQE), a visual vocabulary of 65k visual words is learned, with additional Hamming embedding (HE) <ref type="bibr" target="#b41">[42]</ref> technique that further refines descriptor assignments with a 128 bits binary signature. We follow the same procedure as RootSIFT-HQE <ref type="bibr" target="#b42">[43]</ref> method, by replacing RootSIFT with our learned HardNet++ descriptor. Specifically, we use: (i) weighting of the votes as a decreasing function of the Hamming distance <ref type="bibr" target="#b43">[44]</ref>; (ii) burstiness suppression <ref type="bibr" target="#b43">[44]</ref>; (iii) multiple assignments of features to visual words <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b44">45]</ref>; and (iv) QE with feature aggregation <ref type="bibr" target="#b42">[43]</ref>. All parameters are set as in <ref type="bibr" target="#b42">[43]</ref>. The performance of our method is the best reported on both Oxford5k and Paris6k when learning the vocabulary on an independent dataset (mAP 89.1 was reported <ref type="bibr" target="#b9">[10]</ref> on Oxford5k by learning it on the same dataset comprising the relevant images), and using the same amount of features (mAP 89.4 was reported <ref type="bibr" target="#b42">[43]</ref> on Oxford5k when using twice as many local features, i.e., 22M compared to 12.5M used here).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>We proposed a novel loss function for learning a local image descriptor that relies on the hard negative mining within a mini-batch and the maximization of the distance between the closest positive and closest negative patches. The proposed sampling strategy outperforms classical hard-negative mining and random sampling for softmin, triplet margin and contrastive losses.</p><p>The resulting descriptor is compact -it has the same dimensionality as SIFT (128), it shows stateof-art performance on standard matching, patch verification and retrieval benchmarks and it is fast to compute on a GPU. The training source code and the trained convnets are available at https://github.com/DagnyT/hardnet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Influence of the batch size on descriptor performance. The metric is false positive rate (FPR) at true positive rate equal to 95%, averaged over Notredame and Yosemite validation sequences.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Patch retrieval descriptor performance (mAP) vs. the number of distractors, evaluated on HPatches dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Left to right: Verification, matching and retrieval results on HPatches dataset. Marker color indicates the level of geometrical noise in: EASY, HARD and TOUGH. Marker type indicates the experimental setup. DIFFSEQ and SAMESEQ shows the source of negative examples for the verification task. VIEWPT and ILLUM indicate the type of sequences for matching. None of the descriptors is trained on HPatches.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Contribution to the gradient magnitude from the positive and negative examples. Horizontal and vertical axes show the distance from the anchor (a) to the negative (n) and positive (p) examples respectively. Softmin loss gradient quickly decreases when d(a, n) &gt; d(a, p), unlike the triplet margin loss. For the contrastive loss, negative examples with d(a, n) &gt; m contribute zero to the gradient. The triplet margin loss and the contrastive loss with a big margin behave very similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Descriptor evaluation on the W1BS patch dataset, mean area under precision-recall curve is reported. Letters denote nuisance factor, A: appearance; G: viewpoint/geometry; L: illumination; S: sensor; map2photo: satellite photo vs. map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 1 :</head><label>1</label><figDesc></figDesc><table>Patch correspondence verification performance on the Brown dataset. We report false 
positive rate at true positive rate equal to 95% (FPR95). Some papers report false discovery rate 
(FDR) instead of FPR due to bug in the source code. For consistency we provide FPR, either 
obtained from the original article or re-estimated from the given FDR (marked with *). The best 
results are in bold. 

Training 
Notredame Yosemite Liberty Yosemite Liberty Notredame 
Mean 

Test 
Liberty 
Notredame 
Yosemite 
FDR FPR 

SIFT [9] 
29.84 
22.53 
27.29 
26.55 
MatchNet*[14] 
7.04 
11.47 
3.82 
5.65 
11.6 
8.7 
7.74 
8.05 
TFeat-M* [23] 
7.39 
10.31 
3.06 
3.8 
8.06 
7.24 
6.47 
6.64 
L2Net [24] 
3.64 
5.29 
1.15 
1.62 
4.43 
3.30 
3.24 
HardNet (ours) 
3.06 
4.27 
0.96 
1.4 
3.04 
2.53 
3.00 
2.54 

Augmentation: flip, 90 
• random rotation 

GLoss+[30] 
3.69 
4.91 
0.77 
1.14 
3.09 
2.67 
2.71 
DC2ch2st+[15] 
4.85 
7.2 
1.9 
2.11 
5.00 
4.10 
4.19 
L2Net+ [24] + 
2.36 
4.7 
0.72 
1.29 
2.57 
1.71 
2.23 
HardNet+ (ours) 
2.28 
3.25 
0.57 
0.96 
2.13 
2.22 
1.97 
1.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Both descriptors significantly outperform the previous state-of-the-art, showing the superiority of the selected deep CNN architecture over the shallow TFeat model.</figDesc><table>EASY HARD TOUGH 

DIFFSEQ SAMESEQ 
VIEWPT ILLUM 

87.12% 

86.69% 

86.19% 

84.46% 

81.90% 

81.32% 

58.53% 

0 
20 
40 
60 
80 
100 

Patch Verification mAP [%] 

rSIFT 
HardTFeat 
TFeat-M* 
L2Net 
HardNet 
L2Net+ 
HardNet+ 

50.38% 

48.24% 

45.04% 

40.82% 

38.07% 

32.64% 

27.22% 

0 
20 
40 
60 
80 
100 

Image Matching mAP [%] 

rSIFT 

TFeat-M* 

HardTFeat 

L2Net 

L2Net+ 

HardNet 

HardNet+ 
66.82% 

65.26% 

63.37% 

59.64% 

55.12% 

52.03% 

42.49% 

0 
20 
40 
60 
80 
100 

Patch Retrieval mAP [%] 

rSIFT 
TFeat-M* 
HardTFeat 
L2Net 
L2Net+ 
HardNet 
HardNet+ 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Comparison of the loss functions and sampling strategies on the HPatches matching task, the mean mAP is reported. CPR stands for the regularization penalty of the correlation between descriptor channels, as proposed in [24]. Hard negative mining is performed once per epoch. Best results are in bold. HardNet uses the hardest-in-batch sampling and the triplet margin loss.</figDesc><table>Sampling / Loss 
Softmin 
Triplet margin 
Contrastive 

m = 1 
m = 1 
m = 2 

Random 
overfit 
Hard negative mining 
overfit 
Random + CPR 
0.349 
0.286 
0.007 
0.083 
Hard negative mining + CPR 
0.391 
0.346 
0.055 
0.279 

Hardest in batch (ours) 
0.474 
0.482 
0.444 
0.482 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 3 :</head><label>3</label><figDesc></figDesc><table>Comparison of the descriptors on wide baseline stereo within MODS matcher[4] on wide 
baseline stereo datasets. Number of matched image pairs and average number of inliers are reported. 
Numbers is the header corresponds to the number of image pairs in dataset. 

EF 
EVD 
OxAff 
SymB 
GDB 
WxBS 
LTLL 

Descriptor 33 inl. 15 inl. 40 
inl. 46 inl. 22 inl. 37 inl. 172 inl. 

RootSIFT 
33 
32 15 
34 40 169 45 
43 21 
52 11 
93 123 
27 
TFeat-M* 32 
30 15 
37 40 265 40 
45 16 
72 10 
62 
96 
29 
L2Net+ 
33 
34 15 
34 40 304 43 
46 19 
78 
9 
51 127 
26 
HardNet+ 
33 
35 15 
41 40 316 44 
47 21 
75 11 
54 127 
31 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Performance (mAP) evaluation on bag-of-words (BoW) image retrieval. Vocabulary consisting of 1M visual words is learned on independent dataset, that is, when evaluating on Oxford5k, the vocabulary is learned with features of Paris6k and vice versa. SV: spatial verification. QE: query expansion. The best results are highlighted in bold. All the descriptors except SIFT and HardNet++ were learned on Liberty sequence of Brown dataset [3]. HardNet++ is trained on union of Brown and HPatches [18] datasets.</figDesc><table>Oxford5k 
Paris6k 

Descriptor 
BoW 
BoW+SV BoW+QE BoW 
BoW+SV BoW+QE 

TFeat-M* [23] 46.7 
55.6 
72.2 
43.8 
51.8 
65.3 
RootSIFT [10] 55.1 
63.0 
78.4 
59.3 
63.7 
76.4 
L2Net+ [24] 
59.8 
67.7 
80.4 
63.0 
66.6 
77.2 
HardNet 
59.0 
67.6 
83.2 
61.4 
67.4 
77.5 
HardNet+ 
59.8 
68.8 
83.0 
61.0 
67.0 
77.5 

HardNet++ 
60.8 
69.6 
84.5 
65.0 
70.3 
79.1 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Performance (mAP) comparison with the state-of-the-art image retrieval with local features. Vocabulary is learned on independent dataset, that is, when evaluating on Oxford5k, the vocabulary is learned with features of Paris6k and vice versa. All presented results are with spatial verification and query expansion. VS: vocabulary size. SA: single assignment. MA: multiple assignments. The best results are highlighted in bold.</figDesc><table>Oxford5k 
Paris6k 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors were supported by the Czech Science Foundation Project GACR P103/12/G084, the Austrian Ministry for Transport, Innovation and Technology, the Federal Ministry of Science, Research and Economy, and the Province of Upper Austria in the frame of the COMET center, the CTU student grant SGS17/185/OHK3/3T/13, and the MSMT LL1303 ERC-CZ grant. Anastasiya Mishchuk was supported by the Szkocka Research Group Grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Video google: A text retrieval approach to object matching in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="page" from="1470" to="1477" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">CNN image retrieval learns from BoW: Unsupervised fine-tuning with hard examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3" to="20" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Automatic panoramic image stitching using invariant features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="59" to="73" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Wxbs: Wide baseline stereo generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<idno>Arxiv 1504.06603</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">From single image query to detailed 3D reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Filip</forename><surname>Radenovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5126" to="5134" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Johannes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan-Michael</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4104" to="4113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal correspondence network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christopher</forename><forename type="middle">B</forename><surname>Choy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Gwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Silvio</forename><surname>Savarese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manmohan</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2414" to="2422" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-DOF camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roberto</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Three things everyone should know to improve object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Relja</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2911" to="2918" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Scale &amp; affine invariant interest point detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="63" to="86" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ethan</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kurt</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gary</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2564" to="2571" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">LIFT: Learned invariant feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Kwang Moo Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Lepetit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Fua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="467" to="483" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Matchnet: Unifying feature and metric learning for patch-based matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xufeng</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3279" to="3286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to compare image patches via convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Kernel local descriptors with implicit rotation matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><surname>Bursuc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM International Conference on Multimedia Retrieval</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Domain-size pooling in local descriptors: DSP-SIFT</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingming</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefano</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5097" to="5106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">HPatches: A benchmark and evaluation of handcrafted and learned local descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karel</forename><surname>Lenc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Comparative evaluation of hand-crafted and learned local features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Johannes</forename><forename type="middle">L</forename><surname>Schonberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hans</forename><surname>Hardmeier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torsten</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Descriptor learning using convex optimisation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="243" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Fast approximate nearest neighbors with automatic algorithm configuration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marius</forename><surname>Muja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision Theory and Application (VISSAPP)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="331" to="340" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Discriminative learning of deep convolutional feature point descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eduard</forename><surname>Trulls</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luis</forename><surname>Ferraz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iasonas</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pascal</forename><surname>Fua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francesc</forename><surname>Moreno-Noguer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Learning local feature descriptors with triplets and shallow convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vassileios</forename><surname>Balntas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edgar</forename><surname>Riba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Ponsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">L2-Net: Deep learning of discriminative patch descriptor in euclidean space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fuchao</forename><surname>Bin Fan Yurun Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deeplysupervised nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Yu</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saining</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Gallagher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengyou</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhuowen</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Artificial Intelligence and Statistics</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="562" to="570" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<idno>ArXiv 1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vinod</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pytorch</surname></persName>
		</author>
		<ptr target="http://pytorch.org" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning local image descriptors with deep siamese and triplet convolutional networks by minimising global loss functions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vijay</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">G</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gustavo</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="5385" to="5394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A comparison of affine region detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krystian</forename><surname>Mikolajczyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Frederik</forename><surname>Schaffalitzky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timor</forename><surname>Kadir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luc</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">65</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="43" to="72" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">The general inefficiency of batch training for gradient descent learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Randall</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tony</forename><forename type="middle">R</forename><surname>Martinez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1429" to="1451" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Object retrieval with large vocabularies and fast spatial matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Lost in quantization: Improving particular object retrieval in large scale image databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Josef</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Robust wide baseline stereo from maximally stable extrema regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Urban</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Pajdla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="384" to="393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Efficient representation of local geometry for large scale object retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondrej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="9" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Edge foci interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lawrence</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Krishnan</forename><surname>Ramnath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="359" to="366" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Mods: Fast and robust method for twoview matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dmytro</forename><surname>Mishkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
		<idno type="doi">10.1016/j.cviu.2015.08.005</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2015.08.005" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">141</biblScope>
			<biblScope unit="page" from="81" to="93" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Image matching using local symmetry features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Daniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Noah</forename><surname>Hauagge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Snavely</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="206" to="213" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Registration of challenging image pairs: Initialization, estimation, and decision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gehua</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Charles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chia-Ling</forename><surname>Sofka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tsai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="1973" to="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Location recognition over large time lags</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Basura</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tatiana</forename><surname>Tommasi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tinne</forename><surname>Tuytelaars</surname></persName>
		</author>
		<idno type="doi">10.1016/j.cviu.2015.05.016</idno>
		<idno>1077-3142. doi</idno>
		<ptr target="https://doi.org/10.1016/j.cviu.2015.05.016" />
	</analytic>
	<monogr>
		<title level="j">Computer Vision and Image Understanding</title>
		<imprint>
			<biblScope unit="volume">139</biblScope>
			<biblScope unit="page" from="21" to="28" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Improving bag-of-features for large scale image search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="316" to="336" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Visual query expansion with or without geometry: refining local descriptors by feature aggregation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giorgos</forename><surname>Tolias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">47</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="3466" to="3476" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">On the burstiness of visual elements</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthijs</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1169" to="1176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Accurate image search using the contextual dissimilarity measure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herve</forename><surname>Jegou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cordelia</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hedi</forename><surname>Harzallah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakob</forename><surname>Verbeek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Analysis and Machine Intelligence (PAMI)</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="page" from="2" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Learning vocabularies over a fine quantization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Mikulik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Perdoch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ondřej</forename><surname>Chum</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">103</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="163" to="175" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
