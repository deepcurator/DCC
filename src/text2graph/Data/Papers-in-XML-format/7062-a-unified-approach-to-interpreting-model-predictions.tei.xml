<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><forename type="middle">M</forename><surname>Lundberg</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science Department of Genome Sciences</orgName>
								<orgName type="institution" key="instit1">University of Washington Seattle</orgName>
								<orgName type="institution" key="instit2">University of Washington Seattle</orgName>
								<address>
									<postCode>98105, 98105</postCode>
									<region>WA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science Department of Genome Sciences</orgName>
								<orgName type="institution" key="instit1">University of Washington Seattle</orgName>
								<orgName type="institution" key="instit2">University of Washington Seattle</orgName>
								<address>
									<postCode>98105, 98105</postCode>
									<region>WA, WA</region>
								</address>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Su-In</forename><surname>Lee</surname></persName>
							<email>suinlee@cs.washington.edu</email>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science Department of Genome Sciences</orgName>
								<orgName type="institution" key="instit1">University of Washington Seattle</orgName>
								<orgName type="institution" key="instit2">University of Washington Seattle</orgName>
								<address>
									<postCode>98105, 98105</postCode>
									<region>WA, WA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><forename type="middle">G</forename><surname>Allen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department" key="dep1">School of Computer Science</orgName>
								<orgName type="department" key="dep2">School of Computer Science Department of Genome Sciences</orgName>
								<orgName type="institution" key="instit1">University of Washington Seattle</orgName>
								<orgName type="institution" key="instit2">University of Washington Seattle</orgName>
								<address>
									<postCode>98105, 98105</postCode>
									<region>WA, WA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">A Unified Approach to Interpreting Model Predictions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Understanding why a model makes a certain prediction can be as crucial as the prediction's accuracy in many applications. However, the highest accuracy for large modern datasets is often achieved by complex models that even experts struggle to interpret, such as ensemble or deep learning models, creating a tension between accuracy and interpretability. In response, various methods have recently been proposed to help users interpret the predictions of complex models, but it is often unclear how these methods are related and when one method is preferable over another. To address this problem, we present a unified framework for interpreting predictions, SHAP (SHapley Additive exPlanations). SHAP assigns each feature an importance value for a particular prediction. Its novel components include: (1) the identification of a new class of additive feature importance measures, and (2) theoretical results showing there is a unique solution in this class with a set of desirable properties. The new class unifies six existing methods, notable because several recent methods in the class lack the proposed desirable properties. Based on insights from this unification, we present new methods that show improved computational performance and/or better consistency with human intuition than previous approaches.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The ability to correctly interpret a prediction model's output is extremely important. It engenders appropriate user trust, provides insight into how a model may be improved, and supports understanding of the process being modeled. In some applications, simple models (e.g., linear models) are often preferred for their ease of interpretation, even if they may be less accurate than complex ones. However, the growing availability of big data has increased the benefits of using complex models, so bringing to the forefront the trade-off between accuracy and interpretability of a model's output. A wide variety of different methods have been recently proposed to address this issue <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b0">1]</ref>. But an understanding of how these methods relate and when one method is preferable to another is still lacking.</p><p>Here, we present a novel unified approach to interpreting model predictions. <ref type="bibr" target="#b0">1</ref> Our approach leads to three potentially surprising results that bring clarity to the growing space of methods:</p><p>1. We introduce the perspective of viewing any explanation of a model's prediction as a model itself, which we term the explanation model. This lets us define the class of additive feature attribution methods (Section 2), which unifies six current methods.</p><p>2. We then show that game theory results guaranteeing a unique solution apply to the entire class of additive feature attribution methods (Section 3) and propose SHAP values as a unified measure of feature importance that various methods approximate (Section 4).</p><p>3. We propose new SHAP value estimation methods and demonstrate that they are better aligned with human intuition as measured by user studies and more effectually discriminate among model output classes than several existing methods (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Additive Feature Attribution Methods</head><p>The best explanation of a simple model is the model itself; it perfectly represents itself and is easy to understand. For complex models, such as ensemble methods or deep networks, we cannot use the original model as its own best explanation because it is not easy to understand. Instead, we must use a simpler explanation model, which we define as any interpretable approximation of the original model. We show below that six current explanation methods from the literature all use the same explanation model. This previously unappreciated unity has interesting implications, which we describe in later sections.</p><p>Let f be the original prediction model to be explained and g the explanation model. Here, we focus on local methods designed to explain a prediction f (x) based on a single input x, as proposed in LIME <ref type="bibr" target="#b4">[5]</ref>. Explanation models often use simplified inputs x that map to the original inputs through a mapping function x = h x (x ). Local methods try to ensure</p><formula xml:id="formula_0">g(z ) ≈ f (h x (z )) whenever z ≈ x .</formula><p>(Note that h x (x ) = x even though x may contain less information than x because h x is specific to the current input x.)</p><p>Definition 1 Additive feature attribution methods have an explanation model that is a linear function of binary variables:</p><formula xml:id="formula_1">g(z ) = φ 0 + M i=1 φ i z i ,<label>(1)</label></formula><p>where z ∈ {0, 1} M , M is the number of simplified input features, and φ i ∈ R.</p><p>Methods with explanation models matching Definition 1 attribute an effect φ i to each feature, and summing the effects of all feature attributions approximates the output f (x) of the original model. Many current methods match Definition 1, several of which are discussed below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">LIME</head><p>The LIME method interprets individual model predictions based on locally approximating the model around a given prediction <ref type="bibr" target="#b4">[5]</ref>. The local linear explanation model that LIME uses adheres to Equation 1 exactly and is thus an additive feature attribution method. LIME refers to simplified inputs x as "interpretable inputs," and the mapping x = h x (x ) converts a binary vector of interpretable inputs into the original input space. Different types of h x mappings are used for different input spaces. For bag of words text features, h x converts a vector of 1's or 0's (present or not) into the original word count if the simplified input is one, or zero if the simplified input is zero. For images, h x treats the image as a set of super pixels; it then maps 1 to leaving the super pixel as its original value and 0 to replacing the super pixel with an average of neighboring pixels (this is meant to represent being missing).</p><p>To find φ, LIME minimizes the following objective function:</p><formula xml:id="formula_2">ξ = arg min g∈G L(f, g, π x ) + Ω(g).<label>(2)</label></formula><p>Faithfulness of the explanation model g(z ) to the original model f (h x (z )) is enforced through the loss L over a set of samples in the simplified input space weighted by the local kernel π x . Ω penalizes the complexity of g. Since in LIME g follows Equation <ref type="bibr" target="#b0">1</ref> and L is a squared loss, Equation 2 can be solved using penalized linear regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">DeepLIFT</head><p>DeepLIFT was recently proposed as a recursive prediction explanation method for deep learning <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b6">7]</ref>. It attributes to each input x i a value C ∆xi∆y that represents the effect of that input being set to a reference value as opposed to its original value. This means that for DeepLIFT, the mapping x = h x (x ) converts binary values into the original inputs, where 1 indicates that an input takes its original value, and 0 indicates that it takes the reference value. The reference value, though chosen by the user, represents a typical uninformative background value for the feature.</p><p>DeepLIFT uses a "summation-to-delta" property that states:</p><formula xml:id="formula_3">n i=1 C ∆xi∆o = ∆o,<label>(3)</label></formula><p>where o = f (x) is the model output, ∆o = f (x) − f (r), ∆x i = x i − r i , and r is the reference input. If we let φ i = C ∆xi∆o and φ 0 = f (r), then DeepLIFT's explanation model matches Equation 1 and is thus another additive feature attribution method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3">Layer-Wise Relevance Propagation</head><p>The layer-wise relevance propagation method interprets the predictions of deep networks <ref type="bibr" target="#b0">[1]</ref>. As noted by Shrikumar et al., this menthod is equivalent to DeepLIFT with the reference activations of all neurons fixed to zero. Thus, x = h x (x ) converts binary values into the original input space, where 1 means that an input takes its original value, and 0 means an input takes the 0 value. Layer-wise relevance propagation's explanation model, like DeepLIFT's, matches Equation 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.4">Classic Shapley Value Estimation</head><p>Three previous methods use classic equations from cooperative game theory to compute explanations of model predictions: Shapley regression values <ref type="bibr" target="#b3">[4]</ref>, Shapley sampling values <ref type="bibr" target="#b8">[9]</ref>, and Quantitative Input Influence <ref type="bibr" target="#b2">[3]</ref>.</p><p>Shapley regression values are feature importances for linear models in the presence of multicollinearity. This method requires retraining the model on all feature subsets S ⊆ F , where F is the set of all features. It assigns an importance value to each feature that represents the effect on the model prediction of including that feature. To compute this effect, a model f S∪{i} is trained with that feature present, and another model f S is trained with the feature withheld. Then, predictions from the two models are compared on the current input f S∪{i} (x S∪{i} ) − f S (x S ), where x S represents the values of the input features in the set S. Since the effect of withholding a feature depends on other features in the model, the preceding differences are computed for all possible subsets S ⊆ F \ {i}. The Shapley values are then computed and used as feature attributions. They are a weighted average of all possible differences: Shapley sampling values are meant to explain any model by: (1) applying sampling approximations to Equation 4, and (2) approximating the effect of removing a variable from the model by integrating over samples from the training dataset. This eliminates the need to retrain the model and allows fewer than 2 |F | differences to be computed. Since the explanation model form of Shapley sampling values is the same as that for Shapley regression values, it is also an additive feature attribution method.</p><formula xml:id="formula_4">φ i = S⊆F \{i} |S|!(|F | − |S| − 1)! |F |! f S∪{i} (x S∪{i} ) − f S (x S ) .<label>(4)</label></formula><p>Quantitative input influence is a broader framework that addresses more than feature attributions. However, as part of its method it independently proposes a sampling approximation to Shapley values that is nearly identical to Shapley sampling values. It is thus another additive feature attribution method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Simple Properties Uniquely Determine Additive Feature Attributions</head><p>A surprising attribute of the class of additive feature attribution methods is the presence of a single unique solution in this class with three desirable properties (described below). While these properties are familiar to the classical Shapley value estimation methods, they were previously unknown for other additive feature attribution methods.</p><p>The first desirable property is local accuracy. When approximating the original model f for a specific input x, local accuracy requires the explanation model to at least match the output of f for the simplified input x (which corresponds to the original input x).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Property 1 (Local accuracy)</head><formula xml:id="formula_5">f (x) = g(x ) = φ 0 + M i=1 φ i x i (5)</formula><p>The explanation model g(x ) matches the original model f (x) when x = h x (x ), where φ 0 = f (h x (0)) represents the model output with all simplified inputs toggled off (i.e. missing).</p><p>The second property is missingness. If the simplified inputs represent feature presence, then missingness requires features missing in the original input to have no impact. All of the methods described in Section 2 obey the missingness property.</p><p>Property 2 (Missingness) The third property is consistency. Consistency states that if a model changes so that some simplified input's contribution increases or stays the same regardless of the other inputs, that input's attribution should not decrease.</p><formula xml:id="formula_6">x i = 0 =⇒ φ i = 0<label>(</label></formula><p>Property 3 (Consistency) Let f x (z ) = f (h x (z )) and z \ i denote setting z i = 0. For any two models f and f , if</p><formula xml:id="formula_7">f x (z ) − f x (z \ i) ≥ f x (z ) − f x (z \ i) (7) for all inputs z ∈ {0, 1} M , then φ i (f , x) ≥ φ i (f, x).</formula><p>Theorem 1 Only one possible explanation model g follows Definition 1 and satisfies Properties 1, 2, and 3:</p><formula xml:id="formula_8">φ i (f, x) = z ⊆x |z |!(M − |z | − 1)! M ! [f x (z ) − f x (z \ i)]<label>(8)</label></formula><p>where |z | is the number of non-zero entries in z , and z ⊆ x represents all z vectors where the non-zero entries are a subset of the non-zero entries in x .</p><p>Theorem 1 follows from combined cooperative game theory results, where the values φ i are known as Shapley values <ref type="bibr" target="#b5">[6]</ref>. <ref type="bibr" target="#b9">Young (1985)</ref> demonstrated that Shapley values are the only set of values that satisfy three axioms similar to Property 1, Property 3, and a final property that we show to be redundant in this setting (see Supplementary Material). Property 2 is required to adapt the Shapley proofs to the class of additive feature attribution methods.</p><p>Under Properties 1-3, for a given simplified input mapping h x , Theorem 1 shows that there is only one possible additive feature attribution method. This result implies that methods not based on Shapley values violate local accuracy and/or consistency (methods in Section 2 already respect missingness).</p><p>The following section proposes a unified approach that improves previous methods, preventing them from unintentionally violating Properties 1 and 3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">SHAP (SHapley Additive exPlanation) Values</head><p>We propose SHAP values as a unified measure of feature importance. These are the Shapley values of a conditional expectation function of the original model; thus, they are the solution to Equation 8, where</p><formula xml:id="formula_9">f x (z ) = f (h x (z )) = E[f (z) | z S ]</formula><p>, and S is the set of non-zero indexes in z <ref type="figure" target="#fig_2">(Figure 1</ref>). Based on Sections 2 and 3, SHAP values provide the unique additive feature importance measure that adheres to Properties 1-3 and uses conditional expectations to define simplified inputs. Implicit in this definition of SHAP values is a simplified input mapping, h x (z ) = z S , where z S has missing values for features not in the set S. Since most models cannot handle arbitrary patterns of missing input values, we approximate</p><formula xml:id="formula_10">f (z S ) with E[f (z) | z S ]</formula><p>. This definition of SHAP values is designed to closely align with the Shapley regression, Shapley sampling, and quantitative input influence feature attributions, while also allowing for connections with LIME, DeepLIFT, and layer-wise relevance propagation.</p><p>The exact computation of SHAP values is challenging. However, by combining insights from current additive feature attribution methods, we can approximate them. We describe two model-agnostic approximation methods, one that is already known (Shapley sampling values) and another that is novel (Kernel SHAP). We also describe four model-type-specific approximation methods, two of which are novel (Max SHAP, Deep SHAP). When using these methods, feature independence and model linearity are two optional assumptions simplifying the computation of the expected values (note thatS is the set of features not in S):</p><formula xml:id="formula_11">f (h x (z )) = E[f (z) | z S ] SHAP explanation model simplified input mapping (9) = E zS |z S [f (z)] expectation over zS | z S (10) ≈ E zS [f (z)]</formula><p>assume feature independence (as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref></p><formula xml:id="formula_12">) (11) ≈ f ([z S , E[zS]]).</formula><p>assume model linearity (12)</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Model-Agnostic Approximations</head><p>If we assume feature independence when approximating conditional expectations (Equation 11), as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b2">3]</ref>, then SHAP values can be estimated directly using the Shapley sampling values method <ref type="bibr" target="#b8">[9]</ref> or equivalently the Quantitative Input Influence method <ref type="bibr" target="#b2">[3]</ref>. These methods use a sampling approximation of a permutation version of the classic Shapley value equations (Equation 8). Separate sampling estimates are performed for each feature attribution. While reasonable to compute for a small number of inputs, the Kernel SHAP method described next requires fewer evaluations of the original model to obtain similar approximation accuracy (Section 5).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Kernel SHAP (Linear LIME + Shapley values)</head><p>Linear LIME uses a linear explanation model to locally approximate f , where local is measured in the simplified binary input space. At first glance, the regression formulation of LIME in Equation 2 seems very different from the classical Shapley value formulation of Equation 8. However, since linear LIME is an additive feature attribution method, we know the Shapley values are the only possible solution to Equation 2 that satisfies Properties 1-3 -local accuracy, missingness and consistency. A natural question to pose is whether the solution to Equation 2 recovers these values. The answer depends on the choice of loss function L, weighting kernel π x and regularization term Ω. The LIME choices for these parameters are made heuristically; using these choices, Equation 2 does not recover the Shapley values. One consequence is that local accuracy and/or consistency are violated, which in turn leads to unintuitive behavior in certain circumstances (see Section 5).</p><p>Below we show how to avoid heuristically choosing the parameters in Equation 2 and how to find the loss function L, weighting kernel π x , and regularization term Ω that recover the Shapley values.</p><p>Theorem 2 (Shapley kernel) Under Definition 1, the specific forms of π x , L, and Ω that make solutions of Equation 2 consistent with Properties 1 through 3 are:</p><formula xml:id="formula_13">Ω(g) = 0, π x (z ) = (M − 1) (M choose |z |)|z |(M − |z |) , L(f, g, π x ) = z ∈Z [f (h x (z )) − g(z )] 2 π x (z ),</formula><p>where |z | is the number of non-zero elements in z .</p><p>The proof of Theorem 2 is shown in the Supplementary Material.</p><p>It is important to note that π x (z ) = ∞ when |z | ∈ {0, M }, which enforces φ 0 = f x (∅) and f (x) = M i=0 φ i . In practice, these infinite weights can be avoided during optimization by analytically eliminating two variables using these constraints.</p><p>Since g(z ) in Theorem 2 is assumed to follow a linear form, and L is a squared loss, Equation 2 can still be solved using linear regression. As a consequence, the Shapley values from game theory can be computed using weighted linear regression. <ref type="bibr" target="#b1">2</ref> Since LIME uses a simplified input mapping that is equivalent to the approximation of the SHAP mapping given in Equation 12, this enables regression-based, model-agnostic estimation of SHAP values. Jointly estimating all SHAP values using regression provides better sample efficiency than the direct use of classical Shapley equations (see <ref type="bibr">Section 5)</ref>.</p><p>The intuitive connection between linear regression and Shapley values is that Equation 8 is a difference of means. Since the mean is also the best least squares point estimate for a set of data points, it is natural to search for a weighting kernel that causes linear least squares regression to recapitulate the Shapley values. This leads to a kernel that distinctly differs from previous heuristically chosen kernels <ref type="figure" target="#fig_3">(Figure 2A</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Model-Specific Approximations</head><p>While Kernel SHAP improves the sample efficiency of model-agnostic estimations of SHAP values, by restricting our attention to specific model types, we can develop faster model-specific approximation methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Linear SHAP</head><p>For linear models, if we assume input feature independence (Equation 11), SHAP values can be approximated directly from the model's weight coefficients.</p><formula xml:id="formula_14">Corollary 1 (Linear SHAP) Given a linear model f (x) = M j=1 w j x j + b: φ 0 (f, x) = b and φ i (f, x) = w j (x j − E[x j ])</formula><p>This follows from Theorem 2 and Equation 11, and it has been previously noted by Štrumbelj and Kononenko <ref type="bibr" target="#b8">[9]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Low-Order SHAP</head><p>Since linear regression using Theorem 2 has complexity O(2 M + M 3 ), it is efficient for small values of M if we choose an approximation of the conditional expectations (Equation 11 or 12). </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Max SHAP</head><p>Using a permutation formulation of Shapley values, we can calculate the probability that each input will increase the maximum value over every other input. Doing this on a sorted order of input values lets us compute the Shapley values of a max function with</p><formula xml:id="formula_15">M inputs in O(M 2 ) time instead of O(M 2 M )</formula><p>. See Supplementary Material for the full algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Deep SHAP (DeepLIFT + Shapley values)</head><p>While Kernel SHAP can be used on any model, including deep models, it is natural to ask whether there is a way to leverage extra knowledge about the compositional nature of deep networks to improve computational performance. We find an answer to this question through a previously unappreciated connection between Shapley values and DeepLIFT <ref type="bibr" target="#b7">[8]</ref>. If we interpret the reference value in <ref type="figure" target="#fig_5">Equation  3</ref> as representing E[x] in Equation 12, then DeepLIFT approximates SHAP values assuming that the input features are independent of one another and the deep model is linear. DeepLIFT uses a linear composition rule, which is equivalent to linearizing the non-linear components of a neural network. Its back-propagation rules defining how each component is linearized are intuitive but were heuristically chosen. Since DeepLIFT is an additive feature attribution method that satisfies local accuracy and missingness, we know that Shapley values represent the only attribution values that satisfy consistency. This motivates our adapting DeepLIFT to become a compositional approximation of SHAP values, leading to Deep SHAP.</p><p>Deep SHAP combines SHAP values computed for smaller components of the network into SHAP values for the whole network. It does so by recursively passing DeepLIFT's multipliers, now defined in terms of SHAP values, backwards through the network as in <ref type="figure" target="#fig_3">Figure 2B</ref>:</p><formula xml:id="formula_16">m xj f3 = φ i (f 3 , x) x j − E[x j ]<label>(13)</label></formula><formula xml:id="formula_17">∀ j∈{1,2} m yifj = φ i (f j , y) y i − E[y i ]<label>(14)</label></formula><formula xml:id="formula_18">m yif3 = 2 j=1</formula><p>m yifj m xj f3 chain rule (15)  </p><formula xml:id="formula_19">φ i (f 3 , y) ≈ m yif3 (y i − E[y i ]) linear approximation<label>(16)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Computational and User Study Experiments</head><p>We evaluated the benefits of SHAP values using the Kernel SHAP and Deep SHAP approximation methods. First, we compared the computational efficiency and accuracy of Kernel SHAP vs. LIME and Shapley sampling values. Second, we designed user studies to compare SHAP values with alternative feature importance allocations represented by DeepLIFT and LIME. As might be expected, SHAP values prove more consistent with human intuition than other methods that fail to meet Properties 1-3 (Section 2). Finally, we use MNIST digit image classification to compare SHAP with DeepLIFT and LIME.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1">Computational Efficiency</head><p>Theorem 2 connects Shapley values from game theory with weighted linear regression. Kernal SHAP uses this connection to compute feature importance. This leads to more accurate estimates with fewer evaluations of the original model than previous sampling-based estimates of <ref type="bibr">Equation 8</ref>, particularly when regularization is added to the linear model ( <ref type="figure" target="#fig_5">Figure 3</ref>). Comparing Shapley sampling, SHAP, and LIME on both dense and sparse decision tree models illustrates both the improved sample efficiency of Kernel SHAP and that values from LIME can differ significantly from SHAP values that satisfy local accuracy and consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2">Consistency with Human Intuition</head><p>Theorem 1 provides a strong incentive for all additive feature attribution methods to use SHAP values. Both LIME and DeepLIFT, as originally demonstrated, compute different feature importance values. To validate the importance of Theorem 1, we compared explanations from LIME, DeepLIFT, and SHAP with user explanations of simple models (using Amazon Mechanical Turk). Our testing assumes that good model explanations should be consistent with explanations from humans who understand that model.</p><p>We compared LIME, DeepLIFT, and SHAP with human explanations for two settings. The first setting used a sickness score that was higher when only one of two symptoms was present ( <ref type="figure">Figure 4A</ref>). The second used a max allocation problem to which DeepLIFT can be applied. Participants were told a short story about how three men made money based on the maximum score any of them achieved ( <ref type="figure">Figure 4B</ref>). In both cases, participants were asked to assign credit for the output (the sickness score or money won) among the inputs (i.e., symptoms or players). We found a much stronger agreement between human explanations and SHAP than with other methods. SHAP's improved performance for max functions addresses the open problem of max pooling functions in DeepLIFT <ref type="bibr" target="#b6">[7]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3">Explaining Class Differences</head><p>As discussed in Section 4.2, DeepLIFT's compositional approach suggests a compositional approximation of SHAP values (Deep SHAP). These insights, in turn, improve DeepLIFT, and a new version includes updates to better match Shapley values <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure">Figure 5</ref> extends DeepLIFT's convolutional network example to highlight the increased performance of estimates that are closer to SHAP values. The pre-trained model and <ref type="figure">Figure 5</ref> example are the same as those used in <ref type="bibr" target="#b6">[7]</ref>, with inputs normalized between 0 and 1. Two convolution layers and 2 dense layers are followed by a 10-way softmax output layer. Both DeepLIFT versions explain a normalized version of the linear layer, while SHAP (computed using Kernel SHAP) and LIME explain the model's output. SHAP and LIME were both run with 50k samples <ref type="figure" target="#fig_2">(Supplementary Figure 1)</ref>; to improve performance, LIME was modified to use single pixel segmentation over the digit pixels. To match <ref type="bibr" target="#b6">[7]</ref>, we masked 20% of the pixels chosen to switch the predicted class from 8 to 3 according to the feature attribution given by each method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Conclusion</head><p>The growing tension between the accuracy and interpretability of model predictions has motivated the development of methods that help users interpret predictions. The SHAP framework identifies the class of additive feature importance methods (which includes six previous methods) and shows there is a unique solution in this class that adheres to desirable properties. The thread of unity that SHAP weaves through the literature is an encouraging sign that common principles about model interpretation can inform the development of future methods.</p><p>We presented several different estimation methods for SHAP values, along with proofs and experiments showing that these values are desirable. Promising next steps involve developing faster model-type-specific estimation methods that make fewer assumptions, integrating work on estimating interaction effects from game theory, and defining new explanation model classes.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>For</head><label></label><figDesc>Shapley regression values, h x maps 1 or 0 to the original input space, where 1 indicates the input is included in the model, and 0 indicates exclusion from the model. If we let φ 0 = f ∅ (∅), then the Shapley regression values match Equation 1 and are hence an additive feature attribution method.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>6 )</head><label>6</label><figDesc>Missingness constrains features where x i = 0 to have no attributed impact.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: SHAP (SHapley Additive exPlanation) values attribute to each feature the change in the expected model prediction when conditioning on that feature. They explain how to get from the base value E[f (z)] that would be predicted if we did not know any features to the current output f (x). This diagram shows a single ordering. When the model is non-linear or the input features are not independent, however, the order in which features are added to the expectation matters, and the SHAP values arise from averaging the φ i values across all possible orderings.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (A) The Shapley kernel weighting is symmetric when all possible z vectors are ordered by cardinality there are 2 15 vectors in this example. This is distinctly different from previous heuristically chosen kernels. (B) Compositional models such as deep neural networks are comprised of many simple components. Given analytic solutions for the Shapley values of the components, fast approximations for the full model can be made using DeepLIFT's style of back-propagation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Since the SHAP values for the simple network components can be efficiently solved analytically if they are linear, max pooling, or an activation function with just one input, this composition rule enables a fast approximation of values for the whole model. Deep SHAP avoids the need to heuristically choose ways to linearize components. Instead, it derives an effective linearization from the SHAP values computed for each component. The max function offers one example where this leads to improved attributions (see Section 5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Comparison of three additive feature attribution methods: Kernel SHAP (using a debiased lasso), Shapley sampling values, and LIME (using the open source implementation). Feature importance estimates are shown for one feature in two models as the number of evaluations of the original model function increases. The 10th and 90th percentiles are shown for 200 replicate estimates at each sample size. (A) A decision tree model using all 10 input features is explained for a single input. (B) A decision tree using only 3 of 100 input features is explained for a single input.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :Figure 5 :</head><label>45</label><figDesc>Figure 4: Human feature impact estimates are shown as the most common explanation given among 30 (A) and 52 (B) random individuals, respectively. (A) Feature attributions for a model output value (sickness score) of 2. The model output is 2 when fever and cough are both present, 5 when only one of fever or cough is present, and 0 otherwise. (B) Attributions of profit among three men, given according to the maximum number of questions any man got right. The first man got 5 questions right, the second 4 questions, and the third got none right, so the profit is $5.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://github.com/slundberg/shap 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">During the preparation of this manuscript we discovered this parallels an equivalent constrained quadratic minimization formulation of Shapley values proposed in econometrics [2].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported by a National Science Foundation (NSF) DBI-135589, NSF CAREER DBI-155230, American Cancer Society 127332-RSG-15-097-01-TBG, National Institute of Health (NIH) AG049196, and NSF Graduate Research Fellowship. We would like to thank Marco Ribeiro, Erik Štrumbelj, Avanti Shrikumar, Yair Zick, the Lee Lab, and the NIPS reviewers for feedback that has significantly improved this work.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">On pixel-wise explanations for non-linear classifier decisions by layerwise relevance propagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sebastian</forename><surname>Bach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS One</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page">130140</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Extremal principle solutions of games in characteristic function form: core, Chebychev and Shapley value generalizations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Charnes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Econometrics of Planning and Efficiency</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="123" to="133" />
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Algorithmic transparency via quantitative input influence: Theory and experiments with learning systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anupam</forename><surname>Datta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shayak</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Zick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="598" to="617" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Analysis of regression in game theory approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stan</forename><surname>Lipovetsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Conklin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Applied Stochastic Models in Business and Industry</title>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page" from="319" to="330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Why should i trust you?: Explaining the predictions of any classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameer</forename><surname>Marco Tulio Ribeiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carlos</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guestrin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</title>
		<meeting>the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1135" to="1144" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A value for n-person games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lloyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shapley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Contributions to the Theory of Games</title>
		<imprint>
			<date type="published" when="1953" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="307" to="317" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Learning Important Features Through Propagating Activation Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peyton</forename><surname>Greenside</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anshul</forename><surname>Kundaje</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.02685</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Not Just a Black Box: Learning Important Features Through Propagating Activation Differences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Avanti</forename><surname>Shrikumar</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.01713</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Explaining prediction models and individual predictions with feature contributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erik</forename><surname>Štrumbelj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Igor</forename><surname>Kononenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Knowledge and information systems</title>
		<imprint>
			<biblScope unit="volume">41</biblScope>
			<biblScope unit="page" from="647" to="665" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monotonic solutions of cooperative games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Young</forename><surname>H Peyton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Game Theory</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="65" to="72" />
			<date type="published" when="1985" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
