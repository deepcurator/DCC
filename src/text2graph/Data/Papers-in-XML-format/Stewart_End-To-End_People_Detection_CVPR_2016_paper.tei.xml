<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:25+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">End-to-end people detection in crowded scenes</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Max Planck Institute for Informatics</orgName>
								<address>
									<country key="DE">Germany</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
								<address>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">End-to-end people detection in crowded scenes</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>In this paper we propose a new architecture for detecting objects in images. We strive for an end-to-end approach that accepts images as input and directly generates a set of object bounding boxes as output. This task is challenging because it demands both distinguishing objects from the background and correctly estimating the number of distinct objects and their locations. Such an end-to-end approach capable of directly outputting predictions would be advantageous over methods that first generate a set of bounding boxes, evaluate them with a classifier, and then perform some form of merging or non-maximum suppression on an overcomplete set of detections.</p><p>Sequentially generating a set of detections has an important advantage in that multiple detections on the same object can be avoided by remembering the previously generated output. To control this generation process, we use a recurrent neural network with LSTM units. To produce intermediate representations, we use expressive image fea-tures from GoogLeNet that are further fine-tuned as part of our system. Our architecture can thus be seen as a "decoding" process that converts an intermediate representation of an image into a set of predicted objects. The LSTM can be seen as a "controller" that propagates information between decoding steps and controls the location of the next output (see <ref type="figure" target="#fig_2">Fig. 2</ref> for an overview). Importantly, our trainable end-to-end system allows joint tuning of all components via back-propagation.</p><p>One of the key limitations of merging and non-maximum suppression utilized in <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref> is that these methods typically don't have access to image information, and instead must perform inference solely based on properties of bounding boxes (e.g. distance and overlap). This usually works for isolated objects, but often fails when object instances overlap. In the case of overlapping instances, image information is necessary to decide where to place boxes and how many of them to output. As a workaround, several approaches proposed specialized solutions that specifically address pre-defined constellations of objects (e.g. pairs of pedestrians) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23]</ref>. Here, we propose a generic architecture that does not require a specialized definition of object constellations, is not limited to pairs of objects, and is fully trainable.</p><p>We specifically focus on the task of people detection as an important example of this problem. In crowded scenes such as the one shown in <ref type="figure" target="#fig_1">Fig. 1</ref>, multiple people often occur in close proximity, making it particularly challenging to distinguish between nearby individuals.</p><p>The key contribution of this paper is a trainable, end-toend approach that jointly predicts the objects in an image. This lies in contrast to existing methods that treat prediction or classification of each bonding box as an independent problem and require post-processing on the set of detections. We demonstrate that our approach is superior to existing architectures on a challenging dataset of crowded scenes with large numbers of people. A technical contribution of this paper is a novel loss function for sets of objects that combines elements of localization and detection. An-  other technical contribution is to show that a chain of LSTM units can be successfully utilized to decode image content into a coherent real-valued output of variable length. We envision this technique to be valuable in other structured computer vision prediction tasks such as multi-person tracking and articulated pose estimation of multiple people.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Related work</head><p>Detection of multiple objects in the presence of occlusions has been a notorious problem in computer vision. Early work employed a codebook of local features and Hough voting <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b1">2]</ref>, but still required complex tuning and multi-stage pipelines. Importantly, these models utilized weak representations based on local features that are outperformed by modern deep representations.</p><p>To overcome the difficulties of predicting multiple objects in close proximity, several attempts have been made to jointly predict constellations of objects <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b14">15]</ref>. Our work is more general, as we do not explicitly define these groups, and instead let the model learn any features that are necessary for finding occluded instances.</p><p>Currently, the best performing object detectors operate either by densely scanning the image in a sliding window fashion <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b15">16]</ref>, or by using a proposal mechanism such as <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b20">21]</ref>, and leveraging CNNs to classify a sparsified set of proposals <ref type="bibr" target="#b5">[6]</ref>. Both approaches yield bounding boxes describing image regions that contain an object. Each method then prunes the network outputs by merging heavily overlapping instances. This works well for images with few object instances that do not overlap, but often fails in the presence of strong occlusions.</p><p>For example, Faster R-CNN <ref type="bibr" target="#b15">[16]</ref> learns class independent proposals that are subsequently classified with a CNN. Like Faster R-CNN, we propose a set of bounding boxes from images, but these proposals directly correspond to object instances and do not require post-processing. The Faster R-CNN outputs are necessarily sparse, whereas our system is able to generate predictions in arbitrarily close proximity.</p><p>Our approach is related to the OverFeat model <ref type="bibr" target="#b16">[17]</ref>. We rely on a regression module to generate boxes from a CNN encoding. However, in our case distinct boxes are generated as part of an integrated process, and not independently as in OverFeat. As a result, each output box corresponds directly to an object detected in the image, and we do not require merging or non maximum suppression. Another important advantage of our approach is that it outputs a confidence corresponding to each output that is trained end-to-end. In the case of OverFeat, an end-to-end trained confidence prediction is not available, as the output is the result of a heuristic merging procedure.</p><p>Our work is related to <ref type="bibr" target="#b24">[25]</ref> in that the training objective in our model jointly considers detections on multiple object instances. The main difference is that while the model in <ref type="bibr" target="#b24">[25]</ref> is trained to optimize post non-maximum suppression (NMS) accuracy, it still performs standard detection and NMS at test time, and is thus susceptible to the same difficulties as other models (e.g. suppressing detections on two object instances close to each other). In constrast, our model jointly generates output bounding boxes at test time, allowing it to correctly detect even strongly occluded objects.</p><p>Our work uses tools from recent neural network models for predicting sequences <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b18">19]</ref>. As in <ref type="bibr" target="#b18">[19]</ref>, we rely on an LSTM to predict variable length outputs. Unlike in language generation, detection requires that a system generate over a 2D output space, which lacks a natural linear ordering. MultiBox addressed this challenge by introducing a loss function that allows unordered predictions to be permuted to match ground-truth instances during training <ref type="bibr" target="#b20">[21]</ref>. Faster R-CNN addressed this challenge by partitioning ob- jects into 9 categories with 3 scales and 3 aspect ratios, allowing the net to directly produce multiple overlapping objects provided that they are of different sizes <ref type="bibr" target="#b15">[16]</ref>.</p><p>We build on these contributions by leveraging the capacity of our recurrent decoder to make joint predictions in sequence. In addition to computing an optimal matching of predictions to ground-truth, our loss function encourages the model to make predictions in order of descending confidence. Suitable loss functions have previously been proposed in structured speech recognition and natural language processing <ref type="bibr" target="#b6">[7]</ref>. Here we propose such a loss function for object detection.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Overview</head><p>Deep convolutional architectures such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b19">20]</ref> construct image representations that are effective for a variety of tasks. These architectures have been leveraged for detection, albeit primarily by adapting them into a classification or regression framework. Deep representations have sufficient power to jointly encode the appearance of multiple instances, but one must augment them with a component for multiple instance prediction to realize this potential. In this paper, we consider recurrent neural networks (RNN), and in particular LSTM units <ref type="bibr" target="#b7">[8]</ref> as a candidate for such a component. The key properties that make the combination of deep CNN's with RNN-based decoders appealing are (1) the ability to directly tap into powerful deep convolutional representations and (2) the ability to generate coherent sets of predictions of variable length. These properties have been leveraged successfully in <ref type="bibr" target="#b10">[11]</ref> to generate image captions, and in <ref type="bibr" target="#b18">[19]</ref> for machine translation. The ability to generate coherent sets is particularly important in our case because our system needs to remember previously generated predictions and avoid multiple predictions of the same target.</p><p>We construct a model that first encodes an image into high level descriptors via a convolutional architecture (e.g. <ref type="bibr" target="#b19">[20]</ref>), and then decodes that representation into a set of bounding boxes. As a core machinery for predicting variable length output, we build on a recurring network of LSTM units. An overview of our model is shown on <ref type="figure" target="#fig_2">Fig. 2</ref>. We transform each image into a grid of 1024 dimensional feature descriptors at strided regions throughout the image. The 1024 dimensional vector summarizes the contents of the region and carries rich information regarding the positions of objects. The LSTM draws from this information source and acts as a controller in the decoding of a region. At each step, the LSTM outputs a new bounding box and a corresponding confidence that a previously undetected person will be found at that location. Boxes are encouraged to be produced in order of descending confidence. When the LSTM is unable to find another box in the region with a confidence above a prespecified threshold, a stop symbol is produced. The sequence of outputs is collected and presented as a final description of all object instances in the region.</p><p>The main computational pipeline in our approach involves feed-forward processing only, which allows for fast implementation. On a modern GPU the approach runs at 6 frames per second on 640x480 images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Loss function</head><p>The architecture introduced in Sec. 2.1 predicts a set of candidate bounding boxes along with a confidence score corresponding to each box. Hypotheses are generated in sequence and later predictions depend on previous ones via the memory states of the LSTM. At each recurrence, the LSTM outputs an object bounding box b = {b pos , b c },</p><formula xml:id="formula_0">where b pos = (b x , b y , b w , b h ) ∈ R</formula><p>4 is a relative position, width and height of the bounding box, and b c ∈ [0, 1] is a real-valued confidence. Confidence values lower than a pre-specified threshold (e.g. 0.5) will be interpreted as a stop symbol at test time. Higher values of the bounding box confidence b c should indicate that the box is more likely to correspond to a true positive. We denote the corresponding set of ground truth bounding boxes as G = {b i |i = 1, . . . , M }, and the set of candidate bounding boxes generated by the model as C = {b j |j = 1, . . . , N }. In the following we introduce a loss function suitable for guiding the learning process towards the desired output.</p><p>Consider the example in <ref type="figure" target="#fig_3">Fig. 3</ref>, which schematically shows a detector with four generated hypotheses, each numbered by its prediction step, which we denote as rank. Note the typical detection mistakes such as false positives (hypothesis 3), imprecise localizations (hypothesis 1), and multiple predictions of the same ground-truth instance (hypotheses 1 and 2). Different mistakes require different kinds of feedback. In the case of hypothesis 1, the box location must be fine-tuned. Conversely, hypothesis 3 is a false positive, and the model should instead abandon the prediction by assigning a low confidence score. Hypothesis 2 is a second prediction on the target already reported by hypothesis 1, and should be abandoned as well. To capture these relationships, we introduce a matching algorithm that assigns a unique candidate hypothesis to each ground-truth. The algorithm returns an injective function f : G → C , i.e. f(i) is the index of candidate hypothesis assigned to ground-truth hypothesis i.</p><p>Given f, we define a loss function on pairs of sets G and C as</p><formula xml:id="formula_1">L(G, C, f ) = α |G| i=1 l pos (b i pos ,b f (i) pos ) + |C| j=1 l c (b j c , y j ) (1) where l pos = b i pos −b f (i)</formula><p>pos 1 is a displacement between the position of ground-truth and candidate hypotheses, and l c is a cross-entropy loss on a candidate's confidence that it would be matched to a ground-truth. The label for this cross-entropy loss is provided by y j . It is defined from the matching function as y j = ✶{f −1 (j) = ∅}. α is a term trading off between confidence errors and localization errors. We set α = 0.03 with cross validation. Note that for a fixed matching, we can update the network by backpropagating the gradient of this loss function.</p><p>As an naïve baseline, we consider a simple matching strategy based on the fixed ordering of the ground-truth bounding boxes. We sort ground-truth boxes by image position from top to bottom and from left to right. This fixed order matching sequentially assigns candidates to the sorted ground-truth. We refer to this matching function as "fixed order" matching, denoting it as f fix , and the corresponding loss function as L fix .</p><p>Hungarian loss: The limitation of the fixed order matching is that it might incorrectly assign candidate hypotheses to ground-truth instances when the decoding process produces false positives or false negatives. This issue persists for any specific ordering chosen by f fix . We thus explore loss functions that consider all possible one-to-one assignments between elements in C and G.</p><p>Recall that one of the principled objectives of our model is to output a coherent sequence of predictions on multiple objects. We define the stopping criterion for the generation process to be when a prediction score falls below a specified threshold. For such a score threshold to make sense, we must encourage the model to generate correct hypotheses early in the sequence, and to avoid generating lowconfidence predictions before high-confidence ones. Therefore, when two hypotheses both significantly overlap the same ground-truth (e.g. hypotheses 1 and 2 in <ref type="figure" target="#fig_3">Fig. 3)</ref>, we prefer to match the hypothesis that appears earlier in the predicted sequence.</p><p>To formalize this notion, we introduce the following comparison function between hypotheses and ground-truth:</p><formula xml:id="formula_2">∆(b i ,b j ) = (o ij , r j , d ij )<label>(2)</label></formula><p>The function ∆ : G × C → N × N × R returns a tuple where d ij is the L 1 distance between bounding box locations, r j is the rank or index ofb j in the prediction sequence output by the LSTM, and o ij ∈ {0, 1} is a variable penalizing hypotheses that do not sufficiently overlap a groundtruth instance. Here, the overlapping criterion requires that a candidate's center lie within the extent of the ground-truth bounding box. The o ij variable makes an explicit distinction between localization and detection errors. We define a lexicographic ordering on tuples produced by ∆. That is, when evaluating which of two hypotheses will be assigned to a ground-truth, overlap is paramount, followed by rank and then fine-grained localization.</p><p>Given the definition of the comparison function ∆ in Eq.2, we find the minimal cost bipartite matching between C and G in polynomial time via the Hungarian algorithm. Note that the Hungarian algorithm is applicable to any graph with edge weights that have well-defined addition and pairwise comparison operations. To that end, we define (+) as element-wise addition and (&lt;) as lexicographic comparison. For the example in <ref type="figure" target="#fig_3">Fig. 3</ref>, correctly matching hypotheses 1 and 4 would cost (0, 5, 0.4), whereas matching 1 and 3 would cost <ref type="figure" target="#fig_1">(1, 4, 2.</ref>3), and matching 2 and 4 would cost (0, 6, 0.2). Note how the first term, used for detecting overlap, properly handles the case where a hypothesis has low rank, but is too far from the ground-truth to be a sensible match (as is the case for hypothesis 3 in <ref type="figure" target="#fig_3">Fig. 3)</ref>. We refer to the corresponding loss for this matching as the Hungarian loss and denote is as L hung .</p><p>We</p><note type="other">also consider a simplified version of L hung where only the top k = |G| ranked predictions from C are considered for matching. Note that this is equivalent to removing or zeroing out the pairwise matching terms o ij in Eq. 2. We denote this loss as L firstk . We experimentally compare L fix , L firstk , and L hung in Sec. 4, showing that L hung leads to the best results.</note><p>Loss function analysis Our net is differentiable almost everywhere (DAE), as it is a composition of DAE functions. In neighborhoods where the matching is locally constant, L hung is DAE as well. Further, the matching will be constant in the neighborhood of points for which the optimal matching cost is ǫ-lower than any other matching and all overlap terms hold strictly. In practice, this will occur for every iteration of training, so we may be confident in using gradient descent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Implementation details</head><p>We constructed our model to encode an image into a 15x20 grid of 1024-dimensional top level GoogLeNet features. Each cell in the grid has a receptive field of size 139x139, and is trained to produce the set of all bounding boxes intersecting the central 64x64 region. The 64x64 size was chosen to be large enough to capture challenging local occlusion interactions. Larger regions may also be used, but provide little additional on our scenes, where few occlusion interactions span that scale. 300 distinct LSTM controllers are run in parallel, one for each 1x1x1024 cell of the grid.</p><p>Our LSTM units have 250 memory states, no bias terms, and no output nonlinearities. At each step, we concatenate the GoogLeNet features with the output of the previous LSTM unit, and feed the result into the next LSTM unit. We have produced comparable results by only feeding the image into the first LSTM unit, indicating that multiple presentations of the image may not be necessary. Producing each region of the full 480x640 image in parallel gives an efficient batching of the decoding process.</p><p>Our model must learn to regress on bounding box locations through the LSTM decoder. During training, the decoder outputs an overcomplete set of bounding boxes, each with a corresponding confidence. For simplicity and batching efficiency, the cardinality of the overcomplete set is fixed, regardless of the number of ground-truth boxes. This trains the LSTM to output high confidence scores and correct localizations for boxes corresponding to the ground truth, and low confidence scores elsewhere. Because early outputs are preferred during matching, the model learns to output high confidence, easy boxes first. In our dataset, few regions have more than 4 instances, and we limit the overcomplete set to 5 predictions. Larger numbers of predictions neither improved nor degraded performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model training:</head><p>We use the Caffe open source deep learning framework <ref type="bibr" target="#b9">[10]</ref> for training and evaluation. The decoder portion of our model is a custom LSTM implementation. We train with learning rate ǫ = 0.2 and momentum 0.5. Gradients are clipped to have maximum 2-norm of 0.1 across the network. We decreased the learning rate by a multiple of 0.8 every 100,000 iterations. Convergence is reached at 800,000 iterations. We use dropout with probability 0.15 on LSTM outputs. Removing dropout reduced average precision (AP) by 0.01</p><p>Training proceeds on all subregions of one image at each iteration. Parallelism of the LSTM decoders across regions mitigates efficiency gains for larger batch sizes. All weights are tied between regions and LSTM steps. However, we were surprised to find slight performance gains when using separate weights connecting LSTM outputs to predicted candidates at each step. These weights remain tied across regions. Tying these weights reduced AP from 0.85 to 0.82. Initialization: GoogLeNet weights are initialized with weights pretrained on Imagenet <ref type="bibr" target="#b2">[3]</ref>.</p><p>Fine-tuning of GoogLeNet features to meet the new demands of the decoder is critical. Training without fine-tuning GoogLeNet reduced AP by 0.29.</p><p>All weights in the decoder are initialized from a uniform distribution in [-0.1, 0.1]. Typical LSTM input activations differ significantly from our pretrained GoogLeNet, which has activations in the range <ref type="bibr">[-80, 80]</ref>. To compensate for this mismatch, we use a scale layer to decrease GoogLeNet activations by a factor of 100 before feeding them into the LSTM. Likewise, the initial standard deviation of the fully connected layers output is on the order of 0.3, but bounding box pixel locations and sizes vary in <ref type="bibr">[-64, 64</ref>]. Thus, we scale up the regression predictions by a factor of 100 before comparing them with ground-truth. Note that these modifications are the same as changing weight initializations only if one also introduces proportional learning rate multipliers.</p><p>Stitching: Our algorithm is trained to predict multiple bounding boxes within 64x64 pixel regions. To apply it to a full 640x480 images at test time we generate predictions from each region in a 15x20 grid of the image and then use a stitching algorithm to recursively merge predictions from successive cells on the grid.</p><p>The stitching process is illustrated in <ref type="figure" target="#fig_4">Fig. 4</ref>. At a given iteration, let A denote the current set of all accepted bounding box predictions. We process a new region, evaluating the decoder until a stop symbol is produced and collect a set C of newly proposed bounding boxes. Some of these new bounding boxes may correspond to previous predictions.To remove multiple predictions on the same object we define a bipartite matching problem related to that in section 2.2 with a pairwise loss term</p><formula xml:id="formula_3">∆ ′ : A × C → N × R given as ∆ ′ (b i ,b j ) = (m ij , d ij ).</formula><p>Here, m ij states whether two boxes do not intersect, and d ij is a local disambiguation term given by the L 1 distance between boxes. As before, we leverage the Hungarian algorithm to find a minimum cost matching in polynomial time. We examine each match pair, (b,b), and add any candidateb that does not overlap with its match b to the set of accepted boxes. The important differences between this process and non-maximum suppression is that (1) boxes from the same region do not suppress each other and (2) each box can suppress at most one other box. Jointly this allows to generate predictions on instances even if they overlap significantly in the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental results</head><p>Datasets and evaluation metrics: We evaluate our approach on two datasets. We conduct the primary development and evaluation on a new large dataset of people images. The images were collected from a busy scene using video footage available from a public webcam. We refer to this dataset as Brainwash in the following. We found that having an abundance of images available in Brainwash enabled us to focus on the method development without being potentially limited by a small training set size. We then validate our results on a publicly available TUD-Crossing dataset <ref type="bibr" target="#b0">[1]</ref>. We perform experiments on both datasets using the same network architecture and the same values of hyperparameters. For the Brainwash dataset we collect 11917 images with 91146 labeled people. We extract images from video footage at a fixed interval of 100 seconds to ensure a large variation in images. We allocate 1000 images for testing and validation, and leave the remaining images for training. No temporal overlaps exist between training and test splits. The resulting training set contains 82906 instances. Test and validation sets contain 4922 and 3318 people instances respectively. Images were labeled using Amazon Mechanical Turk by a handful of workers pre-selected through their performance on an example task. We label each person's head to avoid ambiguity in bounding box locations. The annotator labels any person she is able to recognize, even if a substantial part of the person is not visible. Examples of collected images are shown in <ref type="figure" target="#fig_8">Fig. 8</ref>. Images in the Brainwash dataset include challenges such as people at small scales, strong partial occlusions, and a large variability in clothing and appearance <ref type="bibr" target="#b2">3</ref> . We conduct evaluation using the standard protocol defined in <ref type="bibr" target="#b3">[4]</ref>. A hypothesis is considered correct if its intersection-over-union score with a ground-truth bounding box is larger than 0.5. We plot recall-precision curves and summarize results in each experiment with average precision (AP) and equal error rate (EER) in <ref type="figure" target="#fig_7">Fig. 7</ref> and <ref type="figure">Fig. 6</ref>.</p><p>For the Brainwash we also analyze how well each model predicts the total count of people in an image. As in <ref type="bibr" target="#b13">[14]</ref>, we measure count error by computing the average absolute difference between the number of predicted and ground-truth detections in test set images. For each model, an optimal detection threshold is selected on the validation set, and we report the results as COUNT in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p><p>Baseline methods: We compare our approach with Faster-RCNN <ref type="bibr" target="#b15">[16]</ref> and OverFeat <ref type="bibr" target="#b16">[17]</ref> models. The original version of OverFeat provided by <ref type="bibr" target="#b8">[9]</ref> relied on an image representation trained with AlexNet <ref type="bibr" target="#b11">[12]</ref>. We hence refer to the original version as OverFeat-AlexNet. Since both OverFeat and our model are implemented in Caffe, we were able to directly substitute the GoogLeNet architecture into the OverFeat model. We denote the new model as OverFeatGoogLeNet. The comparison of the two OverFeat variants on the Brainwash dataset is shown in <ref type="figure" target="#fig_7">Fig. 7</ref>. We observe that Overfeat-GoogLeNet performs significantly better than OverFeat-AlexNet.</p><p>Note that the image representations used in our model <ref type="bibr" target="#b2">3</ref> The dataset is available at d2.mpi-inf.mpg.de/datasets  and in OverFeat are exactly the same. Both are implemented using the same code, parameters, filter dimensions, and number of filters. This gives us the interesting possibility of directly comparing the models' distinct hypothesis generating components. In the case of OverFeat <ref type="bibr" target="#b16">[17]</ref>, this component corresponds to a bounding box regression from each cell followed by a round of non-maximum suppression. In our model this component corresponds to decoding with an LSTM layer that produces a variable length output. The performance of our best model is shown <ref type="figure" target="#fig_7">Fig. 7</ref> and compared to both versions of OverFeat.</p><p>Performance evaluation: We first compare our approach to OverFeat baseline on the Brainwash dataset. Our approach delivers a substantial improvement over OverFeat, improving recall from 71% to 81%. We also achieve considerable improvement in AP (0.78 for our model vs. 0.67 for OverFeat-GoogLeNet), and people counting error (0.76 vs. 1.05). <ref type="figure" target="#fig_8">Fig. 8</ref> shows several examples of detections obtained by our model and OverFeat-GoogLeNet. The arrows highlight cases where our model can detect people even in the presence of strong occlusions. Examples of a failure cases are indicated by red arrows in <ref type="figure" target="#fig_9">Fig. 9</ref>.</p><p>We compare to prior work in the literature on the TUDCrossing dataset. This dataset includes images from a crowded street scene and has been used for evaluation of an occlusion specific detector in Tang et al. <ref type="bibr" target="#b21">[22]</ref>. We train on the TUD-Brussels dataset <ref type="bibr" target="#b25">[26]</ref> as the TUD-Crossing dataset does not provide a corresponding training set <ref type="bibr" target="#b3">4</ref> . The original ground-truth for the TUD-Crossing does not include labels for strongly occluded people. To get further insights into performance of different methods for the cases of strong occlusions we extend the ground-truth to include all people in the dataset. This increases the number of labeled people from 1008 in the original version to 1530 in the full version. We compare our detector with results reported in Tang et al. <ref type="bibr" target="#b21">[22]</ref>, and with results provided by the authors of Zhang et al. <ref type="bibr" target="#b26">[27]</ref>, whose method represents the current the state of the art for pedestrian detection.</p><p>The results using the original ground-truth are shown in <ref type="figure">Fig. 6 (a)</ref>. At 95% precision, our approach achieves recall of 86% compared with 79% reported in Tang et al. <ref type="bibr" target="#b21">[22]</ref> (equal error rate is 90% for our approach vs. 85% for <ref type="bibr" target="#b21">[22]</ref>). Note that <ref type="bibr" target="#b21">[22]</ref> and similar approaches have been explicitly engineered to address detection of multiple people and employ hand-designed clustering of detection components, whereas our method can be directly trained on the input data. Our approach improves over our OverFeat-GoogLeNet baseline as well as over the recent approach of Zhang et al. <ref type="bibr" target="#b26">[27]</ref>.</p><p>The results for the full ground-truth are shown in <ref type="figure">Fig. 6  (b)</ref>. Note the substantial drop in overall performance, which is due to a larger proportion of strongly occluded people in the full ground-truth. The differences between our approach and the approach of <ref type="bibr" target="#b26">[27]</ref> are even more pronounced in this setting. Our approach achieves EER of 80% compared to 70% for <ref type="bibr" target="#b26">[27]</ref>.</p><p>Comparison to Faster R-CNN: We trained and evaluated Faster R-CNN detector <ref type="bibr" target="#b15">[16]</ref> on the Brainwash and TUD-Crossing using the implementation provided by the authors <ref type="bibr" target="#b4">5</ref> . The results are shown in <ref type="figure">Fig. 6</ref> and <ref type="figure" target="#fig_7">Fig. 7</ref>. We observe that for Faster R-CNN, the optimal level of nonmaximum suppression (NMS) is crucial for obtaining good performance. We compare three levels of NMS controlled by parameter τ ∈ [0, 1]. On TUD-Crossing our approach improves over Faster-RCNN across all NMS levels. On Brainwash it performs comparably to the best setting of Faster-RCNN. Note that Brainwash is less crowded compared to TUD-Crossing and contains lower ratio of overlapping bounding boxes. Faster R-CNN with τ = 0.75 consistently generates multiple predictions on the same person, resulting in poor precision. Stricter NMS with τ = 0.25 mitigates this issue. On the TUD-Crossing dataset τ = 0.25 removes too many predicted boxes which results in poor recall, setting τ = 0.75 preserves detections on people in close proximity but introduces false positives on single people. We show the qualitative comparison between our ap-proach and Faster R-CNN in <ref type="figure">Fig. 5</ref>. Both approaches perform equally well in the case of fully visible people, but our approach is able to better detect partially occluded people.</p><p>In <ref type="figure" target="#fig_7">Fig. 7</ref> we also include a result obtained with our model extended with an additional re-zooming layer that transforms features into a scale-invariant represent prior to classification and leads to further improvement in performance. We refer to <ref type="bibr" target="#b17">[18]</ref> for the details on this extension.</p><p>Comparison of loss functions. We now evaluate the loss functions introduced in Sec. 2.2. The model trained with L fix achieves only 0.60 AP. This suggests that allowing the LSTM to output detections from easy to hard during training, rather than in some fixed spatial ordering, was essential for performance. To explore the importance of overlap terms in our loss function, we evaluate the L firstk loss, which matches the k ground-truth instances in each region to the first k output predictions. We observe that L firstk outperforms L fix at test time by allowing permutations of LSTM outputs during training. However, we found that L firstk struggled to attach confidences to specific box locations. With L firstk , early confidence predictions are often too high, and late predictions too low. It appears that instead of learning the probability that the corresponding box is correct, the model learns on the i th recurrent step to predict the confidence that there are at least i people in a region. These confidences are inappropriate for detection thresholding, and underscore the importance of including the overlap terms, o ij , in our matching function. Precision recall curves for each loss function are shown in <ref type="figure" target="#fig_7">Fig. 7</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we introduced a new method for object detection and demonstrated its performance on the TUDCrossing and Brainwash datasets. Our system addresses the challenge of detecting multiple partially occluded instances by decoding a variable number of outputs from rich intermediate representations of an image. To teach our model to produce coherent sets of predictions, we defined a loss function suitable for training our system end to end. Our approach runs at 15 frames per second on a modern GPU. We envision that this approach may also prove effective in other prediction tasks with structured outputs, such as people tracking and articulated pose estimation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Initial over-complete set of detections of OverFeat (a) and output of post-processing (b). Note the failure to detect the third person in the center. Detection results obtained with our method (c).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Our system first encodes an image into a block of high level features. An LSTM then acts as a controller, decoding this information into a set of detections.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Illustration of the matching of ground-truth instances (black) to accepted (green) and rejected (red) candidates. Matching should respect both precedence (1 vs 2) and localization (4 vs 3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Example of stitching in a new region's predictions (red) with accepted predictions (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Example detection results on the TUD-Crossing dataset. Middle and bottom rows visualize output of Faster R-CNN and our detectors at the operating point with 90% precision. Top row shows output of Faster R-CNN before application of non-maximum suppression.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance evaluation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Example detection results obtained with OverFeat-GoogLeNet (top row) and our approach (bottom row). We show each model's output at 90% precision.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Example failure cases of our method.</figDesc></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The implementation is publicly available at https://github. com/Russell91/ReInspect.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">All hyperparameter AP analysis is performed on the validation set of the Brainwash scene described in Section 4.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">TUD-Brussels contains several images from TUD-Crossing which we exclude from our training set. 5 https://github.com/rbgirshick/py-faster-rcnn</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. This work has been supported by the Max Planck Center for Visual Computing and Communication. The authors would like to thank NVIDIA Corporation for providing a K40 GPU. The authors would also like to thank Will Song and Brody Huval for helpful discussions.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">People-tracking-bydetection and people-detection-by-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">On detection of multiple object instances using Hough transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barinova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2010</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
		<idno>CVPR 2009. 5</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015-01" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Recognition using visual phrases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2011</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jitendra</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;14</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Connectionist temporal classification: Labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernandez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">An empirical evaluation of deep learning on highway driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Brody</forename><surname>Huval</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sameep</forename><surname>Tandon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Kiske</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Will</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Pazhayampallil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pranav</forename><surname>Rajpurkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Toki</forename><surname>Migimatsu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Royce</forename><surname>Cheng-Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fernando</forename><surname>Mujica</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adam</forename><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
		<idno>abs/1504.01716</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergio</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><forename type="middle">Darrell</forename><surname>Caffe</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1408.5093</idno>
		<title level="m">Convolutional architecture for fast feature embedding</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep visual-semantic alignments for generating image descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrej</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;12</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Pedestrian detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Leibe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Seemann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR 2005</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Learning to count objects in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Victor</forename><surname>Lempitsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS&apos;10</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Single-pedestrian detection aided by multi-pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;13</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kaiming</forename><surname>Shaoqing Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Overfeat: Integrated recognition, localization and detection using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR&apos;14. 1</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">End-to-end people detection in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Russell</forename><surname>Stewart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.04878</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Sequence to sequence learning with neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS*2014</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yangqing</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pierre</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno>abs/1409.4842</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Scalable, high-quality object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dumitru</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dragomir</forename><surname>Anguelov</surname></persName>
		</author>
		<idno>abs/1412.1441</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning people detectors for tracking in crowded scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Milan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV&apos;13</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Detection and tracking of occluded people</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Siyu</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mykhaylo</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC 2012</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">End-to-end integration of a convolutional network, deformable parts model and non-maximum suppression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR&apos;15</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Multi-cue onboard pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Wojek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Walk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernt</forename><surname>Schiele</surname></persName>
		</author>
		<idno>CVPR 2009. 8</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Filtered channel features for pedestrian detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
