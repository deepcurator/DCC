<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dilated Recurrent Neural Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyu</forename><surname>Chang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yang</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Han</surname></persName>
							<email>weihan3@illinois.edu</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mo</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaoxiao</forename><surname>Guo</surname></persName>
							<email>xiaoxiao.guo@ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Tan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodong</forename><surname>Cui</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Witbrock</surname></persName>
							<email>witbrock@us.ibm.com</email>
							<affiliation key="aff0">
								<orgName type="institution">IBM Thomas J. Watson Research Center</orgName>
								<address>
									<postCode>10598</postCode>
									<settlement>Yorktown</settlement>
									<region>NY</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Hasegawa-Johnson</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
							<email>t-huang1@illinois.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">University of Illinois at Urbana-Champaign</orgName>
								<address>
									<postCode>61801</postCode>
									<settlement>Urbana</settlement>
									<region>IL</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Dilated Recurrent Neural Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning with recurrent neural networks (RNNs) on long sequences is a notoriously difficult task. There are three major challenges: 1) complex dependencies, 2) vanishing and exploding gradients, and 3) efficient parallelization. In this paper, we introduce a simple yet effective RNN connection structure, the DILATEDRNN, which simultaneously tackles all of these challenges. The proposed architecture is characterized by multi-resolution dilated recurrent skip connections, and can be combined flexibly with diverse RNN cells. Moreover, the DILATEDRNN reduces the number of parameters needed and enhances training efficiency significantly, while matching state-of-the-art performance (even with standard RNN cells) in tasks involving very long-term dependencies. To provide a theory-based quantification of the architecture's advantages, we introduce a memory capacity measure, the mean recurrent length, which is more suitable for RNNs with long skip connections than existing measures. We rigorously prove the advantages of the DILATEDRNN over other recurrent neural architectures. The code for our method is publicly available 1 .</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Recurrent neural networks (RNNs) have been shown to have remarkable performance on many sequential learning problems. However, long sequence learning with RNNs remains a challenging problem for the following reasons: first, memorizing extremely long-term dependencies while maintaining mid-and short-term memory is difficult; second, training RNNs using back-propagationthrough-time is impeded by vanishing and exploding gradients; And lastly, both forward-and back-propagation are performed in a sequential manner, which makes the training time-consuming.</p><p>Many attempts have been made to overcome these difficulties using specialized neural structures, cells, and optimization techniques. Long short-term memory (LSTM) <ref type="bibr" target="#b9">[10]</ref> and gated recurrent units (GRU) <ref type="bibr" target="#b5">[6]</ref> powerfully model complex data dependencies. Recent attempts have focused on multi-timescale designs, including clockwork RNNs <ref type="bibr" target="#b11">[12]</ref>, phased LSTM <ref type="bibr" target="#b16">[17]</ref>, hierarchical multi-scale RNNs <ref type="bibr" target="#b4">[5]</ref>, etc. The problem of vanishing and exploding gradients is mitigated by LSTM and GRU memory gates; other partial solutions include gradient clipping <ref type="bibr" target="#b17">[18]</ref>, orthogonal and unitary weight optimization <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24]</ref>, and skip connections across multiple timestamps <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>. For efficient sequential training, WaveNet <ref type="bibr" target="#b21">[22]</ref> abandoned RNN structures, proposing instead the dilated causal convolutional neural network (CNN) architecture, which provides significant advantages in working directly with raw audio waveforms. However, the length of dependencies captured by a dilated CNN is limited by its kernel size, whereas an RNN's autoregressive modeling can, in theory, capture potentially infinitely  long dependencies with a small number of parameters. Recently, Yu et al. <ref type="bibr" target="#b26">[27]</ref> proposed learningbased RNNs with the ability to jump (skim input text) after seeing a few timestamps worth of data; although the authors showed that the modified LSTM with jumping provides up to a six-fold speed increase, the efficiency gain is mainly in the testing phase.</p><p>In this paper, we introduce the DILATEDRNN, a neural connection architecture analogous to the dilated CNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>, but under a recurrent setting. Our approach provides a simple yet useful solution that tries to alleviate all challenges simultaneously. The DILATEDRNN is a multi-layer, and cell-independent architecture characterized by multi-resolution dilated recurrent skip connections.</p><p>The main contributions of this work are as follows. 1) We introduce a new dilated recurrent skip connection as the key building block of the proposed architecture. These alleviate gradient problems and extend the range of temporal dependencies like conventional recurrent skip connections, but in the dilated version require fewer parameters and significantly enhance computational efficiency. 2) We stack multiple dilated recurrent layers with hierarchical dilations to construct a DILATEDRNN, which learns temporal dependencies of different scales at different layers. 3) We present the mean recurrent length as a new neural memory capacity measure that reveals the performance difference between the previously developed recurrent skip-connections and the dilated version. We also verify the optimality of the exponentially increasing dilation distribution used in the proposed architecture. It is worth mentioning that, the recent proposed Dilated LSTM <ref type="bibr" target="#b22">[23]</ref> can be viewed as a special case of our model, which contains only one dilated recurrent layer with fixed dilation. The main purpose of their model is to reduce the temporal resolution on time-sensitive tasks. Thus, the Dilated LSTM is not a general solution for modeling at multiple temporal resolutions.</p><p>We empirically validate the DILATEDRNN in multiple RNN settings on a variety of sequential learning tasks, including long-term memorization, pixel-by-pixel classification of handwritten digits (with permutation and noise), character-level language modeling, and speaker identification with raw audio waveforms. The DILATEDRNN improves significantly on the performance of a regular RNN, LSTM, or GRU with far fewer parameters. Many studies <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b13">14]</ref> have shown that vanilla RNN cells perform poorly in these learning tasks. However, within the proposed structure, even vanilla RNN cells outperform more sophisticated designs, and match the state-of-the-art. We believe that the DILATEDRNN provides a simple and generic approach to learning on very long sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Dilated Recurrent Neural Networks</head><p>The main ingredients of the DILATEDRNN are its dilated recurrent skip connection and its use of exponentially increasing dilation; these will be discussed in the following two subsections respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Dilated Recurrent Skip Connection</head><p>Denote c (l) t as the cell in layer l at time t. The dilated skip connection can be represented as</p><formula xml:id="formula_0">c (l) t = f ⇣ x (l) t , c (l) t s (l) ⌘ .<label>(1)</label></formula><p>This is similar to the regular skip connection <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b29">30]</ref>, which can be represented as More importantly, computational efficiency of a parallel implementation (e.g., using GPUs) can be greatly improved by parallelizing operations that, in a regular RNN, would be impossible. The middle and right graphs in <ref type="figure" target="#fig_0">figure 1</ref> illustrate the idea with s </p><formula xml:id="formula_1">c (l) t = f ⇣ x (l) t , c (l) t 1 , c (l) t s (l) ⌘ .<label>(2)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Exponentially Increasing Dilation</head><p>To extract complex data dependencies, we stack dilated recurrent layers to construct DILATEDRNN. Similar to settings that were introduced in WaveNet <ref type="bibr" target="#b21">[22]</ref>, the dilation increases exponentially across layers. Denote s (l) as the dilation of the l-th layer. Then,</p><formula xml:id="formula_2">s (l) = M l 1 , l = 1, · · · , L.<label>(3)</label></formula><p>The left side of figure 2 depicts an example of DILATEDRNN with L = 3 and M = 2. On one hand, stacking multiple dilated recurrent layers increases the model capacity. On the other hand, exponentially increasing dilation brings two benefits. First, it makes different layers focus on different temporal resolutions. Second, it reduces the average length of paths between nodes at different timestamps, which improves the ability of RNNs to extract long-term dependencies and prevents vanishing and exploding gradients. A formal proof of this statement will be given in section 3.</p><p>To improve overall computational efficiency, a generalization of our standard DILATEDRNN is also proposed. The dilation in the generalized DILATEDRNN does not start at one, but M l 0 . Formally,</p><formula xml:id="formula_3">s (l) = M (l 1+l 0 ) , l = 1, · · · , L and l0 0,<label>(4)</label></formula><p>where M l 0 is called the starting dilation. To compensate for the missing dependencies shorter than</p><formula xml:id="formula_4">M l 0 , a 1-by-M (l 0 )</formula><p>convolutional layer is appended as the final layer. The right side of figure 2 illustrates an example of l 0 = 1, i.e. dilations start at two. Without the red edges, there would be no edges connecting nodes at odd and even time stamps. As discussed in section 2.1, the computational efficiency can be increased by M l 0 by breaking the input sequence into M l 0 downsampled subsequences, and feeding each into a L l 0 -layer regular DILATEDRNN with shared weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">The Memory Capacity of DILATEDRNN</head><p>In this section, we extend the analysis framework in <ref type="bibr" target="#b29">[30]</ref> to establish better measures of memory capacity and parameter efficiency, which will be discussed in the following two sections respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Memory Capacity</head><p>To facilitate theoretical analysis, we apply the cyclic graph G c notation introduced in <ref type="bibr" target="#b29">[30]</ref>. Definition 3.1 (Cyclic Graph). The cyclic graph representation of an RNN structure is a directed multi-graph, G C = (V C , E C ). Each edge is labeled as e = (u, v, ) 2 E C , where u is the origin node, v is the destination node, and is the number of time steps the edge travels. Each node is labeled as v = (i, p) 2 V C , where i is the time index of the node modulo m, m is the period of the graph, and p is the node index. G C must contain at least one directed cycle. Along the edges of any directed cycle, the summation of must not be zero.</p><p>Define d i (n) as the length of the shortest path from any input node at time i to any output node at time i + n. In <ref type="bibr" target="#b29">[30]</ref>, a measure of the memory capacity is proposed that essentially only looks at d i (m), where m is the period of the graph. This is reasonable when the period is small. However, when the period is large, the entire distribution of d i (n), 8n  m makes a difference, not just the one at span m. As a concrete example, suppose there is an RNN architecture of period m = 10, 000, implemented using equation <ref type="formula" target="#formula_1">(2)</ref>  = m, so that d i (n) = n for n = 1, · · · , 9, 999 and d i (m) = 1. This network rapidly memorizes the dependence on inputs at time i of the outputs at time i + m = i + 10, 000, but shorter dependencies 2  n  9, 999 are much harder to learn. Motivated by this, we proposed the following additional measure of memory capacity. Definition 3.2 (Mean Recurrent Length). For an RNN with cycle m, the mean recurrent length is</p><formula xml:id="formula_5">d = 1 m m X n=1 max i2V di(n).<label>(5)</label></formula><p>Mean recurrent length studies the average dilation across different time spans within a cycle. An architecture with good memory capacity should generally have a small recurrent length for all time spans. Otherwise the network can only selectively memorize information at a few time spans. Also, we take the maximum over i, so as to punish networks that have good length only for a few starting times, which can only well memorize information originating from those specific times. The proposed mean recurrent length has an interesting reciprocal relation with the short-term memory (STM) measure proposed in <ref type="bibr" target="#b10">[11]</ref>, but mean recurrent length emphasizes more on long-term memory capacity, which is more suitable for our intended applications.</p><p>With this, we are ready to illustrate the memory advantage of DILATEDRNN . Consider two RNN architectures. One is the proposed DILATEDRNN structure with d layers and M = 2 (equation <ref type="formula" target="#formula_0">(1)</ref>). The other is a regular d-layer RNN with skip connections (equation <ref type="formula" target="#formula_1">(2)</ref>). If the skip connections are of skip s </p><p>which grows linearly with m. On the other hand, for the proposed DILATEDRNN,</p><formula xml:id="formula_7">d = (3m 1)/2m log 2 m + 1/m + 1,<label>(7)</label></formula><p>whered only grows logarithmically with m, which is much smaller than that of regular skip RNN. It implies that the information in the past on average travels along much fewer edges, and thus undergoes far less attenuation. The derivation is given in appendix A in the supplementary materials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Parameter Efficiency</head><p>The advantage of DILATEDRNN lies not only in the memory capacity but also the number of parameters that achieves such memory capacity. To quantify the analysis, the following measure is introduced. Definition 3.3 (Number of Recurrent Edges per Node). Denote Card{·} as the set cardinality. For an RNN represented as G C = (V C , E C ), the number of recurrent edges per node, N r , is defined as</p><formula xml:id="formula_8">Nr = Card {e = (u, v, ) 2 EC : 6 = 0} / Card{VC }.<label>(8)</label></formula><p>Ideally, we would want a network that has large recurrent skips while maintaining a small number of recurrent weights. It is easy to show that N r for DILATEDRNN is 1 and that for RNNs with regular skip connections is 2. The DILATEDRNN has half the recurrent complexity as the RNN with regular skip RNN because of the removal of the direct recurrent edge. The following theorem states that the DILATEDRNN is able to achieve the best memory capacity among a class of connection structures with N r = 1, and thus is among the most parameter efficient RNN architectures. </p><p>where n i is any arbitrary positive integer. Among this subset, the d-layer DILATEDRNN with dilation</p><formula xml:id="formula_10">rate {M 0 , · · · , M d 1 } achieves the smallestd.</formula><p>The proof is motivated by <ref type="bibr" target="#b3">[4]</ref>, and is given in appendix B.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Comparing with Dilated CNN</head><p>Since DILATEDRNN is motivated by dilated CNN <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b27">28]</ref>, it is useful to compare their memory capacities. Although cyclic graph, mean recurrent length and number of recurrent edges per node are designed for recurrent structures, they happen to be applicable to dilated CNN as well. What's more, it can be easily shown that, compared to a DILATEDRNN with the same number of layers and dilation rate of each layer, a dilated CNN has exactly the same number of recurrent edges per node, and a slightly smaller (by log 2 m) mean recurrent length. Hence both architectures have the same model complexity, and it looks like a dilated CNN has a slightly better memory capacity.</p><p>However, mean recurrent length only measures the memory capacity within a cycle. When going beyond a cycle, it is already shown that the recurrent length grows linearly with the number of cycles <ref type="bibr" target="#b29">[30]</ref> for RNN structures, including DILATEDRNN, whereas for a dilated CNN, the receptive field size is always finite (thus mean recurrent length goes to infinity beyond the receptive field size). For example, with dilation rate M = 2 l 1 and d layers l = 1, · · · , d, a dilated CNN has a receptive field size of 2 d , which is two cycles. On the other hand, the memory of a DILATEDRNN can go far beyond two cycles, particularly with the sophisticated units like GRU and LSTM. Hence the memory capacity advantage of DILATEDRNN over a dilated CNN is obvious.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>In this section, we evaluate the performance of DILATEDRNN on four different tasks, which include long-term memorization, pixel-by-pixel MNIST classification <ref type="bibr" target="#b14">[15]</ref>, character-level language modeling on the Penn Treebank <ref type="bibr" target="#b15">[16]</ref>, and speaker identification with raw waveforms on VCTK <ref type="bibr" target="#b25">[26]</ref>. We also investigate the effect of dilation on performance and computational efficiency.</p><p>Unless specified otherwise, all the models are implemented with Tensorflow <ref type="bibr" target="#b0">[1]</ref>. We use the default nonlinearities and RMSProp optimizer <ref type="bibr" target="#b20">[21]</ref> with learning rate 0.001 and decay rate of 0.9. All weight matrices are initialized by the standard normal distribution. The batch size is set to 128. Furthermore, in all the experiments, we apply the sequence classification setting <ref type="bibr" target="#b24">[25]</ref>, where the output layer only adds at the end of the sequence. Results are reported for trained models that achieve the best validation loss. Unless stated otherwise, no tricks, such as gradient clipping <ref type="bibr" target="#b17">[18]</ref>, learning rate annealing, recurrent weight dropout <ref type="bibr" target="#b19">[20]</ref>, recurrent batch norm <ref type="bibr" target="#b19">[20]</ref>, layer norm <ref type="bibr" target="#b2">[3]</ref>, etc., are applied. All the tasks are sequence level classification tasks, and therefore the "gridding" problem <ref type="bibr" target="#b28">[29]</ref> is irrelevant. No "degridded" module is needed.</p><p>Three RNN cells, Vanilla, LSTM and GRU cells, are combined with the DILATEDRNN , which we refer to as dilated Vanilla, dilated LSTM and dilated GRU, respectively. The common baselines include single-layer RNNs (denoted as Vanilla RNN, LSTM, and GRU), multi-layer RNNs (denoted as stack Vanilla, stack LSTM, and stack GRU), and Vanilla RNN with regular skip connections (denoted as Skip Vanilla). Additional baselines will be specified in the corresponding subsections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Copy memory problem</head><p>This task tests the ability of recurrent models in memorizing long-term information. We follow a similar setup in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b9">10]</ref>. Each input sequence is of length T + 20. The first ten values are randomly generated from integers 0 to 7; the next T 1 values are all 8; the last 11 values are all 9, where the first 9 signals that the model needs to start to output the first 10 values of the inputs. Different from the settings in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b23">24]</ref>, the average cross-entropy loss is only measured at the last 10 timestamps. Therefore, the random guess yields an expected average cross entropy of ln <ref type="formula" target="#formula_8">(8)</ref>   The convergence curves in two settings, T = 500 and 1, 000, are shown in <ref type="figure" target="#fig_7">figure 3</ref>. In both settings, the DILATEDRNN with vanilla cells converges to a good optimum after about 1,000 training iterations, whereas dilated LSTM and GRU converge slower. It might be because the LSTM and GRU cells are much more complex than the vanilla unit. Except for the proposed models, all the other models are unable to do better than the random guess, including the skip Vanilla. These results suggest that the proposed structure as a simple renovation is very useful for problems requiring very long memory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Pixel-by-pixel MNIST</head><p>Sequential classification on the MNIST digits <ref type="bibr" target="#b14">[15]</ref> is commonly used to test the performance of RNNs. We first implement two settings. In the first setting, called the unpermuted setting, we follow the same setups in <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b29">30]</ref> by serializing each image into a 784 x 1 sequence. The second setting, called permuted setting, rearranges the input sequence with a fixed permutations. Training, validation and testing sets are the default ones in Tensorflow. Hyperparameters and results are reported in table 1. In addition to the baselines already described, we also implement the dilated CNN. However, the receptive fields size of a nine-layer dilated CNN is 512, and is insufficient to cover the sequence length of 784. Therefore, we added one more layer to the dilated CNN, which enlarges its receptive field size to 1,024. It also forms a slight advantage of dilated CNN over the DILATEDRNN structures.</p><p>In the unpermuted setting, the dilated GRU achieves the best evaluation accuracy of 99.2. However, the performance improvements of dilated GRU and LSTM over both the single-and multi-layer ones are marginal, which might be because the task is too simple. Further, we observe significant performance differences between stack Vanilla and skip vanilla, which is consistent with the findings in <ref type="bibr" target="#b29">[30]</ref> that RNNs can better model long-term dependencies and achieves good results when recurrent skip connections added. Nevertheless, the dilated vanilla has yet another significant performance gain over the skip Vanilla, which is consistent with our argument in section 3, that the DILATEDRNN has a much more balanced memory over a wide range of time periods than RNNs with the regular skips. The performance of the dilated CNN is dominated by dilated LSTM and GRU, even when the latter have fewer parameters (in the 20 hidden units case) than the former (in the 50 hidden units case).</p><p>In the permuted setting, almost all performances are lower. However, the DILATEDRNN models maintain very high evaluation accuracies. In particular, dilated Vanilla outperforms the previous RNN-based state-of-the-art Zoneout <ref type="bibr" target="#b12">[13]</ref> with a comparable number of parameters. It achieves test accuracy of 96.1 with only 44k parameters. Note that the previous state-of-the-art utilizes the recurrent batch normalization. The version without it has a much lower performance compared to all the dilated models. We believe the consistently high performance of the DILATEDRNN across different permutations is due to its hierarchical multi-resolution dilations. In addition, the dilated CNN is able the achieve the best performance, which is in accordance with our claim in section 3.3 that dilated CNN has a slightly shorter mean recurrent length than DILATEDRNN architectures, when sequence length fall within its receptive field size. However, note that this is achieved by adding one additional layer to expand its receptive field size compared to the RNN counterparts. When the useful information lies outside its receptive field, the dilated CNN might fail completely.  In addition to these two settings, we propose a more challenging task called the noisy MNIST, where we pad the unpermuted pixel sequences with [0, 1] uniform random noise to the length of T . The results with two setups T = 1, 000 and T = 2, 000 are shown in <ref type="figure" target="#fig_8">figure 4</ref>. The dilated recurrent models and skip RNN have 9 layers and 20 hidden units per layer. The number of skips at each layer of skip RNN is 256. The dilated CNN has 10 layers and 11 layers for T = 1, 000 and T = 2, 000, respectively. This expands the receptive field size of the dilated CNN to the entire input sequence. The number of filters per layer is 20. It is worth mentioning that, in the case of T = 2, 000, if we use a 10-layer dilated CNN instead, it will only produce random guesses. This is because the output node only sees the last 1, 024 input samples which do not contain any informative data. All the other reported models have the same hyperparameters as shown in the first three row of table 1. We found that none of the models without skip connections is able to learn. Although skip Vanilla remains learning, its performance drops compared to the first unpermuted setup. On the contrary, the DILATEDRNN and dilated CNN models obtain almost the same performances as before. It is also worth mentioning that in all three experiments, the DILATEDRNN models are able to achieve comparable results with an extremely small number of parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Language modeling</head><p>We further investigate the task of predicting the next character on the Penn Treebank dataset <ref type="bibr" target="#b15">[16]</ref>. We follow the data splitting rule with the sequence length of 100 that are commonly used in previous studies. This corpus contains 1 million words, which is small and prone to over-fitting. Therefore model regularization methods have been shown effective on the validation and test set performances. Unlike many existing approaches, we apply no regularization other than a dropout on the input layer. Instead, we focus on investigating the regularization effect of the dilated structure itself. Results are shown in table 2. Although Zoneout, LayerNorm HM-LSTM and HyperNetowrks outperform the DILATEDRNN models, they apply batch or layer normalizations as regularization. To the best of our knowledge, the dilated GRU with 1.27 BPC achieves the best result among models of similar sizes  without layer normalizations. Also, the dilated models outperform their regular counterparts, Vanilla (didn't converge, omitted), LSTM and GRU, without increasing the model complexity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Speaker identification from raw waveform</head><p>We also perform the speaker identification task using the corpus from VCTK <ref type="bibr" target="#b25">[26]</ref>. Learning audio models directly from the raw waveform poses a difficult challenge for recurrent models because of the vastly long-term dependency. Recently the CLDNN family of models <ref type="bibr" target="#b18">[19]</ref> managed to match or surpass the log mel-frequency features in several speech problems using waveform. However, CLDNNs coarsen the temporal granularity by pooling the first-layer CNN output before feeding it into the subsequent RNN layers, so as to solve the memory challenge. Instead, the DILATEDRNN directly works on the raw waveform without pooling, which is considered more difficult.</p><p>To achieve a feasible training time, we adopt the efficient generalization of the DILATEDRNN as proposed in equation (4) with l 0 = 3 and l 0 = 5 . As mentioned before, if the dilations do not start at one, the model is equivalent to multiple shared-weight networks, each working on partial inputs, and the predictions are made by fusing the information using a 1-by-M l0 convolutional layer. Our baseline GRU model follows the same setting with various resolutions (referred to as fused-GRU), with dilation starting at 8. This baseline has 8 share-weight GRU networks, and each subnetwork works on 1/8 of the subsampled sequences. The same fusion layer is used to obtain the final prediction. Since most other regular baselines failed to converge, we also implemented the MFCC-based models on the same task setting for reference. The 13-dimensional log-mel frequency features are computed with 25ms window and 5ms shift. The inputs of MFCC models are of length 100 to match the input duration in the waveform-based models. The MFCC feature has two natural advantages: 1) no information loss from operating on subsequences; 2) shorter sequence length. Nevertheless, our dilated models operating directly on the waveform still offer a competitive performance <ref type="table" target="#tab_3">(Table 3</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Discussion</head><p>In this subsection, we first investigate the relationship between performance and the number of dilations. We compare the DILATEDRNN models with different numbers of layers on the noisy MNIST T = 1, 000 task. All models use vanilla RNN cells with hidden state size 20. The number of dilations starts at one. In <ref type="figure" target="#fig_9">figure 5</ref>, we observe that the classification accuracy and rate of convergence increases as the models become deeper. Recall the maximum skip is exponential in the number of layers. Thus, the deeper model has a larger maximum skip and mean recurrent length.</p><p>Second, we consider maintaining a large maximum skip with a smaller number of layers, by increasing the dilation at the bottom layer of DILATEDRNN . First, we construct a nine-layer DILATEDRNN   <ref type="figure" target="#fig_10">figure 6</ref>. Then, we remove the bottom hidden layers one-by-one to construct seven new models. The last created model has three layers, and the number of dilations starts at 64. <ref type="figure" target="#fig_10">Figure 6</ref> demonstrates both the wall time and evaluation accuracy for 50,000 training iterations of noisy MNIST dataset. The training time reduces by roughly 50% for every dropped layer (for every doubling of the minimum dilation). Although the testing performance decreases when the dilation does not start at one, the effect is marginal with s (0) = 2, and small with 4  s (0)  16. Notably, the model with dilation starting at 64 is able to train within 17 minutes by using a single Nvidia P-100 GPU while maintaining a 93.5% test accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>Our experiments with DILATEDRNN provide strong evidence that this simple multi-timescale architectural choice can reliably improve the ability of recurrent models to learn long-term dependency in problems from different domains. We found that the DILATEDRNN trains faster, requires less hyperparameter tuning, and needs fewer parameters to achieve the state-of-the-arts. In complement to the experimental results, we have provided a theoretical analysis showing the advantages of DILATEDRNN and proved its optimality under a meaningful architectural measure of RNNs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (left) A single-layer RNN with recurrent skip connections. (mid) A single-layer RNN with dilated recurrent skip connections. (right) A computation structure equivalent to the second graph, which reduces the sequence length by four times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (left) An example of a three-layer DILATEDRNN with dilation 1, 2, and 4. (right) An example of a two-layer DILATEDRNN, with dilation 2 in the first layer. In such a case, extra embedding connections are required (red arrows) to compensate missing data dependencies. regular skip connection is that the dependency on c (l) t 1 is removed in dilated skip connection. The left and middle graphs in figure 1 illustrate the differences between two architectures with dilation or skip length s (l) = 4, where W 0 r is removed in the middle graph. This reduces the number of parameters. More importantly, computational efficiency of a parallel implementation (e.g., using GPUs) can be greatly improved by parallelizing operations that, in a regular RNN, would be impossible. The middle and right graphs in figure 1 illustrate the idea with s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>are given four different colors. The four cell chains, {c, can be computed in parallel by feeding the four subsequences into a regular RNN, as shown in the right of figure 1. The output can then be obtained by interweaving among the four output chains. The degree of parallelization is increased by s (l) times.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>1 , then connections in the RNN are a strict superset of those in the DILATEDRNN , and the RNN accomplishes exactly the samed as the DILATEDRNN , but with twice the number of trainable parameters (see section 3.2). Suppose one were to give every layer in the RNN the largest possible skip for any graph with a period of m = 2 d 1 : set sin every layer, which is the regular skip RNN setting. This apparent advantage turns out to be a disadvantage, because time spans of 2  n &lt; m suffer from increased path lengths, and thereforē d = (m 1)/2 + log 2 m + 1/m + 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Theorem 3. 1 (</head><label>1</label><figDesc>Parameter Efficiency of DILATEDRNN). Consider a subset of d-layer RNNs with period m = M d 1 that consists purely of dilated skip connections (hence N r = 1). For the RNNs in this subset, there are d different dilations, 1 = s 1  s 2  · · ·  s d = m, and si = nisi 1,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>⇡ 2.079.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Results of the copy memory problem with T = 500 (left) and T = 1000 (right). The dilated-RNN converges quickly to the perfect solutions. Except for RNNs with dilated skip connections, all other methods are unable to improve over random guesses. The DILATEDRNN uses 9 layers with hidden state size of 10. The dilation starts from one to 256 at the last hidden layer. The single-layer baselines have 256 hidden units. The multi-layer baselines use the same number of layers and hidden state size as the DILATEDRNN . The skip Vanilla has 9 layers, and the skip length at each layer is 256, which matches the maximum dilation of the DILATEDRNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Results of the noisy MNIST task with T = 1000 (left) and 2000 (right). RNN models without skip connections fail. DILATEDRNN significant outperforms regular recurrent skips and on-pars with the dilated CNN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Results for dilated vanilla with different numbers of layers on the noisy MNIST dataset. The performance and convergent speed increase as the number of layers increases.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Training time (left) and evaluation performance (right) for dilated vanilla that starts at different numbers of dilations at the bottom layer. The maximum dilations for all models are 256.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Results for unpermuted and permuted pixel-by-pixel MNIST. Italic numbers indicate the results copied from the original paper. The best results are bold.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Character-level language modeling on the Penn Tree Bank dataset.</figDesc><table>Method 
# 
hidden 
# parameters 
Max 
Evaluation 
layers 
/ layer 
(⇡, M) 
dilations 
BPC 
LSTM 
1 / 5 
1k / 256 
4.25 / 1.9 
1 
1.31 / 1.33 
GRU 
1 / 5 
1k / 256 
3.19 / 1.42 
1 
1.32 / 1.33 
Recurrent BN-LSTM [7] 
1 
1k 
-
1 
1.32 
Recurrent dropout LSTM [20] 
1 
1k 
4.25 
1 
1.30 
Zoneout [13] 
1 
1k 
4.25 
1 
1.27 
LayerNorm HM-LSTM [5] 
3 
512 
-
1 
1.24 
HyperNetworks [9] 
1 / 2 
1k 
4.91 / 14.41 
1 
1.26 / 1.22 

3 

Dilated Vanilla 
5 
256 
0.6 
64 
1.37 
Dilated LSTM 
5 
256 
1.9 
64 
1.31 
Dilated GRU 
5 
256 
1.42 
64 
1.27 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Speaker identification on the VCTK dataset.</figDesc><table>Method 
# 
hidden 
# parameters 
Min 
Max 
Evaluation 
layers 
/ layer 
(⇡, k) 
dilations dilations 
accuracy 
MFCC GRU 
5 / 1 
20 / 128 
16 / 68 
1 
1 
0.66 / 0.77 

Raw 
Fused GRU 
1 
256 
225 
32 / 8 
32 /8 
0.45 / 0.65 
Dilated GRU 
6 / 8 
50 
103 / 133 
32 / 8 
1024 
0.64 / 0.74 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">with recurrent batch norm [20].</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">with layer normalization [3].</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>Authors would like to thank Tom Le Paine (paine1@illinois.edu) and Ryan Musa (ramusa@us.ibm.com) for their insightful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0" />			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martín</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ashish</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paul</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eugene</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Craig</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andy</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthieu</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Unitary evolution recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amar</forename><surname>Shah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1120" to="1128" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><forename type="middle">Lei</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><forename type="middle">Ryan</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">A systemic study of monetary systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gaetano</forename><surname>Eduardo R Caianiello</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Giovanna</forename><surname>Scarpetta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal Of General System</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="81" to="92" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sungjin</forename><surname>Ahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.01704</idno>
		<title level="m">Hierarchical multiscale recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junyoung</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Caglar</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyunghyun</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.3555</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Cooijmans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">César</forename><surname>Laurent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Çaglar</forename><surname>Gülçehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09025</idno>
		<title level="m">Recurrent batch normalization</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural networks for long-term dependencies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Salah</forename><forename type="middle">El</forename><surname>Hihi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Nips</title>
		<imprint>
			<date type="published" when="1995" />
			<biblScope unit="volume">409</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hypernetworks</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.09106</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jürgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Short term memory in echo state networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Herbert</forename><surname>Jaeger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2001" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">GMD-Forschungszentrum Informationstechnik</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Koutnik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Klaus</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Faustino</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Juergen</forename><surname>Schmidhuber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1402.3511</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">A clockwork rnn. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Krueger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tegan</forename><surname>Maharaj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">János</forename><surname>Kramár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Pezeshki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nan</forename><forename type="middle">Rosemary</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anirudh</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hugo</forename><surname>Larochelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.01305</idno>
		<title level="m">Regularizing rnns by randomly preserving hidden activations</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">A simple way to initialize recurrent networks of rectified linear units</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Navdeep</forename><surname>Quoc V Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Jaitly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1504.00941</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Gradient-based learning applied to document recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Léon</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Haffner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Building a large annotated corpus of english: The penn treebank</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mary</forename><forename type="middle">Ann</forename><surname>Marcus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Beatrice</forename><surname>Marcinkiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Santorini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational linguistics</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="313" to="330" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Phased LSTM: accelerating recurrent network training for long or event-based sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Neil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Pfeiffer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shih-Chii</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09513</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On the difficulty of training recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tomas</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICML</title>
		<imprint>
			<biblScope unit="volume">28</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="1310" to="1318" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Learning the speech frontend with raw waveform cldnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Tara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ron</forename><forename type="middle">J</forename><surname>Sainath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Weiss</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kevin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Vinyals</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Sixteenth Annual Conference of the International Speech Communication Association</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stanislau</forename><surname>Semeniuta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aliaksei</forename><surname>Severyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erhardt</forename><surname>Barth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05118</idno>
		<title level="m">Recurrent dropout without memory loss</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Lecture 6.5-rmsprop: Divide the gradient by a running average of its recent magnitude. COURSERA: Neural networks for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tijmen</forename><surname>Tieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Wavenet: A generative model for raw audio</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aäron</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sander</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heiga</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Oriol</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nal</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>CoRR abs/1609.03499</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Feudal networks for hierarchical reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><forename type="middle">Sasha</forename><surname>Vezhnevets</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Osindero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Schaul</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Max</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01161</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Full-capacity unitary recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Scott</forename><surname>Wisdom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Powers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Les</forename><surname>Atlas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4880" to="4888" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">A brief survey on sequence classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzheng</forename><surname>Xing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Pei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eamonn</forename><surname>Keogh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Sigkdd Explorations Newsletter</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="40" to="48" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">English multi-speaker corpus for cstr voice cloning toolkit</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junichi</forename><surname>Yamagishi</surname></persName>
		</author>
		<ptr target="http://homepages.inf.ed.ac.uk/jyamagis/page3/page58/page58.html" />
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongrae</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quoc V</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06877</idno>
		<title level="m">Learning to skim text</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Multi-scale context aggregation by dilated convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07122</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Dilated residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fisher</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vladlen</forename><surname>Koltun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Funkhouser</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09914</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Architectural complexity measures of recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saizheng</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuhuai</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tong</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhouhan</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Roland</forename><surname>Memisevic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ruslan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1822" to="1830" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
