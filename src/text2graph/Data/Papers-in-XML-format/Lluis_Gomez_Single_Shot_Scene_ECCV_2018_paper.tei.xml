<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Single Shot Scene Text Retrieval</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lluís</forename><surname>Gómez</surname></persName>
							<email>lgomez@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona Edifici O</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Bellaterra (Barcelona)</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrés</forename><surname>Mafla</surname></persName>
							<email>andres.mafla@cvc.uab.es</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona Edifici O</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Bellaterra (Barcelona)</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marçal</forename><surname>Rusiñol</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona Edifici O</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Bellaterra (Barcelona)</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dimosthenis</forename><surname>Karatzas</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Computer Vision Center</orgName>
								<orgName type="institution">Universitat Autònoma de Barcelona Edifici O</orgName>
								<address>
									<postCode>08193</postCode>
									<settlement>Bellaterra (Barcelona)</settlement>
									<country key="ES">Spain</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Single Shot Scene Text Retrieval</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Image retrieval · Scene text · Word spotting · Convolutional Neural Networks · Region Proposals Networks · PHOC</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. Textual information found in scene images provides high level semantic information about the image and its context and it can be leveraged for better scene understanding. In this paper we address the problem of scene text retrieval: given a text query, the system must return all images containing the queried text. The novelty of the proposed model consists in the usage of a single shot CNN architecture that predicts at the same time bounding boxes and a compact text representation of the words in them. In this way, the text based image retrieval task can be casted as a simple nearest neighbor search of the query text representation over the outputs of the CNN over the entire image database. Our experiments demonstrate that the proposed architecture outperforms previous state-of-the-art while it offers a significant increase in processing speed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>The world we have created is full of written information. A large percentage of everyday scene images contain text, especially in urban scenarios <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b1">2]</ref>. Text detection, text recognition and word spotting are important research topics which have witnessed a rapid evolution during the past few years. Despite significant advances achieved, propelled by the emergence of deep learning techniques <ref type="bibr" target="#b2">[3]</ref>, scene text understanding in unconstrained conditions remains an open problem attracting an increasing interest from the Computer Vision research community. Apart from the scientific interest, a key motivation comes by the plethora of potential applications enabled by automated scene text understanding, such as improved scene-text based image search, image geo-localization, human-computer interaction, assisted reading for the visually-impaired, robot navigation and industrial automation to mention just a few.</p><p>The textual content of scene images carries high level semantics in the form of explicit, non-trivial data, which is typically not possible to obtain from analyzing the visual information of the image alone. For example, it is very challenging, even for humans, to automatically label images such as the ones illustrated in <ref type="figure">Fig. 1</ref>. The visual appearance of different tea shops' images can be extremely variable. It seems impossible to correctly label them without reading the text within them. Our scene text retrieval method returns all the images shown here within the top-10 ranked results among more than 10, 000 distractors for the text query "tea". <ref type="figure">Figure 1</ref> as tea shops solely by their visual appearance, without actually reading the storefront signs. Recent research actually demonstrated that a shop classifier ends up automatically learning to interpret textual information, as this is the only way to distinguish between businesses <ref type="bibr" target="#b3">[4]</ref>. In recent years, several attempts to take advantage of text contained in images have been proposed not only to achieve fine-grained image classification <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b5">6]</ref> but to facilitate image retrieval.</p><p>Mishra et al. <ref type="bibr" target="#b6">[7]</ref> introduced the task of scene text retrieval, where, given a text query, the system must return all images that are likely to contain such text. Successfully tackling such a task entails fast word-spotting methods, able to generalize well to out-of-dictionary queries never seen before during training.</p><p>A possible approach to implement scene text retrieval is to use an end-toend reading system and simply look for the occurrences of the query word within its outputs. It has been shown <ref type="bibr" target="#b6">[7]</ref> that such attempts generally yield low performance for various reasons. First, it is worth noting that end-to-end reading systems are evaluated on a different task, and optimized on different metrics, opting for high precision, and more often than not making use of explicit information about each of the images (for example, short dictionaries given for each image). In contrary, in a retrieval system, a higher number of detections can be beneficial. Secondly, end-to-end systems are generally slow in processing images, which hinders their use in real-time scenarios or for indexing large-scale collections.</p><p>In this paper we propose a real-time, high-performance word spotting method that detects and recognizes text in a single shot. We demonstrate state of the art performance in most scene text retrieval benchmarks. Moreover, we show that our scene text retrieval method yields equally good results for in-dictionary and out-of-dictionary (never before seen) text queries. Finally, we show that the resulting method is significantly faster than any state of the art approach for word spotting in scene images.</p><p>The proposed architecture is based on YOLO <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref>, a well known single shot object detector which we recast as a PHOC (Pyramidal Histogram Of Characters) <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b10">11]</ref> predictor, thus being able to effectively perform word detection and recognition at the same time. The main contribution of this paper is the demonstration that using PHOC as a word representation instead of a direct word classification over a closed dictionary, provides an elegant mechanism to generalize to any text string, allowing the method to tackle efficiently out-ofdictionary queries. By learning to predict PHOC representations of words the proposed model is able to transfer the knowledge acquired from training data to represent words it has never seen before.</p><p>The remainder of this paper is organized as follows. Section 2 presents an overview of the state of the art in scene text understanding tasks, Section 3 describes the proposed architecture for single shot scene text retrieval. Section 4 reports the experiments and results obtained on different benchmarks for scene text based image retrieval. Finally, the conclusions and pointers to further research are given in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related work</head><p>The first attempts at recognizing text in scene images divided the problem in two distinguished steps, text detection and text recognition. For instance, in the work of Jaderberg et al. <ref type="bibr" target="#b11">[12]</ref> scene text segmentation was performed by a text proposals mechanism that was later refined by a CNN that regressed the correct position of bounding boxes. Afterwards, those bounding boxes were inputed to a CNN that classified them in terms of a predefined vocabulary. Gupta et al. <ref type="bibr" target="#b12">[13]</ref> followed a similar strategy by first using a Fully Convolutional Regression Network for detection and the same classification network than Jaderberg for recognition. Liao et al. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref> used a modified version of the SSD <ref type="bibr" target="#b15">[16]</ref> object detection architecture adapted to text and then a CRNN <ref type="bibr" target="#b16">[17]</ref> for text recognition. However, breaking the problem into two separate and independent steps presented an important drawback since detection errors might significantly hinder the further recognition step. Recently, end-to-end systems that approach the problem as a whole have gained the attention of the community. Since the segmentation and recognition tasks are highly correlated from an end to end perspective, in the sense that learned features can be used to solve both problems, researchers started to jointly train their models. Buvsta et al. <ref type="bibr" target="#b17">[18]</ref> proposed to use a Fully Convolutional Neural Network for text detection and another module that employed a CTC (Connectionist Temporal Classification) for text recognition. Both modules were first trained independently and further joined together in order to make an end-to-end trainable architecture. Li et al. <ref type="bibr" target="#b18">[19]</ref> proposed a pipeline that included a CNN to obtain text region proposals followed by a region feature encoding module that is the input to an LSTM to detect text. The detected regions are the input to another LSTM which outputs features to be decoded by a LSTM with attention to recognize the words. In that sense, we strongly believe that single shot object detection paradigms such as YOLO <ref type="bibr" target="#b8">[9]</ref> can bring many benefits to the field of scene text recognition by having a unique architecture that is able to locate and recognize the desired text in an unique step.</p><p>However, the scene text retrieval problem slightly differs from classical scene text recognition applications. In a retrieval scenario the user should be able to cast whatever textual query he wants to retrieve, whereas most of recognition approaches are based on using a predefined vocabulary of the words one might find within scene images. For instance, both Mishra et al. <ref type="bibr" target="#b6">[7]</ref>, who introduced the scene text retrieval task, and Jaderberg et al. <ref type="bibr" target="#b11">[12]</ref>, use a fixed vocabulary to create an inverted index which contains the presence of a word in the image. Such approach obviously limits the user that does not have the freedom to cast out of vocabulary queries. In order to tackle such problem, text string descriptors based on n-gram frequencies, like the PHOC descriptor, have been successfully used for word spotting applications <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b20">21]</ref>. By using a vectorial codification of text strings, users can cast whatever query at processing time without being restricted to specific word sets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Single shot word spotting architecture</head><p>The proposed architecture, illustrated in <ref type="figure" target="#fig_0">Figure 2</ref>, consists in a single shot CNN model that predicts at the same time bounding boxes and a compact text representation of the words within them. To accomplish this we adapt the YOLOv2 object detection model <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9]</ref> and recast it as a PHOC <ref type="bibr" target="#b9">[10]</ref> predictor. Although the proposed method can be implemented on top of other object detection frameworks we opted for YOLOv2 because it can be up to 10× faster than two-stage frameworks like Faster R-CNN <ref type="bibr" target="#b21">[22]</ref>, and processing time is critical for us since we aim at processing images at high resolution to correctly deal with small text.</p><p>The YOLOv2 architecture is composed of 21 convolutional layers with a leaky ReLU activation and batch normalization <ref type="bibr" target="#b6">[7]</ref> and 5 max pooling layers. It uses 3 × 3 filters and double the number of channels after every pooling step as in VGG models <ref type="bibr" target="#b16">[17]</ref>, but also uses 1 × 1 filters interspersed between 3 × 3 convolutions to compress the feature maps as in <ref type="bibr" target="#b8">[9]</ref>. The backbone includes a pass-through layer from the second convolution layer and is followed by a final 1 × 1 convolutional layer with a linear activation with the number of filters matching the desired output tensor size for object detection. For example, in the PASCAL VOC challenge dataset (20 object classes) it needs 125 filters to predict 5 boxes with 4 coordinates each, 1 objectness value, and 20 classes per box ((4 + 1 + 20) × 5 = 125). The resulting model achieves state of the art in object detection, has a smaller number of parameters than other single shot models, and features real time object detection.</p><p>A straightforward application of the YOLOv2 architecture to the word spotting task would be to treat each possible word as an object class. This way the one hot classification vectors in the output tensor would encode the word class probability distribution among a predefined list of possible words (the dictionary) for each bounding box prediction. The downside of such an approach is that we are limited in the number of words the model can detect. For a dictionary of 20 words the model would theoretically perform as well as for the 20 object classes of the PASCAL dataset, but training for a larger dictionary (e.g. the list of 100, 000 most frequent words from the English vocabulary <ref type="bibr" target="#b11">[12]</ref>) would require a final layer with 500, 000 filters, and a tremendous amount of training data if we want to have enough samples for each of the 100, 000 classes. Even if we could manage to train such a model, it would still be limited to the dictionary size and not able to detect any word not present on it.</p><p>Instead of the fixed vocabulary approach we would like to have a model that is able to generalize to words that were not seen at training time. This is the rationale behind casting the network as a PHOC predictor. PHOC <ref type="bibr" target="#b9">[10]</ref> is a compact representation of text strings that encodes if a specific character appears in a particular spatial region of the string (see <ref type="figure" target="#fig_2">Figure 3</ref>). Intuitively a model that effectively learns to predict PHOC representations will implicitly learn to identify the presence of a particular character in a particular region of the bounding box by learning character attributes independently. This way the knowledge acquired from training data can be transfered at test time for words never observed during training, because the presence of a character at a particular location of the word translates to the same information in the PHOC representation independently of the other characters in the word. Moreover, the PHOC representation offers unlimited expressiveness (it can represent any word) with a fixed length low dimensional binary vector (604 dimensions in the version we use).</p><p>In order to adapt the YOLOv2 network for PHOC prediction we need to address some particularities of this descriptor. First, since the PHOC representation is not a one hot vector we need to get rid of the softmax function used by YOLOv2 in the classification output. Second, since the PHOC is a binary representation it makes sense to squash the network output corresponding to the PHOC vector to the range 0...1. To accomplish this, a sigmoid activation function was used in the last layer. Third, we propose to modify the original YOLOv2 loss function in order to help the model through the learning process. The original YOLOv2 model optimizes the following multi-part loss function:  where b is a vector with coordinates' offsets to an anchor bounding box, C is the probability of that bounding box containing an object, c is the one hot classification vector, and the three terms L box , L obj , and L cls are respectively independent losses for bounding box regression, objectness estimation, and classification. All the aforementioned losses are essentially the sum-squared errors of ground truth (b, C, c) and predicted (b,Ĉ,ĉ) values. In the case of PHOC prediction, with c andĉ being binary vectors but with an unrestricted number of 1 values we opt for using a cross-entropy loss function in L cls as in a multi-label classification task:</p><formula xml:id="formula_0">L(b, C, c,b,Ĉ,ĉ) = λ box L box (b,b) + L obj (C,Ĉ, λ obj , λ noobj ) + λ cls L cls (c,ĉ) (1)</formula><formula xml:id="formula_1">[ · · · ] [ · · · ] x y z [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ] [ · · · ]</formula><formula xml:id="formula_2">L cls (c,ĉ) = −1 N N n=1 [c n log(ĉ n ) + (1 − c n ) log(1 −ĉ n )]<label>(2)</label></formula><p>where N is the dimensionality of the PHOC descriptor. Similarly as in <ref type="bibr" target="#b7">[8]</ref> the combination of the sum-squared errors L box and L obj with the cross-entropy loss L cls is controlled by the scaling parameters λ box , λ obj , λ noobj , and λ cls .</p><p>Apart of the modifications made so far on top of the original YOLOv2 architecture we also changed the number, the scales, and the aspect ratios of the pre-defined anchor boxes used by the network to predict bounding boxes. Similarly as in <ref type="bibr" target="#b7">[8]</ref> we have found the ideal set of anchor boxes B for our training dataset by requiring that for each bounding box annotation there exists at least one anchor box in B with an intersection over union of at least 0.6. <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the 13 bounding boxes found to be better suited for our training data and their difference with the ones used in object detection models.</p><p>At test time, our model provides a total of W/32 × H/32 × 13 bounding box proposals, with W and H being the image input size, each one of them with an objectness score (Ĉ) and a PHOC prediction (ĉ). The original YOLOv2 model filters the bounding box candidates with a detection threshold τ considering that a bounding box is a valid detection ifĈmax(ĉ) ≥ τ . If the threshold condition is met, a non-maximal suppression (NMS) strategy is applied in order to get rid of overlapping detections of the same object. In our case the threshold is applied only on the objectness score (Ĉ) but with a much smaller value (τ = 0.0025) than in the original model (τ ≈ 0.2), and we do not apply NMS. The reason is that any evidence of the presence of a word, even if it is small, it may be beneficial in terms of retrieval if its PHOC representation has a small distance to the PHOC of the queried word. With this threshold we generate an average of 60 descriptors for every image in the dataset and all of them conform our retrieval database. In this way, the scene text retrieval of a given query word is performed with a simple nearest neighbor search of the query PHOC representation over the outputs of the CNN in the entire image database. While the distance between PHOCs is usually computed using the cosine similarity, we did not find any noticeable downside on using an Euclidean distance for the nearest neighbor search.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Implementation details</head><p>We have trained our model in a modified version of the synthetic dataset of Gupta et al. <ref type="bibr" target="#b12">[13]</ref>. First the dataset generator has been evenly modified to use a custom dictionary with the 90K most frequent English words, as proposed by Jaderberg et al. <ref type="bibr" target="#b11">[12]</ref>, instead of the Newsgroup20 dataset <ref type="bibr" target="#b22">[23]</ref> dictionary originally used by Gupta et al. The rationale was that in the original dataset there was no control about word occurrences, and the distribution of word instances had a large bias towards stop-words found in newsgroups' emails. Moreover, the text corpus of the Newsgroup20 dataset contains words with special characters and non ASCII strings that we do not contemplate in our PHOC representations. Finally, since the PHOC representation of a word with a strong rotation does not make sense under the pyramidal scheme employed, the dataset generator was modified to allow rotated text up to 15 degrees. This way we generated a dataset of 1 million images for training purposes. <ref type="figure">Figure 5</ref> shows a set of samples of our training data. <ref type="figure">Fig. 5</ref>. Synthetic training data generated with a modified version of the method of Gupta et al. <ref type="bibr" target="#b12">[13]</ref>. We make use of a custom dictionary with the 90K most frequent English words, and restrict the range of random rotation to 15 degrees.</p><p>The model was trained for 30 epochs of the dataset using SGD with a batch size of 64, an initial learning rate of 0.001, a momentum of 0.9, and a decay of 0.0005. We initialize the weights of our model with the YOLOv2 backbone pre-trained on Imagenet. During the firsts 10 epochs we train the model only for word detection, without backpropagating the loss of the PHOC prediction and using a fixed input size of 448 × 448. On the following 10 epochs we start learning the PHOC prediction output with the λ cls parameter set to 1.0. After that, we continue learning for 10 more epochs with a learning rate of 0.0001 and setting the parameters λ box and λ cls to 5.0 and 0.015 respectively. At his point we also adopted a multi-resolution training, by randomly resizing the input images among 14 possible sizes in the range from 352 × 352 to 800 × 800, and we added new samples in our training data. In particular, the added samples were the 1, 233 training images of the ICDAR2013 <ref type="bibr" target="#b23">[24]</ref> and ICDAR2015 <ref type="bibr" target="#b24">[25]</ref> datasets. During the whole training process we used the same basic data augmentation as in <ref type="bibr" target="#b7">[8]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments and results</head><p>In this section we present the experiments and results obtained on different standard benchmarks for text based image retrieval. First we describe the datasets used throughout our experiments, after that we present our results and compare them with the published state-of-the-art. Finally we discuss the scalability of the proposed retrieval method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Datasets</head><p>The IIIT Scene Text Retrieval (STR) <ref type="bibr" target="#b6">[7]</ref> dataset is a scene text image retrieval dataset composed of 10, 000 images collected from the Google image search engine and Flickr. The dataset has 50 predefined query words and for each of them a list of 10 − 50 relevant images (that contain the query word) is provided. It is a challenging dataset where relevant text appears in many different fonts and styles, and from different view points, among many distractors (images without any text).</p><p>The IIIT Sports-10k dataset <ref type="bibr" target="#b6">[7]</ref> is another scene text retrieval dataset composed of 10, 000 images extracted from sports video clips. It has 10 predefined query words with their corresponding relevant images' lists. Scene text retrieval in this dataset is specially challenging because images are low resolution and often noisy or blurred, with small text generally located on advertisements signboards.</p><p>The Street View Text (SVT) dataset <ref type="bibr" target="#b25">[26]</ref> is comprised of images harvested from Google Street View where text from business signs and names appear. It contains more than 900 words annotated in 350 different images. In our experiments we use the official partition that splits the images in a train set of 100 images and a test set of 249 images. This dataset also provides a lexicon of 50 words per image for recognition purposes, but we do not make use of it. For the image retrieval task we consider as queries the 427 unique words annotated on the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Scene text retrieval</head><p>In the scene text retrieval task, the goal is to retrieve all images that contain instances of the query words in a dataset partition. Given a query, the database elements are sorted with respect to the probability of containing the queried word. We use the mean average precision as the accuracy measure, which is the standard measure of performance for retrieval tasks and is essentially equivalent to the area below the precision-recall curve. Notice that, since the system always returns a ranked list with all the images in the dataset, the recall is always 100%. An alternative performance measure consist in considering only the top-n ranked images and calculating the precision at this specific cut-off point (P @n). <ref type="table" target="#tab_0">Table 1</ref> compares the proposed method to previous state of the art for text based image retrieval on the IIIT-STR, Sports-10K, and SVT datasets. We show the mean average precision (mAP) and processing speed for the same trained model using two different input sizes (576 × 576 and 608 × 608), and a multiresolution version that combines the outputs of the model at three resolutions (544, 576 and 608). Processing time has been calculated using a Titan X (Pascal) GPU with a batch size of 1. We appreciate that our method outperforms previously published methods in two of the benchmarks while it shows a competitive performance on the SVT dataset. In order to compare with state-of-the-art end-to-end text recognition methods, we also provide a comparison with pretrained released versions of the models of Bušta et al. <ref type="bibr" target="#b17">[18]</ref> and He et al. <ref type="bibr" target="#b26">[27]</ref>. For recognition-based results the look-up is performed by a direct matching between the query and the text detected by each model. Even when making use of a predefined word dictionary to filter results, our method, which is dictionary-free, yields superior results. Last, we compared against a variant of He et al. <ref type="bibr" target="#b26">[27]</ref> but this time both queries and the model's results are first transformed to PHOC descriptors and the look-up is based on similarity on PHOC space. It can be seen that the PHOC space does not offer any advantage to end-to-end recognition methods.  <ref type="table">Table 2</ref> further compares the proposed method to previous state of the art by the precisions at 10 (P@10) and 20 (P@20) on the Sports-10K dataset. <ref type="table">Table 2</ref>. Comparison to previous state of the art for text based image retrieval: precision at n (P@n) for Sports-10K dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Sports-10K (P@10) Sports-10K (P@20)</p><p>Mishra et al. <ref type="bibr" target="#b6">[7]</ref> 44.82 43.42 Mishra <ref type="bibr" target="#b31">[32]</ref> 47.20 46.25 Jaderberg et al. <ref type="bibr" target="#b11">[12]</ref> 91.00 92.50</p><p>Proposed (576 × 576) 91.00 90.50 Proposed (multi-res.)</p><p>92.00 90.00</p><p>In <ref type="table" target="#tab_1">Table 3</ref> we show per-query mean average precision and precisions at 10 and 20 for the Sports-10K dataset. The low performance for the query "castrol" in comparison with the rest may initially be attributed to the fact that it is the only query word not seen by our model at training time. However, by visualizing the top-10 ranked images for this query, shown in <ref type="figure">Figure 6</ref> we can see that the dataset has many unannotated instances of "castrol". The real P@10 of our model is in fact 90% and not 50%. It appears that the annotators did not consider occluded words, while our model is able to retrieve images with partial occlusions in a consistent manner. Actually, the only retrieved image among the top-10 without the "castrol" word contains an instance of "castel". By manual inspection we have computed P@10 and P@20 to be 95.0 and 93.5 respectively. Overall, the performance exhibited with the "castrol" query is a very important result, since it demonstrates that our model is able to generalize the PHOC prediction for words that has never seen at training time, and even to correctly retrieve them under partial occlusions. We found further support for this claim by analyzing the results for the six IIIT-STR query words that our model has not seen during training. <ref type="figure">Figure 7</ref> shows the top-5 ranked images for the queries "apollo", "bata", "bawarchi", "maruti", "newsagency", and "vodafone". In all of them our model reaches a 100% precision at 5. In terms of mAP the results for these queries do not show a particular decrease when compared to those obtained with other words that are part of the training set, in fact in some cases they are even better. The mean average precision for the six words in question is 74.92, <ref type="figure">Fig. 7</ref>. From top to bottom, top-5 ranked images for the queries "apollo", "bata", "bawarchi", "maruti", "newsagency", and "vodafone". Although our model has not seen this words at training time it is able to achieve a 100% P@5 for all of them.</p><p>while for the remaining 44 queries is 69.14. To further analyze our model's ability for recognizing words it has never seen at training time, we have done an additional experiment within a multi-lingual setup. For this we manually added some images with text in different Latin script languages (French, Italian, Catalan, and Spanish) to the IIIT-STR dataset. We have observed that our model, while being trained only using English words, was always able to correctly retrieve the queried text in any of those languages.</p><p>In order to analyze the errors made by our model we have manually inspected the output of our model as well as the ground truth for the five queries with a lower mAP on the IIIT-STR dataset: "ibm", "indian", "institute", "sale", and "technology". In most of these queries the low accuracy of our model can be explained in terms of having only very small and blurred instances in the database. In the case of "ibm", the characteristic font type in all instances of this word tends to be ignored by our model, and the same happens for some computer generated images (i.e.non scene images) that contain the word "sale". <ref type="figure" target="#fig_4">Figure 8</ref> shows some examples of those instances. All in all, the analysis indicates that while our model is able to generalize well for text strings not seen at training time it does not perform properly with text styles, fonts, sizes not seen before. Our intuition is that this problem can be easily alleviated with a richer training dataset. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Retrieval speed analysis</head><p>To analyze the retrieval speed of the proposed system, we have run the retrieval experiments for the IIIT-STR and Sports-10K datasets with different approximate nearest neighbor (ANN) algorithms in a standard PC with an i7 CPU and 32Gb of RAM. In <ref type="table" target="#tab_2">Table 4</ref> we appreciate that those ANN methods, with a search time sublinear in the number of indexed samples, reach retrieval speeds a couple of orders of magnitude faster than the exact nearest neighbor search based on ball-trees without incurring in any significant loss of retrieval accuracy. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>In this paper we detailed a real-time word spotting method, based on a simple architecture that allows it to detect and recognise text in a single shot and real-time speeds. The proposed method significantly improves state of the art results on scene text retrieval on the IIIT-STR and Sports-10K datasets, while yielding comparable results to state of the art in the SVT dataset. Moreover, it can do so achieving faster speed compared to other state of the art methods.</p><p>Importantly, the proposed method is fully capable to deal with out-of-dictionary (never before seen) text queries, seeing its performance unaffected compared to query words previously seen in the training set.</p><p>This is due to the use of PHOC as a word representation instead of aiming for a direct word classification. It can be seen that the network is able to learn how to extract such representations efficiently, generalizing well to unseen text strings. Synthesizing training data with different characteristics could boost performance, and is one of the directions we will be exploring in the future along with investigating the use of word embeddings other than PHOC.</p><p>The code, pre-trained models, and data used in this work are made publicly available at https://github.com/lluisgomez/single-shot-str.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Our Convolutional Neural Network predicts at the same time bounding box coordinates x, y, w, h, an objectness score c, and a pyramidal histogram of characters (PHOC) of the word in each bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Pyramidal histogram of characters (PHOC) [10] of the word "beyond" at levels 1, 2, and 3. The final PHOC representation is the concatenation of these partial histograms.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Anchor boxes used in the original YOLOv2 model for object detection in COCO (a) and PASCAL (b) datasets. (c) Our set of anchor boxes for text detection.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 8 .</head><label>8</label><figDesc>Fig. 8. Error analysis: most of the errors made by our model come from text instances with a particular style, font type, size, etc. that is not well represented in our training data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Comparison to previous state of the art for text based image retrieval: mean average precision (mAP) for IIIT-STR, and Sports-10K, and SVT datasets. (*) Results reported by Mishra et al. in [7], not by the original authors. ( †) Results computed with publicly available code from the original authors.</figDesc><table>Method 
STR 
(mAP) 

Sports 
(mAP) 

SVT 
(mAP) 

fps 

SWT [28]+ Mishra et al. [29] 
-
-
19.25 
Wang et al. [26] 
-
-
21.25* 
TextSpotter [30] 
-
-
23.32* 
1.0 
Mishra et al. [7] 
42.7 
-
56.24 
0.1 
Ghosh et al. [31] 
-
-
60.91 
Mishra [32] 
44.5 
-
62.15 
0.1 
Almazán et al. [10] 
-
-
79.65 
TextProposals [33] + DictNet [34] 
64.9 

 † 

67.5 

 † 

85.90 

 † 

0.4 
Jaderberg et al. [12] 
66.5 
66.1 
86.30 
0.3 
Bušta et al. [18] 
62.94 

 † 

59.62 

 † 

69.37 

 † 

44.21 
He et al. [27] 
50.16 

 † 

50.74 

 † 

72.82 

 † 

1.25 
He et al. [27] (with dictionary) 
66.95 

 † 

74.27 

 † 

80.54 

 † 

2.35 
He et al. [27] (PHOC) 
46.34 

 † 

52.04 

 † 

57.61 

 † 

2.35 

Proposed (576 × 576) 
68.13 
72.99 
82.02 
53.0 
Proposed (608 × 608) 
69.83 
73.75 
83.74 
43.5 
Proposed (multi-res.) 
71.37 
74.67 
85.18 
16.1 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Sports-10K per-query average precision (AP), P@10, and P@20 scores.Fig. 6. Top 10 ranked images for the query "castrol". Our model has not seen this word at training time.</figDesc><table>adidas 
castrol 
duty 
free 
hyundai 
nokia 
pakistan 
pepsi 
reliance 
sony 

AP 
94 
16 
74 
61 
77 
75 
92 
70 
89 
89 
P@10 
100 
50 
100 
90 
100 
80 
100 
90 
100 
90 
P@20 
100 
55 
100 
85 
100 
85 
100 
95 
100 
90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Mean Average Precision and retrieval time performance (in seconds) of dif- ferent approximate nearest neighbor algorithms on the IIIT-STR and Sports datasets.</figDesc><table>IIIT-STR 
Sports-10K 

Algorithm 
mAP 
secs #PHOCs mAP 
secs #PHOCs 

Baseline (Ball tree) 
0.6983 0.4321 620K 
0.7375 0.6826 
1M 
Annoy (approx NN) [35] 
0.6883 0.0027 620K 
0.7284 0.0372 
1M 
HNSW (approx NN) [36] 
0.6922 0.0018 620K 
0.7247 0.0223 
1M 
Falconn LSH (approx NN) [37] 0.6903 0.0151 620K 
0.7201 0.0178 
1M 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>This work has been partially supported by the Spanish research project TIN2014-52072-P, the CERCA Programme / Generalitat de Catalunya, the H2020 Marie Skodowska-Curie actions of the European Union, grant agreement No 712949 (TECNIOspring PLUS), the Agency for Business Competitiveness of the Government of Catalonia (ACCIO), CEFIPRA Project 5302-1 and the project "aB-SINTHE -AYUDAS FUNDACIÓN BBVA A EQUIPOS DE INVESTIGACION CIENTIFICA 2017. We gratefully acknowledge the support of the NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Microsoft COCO: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision</title>
		<meeting>of the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">COCO-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">521</biblScope>
			<biblScope unit="issue">7553</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Ontological supervision for fine grained classification of street view storefronts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Con-text: Text detection for fine-grained object classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karaoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Van Gemert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3965" to="3980" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Integrating scene text and visual appearance for fine-grained image classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.04613</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image retrieval using textual cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3040" to="3047" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="779" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">YOLO9000: better, faster, stronger</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Word spotting and recognition with embedded attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Almazán</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gordo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fornés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2552" to="2566" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Phocnet: A deep convolutional neural network for word spotting in handwritten documents</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sudholt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">A</forename><surname>Fink</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Frontiers in Handwriting Recognition</title>
		<meeting>of the IEEE International Conference on Frontiers in Handwriting Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="277" to="282" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2315" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Textboxes: A fast text detector with a single deep neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the AAAI Conference on Artificial Intelligence</title>
		<meeting>of the AAAI Conference on Artificial Intelligence</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4161" to="4167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Textboxes++: A single-shot oriented scene text detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.02765</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the European Conference on Computer Vision</title>
		<meeting>of the European Conference on Computer Vision</meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for imagebased sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Deep textspotter: An end-to-end trainable scene text localization and recognition framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Buvsta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2204" to="2212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Towards end-to-end text spotting with convolutional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03985</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Integrating visual and textual cues for query-by-string word spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Aldavert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rusiñol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Toledo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lladós</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Document Analysis and Recognition</title>
		<meeting>of the IEEE International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="511" to="515" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Query by string word spotting based on character bigram indexing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Document Analysis and Recognition</title>
		<meeting>of the IEEE International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="881" to="885" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the International Conference on Neural Information Processing Systems</title>
		<meeting>of the International Conference on Neural Information essing Systems</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mitchell</surname></persName>
		</author>
		<title level="m">Newsgroup 20 dataset</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">G</forename><surname>Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A L P</forename><surname>Almazan</surname></persName>
		</author>
		<title level="m">Proc. of the IEEE International Conference on Document Analysis and Recognition</title>
		<meeting>of the IEEE International Conference on Document Analysis and Recognition<address><addrLine>De Las Heras</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1484" to="1493" />
		</imprint>
	</monogr>
	<note>ICDAR 2013 robust reading competition</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bagdanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<title level="m">Proc. of the IEEE International Conference on Document Analysis and Recognition</title>
		<meeting>of the IEEE International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1156" to="1160" />
		</imprint>
	</monogr>
	<note>ICDAR 2015 competition on robust reading</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">End-to-end scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Computer Vision</title>
		<meeting>of the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1457" to="1464" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">An end-to-end textspotter with explicit alignment and attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<editor>CVPR.</editor>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Detecting text in natural scenes with stroke width transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Epshtein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ofek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wexler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="2963" to="2970" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Top-down and bottom-up cues for scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Alahari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2687" to="2694" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Real-time scene text localization and recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>of the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Efficient indexing for query by string text retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. of the IEEE International Conference on Document Analysis and Recognition</title>
		<meeting>of the IEEE International Conference on Document Analysis and Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1236" to="1240" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Understanding Text in Scene Images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mishra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Institute of Information Technology Hyderabad</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Textproposals: a text-specific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gómez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="60" to="74" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1406.2227</idno>
		<title level="m">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">ANNOY: Approximate nearest neighbors in C++/Python optimized for memory usage and loading/saving to disk</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Bernhardsson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Efficient and robust approximate nearest neighbor search using hierarchical navigable small world graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">A</forename><surname>Malkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yashunin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.09320</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Practical and optimal LSH for angular distance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Andoni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Indyk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Laarhoven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Razenshteyn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In: NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
