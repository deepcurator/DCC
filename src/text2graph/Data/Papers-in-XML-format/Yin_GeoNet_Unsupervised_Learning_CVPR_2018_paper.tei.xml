<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhichao</forename><surname>Yin</surname></persName>
							<email>yinzhichao@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianping</forename><surname>Shi</surname></persName>
							<email>shijianping@sensetime.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sensetime</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">GeoNet: Unsupervised Learning of Dense Depth, Optical Flow and Camera Pose</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose GeoNet, a jointly unsupervised learning framework for monocular depth, optical flow and egomotion estimation from videos. The three components are coupled by the nature of 3D scene geometry, jointly learned by our framework in an end-to-end manner. Specifically, geometric relationships are extracted over the predictions of individual modules and then combined as an image reconstruction loss, reasoning about static and dynamic scene parts separately. Furthermore, we propose an adaptive geometric consistency loss to increase robustness towards outliers and non-Lambertian regions, which resolves occlusions and texture ambiguities effectively. Experimentation on the KITTI driving dataset reveals that our scheme achieves state-of-the-art results in all of the three tasks, performing better than previously unsupervised methods and comparably with supervised ones.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding 3D scene geometry from video is a fundamental topic in visual perception. It includes many classical computer vision tasks, such as depth recovery, flow estimation, visual odometry, etc. These technologies have wide industrial applications, including autonomous driving platforms <ref type="bibr" target="#b5">[6]</ref>, interactive collaborative robotics <ref type="bibr" target="#b10">[11]</ref>, and localization and navigation systems <ref type="bibr" target="#b11">[12]</ref>, etc.</p><p>Traditional Structure from Motion (SfM) methods <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b41">42]</ref> tackle them in an integrated way, which aim to simultaneously reconstruct the scene structure and camera motion. Advances have been achieved recently in robust and discriminative feature descriptors <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b38">39]</ref>, more efficient tracking systems <ref type="bibr" target="#b54">[55]</ref>, and better exploitation of semantic level information <ref type="bibr" target="#b3">[4]</ref>, etc. Even though, the proneness to outliers and failure in non-textured regions are still not completely eliminated for their inherent reliance on high-quality lowlevel feature correspondences.</p><p>To break through these limitations, deep models <ref type="bibr" target="#b34">[35,</ref><ref type="bibr" target="#b44">45]</ref> have been applied to each of the low-level subproblems and achieve considerable gains against traditional methods. The <ref type="figure">Figure 1</ref>. Example predictions by our method on KITTI 2015 <ref type="bibr" target="#b30">[31]</ref>. Top to bottom: input image (one of the sequence), depth map and optical flow. Our model is fully unsupervised and can handle dynamic objects and occlusions explicitly.</p><p>major advantage comes from big data, which helps capturing high-level semantic correspondences for low level clue learning, thus performing better even in ill-posed regions compared with traditional methods.</p><p>Nevertheless, to preserve high performance with more general scenarios, large corpus of groundtruth data are usually needed for deep learning. In most circumstances, expensive laser-based setups and differential GPS are required, restricting the data grow to a large scale. Moreover, previous deep models are mostly tailored to solve one specific task, such as depth <ref type="bibr" target="#b25">[26]</ref>, optical flow <ref type="bibr" target="#b7">[8]</ref>, camera pose <ref type="bibr" target="#b21">[22]</ref>, etc. They do not explore the inherent redundancy among these tasks, which can be formulated by geometry regularities via the nature of 3D scene construction.</p><p>Recent works have emerged to formulate these problems together with deep learning. But all possess certain inherent limitations. For example, they require large quantities of laser scanned depth data for supervision <ref type="bibr" target="#b47">[48]</ref>, demand stereo cameras as additional equipment for data acquisition <ref type="bibr" target="#b14">[15]</ref>, or cannot explicitly handle non-rigidity and occlusions <ref type="bibr" target="#b49">[50,</ref><ref type="bibr" target="#b55">56]</ref>.</p><p>In this paper, we propose an unsupervised learning framework GeoNet for jointly estimating monocular depth, optical flow and camera motion from video. The foundation of our approach is built upon the nature of 3D scene geometry (see Sec. 3.1 for details). An intuitive explanation is that most of the natural scenes are comprised of rigid staic sur-faces, i.e. roads, houses, trees, etc. Their projected 2D image motion between video frames can be fully determined by the depth structure and camera motion. Meanwhile, dynamic objects such as pedestrians and cars commonly exist in such scenes and usually possess the characteristics of large displacement and disarrangement.</p><p>As a result, we grasp the above intuition using a deep convolutional network. Specifically, our paradigm employs a divide-and-conquer strategy. A novel cascaded architecture consisting of two stages is designed to solve the scene rigid flow and object motion adaptively. Therefore the global motion field is able to get refined progressively, making our full learning pipeline a decomposed and easierto-learn manner. The view synthesis loss guided by such fused motion field leads to natural regularization for unsupervised learning. Example predictions are shown in <ref type="figure">Fig. 1</ref>.</p><p>As a second contribution, we introduce a novel adaptive geometric consistency loss to overcome factors not included in a pure view synthesis objective, such as occlusion handling and photo inconsistency issues. By mimicking the traditional forward-backward (or left-right) consistency check, our approach filters possible outliers and occlusions out automatically. Prediction coherence is enforced between different views in non-occluded regions, while erroneous predictions get smoothed out especially in occluded regions.</p><p>Finally, we perform comprehensive evaluation of our model in all of the three tasks on the KITTI dataset <ref type="bibr" target="#b30">[31]</ref>. Our unsupervised approach outperforms previously unsupervised manners and achieves comparable results with supervised ones, which manifests the effectiveness and advantages of our paradigm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Traditional Scene Geometry Understanding Structurefrom-Motion (SfM) is a long standing problem which infers scene structure and camera motion jointly from potentially very large unordered image collections <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b15">16]</ref>. Modern approaches commonly start with feature extraction and matching, followed by geometric verification <ref type="bibr" target="#b39">[40]</ref>. During the reconstruction process, bundle adjustment <ref type="bibr" target="#b46">[47]</ref> is iteratively applied for refining the global reconstructed structure. Lately wide varieties of methods have been proposed in both global and incremental genres <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b52">53]</ref>. However, these existing methods still heavily rely on accurate feature matching. Without good photo-consistency promise, the performance cannot be guaranteed. Typical failure cases may be caused by low texture, stereo ambiguities, occlusions, etc., which may commonly appear in natural scenes.</p><p>Scene flow estimation is another closely related topic to our work, which solves the dense 3D motion field of a scene from stereoscopic image sequences <ref type="bibr" target="#b48">[49]</ref>. Top ranked methods on the KITTI benchmark typically involve the joint reasoning of geometry, rigid motion and segmentation <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b50">51]</ref>.</p><p>MRFs <ref type="bibr" target="#b26">[27]</ref> are widely adopted to model these factors as a discrete labeling problem. However, since there exist large quantities of variables to optimize, these off-the-shelf approaches are usually too slow for practical use. On the other hand, several recent methods have emphasized the rigid regularities in generic scene flow. Taniai et al. <ref type="bibr" target="#b45">[46]</ref> proposed to segment out moving objects from the rigid scene with a binary mask. Sevilla-Lara et al. <ref type="bibr" target="#b40">[41]</ref> defined different models of image motion according to semantic segmentation. Wulff et al. <ref type="bibr" target="#b53">[54]</ref> modified the Plane+Parallax framework with semantic rigid prior learned by a CNN. Different from the above mentioned approaches, we employ deep neural networks for better exploitation of high level cues, not restricted to a specific scenario. Our end-to-end method only takes on the order of milliseconds for geometry inference on a consumer level GPU. Moreover, we robustly estimate high-quality ego-motion which is not included in the classical scene flow conception.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Supervised Deep Models for Geometry Understanding</head><p>With recent development of deep learning, great progress has been made in many tasks of 3D geometry understanding, including depth, optical flow, pose estimation, etc.</p><p>By utilization of a two scale network, Eigen et al. <ref type="bibr" target="#b8">[9]</ref> demonstrated the capability of deep models for single view depth estimation. While such monocular formulation typically has heavy reliance on scene priors, a stereo setting is preferred by many recent methods. Mayer et al. <ref type="bibr" target="#b28">[29]</ref> introduced a correlation layer to mimic traditional stereo matching techniques. Kendall et al. <ref type="bibr" target="#b23">[24]</ref> proposed 3D convolutions over cost volumes by deep features to better aggregate stereo information. Similar spirits have also been adopted in learning optical flow. E. Ilg et al. <ref type="bibr" target="#b17">[18]</ref> trained a stacked network on large corpus of synthetic data and achieved impressive result on par with traditional methods.</p><p>Apart from the above problems as dense pixel prediction, camera localization and tracking have also proven to be tractable as a supervised learning task. Kendall et al. <ref type="bibr" target="#b22">[23]</ref> cast the 6-DoF camera pose relocalization problem as a learning task, and extended it upon the foundations of multiview geometry <ref type="bibr" target="#b21">[22]</ref>. Oliveira et al. <ref type="bibr" target="#b35">[36]</ref> demonstrated how to assemble visual odometry and topological localization modules and outperformed traditional learning-free methods. Brahmbhatt et al. <ref type="bibr" target="#b4">[5]</ref> exploited geometric constraints from a diversity of sensory inputs for improving localization accuracy on a broad scale.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Unsupervised Learning of Geometry Understanding</head><p>For alleviating the reliances on expensive groundtruth data, various unsupervised approaches have been proposed recently to address the 3D understanding tasks. The core supervision typically comes from a view synthesis objective based on geometric inferences. Here we briefly review on the most closely related ones and indicate the crucial differences between ours.</p><p>Garg et al. <ref type="bibr" target="#b13">[14]</ref> proposed a stereopsis based auto-encoder for single view depth estimation. While their differentiable inverse warping is based on Taylor expansion, making the training objective sub-optimal. Both Ren et al. <ref type="bibr" target="#b36">[37]</ref> and Yu et al. <ref type="bibr" target="#b20">[21]</ref> extended the image reconstruction loss together with a spatial smoothness loss for unsupervised optical flow learning, but took no advantage of geometric consistency among predictions. By contrast, Godard et al. <ref type="bibr" target="#b14">[15]</ref> exploited such constraints in monocular depth estimation by introducing a left-right consistency loss. However, they treat all the pixels equally, which would affect the effectiveness of geometric consistency loss in occluded regions. Concurrent to our work, Meister et al. <ref type="bibr" target="#b29">[30]</ref> also independently introduce a bidirectional census loss. Different to their stacked structure focusing on unsupervised learning of optical flow, we tackle several geometry understanding tasks jointly. Zhou et al. <ref type="bibr" target="#b55">[56]</ref> mimicked the traditional structure from motion by learning the monocular depth and ego-motion in a coupled way. Building upon the rigid projective geometry, they do not consider the dynamic objects explicitly and in turn learn a explainability mask for compensation. Similarly, Vijayanarasimhan et al. <ref type="bibr" target="#b49">[50]</ref> learned several object masks and corresponding rigid motion parameters for modelling moving objects. In contrast, we introduce a residual flow learning module to handle non-rigid cases and emphasize the importance of enforcing geometric consistency in predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we start by the nature of 3D scene geometry. Then we give an overview of our GeoNet. It follows by its two components: rigid structure reconstructor and nonrigid motion localizer respectively. Finally, we raise the geometric consistency enforcement, which is the core of our GeoNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Nature of 3D Scene Geometry</head><p>Videos or images are the screenshots of 3D space projected into certain dimensions. The 3D scene is naturally comprised of static background and moving objects. The movement of static parts in a video is solely caused by camera motion and depth structure. Whereas movement of dynamic objects is more complex, contributed by both homogeneous camera motion and specific object motion.</p><p>Understanding the homogeneous camera motion is relatively easier compared to complete scene understanding, since most of the region is bounded by its constraints. To decompose the problem of 3D scene understanding by its nature, we would like to learn the scene level consistent movement governed by camera motion, namely the rigid flow, and the object motion separately.</p><p>Here we briefly introduce the notations and basic concepts used in our paper. To model the strictly restricted rigid flow, we define the static scene geometries by a collection of depth maps D i for frame i, and the relative camera motion T t→s from target to source frame. The relative 2D rigid flow from target image I t to source image I s can be represented by</p><formula xml:id="formula_0">1 f rig t→s (p t ) = KT t→s D t (p t )K −1 p t − p t ,<label>(1)</label></formula><p>where K denotes the camera intrinsic and p t denotes homogeneous coordinates of pixels in frame I t . On the other hand, we model the unconstrained object motion as classical optical flow conception, i.e. 2D displacement vectors. We learn the residual flow f res t→s instead of the full representation for non-rigid cases, which we will explain later in Sec. 3.4. For brevity, we mainly illustrate the cases from target to source frames in the following, which one can easily generalize to the reversed cases. Guided by these positional constraints, we can apply differentiable inverse warping <ref type="bibr" target="#b19">[20]</ref> between nearby frames, which later become the foundation of our fully unsupervised learning scheme.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Overview of GeoNet</head><p>Our proposed GeoNet perceives the 3D scene geometry by its nature in an unsupervised manner. In particular, we use separate components to learn the rigid flow and object motion by rigid structure reconstructor and non-rigid motion localizer respectively. The image appearance similarity is adopted to guide the unsupervised learning, which can be generalized to infinite number of video sequences without any labeling cost.</p><p>An overview of our GeoNet has been depicted in <ref type="figure">Fig. 2</ref>. It contains two stages, the rigid structure reasoning stage and the non-rigid motion refinement stage. The first stage to infer scene layout is made up of two sub-networks, i.e. the DepthNet and the PoseNet. Depth maps and camera poses are regressed respectively and fused to produce the rigid flow. Furthermore, the second stage is fulfilled by the ResFlowNet to handle dynamic objects. The residual non-rigid flow learned by ResFlowNet is combined with rigid flow, deriving our final flow prediction. Since each of our subnetworks targets at a specific sub-task, the complex scene geometry understanding goal is decomposed to some easier ones. View synthesis at different stage works as fundamental supervision for our unsupervised learning paradigm.</p><p>Last but not the least, we conduct geometric consistency check during training, which significantly enhances the coherence of our predictions and achieves impressive performance. </p><p>(backward) <ref type="figure">Figure 2</ref>. Overview of GeoNet. It consists of rigid structure reconstructor for estimating static scene geometry and non-rigid motion localizer for capturing dynamic objects. Consistency check within any pair of bidirectional flow predictions is adopted for taking care of occlusions and non-Lambertian surfaces.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Rigid Structure Reconstructor</head><p>Our first stage aims to reconstruct the rigid scene structure with robustness towards non-rigidity and outliers. The training examples are temporal continuous frames I i (i = 1 ∼ n) with known camera intrinsics. Typically, a target frame I t is specified as the reference view, and the other frames are source frames I s . Our DepthNet takes single view as input and exploits accumulated scene priors for depth prediction. During training, the entire sequence is treated as a mini-batch of independent images and fed into the DepthNet. In contrast, to better utilize the feature correspondences between different views, our PoseNet takes the entire sequence concated along channel dimension as input to regress all the relative 6DoF camera poses T t→s at once. Building upon these elementary predictions, we are able to derive the global rigid flow according to Eq. (1). Immediately we can synthesize the other view between any pair of target and source frames. Let us denoteĨ However, it should be pointed out that rigid flow only dominates the motion of non-occluded rigid region while becomes invalid in non-rigid region. Although such negative effect is slightly mitigated within the rather short sequence, we adopt a robust image similarity measurement <ref type="bibr" target="#b14">[15]</ref> for the photometric loss, which maintains the balance between appropriate assessment of perceptual similarity and modest resilience for outliers, and is differentiable in nature as follows</p><formula xml:id="formula_2">L rw = α 1 − SSIM (I t ,Ĩ rig s ) 2 + (1 − α) I t −Ĩ rig s 1 ,<label>(2)</label></formula><p>where SSIM denotes the structural similarity index <ref type="bibr" target="#b51">[52]</ref> and α is taken to be 0.85 by cross validation. Apart from the rigid warping loss L rw , to filter out erroneous predictions and preserve sharp details, we introduce an edge-aware depth smoothness loss L ds weighted by image gradients</p><formula xml:id="formula_3">L ds = pt |∇D(p t )| · (e −|∇I(pt)| ) T ,<label>(3)</label></formula><p>where | · | denotes elementwise absolute value, ∇ is the vector differential operator, and T denotes the transpose of image gradient weighting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Non-rigid Motion Localizer</head><p>The first stage provides us with a stereoscopic perception of rigid scene layout, but ignores the common existence of dynamic objects. Therefore, we raise our second component, i.e. the ResFlowNet to localize non-rigid motion.</p><p>Intuitively, generic optical flow can directly model the unconstrained motion, which is commonly adopted in offthe-shelf deep models <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b17">18]</ref>. But they do not fully exploit the well-constrained property of rigid regions, which we have already done in the first stage actually. Instead, we formulate our ResFlowNet for learning the residual non-rigid flow, the shift solely caused by relative object movement to the world plane. Specifically, we cascade the ResFlowNet after the first stage in a way recommended by <ref type="bibr" target="#b17">[18]</ref>. For any given pair of frames, the ResFlowNet takes advantage of output from our rigid structure reconstructor, and predicts the corresponding residual signal f As illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>, our first stage, rigid structure reconstructor, produces high-quality reconstruction in most rigid scenes, which sets a good starting point for our second stage. Thereby, our ResFlowNet in motion localizer simply focuses on other non-rigid residues. Note that ResFlowNet can not only rectify wrong predictions in dynamic objects, but also refine imperfect results from first stage thanks to our end-to-end learning protocol, which may arise from high saturations and extreme lighting conditions. Likewise, we can extend the supervision in Sec. 3.3 to current stage with slight modifications. In detail, following the full flow f f ull t→s , we perform image warping between any pair of target and source frames again. Replacing theĨ rig s withĨ f ull s in Eq. <ref type="formula" target="#formula_2">(2)</ref>, we obtain the full flow warping loss L f w . Similarly, we extend the smoothness loss in Eq. (3) over 2D optical flow field, which we denote as L f s .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Geometric Consistency Enforcement</head><p>Our GeoNet takes rigid structure reconstructor for static scene, and non-rigid motion localizer as compensation for dynamic objects. Both stages utilize the view synthesis objective as supervision, with the implicit assumption of photometric consistency. Though we employ robust image similarity assessment such as Eq. <ref type="formula" target="#formula_2">(2)</ref>, occlusions and non-Lambertian surfaces still cannot be perfectly handled in practice.</p><p>To further mitigate these effects, we apply a forwardbackward consistency check in our learning framework without changing the network architecture. The work by Godard et al. <ref type="bibr" target="#b14">[15]</ref> incorporated similar idea into their depth learning scheme with the left-right consistency loss. However, we argue that such consistency constraints, as well as the warping loss, should not be imposed at occluded regions (see Sec. 4.3). Instead we optimize an adaptive consistency loss across the final motion field.</p><p>Concretely, our geometric consistency enforcement is fulfilled by optimizing the following objective</p><formula xml:id="formula_4">L gc = pt [δ(p t )] · ∆f f ull t→s (p t ) 1 ,<label>(4)</label></formula><p>where ∆f f ull t→s (p t ) is the full flow difference computed by forward-backward consistency check at pixel p t in I t , [·] is the Iverson bracket, and δ(p t ) denotes the condition of</p><formula xml:id="formula_5">∆f f ull t→s (p t ) 2 &lt; max{α, β f f ull t→s (p t ) 2 },<label>(5)</label></formula><p>in which (α, β) are set to be (3.0, 0.05) in our experiment. Pixels where the forward/backward flows contradict seriously are considered as possible outliers. Since these regions violate the photo consistency as well as geometric consistency assumptions, we handle them only with the smoothness loss L f s . Therefore both our full flow warping loss L f w and geometric consistency loss L gc are weighted by [δ(p t )] pixelwise. To summarize, our final loss through the entire pipeline becomes</p><formula xml:id="formula_6">L = l t,s {L rw + λ ds L ds + L f w + λ f s L f s + λ gc L gc },<label>(6)</label></formula><p>where λ denotes respective loss weight, l indexes over pyramid image scales, and t, s indexes over all the target and source frame pairs and their inverse combinations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section, we firstly introduce our network architecture and training details. Then we will show qualitative and quantitative results in monocular depth, optical flow and camera pose estimation tasks respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>Network Architecture Our GeoNet mainly contains three subnetworks, the DepthNet, the PoseNet, together to form the rigid structure reconstructor, and the ResFlowNet, incorporated with the output from previous stage to localize non-rigid motion. Since both the DepthNet and the ResFlowNet reason about pixel-level geometry, we adopt the network architecture in <ref type="bibr" target="#b14">[15]</ref> as backbone. Their structure mainly consists of two components: the encoder and the decoder parts. The encoder follows the basic structure of ResNet50 as its more effective residual learning manner. The decoder is made up of deconvolution layers to enlarge the spatial feature maps to full scale as input. To preserve  and its error map compared with original frame I t . Our PoseNet regresses the 6-DoF camera poses, i.e. the euler angles and translational vectors. The architecture is same as in <ref type="bibr" target="#b55">[56]</ref>, which contains 8 convolutional layers followed by a global average pooling layer before final prediction. We adopt batch normalization <ref type="bibr">[19]</ref> and ReLUs <ref type="bibr" target="#b32">[33]</ref> interlaced with all the convolutional layers except the prediction layers.</p><p>Training Details Our experiment is conducted using the TensorFlow framework <ref type="bibr" target="#b0">[1]</ref>. Though the sub-networks can be trained together in an end-to-end fashion, there is no guarantee that the local gradient optimization could get the network to that optimal point. Therefore, we adopt a stage-wise training strategy, reducing computational cost and memory consumption at meantime. Generally speaking, we first train the DepthNet and the PoseNet, then by fixing their weights, the ResFlowNet is trained thereafter. We also evaluated finetuning the overall network with a smaller batch size and learning rate afterwards, but achieved limited gains. During training, we resize the image sequences to a resolution of 128 × 416. We also perform random resizing, cropping, and other color augmentations to prevent overfitting. The network is optimized by Adam <ref type="bibr" target="#b24">[25]</ref>, where β 1 = 0.9, β 2 = 0.999. The loss weights are set to be λ ds = 0.5, λ f s = 0.2 and λ gc = 0.2 for all the experiments. We take an initial learning rate of 0.0002 and minibatch size of 4 at both stages. The network is trained on a single TitanXP GPU and infers depth, optical flow and camera pose with the speed of 15ms, 45ms and 4ms per example at test time. The training process typically takes around 30 epochs for the first stage and 200 epochs for the second stage to converge. To make a fair evaluation, we compare our method with different training/test split for each task on the popular KITTI dataset <ref type="bibr" target="#b30">[31]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Monocular Depth Estimation</head><p>To evaluate the performance of our GeoNet in monocular depth estimation, we take the split of Eigen et al. <ref type="bibr" target="#b8">[9]</ref> to compare with related works. Visually similar frames to the test scenes as well as static frames are excluded following <ref type="bibr" target="#b55">[56]</ref>. The groundtruth is obtained by projecting the Velodyne laser scanned points into image plane. To evaluate at input image resolution, we resize our predictions by interlinear interpolation. The sequence length is set to be 3 during training.</p><p>As shown in <ref type="table">Table 1</ref>, "Ours VGG" trained only on KITTI shares the same network architecture with "Zhou et al. <ref type="bibr" target="#b55">[56]</ref> without BN", which reveals the effectiveness of our loss functions. While the difference between "Ours VGG" and "Ours ResNet" validates the gains achieved by different network architectures. Our method significantly outperforms both supervised methods <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b27">28]</ref> and previously unsupervised work <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b55">56]</ref>. A qualitative comparison has been visualized in <ref type="figure" target="#fig_5">Fig. 4</ref>. Interestingly, our result is slightly inferior to Godard et al. <ref type="bibr" target="#b14">[15]</ref>   <ref type="bibr" target="#b30">[31]</ref> by the split of Eigen et al. <ref type="bibr" target="#b8">[9]</ref>. For training, K is the KITTI dataset <ref type="bibr" target="#b30">[31]</ref> and CS is Cityscapes <ref type="bibr" target="#b6">[7]</ref>. Errors for other methods are taken from <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b55">56]</ref>. We show the best result trained only on KITTI in bold. The results of Garg et al. <ref type="bibr" target="#b13">[14]</ref> are capped at 50m and we seperately list them for comparison.</p><p>datasets both. We believe this is due to the profound distinctions between training data characteristics, i.e. rectified stereo image pairs and monocular video sequences. Still, the results manifest the geometry understanding ability of our GeoNet, which successfully captures the regularities among different tasks out of videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Optical Flow Estimation</head><p>The performance of optical flow component is validated on the KITTI stereo/flow split. The official 200 training images are adopted as testing set. Thanks to our unsupervised nature, we could take the raw images without groundtruth for training. All the related images in the 28 scenes covered by testing data are excluded. To compare our residual flow learning scheme with direct flow learning, we specifically trained modified versions of FlowNetS <ref type="bibr" target="#b7">[8]</ref> with the unsupervised losses: "Our DirFlowNetS (no GC)" is guided by the warping loss and smoothness loss as in Sec. 3.4, while "Our DirFlowNetS" further incorporates the geometric consistency loss as in Sec. 3.5 during training. Moreover, we conduct ablation study in adaptive consistency loss versus naive consistency loss, i.e. without weighting in Eq. <ref type="bibr" target="#b3">(4)</ref>.</p><p>As demonstrated in <ref type="table" target="#tab_2">Table 2</ref>, our GeoNet achieves the lowest EPE in overall regions and comparable result in nonoccluded regions against other unsupervised baselines. The comparison between "Our DirFlowNetS (no GC)" and "Our DirFlowNetS" already manifests the effectiveness of our geometric consistency loss even in a variant architecture. Futhermore, "Our GeoNet" adopts the same losses but beats "Our DirFlowNetS" in overall regions, demonstrating the advantages of our architecture based on nature of 3D scene geometry (see <ref type="figure">Fig. 5</ref>   less, naively enforcing consistency loss proves to deteriorate accuracy as shown in "Our Naive GeoNet" entry.</p><p>Gradient Locality of Warping Loss However, the direct unsupervised flow network DirFlowNetS performs better in non-occluded regions than GeoNet, which seems unreasonable. We investigate into the end-point error (EPE) distribution over different magnitudes of groundtruth residual flow i.e. f gt − f rig , where f gt denotes the groundtruth full flow. As shown in <ref type="figure">Fig. 6</ref>, our GeoNet achieves much lower error in small displacement relative to f rig , while the error increases with large displacement. Experimentally, we find that GeoNet is extremely good at rectifying small errors from rigid flow. However, the predicted residual flow tends to prematurely converge to a certain range, which is in consistency with the observations of <ref type="bibr" target="#b14">[15]</ref>. It is because the gradients of warping based loss are derived by local pixel intensity differences, which would be amplified in a more complicated cascaded architecture, i.e. the GeoNet. We have experimented by replacing the warping loss with a nu-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Input</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Groundtruth</head><p>GeoNet Prediction GeoNet Error DirFlowNetS Error merically supervised one (guided by groundtruth or knowledge distilled from the DirFlowNetS <ref type="bibr" target="#b16">[17]</ref>) without changing network architecture, and found such issue disappeared. Investigating practical solution to avoid the gradient locality of warping loss is left as our future work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Camera Pose Estimation</head><p>We have evaluated the performance of our GeoNet on the official KITTI visual odometry split. To compare with Zhou et al. <ref type="bibr" target="#b55">[56]</ref>, we divide the 11 sequences with groundtruth into two parts: the 00-08 sequences are used for training and the 09-10 sequences for testing. The sequence length is set to be 5 during training. Moreover, we compare our method with a traditional representative SLAM framework: ORB-SLAM <ref type="bibr" target="#b31">[32]</ref>. It involves global optimization steps such as loop closure detection and bundle adjustment. Here we present two versions: "The ORB-SLAM (short)" only takes 5 frames as input and "ORB-SLAM (long)" takes the entire sequence as input. All of the results are evaluated in terms of 5-frame trajectories, and scaling factor is optimized to align with groundtruth to resolve scale ambiguity <ref type="bibr" target="#b42">[43]</ref>. As shown in <ref type="table">Table 3</ref>, our method outperforms all of the competing baselines. Note that even though our GeoNet only utlizes limited information within a rather short sequence, it still achieves better result than "ORB-SLAM(full)". This reveals again that our geometry anchored GeoNet captures additional high level cues other than sole low level feature correspondences. Finally, we analyse the failure cases and find the network sometimes gets confused about the reference system when large dynamic objects appear nearby in front of the camera, which commonly exist in direct visual SLAM <ref type="bibr" target="#b9">[10]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Seq.09 Seq.10 ORB-SLAM (full) 0.014 ± 0.008 0.012 ± 0.011 ORB-SLAM (short) 0.064 ± 0.141 0.064 ± 0.130 Zhou et al. <ref type="bibr" target="#b55">[56]</ref> 0.021 ± 0.017 0.020 ± 0.015 Zhou et al. <ref type="bibr" target="#b55">[56]</ref> updated 0.016 ± 0.009 0.013 ± 0.009 Our GeoNet 0.012 ± 0.007 0.012 ± 0.009 <ref type="table">Table 3</ref>. Absolute Trajectory Error (ATE) on KITTI odometry dataset. The results of other baselines are taken from <ref type="bibr" target="#b55">[56]</ref>. Our method outperforms all of the other methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We propose the jointly unsupervised learning framework GeoNet, and demonstrate the advantages of exploiting geometric relationships over different previously "isolated" tasks. Our unsupervised nature profoundly reveals the capability of neural networks in capturing both high level cues and feature correspondences for geometry reasoning. The impressive results compared to other baselines including the supervised ones indicate possibility of learning these low level vision tasks without costly collected groundtruth data.</p><p>For future work, we would like to tackle the gradient locality issue of warping based loss, and validate the possible improvement of introducing semantic information into our GeoNet.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>in- verse warped image from I s to target image plane by f rig t→s . Thereby the supervision signal for our current stage natu- rally comes in form of minimizing the dissimilarities be- tween the synthesized viewĨ rig s and original frame I t (or inversely).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Comparison between flow predictions at different stages. Rigid flow gives satisfactory result in most static regions, while residual flow module focuses on localizing non-rigid motion such as cars, and refining initial prediction in challenging cases such as dark illuminations and thin structures.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Comparison of monocular depth estimation between Eigen et al. [9] (supervised by depth), Zhou et al. [56] (unsupervised) and ours (unsupervised). The groundtruth is interpolated for visualization purpose. Our method captures details in thin structures and preserves consistently high-quality predictions both in close and distant regions. both global high-level and local detailed information, we use skip connections between encoder and decoder parts at different corresponding resolutions. Both the depth and residual flow are predicted in a multi-scale scheme. The input to ResFlowNet consists of batches of tensors concated in channel dimension, including the image pair I s and I t , the rigid flow f rig t→s , the synthesized viewĨ rig s</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 5 .Figure 6 .</head><label>56</label><figDesc>Figure 5. Comparison of direct flow learning method DirFlowNetS (geometric consistency loss enforced) and our GeoNet framework. As shown in the figure, GeoNet shows clear advantages in occluded, texture ambiguous regions, and even in shaded dim area.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>when trained on KITTI and Cityscapes Method Supervised Dataset Abs Rel Sq Rel RMSE RMSE log δ &lt; 1.25 δ &lt; 1.25</figDesc><table>2 

δ &lt; 1.25 

3 

Eigen et al. [9] Coarse 
Depth 
K 
0.214 
1.605 
6.563 
0.292 
0.673 
0.884 
0.957 
Eigen et al. [9] Fine 
Depth 
K 
0.203 
1.548 
6.307 
0.282 
0.702 
0.890 
0.958 
Liu et al. [28] 
Depth 
K 
0.202 
1.614 
6.523 
0.275 
0.678 
0.895 
0.965 
Godard et al. [15] 
Pose 
K 
0.148 
1.344 
5.927 
0.247 
0.803 
0.922 
0.964 
Zhou et al. [56] 
No 
K 
0.208 
1.768 
6.856 
0.283 
0.678 
0.885 
0.957 
Zhou et al. [56] updated 

2 

No 
K 
0.183 
1.595 
6.709 
0.270 
0.734 
0.902 
0.959 
Ours VGG 
No 
K 
0.164 
1.303 
6.090 
0.247 
0.765 
0.919 
0.968 
Ours ResNet 
No 
K 
0.155 
1.296 
5.857 
0.233 
0.793 
0.931 
0.973 
Garg et al. [14] cap 50m 
Pose 
K 
0.169 
1.080 
5.104 
0.273 
0.740 
0.904 
0.962 
Ours VGG cap 50m 
No 
K 
0.157 
0.990 
4.600 
0.231 
0.781 
0.931 
0.974 
Ours ResNet cap 50m 
No 
K 
0.147 
0.936 
4.348 
0.218 
0.810 
0.941 
0.977 
Godard et al. [15] 
Pose 
CS + K 
0.124 
1.076 
5.311 
0.219 
0.847 
0.942 
0.973 
Zhou et al. [56] 
No 
CS + K 
0.198 
1.836 
6.565 
0.275 
0.718 
0.901 
0.960 
Ours ResNet 
No 
CS + K 
0.153 
1.328 
5.737 
0.232 
0.802 
0.934 
0.972 

Table 1. Monocular depth results on KITTI 2015 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>for visualized comparison). Neverthe-</figDesc><table>Method 
Dataset 
Noc 
All 
EpicFlow [38] 
-
4.45 
9.57 
FlowNetS [8] 
C+S 
8.12 
14.19 
FlowNet2 [18] 
C+T 
4.93 
10.06 
DSTFlow [37] 
K 
6.96 
16.79 
Our DirFlowNetS (no GC) 
K 
6.80 
12.86 
Our DirFlowNetS 
K 
6.77 
12.21 
Our Naive GeoNet 
K 
8.57 
17.18 
Our GeoNet 
K 
8.05 
10.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Average end-point error (EPE) on KITTI 2015 flow train- ing set over non-occluded regions (Noc) and overall regions (All). The handcrafted EpicFlow takes 16s per frame at runtime; The su- pervised FlowNetS is trained on FlyingChairs and Sintel; Likewise the FlowNet2 is trained on FlyingChairs and FlyingThings3D.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Similar to [56], we omit the necessary conversion to homogeneous coordinates here for notation brevity.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Results are updated from https://github.com/tinghuiz/ SfMLearner with improved implementation.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Tensorflow: Large-scale machine learning on heterogeneous distributed systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Speeded-up robust features (SURF). CVIU</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bounding boxes, segmentations and object coordinates: How important is recognition for 3d scene flow estimation in autonomous driving scenarios? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Behl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">H</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">K</forename><surname>Mustikovela</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">A</forename><surname>Alhaija</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rother</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large-scale semantic 3d reconstruction: an adaptive multi-resolution model for multi-class volumetric labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wegner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mapnet: Geometry-aware learning of maps for camera localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Brahmbhatt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Flownet: Learning optical flow with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hazirbas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Golkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Van Der Smagt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">LSD-SLAM: Largescale direct monocular slam</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Engel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schöps</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A survey of socially interactive robots</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Fong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Nourbakhsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dautenhahn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Robotics and Autonomous Systems</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Topological mapping, localization and navigation using image collections</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Fraundorfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Engels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Nistér</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IROS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Multi-view stereo: A tutorial. Found. Trends. Comp. Graphics and Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Furukawa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hernndez</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Unsupervised cnn for single view depth estimation: Geometry to the rescue</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Carneiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Unsupervised monocular depth estimation with left-right consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Godard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Mac</forename><surname>Aodha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Brostow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Multiple View Geometry in Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<publisher>Cambridge University Press</publisher>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CoRR</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Flownet 2.0: Evolution of optical flow estimation with deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Saikia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Keuper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Back to basics: Unsupervised learning of optical flow via brightness constancy and motion smoothness</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Jason</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV Workshops</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Geometric loss functions for camera pose regression with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Posenet: A convolutional network for real-time 6-dof camera relocalization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grimes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end learning of geometry and context for deep stereo regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Martirosyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dasgupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Henry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kennedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bachrach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bry</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Markov random field models in computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Z</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">A large dataset to train convolutional networks for disparity, optical flow, and scene flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hausser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">UnFlow: Unsupervised learning of optical flow with a bidirectional census loss</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Meister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object scene flow for autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Menze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">ORB-SLAM: a versatile and accurate monocular SLAM system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mur-Artal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">D</forename><surname>Tards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M M</forename><surname>Montiel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Glvezlpez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions on Robotics</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">DTAM: Dense tracking and mapping in real-time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">A</forename><surname>Newcombe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Lovegrove</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Davison</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Stacked hourglass networks for human pose estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Newell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Topometric localization with deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">L</forename><surname>Oliveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Radwan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ISRR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised deep learning for optical flow estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Epicflow: Edge-preserving interpolation of correspondences for optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">ORB: An efficient alternative to SIFT or SURF</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rublee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Konolige</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bradski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Structure-from-motion revisited</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Schönberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-M</forename><surname>Frahm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Optical flow with semantic segmentation and localized layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">Modeling the world from internet photo collections. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A benchmark for the evaluation of rgb-d slam systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sturm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Engelhard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Burgard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In IROS</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Optimizing the viewing graph for structure-frommotion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sweeney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sattler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hollerer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Turk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Fast multi-frame stereo scene flow with motion segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Taniai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">N</forename><surname>Sinha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Fitzgibbon. Bundle adjustmenta modern synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Mclauchlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">I</forename><surname>Hartley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Workshop on Vision Algorithms</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Demon: Depth and motion network for learning monocular stereo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ummenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Mayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ilg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Three-dimensional scene flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vedula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Rander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<monogr>
		<title level="m" type="main">Sfm-net: Learning of structure and motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">3d scene flow estimation with a piecewise rigid scene model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vogel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Towards linear-time incremental structure from motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DTV-CON</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Optical flow in mostly rigid scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sevilla-Lara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Efficient non-consecutive feature tracking for robust structure-from-motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Wong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Unsupervised learning of depth and ego-motion from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brown</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
