<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep 360 Pilot: Learning a Deep Agent for Piloting through 360 • Sports Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hou-Ning</forename><surname>Hu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yen-Chen</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ming-Yu</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">NVIDIA research</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hsien-Tzu</forename><surname>Cheng</surname></persName>
							<email>hsientzucheng@gapp.nthu.edu.twarmuro@cs.nctu.edu.tw</email>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yung-Ju</forename><surname>Chang</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">National Chiao Tung University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Sun</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National Tsing Hua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Deep 360 Pilot: Learning a Deep Agent for Piloting through 360 • Sports Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Watching a 360</head><p>•  <ref type="figure">Fig. 1</ref>). Then, a recurrent neural network is used to select the main object (green dash boxes in <ref type="figure">Fig. 1</ref> </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>sports videos automatically. At each frame, the agent observes a panoramic image and has the knowledge of previously selected viewing angles. The task of the agent is to shift the current viewing angle (i.e. action) to the next preferred one (i.e., goal). We propose to directly learn an online policy of the agent from data. Specifically, we leverage a state-of-the-art object detector to propose a few candidate objects of interest (yellow boxes in</head> <ref type="figure">Fig. 1</ref><p>). Then, a recurrent neural network is used to select the main object (green dash boxes in <ref type="figure">Fig. 1</ref>). Given the main object and previously selected viewing angles, our method regresses a shift in viewing angle to move to the next one. We use the policy gradient technique to jointly train our pipeline, by minimizing: <ref type="bibr" target="#b0">(1)</ref>  <ref type="bibr" target="#b53">[53]</ref> and other baselines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss encouraging smooth transition in viewing angle, and (3) maximizing an expected reward of focusing on a foreground object. To evaluate our method, we built a new 360-Sports video dataset consisting of five sports domains. We trained domain-specific agents and achieved the best performance on viewing angle selection accuracy and users' preference compared to</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>360</head><p>• video gives a viewer immersive experiences through displaying full surroundings of a camera in a spherical canvas, which differentiates itself from traditional multimedia. As consumer-and production-grade 360</p><p>• cameras become readily available, 360</p><p>• videos are captured every minute. Moreover, the promotion of 360</p><p>• videos by social media giants including YouTube and Facebook further boosts their fast adoption. It is expected that 360</p><p>• videos * indicates equal contribution <ref type="figure">Figure 1</ref>. Panel (a) overlaps three panoramic frames sampled from a 360</p><p>• skateboarding video with two skateboarders. One skateboarder is more active than the other in this example. For each frame, the proposed "deep 360 pilot" selects a view -a viewing angle, where a Natural Field of View (NFoV) (cyan box) is centered at. It first extracts candidate objects (yellow boxes), and then selects a main object (green dash boxes) in order to determine a view (just like a human agent). Panel (b) shows the NFoV from a viewer's perspective.</p><p>will become a major video format in the near future. Studying how to display 360</p><p>• videos to a human viewer, who has a limited field of visual attention, emerges as an increasingly important problem.</p><p>Hand Manipulation (HM) and Virtual Reality (VR) are two main ways for displaying 360</p><p>• videos on a device with a Natural Field of View (NFoV) (typically a 60</p><p>• to 110</p><p>• FoV as shown in <ref type="figure">Fig. 1</ref>). In HM, a viewer navigates a 360</p><p>• video via a sequence of mouse clicks; whereas, in VR, a viewer uses embedded motion sensors in a VR headset for navigation. Note that both HM and VR require a viewer to select a viewing angle at each frame, while the FoV is defined by the device. For sports videos, such a selection mechanism could be cumbersome because "foreground objects" of interest change their locations continuously. In fact, a recent study <ref type="bibr" target="#b31">[32]</ref> showed that both HM and VR can cause a viewer to feel discomfort. Just imagine how hard it is to follow an X-game skateboarder in a 360</p><p>• video. Hence, a mechanism to automatically navigate a 360</p><p>• video in a way that captures most of the interest events for a viewer would be beneficial.</p><p>Conceptually, a 360</p><p>• -video viewer is a human agent: at each frame, the agent observes a panoramic image (i.e., the observed state) and steers the viewing angle (i.e. the action) to cover the next preferred viewing angle (i.e., the goal). We refer to this process as 360 piloting. Based on this analogy and, more importantly, to relieve the viewer from constantly steering the viewing angle while watching 360</p><p>• videos, we argue for an intelligent agent that can automatically piloting through 360</p><p>• sports videos. Using an automatic mechanism for displaying video contents is not a new idea. For example, video summarizationcondensing a long video into a short summary video <ref type="bibr" target="#b58">[58]</ref>has been used in reviewing hourly long surveillance videos. However, while a video summarization algorithm makes binary decisions on whether to select a frame or not, an agent for 360 piloting needs to operate on a spatial space to steer the viewing angle to consider events of interest in a 360</p><p>• video. On the other hand, in virtual cinematography, most camera manipulation tasks are performed within relatively simpler virtual environments <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40]</ref> and there is no need to deal with viewers' perception difficulty because 3-D positions and poses of all entities are known. However, a practical agent for 360 piloting needs to directly work with raw 360</p><p>• videos. For displaying 360</p><p>• videos, Su et al. <ref type="bibr" target="#b53">[53]</ref> proposed firstly detecting candidate events of interest in the entire video, and then applying dynamic programming to link detected events. However, as this method requires observing an entire video, it is non-suited for video streaming applications such as foveated rendering <ref type="bibr" target="#b45">[45]</ref>. We argue that being able to make a selection based on the current and previous frames (like a human agent does) is critical for 360 piloting. Finally, both <ref type="bibr" target="#b53">[53]</ref> and recent virtual cinematography works <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b5">6]</ref> aim for smooth viewing angle transition. Such transition should also be enforced for 360</p><p>• piloting. We propose "deep 360 pilot"-a deep learning-based agent that navigates a 360</p><p>• sports video in a way that smoothly captures interesting moments in the video. Our "deep 360 pilot" agent not only follows foreground objects of interest but also steers the viewing angle smoothly to increase viewers' comfort. We propose the following online pipeline to learn an online policy from human agents to model how a human agent takes actions in watching sports videos. First, because in sports videos foreground objects are those of viewers' interest, we leverage a state-of-the-art object detector <ref type="bibr" target="#b50">[50]</ref> to identify candidate objects of interest. Then, a Recurrent Neural Network (RNN) is used to select the main object among candidate objects. Given the main object and previously selected viewing angles, our method predicts how to steer the viewing angle to the preferred one by learning a regressor. In addition, our pipeline is jointly trained with the following functions: (1) a regression loss measuring the distance between the selected and ground truth viewing angles, (2) a smoothness loss to encourage smooth transition in viewing angle, and (3) an expected reward of focusing on a foreground object. We used the policy gradient technique <ref type="bibr" target="#b62">[62]</ref> to train the pipeline since it involves making an intermediate discrete decision corresponding to selecting the main object. To evaluate our method, we collected a new 360</p><p>• sports video dataset consisting of five domains and trained an agent for each domain (referred to as 360-Sports). These domain-specific agents achieve the best performance in regression accuracy and transition smoothness in viewing angle.</p><p>Our main contributions are as follows: <ref type="formula" target="#formula_0">(1)</ref> We develop the first human-like online agent for automatically navigating 360</p><p>• videos for viewers. The online processing nature suits the agent for streaming videos and predicting views for foveated VR rendering. <ref type="formula" target="#formula_1">(2)</ref> We propose a jointly trainable pipeline for learning the agent. Since the main object selection objective is non-differentiable, we employ a policy gradient technique to optimize the pipeline. (3) Our agent considers both viewing angle selection accuracy and transition smoothness. (4) We build the first 360</p><p>• sports videos dataset to train and evaluate our "deep 360 pilot" agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We review related works in video summarization, saliency detection, and virtual cinematography.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Video Summarization</head><p>We selectively review several most relevant video summarization works from a large body of literature <ref type="bibr" target="#b58">[58]</ref>. Important frame sampling. <ref type="bibr" target="#b33">[33,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b43">43,</ref><ref type="bibr" target="#b26">27]</ref> proposed to sample a few important frames as the summary of a video. <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b63">63]</ref> focused on sampling domain-specific highlights. <ref type="bibr" target="#b67">[67,</ref><ref type="bibr" target="#b54">54,</ref><ref type="bibr" target="#b17">18]</ref> proposed weakly-supervised methods to select important frames. Recently, a few deep learningbased methods <ref type="bibr" target="#b66">[66,</ref><ref type="bibr" target="#b66">66,</ref><ref type="bibr" target="#b65">65]</ref> have shown impressive performance. <ref type="bibr" target="#b48">[48,</ref><ref type="bibr" target="#b49">49,</ref><ref type="bibr" target="#b55">55]</ref> focused on extracting highlights and generating synopses which showed several spatially nonoverlapping actions from different times of a video. Several methods <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b24">25]</ref> involving user interaction have also been proposed in the graphics and the HCI communities. Ego-centric video summarization. In ego-centric videos, cues from hands and objects become easier to extract compare to third-person videos. <ref type="bibr" target="#b29">[30]</ref> proposed video summarization based on the interestingness and diverseness of objects and faces. <ref type="bibr" target="#b36">[36]</ref> further proposed tracking objects and measuring the influence of individual frames. <ref type="bibr" target="#b27">[28]</ref> proposed a novel approach to speed-up ego-centric videos while removing unpleasant camera movements.</p><p>In contrary to most video summarization methods which concern whether to select a frame or not, a method for 360 piloting concerns which viewing angle to select for each panoramic frame in a 360</p><p>• video.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Saliency Detection</head><p>Many methods have been proposed to detect salient regions typically measured by human gaze. <ref type="bibr" target="#b35">[35,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b46">46,</ref><ref type="bibr" target="#b59">59,</ref><ref type="bibr" target="#b64">64,</ref><ref type="bibr" target="#b46">46]</ref> focused on detecting salient regions on images. Recently, <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b44">44,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b61">61,</ref><ref type="bibr" target="#b60">60,</ref><ref type="bibr" target="#b57">57]</ref> leveraged deep learning and achieved significant performance gain. For videos, <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b37">37,</ref><ref type="bibr" target="#b52">52,</ref><ref type="bibr" target="#b41">41,</ref><ref type="bibr" target="#b28">29]</ref> relied on low-level appearance and motion cues as inputs. In addition, <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b51">51,</ref><ref type="bibr" target="#b39">39,</ref><ref type="bibr" target="#b12">13]</ref> included information about face, people, objects, or other contexts. Note that saliency detection methods do not select views directly, but output a saliency score map. Our method is also different to visual attention methods for object detection <ref type="bibr" target="#b38">[38,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b42">42]</ref> in that it considers view transition smoothness as selecting views, which is crucial for video watching experience. Ranking foreground objects of interest. Since regions of interest in sports videos are typically foreground objects, <ref type="bibr" target="#b55">[55]</ref> proposed to use an object detector <ref type="bibr" target="#b3">[4]</ref> to extract candidate objects of interest, then rank the saliency of these candidate objects. For 360 piloting, we propose a similar baseline which first detects objects using RCNN <ref type="bibr" target="#b50">[50]</ref>, then select the viewing angle focusing on the most salient object according to a saliency detector <ref type="bibr" target="#b64">[64]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Virtual Cinematography</head><p>Finally, existing virtual cinematography works focused on camera manipulation in simple virtual environments/video games <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b40">40]</ref> and did not deal with the perception difficulty problem. <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b56">56,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b5">6]</ref> relaxed the assumption and controlled virtual cameras within restricted static wide field-of-view video of a classroom, video conference, or basketball court, where objects of interest could be easily extracted. In contrast, our method handles raw 360</p><p>• sports videos downloaded from YouTube 1 in five domains (e.g., basketball, parkour, etc.). Recently, Su et al. <ref type="bibr" target="#b53">[53]</ref> also proposed handling raw 360</p><p>• videos download from YouTube. They referred to this problem as Pano2Vid -automatic cinematography in 360</p><p>• videos -and proposed an offline method. In contrast, we propose an online humanlike agent acting based on both present and previous observations. We argue that for handling streaming videos and other human-in-the-loop applications (e.g., foveated rendering <ref type="bibr" target="#b45">[45]</ref>) a human-like online agent is necessary in order to provide more effective video-watching support.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Our Approach</head><p>We first define the 360 piloting problem in details (Sec. 3.1). Then, we introduce our deep 360 pilot approach (Sec. 3.2-Sec. 3.6). Finally, we describe the training procedure of our model (Sec. 3.7).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Definitions</head><p>We formulate the 360 piloting task as the following online viewing angle selection task. Observation. At time t, the agent observes a new frame v t , which is the t-th frame of the 360</p><p>• video. The sequence of frames that the agent has observed up to this time is referred to as V t = {v 1 , ..., v t }. Goal. The goal of the agent is to select a viewing angle l t at time t so that the sequence of viewing angles L t = {l 1 , ..., l t } smoothly capture events of interest in the 360</p><p>• video. Note that l t = (θ t , φ t ) is a point on the 360</p><p>• viewing sphere, parameterized by the azimuth angle θ t ∈ [0 • , 360 • ] and elevation angle φ t ∈ [−90</p><p>• , 90</p><p>• ] Action. In order to achieve the goal, the agent takes the action of steering the viewing angle by ∆ t at time t. Given the previous viewing angle l t−1 and current action ∆ t , the current viewing angle l t is computed as follows,</p><formula xml:id="formula_0">l t = ∆ t + l t−1 .<label>(1)</label></formula><p>Online policy. We assume that the agent takes an action ∆ t at frame t according to an online policy π as follows,</p><formula xml:id="formula_1">∆ t = π(V t , L t−1 ),<label>(2)</label></formula><p>where the online policy depends on both the current and previous observation V t and previous viewing angles L t−1 . This implies that the previous viewing angles affect the current action similar to what a human viewer acts when viewing a 360</p><p>• sports video. Hence, the main task of 360 piloting is about learning the online policy from data.</p><p>In the following, we discuss various design choices of our proposed deep 360 pilot where the online policy in Eq. 2 is modeled as a deep neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Observing in Object Level</head><p>Instead of extracting information from the whole 360</p><p>• panoramic frame at each time instance, we propose to focus on foreground objects <ref type="figure" target="#fig_0">(Fig. 2(b)</ref>) for two reasons. Firstly, in sports videos, foreground objects are typically the targets to be followed. Moreover, the relative size of foreground objects is small compared to the whole panoramic image. If processing is done at the frame level, information of object fine details would be diluted. Working with object-based observations help our method extract subtle appearance and motion cues to take an action. We define object-level observation V O t as,</p><formula xml:id="formula_2">V O t = {v O 1 , ..., v O t }<label>(3)</label></formula><p>where</p><formula xml:id="formula_3">v O t is given by v O t = con V (O t , P t , M t ). (4) and O t = con H ({o i t }), P t = con H ({p i t }),<label>(5)</label></formula><formula xml:id="formula_4">M t = con H ({m i t }).<label>(6)</label></formula><p>Note that con H () and con V () denote horizontal and vertical concatenation of vectors, respectively. The vector </p><formula xml:id="formula_5">o i t ∈ R d</formula><note type="other">denotes the i-th object appearance feature, the vector p i t ∈ R 2 denotes the i-th object location (the same parameterization as l t ) on the view sphere at frame t and the vector m i t ∈ R k denotes the i-th object motion feature. If there are N objects, the dimension of O t , P t , and M t are d × N , 2 × N , and k × N , respectively. Then the dimension of concatenated object feature v</note><formula xml:id="formula_6">O t is (d + 2 + k) × N .</formula><p>Note that our agent is invariant to the order of objects. More explanation is shown in technical report <ref type="bibr" target="#b22">[23]</ref>. In the online policy (Eq. 2), we replace V t with V O t which consists of object appearance, motion, and location.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Focusing on the Main Object</head><p>We know that as watching a sports video a human agent gazes at the main object of interest. Assuming the location of the main object of interest, p i * t , is known, a naive policy for 360 piloting would be a policy that closely follows the main object and the action taken at each time instance iŝ</p><formula xml:id="formula_7">∆ t = p i * t − l t−1 .<label>(7)</label></formula><p>Since a machine agent does not know which object is the main one, we propose the following method to estimate the index i * of the main object. We treat this task as a classification task and predict the probability S t (i) that the object i is the main object as follows,</p><formula xml:id="formula_8">S t = π(V O t ) ∈ [0, 1] N ,<label>(8)</label></formula><formula xml:id="formula_9">where i S t (i) = 1. Given S t , i * = arg max i S t (i).<label>(9)</label></formula><p>In this case, the agent's task becomes discretely selecting one main object <ref type="figure" target="#fig_0">(Fig. 2(c)</ref>). We will need to handle this discrete selecting while introducing policy gradient <ref type="bibr" target="#b62">[62]</ref>. We note that the size of V O t grows with the number of observed frames, which increase the computation cost. We propose to aggregate object previous information via a Recurrent Neural Network (RNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Aggregating Object Information</head><p>Our online policy is implemented as a selector network as shown in <ref type="figure" target="#fig_0">Fig. 2(b)</ref>). It consists of a RNN followed by a softmax layer. The RNN aggregates information from the current frame and past state to update its current state, while the softmax layer maps the current state of the RNN into a probability distribution via W s .</p><formula xml:id="formula_10">h t = RN N S (v O t , h t−1 ), S t = softmax(W s h t )<label>(10)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning Smooth Transition</head><p>So far our model dose not take care of the smooth transition in viewing angle. Hence, we propose to refine the action from the selector network,∆ t = p i * t − l t−1 , with the motion feature, m i * t ( <ref type="figure" target="#fig_0">Fig. 2(d)</ref>), as follows,</p><formula xml:id="formula_11">µ t = RN N R (con V (m i * t ,∆ t ), µ t−1 ). ∆ t = W R µ t ,<label>(11)</label></formula><p>Here, we concatenate the motion feature and the proposed action from the selection network to form the input at time t to the regressor network RN N R . The RN N R then updates its state from µ t−1 to µ t . While RN N S focuses on main object selection, RN N R focuses on action refinement. The state of RN N R is then mapped to the final steering action vector ∆ t via W R . The resulting viewing angle is then given by l t = ∆ t + l t−1 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Our Final Model</head><p>As shown in <ref type="figure" target="#fig_0">Fig. 2</ref>, our model has three main blocks. The detector block extracts object-based observation v o t as described in Eq. 4. The selector block selects the main object index i * following Eq. 10 and Eq. 9. The regressor block regresses the viewing angle l t given main object location p i * t and motion m i * t following Eq. 7, Eq. 11, and Eq. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.7.">Training</head><p>We will first discuss the training of the regressor network and then discuss the training of the selector network.</p><p>Finally, we show how to train these two networks jointly. Note that we use the viewing angle l gt t at each time instance provided by human annotators as the ground truth. Regressor network. We train the regressor network by minimizing the Euclidean distance between the predicted viewing angle and the ground truth viewing angle at each time instance. For enforcing a smooth steering, we also regularize the training with a smoothness term, which penalizes a large rate of change in viewing angles between two consecutive frames. Let v t = l t − l t−1 be the viewing angle velocities at time t. The loss function is then given by</p><formula xml:id="formula_12">T t=1 l t − l gt t 2 + λ v t − v t−1 2<label>(12)</label></formula><p>where λ is a hyper-parameter balancing the two terms and T is the number of frames in a video. Selector network. As the ground truth annotation for each frame is provided as the human viewing angle, the main object i * gt to be focused on at each frame is unknown. Therefore, we resort to the approximated policy gradient technique proposed in <ref type="bibr" target="#b62">[62]</ref> to train the selector network. Let l(i) be a viewing angle associated with object i that is computed by the regressor network. We define the reward of selecting object i (steering the viewing angle to l(i)) to be r(l(i)) where the reward function r is defined based on the overlapping ratio between the NFOV centering at l i * t and the NFOV centering at l(i). The details of the reward function design is shown in technical report <ref type="bibr" target="#b22">[23]</ref>. We then train the selector network by maximizing the expected reward</p><formula xml:id="formula_13">E(θ) = E i∼S(i,θ) [r(l(i))],<label>(13)</label></formula><p>using the policy gradient <ref type="bibr" target="#b14">(15)</ref> where θ is the model parameter of the selector network.</p><formula xml:id="formula_14">∇ θ E(θ) = ∇ θ E i∼S(i,θ) [r(l(i))] (14) = E i∼S(i,θ) [r(l(i))∇ θ log S(i, θ)],</formula><p>We further approximate ∇ θ E(θ) using sampling as,</p><formula xml:id="formula_15">∇ θ E(θ) ≃ 1 Q Q q=1 r(l(i q ))∇ θ log S(i q , θ),<label>(16)</label></formula><p>where q is the index of sampled main object, Q is the number of samples, and the approximated gradient is referred to as the policy gradient. Joint training. Since the location of the object selected by the selector network is fed into the regressor network for computing the final viewing angle and the reward function for training the selector network is based on the regressor network's output, the two networks are trained jointly. Specifically, we joint update the trainable parameters in both networks similar to <ref type="bibr" target="#b42">[42]</ref>, which hybrids the gradients from the reinforcement signal and supervised signal. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Sports-360 Dataset</head><p>We have collected a new dataset called Sports-360 2 , which consists of 342 360</p><p>• videos downloaded from YouTube in five sports domains: basketball, parkour, BMX, skateboarding, and dance <ref type="figure" target="#fig_1">(Fig. 3)</ref>. Domains were selected according to the following criteria: (i) high availability of such videos on YouTube, (ii) the retrieved videos contain dynamic activities rather than static scenes, and (iii) containing a clear human-identifiable object of interest in most of the video frames. The third criterion is required to obtain unambiguous ground truth viewing angle in our videos.</p><p>In each domain, we downloaded the top 200 videos sorted by relevance. Then, we removed videos that were either in poor resolution or stitching quality. Next, we sampled and extracted a continuous video clip from each video where a scene transition is absent (many 360</p><p>• videos are edited and contain scene transitions). Finally, we recruited 5 human annotators, and 3 were asked to "label the most salient object for VR viewers" in each frame in a set of video segments containing human-identifiable objects. Each video segment was annotated by 1 annotator in the panorama view (see <ref type="figure" target="#fig_2">Fig. 4a</ref>). The annotation results were verified and corrected by the other 2 annotators.</p><p>We show example panoramic frames and NFoV images centered at ground truth viewing angles in <ref type="figure" target="#fig_1">Fig. 3</ref>. Our dataset includes both video segments and their annotated ground truth viewing angles. The statistics of our dataset (i.e., number of videos and frames per domain) is shown in <ref type="table">Table.</ref> 1. We split them by assigning 80% of the videos for training, and 20% for testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate deep 360 pilot on the Sports-360 dataset. We show that our model outperforms baselines by a large margin both quantitatively and qualitatively. In addition, we also conduct a user preference study. In the following, we first define the evaluation metric. Then, we describe the implementation details and baseline methods. Finally, we report the quantitative, qualitative, and human study results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Metrics.</head><p>To quantify our results, we report both Mean Overlap (MO) and Mean Velocity Difference (MVD). MO measures how much the NFoV centered at the predicted viewing angle overlaps (i.e., Intersection over Union (IoU)) with that of the ground truth one at each frame. A prediction is pre-  cise if the IoU is 1. MVD evaluates the curvature of the predicted viewing angle trajectories. It is given by the norm of the difference of viewing angle velocities in two consecutive frames given by v t − v t−1 2 . Note that, in average, the trajectory is smoother if its MVD at each frame is low.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Implementation Details</head><p>Detector. We use the Faster R-CNN <ref type="bibr" target="#b50">[50]</ref> model pre-trained on 2014 COCO detection dataset <ref type="bibr" target="#b30">[31]</ref> to generate about 400 bounding boxes for each frame. Then, we apply the tracking-by-detection algorithm <ref type="bibr" target="#b1">[2]</ref> to increase the recall of the object detection. Finally, we apply detection-bytracking <ref type="bibr" target="#b1">[2]</ref> to select reliable detection linked into long tracklets. Given these tracklets, we select top N = 16 reliable boxes per frame as our object-based observation. Detailed sensitivity experiment results can be found in the technical report <ref type="bibr" target="#b22">[23]</ref>. We found it is beneficial to use general object detectors. In the sport video domains studied, non-human objects such as skateboard, basketball, or bmx bike <ref type="figure" target="#fig_2">(Fig. 4b)</ref> provides strong cues for main objects. For each object, we extract mean pooling of the Conv5 feature ∈ R 512 in the network of R-CNN as the appearance feature o Regressor. The hidden representation of RN N R is set to 8. We set λ to 10. Learning. We optimize our model using stochastic gradients with batch size = 10 and maximum epochs = 400. The learning rate is decayed by a factor of 0.9 from the initial learning rate of 1e −5 every 50 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Methods to be Compared</head><p>We compared the proposed deep 360 pilot with a number of methods, including the state-of-the-art method AUTO-CAM <ref type="bibr" target="#b53">[53]</ref>, two baseline methods combining saliency detection with the object detector <ref type="bibr" target="#b50">[50]</ref>, and a variant of deep 360 pilot without a regressor. AUTOCAM <ref type="bibr" target="#b53">[53]</ref>: Since their model is not publicly available, we use the ground truth viewing angles to generate NFoV videos from our dataset. These NFoV videos are used to discriminatively assign interestingness on a set of pre-defined viewing angles at each frame in a testing video. Then, AUTOCAM uses dynamic programming to select optimal sequence of viewing angles. Finally, the sequence of viewing angles is smoothed in a post-processing step. Note that since AUTOCAM proposes multiple paths for each video, we use ground truth in testing data to select top ranked sequence of viewing angles as the system's final output. This creates a strong "offline" baseline. RCNN+Motion: We first extract detected boxes' optical flow. Then, we use a simple motion saliency proposed by <ref type="bibr" target="#b14">[15]</ref>, median flow, and HoF <ref type="bibr" target="#b10">[11]</ref> as features to train a gradient boosting classifier to select the box that is most likely to contain the main object. Finally, we use center of the box selected sequentially by the classifier as predictions. RCNN+BMS: We leverage the saliency detector proposed by Zhang et al. <ref type="bibr" target="#b64">[64]</ref> to detect the most salient region in a frame. With the knowledge of saliency map, we can extract the max saliency scores in each box as a score. Then we emit the most salient box center sequentially as our optimal viewing angle trajectories. Ours w/o Regressor: We test the performance of our deep 360 pilot without regressor. It emits box center of the selected main object as prediction at each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Benchmark Experiments</head><p>We compare our method with our variant and baseline methods in <ref type="table">Table.</ref> 2. In the following, we summarize our findings. AUTOCAM achieves the best MO among three baseline methods in 4 out of 5 domains. Our method significantly outperforms AUTOCAM in MO (at most 22%  <ref type="table">Table 3</ref>. User study results. For all of the five sports domains, our method is significantly preferred over AUTOCAM and Our w/o Regressor. Also, it is comparable to expert human in skateboarding and dance. gain in BMX and at least 3% gain in Dance). Although AUTOCAM achieves significantly lower MVD compared to our method, we argue that its lower MO will critically affect its viewing quality, since the majority of our videos typically contain fast moving main objects. Since we do not know how to trade MVD over MO and vice versa, we resort to a user study to compare AUTOCAM with our method. Our comparison with ours w/o regressor is the other way around. Both methods achieve similar MO while our method achieves lower MVD. These results show that with regressor, the agent steers the viewing angle more smoothly. <ref type="figure" target="#fig_4">Fig. 5</ref> shows the trajectories of viewing angles predicted by both methods for a testing video. From this visual inspection, we verify that the smoothness term results in a less jittering trajectory.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.5.">User Study</head><p>We conduct a user study mainly to compare our method with AUTOCAM and ours w/o regressor. The following is the experimental setting. For each domain, we sample two videos where all three methods achieve MO larger than 0.6 and MVD smaller than 10. This is to prevent users from comparing bad quality results, which makes identifying a better method difficult. For each video, we ask 18 users to compare two methods. In each comparison, we show videos piloted by two methods with random order via a 360</p><p>• video player. The number of times that our method wins or loses is shown in <ref type="table">Table 3</ref>. Based on a two-tailed binomial test, our method is statistically superior to AUTOCAM with p-value&lt; 0.001. This implies that users consider MO more important in this comparison. Base on the same test, our method is statistically superior to our w/o regressor with p-value&lt; 0.05. This implies that when MOs are similarly good, a small advantage of MVD results in a strong preference for our method. We also conduct a comparison between our method with the human labeled ground truth viewing angles. Base on the same test, our method is indistinguishable to human on skateboarding with p-value&lt; 0.405 and on dance with p-value&lt; 0.242.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.6.">Typical Examples</head><p>We compare our "deep 360 pilot" with AUTOCAM in <ref type="figure">Fig. 6</ref>. In the first example, both our method and AUTO-CAM work well since the main object in dancing does not move globally. Hence, the ground truth viewing angle is not constantly moving. In the next three examples, our method produces smooth trajectories while maintaining adequate view selection without any post-processing step. In contrast, AUTOCAM struggles on capturing fast-moving objects since Su et al. <ref type="bibr" target="#b53">[53]</ref> constrains every glimpses' length up to 5 seconds. Moreover, the pre-defined 198 views force many actions to be cut in half by the rendered NFoV. We further compare our method on a subset of publicly avail- <ref type="figure">Figure 6</ref>. Typical examples from four domains: (a) dance, (b) BMX, (c) parkour, and (d) skateboarding. For each example, the middle panel shows a panoramic image with motaged foreground objects. The top and bottom panels show zoomed in NFoV centered at viewing angles generated by AUTOCAM and our method, respectively. We further overlaid the NFov from AUTOCAM and our method in red and green boxes, respectively, in the middle panoramic image.</p><p>able videos from dataset of <ref type="bibr" target="#b53">[53]</ref>. We get a 140% performance boost in quantitative metrics of <ref type="bibr" target="#b53">[53]</ref>. Similar comparisons to other baseline methods and more results on dataset of <ref type="bibr" target="#b53">[53]</ref> are shown in the technical report <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We developed the first online agent for automatic 360</p><p>• video piloting. The agent was trained and evaluated using a newly composed Sport-360 dataset. We aimed at developing a domain-specific agent for the domain where the definition of a most salient object is clear (e.g., skateboarder). The experiment results showed that our agent achieved much better performance as compared to the baseline methods including <ref type="bibr" target="#b53">[53]</ref>. However, our algorithm would suffer in the domains where our assumption is violated (containing equally salient objects or no objects at all). In the future, we would like to reduce the amount of ground truth annotation needed for training our agent.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of our deep 360 pilot model. Panel (a) shows two consecutive frames. Panel (b) shows the top-N confident object bounding boxes (yellow boxes) given by the detector. Panel (c) shows the selected main object (green dash box) given by the RNN-based Selector. Panel (d) shows the final NFoV centered at the viewing angle (cyan box) predicted by the RNN-based regressor.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Our Sports-360 dataset. We show example pairs of panoramic and NFoV images in five domains: BMX, parkour, skateboarding, basketball, and dance. In each example, a panoramic frame with ground truth viewing angle (green circle) is shown on the left. The zoomed in NFoV (yellow box) centered at the ground truth viewing angle is shown on the right. The NFoV illustrates the viewers perspective.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Annotators mark main objects in 360 • videos with a mouse. The blue cross helps annotators locate cursor position, and the cyan box indicates NFoV. Main reason to label in panorama is shown in the technical report [23]. (b) Example of bmx bike.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Selector. The hidden representation of RN N S is set to 256 and it processes input v O t ∈ R (d+2+k)×N in sequences of 50 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Comparison of Ours and Ours w/o Regressor. These two methods yields similar MO, while Ours predicts smoother viewing angles in both principal axes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Table 2. Benchmark experiment results. Except "AUTOCAM" achieving a very low MVD through an offline process, "Ours w/o Regressor" achieves the best MO (the higher the better) and "Ours" achieves the best MVD (the lower the better). Most importantly, "Ours" strikes a good balance between MO and MVD.</figDesc><table>Method 

Skateboarding 
Parkour 
BMX 
Dance 
Basketball 
MO 
MVD 
MO MVD MO MVD MO MVD MO MVD 
Ours w/o Regressor. 0.71 
6.03 
0.74 
4.72 
0.71 10.73 0.79 
4.32 
0.67 
8.62 
Ours 
0.68 
3.06 
0.74 
4.41 
0.69 
8.36 
0.76 
2.45 
0.66 
6.50 

AUTOCAM [53] 
0.56 
0.25 
0.56 
0.71 
0.47 
0.55 
0.73 
0.15 
0.51 
0.66 
RCNN+BMS. 
0.25 
37.5 
0.2 
30.8 
0.22 
32.4 
0.24 
40.5 
0.2 
25.27 
RCNN+Motion. 
0.56 
34.8 
0.47 
26.2 
0.42 
25.2 
0.72 
31.4 
0.54 
25.2 

Skateboarding 
Parkour 
BMX 
Dance 
Basketball 
Comparison 
win / loss 
win / loss win / loss win / loss 
win / loss 
vs AUTOCAM 
34 / 2 
35 / 1 
31 / 5 
34 / 2 
36 / 0 
vs Ours w/o Regressor 
28 / 8 
29 / 7 
26 / 10 
31 / 5 
34 / 2 
vs human 
15 / 21 
10 / 26 
7 / 29 
14 / 22 
7 / 29 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">https://www.youtube.com/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Our dataset and code can be downloaded from https:// aliensunmin.github.io/project/360video</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank NOVATEK, MEDIATEK and NVIDIA for their support.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Frequency-tuned salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Achanta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Hemami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">J</forename><surname>Estrada</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ssstrunk</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">People-tracking-bydetection and people-detection-by-tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriluka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno>ICLR&apos;15. 2015. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Poselets: Body part detectors trained using 3d human pose annotations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A deeper look at saliency: Feature contrast, semantics, and beyond</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D B</forename><surname>Bruce</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Catton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Janjic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Mimicking human camera operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Learning online smooth predictors for realtime camera planning using recurrent decision trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Carr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Little</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Declarative camera control for automatic cinematography</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Christianson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">S</forename><surname>Weld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A deep multi-level network for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cornia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baraldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cucchiara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Temporal spectral residual: fast motion saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM Multimedia</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Human detection using oriented histograms of flow and appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A Lightweight Intelligent Virtual Cinematography System for Machinima Production</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">K</forename><surname>Elson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">O</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AIIDE</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Flycam: Practical panoramic video and automatic camera control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICME</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Context-aware saliency detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Goferman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnik-Manor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tal</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>TPAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1915" to="1926" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Schematic storyboarding for video visualization and editing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Salesin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Diverse sequential subset selection for supervised video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Video summarization using singular value decomposition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Spatio-temporal saliency detection using phase spectrum of quaternion fourier transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Graph-based visual saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Harel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">The virtual cinematographer: A paradigm for automatic real-time camera control and directing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CGI</title>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<idno>of deep 360 pi- lot. 2017</idno>
		<ptr target="https://aliensunmin.github.io/project/360video.4" />
		<imprint>
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">End-to-end saliency mapping via probability distribution prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jetley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Cliplets: Juxtaposing still and dynamic imagery</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Joshi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Metha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Stollnitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hoppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Uyttendaele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">UIST</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Largescale video summarization using web-image priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaresan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">First-person hyperlapse videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">F</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Szeliski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Lowcomplexity hog for efficient video saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hwangbo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Alan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tickoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Iyer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICIP</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3749" to="3752" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Discovering important people and objects for egocentric video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-N</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-T</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-W</forename></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Tell me where to look: Investigating ways for assisting focus in 360 video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CHI</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A hierarchical visual model for video object summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hua</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2178" to="2190" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dhsnet: Deep hierarchical saliency network for salient object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning to detect a salient object</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-Y</forename><surname>Shum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="353" to="367" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Story-driven summarization for egocentric video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Spatiotemporal saliency in dynamic scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mahadevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Reinforcement Learning for Visual Object Detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pirinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Actions in the eye: Dynamic gaze datasets and learnt saliency models for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mathe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Automatized summarization of multiplayer games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mindek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Čmolík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gröller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bruckner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM CCG</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Clustering of gaze during dynamic scene viewing is predicted by motion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mital</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Smith</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hill</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Henderson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Cognitive Computation</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="5" to="24" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Video summarization and scene detection by graph modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ngo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CSVT</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Perceptually-based foveated virtual reality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Patney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salvi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kaplanyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Benty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lefohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Luebke</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Saliency filters: Contrast based filtering for salient region detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Category-specific video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Potapov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Douze</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Webcam synopsis: Peeking around the world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gutman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Making a long video short</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rav-Acha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pritch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Peleg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Learning video saliency from human gaze using candidate selection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rudoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Goldman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zelnikmanor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1147" to="1154" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Static and space-time visual saliency detection by self-resemblance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Milanfar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Vision</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Pano2vid: Automatic cinematography for watching 360 videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-C</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jayaraman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Ranking domain-specific highlights by analyzing edited videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Summarizing unconstrained videos using salient montages</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Region of interest extraction and virtual camera control based on panoramic video capturing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Foote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kimber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMM</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="981" to="990" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Saliency detection via combining regionlevel and pixel-level predictions with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Video abstraction: A systematic review and classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Truong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venkatesh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TMCCA</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Learning a combined model of visual saliency for fixation prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-C</forename><forename type="middle">J</forename><surname>Kuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="1566" to="1579" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Saliency detection with recurrent fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ruan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">Grab: Visual saliency via novel graph model and background priors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Piramuthu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Simple statistical gradient-following algorithms for connectionist reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="1992" />
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">Highlight detection with pairwise deep ranking for first-person video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b64">
	<analytic>
		<title level="a" type="main">Exploiting surroundedness for saliency detection: a boolean map approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">6</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b65">
	<analytic>
		<title level="a" type="main">Video summarization with long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b66">
	<analytic>
		<title level="a" type="main">Summary transfer: Exemplar-based subset selection for video summarization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-L</forename><surname>Chao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b67">
	<analytic>
		<title level="a" type="main">Quasi real-time summarization for consumer videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
