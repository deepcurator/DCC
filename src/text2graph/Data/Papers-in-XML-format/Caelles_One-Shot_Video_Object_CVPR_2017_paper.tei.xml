<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">One-Shot Video Object Segmentation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Caelles</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K.-K</forename><surname>Maninis</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Leal-Taixé</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Eth</forename><surname>Zürich</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">U</forename><surname>München</surname></persName>
						</author>
						<title level="a" type="main">One-Shot Video Object Segmentation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>. Example result of our technique: The segmentation of the first frame (red) is used to learn the model of the specific object to track, which is segmented in the rest of the frames independently (green). One every 20 frames shown of 90 in total.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>This paper tackles the task of semi-supervised video object segmentation, i.e., the separation of an object from the background in a video, given the mask of the first frame. We present One-Shot Video Object Segmentation (OSVOS), based on a fully-convolutional neural network architecture that is able to successively transfer generic semantic information, learned on ImageNet, to the task of foreground segmentation, and finally to learning the appearance of a single annotated object of the test sequence (hence one-shot). Although all frames are processed independently, the results are temporally coherent and stable. We perform experiments on two annotated video segmentation databases, which show that OSVOS is fast and improves the state of the art by a significant margin (79.8% vs 68.0%).</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>From Pre-Trained Networks...</p><p>Convolutional Neural Networks (CNNs) are revolutionizing many fields of computer vision. For instance, they have dramatically boosted the performance for problems like image classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref> and object detection <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b25">26]</ref>. Image segmentation has also been taken over by CNNs recently <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b3">4]</ref>, with deep architectures pre-trained on the weakly related task of image classification on ImageNet <ref type="bibr" target="#b43">[44]</ref>. One of the major downsides of deep network approaches is their hunger for training data. Yet, with various pre-trained network architectures one may ask how much training data do we really need for the specific problem at hand? This paper investigates segmenting an object along an entire video, when we only have one single labeled training example, e.g. the first frame. * First two authors contributed equally</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>...to One-Shot Video Object Segmentation</head><p>This paper presents One-Shot Video Object Segmentation (OSVOS), a CNN architecture to tackle the problem of semi-supervised video object segmentation, that is, the classification of all pixels of a video sequence into background and foreground, given the manual annotation of one (or more) of its frames. <ref type="figure">Figure 1</ref> shows an example result of OSVOS, where the input is the segmentation of the first frame (in red), and the output is the mask of the object in the 90 frames of the sequence (in green).</p><p>The first contribution of the paper is to adapt the CNN to a particular object instance given a single annotated image (hence one-shot). To do so, we adapt a CNN pre-trained on image recognition <ref type="bibr" target="#b43">[44]</ref> to video object segmentation. This is achieved by training it on a set of videos with manually segmented objects. Finally, it is fine-tuned at test time on a specific object that is manually segmented in a single frame. <ref type="figure">Figure 2</ref> shows the overview of the method. Our proposal tallies with the observation that leveraging these different levels of information to perform object segmentation would stand to reason: from generic semantic information of a large amount of categories, passing through the knowledge of the usual shapes of objects, down to the specific properties of a particular object we are interested in segmenting.</p><p>The second contribution of this paper is that OSVOS processes each frame of a video independently, obtaining temporal consistency as a by-product rather than as the result of an explicitly imposed, expensive constraint. In other words, we cast video object segmentation as a per-frame segmentation problem given the model of the object from one (or various) manually-segmented frames. This stands in contrast to the dominant approach where temporal consistency plays the central role, assuming that objects do not change too much between one frame and the next. Such methods adapt their single-frame models smoothly throughout  <ref type="figure">Figure 2</ref>. Overview of OSVOS: (1) We start with a pre-trained base CNN for image labeling on ImageNet; its results in terms of segmentation, although conform with some image features, are not useful. <ref type="formula">(2)</ref> We then train a parent network on the training set of DAVIS; the segmentation results improve but are not focused on an specific object yet. (3) By fine-tuning on a segmentation example for the specific target object in a single frame, the network rapidly focuses on that target.</p><p>the video, looking for targets whose shape and appearance vary gradually in consecutive frames, but fail when those constraints do not apply, unable to recover from relatively common situations such as occlusions and abrupt motion. In this context, motion estimation has emerged as a key ingredient for state-of-the-art video segmentation algorithms <ref type="bibr" target="#b48">[49,</ref><ref type="bibr" target="#b41">42,</ref><ref type="bibr" target="#b16">17]</ref>. Exploiting it is not a trivial task however, as one e.g. has to compute temporal matches in the form of optical flow or dense trajectories <ref type="bibr" target="#b4">[5]</ref>, which can be an even harder problem.</p><p>We argue that temporal consistency was needed in the past, as one had to overcome major drawbacks of the then inaccurate shape or appearance models. On the other hand, in this paper deep learning will be shown to provide a sufficiently accurate model of the target object to produce temporally stable results even when processing each frame independently. This has some natural advantages: OSVOS is able to segment objects through occlusions, it is not limited to certain ranges of motion, it does not need to process frames sequentially, and errors are not temporally propagated. In practice, this allows OSVOS to handle e.g. interlaced videos of surveillance scenarios, where cameras can go blind for a while before coming back on again.</p><p>Our third contribution is that OSVOS can work at various points of the trade-off between speed and accuracy. In this sense, it can be adapted in two ways. First, given one annotated frame, the user can choose the level of finetuning of OSVOS, giving him/her the freedom between a faster method or more accurate results. Experimentally, we show that OSVOS can run at 181 ms per frame and 71.5% accuracy, and up to 79.7% when processing each frame in 7.83 s. Second, the user can annotate more frames, those on which the current segmentation is less satisfying, upon which OSVOS will refine the result. We show in the experiments that the results indeed improve gradually with more supervision, reaching an outstanding level of 84.6% with two annotated frames per sequence, and 86.9% with four, from 79.8% from one annotation.</p><p>Technically, we adopt the architecture of Fully Convolutional Networks (FCN) <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b26">27]</ref>, suitable for dense predictions. FCNs have recently become popular due to their performance both in terms of accuracy and computational efficiency <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9]</ref>. Arguably, the Achilles' heel of FCNs when it comes to segmentation is the coarse scale of the deeper layers, which leads to inaccurately localized predictions. To overcome this, a large variety of works from different fields use skip connections of larger feature maps <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref>, or learnable filters to improve upscaling <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>. To the best of our knowledge, this work is the first to use FCNs for the task of video segmentation.</p><p>We perform experiments on two video object segmentation datasets (DAVIS <ref type="bibr" target="#b36">[37]</ref> and Youtube-Objects <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>) and show that OSVOS significantly improves the state of the art 79.8% vs 68.0%. Our technique is able to process a frame of DAVIS (480×854 pixels) in 102 ms. By increasing the level of supervision, OSVOS can further improve its results to 86.9% with just four annotated frames per sequence, thus providing a vastly accelerated rotoscoping tool.</p><p>All resources of this paper, including training and testing code, pre-computed results, and pre-trained models are publicly available at www.vision.ee.ethz.ch/ cvlsegmentation/osvos/.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video Object Segmentation and Tracking: Most of the current literature on semi-supervised video object segmentation enforces temporal consistency in video sequences to propagate the initial mask into the following frames. First of all, in order to reduce the computational complexity some works make use of superpixels <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b16">17]</ref>, patches <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b10">11]</ref>, or even object proposals <ref type="bibr" target="#b37">[38]</ref>. Märki et al. <ref type="bibr" target="#b32">[33]</ref> cast the problem into a bilateral space in order to solve it more efficiently. After that, an optimization using one of the previous aggregations of pixels is usually performed; which can consider the full video sequence <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b32">33]</ref>, a subset of frames <ref type="bibr" target="#b16">[17]</ref>, or only the results in frame n to obtain the mask in n + 1 <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b10">11]</ref>. As part of their pipeline, some of the methods include the computation of optical flow <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>, which considerably reduces speed. Concurrent works have also used deep learning to address Video Object Segmentation. MaskTrack <ref type="bibr" target="#b21">[22]</ref> learns to refine the detected masks frame by frame, by using the detections of the previous frame, along with Optical Flow and post-processing with CRFs. In <ref type="bibr" target="#b20">[21]</ref>, the authors combine training of a CNN with ideas of bilateral filtering. Different from those approaches, OSVOS is a simpler pipeline which segments each frame independently, and produces more accurate results, while also being significantly faster. In the case of visual tracking (bounding boxes instead of segmentation) Nam and Han <ref type="bibr" target="#b31">[32]</ref> use a CNN to learn a representation of the object to be tracked, but only to look for the most similar window in frame n + 1 given the object in frame n. In contrast, our CNN learns a single model from frame 1 and segments the rest of the frames from this model.</p><p>FCNs for Segmentation: Segmentation research has closely followed the innovative ideas of CNNs in the last few years. The advances observed in image recognition <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref> have been beneficial to segmentation in many forms (semantic <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b33">34]</ref>, instance-level <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b7">8]</ref>, biomedical <ref type="bibr" target="#b42">[43]</ref>, generic <ref type="bibr" target="#b28">[29]</ref>, etc.). Many of the current best performing methods have in common a deep architecture, usually pre-trained on ImageNet, trainable end-to-end. The idea of dense predictions with CNNs was pioneered by <ref type="bibr" target="#b11">[12]</ref> and formulated by <ref type="bibr" target="#b26">[27]</ref> in the form of Fully Convolutional Networks (FCNs) for semantic segmentation. The authors noticed that by changing the last fully connected layers to 1 × 1 convolutions it is possible to train on images of arbitrary size, by predicting correspondingly-sized outputs. Their approach boosts efficiency over patch-based approaches where one needs to perform redundant computations in overlapping patches. More importantly, by removing the parameter-intensive fully connected layers, the number of trainable parameters drops significantly, facilitating training with relatively few labeled data.</p><p>In most CNN architectures <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref>, activations of the intermediate layers gradually decrease in size, because of spatial pooling operations or convolutions with a stride. Making dense predictions from downsampled activations results in coarsely localized outputs <ref type="bibr" target="#b26">[27]</ref>. Deconvolutional layers that learn how to upsample are used in <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b51">52]</ref>. In <ref type="bibr" target="#b38">[39]</ref>, activations from shallow layers are gradually injected into the prediction to favor localization. However, these architectures come with many more trainable parameters and their use is limited to cases with sufficient data.</p><p>Following the ideas of FCNs, Xie and Tu <ref type="bibr" target="#b50">[51]</ref> separately supervised the intermediate layers of a deep network for contour detection. The duality between multiscale contours and hierarchical segmentation <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref> was further studied by Maninis et al. <ref type="bibr" target="#b28">[29]</ref> by bringing CNNs to the field of generic image segmentation. In this work we explore how to train an FCN for accurately localized dense prediction based on very limited annotation: a single segmented frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">One-Shot Deep Learning</head><p>Let us assume that one would like to segment an object in a video, for which the only available piece of information is its foreground/background segmentation in one frame. Intuitively, one could analyze the entity, create a model, and search for it in the rest of the frames. For humans, this very limited amount of information is more than enough, and changes in appearance, shape, occlusions, etc. do not pose a significant challenge, because we leverage strong priors: first "It is an object," and then "It is this particular object." Our method is inspired by this gradual refinement.</p><p>We train a Fully Convolutional Neural Network (FCN) for the binary classification task of separating the foreground object from the background. We use two successive training steps: First we train on a large variety of objects, offline, to construct a model that is able to discriminate the general notion of a foreground object, i.e., "It is an object." Then, at test time, we fine-tune the network for a small number of iterations on the particular instance that we aim to segment, i.e., "It is this particular object." The overview of our method is illustrated in <ref type="figure">Figure 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">End-to-end trainable foreground FCN</head><p>Ideally, we would like our CNN architecture to satisfy the following criteria:</p><p>1. Accurately localized segmentation output, as discussed in Section 2. 2. Relatively small number of parameters to train from a limited amount of annotation data. 3. Relatively fast testing times. We draw inspiration from the CNN architecture of <ref type="bibr" target="#b29">[30]</ref>, originally used for biomedical image segmentation. It is based on the VGG <ref type="bibr" target="#b46">[47]</ref> network, modified for accurately localized dense prediction (Point 1). The fully-connected layers needed for classification are removed (Point 2), and efficient image-to-image inference is performed (Point 3). The VGG architecture consists of groups of convolutional plus Rectified Linear Units (ReLU) layers grouped into 5 stages. Between the stages, pooling operations downscale the feature maps as we go deeper into the network. We connect convolutional layers to form separate skip paths from the last layer of each stage (before pooling). Upscaling operations take place wherever necessary, and feature maps from the separate paths are concatenated to construct a volume with information from different levels of detail. We linearly fuse the feature maps to a single output which has the same dimensions as the image, and we assign a loss function to it. The proposed architecture is shown in <ref type="figure" target="#fig_1">Figure 4 (1)</ref>, foreground branch.</p><p>The pixel-wise cross-entropy loss for binary classification (we keep the notation of Xie and Tu <ref type="bibr" target="#b50">[51]</ref>) is in this case defined as:</p><formula xml:id="formula_0">L (W) = − j y j logP (y j =1|X;W)+(1−y j )log (1−P (y j =1|X;W)) = − j∈Y + logP (y j =1|X;W) − j∈Y − logP (y j =0|X; W)</formula><p>where W are the standard trainable parameters of a CNN, X is the input image, y j ∈ 0, 1, j = 1, .., |X| is the pixelwise binary label of X, and Y + and Y − are the positive and negative labeled pixels. P (·) is obtained by applying a sigmoid to the activation of the final layer.</p><p>In order to handle the imbalance between the two binary classes, Xie and Tu <ref type="bibr" target="#b50">[51]</ref> proposed a modified version of the cost function, originally used for contour detection (we drop W for the sake of readability):</p><formula xml:id="formula_1">L mod = −β j∈Y + logP (y j =1|X) − (1−β) j∈Y − logP (y j =0|X) (1)</formula><p>where β = |Y − |/|Y |. Equation 1 allows training for imbalanced binary tasks <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b29">30]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Training details</head><p>Offline training: The base CNN of our architecture <ref type="bibr" target="#b46">[47]</ref> is pre-trained on ImageNet for image labeling, which has proven to be a very good initialization to other tasks <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b51">52]</ref>. Without further training, the network is not capable of performing segmentation, as illustrated in <ref type="figure">Figure 2</ref> (1). We refer to this network as the "base network."</p><p>We therefore further train the network on the binary masks of the training set of DAVIS, to learn a generic notion of how to segment objects from their background, their usual shapes, etc. We use Stochastic Gradient Descent (SGD) with momentum 0.9 for 50000 iterations. We augment the data by mirroring and zooming in. The learning rate is set to 10 −8 , and is gradually decreased. After offline training, the network learns to segment foreground objects from the background, as illustrated in <ref type="figure">Figure 2</ref> (2). We refer to this network as the "parent network."</p><p>Online training/testing: With the parent network available, we can proceed to our main task ("test network" in <ref type="figure">Figure 2</ref> (3)): Segmenting a particular entity in a video, given the image and the segmentation of the first frame. We proceed by further training (fine-tuning) the parent network for the particular image/ground-truth pair, and then testing on the entire sequence, using the new weights. The timing of our method is therefore affected by two times: the finetuning time (once per annotated mask) and the segmentation of all frames (once per frame). In the former we have a trade-off between quality and time: the more iterations we allow the technique to learn, the better results but the longer the user will have to wait for results. The latter does not depend on the training time: OSVOS is able to segment each 480p frame (480 × 854) in 102 ms.</p><p>Regarding the fine-tuning time, we present two different modes: One can either need to fine-tune online, by segmenting a frame and waiting for the results in the entire sequence, or offline, having access to the object to segment beforehand. Especially in the former mode, there is the need to control the amount of time dedicated to training: the more time allocated for fine-tuning, the more the user waits and the better the results are. In order to explore this trade-off, in our experiments we train for a period between 10 seconds and 10 minutes per sequence. <ref type="figure" target="#fig_0">Figure 3</ref> shows a qualitative example of the evolution of the results' quality depending on the time allowed for fine-tuning.</p><p>In the experiments section, <ref type="figure" target="#fig_6">Figure 8</ref> quantifies this evolution. Ablation analysis shows that both offline and online training are crucial for good performance: If we perform our online training directly from the ImageNet model, the performance drops significantly. Only dropping the online training for a specific object also yields a significantly worse performance, as already transpired from <ref type="figure">Figure 2</ref> (2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Contour snapping</head><p>In the field of image classification <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b18">19]</ref>, where our base network was designed and trained, spatial invariance is a design choice: no matter where an object appears in the image, the classification result should be the same. This is in contrast to the accurate localization of the object contours that we expect in (video) object segmentation. Despite the use of skip connections <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b50">51,</ref><ref type="bibr" target="#b29">30]</ref> to minimize the loss of spatial accuracy, we observe that OSVOS's segmentations have some room for improvement in terms of contour localization. We propose two different strategies to improve the results in this regard.</p><p>First, we propose the use of the Fast Bilateral Solver (FBS) <ref type="bibr" target="#b1">[2]</ref> to snap the background prediction to the image edges. It performs a Gaussian smoothing in the five-dimensional color-location space, which results in a smoothing of the input signal (foreground segmentation) that preserves the edges of the image. It is useful in practice because it is fast (≈60 ms per frame), and it is differentiable so it can be included in an end-to-end trainable deep learn- ing architecture. The drawback of this approach, though, is that it preserves naive image gradients, i.e. pixels with high Euclidean differences in the color channels.</p><p>To overcome this limitation, our second approach snaps the results to learned contours instead of simple image gradients. To this end, we propose a complementary CNN in a second branch, that is trained to detect object contours. The proposed architecture is presented in <ref type="figure" target="#fig_1">Figure 4</ref>: <ref type="formula">(1)</ref> shows the main foreground branch, where the foreground pixels are estimated; (2) shows the contour branch, which detects all contours in the scene (not only those of the foreground object). This allows us to train offline, without the need to fine-tune on a specific example online. We used the exact same architecture in the two branches, but training for different losses. We noticed that jointly training a network with shared layers for both tasks rather degrades the obtained results thus we kept the computations for the two objectives uncorrelated. This allows us to train the contour branch only offline and thus it does not affect the online timing. Since there is need for high recall in the contours, we train on the PASCAL-Context <ref type="bibr" target="#b30">[31]</ref> database, which provides contour annotations for the full scene of an image. Finally, in the boundary snapping step <ref type="figure" target="#fig_0">(Figure 4 (3)</ref>, we compute superpixels that align to the computed contours (2) by means of an Ultrametric Contour Map (UCM) <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b39">40]</ref>, which we threshold at a low value. We then take a foreground mask (1) and we select superpixels via majority voting (those that overlap with the foreground mask over 50%) to form the final foreground segmentation.</p><p>In this second case, we trade accuracy for speed, since the snapping process takes longer (400 ms instead of 60 ms per frame), but we achieve more accurate results. Both refinement processes result in a further boost in performance, and are fully modular, meaning that depending on the requirements one can choose not to use them, sacrificing accuracy for execution time, since both modules come with a small, yet avoidable computational overhead.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Validation</head><p>Databases, state-of-the-art, and measures: The main part of our experiments is done on the recently-released DAVIS database <ref type="bibr" target="#b36">[37]</ref>, which consists of 50 full-HD video sequences with all of their frames segmented with pixellevel accuracy. We use three measures: region similarity in terms of intersection over union (J ), contour accuracy (F), and temporal instability of the masks (T ). All evaluation results are computed on the validation set of DAVIS.</p><p>We compare to a large set of state-of-the-art methods, including two very recent semi-supervised techniques, OFL <ref type="bibr" target="#b48">[49]</ref>, BVS <ref type="bibr" target="#b32">[33]</ref>, as well as the methods originally compared on the DAVIS benchmark: FCP <ref type="bibr" target="#b37">[38]</ref>, JMP <ref type="bibr" target="#b10">[11]</ref>, HVS <ref type="bibr" target="#b16">[17]</ref>, SEA <ref type="bibr" target="#b41">[42]</ref>, and TSP <ref type="bibr" target="#b5">[6]</ref>. We also add the unsupervised techniques: FST <ref type="bibr" target="#b35">[36]</ref>, SAL <ref type="bibr" target="#b45">[46]</ref>, KEY <ref type="bibr" target="#b24">[25]</ref>, MSG <ref type="bibr" target="#b4">[5]</ref>, TRC <ref type="bibr" target="#b12">[13]</ref>, CVOS <ref type="bibr" target="#b47">[48]</ref>, and NLC <ref type="bibr" target="#b9">[10]</ref>. We add two informative bounds: the quality that an oracle would reach by selecting the best segmented object proposal out of two state-of-the-art techniques (COB <ref type="bibr" target="#b28">[29]</ref> and MCG <ref type="bibr" target="#b39">[40]</ref>), and by selecting the best superpixels from COB (COB|SP).</p><p>For completeness, we also experiment on Youtubeobjects <ref type="bibr" target="#b40">[41]</ref>, manually segmented by Jain and Grauman <ref type="bibr" target="#b19">[20]</ref>. We compare to OFL <ref type="bibr" target="#b48">[49]</ref>, BVS <ref type="bibr" target="#b32">[33]</ref>, LTV <ref type="bibr" target="#b34">[35]</ref>, HBT <ref type="bibr" target="#b15">[16]</ref>, AFS <ref type="bibr" target="#b49">[50]</ref>, SCF <ref type="bibr" target="#b19">[20]</ref>, and JFS <ref type="bibr" target="#b44">[45]</ref> and take the pre-computed evaluation results from previous work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ablation Study on DAVIS:</head><p>To analyze and quantify the importance and need of each of the proposed blocks of our algorithm, <ref type="table">Table 1</ref> shows the evaluation of OSVOS compared to ablated versions without each of its building blocks. Each column shows: the original method without boundary snapping (-BS), without pre-training the parent network on DAVIS (-PN), or without performing the oneshot learning on the specific sequence (-OS). In smaller and italic font we show the loss (in blue) or gain (in red) on each metric with respect to our final approach.</p><p>We can see that both the pre-training of the parent network and the one-shot learning play an important role (we lose 15.2 and 27.3 points in J without them, respectively). Removing both, i.e., using the Imagenet raw CNN, the results in terms of segmentation (J = 17.6%) are completely random. The boundary snapping adds 2.  provement, and is faster than conventional methods such as adding a CRF on top of the segmentation <ref type="bibr" target="#b6">[7]</ref>. <ref type="figure" target="#fig_2">Figure 5</ref> further analyzes the type of errors that OSVOS produces (with and without boundary snapping), by dividing them into False Positives (FP) and False Negatives (FN). FP are further divided into close and far, setting the division at 20 pixels from the object. We can observe that the majority of the errors come from false negatives. Boundary snapping mainly reduces the false positives, both the ones close to the boundaries (more accurate contours) and the spurious detections far from the object, because they do not align with the trained generic contours.</p><p>Comparison to the State of the Art on DAVIS: <ref type="table">Table 2</ref> compares OSVOS to the rest of the state of the art. In terms of region similarity J , OSVOS is 11.8 points above the second best technique and 19.8 above the third best. In terms of contour accuracy F, OSVOS is 17.2 and 21.8 points above them. Our results are better than those obtained by an oracle selecting the best object proposal from the state-of-theart object proposals COB. Even if the oracle would select the best set of superpixels to form each mask (COB|SP), OSVOS would be only 6.7 points below. <ref type="table">Table 3</ref> shows an evaluation with respect to different attributes annotated in the DAVIS dataset, by comparing the performance of the methods on the sequences with a given attribute (challenge) versus the performance on those without it. OSVOS has the best performance on all attributes, and it has a significant resilience to these challenges (smallest decrease of performance when the attribute is presentnumbers in italics). <ref type="figure">Figure 6</ref> shows the results per sequence compared to the  <ref type="table">Table 3</ref>. Attribute-based performance: Quality of the techniques on sequences with a certain attribute and the gain with respect to this quality in the sequences without it (in italics and smaller font). See DAVIS <ref type="bibr" target="#b36">[37]</ref> for the meaning of the acronyms.</p><p>state of the art. OSVOS has the best performance in the majority of sequences and is very close to the best in the rest. The results are especially impressive in sequences such as Drift-Chicane or Bmx-Trees, where the majority of techniques fail. <ref type="figure" target="#fig_5">Figure 7</ref> shows the qualitative results on these two sequences. In the first row, the problem is especially challenging because of the smoke and the small initial size of the car. In the second row, OSVOS' worse sequence, despite vastly outperforming the rest of techniques. In this case, OSVOS loses track of the biker when he is occluded, but recovers when he is visible again. The rest of techniques lose the object because of the heavy occlusions.</p><p>Number of training images (parent network): To evaluate how much annotated data are needed to retrain a parent network, <ref type="table">Table 4</ref> shows the performance of OSVOS (-BS) when using a subset of the DAVIS train set. We randomly selected a fixed percentage of the annotated frames in each video. We conclude that by using only~200 anno- tated frames, we are able to reach almost the same performance than when using the full DAVIS train split, thus not requiring full video annotations for the training procedure.</p><p>Timing: The computational efficiency of video object segmentation is crucial for the algorithms to be usable in practice. OSVOS can adapt to different timing requirements, providing progressively better results the more time .7</p><p>.8</p><p>.9</p><p>1 Ours OFL <ref type="bibr" target="#b48">[49]</ref> BVS <ref type="bibr" target="#b32">[33]</ref> FCP <ref type="bibr" target="#b37">[38]</ref> JMP <ref type="bibr" target="#b10">[11]</ref> HVS <ref type="bibr" target="#b16">[17]</ref> SEA <ref type="bibr" target="#b41">[42]</ref> TSP <ref type="bibr" target="#b5">[6]</ref> Figure 6. DAVIS Validation: Per-sequence results of region similarity (J ).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Drift-Chicane</head><p>Bmx-Trees we can afford, by letting the fine-tuning algorithm at test time do more or fewer iterations. To show this behavior, <ref type="figure" target="#fig_6">Figure 8</ref> shows the quality of the result with respect to the time it takes to process each 480p frame. As introduced before, OSVOS' time can be divided into the fine-tuning time plus the time to process each frame independently. The first mode we evaluate is -OS-BS ( ), in which we do not fine-tune to the particular sequence, and thus use the parent network directly. In this case, the quality is not very good (although comparable to some previous techniques), but we only need to do a forward pass of the CNN for each frame.  To take into account the fine-tuning time, we can consider two scenarios. First, in Ours ( ) or -BS ( ) we average the fine-tuning time (done once per sequence) over the length of that sequence. This way, the curves show the gain in quality with respect to the fine-tuning time, plus the forward pass on each frame. Using the same notation than in the ablation study, the two different curves refer to whether we do not perform boundary snapping (-BS) or we snap to the learned contours (Ours). The better results come at the price of adding the snapping cost so depending on the needed speed, one of the two can be chosen.</p><p>Since OSVOS processes frames independently, one could also perform the fine-tuning offline, by training on a picture of the object to be segmented beforehand (e.g. take a picture of a racing horse before the race). In this scenario, OSVOS can process each frame by one forward pass of the CNN (Ours Pre , -BS Pre ), and so be considerably fast.</p><p>Compared to other techniques, OSVOS is significantly faster and/or more accurate at all regimes, from fast modes: 74.7 versus 60.0 of BVS ( ) at 400 ms, and 79.8 versus 68.0 of OFL ( ) at lower speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Refinement of results:</head><p>Another advantage of our technique is that we can naturally incorporate more supervision in the form of more annotated frames. In a production environment, for instance, one needs a certain quality below which results are not usable. In this scenario, OSVOS can provide the results with one annotated frame, then the operator can decide whether the quality is good enough, and if not, segment another frame. OSVOS can then incorporate that knowledge into further fine-tuning the result.</p><p>To model this scenario, we take the results with N manual annotations, select the frame in which OSVOS performs worse, similarly to what an operator would do, i.e. select a frame where the result is not satisfactory; and add the ground-truth annotation into the fine-tuning. <ref type="table">Table 5</ref> shows the evolution of the quality when more annotations are added (0 means we test the parent network directly, i.e. zero-shot). We can see that the quality significantly increases from one to two annotations and saturates at around  five. As a measure of the upper bound of OSVOS, we finetuned on all annotated frames and tested on the same ones (last column), which indeed shows us that five annotated frames almost get the most out of this architecture. <ref type="figure" target="#fig_7">Figure 9</ref> shows a qualitative example of this process, where the user annotates frame 0, where only one camel is visible (a). In frame 35, OSVOS also segments the second camel that appears (b), which has almost the exact same appearance. This can be solved (f) by annotating two more frames, 88 (c) and 46 (e), which allows OSVOS to learn the difference between these two extremely similar objects, even without taking temporal consistency into account.</p><p>Evaluation as a tracker: Video object segmentation could also be evaluated as a Visual Object Tracking (VOT) <ref type="bibr" target="#b27">[28]</ref> algorithm, by computing the bounding box around each of the segmentations. We compare to the winner of the VOT Challenge 2015 <ref type="bibr" target="#b27">[28]</ref>: MDNET <ref type="bibr" target="#b31">[32]</ref>. Since we cannot compare in the original dataset of the VOT Challenge (the ground-truth objects are not segmented so we cannot fine-tune on it), we run MDNET on DAVIS. Table 6 shows the percentage of bounding boxes coming from each technique that have an intersection over union with the ground-truth bounding box above different thresholds. The higher the threshold, the more alignment with the ground truth is required. We can see that OSVOS has significant better results as tracker than MDNET at all regimes, with more margin at higher thresholds.</p><p>Results on Youtube-Objects: For completeness, we also do experiments on Youtube-objects <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b19">20]</ref>, where we take the pre-computed evaluation from other papers. <ref type="table">Table 7</ref> shows that we perform slightly better than the state of the art OFL, which is significantly slower, and despite the fact that the sequences in this database have significant less occlu-  <ref type="table">Table 7</ref>. Youtube-Objects evaluation: Per-category mean intersection over union (J ).</p><p>sions and motion than in DAVIS, which favors techniques that enforce temporal consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>Deep learning approaches often require a huge amount of training data in order to solve a specific problem such as segmenting an object in a video. Quite in contrast, human observers can solve similar challenges with only a single training example. In this paper, we demonstrate that one can reproduce this capacity of one-shot learning in a machine: Based on a network architecture pre-trained on generic datasets, we propose One-Shot Video Object Segmentation (OSVOS) as a method which fine-tunes it on merely one training sample and subsequently outperforms the state-of-the-art on DAVIS by 11.8 points. Interestingly, our approach does not require explicit modeling of temporal consistency using optical flow algorithms or temporal smoothing and thus does not suffer from error propagation over time (drift). Instead, OSVOS processes each frame of the video independently and gives rise to highly accurate and temporally consistent segmentations. All resources of this paper can be found at www.vision.ee.ethz.ch/ cvlsegmentation/osvos/</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Qualitative evolution of the fine tuning: Results at 10 seconds and 1 minute per sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Two-stream FCN architecture: The main foreground branch (1) is complemented by a contour branch (2) which improves the localization of the boundaries (3).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Error analysis of our method: Errors divided into False Positives (FP-Close and FP-Far) and False Negatives (FN). Values are total error pixels relative to the error in the -BS case.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>.</head><label></label><figDesc>Amount of training data: Region similarity (J ) as a function of the number of training images. Full DAVIS is 2079.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Qualitative results: First row, an especially difficult sequence which OSVOS segments well. Second row, OSVOS' worst result.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Quality versus timing: Region similarity with respect to the processing time per frame.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. Qualitative incremental results: The segmentation on frame 35 improves after frames 0, 88, and 46 are annotated.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>4 points of im-Table 1. Ablation study on DAVIS: Comparison of OSVOS against downgraded versions without some of its components.Measure Ours OFL BVS FCP JMP HVS SEA TSP FST NLC MSG KEY CVOS TRC SAL COB|SP COB MCGTable 2. DAVIS Validation: OSVOS versus the state of the art, and practical bounds.</figDesc><table>Measure 
Ours 
-BS 
-PN-BS -OS-BS -PN-OS-BS 

Mean M ↑ 79.8 77.4 2.4 64.6 15.2 52.5 27.3 17.6 62.2 
J Recall O ↑ 93.6 91.0 2.6 70.5 23.2 57.7 35.9 2.3 91.3 
Decay D ↓ 14.9 17.4 2.5 27.8 13.0 −1.9 16.7 1.8 13.1 
Mean M ↑ 80.6 78.1 2.5 66.7 13.9 47.7 32.9 20.3 60.4 
F Recall O ↑ 92.6 92.0 0.6 74.4 18.3 47.9 44.7 2.4 90.2 
Decay D ↓ 15.0 19.4 4.5 26.4 11.4 
0.6 14.3 2.4 12.6 
T 
Mean M ↓ 37.6 33.5 4.0 60.9 23.3 53.8 16.2 46.0 8.4 

Semi-Supervised 

Unsupervised 
Bounds 

Mean M ↑ 79.8 68.0 60.0 58.4 57.0 54.6 50.4 31.9 55.8 55.1 53.3 49.8 
48.2 
47.3 39.3 
86.5 
79.3 
70.7 
J Recall O ↑ 93.6 75.6 66.9 71.5 62.6 61.4 53.1 30.0 64.9 55.8 61.6 59.1 
54.0 
49.3 30.0 
96.5 
94.4 
91.7 
Decay D ↓ 14.9 26.4 28.9 −2.0 39.4 23.6 36.4 38.1 
0.0 12.6 
2.4 14.1 
10.5 
8.3 
6.9 
2.8 
3.2 
1.3 
Mean M ↑ 80.6 63.4 58.8 49.2 53.1 52.9 48.0 29.7 51.1 52.3 50.8 42.7 
44.7 
44.1 34.4 
87.1 
75.7 
62.9 
F Recall O ↑ 92.6 70.4 67.9 49.5 54.2 61.0 46.3 23.0 51.6 51.9 60.0 37.5 
52.6 
43.6 15.4 
92.4 
88.5 
76.7 
Decay D ↓ 15.0 27.2 21.3 −1.1 38.4 22.7 34.5 35.7 
2.9 11.4 
5.1 10.6 
11.7 
12.9 
4.3 
2.3 
3.9 
1.9 
T Mean M ↓ 37.6 21.7 34.5 29.6 15.3 35.0 14.9 41.2 34.3 41.4 29.1 25.2 
24.4 
37.6 64.1 
27.4 
44.1 
69.8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Table 5. Progressive refinement: Quality achieved with respect to the number of annotated frames OSVOS trains from.</figDesc><table>Annotations 

0 
1 
2 
3 
4 
5 
All 

Quality (J ) 58.5 79.8 84.6 85.9 86.9 87.5 88.7 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 6 .</head><label>6</label><figDesc>MDNET [32] 66.4 57.8 43.4 29.5 14.7Evaluation as a tracker: Percentage of bounding boxes that match with the ground truth at different levels of overlap.</figDesc><table>Overlap 
0.5 
0.6 
0.7 
0.8 
0.9 

Ours 
78.2 72.2 65.8 59.4 49.6 
Category 
Ours OFL 
JFS BVS SCF AFS FST HBT LTV 

Aeroplane 88.2 89.9 89.0 86.8 86.3 79.9 70.9 73.6 13.7 
Bird 
85.7 84.2 81.6 80.9 81.0 78.4 70.6 56.1 12.2 
Boat 
77.5 74.0 74.2 65.1 68.6 60.1 42.5 57.8 10.8 
Car 
79.6 80.9 70.9 68.7 69.4 64.4 65.2 33.9 23.7 
Cat 
70.8 68.3 67.7 55.9 58.9 50.4 52.1 30.5 18.6 
Cow 
77.8 79.8 79.1 69.9 68.6 65.7 44.5 41.8 16.3 
Dog 
81.3 76.6 70.3 68.5 61.8 54.2 65.3 36.8 18.0 
Horse 
72.8 72.6 67.8 58.9 54.0 50.8 53.5 44.3 11.5 
Motorbike 73.5 73.7 61.5 60.5 60.9 58.3 44.2 48.9 10.6 
Train 
75.7 76.3 78.2 65.2 66.3 62.4 29.6 39.2 19.6 

Mean 
78.3 77.6 74.0 68.0 67.6 62.5 53.8 46.3 15.5 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Contour detection and hierarchical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">The fast bilateral solver</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">High-for-low and lowfor-high: Efficient boundary detection from deep object features and its applications to high-level vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Semantic segmentation with boundary neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Bertasius</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Object segmentation by long term analysis of point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A video representation using temporal superpixels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">W</forename><surname>Fisher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iii</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Semantic image segmentation with deep convolutional nets and fully connected CRFs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instance-sensitive fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">R-FCN: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Video segmentation by non-local consensus voting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Faktor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Jumpcut: Non-successive mask transfer and interpolation for video cutout</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cohen-Or</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Trans. Graph</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning hierarchical features for scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Najman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Video segmentation by tracing discontinuities in a trajectory embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Rich feature hierarchies for accurate object detection and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Hough-based tracking of non-rigid objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Godec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">M</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">CVIU</title>
		<imprint>
			<biblScope unit="volume">117</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="1245" to="1256" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient hierarchical graph-based video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Grundmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kwatra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">A</forename><surname>Essa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Hypercolumns for object segmentation and fine-grained localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Supervoxel-consistent foreground propagation in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Video propagation networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Jampani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gadde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning video object segmentation from static images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Pushing the boundaries of boundary detection using deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Key-segments for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">SSD: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">The visual object tracking VOT2015 challenge results</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kristan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Visual Object Tracking Workshop 2015 at ICCV 2015</title>
		<imprint>
			<date type="published" when="2015-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Convolutional oriented boundaries</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Deep retinal image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">The role of context for object detection and semantic segmentation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N.-G</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-W</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning multi-domain convolutional neural networks for visual tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Nam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Bilateral space video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">Nicolas</forename><surname>Märki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkinehornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning deconvolution network for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Noh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Han</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Segmentation of moving objects by long term video analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ochs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1187" to="1200" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Fast object segmentation in unconstrained video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Papazoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">A benchmark dataset and evaluation methodology for video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcwilliams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Fully connected object proposals for video segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perazzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sorkine-Hornung</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning to refine object segments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<monogr>
		<title level="m" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning object class detectors from weakly annotated video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prest</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Leistner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Civera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Seamseg: Video object segmentation using patch seams</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">A</forename><surname>Ramakanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">V</forename><surname>Babu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">U-Net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">MICCAI</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
		<respStmt>
			<orgName>ImageNet Large Scale Visual Recognition Challenge. IJCV</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Video segmentation with just a few strokes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Shankar Nagaraja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Schmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Saliency-Aware geodesic video object segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wenguan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Porikli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Causal video object segmentation from persistence of occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Karasev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Video segmentation via object flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Active frame selection for label propagation in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Holistically-nested edge detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Object contour detection with a fully convolutional encoder-decoder network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
