<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:39+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">NeuralFDR: Learning Discovery Thresholds from Hypothesis Features</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fei</forename><surname>Xia</surname></persName>
							<email>feixia@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><forename type="middle">J</forename><surname>Zhang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><surname>Zou</surname></persName>
							<email>jamesz@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Tse</surname></persName>
							<email>dntse@stanford.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Stanford University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">NeuralFDR: Learning Discovery Thresholds from Hypothesis Features</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>As datasets grow richer, an important challenge is to leverage the full features in the data to maximize the number of useful discoveries while controlling for false positives. We address this problem in the context of multiple hypotheses testing, where for each hypothesis, we observe a p-value along with a set of features specific to that hypothesis. For example, in genetic association studies, each hypothesis tests the correlation between a variant and the trait. We have a rich set of features for each variant (e.g. its location, conservation, epigenetics etc.) which could inform how likely the variant is to have a true association. However popular empirically-validated testing approaches, such as Benjamini-Hochberg's procedure (BH) and independent hypothesis weighting (IHW), either ignore these features or assume that the features are categorical or uni-variate. We propose a new algorithm, NeuralFDR, which automatically learns a discovery threshold as a function of all the hypothesis features. We parametrize the discovery threshold as a neural network, which enables flexible handling of multi-dimensional discrete and continuous features as well as efficient end-to-end optimization. We prove that NeuralFDR has strong false discovery rate (FDR) guarantees, and show that it makes substantially more discoveries in synthetic and real datasets. Moreover, we demonstrate that the learned discovery threshold is directly interpretable.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>In modern data science, the analyst is often swarmed with a large number of hypotheses -e.g. is a mutation associated with a certain trait or is this ad effective for that section of the users. Deciding which hypothesis to statistically accept or reject is a ubiquitous task. In standard multiple hypothesis testing, each hypothesis is boiled down to one number, a p-value computed against some null distribution, with a smaller value indicating less likely to be null. We have powerful procedures to systematically reject hypotheses while controlling the false discovery rate (FDR) Note that here the convention is that a "discovery" corresponds to a "rejected" null hypothesis.</p><p>These FDR procedures are widely used but they ignore additional information that is often available in modern applications. Each hypothesis, in addition to the p-value, could also contain a set of features pertinent to the objects being tested in the hypothesis. In the genetic association setting above, each hypothesis tests whether a mutation is correlated with the trait and we have a p-value for this. Moreover, we also have other features about both the mutation (e.g. its location, epigenetic status, conservation etc.) and the trait (e.g. if the trait is gene expression then we have features on the gene). Together these form a feature representation of the hypothesis. This feature vector is ignored by the standard multiple hypotheses testing procedures.</p><p>In this paper, we present a flexible method using neural networks to learn a nonlinear mapping from hypothesis features to a discovery threshold. Popular procedures for multiple hypotheses testing correspond to having one constant threshold for all the hypotheses (BH <ref type="bibr" target="#b2">[3]</ref>), or a constant for each group of hypotheses (group BH <ref type="bibr" target="#b12">[13]</ref>, IHW <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b14">15]</ref>). Our algorithm takes account of all the features to automatically learn different thresholds for different hypotheses. Our deep learning architecture enables efficient optimization and gracefully handles both continuous and discrete multidimensional hypothesis features. Our theoretical analysis shows that we can control false discovery proportion (FDP) with high probability. We provide extensive simulation on synthetic and real datasets to demonstrate that our algorithm makes more discoveries while controlling FDR compared to state-of-the-art methods.</p><p>Contribution. As shown in <ref type="figure" target="#fig_0">Fig. 1</ref>, we provide NeuralFDR, a practical end-to-end algorithm to the multiple hypotheses testing problem where the hypothesis features can be continuous and multi-dimensional. In contrast, the currently widely-used algorithms either ignore the hypothesis features (BH <ref type="bibr" target="#b2">[3]</ref>, Storey's BH <ref type="bibr" target="#b20">[21]</ref>) or are designed for simple discrete features (group BH <ref type="bibr" target="#b12">[13]</ref>, IHW <ref type="bibr" target="#b14">[15]</ref>). Our algorithm has several innovative features. We learn a multi-layer perceptron as the discovery threshold and use a mirroring technique to robustly estimate false discoveries. We show that NeuralFDR controls false discovery with high probability for independent hypotheses and asymptotically under weak dependence <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>, and we demonstrate on both synthetic and real datasets that it controls FDR while making substantially more discoveries. Another advantage of our end-to-end approach is that the learned discovery threshold are directly interpretable. We will illustrate in Sec. 4 how the threshold conveys biological insights.</p><p>Related works. Holm <ref type="bibr" target="#b11">[12]</ref> investigated the use of p-value weights, where a larger weight suggests that the hypothesis is more likely to be an alternative. Benjamini and Hochberg <ref type="bibr" target="#b3">[4]</ref> considered assigning different losses to different hypotheses according to their importance. Some more recent works are <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b12">13]</ref>. In these works, the features are assumed to have some specific forms, either prespecified weights for each hypothesis or the grouping information. The more general formulation considered in this paper was purposed quite recently <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b18">19]</ref>. It assumes that for each hypothesis, we observe not only a p-value P i but also a feature X i lying in some generic space X . The feature is meant to capture some side information that might bear on the likelihood of a hypothesis to be significant, or on the power of P i under the alternative, but the nature of this relationship is not fully known ahead of time and must be learned from the data.</p><p>The recent work most relevant to ours is IHW <ref type="bibr" target="#b14">[15]</ref>. In IHW, the data is grouped into G groups based on the features and the decision threshold is a constant for each group. IHW is similar to NeuralFDR in that both methods optimize the parameters of the decision rule to increase the number of discoveries while using cross validation for asymptotic FDR control. IHW has several limitations: first, binning the data into G groups can be difficult if the feature space X is multi-dimensional; second, the decision rule, restricted to be a constant for each group, is artificial for continuous features; and third, the asymptotic FDR control guarantee requires the number of groups going to infinity, which can be unrealistic. In contrast, NeuralFDR uses a neural network to parametrize the decision rule which is much more general and fits the continuous features. As demonstrated in the empirical results, it works well with multi-dimensional features. In addition to asymptotic FDR control, NeuralFDR also has high-probability false discovery proportion control guarantee with a finite number of hypotheses.</p><p>SABHA <ref type="bibr" target="#b18">[19]</ref> and AdaPT <ref type="bibr" target="#b15">[16]</ref> are two recent FDR control frameworks that allow flexible methods to explore the data and compute the feature dependent decision rules. The focus there is the framework rather than the end-to-end algorithm as compared to NueralFDR. For the empirical experiment, SABHA estimates the null proportion using non-parametric methods while AdaPT estimates the distribution of the p-value and the features with a two-group Gamma GLM mixture model and spline regression. The multi-dimensional case is discussed without empirical validation. Hence both methods have a similar limitation to IHW in that they do not provide an empirically validated end-to-end approach for multi-dimensional features. This issue is addressed in <ref type="bibr" target="#b4">[5]</ref>, where the null proportion is modeled as a linear combination of some hand-crafted transformation of the features. NeuralFDR models this relation in a more flexible way.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Preliminaries</head><p>We have n hypotheses and each hypothesis i is characterized by a tuple (P i , X i , H i ), where P i 2 (0, 1) is the p-value, X i 2 X is the hypothesis feature, and H i 2 {0, 1} indicates if this hypothesis is null ( H i = 0) or alternative ( H i = 1). The p-value P i represents the probability of observing an equally or more extreme value compared to the testing statistic when the hypothesis is null, and is calculated based on some data different from X i . The alternate hypotheses (H i = 1) are the true signals that we would like to discover. A smaller p-value presents stronger evidence for a hypothesis to be alternative. In practice, we observe P i and X i but do not know H i . We define the null proportion ⇡ 0 (x) to be the probability that the hypothesis is null conditional on the feature X i = x. The standard assumption is that under the null (H i = 0), the p-value is uniformly distributed in (0, 1). Under the alternative (H i = 1), we denote the p-value distribution by f 1 (p|x). In most applications, the p-values under the alternative are systematically smaller than those under the null. A detailed discussion of the assumptions can be found in Sec. 5.</p><p>The general goal of multiple hypotheses testing is to claim a maximum number of discoveries based on the observations {(P i , X i )} n i=1 while controlling the false positives. The most popular quantities that conceptualize the false positives are the family-wise error rate (FWER) <ref type="bibr" target="#b7">[8]</ref> and the false discovery rate (FDR) <ref type="bibr" target="#b2">[3]</ref>. We specifically consider FDR in this paper. FDR is the expected proportion of false discoveries, and one closely related quantity, the false discovery proportion (FDP), is the actual proportion of false discoveries. We note that FDP is the actual realization of FDR. Formally, Definition 1. (FDP and FDR) For any decision rule t, let D(t) and F D(t) be the number of discoveries and the number of false discoveries. The false discovery proportion F DP (t) and the false discovery rate F DR(t) are defined as F DP (t) , F D(t)/D(t) and F DR(t) , E[F DP (t)].</p><p>In this paper, we aim to maximize D(t) while controlling F DP (t)  ↵ with high probability. This is a stronger statement than those in FDR control literature of controlling FDR under the level ↵.</p><p>Motivating example. Consider a genetic association study where the genotype and phenotype (e.g. height) are measured in a population. Hypothesis i corresponds to testing the correlation between the variant i and the individual's height. The null hypothesis is that there is no correlation, and P i is the probability of observing equally or more extreme values than the empirically observed correlation conditional on the hypothesis is null H i = 0. Small P i indicates that the null is unlikely. Here H i = 1 (or 0) corresponds to the variant truly is (or is not) associated with height. The features X i could include the location, conservation, etc. of the variant. Note that X i is not used to compute P i , but it could contain information about how likely the hypotheses is to be an alternative. Careful readers may notice that the distribution of P i given X i is uniform between 0 and 1 under the null and f 1 (p|x) under the alternative, which depends on x. This implies that P i and X i are independent under the null and dependent under the alternative.</p><p>To illustrate why modeling the features could improve discovery power, suppose hypothetically that all the variants truly associated with height reside on a single chromosome j ⇤ and the feature is the chromosome index of each SNP (see <ref type="figure" target="#fig_1">Fig. 2 (a)</ref>). Standard multiple testing methods ignore this feature and assign the same discovery threshold to all the chromosomes. As there are many purely noisy chromosomes, the p-value threshold must be very small in order to control FDR. In contrast, a method that learns the threshold t(x) could learn to assign a higher threshold to chromosome j ⇤ and 0 to other chromosomes. As a higher threshold leads to more discoveries and vice versa, this would effectively ignore much of the noise and make more discoveries under the same FDR.</p><formula xml:id="formula_0">(a) Train CV Test t * (x; θ) γ * t * (x; θ) Optimize (3) Rescale</formula><p>Mirroring estimator</p><formula xml:id="formula_1">D(t) d F D(t) Covariate X p (b) Train CV Test t * (x; θ) γ * t * (x; θ)</formula><p>Optimize <ref type="formula">(</ref> distribution f 1 (p|x) vary with x, the threshold should also depend on x. Therefore, we can write the rule as t(x) in general, which claims hypothesis i to be significant if P i &lt; t(X i ). Let I be the indicator function. For t(x), the number of discoveries D(t) and the number of false discoveries <ref type="bibr">Hi=0}</ref> . Note that computing F D(t) requires the knowledge of H i , which is not available from the observations. Ideally we want to solve t for the following problem:</p><formula xml:id="formula_2">F D(t) can be expressed as D(t) = P n i=1 I {Pi&lt;t(Xi)} and F D(t) = P n i=1 I {Pi&lt;t(Xi),</formula><formula xml:id="formula_3">maximize t D(t), s.t. FDP(t)  ↵.<label>(1)</label></formula><p>Directly solving <ref type="formula" target="#formula_3">(1)</ref> is not possible. First, without a parametric representation, t can not be optimized. Second, while D(t) can be calculated from the data, F D(t) can not, which is needed for evaluating F DP (t). Third, while each decision rule candidate t j controls FDP, optimizing over them may yield a rule that overfits the data and loses FDP control. We next address these three difficulties in order.</p><p>First, the representation of the decision rule t(x) should be flexible enough to address different structures of the data. Intuitively, to have maximal discoveries, the landscape of t(x) should be similar to that of the alternative proportion ⇡ 1 (x): t(x) is large in places where the alternative hypotheses abound. As discussed in detail in Sec. 4, two structures of ⇡ 1 (x) are typical in practice. The first is bumps at a few locations, and the second is slopes that vary with x. Hence the representation should at least be able to address these two structures. In addition, the number of parameters needed for the representation should not grow exponentially with the dimensionality of x. Hence non-parametric models, such as the spline-based methods or the kernel based methods, are infeasible. Take kernel density estimation in 5D as example. If we let the kernel width be 0.1, each kernel contains on average 0.001% of the data. Then we need at least a million alternative hypothesis data to have a reasonable estimate of the landscape of ⇡ 1 (x). In this work, we investigate the idea of modeling t(x) using a multilayer perceptron (MLP), which has a high expressive power and has a number of parameters that does not grow exponentially with the dimensionality of the features. As demonstrated in Sec. 4, it can efficiently recover the two common structures, bumps and slopes, and yield promising results in all real data experiments.</p><p>Second, although F D(t) can not be calculated from the data, if it can be overestimated by some</p><formula xml:id="formula_4">d F D(t), then the corresponding estimate of FDP, namely \ F DP (t) = d F D(t)/D(t), is also an overestimate. Then if \ F DP (t)  ↵, then F DP (t)  ↵, yielding the desired FDP control. Moreover, if d F D(t) is close to F D(t)</formula><p>, the FDP control is tight. Conditional on X = x, the rejection region of p, namely (0, t(x)), contains a mixture of nulls and alternatives. As the null distribution Unif(0, 1) is symmetrical w.r.t. p = 0.5 while the alternative distribution f 1 (p|x) is highly asymmetrical, the mirrored region (1 t(x), 1) will contain roughly the same number of nulls but very few alternatives. Then the number of hypothesis in (t(x), 1) can be a proxy of the number of nulls in (0, t(x)). This idea is illustrated in <ref type="figure" target="#fig_1">Fig. 2 (b)</ref> and we refer to this estimator as the mirroring estimator. This estimator is also used in <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17]</ref>. Definition 2. (The mirroring estimator) For any decision rule t, let C(t) = {(p, x) : p &lt; t(x)} be the rejection region of t over (P i , X i ) and let its mirrored region be</p><formula xml:id="formula_5">C M (t) = {(p, x) : p &gt; 1 t(x)}.The mirroring estimator of F D(t) is defined as d F D(t) = P i I {(Pi,Xi)2C M (t)} .</formula><p>The mirroring estimator overestimates the number of false discoveries in expectation: Lemma 1. (Positive bias of the mirroring estimator)</p><formula xml:id="formula_6">E[ d F D(t)] E[F D(t)] = n X i=1 P ⇥ (P i , X i ) 2 C M (t), H i = 1 ⇤ 0.<label>(2)</label></formula><p>Remark 1. In practice, t(x) is always very small and f 1 (p|x) approaches 0 very fast as p ! 1.</p><p>Then for any hypothesis with (P i , X i ) 2 C M (t), P i is very close to 1 and hence P(H i = 1) is very small. In other words, the bias in (2) is much smaller than E[F D(t)]. Thus the estimator is accurate. In addition, d</p><p>F D(t) and F D(t) are both sums of n terms. Under mild conditions, they concentrate well around their means. Thus we should expect that d F D(t) approximates F D(t) well most of the times. We make this precise in Sec. 5 in the form of the high probability FDP control statement.</p><p>Third, we use cross validation to address the overfitting problem introduced by optimization. To be more specific, we divide the data into M folds. For fold j, the decision rule t j (x; ✓), before applied on fold j, is trained and cross validated on the rest of the data. The cross validation is done by rescaling the learned threshold t j (x) by a factor j so that the corresponding mirror estimate \ F DP on the CV set is ↵. This will not introduce much of additional overfitting since we are only searching over a scalar . The discoveries in all M folds are merged as the final result. We note here distinct folds correspond to subsets of hypotheses rather than samples used to compute the corresponding p-values. This procedure is shown in <ref type="figure" target="#fig_1">Fig. 2 (c)</ref>. The details of the procedure as well as the FDP control property are also presented in Sec. 5.</p><formula xml:id="formula_7">Algorithm 1 NeuralFDR 1: Randomly divide the data {(P i , X i )} n i=1 into M folds. 2: for fold j = 1, · · · , M do 3:</formula><p>Let the testing data be fold j, the CV data be fold j 0 6 = j, and the training data be the rest.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Train t j (x; ✓) based on the training data by optimizing</p><formula xml:id="formula_8">maximize ✓ D(t(✓)) s.t. \ F DP (t ⇤ j (✓))  ↵.<label>(3)</label></formula><p>5:</p><p>Rescale t  The proposed method NeuralFDR is summarized as Alg. 1. There are two techniques that enabled robust training of the neural network. First, to have non-vanishing gradients, the indicator functions in (3) are substituted by sigmoid functions with the intensity parameters automatically chosen based on the dataset. Second, the training process of the neural network may be unstable if we use random initialization. Hence, we use an initialization method called the k-cluster initialization: 1) use k-means clustering to divide the data into k clusters based on the features; 2) compute the optimal threshold for each cluster based on the optimal group threshold condition ( <ref type="formula" target="#formula_13">(7)</ref> in Sec. 5); 3) initialize the neural network by training it to fit a smoothed version of the computed thresholds. See Supp. Sec. 2 for more implementation details.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Empirical Results</head><p>We evaluate our method using both simulated data and two real-world datasets <ref type="bibr" target="#b2">3</ref> . The implementation details are in Supp. Sec. 2. We compare NeuralFDR with three other methods: BH procedure (BH) <ref type="bibr" target="#b2">[3]</ref>, Storey's BH procedure (SBH) with threshold = 0.4 <ref type="bibr" target="#b20">[21]</ref>, and Independent Hypothesis Weighting (IHW) with number of bins and folds set as default <ref type="bibr" target="#b14">[15]</ref>. BH and SBH are two most popular methods without using the hypothesis features and IHW is the state-of-the-art method that utilizes hypothesis features. For IHW, in the multi-dimensional feature case, k-means is used to group the hypotheses. In all experiments, k is set to 20 and the group index is provided to IHW as the hypothesis feature. Other than the FDR control experiment, we set the nominal FDR level ↵ = 0.1.   <ref type="bibr" target="#b14">[15]</ref>). Then, we use our own data that are generated to have two feature structures commonly seen in practice, the bumps and the slopes. For the bumps, the alternative proportion ⇡ 1 (x) is generated from a Gaussian mixture (GM) to have a few peaks with abundant alternative hypotheses. For the slopes, ⇡ 1 (x) is generated linearly dependent with the features. After generating ⇡ 1 (x), the p-values are generated following a beta mixture under the alternative and uniform (0, 1) under the null. We generated the data for both 1D and 2D cases, namely 1DGM, 2DGM, 1Dslope, 2Dslope. For example, <ref type="figure" target="#fig_6">Fig. 4 (a)</ref> shows the alternative proportion of 2Dslope. In addition, for the high dimensional feature scenario, we generated a 5D data, 5DGM, which contains the same alternative proportion as 2DGM with 3 addition non-informative directions.</p><p>We first examine the FDR control property using DataIHW and 1DGM. Knowing the ground truth, we plot the FDP (actual FDR) over different values of the nominal FDR ↵ in <ref type="figure" target="#fig_4">Fig. 3</ref>. For a perfect FDR control, the curve should be along the 45-degree dashed line. As we can see, all the methods control FDR. NeuralFDR controls FDR accurately while IHW tends to make overly conservative decisions. Second, we visualize the learned threshold by both NeuralFDR and IWH. As mentioned in Sec. 3, to make more discoveries, the learned threshold should roughly have the same shape as ⇡ 1 (x). The learned thresholds of NeuralFDR and IHW for 2Dslope are shown in <ref type="figure" target="#fig_4">Fig. 3 (b,c)</ref>. As we can see, NeuralFDR well recovers the slope structure while IHW fails to assign the highest threshold to the bottom right block. IHW is forced to be piecewise constant while NeuralFDR can learn a smooth threshold, better recovering the structure of ⇡ 1 (x). In general, methods that partition the hypotheses into discrete groups would not scale for higher-dimensional features. In Appendix 1, we show that NeuralFDR is also able to recover the correct threshold for the Gaussian signal. Finally, we report the total numbers of discoveries in Tab. 1.</p><p>In addition, we ran an experiment with dependent p-values with the same dependency structure as Sec. 3.2 in <ref type="bibr" target="#b14">[15]</ref>. We call this dataset DataIHW(WD). The number of discoveries are shown in Tab. 1. NeuralFDR has the actual FDP 9.7% while making more discoveries than SBH and IHW. This empirically shows that NeuralFDR also works for weakly dependent data.</p><p>All numbers are averaged over 10 runs of the same simulation setting. We can see that NeuralFDR outperforms IHW in all simulated datasets. Moreover, it outperforms IHW by a large margin multi-dimensional feature settings.  Each dot corresponds to one hypothesis. The red curves shows the learned threshold by NeuralFDR: (d) for log count for airway data; (e) for log distance for GTEx data; (f) for expression level for GTEx data. Airway RNA-Seq data. Airway data <ref type="bibr" target="#b10">[11]</ref> is a RNA-Seq dataset that contains n = 33469 genes and aims to identify glucocorticoid responsive (GC) genes that modulate cytokine function in airway smooth muscle cells. The p-values are obtained by a standard two-group differential analysis using DESeq2 <ref type="bibr" target="#b19">[20]</ref>. We consider the log count for each gene as the hypothesis feature. As shown in the first column in Tab. 2, NeuralFDR makes 800 more discoveries than IHW. The learned threshold by NeuralFDR is shown in <ref type="figure" target="#fig_6">Fig. 4 (d)</ref>. It increases monotonically with the log count, capturing the positive dependency relation. Such learned structure is interpretable: low count genes tend to have higher variances, usually dominating the systematic difference between the two conditions; on the contrary, it is easier for high counts genes to show a strong signal for differential expression <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>GTEx data. A major component of the GTEx <ref type="bibr" target="#b5">[6]</ref> study is to quantify expression quantitative trait loci (eQTLs) in human tissues. In such an eQTL analysis, each pair of single nucleotide polymorphism (SNP) and nearby gene forms one hypothesis. Its p-value is computed under the null hypothesis that the SNP's genotype is not correlated with the gene expression.We obtained all the GTEx p-values from chromosome 1 in a brain tissue (interior caudate), corresponding to 10, 623, 893 SNP-gene combinations. In the original GTEx eQTL study, no features were considered in the FDR analysis, corresponding to running the standard BH or SBH on the p-values. However, we know many biological features affect whether a SNP is likely to be a true eQTL; i.e. these features could vary the alternative proportion ⇡ 1 (x) and accounting for them could increase the power to discover true eQTL's while guaranteeing that the FDR remains the same. For each hypothesis, we generated three features: 1) the distance (GTEx-dist) between the SNP and the gene (measured in log base-pairs) ; 2) the average expression (GTEx-exp) of the gene across individuals (measured in log rpkm); 3) the evolutionary conservation measured by the standard PhastCons scores (GTEx-PhastCons).</p><p>The numbers of discoveries are shown in Tab. 2. For GTEx-2D, GTEx-dist and GTEx-exp are used. For NeuralFDR, the number of discoveries increases as we put in more and more features, indicating that it can work well with multi-dimensional features. For IHW, however, the number of discoveries decreases as more features are incorporated. This is because when the feature dimension becomes higher, each bin in IHW will cover a larger space, decreasing the resolution of the piecewise constant function, preventing it from capturing the informative part of the feature.</p><p>The learned discovery thresholds of NeuralFDR are directly interpretable and match prior biological knowledge. <ref type="figure" target="#fig_6">Fig. 4 (e)</ref> shows that the threshold is higher when SNP is closer to the gene. This allows more discoveries to be made among nearby SNPs, which is desirable since we know there most of the eQTLs tend to be in cis (i.e. nearby) rather than trans (far away) from the target gene <ref type="bibr" target="#b5">[6]</ref>. <ref type="figure" target="#fig_6">Fig. 4 (f)</ref> shows that the NeuralFDR threshold for gene expression decreases as the gene expression becomes large. This also confirms known biology: the highly expressed genes tend to be more housekeeping genes which are less variable across individuals and hence have fewer eQTLs <ref type="bibr" target="#b5">[6]</ref>. Therefore it is desirable that NeuralFDR learns to place less emphasis on these genes. We also show that NeuralFDR learns to give higher threshold to more conserved variants in Supp. Sec. 1, which also matches biology.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Theoretical Guarantees</head><p>We assume the tuples {(</p><formula xml:id="formula_9">P i , X i , H i )} n i=1 are i.i.d.</formula><p>samples from an empirical Bayes model:</p><formula xml:id="formula_10">X i i.i.d. ⇠ µ(X), [H i |X i = x] ⇠ Bern(1 ⇡ 0 (x)), ⇢ [P i |H i = 0, X = x] ⇠ Unif(0, 1) [P i |H i = 1, X = x] ⇠ f 1 (p|x)<label>(4)</label></formula><p>The features X i are drawn i.i.d. from some unknown distribution µ(x). Conditional on the feature X i = x, hypothesis i is null with probability ⇡ 0 (x) and is alternative otherwise. The conditional distributions of p-values are Unif(0, 1) under the null and f 1 (p|x) under the alternative.</p><p>FDR control via cross validation. The cross validation procedure is described as follows. The data is divided randomly into M folds of equal size m = n/M . For fold j, let the testing set D te (j) be itself, the cross validation set D cv (j) be any other fold, and the training set D tr (j) be the remaining. The size of the three are m, m, (M 2)m respectively. For fold j, suppose at most L decision rules are calculated based on the training set, namely t j1 , · · · , t jL . Evaluated on the cross validation set, let l ⇤ -th rule be the rule with most discoveries among rules that satisfies 1) its mirroring estimate \ F DP (t jl )  ↵; 2) D(t jl )/m &gt; c 0 , for some small constant c 0 &gt; 0. Then, t jl ⇤ is selected to apply on the testing set (fold j). Finally, discoveries from all folds are combined.</p><p>The FDP control follows a standard argument of cross validation. Intuitively, the FDP of the rules {t jl } L l=1 are estimated based on D cv (j), a dataset independent of the training set. Hence there is no overfitting and the overestimation property of the mirroring estimator, as in Lemma 1, is statistical valid, leading to a conservative decision that controls FDP. This is formally stated as below.   <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b20">21]</ref>. Generalizing the concept in <ref type="bibr" target="#b12">[13]</ref> from discrete groups to continuous features X, the data are under weak dependence if the CDF of (P i , X i ) for both the null and the alternative proportion converge almost surely to their true values respectively. The linkage disequilibrium (LD) in GWAS and the correlated genes in RNA-Seq can be addressed by such dependence structure. In this case, if learned threshold is c-Lipschitz continuous for some constant c, NeuralFDR will control FDR asymptotically. The Lipschitz continuity can be achieved, for example, by weight clipping <ref type="bibr" target="#b1">[2]</ref>, i.e. clamping the weights to a bounded set after each gradient update when training the neural network. See Supp. 3 for details.</p><p>Optimal decision rule with infinite hypotheses. When n = 1, we can recover the joint density f P X (p, x). Based on that, the explicit form of the optimal decision rule can be obtained if we are willing to further assumer f 1 (p|x) is monotonically non-increasing w.r.t. p. This rule is used for the k-cluster initialization for NeuralFDR as mentioned in Sec. 3. Now suppose we know f P X (p, x). Then µ(x) and f P |X (p|x) can also be determined. Furthermore, as f 1 (p|x) = 1 1 ⇡0(x) (f P |X (p|x) ⇡ 0 (x)), once we specify ⇡ 0 (x), the entire model is specified. Let S(f P X ) be the set of null proportions ⇡ 0 (x) that produces the model consistent with f P X . Because f 1 (p|x) 0, we have 8p, x, ⇡ 0 (x)  f P |X (p|x). This can be further simplified as ⇡ 0 (x)  f P |X (1|x) by recalling that f P |X (p|x) is monotonically decreasing w.r.t. p. Then we know S(f P X ) = {⇡ 0 (x) : 8x, ⇡ 0 (x)  f P |X (1|x)}.</p><p>Given f P X (p, x), the model is not fully identifiable. Hence we should look for a rule t that maximizes the power while controlling FDP for all elements in S(f P X ). For (P 1 , X 1 , H 1 ) ⇠ (f P X , ⇡ 0 , f 1 ) following (4), the probability of discovery and the probability of false discovery are P D (t, f P X ) = P(P 1  t(X 1 )), P F D (t, f P X , ⇡ 0 ) = P(P 1  t(X 1 ), H 1 = 0). Then the FDP is F DP (t, f P X , ⇡ 0 ) = P F D (t,f P X ,⇡0) P D (t,f P X ) . In this limiting case, all quantities are deterministic and FDP coincides with FDR. Given that the FDP is controlled, maximizing the power is equivalent to maximizing the probability of discovery. Then we have the following minimax problem:</p><formula xml:id="formula_12">max t min ⇡02S(f P X ) P D (t, f P X ) s.t. max ⇡02S(f P X ) F DP (t, f P X , ⇡ 0 )  ↵,<label>(6)</label></formula><p>where S(f P X ) is the set of possible null proportions consistent with f P X , as defined in <ref type="formula" target="#formula_11">(5)</ref>. Theorem 2. Fixing f P X and let ⇡ ⇤ 0 (x) = f P |X (1|x). If f 1 (p|x) is monotonically non-increasing w.r.t. p, the solution to problem (6), t ⇤ (x), satisfies</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1.</head><p>f P X (1, x) f P X (t ⇤ (x), x) = const, almost surely w.r.t. µ(x) 2. FDR(t ⇤ , f P X , ⇡ ⇤ 0 ) = ↵.</p><p>Remark 4. To compute the optimal rule t ⇤ by the conditions (7), consider any t that satisfies (7.1). According to (7.1), once we specify the value of t(x) at any location x, say t(0), the entire function is determined. Also, F DP (t, f P X , ⇡ ⇤ 0 ) is monotonically non-decreasing w.r.t. t(0). These suggests the following strategy: starting with t(0) = 0, keep increasing t(0) until the corresponding FDP equals ↵, which gives us the optimal threshold t ⇤ . Similar conditions are also mentioned in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b15">16]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: NeuralFDR: an end-to-end learning procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: (a) Hypothetical example where small p-values are enriched at chromosome j ⇤ . (b) The mirroring estimator. (c) The training and cross validation procedure.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>j</head><label></label><figDesc>(✓) on the data in fold j (the testing data). 7: Report the discoveries in all M folds.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FDP for (a) DataIHW and (b) 1DGM. Dashed line indicate 45 degrees, which is optimal.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>NeuralFDR's learned thresh- old for GTEx expression level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: (a-c) Results for 2Dslope: (a) the alternative proportion for 2Dslope; (b) NeuralFDR's learned threshold; (c) IHW's learned threshold. (d-f): Each dot corresponds to one hypothesis. The red curves shows the learned threshold by NeuralFDR: (d) for log count for airway data; (e) for log distance for GTEx data; (f) for expression level for GTEx data.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 1 .</head><label>1</label><figDesc>(FDP control) Let M be the number of folds and let L be the maximum number of decision rule candidates evaluated by the cross validation set. Then with probability at least 1 , the overall FDP is less than (1 + )↵, where = O ⇣q M ↵n log ML ⌘ .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Simulated data: # of discoveries and gain over BH at FDR = 0.1.+33.8%) 18844(+90.0%) 10318(+21.7%) 18364(+85.1%) Simulated data. We first consider DataIHW, the simulated data in the IHW paper ( Supp. 7.2.2</figDesc><table>DataIHW 
DataIHW(WD) 1D GM 
BH 
2259 
6674 
8266 
SBH 
2651(+17.3%) 
7844(+17.5%) 
9227(+11.62%) 
IHW 
5074(+124.6%) 10382(+55.6%) 11172(+35.2%) 
NeuralFDR 6222(+175.4%) 12153(+82.1%) 14899(+80.2%) 
1D slope 
2D GM 
2D slope 
5D GM 
BH 
11794 
9917 
8473 
9917 
SBH 
13593(+15.3%) 11334(+14.2%) 9539(+12.58%) 11334(+14.28%) 
IHW 
12658(+7.3%) 
12175(+22.7%) 8758(+3.36%) 
11408(+15.0%) 
NeuralFDR 15781(</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Real data: # of discoveries at FDR = 0.1.</figDesc><table>Airway 
GTEx-dist 
GTEx-exp 
BH 
4079 
29348 
29348 
SBH 
4038(-1.0%) 
29758(+1.4%) 
29758(+1.4%) 
IHW 
4873(+19.5%) 35771(+21.9%) 32195(+9.7%) 
NeuralFDR 6031(+47.9%) 36127(+23.1%) 32214(+9.8%) 
GTEx-PhastCons GTEx-2D 
GTEx-3D 
BH 
29348 
29348 
29348 
SBH 
29758(+1.4%) 
29758(+1.4%) 
29758(+1.4%) 
IHW 
30241(+3.0%) 
35705(+21.7%) 35598(+21.3%) 
NeuralFDR 30525(+4.0%) 
37095(+26.4%) 37195(+26.7%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Remark 2. There are two subtle points. First, L can not be too large. Otherwise D cv (j) may eventually be overfitted by being used too many times for FDP estimation. Second, the FDP estimates may be unstable if the probability of discovery E[D(t jl )/m] approaches 0. Indeed, the mirroring method estimates FDP by \ F DP (t jl ) =D(t jl ) , where both d F D(t jl ) and D(t jl ) are i.i.d. sums of n Bernoulli random variables with mean roughly ↵E[D(t jl )/m] and E[D(t jl )/m]. When their means are small, the concentration property will fail. So we need E[D(t jl )/m] to be bounded away from zero. Nevertheless this is required in theory but may not be used in practice. Remark 3. (Asymptotic FDR control under weak dependence) Besides the i.i.d. case, NeuralFDR can also be extended to control FDR asymptotically under weak dependence</figDesc><table>d 
F D(t jl ) 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Algorithm Description Since a smaller p-value presents stronger evidence against the null hypothesis, we consider the threshold decision rule without loss of generality. As the null proportion ⇡ 0 (x) and the alternative</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We released the software at https://github.com/fxia22/NeuralFDR</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Discussion</head><p>We proposed NeuralFDR, an end-to-end algorithm to the learn discovery threshold from hypothesis features. We showed that the algorithm controls FDR and makes more discoveries on synthetic and real datasets with multi-dimensional features. While the results are promising, there are also a few challenges. First, we notice that NeuralFDR performs better when both the number of hypotheses and the alternative proportion are large. Indeed, in order to have large gradients for the optimization, we need a lot of elements at the decision boundary t(x) and the mirroring boundary 1 t(x). It is important to improve the performance of NeuralFDR on small datasets with small alternative proportion. Second, we found that a 10-layer MLP performed well to model the decision threshold and that shallower networks performed more poorly. A better understanding of which network architectures optimally capture signal in the data is also an important question.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Distribution-free multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ery</forename><surname>Arias-Castro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shiyun</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Electronic Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="1983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Arjovsky</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<title level="m">Soumith Chintala, and Léon Bottou. Wasserstein gan</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Controlling the false discovery rate: a practical and powerful approach to multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the royal statistical society. Series B (Methodological)</title>
		<imprint>
			<biblScope unit="page" from="289" to="300" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multiple hypotheses testing with weights</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Benjamini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yosef</forename><surname>Hochberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian Journal of Statistics</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="407" to="418" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">A regression framework for the proportion of true null hypotheses. bioRxiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeffrey</forename><forename type="middle">T</forename><surname>Boca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Leek</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page">35675</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">The genotype-tissue expression (gtex) pilot analysis: Multitissue gene regulation in humans</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gtex</forename><surname>Consortium</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">348</biblScope>
			<biblScope unit="issue">6235</biblScope>
			<biblScope unit="page" from="648" to="660" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Adaptive subgradient methods for online learning and stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Duchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Elad</forename><surname>Hazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoram</forename><surname>Singer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="2121" to="2159" />
			<date type="published" when="2011-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multiple comparisons among means</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Olive Jean Dunn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">293</biblScope>
			<biblScope unit="page" from="52" to="64" />
			<date type="published" when="1961" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Simultaneous inference: When should hypothesis testing problems be combined? The annals of applied statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bradley</forename><surname>Efron</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="197" to="223" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">False discovery control with p-value weighting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kathryn</forename><surname>Christopher R Genovese</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Larry</forename><surname>Roeder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wasserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Biometrika</title>
		<imprint>
			<biblScope unit="page" from="509" to="524" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rna-seq transcriptome profiling identifies crispld2 as a glucocorticoid responsive gene that modulates cytokine function in airway smooth muscle cells</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Blanca</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaofeng</forename><surname>Himes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peter</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruoxi</forename><surname>Wagner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Barbara</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Klanderman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Reid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingling</forename><surname>Whitaker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jessica</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christina</forename><surname>Lasky-Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nikolos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">PloS one</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">99625</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A simple sequentially rejective multiple test procedure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sture</forename><surname>Holm</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Scandinavian journal of statistics</title>
		<imprint>
			<biblScope unit="page" from="65" to="70" />
			<date type="published" when="1979" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">False discovery rate control with groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hongyu</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Harrison H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the American Statistical Association</title>
		<imprint>
			<biblScope unit="volume">105</biblScope>
			<biblScope unit="issue">491</biblScope>
			<biblScope unit="page" from="1215" to="1227" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Covariate-powered weighted multiple testing with false discovery rate control</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Ignatiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Huber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.05179</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Data-driven hypothesis weighting increases detection power in genome-scale multiple testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikolaos</forename><surname>Ignatiadis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernd</forename><surname>Klaus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Judith</forename><forename type="middle">B</forename><surname>Zaugg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Huber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature methods</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="577" to="580" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adapt: An interactive procedure for multiple testing with side information</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fithian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06035</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Power of ordered hypothesis testing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fithian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2924" to="2932" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Star: A general interactive framework for fdr control under structural constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lihua</forename><surname>Lei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaditya</forename><surname>Ramdas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><surname>Fithian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.02776</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ang</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rina</forename><forename type="middle">Foygel</forename><surname>Barber</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.07926</idno>
		<title level="m">Multiple testing with the structure adaptive benjamini-hochberg algorithm</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Moderated estimation of fold change and dispersion for rna-seq data with deseq2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wolfgang</forename><surname>Michael I Love</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Huber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Genome biology</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page">550</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Strong control, conservative point estimation and simultaneous conservative consistency of false discovery rates: a unified approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>John</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><forename type="middle">E</forename><surname>Storey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Siegmund</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of the Royal Statistical Society: Series B (Statistical Methodology)</title>
		<imprint>
			<biblScope unit="volume">66</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="187" to="205" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
