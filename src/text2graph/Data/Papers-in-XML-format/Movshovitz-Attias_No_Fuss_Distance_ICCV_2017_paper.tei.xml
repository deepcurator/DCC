<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:30+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">No Fuss Distance Metric Learning using Proxies</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yair</forename><surname>Movshovitz-Attias</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Toshev</surname></persName>
							<email>toshev@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
							<email>leungt@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
							<email>sioffe@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Saurabh</forename><surname>Singh</surname></persName>
							<email>saurabhsingh@google.com</email>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Google</forename><surname>Research</surname></persName>
						</author>
						<title level="a" type="main">No Fuss Distance Metric Learning using Proxies</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We address the problem of distance metric learning (DML) </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Distance metric learning (DML) is a major tool for a variety of problems in computer vision. It has successfully been employed for image retrieval <ref type="bibr" target="#b13">[14]</ref>, near duplicate detection <ref type="bibr" target="#b18">[19]</ref>, clustering <ref type="bibr" target="#b3">[4]</ref> and zero-shot learning <ref type="bibr" target="#b7">[8]</ref>.</p><p>A wide variety of formulations have been proposed. Traditionally, these formulations encode a notion of similar and dissimilar data points. For example, contrastive loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>, which is defined for a pair of either similar or dissimilar data points. Another commonly used family of losses is triplet loss, which is defined by a triplet of data points: an anchor point, and a similar and dissimilar data points. The goal in </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R@1</head><p>Triplet <ref type="bibr" target="#b11">[12]</ref> Lifted Struct <ref type="bibr" target="#b7">[8]</ref> Npairs <ref type="bibr" target="#b13">[14]</ref> Struct Clust <ref type="bibr" target="#b14">[15]</ref> Proxy-NCA a triplet loss is to learn a distance in which the anchor point is closer to the similar point than to the dissimilar one. The above losses, which depend on pairs or triplets of data points, empirically suffer from sampling issues -selecting informative pairs or triplets is crucial for successfully optimizing them and improving convergence rates. In this work we address this aspect and propose to re-define triplet based losses over a different space of points, which we call proxies. This space approximates the training set of data points (for each data point in the original space there is a proxy point close to it), additionally, it is small enough so that we do not need to sample triplets but can explicitly write the loss over all (or most) of the triplets involving proxies. As a result, this re-defined loss is easier to optimize, and it trains faster (See <ref type="figure" target="#fig_1">Figure 1)</ref>. Note that the proxies are learned as part of the model parameters.</p><p>In addition, we show that the proxy-based loss is an upper bound to triplet loss and that, empirically, the bound tightness improves as training converges, which justifies the use of proxy-based loss to optimize the original loss.</p><p>Further, we demonstrate that the resulting distance metric learning problem has several desirable properties. First and foremost, the obtained metric performs well in the zeroshot scenario, improving state of the art, as demonstrated on three widely used datasets for this problem (CUB200, Cars196 and Stanford Products). Second, the learning problem formulated over proxies exhibits empirically faster convergence than other metric learning approaches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>There is a large body of work on metric learning, here we focus on its use in computer vision using deep methods.</p><p>An early use of deep methods for metric learning was the introduction of Siamese networks with a contrastive loss <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b2">3]</ref>. Pairs of data points were fed into a network, and the difference between the embeddings produced was used to pull together points from the same class, and push away from each other points from different classes. A shortcoming of this approach is that it can not take directly into account relative distances between classes. Since then, most methods use a notion of triplets to provide supervision.</p><p>In <ref type="bibr" target="#b17">[18]</ref> a large margin, nearest neighbor approach is designed to enable k-NN classification. It strives to ensure for each image x a predefined set of images from the same class as neighbors that are closer to x than images from other classes with a high separation margin. The set of target neighbors is defined using l 2 metric on the input space. The loss function is defined over triplets of points which are sampled during training. This sampling becomes prohibiting when the number of classes and training instances becomes large, see Sec 3.2 for more details.</p><p>To address some of the issues in this and similar work <ref type="bibr" target="#b12">[13]</ref> a Semi-Hard negative mining approach was introduced in <ref type="bibr" target="#b11">[12]</ref>. In this approach, hard triplets were formed by sampling positive/negative instances within a mini-batch with the goal of finding negative examples that are within the margin, but are not too confusing, as those might come from labeling errors in the data. This improved training stability but required large mini-batches -1800 images in the case of <ref type="bibr" target="#b11">[12]</ref>, and training was still slow. Large batches also require non trivial engineering work, e.g. synchronized training with multiple GPUs.</p><p>This idea, of incorporating information beyond a single triplet has influenced many approaches. Song et. al. <ref type="bibr" target="#b7">[8]</ref> proposed Lifted Structured Embedding, where each positive pair compares the distances against all negative pairs in the batch weighted by the margin violation. This provided a smooth loss which incorporates the negative mining functionality. In <ref type="bibr" target="#b13">[14]</ref>, the N-Pair Loss was proposed, which used Softmax cross-entropy loss on pairwise similarity values within the batch. Inner product is used as a similarity measure between images. The similarity between examples from the same class is encouraged to be higher than the similarity with other images in the batch. A cluster ranking loss was proposed in <ref type="bibr" target="#b14">[15]</ref>. The network first computed the embedding vectors for all images in the batch and ranked a clustering score for the ground truth clustering assignment higher than the clustering score for any other batch assignment with a margin.</p><p>Magnet Loss <ref type="bibr" target="#b8">[9]</ref> was designed to compare distributions of classes instead of instances. Each class was represented by a set of K cluster centers, constructed by k-means. In each training iteration, a cluster was sampled, and the M nearest impostor clusters (clusters from different classes) retrieved. From each imposter cluster a set of images were then selected and NCA <ref type="bibr" target="#b9">[10]</ref> loss used to compare the examples. Note that, in order to update the cluster assignments, training was paused periodical, and K-Means reapplied.</p><p>Our proxy-based approach compares full sets of examples, but both the embeddings and the proxies are trained end-to-end (indeed the proxies are part of the network architecture), without requiring interruption of training to recompute the cluster centers, or class indices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Metric Learning using Proxies</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Problem Formulation</head><p>We address the problem of learning a distance d(x, y; θ) between two data points x and y. For example, it can be defined as Euclidean distance between embeddings of data obtained via a deep neural net e(x; θ): d(x, y; θ) = ||e(x; θ) − e(y; θ)|| 2 2 , where θ are the parameters of the network. To simplify the notation, in the following we drop the full θ notation, and use x and e(x; θ) interchangeably.</p><p>Often times such distances are learned using similarity style supervision, e. g. triplets of similar and dissimilar points (or groups of points) D = {(x, y, z)}, where in each triplet there is an anchor point x, and the second point y (the positive) is more similar to x than the third point z (the negative). Note that both y and, more commonly, z can be sets of positive/negative points. We use the notation Y , and Z whenever sets of points are used.</p><p>The DML task is to learn a distance respecting the similarity relationships encoded in D:</p><formula xml:id="formula_0">d(x, y; θ) ≤ d(x, z; θ) for all (x, y, z) ∈ D (1)</formula><p>An ideal loss, precisely encoding Eq. (1), reads:</p><formula xml:id="formula_1">L Ranking (x, y, z) = H(d(x, y) − d(x, z))<label>(2)</label></formula><p>where H is the Heaviside step function. Unfortunately, this loss is not amenable directly to optimization using stochastic gradient descent as its gradient is zero everywhere. As a result, one resorts to surrogate losses such as Neighborhood Component Analysis (NCA) <ref type="bibr" target="#b9">[10]</ref> or margin-based triplet loss <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b11">12]</ref>. For example, Triplet Loss uses a hinge function to create a fixed margin between the anchor-positive difference, and the anchor-negative difference:</p><formula xml:id="formula_2">L triplet (x, y, z) = [d(x, y) + M − d(x, z)] +<label>(3)</label></formula><p>Where M is the margin, and [·] + is the hinge function. Similarly, the NCA loss <ref type="bibr" target="#b9">[10]</ref> tries to make x closer to y than to any element in a set Z using exponential weighting:</p><formula xml:id="formula_3">L NCA (x, y, Z) = − log exp(−d(x, y)) z∈Z exp(−d(x, z))<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Sampling and Convergence</head><p>Neural networks are trained using a form of stochastic gradient descent, where at each optimization step a stochastic loss is formulated by sampling a subset of the training set D, called a batch. The size of a batch b is small, e.g. in many modern computer vision network architectures b = 32. While for classification or regression the loss depends on a single data point from D, the above distance learning losses depend on at least three data points, i.e. total number of possible samples could be in O(n 3 ) for |D| = n. To see this, consider that a common source of triplet supervision is from a classification-style labeled dataset: a triplet (x, y, z) is selected such that x and y have the same label while x and z do not. For illustration, consider a case where points are distributed evenly between k classes. The number of all possible triplets is then kn/k· 3 ) steps, while in the case of classification or regression the needed number of steps is O(n/b). Note that n is in the order of hundreds of thousands, while b is between a few tens to about a hundred, which leads to n/b being in the tens of thousands.</p><formula xml:id="formula_4">((n/k)−1)(k− 1) · n/k = n 2 (n − k)(k − 1)/k 2 = O(n 3 ).</formula><p>Empirically, the convergence rate of the optimization procedure is highly dependent on being able to see useful triplets, e.g., triplets which give a large loss value as motivated by <ref type="bibr" target="#b11">[12]</ref>. The authors propose to sample triplets within the data points present in the current batch, this however, does not address the problem of sampling from the whole set of triplets D. This is particularly challenging as the number of triplets is so overwhelmingly large.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Proxy Ranking Loss</head><p>To address the above sampling problem, we propose to learn a small set of data points P with |P | ≪ |D|. Intuitively we would like to have P approximate the set of all data points, i.e. for each x there is one element in P which is close to x w.r.t. the distance metric d. We call such an element a proxy for x:</p><formula xml:id="formula_5">p(x) = arg min p∈P d(x, p)<label>(5)</label></formula><p>and denote the proxy approximation error by the worst approximation among all data points We propose to use these proxies to express the ranking loss, and because the proxy set is smaller than the original training data, the number of triplets would be significantly reduced (see <ref type="figure" target="#fig_3">Figure 2)</ref>. Additionally, since the proxies represent our original data, the reformulation of the loss would implicitly encourage the desired distance relationship in the original training data.</p><formula xml:id="formula_6">ǫ = max x d(x, p(x))<label>(6)</label></formula><p>To see this, consider a triplet (x, y, z) for which we are to enforce Eq. (1). By triangle inequality,</p><formula xml:id="formula_7">|{d(x, y) − d(x, z)} − {d(x, p(y)) − d(x, p(z))}| ≤ 2ǫ</formula><p>As long as |d(x, p(y)) − d(x, p(z))| &gt; 2ǫ, the ordinal relationship between the distance d(x, y) and d(x, z) is not changed when y, z are replaced by the proxies p(y), p(z). Thus, we can bound the expectation of the ranking loss over the training data:</p><formula xml:id="formula_8">E[L Ranking (x; y, z)] ≤ E[L Ranking (x; p(y), p(z))] + Pr[|d(x, p(y) − d(x, p(z)| ≤ 2ǫ]</formula><p>Under the assumption that all the proxies have norm p = N p and all embeddings have the same norm x = N x , the bound can be tightened. Note that in this case we have, for any α &gt; 0:</p><formula xml:id="formula_9">L Ranking (x, y, z) = H( αx − p(y) − αx − p(z) ) = H( αx − p(y) 2 − αx − p(z) 2 ) = H(2α(x T p(z) − x T p(y))) = H(x T p(z) − x T p(y)).</formula><p>I.e. the ranking loss is scale invariant in x. However, such re-scaling affects the distances between the embeddings and proxies. We can judiciously choose α to obtain a better bound. A good value would be one that makes the embeddings and proxies lie on the same sphere, i.e. α = N p /N x . These assumptions prove easy to satisfy, see Section 4.</p><p>The ranking loss is difficult to optimize, particularly with gradient based methods. We argue that many losses, such as NCA loss <ref type="bibr" target="#b9">[10]</ref>, Hinge triplet loss <ref type="bibr" target="#b17">[18]</ref>, N-pairs loss <ref type="bibr" target="#b13">[14]</ref>, etc are merely surrogates for the ranking loss. In this next section, we show how the proxy approximation can be used to bound the popular NCA loss for distance metric learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Training</head><p>In this section we explain how to use the introduced proxies to train a distance based on the NCA formulation. We would like to minimize the total loss, defined as a sum over triplets (x, y, Z) (see Eq. <ref type="formula">(1)</ref>). Instead, we minimize the upper bound, defined as a sum over triplets over an anchor and two proxies (x, p(y), p(Z)) (see Eq. <ref type="formula">(7)</ref>).</p><formula xml:id="formula_10">Algorithm 1 Proxy-NCA Training.</formula><p>Randomly init all values in θ including proxy vectors.</p><formula xml:id="formula_11">for i = 1 . . . T do Sample triplet (x, y, Z) from D Formulate proxy triplet (x, p(y), p(Z)) l = − log exp(−d(x,p(y))) p(z)∈p(Z) exp(−d(x,p(z))) θ ← θ − λ∂ θ l end for</formula><p>We perform this optimization by gradient descent, as outlined in Algorithm 1. At each step, we sample a triplet of a data point and two proxies (x, p(y), p(z)), which is defined by a triplet (x, y, z) in the original training data. However, each triplet defined over proxies upper bounds all triplets (x, y ′ , z ′ ) whose positive y ′ and negative z ′ data points have the same proxies as y and z respectively. This provides convergence speed-up. The proxies can all be held in memory, and sampling from them is simple. In practice, when an anchor point is encountered in the batch, one can use its positive proxy as y, and all negative proxies as Z to formulate triplets that cover all points in the data. We back propagate through both points and proxies, and do not need to pause training to re-calculate the proxies at any time.</p><p>We train our model with the property that all proxies have the same norm N P and all embeddings have the norm N X . Empirically such a model performs at least as well as without this constraint, and it makes applicable the tighter bounds discussed in Section 3.3. While in the future we will incorporate the equal norm property into the model during training, for the experiments here we simply trained a model with the desired loss, and re-scaled all proxies and embeddings to the unit sphere (note that the transformed proxies are only useful for analyzing the effectiveness of the bounds, and are not used during inference).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Proxy Assignment and Triplet Selection</head><p>In the above algorithm we need to assign the proxies for the positive and negative data points. We experiment with two assignment procedures.</p><p>When triplets are defined by the semantic labels of data points (the positive data point has the same semantic label as the anchor; the negative a different label), then we can associate a proxy with each semantic label: P = {p 1 . . . p L }. Let c(x) be the label of x. We assign to a data point the proxy corresponding to its label: p(x) = p c(x) . We call this static proxy assignment as it is defined by the semantic label and does not change during the execution of the algorithm. Critically, in this case, we no longer need to sample triplets at all. Instead one just needs to sample an anchor point x, and use the anchor's proxy as the positive, and the rest as negatives L N CA (x, p(x), p(Z); θ)</p><p>In the more general case, however, we might not have semantic labels. Thus, we assign to a point x the closest proxy, as defined in Eq. (5). We call this dynamic proxy assignment and note that is aligned with the original definition of the term proxy. See Section 6 for evaluation with the two proxy assignment methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Proxy-based Loss Bound</head><p>In addition to the motivation for proxies in Sec. 3.3, we also show in the following that the proxy based surrogate losses upper bound versions of the same losses defined over the original training data. In this way, the optimization of a single triplet of a data point and two proxies bounds a large number of triplets of the original loss.</p><p>More precisely, if a surrogate loss L over triplet (x, y, z) can be bounded by proxy triplet L(x, y, z) ≤ αL(x, p(y), p(z)) + δ for constant α and δ, then the following bound holds for the total loss:</p><formula xml:id="formula_12">L(D) ≤ α |D| x;py,pz∈P n x,py,pz L(x, p(y), p(z)) + δ (7)</formula><p>where n x,py,pz = |{(x, y, z) ∈ D|p(y) = p y , p(z) = p z }| denotes the number of triplets in the training data with anchor x and proxies p y and p z for the positive and negative data points. The quality of the above bound depends on δ, which depends on the loss and as we will see also on the proxy approximation error ǫ. We will show for concrete loss that the bound gets tighter for small proxy approximation error.</p><p>The proxy approximation error depends to a degree on the number of proxies |P |. In the extreme case, the number of proxies is equal to the number of data points, and the approximation error is zero. Naturally, the smaller the number of proxies the higher the approximation error. However, the number of terms in the bound is in O(n|P | 2 ). If |P | ≅ n then the number of samples needed will again be O(n 3 ). We would like to keep the number of terms as small as possible, as motivated in the previous section, while keeping the approximation error small as well. Thus, we seek a balance between small approximation error and small number of terms in the loss. In our experiments, the number of proxies varies from a few hundreds to a few thousands, while the number of data points is in the tens/hundreds of thousands.</p><p>Proxy loss bounds For the following we assume that the norms of proxies and data points are constant |p x | = N p and |x| = N x , we will denote α =  <ref type="formula" target="#formula_3">(4)</ref>) is proxy bounded:</p><formula xml:id="formula_13">L NCA (x, y, Z) ≤ αL NCA (x, p y , p Z )+(1−α) log(|Z|)+2 √ 2ǫ</formula><p>whereL NCA is defined as L NCA with normalized data points and |Z| is the number of negative points used in the triplet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proposition 4.2. The margin triplet loss (see Eq. (3)) is proxy bounded:</head><formula xml:id="formula_14">L triplet (x, y, z) ≤ αL triplet (x, p y , p z ) + (1 − α)M + 2 √ ǫ</formula><p>whereL triplet is defined as L triplet with normalized data points.</p><p>See Appendix for proofs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>We used the TensorFlow Deep Learning framework [1] for all methods described below. For fair comparison <ref type="bibr" target="#b0">1</ref> we follow the implementation details of <ref type="bibr" target="#b14">[15]</ref>. We use the Inception <ref type="bibr" target="#b15">[16]</ref> architecture with batch normalization <ref type="bibr" target="#b4">[5]</ref>. All methods are first pretrained on ILSVRC 2012-CLS data <ref type="bibr" target="#b10">[11]</ref>, and then finetuned on the tested datasets. The size of the learned embeddings is set to 64. The inputs are resized to 256 × 256 pixels, and then randomly cropped to 227 × 227. The numbers reported in <ref type="bibr" target="#b13">[14]</ref> are using multiple random crops during test time, but for fair comparison with the other methods, and following the procedure in <ref type="bibr" target="#b14">[15]</ref>, our implementation uses only a center crop during test time. We use the RMSprop optimizer with the margin multiplier constant γ decayed at a rate of 0.94. The only difference we take from the setup described in <ref type="bibr" target="#b14">[15]</ref> is that for our proposed method, we use a batch size m of 32 images (all other methods use m = 128). We do this to illustrate one of the benefits of the proposed method -it does not require large batches. We have experimentally confirmed that the results are stable when we use larger batch sizes for our method.</p><p>Most of our experiments are done with a Proxy-NCA loss. However, proxies can be introduced in many popular metric learning algorithms, as outlined in Section 3. To illustrate this point, we also report results of using a ProxyTriplet approach on one of the datasets, see Section 6 below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Evaluation</head><p>Based on the experimental protocol detailed in <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b13">14]</ref> we evaluate retrieval at k and clustering quality on data from unseen classes on 3 datasets: CUB200-2011 <ref type="bibr" target="#b16">[17]</ref>, Cars196 <ref type="bibr" target="#b5">[6]</ref>, and Stanford Online Products <ref type="bibr" target="#b7">[8]</ref>. Clustering quality is evaluated using the Normalized Mutual Information measure (NMI). NMI is defined as the ratio of the mutual information of the clustering and ground truth, and their harmonic mean. Let Ω = {ω 1 , ω 2 , . . . , ω k } be the cluster assignments that are, for example, the result of K-Means clustering. That is, ω i contains the instances assigned to the i'th cluster. Let C = {c 1 , c 2 , . . . , c m } be the ground truth classes, where c j contains the instances from class j.</p><formula xml:id="formula_15">NMI(Ω, C) = 2 I(Ω, C) H(Ω) + H(C) .<label>(8)</label></formula><p>Note that NMI is invariant to label permutation which is a desirable property for for our evaluation. For more information on clustering quality measurement see <ref type="bibr" target="#b6">[7]</ref>. We compare our Proxy-based method with 4 state-ofthe-art deep metric learning approaches: Triplet Learning with semi-hard negative mining <ref type="bibr" target="#b11">[12]</ref>, Lifted Structured Embedding <ref type="bibr" target="#b7">[8]</ref>, the N-Pairs deep metric loss <ref type="bibr" target="#b13">[14]</ref>, and Learnable Structured Clustering <ref type="bibr" target="#b14">[15]</ref>. In all our experiments we use the same data splits as <ref type="bibr" target="#b14">[15]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Cars196</head><p>The Cars196 dataset <ref type="bibr" target="#b5">[6]</ref> is a fine-grained car category dataset containing 16,185 images of 196 car models. Classes are at the level of make-model-year, for example,</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Stanford Online Products dataset</head><p>The Stanford product dataset contains 120,053 images of 22,634 products downloaded from eBay.com. For training, 59,5511 out of 11,318 classes are used, and 11,316 classes (60,502 images) are held out for testing. This dataset is  more challenging as each product has only about 5 images, and at first seems well suited for tuple-sampling approaches, and less so for our proxy formulation. Note that holding in memory 11,318 float proxies of dimension 64 takes less than 3Mb. <ref type="figure" target="#fig_6">Figure 4</ref> shows recall-at-1 results on this dataset. Proxy-NCA has over a 6% gap from previous state of the art. Proxy-NCA compares favorably on clustering as well, with a score of 90.6. This, compared with the top method, described in <ref type="bibr" target="#b14">[15]</ref> which has an NMI score of 89.48. The difference is statistically significant. <ref type="figure" target="#fig_7">Figure 5</ref> shows example retrieval results on images from the Stanford Product dataset. Interestingly, the embeddings show a high degree of rotation invariance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">CUB200</head><p>The Caltech-UCSD Birds-200-2011 dataset contains 11,788 images of birds from 200 classes of fine-grained bird species. We use the first 100 classes as training data for the metric learning methods, and the remaining 100 classes for   evaluation. <ref type="table" target="#tab_3">Table 2</ref> compares the proxy-NCA with the baseline methods. Birds are notoriously hard to classify, as the inner-class variation is quite large when compared to the initra-class variation. This is apparent when observing the results in the table. All methods perform less well than in the other datasets. Proxy-NCA improves on SOTA for recall at 1-2 and on the clustering metric.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4.">Convergence Rate</head><p>The tuple sampling problem that affects most metric learning methods makes them slow to train. By keeping all proxies in memory we eliminate the need for sampling tuples, and mining for hard negative to form tuples. Furthermore, the proxies act as a memory that persists between R@1 R@2 R@4 R@8 NMI Triplet Semihard <ref type="bibr" target="#b11">[12]</ref>   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>R@1</head><p>Triplet <ref type="bibr" target="#b11">[12]</ref> Lifted Struct <ref type="bibr" target="#b7">[8]</ref> Npairs <ref type="bibr" target="#b13">[14]</ref> Struct Clust <ref type="bibr" target="#b14">[15]</ref> Proxy-NCA <ref type="figure">Figure 6</ref>: Recall@1 results as a function of ratio of proxies to semantic labels. When allowed 0.5 proxies per label or more, Proxy-NCA compares favorably with previous state of the art.</p><p>batches. This greatly speeds up learning. <ref type="figure" target="#fig_1">Figure 1</ref> compares the training speed of all methods on the Cars196 dataset. Proxy-NCA trains much faster than other metric learning methods, and converges about three times as fast.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.5.">Fractional Proxy Assignment</head><p>Metric learning requires learning from a large set of semantic labels at times. Section 6.2 shows an example of such a large label set. Even though Proxy-NCA works well in that instance, and the memory footprint of the proxies is small, here we examine the case where one's computational budget does not allow a one-to-one assignment of proxies to semantic labels. <ref type="figure">Figure 6</ref> shows the results of an experiment in which we vary the ratio of labels to proxies on the Cars196 dataset. We modify our static proxy assignment method to randomly pre-assign semantic labels to proxies. If the number of proxies is smaller than the number of labels, multiple labels are assigned to the same proxy. So in effect each semantic label has influence on a fraction of a proxy. Note that when proxy-per-class ≥ 0.5 Proxy-NCA has better performance than previous methods.  <ref type="figure">Figure 7</ref>: Recall@1 results for dynamic assignment on the Cars196 dataset as a function of proxy-to-semantic-label ratio. More proxies allow for better fitting of the underlying data, but one needs to be careful to avoid over-fitting.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.6.">Dynamic Proxy Assignment</head><p>In many cases, the assignment of triplets, i.e. selection of a positive, and negative example to use with the anchor instance, is based on the use of a semantic concept -two images of a dog need to be more similar than an image of a dog and an image of a cat. These cases are easily handled by our static proxy assignment, which was covered in the experiments above. In some cases however, there are no semantic concepts to be used, and a dynamic proxy assignment is needed. In this section we show results using this assignment scheme. <ref type="figure">Figure 7</ref> shows recall scores for the Cars196 dataset using the dynamic assignment. The optimization becomes harder to solve, specifically due to the non-differentiable argmin term in Eq.(5). However, it is interesting to note that first, a budget of 0.5 proxies per semantic concept is again enough to improve on state of the art, and one does see some benefit of expanding the proxy budget beyond the number of semantic concepts.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Discussion</head><p>In this paper we have demonstrated the effectiveness of using proxies for the task of deep metric learning. Using proxies, which are saved in memory and trained using back-prop, training time is reduced, and the resulting models achieve a new state of the art. We have presented two proxy assignment schemes -a static one, which can be used when semantic label information is available, and a dynamic one which is used when the only supervision comes in the form of similar and dissimilar triplets. Furthermore, we show that a loss defined using proxies, upper bounds the original, instance-based loss. If the proxies and instances have constant norms, we show that a well optimized proxybased model does not change the ordinal relationship between pairs of instances.</p><p>Our formulation of Proxy-NCA loss produces a loss very similar to the standard cross-entropy loss used in classification. However, we arrive at our formulation from a different direction: we are not interested in the actual classifier and indeed discard the proxies once the model has been trained. Instead, the proxies are auxiliary variables, enabling more effective optimization of the embedding model parameters. As such, our formulation not only enables us to surpass the state of the art in zero-shot learning, but also offers an explanation to the effectiveness of the standard trick of training a classifier, and using its penultimate layer's output as the embedding.</p><p>Similarly, one can obtain an upper bound for the negative dot product:</p><formula xml:id="formula_16">−x Tŷ ≤ −x Tp y + √ ǫ<label>(11)</label></formula><p>Using the above two bounds we can upper bound the original NCA loss L NCA (x,ŷ, Z): </p><formula xml:id="formula_17">=</formula><p>Further, we can upper bound the above loss of unit normalized vectors by a loss of unnormalized vectors. For this we would make the assumption, which empirically we have found true, that for all data points |x| = N x &gt; 1. In practice these norm are much larger than 1.</p><p>Lastly, if we denote by β =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>NxNp and under the assumption that β &lt; 1, we can apply the following version of the Hoelder inequality defined for positive real numbers a i : </p><p>for β =</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>1</head><p>NxNp . The propositions follows from Eq. (13) and Eq. <ref type="bibr" target="#b13">(14)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Proof Proposition 4.2:</head><p>We will bound the term inside the hinge function in Eq. (3) for normalized data points using the bounds <ref type="bibr" target="#b9">(10)</ref> and <ref type="formula" target="#formula_16">(11)</ref>  </p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Recall@1 as a function of training step on the Cars196 dataset. Proxy-NCA converges about three times as fast compared with the baseline methods, and results in higher Recall@1 values.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>As a result, in metric learning each batch samples a very small subset of all possible triplets, i.e., in the order of O(b 3 ). Thus, in order to see all triplets in the training one would have to go over O((n/b)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Illustrative example of the power of proxies. [Left panel] There are 48 triplets that can be formed from the instances (small circles/stars). [Right panel] Proxies (large circle/star) serve as a concise representation for each semantic concept, one that fits in memory. By forming triplets using proxies, only 8 comparisons are needed.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>1</head><label></label><figDesc>NpNx . Then the following bounds of the original losses by their proxy versions are: Proposition 4.1. The NCA loss (see Eq.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Retrieval results on a set of images from the Cars196 dataset using our proposed proxy-based training method. Left column contains query images. The results are ranked by distance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Recall@1 results on the Stanford Product Dataset. Proxy-NCA has a 6% point gap with previous SOTA.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Retrieval results on a randomly selected set of images from the Stanford Product dataset. Left column contains query images. The results are ranked by distance. Note the rotation invariance exhibited by the embedding.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>=</head><label></label><figDesc>L NCA (x,p y ,p Z ) + 2 √ ǫ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head></head><label></label><figDesc>the above loss L NCA with unit normalized points is bounded as: L NCA (x,p y ,p Z )NCA (x, p y , p Z ) + (1 − β) log(|Z|)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>=</head><label></label><figDesc>Under the assumption that the data points and the proxies have constant norms, we can convert the above dot products to products of unnormalized points:α(|x − p y | 2 − |x − p z | 2 + M ) + (1 − α)M + 2 √ ǫ</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Retrieval and Clustering Performance on the 
CUB200 dataset. 

0.25 
0.50 
0.75 
1.00 

Proxy Per Class 

40 

45 

50 

55 

60 

65 

70 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We thank the authors of [15] for providing their code for the baseline methods, in which we based our model, and for helpful discussions.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">Mazda-3-2011. In our experiments we split the dataset such that 50% of the classes are used for training, and 50% are used for evaluation. Table 1 shows recall-at-k and NMI scores for all methods on the Cars196 dataset. Proxy-NCA has a 15 percentage points (26% relative) improvement in recall@1 from previous state-of-the-art, and a 6% point gain in NMI. Figure 3 shows example retrieval results on the test set of the Cars196 dataset.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgments</head><p>The authors would like to thank Hossein Mobahi, Zhen Li, Hyun Oh Song, Vincent Vanhoucke, and Florian Schroff for helpful discussions.</p></div>
			</div>

			<div type="annex">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Appendix</head><p>Proof of Proposition 4.1: In the following for a vector x we will denote its unit norm vector byx = x/|x|.</p><p>First, we can upper bound the dot product of a unit normalized data pointsx andŷ by the dot product of unit normalized pointx and proxyp y using the Cauchy inequality as follows:x</p><p>Hence:</p></div>			</div>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.04467</idno>
		<title level="m">Large-scale machine learning on heterogeneous distributed systems</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer vision and pattern recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep clustering: Discriminative embeddings for segmentation and separation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Hershey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Le</forename><surname>Roux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Watanabe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Acoustics, Speech and Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="31" to="35" />
		</imprint>
	</monogr>
	<note>2016 IEEE International Conference on</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03167</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">3d object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision Workshops</title>
		<meeting>the IEEE International Conference on Computer Vision Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">Introduction to information retrieval</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Raghavan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Schütze</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>Cambridge university press</publisher>
			<pubPlace>Cambridge</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05939</idno>
		<title level="m">Metric learning with adaptive density discrimination</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Neighbourhood component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Adv. Neural Inf. Process. Syst.(NIPS)</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Learning a distance metric from relative comparisons</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schultz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class n-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">D</forename><surname>Sohn ; D</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><forename type="middle">V</forename><surname>Sugiyama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Luxburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Garnett</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learnable structured clustering framework for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.4842</idno>
		<title level="m">Going deeper with convolutions</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The caltech-ucsd birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Distance metric learning for large margin nearest neighbor classification. Advances in neural information processing systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Blitzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Saul</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Improving the robustness of deep neural networks via stability training</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
