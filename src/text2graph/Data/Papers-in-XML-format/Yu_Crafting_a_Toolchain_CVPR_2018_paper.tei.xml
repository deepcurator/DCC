<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:15+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ke</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chao</forename><surname>Dong</surname></persName>
							<email>dongchao@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@sensetime.com</email>
							<affiliation key="aff1">
								<orgName type="department">SenseTime Research</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Sun Yat-sen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename><forename type="middle">Change</forename><surname>Loy</surname></persName>
							<email>ccloy@ie.cuhk.edu.hk</email>
							<affiliation key="aff0">
								<orgName type="laboratory">CUHK -SenseTime Joint Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Crafting a Toolchain for Image Restoration by Deep Reinforcement Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural network (CNN) has achieved immense success, not only in high-level vision tasks, but also low-level vision tasks such as deblurring <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>, denoising <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>, JPEG artifacts reduction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> and super-resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. In particular, good performance and fast testing speed are demonstrated over conventional model-based optimization methods.</p><p>Owing to the discriminative nature of CNN, most of these models are trained to handle a specialized low-level vision task. In JPEG artifacts reduction <ref type="bibr" target="#b6">[7]</ref>, for instance, different networks for different compression qualities have been designed to achieve satisfactory restoration. In the case of super-resolution <ref type="bibr" target="#b7">[8]</ref>, it is common to have different networks to handle different scaling factors. Some recent studies <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b36">37]</ref> have shown the possibility of handling multiple distortion types or coping with different levels of degradation at once using CNN. Nevertheless, this usually comes with the expenses of using much deeper networks. In addition, such networks process all images with the same structure, despite some of which are inherently less difficult and can be restored in a cheaper way.</p><p>In this paper, we explore the possibility of having some smaller-scale but specialized CNNs to solve a harder restoration task collaboratively. Our idea departs from the current philosophy that one would need a large-capacity CNN to solve a complex restoration task. Instead, we wish to have a set of tools (based on small CNNs) and learn to use them adaptively for solving the task at hand. The aforementioned idea could provide new insights how CNN can be used for solving real-world restoration tasks, of which images are potentially contaminated with a mix of distortions, e.g., blurring, noise and blockiness after several stages of processing. Moreover, the new approach may lead to parameter-efficient restoration in comparison to existing CNN-based models. In particular, tools of different complexities can be selected based on the severity of distortion.</p><p>Towards this goal, we present a framework that treats image restoration as a decision making process by which an agent would adaptively select a sequence of tools to progressively refine an image, and the agent may choose to stop if the restored quality is deemed satisfactory. In our framework, we prepare a number of light-weight CNNs with different complexities. They are task-specific aiming to handle different types of restoration assignments including deblurring, denoising, or JPEG artifacts reduction. Choosing the order of tools is formulated in a reinforcement learning (RL) framework. An agent learns to decide the next best tool to select by analyzing the content of the restored image in the current step and observing the last action chosen. Rewards are accumulated when the agent improves the quality of the input image.</p><p>We refer to the proposed framework as RL-Restore. We summarize our contributions as follows: 1) We present a new attempt to address image restoration in a reinforcement learning framework. Unlike existing methods that deploy a single and potentially large network structure, RL-Restore enjoys the flexibility of using tools of dif- ferent capacities to achieve the desired restoration.</p><p>2) We propose a joint learning scheme to train the agent and tools simultaneously so that the framework possesses better capability in coping with new and unknown artifacts emerged in the mid of processing.</p><p>3) We show that the dynamically formed toolchain performs competitively against strong human-designed networks with less computational complexity. Our approach can cope with unseen distortions to certain extent. Interestingly, our approach is more transparent than existing methods as it can reveal how complicated distortions could be removed step by step using different tools. <ref type="figure">Figure 1</ref>(b-c) illustrate a learned policy to restore an image corrupted by multiple distortions, where image quality is refined step-by-step. The results of two baseline CNN models are depicted in <ref type="figure">Figure 1(d-e)</ref>, where (d) has similar number of parameters as ours (agent + tools applied), while (e) has twice more. As we will further present in the experimental section, RL-Restore is superior to CNN approaches given similar complexity and it requires 82.2% fewer computations to achieve the same performance as a single large CNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work CNN for Image Restoration.</head><p>Image restoration is an extensively studied topic that aims at estimating the clear/original image from a corrupted/noisy observation. Convolutional neural networks (CNN) based methods have demonstrated outstanding performance in various image restoration tasks. Most of these studies train a single network specializing on the task at hand, e.g., deblurring <ref type="bibr" target="#b30">[31,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b41">42]</ref>, denoising <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b23">24]</ref>, JPEG artifacts reduction <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b40">41]</ref> and super-resolution <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b38">39]</ref>. Our work offers an alternative that is more parameter efficient yet adaptive to the form of distortions.</p><p>There are several pioneering studies that deal with multiple degradations simultaneously. By developing a 20-layer deep CNN, Kim et al. <ref type="bibr" target="#b18">[19]</ref> use a single model to handle multi-scale image super-resolution. Guo et al. <ref type="bibr" target="#b9">[10]</ref> build a one-to-many network that can handle images with different levels of compression artifacts. Zhang et al. <ref type="bibr" target="#b43">[44]</ref> propose a 20-layer deep CNN to address multiple restoration tasks simultaneously, including image denoising, JPEG artifacts reduction and super-resolution. None of these studies considers mixed distortion, where a single image is affected by multiple distortions. Different from the aforementioned works, we are interested to explore if smaller-scale CNNs of 3 to 8 layers could be used to jointly restore images that are contaminated with mixed distortions.</p><p>There exist approaches <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b13">14]</ref> that can be used to compress a large network to a smaller one for computational efficiency. In the domain of image restoration, recursive neural networks <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b36">37]</ref> are investigated to reduce network parameters. However, the computational cost is still high due to the large number of recursions. The objective of our work is orthogonal to the aforementioned studiesour framework saves parameters and computation through learning a policy to make decision in selecting appropriate CNNs for a task rather than compressing an existing one. Deep Reinforcement Learning. Reinforcement learning is a powerful tool for learning an agent making sequential decisions to maximize accumulative rewards. Early works of RL mainly focus on robotic control <ref type="bibr" target="#b26">[27,</ref><ref type="bibr" target="#b37">38]</ref>. Recently traditional RL algorithms are incorporated in deep learning frameworks and are successfully applied in various domains such as game agents <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b33">34]</ref> and neural network architecture design <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b44">45]</ref>. Attention is also drawn to deep RL in the field of computer vision <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b42">43]</ref>. For instance, Huang et al. <ref type="bibr" target="#b15">[16]</ref> use RL to learn an early decision policy for speeding up object tracking by CNN. Cao et al. <ref type="bibr" target="#b3">[4]</ref> explore deep RL algorithms in low-level vision and apply attention mechanism <ref type="bibr" target="#b28">[29]</ref> to face hallucination. In this study, we investigate restoration tool selection in a RL framework. The problem is new in the literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Learning a Restoration Toolchain</head><p>Problem Definition. Given a distorted image I dis , our goal is to restore a clear image I res that is close to the ground truth image I gt . The distortion process can be formulated + .</p><p>-.</p><p>-.</p><p>-.</p><p>-.</p><p>+ .</p><p>+ .</p><p>+ .</p><p>-.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>+ .</head><p>+ .</p><p>+ .</p><p>+ .</p><p>-. as:</p><formula xml:id="formula_0">I dis = D(I gt ); D = D n • · · · • D 1 ,<label>(1)</label></formula><p>where • denotes function composition and each of D 1 , . . . , D n represents a specific type of distortion. In contrast to existing methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b40">41]</ref> that concentrate on a single type of distortion, we intend to handle a mix of multiple distortions (i.e., n &gt; 1). For example, the final output image may be sequentially affected by out-of-focus blur, exposure noise and JPEG compression. In such a case, the number of distortions n is 3, and D 1 , D 2 , D 3 represent blur, noise and compression, respectively. To address mixed distortions, we propose to restore the corrupted image step by step with a sequence of restoration tools.</p><p>Challenges. The task of tool selection is non-trivial and presents unique challenges to RL. Then we slightly re-arrange the tools order as in <ref type="figure" target="#fig_0">Figure 2</ref>(b, e) or adjust the restoration level of the tools as in <ref type="figure" target="#fig_0">Figure 2</ref>(a, f). The results indicate that minor changes in a toolchain can severely impact the restoration performance. Specifically, using improper tools may lead to unnatural outputs, such as over-sharpening in <ref type="figure" target="#fig_0">Figure 2</ref>(a) and blurring in <ref type="figure" target="#fig_0">Figure 2</ref>(f). Even the tools are well chosen, an inappropriate order could decrease the performance ( <ref type="figure" target="#fig_0">Figure 2</ref>(b, e)). Since the sequence of toolchain dramatically influences the results, selecting which tool to use at each step becomes crucial. When the tools are trained on specific tasks, we encounter another problem that none of the tools can perfectly handle the 'middle state', which refers to the intermediate result after several steps of processing. As most distortions are irreversible, the restoration of their mixture is not a simple composition of the corresponding restorers. New artifacts could be introduced in the middle states. For example, the deblurring operation will also enhance the noises, causing the following denoisers fail in removing the newly introduced artifacts. The challenge is unique to our task.</p><p>To address the first challenge, we treat the sequential tool selection problem as a Markov Decision Process (MDP) and solve it in a deep reinforcement learning manner. To address the second challenge, we propose a training scheme to refine the agent and tools jointly so that the tools are more well-informed with the middle states observable by the agent. We first provide an overview of the proposed framework as follows. Overview of RL-Restore. The proposed framework aims at discovering a toolchain given a corrupted input image. As shown in <ref type="figure">Figure 3</ref>, RL-Restore consists of two components: 1) a toolbox that contains various tools for image restoration and 2) an agent with a recurrent structure that dynamically chooses a tool at each step or an early stopping action. We cast the tool selection process as a reinforcement learning procedure -a sequence of decision on tool selection is made to maximize a reward proportional to the quality of the restored image. Next, we first describe a plausible setting of toolbox and then explain the details of the agent.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Toolbox</head><p>The toolbox contains a set of tools that might be applied to the corrupted image. Our goal is to design a powerful and light-weight toolbox, we thus restrict each tool to be proficient in a specific task. That is, each tool is trained only on a narrow range of distortions. To further reduce the overall complexity, we use smaller networks for easier tasks. For the purpose of our research, we prepare 12 tools as shown in <ref type="table" target="#tab_3">Table 1</ref>, where each tool is assigned to address a certain level of Gaussian blur, Gaussian noise or JPEG compression. We apply a three-layer CNN (as in <ref type="bibr" target="#b7">[8]</ref>) for slight distortions and a deeper eight-layer CNN for severe distortions. Note that the tools need not be restricted to solve the aforementioned distortions. We made these selections since they are typically considered in the literature of image restoration. In practice, one could design their tools with appropriate complexity based on the task at hand.</p><p>As discussed at the beginning of Sec. 3, a finite set of tools is not perfect to handle new artifacts emerged in middle states. To address this issue, we propose two strategies : Step 1</p><p>Step T LSTM</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>RL-Restore</head><p>Tool ox Age t <ref type="figure">Figure 3</ref>. Illustration of our RL-Restore framework. At each step t, the agent fag observes the current state St, including the current restored image It and input value vectorṽt, which is the output of the agent at the previous step. Note that I1 represents the input image andṽ1 is a zero vector. Based on the maximum value of the agent's output vt, an action at is selected and the corresponding tool is used to restore the current image. After restoration process fr, with the newly restored image It+1 and value vectorṽt+1 = vt, RL-Restore conducts another step of restoration iteratively until the stopping action is selected. 1) To increase robustness of the tools, we add slight Gaussian noises and JPEG compression to all the training data.</p><p>2) After training the agent, all tools are jointly fine-tuned on the basis of the well-trained toolchains. Then the tools will be more adaptive to the agent task, and be able to deal with middle states more robustly. We discuss the training steps in Sec. 3.3. Experiments in Sec. 4 validate the effectiveness of the proposed strategies.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Agent</head><p>The processing pipeline of RL-Restore is shown in <ref type="figure">Figure 3</ref>. Given an input image, the agent first selects a tool from the toolbox and uses it to restore the image, then the agent chooses another tool according to the previous result and repeats the restoration process until it decides to stop. We will first clarify some terminologies such as action, state and reward, and then go into the details of the agent structure and restoration procedure.</p><p>Action. The action space, denoted as A, is a set of all possible actions that the agent could take. At each step t, an action a t is selected and applied to the current input image. Each action represents a tool in the toolbox and there is one additional action that represents stopping. If there are N tools in the toolbox, then the cardinality of A is N + 1. Hence, the output, v t , of the agent is an (N + 1)-dimensional vector that implicates the value of each action. Once the stopping action is chosen, the restoration procedure will be terminated and the current input image will become the final result. State. The state contains information that the agent could observe. In our formulation, the state is formulated as S t = {I t ,ṽ t }, where I t is the current input image, andṽ t is the past historical action vector. At step 1, I 1 is the input image andṽ 1 is a zero vector. The state provides rich contextual knowledge to the agent. 1) The current input image I t is essential because the selected action will be directly applied to this image to derive a better restored result. 2) the information of previous action vectorṽ t , which is the output value vector of the agent at t − 1 step, i.e.,ṽ t = v t−1 , is important too. The knowledge of the previous decision could help the action selection at the current step. This is found to work better empirically than using I t only. Reward. The reward drives the training of the agent as it learns to maximize the cumulative reward. The agent is supposed to learn a good policy so that the final restored image is satisfactory. We wish to ensure that the image quality is enhanced at each step, therefore a stepwise reward is designed as follows:</p><formula xml:id="formula_1">r t = P t+1 − P t ,<label>(2)</label></formula><p>where r t is the reward function at step t, P t+1 denotes the PSNR between I t+1 and the reference image I gt at the end of the t-th step restoration, and P t represents the input PSNR at step t. The cumulative reward can be written as R = T t=1 r t = P T +1 − P 1 , which is the overall PSNR gain during the restoration procedure, and it is maximized to achieve optimal enhancement. Note that it is flexible to use other image quality metrics (e.g., perceptual loss <ref type="bibr" target="#b17">[18]</ref>, GAN loss <ref type="bibr" target="#b22">[23]</ref>) as the reward in our framework. The investigation is beyond the focus of this paper. Structure. At each step t, the agent assesses the value of each action given the input state S t , which can be formu-lated as follows:</p><formula xml:id="formula_2">v t = f ag (S t ; W ag ),<label>(3)</label></formula><p>where f ag indicates the agent network and W ag denotes its parameters. The vector v t represents the value of actions. The action with the maximum value is selected as a t , i.e., a t = argmax a v t,a , where v t,a indicates the element of value vector v t corresponding to action a. The agent is composed of three modules as depicted in <ref type="figure">Figure 3</ref>. The first module, named feature extractor, is a four-layer CNN followed by a fully-connected (fc) layer that outputs a 32-dimensional feature. The second module is a one-hot encoder with N + 1 dimensional input and N dimensional output, preserving the information of the previous chosen action. Note that the output is one dimension lower than the input, because the stopping action cannot be adopted at the previous step, and thus we simply drop the last dimension. The outputs of the first two modules are concatenated into the input of the third module, which is a Long Short-Term Memory (LSTM) <ref type="bibr" target="#b14">[15]</ref>. The LSTM not only observes the input state, but also stores historical states in its memory, which offers contextual information of historical restored images and actions. Finally, with another fc layer following LSTM, a value vector v t is derived for tool selection. Restoration. Once an action a t is obtained based on the maximum value in v t , the corresponding tool will be applied to the input image I t to get a new restored image:</p><formula xml:id="formula_3">I t+1 = f r (I t , a t ; W r ),<label>(4)</label></formula><p>where f r denotes the restoration fucntion and W r indicates the parameters of a tool in the toolbox. If a stopping action is selected, f r represents an identity mapping. By denoting I dis and I res as the input distorted image and final restored output respectively, the overall procedure of restoration can be expressed as:</p><formula xml:id="formula_4">     I 1 = I dis I t+1 = f (I t ; W ) 1 ≤ t ≤ T I res = I T +1 ,<label>(5)</label></formula><p>where</p><formula xml:id="formula_5">f = [f ag ; f r ] and W = [W ag ; W r ].</formula><p>T is the step when the stopping action is chosen. We also set a maximum step T max to prevent excessive restoration. When t = T max and the stopping action is not selected, we will terminate the restoration process after the current step. In other words, we add a constraint that T ≤ T max .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Training</head><p>The training of tools follows a standard setting in <ref type="bibr" target="#b18">[19]</ref>, where a mean square error (MSE)</p><formula xml:id="formula_6">1 2 y − h(x) 2</formula><p>2 is minimized. The ground truth image, input image and the tool are denoted as y, x and h, respectively. As for the agent, the training is addressed by deep Q-learning <ref type="bibr" target="#b29">[30]</ref> since we do not have a priori knowledge about the correct action to choose. In the proposed framework, each element of v t is an action value as defined in <ref type="bibr" target="#b29">[30]</ref>, so the loss function can be written as L = (y t − v t,at ) 2 where</p><formula xml:id="formula_7">y t = r t + γ max a ′ v t+1,a ′ 1 ≤ t &lt; T r T t = T,<label>(6)</label></formula><p>and γ = 0.99 is a discount factor. We also employ a target network f ′ ag to stabilize training, which is a clone of f ag and updates its parameters every C steps while training. In the above formula, v t+1,a ′ is derived from f ′ ag and v t,at is from f ag . While training, episodes are randomly selected from a replay memory, and there are two updating strategies as proposed in <ref type="bibr" target="#b11">[12]</ref>, where 'random updates' refer to updating from a random point of each episode and proceeding a fixed number of steps, and 'sequential updates' indicate that all the updates begin at the beginning of the episode and proceed to its endpoint. In <ref type="bibr" target="#b11">[12]</ref>, it is claimed that both updating strategies have similar performance. Since our toolchain is not too long, we simply adopt 'sequential updates' where each training sequence contains an entire toolchain. Joint Training. As discussed in Section 3.1, none of the tools can perfectly handle the middle state, where new and complex artifacts may be introduced in the previous steps of restoration. In order to address this issue, we propose a joint training algorithm, as shown in Algorithm 1, to train the tools in an end-to-end manner so that all the tools can learn to deal with the middle state. Specifically, for each toolchain in a batch, the distorted image I 1 is forwarded to get a restored result I T +1 . Given a final MSE loss, the gradients then pass backward along the same toolchain. Meanwhile, the gradients of each tool are accumulated within a batch, and finally an average of gradient is used to update the corresponding tool. The above updating process is repeatedly conducted for a few iterations. Implementation Details. In our implementation, the training of tools is similar to <ref type="bibr" target="#b18">[19]</ref>, where all experiments run over 80 epochs (3.2 × 10 5 iterations) with a batch size of 64. The initial learning rate is 0.1 and it decreases by a factor of 0.1 every 20 epochs. For joint training, we set M = 64, α = 0.0001 in Algorithm 1, denoting the batch size and learning rate respectively. The joint training runs over 2 × 10 5 iterations. While training the agent, we use Adam <ref type="bibr" target="#b20">[21]</ref> optimizer and a batch size of 32. The maximum step T max is set to be 3 empirically and the size of replay memory is chosen as 5 × 10</p><p>5 . The updating frequency C = 2, 500 so that the target network f ′ ag is copied from the latest agent network f ag every 2, 500 iterations. The learning rate is decayed exponentially from 2.5 × 10 </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Joint training algorithm (1 iteration)</head><p>Initialize counters c1, c2, . . . , cN = 0 Initialize gradients G1, G2, . . . , GN = 0</p><formula xml:id="formula_8">for m = 1, M do ⊲ For each toolchain I1 ← Input image for t = 1, T do ⊲ Forward paths at ← fag(St) It+1 ← fr(It, at) end for L ← 1 2 Igt − IT +1 2 2 for t = T to 1 step −1 do ⊲ Backward paths ca t ← ca t + 1 Ga t ← Ga t + ∂L/∂Wa t L ← It · ∂L/∂It</formula><note type="other">end for end for for i = 1, N do ⊲ Update tools if ci &gt; 0 then Wi ← Wi − αGi/ci end if end for</note></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>Datasets and Evaluation Metrics. We perform experiments on the DIV2K dataset <ref type="bibr" target="#b0">[1]</ref>, which is the most recent large-scale and high-quality dataset for image restoration. The 800 DIV2K training images are divided into two parts: 1) the first 750 images for training and 2) the rest 50 images for testing. The DIV2K validation images are used for validation. Training images are augmented by down-scaling with factors of 2, 3 and 4. The images are then cropped into 63×63 sub-images, forming our training set and testing set with 249,344 and 3,584 sub-images, respectively.</p><p>We employ mixed distortions for agent training and testing. Specifically, a sequence of Gaussian blur, Gaussian noise and JPEG compression is added to the training images with random levels. The standard deviations of Gaussian blur and Gaussian noise are uniformly distributed in [0, 5] and [0, 50], respectively, while the quality of JPEG compression is subjected to a uniform distribution in <ref type="bibr" target="#b9">[10,</ref><ref type="bibr">100]</ref>. All mixed distortions are categorized into five groups, as shown in <ref type="figure" target="#fig_3">Figure 4</ref>, from extremely mild to extremely severe. We discard two extreme cases that are either too easy or too hard for restoration. Training and testing are performed on the moderate group. To further test the generalization ablity, we also perform testing on mild and severe groups that are not included in the training data. Comparisons. We compare RL-Restore with DnCNN <ref type="bibr" target="#b43">[44]</ref> and VDSR <ref type="bibr" target="#b18">[19]</ref>, which are the state-of-the-art models for image restoration and super-resolution, and both of them are capable of handling multiple degradations. DnCNN and VDSR share similar structure with 20 convolutional layers while batch normalization is adopted in DnCNN. Their pa-  rameters are over 0.6 million (shown in <ref type="table" target="#tab_4">Table 2</ref>). In contrast, the complexity of RL-Restore (including the agent and the selected tools <ref type="bibr" target="#b1">2</ref> ) is only about a third of those for DnCNN and VDSR, with 0.19 million parameters in total. A much larger gap can be observed on computations when we refer to the number of multiplications on a 63 × 63 input image. For a fair comparison with RL-Restore, we shrink VDSR from 20 to 15 layers (42 filters in each layer) to form a new baseline, named VDSR-s, which bares similar complexity as RL-Restore. Following the same training strategy in <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b43">44]</ref>, we first train the baselines with the agent training set. Then we fine-tune the models with both the agent and tools training sets till convergence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Quantitative Evaluation on Synthetic Dataset</head><p>We present quantitative results of RL-Restore and baselines on different test sets in <ref type="table">Table 3</ref>. The results on mild and moderate sets show that our approach is apparently superior to VDSR-s while comparable to DnCNN and VDSR, demonstrating that the proposed RL-Restore could achieve the same performance as a deep CNN with much lower complexity. It is worth noting that on severe test set RLRestore surpasses DnCNN and VDSR by 0.2 dB and 0.3 dB, respectively, where the distortions are not observed in the training data. It indicates that our RL-based approach is more flexible in handling unseen distortions, while it is more difficult for a fixed CNN to generalize towards unseen cases. Visual results are shown in <ref type="figure" target="#fig_4">Figure 5</ref>.</p><p>To examine the internal behaviors of RL-Restore , we analyze the frequency of the tool selection at each step. Results are shown in <ref type="figure" target="#fig_5">Figure 6</ref>, where 0-12 on x-axis represent the 12 tools in <ref type="table" target="#tab_3">Table 1</ref> and 13 is the stopping action. As can  be observed on the three charts, the tool selection is diverse, and all tools are utilized in a different ratio. Specifically, deblurring and denoising tools are preferred at the first step, while denoising and de-JPEG tools are frequently chosen at the second step. The last step tends to stop the agent with a large probablity -47%. Interestingly, when testing on unseen data, the ratios of stopping action at the last step are 60% and 38% on mild and severe test sets, respectively, which indicates that more severe and complex distortions require a longer toolchain to restore.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Evaluation on Real-World Images</head><p>In real-world cases, images are always distorted by a variety of complex and mixed distortions with unknown degradation kernels, making restoration tasks extremely difficult for current methods. The proposed RL-based method may shed some light on possible solutions. When realworld distortions (e.g., slight out-of-focus blur, exposure noise and JPEG artifacts) are close to the training data, the proposed RL-Restore can be easily generalized to these problems and performs better than a single CNN model. Examples are shown in <ref type="figure" target="#fig_6">Figure 7</ref>, where the input images, combined with different distortions (e.g., blurring, noise, compression), are captured by smart phones. We directly apply the well-trained RL-Restore and VDSR on those realworld images, without further fine-tuning on the test data. It is obvious that our approach, benefiting from flexible toolchains, is more effective for restoring real-world images. Specifically, <ref type="figure" target="#fig_6">Figure 7(a, c)</ref> show that RL-Restore can successfully deal with severe artifacts caused by exposure and compression, while <ref type="figure" target="#fig_6">Figure 7</ref>(b, d, e) demonstrate that our approach is able to restore a mix of blur and complex noise. It is also worth noting that the stopping action is selected by the agent when it is confident in the restored quality ( <ref type="figure" target="#fig_6">Figure 7(c, d, e)</ref>). We believe that the proposed framework has the potential to deal with more complex real distortions with more powerful restoration tools.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Ablation Studies</head><p>In this section, we investigate different settings of the proposed RL-Restore, and give some insights on the choice of hyper-parameters. To better distinguish the effectiveness of each factor, we exclude the joint training strategy on all the experiments below. Toolbox Size and Toolchain Length. The capacity of toolbox and the number of restoring actions dominate the restoration performance. We alternatively vary the length of toolchain and the size of toolbox. As observed in <ref type="table" target="#tab_5">Table 4</ref>, RL-Restore performs well with N = 12 and T max = 3 under the current problem settings. Fewer tools and a shorter toolchain will decrease the performance. More tools and a longer toolchain achieve comparable performance. We attribute this phenomenon to the increased difficulty in learning more complex toolchains. It is worth pointing out that a toolchain with a length of two has a comparable PSNR as longer toolchains on the mild test set, indicating that slight distortions require fewer steps to restore. Tools Training. As discussed in Sec. 3.1, we propose two training strategies for tools to eliminate the complex artifacts in middle states: 1) Add slight noise and compression in the tools training data. 2) Perform joint training with the agent. Control experiments are conducted as in <ref type="table">Table 5</ref>, where the 'Original' setting represents the baseline, the '+Noise' adopts the first strategy and the '+Joint' uses both of them. It is obvious that adding noise to the training data successfully improves the PSNR by 0.2 dB, and joint training further pushes another 0.2 dB on all test sets, demonstrating the effectiveness of both training strategies. Reward Function. We experimentally find that the choice of reward functions can largely influence the performance. Besides the proposed stepwise reward based on PSNR, we also investigate other reward functions: 1) stepwise SSIM <ref type="bibr" target="#b39">[40]</ref> where the reward is the SSIM gain at each step; 2) final PSNR where the reward is the final PSNR gain given at the last step; 3) final MSE as in <ref type="bibr" target="#b3">[4]</ref> where the reward is the negative MSE in the end. We adaptively adjust the learning rate for different rewards. As can be seen in <ref type="table">Table 6</ref>, the stepwise SSIM, which performs the worst on PSNR met- ric, seems not to be a good choice for reward. The final MSE is slightly better on PSNR, but performs the worst on SSIM. The final PSNR achieves similar performance as the proposed stepwise PSNR reward. Nevertheless, we do not claim that PNSR is the best reward, and other evaluation methods are also encouraged for further comparison. Automatic Stopping. The stopping action gives the agent the flexibility to terminate the restoration process when it is confident about the restored results. Thanks to this flexible stopping mechanism, it can prevent the images from over restored and save much computation. To demonstrate its effectiveness, we compare the results with/without the stopping action. As can be observed in <ref type="table">Table 7</ref>, the PSNR values drop around 0.15 dB when removing the stopping action. It is observed that the gap on mild test set is larger than that on other test sets. This is consistent with our experience that slight distortions are easily over restored if the agent does not stop in time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have presented a novel approach for image restoration based on reinforcement learning. Unlike most existing deep learning based methods, in our approach an agent is learned to dynamically select a toolchain to progressively restore an image that is corrupted by complex and mixed distortions. Extensive results on synthetic and real-world images validate the effectiveness of the proposed approach. With its inherent flexibility, the proposed framework can be applied to more challenging restoration tasks or other lowlevel vision problems by developing powerful tools and an appropriate reward.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Different toolchains for image restoration. We perform a preliminary test here. Given two distorted images and the corresponding appropriate toolchains as (c) and (d), we construct other toolchains by rearranging the order (represented by shape) or adjusting the level (represented by color) of the selected tools. The restored results indicate that such minor changes of a toolchain could lead to very different performance.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>First, the choice of the restoration type, level and the processing order all influ- ence the final performance. An example is shown in Fig- ure 2, where the images are corrupted by two different com- binations of distortions. With an appropriate toolchain, as in Figure 2 (c, d), the image quality and the Peak Signal- to-Noise Ratio (PSNR) values are improved sequentially.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>.5 × 10 −5 within 5 × 10 5 iterations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Different levels of distortions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Qualitative comparisons with baselines on synthetic dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. The chosen ratio of tool selection at each step.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Results of real-world images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>has comparable parameters to RL-Restore while (e) has twice more. PSNR values are presented for better comparison.</figDesc><table>a orrupted 
image 
st step of 
RL-Restore 
rd step of 
RL-Restore 
d VDSR-s 
e VDSR 

. dB 
. dB 
. dB 
. dB 
. dB 

. dB 
. dB 
. dB 
. dB 
. dB 

. dB 
. dB 
. dB 
. dB 
. dB 

. dB 
. dB 
. dB 
. dB 
. dB 

Figure 1. (a) shows images corrupted by complex distortions. (b-c) 
depict some chosen steps of the decision process to restore an im-
age by RL-Restore. At each step, a specific tool is selected by the 
agent to improve the image quality. (d-e) are CNN-based results, 
where (d) </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Tools in the toolbox. We consider three types of distortion and various degradation levels. Each tool is either a 3-layer CNN or an 8-layer CNN according to the distortion it targets to solve.</figDesc><table>Distortion Type 
Distortion Level Interval 
CNN 
(Parameters) 
Depth 

Gaussian Blur (σ) 
[0, 1.25], [1.25, 2.5] 
3 
[2.5, 3.75], [3.75, 5] 
8 

Gaussian Noise (σ) 
[0, 12.5], [12.5, 25] 
3 
[25, 37.5], [37.5, 50] 
8 

JPEG Compression (Q) 
[60, 100], [35, 60] 
3 
[20, 35], [10, 20] 
8 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 2 .</head><label>2</label><figDesc>Complexity of baselines and RL-Restore.</figDesc><table>Model 
DnCNN VDSR 
VDSR-s RL-Restore 
Parameters (×10 5 ) 
6.69 
6.67 
2.09 
1.96 
Computations (×10 9 ) 
2.66 
2.65 
0.828 
0.474 

Table 3. Quantitative results on DIV2K test sets. 

Test Set 
Mild (unseen) 
Moderate 
Severe (unseen) 
Metric 
PSNR 
SSIM 
PSNR 
SSIM 
PSNR 
SSIM 
DnCNN 
28.03 
0.6503 
26.42 
0.5554 
24.99 
0.4658 
VDSR 
28.04 
0.6496 
26.40 
0.5544 
24.90 
0.4629 
VDSR-s 
27.69 
0.6383 
25.99 
0.5399 
24.50 
0.4505 
RL-Restore 
28.04 
0.6498 
26.45 
0.5587 
25.20 
0.4777 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 .</head><label>4</label><figDesc>Ablation study on toolbox's size and toolchain's length.</figDesc><table>Test Set 
Mild (unseen) 
Moderate 
Severe (unseen) 
Metric 
PSNR 
SSIM 
PSNR 
SSIM 
PSNR 
SSIM 

Size 

6 
27.57 
0.6241 
25.72 
0.5142 
24.27 
0.4291 
12 27.78 
0.6372 
26.20 
0.5441 
24.97 
0.4643 
18 27.77 
0.6361 
26.17 
0.5417 
24.93 
0.4650 

Length 

2 
27.74 
0.6264 
25.99 
0.5233 
24.63 
0.4444 
3 
27.78 
0.6372 
26.20 
0.5441 
24.97 
0.4643 
4 
27.73 
0.6368 
26.20 
0.5450 
24.98 
0.4663 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 .Table 6 .</head><label>56</label><figDesc>Ablation study on tools training.Ablation study on reward functions.Table 7. Ablation study on stopping action.</figDesc><table>Test Set 
Mild (unseen) 
Moderate 
Severe (unseen) 
Metric 
PSNR 
SSIM 
PSNR 
SSIM 
PSNR 
SSIM 
+Joint 
28.04 
0.6498 
26.45 
0.5587 
25.20 
0.4777 
+Noise 
27.78 
0.6372 
26.20 
0.5441 
24.97 
0.4643 
Original 
27.52 
0.6027 
25.91 
0.5119 
24.81 
0.4490 

Test Set 
Mild (unseen) 
Moderate 
Severe (unseen) 
Metric 
PSNR 
SSIM 
PSNR 
SSIM 
PSNR 
SSIM 
Step. PSNR 27.78 
0.6372 
26.20 
0.5441 
24.97 
0.4643 
Step. SSIM 
26.58 
0.6341 
25.20 
0.5368 
24.18 
0.4579 
Final PSNR 27.71 
0.6350 
26.11 
0.5417 
24.86 
0.4656 
Final MSE 
27.14 
0.6009 
25.66 
0.5166 
24.55 
0.4470 

Test Set 
Mild (unseen) 
Moderate 
Severe (unseen) 
Metric 
PSNR 
SSIM 
PSNR 
SSIM 
PSNR 
SSIM 
w/ Stopping 
27.78 
0.6372 
26.20 
0.5441 
24.97 
0.4643 
w/o Stopping 27.61 
0.6284 
26.08 
0.5351 
24.85 
0.4589 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Codes and data are available at http://mmlab.ie.cuhk.edu. hk/projects/RL-Restore/</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">The complexity of toolchain is calculated under the assumption that each tool is chosen with equal probabilities and the stopping action is ignored. We do not adopt batch normalization in any model.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement. This work is supported by SenseTime Group Limited and the General Research Fund sponsored by the Research Grants Council of the Hong Kong SAR (CUHK 14241716, 14224316. 14209217).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Ntire 2017 challenge on single image super-resolution: Dataset and study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Agustsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Timofte</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Multiple object recognition with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Designing neural network architectures using reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Naik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Raskar</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Attention-aware face hallucination via deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Compressing neural networks with the hashing trick</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wilson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">On learning optimized reaction diffusion processes for effective image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Compression artifacts reduction by a deep convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TPAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Building dual-domain representations for compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One-to-many network for visually pleasing compression artifacts reduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Chao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04994</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep compression: Compressing deep neural networks with pruning, trained quantization and huffman coding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Mao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">J</forename><surname>Dally</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Deep recurrent q-learning for partially observable mdps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Stone</surname></persName>
		</author>
		<idno>abs/1507.06527</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Merge or not? learning to group faces via imitation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning policies for adaptive tracking with deep feature cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Depth map superresolution by deep multi-scale guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-W</forename><surname>Hui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Accurate image superresolution using very deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Deeply-recursive convolutional network for image super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K. Mu</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep laplacian pyramid networks for fast and accurate superresolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-B</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ahuja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cunningham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Acosta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Non-local color image denoising with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lefkimmiatis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep variation-structured reinforcement learning for visual relationship and attribute detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Continuous control with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Hunt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Erez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tassa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Reinforcement learning for robots using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Lin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1993" />
		</imprint>
		<respStmt>
			<orgName>Fujitsu Laboratories Ltd</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Learning dynamic hierarchical models for anytime scene labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Recurrent models of visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">G</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Deep multi-scale convolutional neural network for dynamic scene deblurring</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Deep reinforcement learning-based image captioning with embedding reward</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Lv</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Mastering the game of Go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Van Den Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Mastering the game of Go without human knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bolton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">550</biblScope>
			<biblScope unit="issue">7676</biblScope>
			<biblScope unit="page" from="354" to="359" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Learning a convolutional neural network for non-uniform motion blur removal</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Image super-resolution via deep recursive residual network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<monogr>
		<title level="m" type="main">Memnet: A persistent memory network for image restoration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Multi-armed bandit algorithms and empirical evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Vermorel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mohri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECML</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Recovering realistic texture in image super-resolution by deep spatial feature transform</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TIP</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">D3: Deep dual-domain based fast restoration of JPEG-compressed images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">S</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Inverse kernels for fast spatial deconvolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Actiondecision networks for visual tracking with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Choi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<monogr>
		<title level="m" type="main">Beyond a gaussian denoiser: Residual learning of deep cnn for image denoising</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Meng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Neural architecture search with reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zoph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
