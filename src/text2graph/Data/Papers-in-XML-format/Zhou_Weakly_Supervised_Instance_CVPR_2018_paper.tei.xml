<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:16+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Weakly Supervised Instance Segmentation using Class Peak Response</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yanzhao</forename><surname>Zhou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Zhu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qixiang</forename><surname>Ye</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Qiu</surname></persName>
							<email>qiang.qiu@duke.edu</email>
							<affiliation key="aff1">
								<orgName type="institution">Duke University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jianbin</forename><surname>Jiao</surname></persName>
							<email>jiaojb@ucas.ac.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Chinese Academy of Sciences</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Weakly Supervised Instance Segmentation using Class Peak Response</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Most contemporary methods of semantic segmentation rely on large-scale dense annotations for training deep models; however, annotating pixel-level masks is expensive and labor-intensive <ref type="bibr" target="#b17">[18]</ref>. In contrast, image-level annotations, i.e., presence or absence of object categories in an image, are much cheaper and easier to define. This motivates the † Corresponding Authors <ref type="bibr" target="#b0">1</ref> Source code is publicly available at yzhou.work/PRM  <ref type="figure">Figure 1</ref>: Class peak responses correspond to strong visual cues residing inside each respective instance. Those peaks can be back-propagated and effectively mapped to highly informative regions of each object, which allow instance masks to be extracted. Best viewed in color.</p><p>development of weakly supervised semantic segmentation methods, which use image labels to learn convolutional neural networks (CNNs) for class-aware segmentation. Most existing weakly supervised semantic segmentation methods consider convolutional filters in CNN as object detectors and aggregate the deep feature maps to extract class-aware visual evidence <ref type="bibr" target="#b46">[47,</ref><ref type="bibr" target="#b42">43]</ref>. Typically, pre-trained classification networks are first converted to fully convolutional networks (FCNs) to produce class response maps in a single forward pass. Such class response maps indicate essential image regions used by the network to identify an image class; however, cannot distinguish different object instances from the same category. Therefore, existing weakly supervised semantic segmentation methods cannot be simply generalized to instance-level semantic segmentation <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b11">12]</ref>, which aims to detect all objects in an image as well as predicting precise masks for each instance.</p><p>In this paper, we explore the challenging problem of training CNNs with image-level weak supervision for instance-level semantic segmentation (instance segmentation for short). Specifically, we propose to exploit peaks in a class response map to enable a classification network, e.g., VGGNet, ResNet, for instance mask extraction.</p><p>Local maximums, i.e., peaks, in a class response map typically correspond to strong visual cues residing inside an instance, <ref type="figure">Fig. 1</ref>. Motivated by such observation, we first design a process to stimulate, during the training stage, peaks to emerge from a class response map. At the inference stage, the emerged peaks are back-propagated and effectively mapped to highly informative regions of each object instance, such as instance boundaries. The above maps generated from class peak responses are referred to as Peak Response Maps (PRMs). As shown in <ref type="figure">Fig. 1</ref>, PRMs serve as an instance-level representation, which specifies both spatial layouts and fine-detailed boundaries of each object; thus allows instance masks to be extracted even with some offthe-shelf methods <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b37">38,</ref><ref type="bibr" target="#b26">27]</ref>.</p><p>Compared with many fully supervised approaches that typically use complex frameworks including conditional random fields (CRF) <ref type="bibr" target="#b45">[46,</ref><ref type="bibr" target="#b44">45]</ref>, recurrent neural networks (RNN) <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b31">32]</ref>, or template matching <ref type="bibr" target="#b36">[37]</ref>, to handle instance extraction; our approach is simple yet effective. It is compatible with any modern network architectures and can be trained using standard classification settings, e.g., image class labels and cross entropy loss, with negligible computational overhead. Thanks to its training efficiency, our method is well suited for application to large-scale data.</p><p>To summarize, the main contributions of this paper are:</p><p>• We observe that peaks in class response maps typically correspond to strong visual cues residing inside each respective instance, and such simple observation leads to an effective weakly supervised instance segmentation technique.</p><p>• We propose to exploit class peak responses to enable a classification network for instance mask extraction. We first stimulate peaks to emerge from a class response map and then back-propagate them to map to highly informative regions of each object instance, such as instance boundaries.</p><p>• We implement the proposed method in popular CNNs, e.g., VGG16 and ResNet50, and show top performance on multiple benchmarks. To the best of our knowledge, we for the first time report results for the challenging image-level supervised instance segmentation task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Weakly supervised semantic segmentation. Semantic segmentation approaches typically require dense annotations in the training phase. Given the inefficiency of pixellevel annotating, previous efforts have explored various alternative weak annotations, e.g., points on instances <ref type="bibr" target="#b0">[1]</ref>, object bounding boxes <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b21">22]</ref>, scribbles <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b41">42]</ref>, and human selected foreground <ref type="bibr" target="#b33">[34]</ref>. Although effective, these Compared to existing weakly supervised methods which aim to obtain a saliency map (middle) for each class, the proposed approach extracts fine-detailed representation (right), including both explicit layouts and boundaries, for each instance (visualized with different colors).</p><p>approaches require significant more human efforts than image-level supervised methods <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b40">41,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b32">33]</ref>. Some works leverage object cues in an unsupervised manner. For examples, graphical models have been used to infer labels for segments <ref type="bibr" target="#b43">[44,</ref><ref type="bibr" target="#b14">15]</ref>, yet their object localization capacity remains limited. External localization network is therefore used to initialize object locations <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b22">23]</ref>, and refining low-resolution CNN planes with pre-generated object segment proposal priors. Previous works usually involve time-consuming training strategies, e.g., repeatedly model learning <ref type="bibr" target="#b39">[40]</ref> or online proposal selection <ref type="bibr" target="#b28">[29,</ref><ref type="bibr" target="#b38">39]</ref>. In this work, instead, we use the standard classification networks to produce class-aware and instance-aware visual cues born with convolutional responses.</p><p>Instance segmentation. Compared with semantic segmentation that seeks to produce class-aware masks, instance segmentation requires to produce, at the same time, instance-aware region labels and fine-detailed segmentation masks and thus is much more challenging. Even with supervision from accurate pixel-level annotations, many instance segmentation approaches resort to additional constraints from precise object bounding boxes. The FCIS approach <ref type="bibr" target="#b15">[16]</ref> combines a segment proposal module <ref type="bibr" target="#b5">[6]</ref> and an object detection system <ref type="bibr" target="#b6">[7]</ref>. Mask R-CNN <ref type="bibr" target="#b11">[12]</ref> fully leverages the precise object bounding boxes generated with a proposal network <ref type="bibr" target="#b30">[31]</ref> to aid the prediction of object masks.</p><p>With strong supervision from pixel-level GT masks, the above approaches have greatly boosted the performance of instance segmentation. However, the problem that how to perform instance segmentation under weak supervision remains open. Khoreva et al. <ref type="bibr" target="#b12">[13]</ref> propose to obtain pseudo ground truth masks from bounding box supervision to alleviate labeling cost. In contrast, we leverage instance-aware visual cues naturally learned with classification networks; thus only image-level annotations are required for training.</p><p>Object prior information. When accurate annota-</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Network Classifier</head><p>Image Peak Response Map tions are unavailable, visual recognition approaches leverage prior information typically to obtain additional visual cues. Object proposal methods that hypothesize object locations and extent are often used in weakly supervised object detection and segmentation to provide object priors. Selective Search <ref type="bibr" target="#b37">[38]</ref> and Edge Boxes <ref type="bibr" target="#b48">[49]</ref> use low-level features like color and edges as cues to produce object candidate windows. Multi-scale Combinatorial Grouping (MCG) <ref type="bibr" target="#b26">[27]</ref> uses low-level contour information, e.g., Structured Edge <ref type="bibr" target="#b7">[8]</ref> or Ultrametric Contour Map <ref type="bibr" target="#b19">[20]</ref>, to extract object proposals, which contain fine-detailed object boundaries that is valuable to instance segmentation. In this paper, we perform instance mask extraction with the help of object priors from MCG proposals.</p><p>Image-level supervised deep activation. With imagelevel supervision only, it is required to aggregate deep responses, i.e., feature maps, of CNNs into global class confidences so that image labels can be used for training. Global max pooling (GMP) <ref type="bibr" target="#b20">[21]</ref> chooses the most discriminative response for each class to generate classification confidence scores, but many other informative regions are discarded. Global average pooling (GAP) <ref type="bibr" target="#b46">[47]</ref> assigns equal importance to all responses, which makes it hard to differentiate foreground and background. The log-sum-exponential (LSE) <ref type="bibr" target="#b34">[35]</ref> provides a smooth combination of GMP and GAP to constrain class-aware object regions. Global rank max-min pooling (GRP) <ref type="bibr" target="#b8">[9]</ref> selects a portion of high-scored pixels as positives and low-scored pixels as negatives to enhance discrimination capacity.</p><p>Existing approaches usually activate deep responses from a global perspective without considering local spatial relevance, which makes it hard to discriminate object instances in an image. Peaks in the convolution response imply a maximal local match between the learned filters and the informative receptive field. In our method, the peak stimulation process aggregates responses from local maximums to enhance the network's localization ability.</p><p>Based on the deep responses, top-down attention methods are proposed to generate refined class saliency maps by exploring visual attention evidence <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b42">43]</ref>. These classaware and instance-agnostic cues can be used in semantic segmentation <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b32">33]</ref> yet is insufficient for instance segmentation, <ref type="figure" target="#fig_0">Fig. 2</ref>. In contrast, our methods provide finedetailed instance-aware cues that are suitable for weakly supervised instance-level problems.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>In this section, we present an image-level supervised instance segmentation technique that utilizes class peak response. CNN classifiers in the fully convolutional manner can produce class response maps, which specify classification confidence at each image location <ref type="bibr" target="#b20">[21]</ref>. Based on our observation that local maximums, i.e., peaks, of class response maps typically correspond to strong visual cues residing inside an instance, we first design an process to stimulate peaks to emerge from a class response map in the network training phase. During the inference phase, emerged peaks are back-propagated to generate maps that highlight informative regions for each object, referred to as Peak Response Maps (PRMs). PRMs provide a fine-detailed separate representation for each instance, which are further exploited to retrieve instance masks from object segment proposals off-the-shelf, <ref type="figure" target="#fig_1">Fig. 3</ref>. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully Convolutional Architecture</head><p>By simply removing the global pooling layer and adapting fully connected layers to 1x1 convolution layers, modern CNN classifiers can be seamlessly converted to fully convolutional networks (FCNs) <ref type="bibr" target="#b18">[19]</ref> that naturally preserve spatial information throughout the forwarding. The converted network outputs class response maps with a single forward pass; therefore are suitable for spatial predictions. In this work, networks are converted to FCN first.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Peak Stimulation</head><p>To stimulate peaks to emerge from class response maps, we construct a peak stimulation layer, to be inserted after the top layer, <ref type="figure" target="#fig_1">Fig. 3</ref>. Consider a standard network, let M ∈ R C×H×W denotes the class response maps of the top convolutional layer, where C is the number of classes, and H ×W denotes the spatial size of the response maps. Therefore, the input of the peak stimulation layer is M and the output is class-wise confidence scores s ∈ R C . Peaks of the c-th response map M c are defined to be the local maximums within a window size of r 2 , and the location of peaks are denoted as</p><formula xml:id="formula_0">P c = {(i 1 , j 1 ), (i 2 , j 2 ), ..., (i N c , j N c )}, where N</formula><p>c is the number of valid peaks for the c-th class. During the forwarding pass, a sampling kernel G c ∈ R H×W is generated for computing the classification confidence score of the c-th object category. Each kernel element at the location (x, y) can be accessed with G c x,y . Without loss of generality, the kernel is formed as</p><formula xml:id="formula_1">G c x,y = N c k=1 f (x − i k , y − j k ),<label>(1)</label></formula><formula xml:id="formula_2">where 0 ≤ x &lt; H, 0 ≤ y &lt; W , (i k , j k )</formula><p>is the coordinate of the k-th peak, and f is a sampling function. In our settings, f is a Dirac delta function for aggregating features from the peaks only; therefore the confidence score of the c-th category s c is then computed by the convolution between the class response map M c and sampling kernel G c , as <ref type="bibr" target="#b1">2</ref> The region radius r for peak finding is set to 3 in all our experiments.</p><formula xml:id="formula_3">s c = M c * G c = 1 N c N c k=1 M c i k ,j k .<label>(2)</label></formula><p>It can be seen from Eq. 2 that the network uses peaks only to make the final decision; naturally, during the backward pass, the gradient is apportioned by G c to all the peak locations, as</p><formula xml:id="formula_4">δ c = 1 N c · ∂L ∂s c · G c ,<label>(3)</label></formula><p>where δ c is the gradient for the c-th channel of the top convolutional layer and L is the classification loss.</p><p>From the perspective of model learning, the class response maps are computed by the dense sampling of all receptive fields (RFs), in which most of RFs are negative samples that do not contain valid instances. Eq. 3 indicates that in contrast to conventional networks which unconditionally learn from the extreme foreground-background imbalance set, peak stimulation forces the learning on a sparse set of informative RFs (potential positives and hard negatives) estimated via class peak responses, thus prevents the vast number of easy negatives from overwhelming the learned representation during training, <ref type="figure" target="#fig_2">Fig. 4 (right)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Peak Back-propagation</head><p>We propose a probability back-propagation process for peaks to further generate the fine-detailed and instanceaware representation, i.e., Peak Response Map. In contrast to previous top-down attention models <ref type="bibr" target="#b42">[43,</ref><ref type="bibr" target="#b35">36]</ref>, which seek the most relevant neurons of an output category to generate class-aware attention maps, our formulation explicitly considers the receptive field and can extract instance-aware visual cues from the specific spatial locations, i.e., class peak responses. The peak back-propagation can be interpreted as a procedure that a walker starts from the peak (top layer) and walk randomly to the bottom layer. The top-down relevance of each location in the bottom layer is then formulated as its probability of being visited by the walker.</p><p>Consider a convolution layer that has a single filter W ∈ R kH×kW for mathematical simplification, the input and output feature maps are denoted as U and V , where each spatial locations can be accessed by U ij and V pq respectively. The visiting probability P (U ij ) can be obtained by P (V pq ) and the transition probability between two maps, as</p><formula xml:id="formula_5">P (U ij ) = i+ kH 2 p=i− kH 2 j+ kW 2 q=j− kW 2 P (U ij |V pq ) × P (V pq ),<label>(4)</label></formula><p>where the transition probability is defined as</p><formula xml:id="formula_6">P (U ij |V pq ) = Z pq ×Û ij W + (i−p)(j−q) .<label>(5)</label></formula><p>U ij is the bottom-up activation (computed in the forward pass) at the location (i, j) of U , W + = ReLU (W ), which  <ref type="figure">Figure 5</ref>: Peak back-propagation process maps class peak responses to fine detailed visual cues residing inside each object, i.e., Peak Response Maps (PRMs), enabling the instance-level masks to be extracted. Best viewed in color.</p><p>discards negative connections, and Z pq is a normalization factor to guarantee p,q P (U ij |V pq ) = 1. Note in most modern CNNs that adopt ReLU as transfer function, negative weights have no positive effects in enhancing the output response, thus are excluded from propagation. Other commonly used intermediate layers, e.g., the average pooling and max-pooling layers, are regarded as the same type of layers that perform an affine transform of the input <ref type="bibr" target="#b42">[43]</ref>; thus the corresponding back-propagation can be modeled in the same way of convolution layers.</p><p>With the probability propagation defined by Eq. 4 and Eq. 5, we can localize most relevant spatial locations for each class peak response in a top-down fashion, to generate fine-detailed instance-aware visual cues, referred to as Peak Response Map, <ref type="figure">Fig. 5</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Weakly Supervised Instance Segmentation</head><p>We further leverage the instance-aware cues of PRMs to perform challenging instance segmentation tasks. Specifically, we propose a simple yet effective strategy to predict mask for each object instance by combining instanceaware cues from PRMs, class-aware cues from class response maps, and spatial continuity priors from object proposals off-the-shelf <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b19">20]</ref>.</p><p>We retrieve instance segmentation masks from a proposal gallery, <ref type="figure" target="#fig_1">Fig. 3</ref>, with the metric,</p><formula xml:id="formula_7">Score = α · R * S instance-aware + R * Ŝ boundary-aware − β · Q * S class-aware ,<label>(6)</label></formula><p>where R is the PRM corresponds to a class peak response,Ŝ is the contour mask of the proposal S computed by morphological gradient, and Q is the background mask obtained by the class response map and a bias (based on the mean value of the map). The class independent free parameters α and β are selected on the validation set.</p><p>In Eq. 6, the instance-aware term encourages proposal to maximize the overlap with PRM, while the boundaryaware term leverages the fine-detailed boundary information within the PRM to select proposal with a similar shape. Furthermore, the class-aware term uses class response map to suppress class-irrelevant regions. The effects of three terms are ablation studied in Sec. 4.3.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Segment Instances via Class Peak Response</head><p>Input: A test image I, segment proposals S, and a network trained with peak stimulation. Output: Instance segmentation prediction set A 1: Initialize instance prediction set A = ∅; 2: Forward I to get class response maps M ; <ref type="bibr">3:</ref> for map M k of the k-th class in M do</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4:</head><p>Detect peaks P k i and add to P, Sec. 3.2; 5: end for 6: for peak P for proposal S j in S do <ref type="bibr">9:</ref> Compute score using R and M k , Eq. 6; 10:</p><p>end for</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>11:</head><p>Add top-ranked proposal and label (S * , k) to A; 12: end for 13: Do Non-Maximum Suppression (NMS) over A.</p><p>The overall algorithm for weakly supervised instance segmentation is specified in Alg. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We implement the proposed method using state-of-theart CNN architectures, including VGG16 and ResNet50, and evaluate it on several benchmarks. In Sec. 4.1, we perform a detailed analysis of the peak stimulation and backpropagation process, to show that the proposed technique can generate accurate object localization and high-quality instance-aware cues. In Sec. 4.2, on weakly supervised semantic segmentation, the ability of PRMs to extract classaware masks with the help of segment proposals is shown. In Sec. 4.3, we for the first time report results for challenging image-level supervised instance segmentation. Ablation study and upper bound analysis are further performed to demonstrate the effectiveness and potential of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Peak Response Analysis</head><p>Pointwise localization. A pointwise object localization metric <ref type="bibr" target="#b20">[21]</ref> is used to evaluate the localization ability of class peak responses and effectiveness of peak stimulation. We first upsample the class response maps to the size of the image via bilinear interpolation. if the coordinate of the maximum class peak response falls into a ground truth bounding box of the same category, we count a true positive. We fine-tune ResNet50 equipped with/without peak stimulation on the training set of PASCAL VOC 2012 <ref type="bibr" target="#b10">[11]</ref> as well as MS COCO 2014 <ref type="bibr" target="#b17">[18]</ref>, and report performances on the validation set, Tab. 1. The results show that class peak responses correspond to visual cues of objects and can be used to localize objects. Our full approach shows top performance against state-of-the-arts and outperforms the baseline (w/o stimulation) by a large margin, which indicates the stimulation process can lead the network to discover better visual cues correspond to valid instances.</p><p>Quality of peak response maps. To evaluate the quality of extracted instance-aware cues, we measure the correlation between a Peak Response Map (PRM) R and a GT mask G with R⊙G R , which indicates the ability of the PRM to discover visual cues residing inside the instance. For each PRM, we define its score to be the largest correlation with GT masks of the same class. Thus, a score of 0 indicates that the corresponding PRM does not locate any valid object region, while a score of 1 implies the PRM perfectly distinguishes the visual cues of an instance from the background. PRMs with a score higher than 0.5 are considered as true positives. On VOC 2012, we use classification data to train ResNet50 equipped with response aggregation strategies from different methods, and evaluate the quality of resulting PRMs on the validation set of the segmentation data in terms of mAP, Tab. 2. Peak stimulation forces networks to learn an explicit representation from informative receptive fields; thus obtaining higher quality of PRMs.</p><p>We perform statistical analysis on the relationship between the PRM quality and the crowding level of images, <ref type="figure">Fig. 6 (left)</ref>. On average, the energy of PRMs that falls into an instance reaches 78% for images with a single object, and 67% for images with 2-5 objects. Surprisingly, even for crowded scenes with more than six objects, the instances collect more energy than the background on average, which shows that the instance-aware visual cues from PRMs are of high quality. We further analyze the impact of object size, <ref type="figure">Fig. 6 (right)</ref>, and results show that PRMs can localize fine-detailed evidence from common size objects.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Response Aggregation Strategy mAP CAM <ref type="bibr" target="#b46">[47]</ref> Global Average Pooling 55.7 DeepMIL <ref type="bibr" target="#b20">[21]</ref> Global Max Pooling 60.9 WILDCAT <ref type="bibr" target="#b8">[9]</ref> Global Max-Min Pooling 62.4 PRM (Ours) Peak Stimulation 64.0  43.7 CRF post-processing SEC <ref type="bibr" target="#b13">[14]</ref> 50.7 CRF as boundary loss Check mask <ref type="bibr" target="#b33">[34]</ref> 51.5 CRF &amp; Human in the loop Combining <ref type="bibr" target="#b32">[33]</ref> 52.8 CRF as RNN PRM (Ours) †</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>53.4</head><p>Object segment proposals <ref type="table">Table 3</ref>: Weakly supervised semantic segmentation results on VOC 2012 val. set in terms of the mean IoU (%). Mark † indicates methods that introduce negligible training costs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weakly Supervised Semantic Segmentation</head><p>Experiments above shows that the PRMs correspond to accurate instance "seeds" while another challenging thing is to expand each seed into full object segmentation. We evaluate the ResNet50 model equipped with peak stimulation on the weakly supervised semantic segmentation task, which requires assigning objects from the same categories as the same segmentation labels. On the validation set of VOC 2012 segmentation data, We merge the instance segmentation masks of the same class to produce semantic segmentation predictions. The performance is measured regarding pixel intersection-over-union averaged across 20+1 classes (20 object categories and background).</p><p>Instead of using time-consuming training strategies <ref type="bibr" target="#b32">[33]</ref>, or additional supervisions <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b33">34]</ref>, our method trains models using image-level labels and standard classification settings, and reports competitive results, on weakly supervised semantic segmentation without CRF post-processing, Tab. 3. <ref type="figure" target="#fig_5">Fig. 7</ref> shows examples of predictions in different scenarios. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Weakly Supervised Instance Segmentation</head><p>With the proposed technique, we perform instance segmentation on the PASCAL VOC 2012 segmentation set with ResNet50 and VGG16 models trained on the classification set. To the best of our knowledge, this is the first work reporting results for image-level supervised instance segmentation. We construct several baselines based on object bounding boxes obtained from ground truth and weakly supervised localization methods <ref type="bibr" target="#b38">[39,</ref><ref type="bibr" target="#b46">47,</ref><ref type="bibr" target="#b47">48]</ref>, Tab. 4. With the localized bounding boxes, we set three reasonable mask extraction strategies: (1) Rect. Simply filling in the object boxes with instance labels, (2) Ellipse. Fitting a maximum ellipse inside each box, and (3) MCG. Retrieving an MCG segment proposal of maximum IoU with the bounding box.</p><p>Numerical results. The instance segmentation is evaluated with the mAP r at IoU threshold 0.25, 0.5 and 0.75, and the Average Best Overlap (ABO) <ref type="bibr" target="#b27">[28]</ref> metric is also employed for evaluation to give a different perspective. Tab. 4 shows that our approach significantly outperforms weakly supervised localization techniques that use the same setting, i.e., using image-level labels only for model training. The performance improvement at lower IoU thresholds, e.g., 0.25 and 0.5, shows the effectiveness of peak stimulation for object location, while the improvement at higher IoU threshold, e.g., 0.75, indicates the validity of peak backpropagation for capturing fine-detailed instance cues.</p><p>Compare with the latest state-of-the-art MELM <ref type="bibr" target="#b38">[39]</ref>, which is trained with multi-scale augmentation, online proposal selection, and a specially designed loss, our method is simple yet effective and shows a competitive performance.</p><p>Ablation study. To investigate the contribution of peak stimulation as well as each term in our proposal retrieval metric, we perform instance segmentation based on different backbones in which different factors were omitted. The results are presented in Tab. 5. From the ablation study, we can draw the following conclusions: 1). Peak stimulation process, which stimulates peaks during network train-   ing, is crucial to the instance segmentation performance of our method. 2). The mAP r 0.5 dramatically drops from 26.8% to 13.3% when omitting the instance-aware term, which demonstrates the effectiveness of the well-isolated instance-aware representation generated by our method. 3). Boundary-aware term significantly improves the performance by 2.5% shows our method does extract fine-detailed boundary information of instances. 4). Class-aware cues depress class-irrelevant regions; thus substantially improve the instance segmentation performance of our method.</p><p>Qualitative results. In <ref type="figure" target="#fig_6">Fig. 8</ref>, we illustrate some instance segmentation examples including successful cases and typical failure cases. It can be seen that our approach can produce high quality visual cues and obtain decent instance segmentation results in many challenging scenarios. In the first and second columns, it can distinguish instances when they are closed or occluded with each other. Examples in the third and fourth columns show that it performs well with objects from different scales. In the fifth column, objects from different class are well segmented, which shows that the proposed method can extract both class-discriminative and instance-aware visual cues from classification networks. As is typical for weakly-supervised systems, PRMs can be misled by noisy co-occurrence patterns and sometimes have problems telling the difference between object parts and multiple objects. We address this  problem with a proposal retrieval step; nevertheless, the performance remains limited by proposal quality. Upper bound analysis. To explore the upper bound of our method, we construct different proposal galleries, Tab. 6. First, we mix GT masks into MCG proposals to get a gallery with 100% recall, and the results show that the capability of our method (image-level supervised) to retrieve proposals is comparable to GT bbox (26.9% vs. 29.2%). Next, we use GT masks as a perfect proposal gallery (note that GT bbox still fails in highly occlusion cases) to evaluate the instance localization ability of PRMs. Our result further boosts to 73.3% and outperforms SPN by a large margin, demonstrating the potential of the proposed technique on video/RGB-D applications where rich information can be exploited to generate proposals of high quality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we propose a simple yet effective technique to enable classification networks for instance mask extraction. Based on class peak responses, the peak stimulation shows effective to reinforce object localization, while the peak back-propagation extracts fine-detailed visual cues for each instance. We show top results for pointwise localization as well as weakly supervised semantic segmentation and, to the best of our knowledge, for the first time report results for image-level supervised instance segmentation. The underlying fact is that instance-aware cues are naturally learned by convolutional filters and encoded in hierarchical response maps. To discover these cues provides fresh insights for weakly supervised instance-level problems.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Compared to existing weakly supervised methods which aim to obtain a saliency map (middle) for each class, the proposed approach extracts fine-detailed representation (right), including both explicit layouts and boundaries, for each instance (visualized with different colors).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The generation and utilization of Peak Response Maps (PRMs). A stimulation procedure selectively activates strong visual cues residing inside each object into class peak responses. A back-propagation process further extracts fine details of each instance from the resulting peaks. Finally, class-aware cues, instance-aware cues, and object priors from proposals are considered together to predict instance masks. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: With peak stimulation, multiple instances can be better distinguished on the class response map (middle). The learned representations (right) are visualized by activation maximization [10]. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>2 :Figure 6 :</head><label>26</label><figDesc>Figure 6: Statistical analysis of the effect of the number and size of objects on the quality of Peak Response Maps. Method mIoU Comments MIL+ILP+SP-seg † [26] 42.0 Object segment proposals WILDCAT † [9] 43.7 CRF post-processing SEC [14] 50.7 CRF as boundary loss Check mask [34] 51.5 CRF &amp; Human in the loop Combining [33] 52.8 CRF as RNN PRM (Ours)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Examples of predicted semantic segmentations. Different colors indicate different classes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Instance segmentation examples on the PASCAL VOC 2012 val. set. It can be seen that the Peak Response Maps (second row) incorporate fine-detailed instance-aware information, which can be exploited to produce instance-level masks (third row). The last row shows typical failure cases. Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>For each predicted class,</figDesc><table>Method 

VOC 2012 MS COCO 
DeepMIL [21] 
74.5 
41.2 
WSLoc [2] 
79.7 
49.2 
WILDCAT [9] 
82.9 
53.5 
SPN [48] 
82.9 
55.3 
Ours (w/o Peak Stimulation) 
81.5 
53.1 
Ours (full approach) 
85.5 
57.5 

Table 1: Mean Average Precision (mAP%) of pointwise lo-
calization on VOC2012 and COCO2014 val. set. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>Training requires image-level labels and object proposals</figDesc><table>Method 
mAP 

r 
0.25 

mAP 

r 
0.5 

mAP 

r 
0.75 

ABO 

Ground 
Truth 

Rect. 
78.3 
30.2 
4.5 
47.4 
Ellipse 
81.6 
41.1 
6.6 
51.9 
MCG 
69.7 
38.0 
12.3 
53.3 
MELM [39] 

Rect. 
36.0 
14.6 
1.9 
26.4 
Ellipse 
36.8 
19.3 
2.4 
27.5 
MCG 
36.9 
22.9 
8.4 
32.9 
Training requires only image-level labels 

CAM [47] 

Rect. 
18.7 
2.5 
0.1 
18.9 
Ellipse 
22.8 
3.9 
0.1 
20.8 
MCG 
20.4 
7.8 
2.5 
23.0 

SPN [48] 

Rect. 
29.2 
5.2 
0.3 
23.0 
Ellipse 
32.0 
6.1 
0.3 
24.0 
MCG 
26.4 
12.7 
4.4 
27.1 
PRM (Ours) 
44.3 
26.8 
9.0 
37.6 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Weakly supervised instance segmentation results on the PASCAL VOC 2012 val. set in terms of mean aver- age precision (mAP%) and Average Best Overlap (ABO).22.8 13.3 16.5 24.3 26.8 11.9 22.0</figDesc><table>ResNet50 
VGG16 
Peak Stimulation 
Instance-aware term 
Class-aware term 
Boundary-aware term 
mAP 

r 
0.5 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Ablation study on the PASCAL VOC2012 val. set based on different network backbones.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Comparison of instance segmentation results (mAP r 0.75 ) on the PASCAL VOC 2012 val. set.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors are very grateful for support by the NSFC grant 61771447 / 61671427, BMSTC, and NSF.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Whats the point: Semantic segmentation with point supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bearman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Weakly supervised localization using deep feature maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">J</forename><surname>Bency</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kwon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karthikeyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">S</forename><surname>Manjunath</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="714" to="731" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Interactive graph cuts for optimal boundary &amp; region segmentation of objects in nd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Boykov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-P</forename><surname>Jolly</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2001" />
			<biblScope unit="page" from="105" to="112" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Look and think twice: Capturing top-down visual attention with feedback convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2956" to="2964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Boxsup: Exploiting bounding boxes to supervise convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1635" to="1643" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Instance-aware semantic segmentation via multi-task network cascades</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3150" to="3158" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">R-fcn: Object detection via region-based fully convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="379" to="387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Fast edge detection using structured forests</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1558" to="1570" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Wildcat: Weakly supervised learning of deep convnets for image classification, pointwise localization and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Visualizing higher-layer features of a deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1341</biblScope>
		</imprint>
		<respStmt>
			<orgName>University of Montreal</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The pascal visual object classes challenge: A retrospective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M A</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J V</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">111</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="98" to="136" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mask R-Cnn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Corr</surname></persName>
		</author>
		<idno>abs/1703.06870</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Simple does it: Weakly supervised instance and semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khoreva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Seed, expand and constrain: Three principles for weakly-supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kolesnikov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">H</forename><surname>Lampert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Saliency guided dictionary learning for weakly-supervised image parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Gong</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3639" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Fully convolutional instance-aware semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Scribblesup: Scribble-supervised convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3159" to="3167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Microsoft COCO: common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Convolutional oriented boundaries: From image segmentation to high-level tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Maninis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Arbeláez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
		<idno>abs/1701.04658</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Is object localization for free? -weakly-supervised learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Oquab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a DCNN for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Weakly-and semi-supervised learning of a deep convolutional network for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1742" to="1750" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Fully convolutional multi-class multiple instance learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pathak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">From image-level to pixel-level labeling with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1713" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation with convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">O</forename><surname>Pinheiro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Multiscale combinatorial grouping for image segmentation and object proposal generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Arbelaez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Marqués</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Pattern Anal. Mach. Intell</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Boosting object proposals: From pascal to coco</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pont-Tuset</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1546" to="1554" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Augmented feedback in semantic segmentation under image level supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="90" to="105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">End-to-end instance segmentation and counting with recurrent attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09410</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Recurrent instance segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Romera-Paredes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H S</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="312" to="329" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Combining bottom-up, top-down, and smoothness cues for weakly supervised image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Built-in foreground/background prior for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S A</forename><surname>Akbarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Petersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Pronet: Learning to propose object-specific boxes for cascaded neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Nevatia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3485" to="3493" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Modeling visual attention via selective tuning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Tsotsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Culhane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y K</forename><surname>Wai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Nuflo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Artificial intelligence</title>
		<imprint>
			<biblScope unit="volume">78</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="507" to="545" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Pixel-level encoding and depth layering for instance-level semantic labeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uhrig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">German Conference on Pattern Recognition</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="14" to="25" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R R</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">E A</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">W M</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision (IJCV)</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Min-entropy latent model for weakly supervised object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Stc: A simple to complex framework for weakly-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</title>
		<imprint>
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2314" to="2320" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Learning to segment under various forms of weak supervision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3781" to="3790" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Topdown neural attention by excitation backprop</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Brandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="543" to="559" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Weakly supervised semantic segmentation for social images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2718" to="2726" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Instance-level segmentation for autonomous driving with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="669" to="677" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">Monocular object instance segmentation and depth ordering with cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Schwing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision (ICCV)</title>
		<meeting>the IEEE International Conference on Computer Vision (ICCV)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2614" to="2622" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">Learning deep features for discriminative localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">À</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2921" to="2929" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">Soft proposal networks for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jiao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.01829</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
