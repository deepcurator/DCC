<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader>
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:33+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaozhi</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Huimin</forename><surname>Ma</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Electronic Engineering</orgName>
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ji</forename><surname>Wan</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tian</forename><surname>Xia</surname></persName>
							<email>xiatian@baidu.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Baidu Inc</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Multi-View 3D Object Detection Network for Autonomous Driving</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract xml:lang="en">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text>
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D object detection plays an important role in the visual perception system of Autonomous driving cars. Modern self-driving cars are commonly equipped with multiple sensors, such as LIDAR and cameras. Laser scanners have the advantage of accurate depth information while cameras preserve much more detailed semantic information. The fusion of LIDAR point cloud and RGB images should be able to achieve higher performance and safty to self-driving cars.</p><p>The focus of this paper is on 3D object detection utilizing both LIDAR and image data. We aim at highly accurate 3D localization and recognition of objects in the road scene. Recent LIDAR-based methods place 3D windows in 3D voxel grids to score the point cloud <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6]</ref> or apply convolutional networks to the front view point map in a dense box prediction scheme <ref type="bibr" target="#b15">[16]</ref>. Image-based methods <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b2">3]</ref> typically first generate 3D box proposals and then perform region-based recognition using the Fast R-CNN <ref type="bibr" target="#b8">[9]</ref> pipeline. Methods based on LIDAR point cloud usually achieve more accurate 3D locations while imagebased methods have higher accuracy in terms of 2D box evaluation. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b6">7]</ref> combine LIDAR and images for 2D detection by employing early or late fusion schemes. However, for the task of 3D object detection, which is more challenging, a well-designed model is required to make use of the strength of multiple modalities.</p><p>In this paper, we propose a Multi-View 3D object detection network (MV3D) which takes multimodal data as input and predicts the full 3D extent of objects in 3D space. The main idea for utilizing multimodal information is to perform region-based feature fusion. We first propose a multi-view encoding scheme to obtain a compact and effective representation for sparse 3D point cloud. As illustrated in <ref type="figure">Fig. 1</ref>, the multi-view 3D detection network consists of two parts: a 3D Proposal Network and a Regionbased Fusion Network. The 3D proposal network utilizes a bird's eye view representation of point cloud to generate highly accurate 3D candidate boxes. The benefit of 3D object proposals is that it can be projected to any views in 3D space. The multi-view fusion network extracts regionwise features by projecting 3D proposals to the feature maps from mulitple views. We design a deep fusion approach to enable interactions of intermediate layers from different views. Combined with drop-path training <ref type="bibr" target="#b13">[14]</ref> and auxiliary loss, our approach shows superior performance over the early/late fusion scheme. Given the multi-view feature representation, the network performs oriented 3D box regression which predict accurate 3D location, size and orientation of objects in 3D space.</p><p>We evaluate our approach for the tasks of 3D proposal generation, 3D localization, 3D detection and 2D detection on the challenging KITTI <ref type="bibr" target="#b7">[8]</ref> object detection benchmark. Experiments show that our 3D proposals significantly outperforms recent 3D proposal methods 3DOP <ref type="bibr" target="#b3">[4]</ref> and Mono3D <ref type="bibr" target="#b2">[3]</ref>. In particular, with only 300 proposals, we obtain 99.1% and 91% 3D recall at Intersection-overUnion (IoU) threshold of 0.25 and 0.5, respectively. The  <ref type="figure">Figure 1</ref>: Multi-View 3D object detection network (MV3D): The network takes the bird's eye view and front view of LIDAR point cloud as well as an image as input. It first generates 3D object proposals from bird's eye view map and project them to three views. A deep fusion network is used to combine region-wise features obtained via ROI pooling for each view. The fused features are used to jointly predict object class and do oriented 3D box regression.</p><p>LIDAR-based variant of our approach achieves around 25% higher accuracy in 3D localization task and 30% higher 3D Average Precision (AP) in the task of 3D object detection. It also outperforms all other LIDAR-based methods by 14.9% AP for 2D detection on KITTI's hard test set. When combined with images, further improvements are achieved over the LIDAR-based results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We briefly review existing work on 3D object detection from point cloud and images, multimodal fusion methods and 3D object proposals.</p><p>3D Object Detection in Point Cloud. Most existing methods encode 3D point cloud with voxel grid representation. Sliding Shapes <ref type="bibr" target="#b20">[21]</ref> and Vote3D <ref type="bibr" target="#b24">[25]</ref> apply SVM classifers on 3D grids encoded with geometry features. Some recently proposed methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> improve feature representation with 3D convolutions.networks, which, however require expensive computations. In addition to the 3D voxel representation, VeloFCN <ref type="bibr" target="#b15">[16]</ref> projects point cloud to the front view, obtaining a 2D point map. They apply a fully convolutional network on the 2D point map and predict 3D boxes densely from the convolutional feature maps. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11]</ref> investigate volumetric and multi-view representation of point cloud for 3D object classification. In this work, we encode 3D point cloud with multi-view feature maps, enabling region-based representation for multimodal fusion.</p><p>3D Object Detection in Images. 3DVP <ref type="bibr" target="#b26">[27]</ref> introduces 3D voxel patterns and employ a set of ACF detectors to do 2D detection and 3D pose estimation. 3DOP <ref type="bibr" target="#b3">[4]</ref> reconstructs depth from stereo images and uses an energy minimization approach to generate 3D box proposals, which are fed to an R-CNN <ref type="bibr" target="#b8">[9]</ref> pipeline for object recognition. While Mono3D <ref type="bibr" target="#b2">[3]</ref> shares the same pipeline with 3DOP, it generates 3D proposals from monocular images. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> introduces a detailed geometry representation of objects using 3D wireframe models. To incorporate temporal information, some work <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b19">20]</ref> combine structure from motion and ground estimation to lift 2D detection boxes to 3D bounding boxes. Image-based methods usually rely on accurate depth estimation or landmark detection. Our work shows how to incorporate LIDAR point cloud to improve 3D localization.</p><p>Multimodal Fusion Only a few work exist that exploit multiple modalities of data in the context of autonomous driving. <ref type="bibr" target="#b9">[10]</ref> combines images, depth and optical flow using a mixture-of-experts framework for 2D pedestrian detection. <ref type="bibr" target="#b6">[7]</ref> fuses RGB and depth images in the early stage and trains pose-based classifiers for 2D detection. In this paper, we design a deep fusion approach inspired by FractalNet <ref type="bibr" target="#b13">[14]</ref> and Deeply-Fused Net <ref type="bibr" target="#b25">[26]</ref>. In FractalNet, a base module is iteratively repeated to construct a network with exponentially increasing paths. Similarly, <ref type="bibr" target="#b25">[26]</ref> constructs deeply-fused networks by combining shallow and deep subnetworks. Our network differs from them by using the same base network for each column and adding auxiliary paths and losses for regularization.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>3D Object Proposals</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">MV3D Network</head><p>The MV3D network takes a multi-view representation of 3D point cloud and an image as input. It first generates 3D object proposals from the bird's eye view map and deeply fuses multi-view features via region-based representation. The fused features are used for category classification and oriented 3D box regression.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">3D Point Cloud Representation</head><p>Existing work usually encodes 3D LIDAR point cloud into a 3D grid <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b5">6]</ref> or a front view map <ref type="bibr" target="#b15">[16]</ref>. While the 3D grid representation preserves most of the raw information of the point cloud, it usually requires much more complex computation for subsequent feature extraction. We propose a more compact representation by projecting 3D point cloud to the bird's eye view and the front view. <ref type="figure" target="#fig_0">Fig. 2</ref> visualizes the point cloud representation. Bird's Eye View Representation. The bird's eye view representation is encoded by height, intensity and density. We discretize the projected point cloud into a 2D grid with resolution of 0.1m. For each cell, the height feature is computed as the maximum height of the points in the cell. To encode more detailed height information, the point cloud is devided equally into M slices. A height map is computed for each slice, thus we obtain M height maps. The intensity feature is the reflectance value of the point which has the maximum height in each cell. The point cloud density indicates the number of points in each cell. To normalize the feature, it is computed as min(1.0,</p><formula xml:id="formula_0">log(N +1)</formula><p>log <ref type="formula" target="#formula_6">(64)</ref> ), where N is the number of points in the cell. Note that the intensity and density features are computed for the whole point cloud while the height feature is computed for M slices, thus in total the bird's eye view map is encoded as (M +2)-channel features.</p><p>Front View Representation. Front view representation provides complementary information to the bird's eye view representation. As LIDAR point cloud is very sparse, projecting it into the image plane results in a sparse 2D point map. Instead, we project it to a cylinder plane to generate a dense front view map as in <ref type="bibr" target="#b15">[16]</ref>. Given a 3D point p = (x, y, z), its coordinates p f v = (r, c) in the front view map can be computed using</p><formula xml:id="formula_1">c = ⌊atan2(y, x)/∆θ]⌋ r = ⌊atan2(z, x 2 + y 2 )/∆φ⌋,<label>(1)</label></formula><p>where ∆θ and ∆φ are the horizontal and vertical resolution of laser beams, respectively. We encode the front view map with three-channel features, which are height, distance and intensity, as visualized in <ref type="figure" target="#fig_0">Fig. 2</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">3D Proposal Network</head><p>Inspired by Region Proposal Network (RPN) which has become the key component of the state-of-the-art 2D object detectors <ref type="bibr" target="#b17">[18]</ref>, we first design a network to generate 3D object proposals. We use the bird's eye view map as input. In 3D object detection, The bird's eye view map has several advantages over the front view/image plane. First, objects preserve physical sizes when projected to the bird's eye view, thus having small size variance, which is not the case in the front view/image plane. Second, objects in the bird's eye view occupy different space, thus avoiding the occlusion problem. Third, in the road scene, since objects typically lie on the ground plane and have small variance in vertical location, the bird's eye view location is more crucial to obtaining accurate 3D bounding boxes. Therefore, using explicit bird's eye view map as input makes the 3D location prediction more feasible.</p><p>Given a bird's eye view map. the network generates 3D box proposals from a set of 3D prior boxes. Each 3D box is parameterized by (x, y, z, l, w, h), which are the center and size (in meters) of the 3D box in LIDAR coordinate system. For each 3D prior box, the corresponding bird's eye view anchor (x bv , y bv , l bv , w bv ) can be obtained by discretizing (x, y, l, w). We design N 3D prior boxes by clustering ground truth object sizes in the training set. In the case of car detection, (l, w) of prior boxes takes values in {(3.9, 1.6), (1.0, 0.6)}, and the height h is set to 1.56m. By rotating the bird's eye view anchors 90 degrees, we obtain N = 4 prior boxes. (x, y) is the varying positions in the bird's eye view feature map, and z can be computed based on the camera height and object height. We do not do orientation regression in proposal generation, whereas we left it to the next prediction stage. The orientations of 3D boxes are restricted to {0</p><p>• , 90</p><p>• }, which are close to the actual orientations of most road scene objects. This simplification makes training of proposal regression easier.</p><p>With a disretization resolution of 0.1m, object boxes in the bird's eye view only occupy 5∼40 pixels. Detecting such extra-small objects is still a difficult problem for deep networks. One possible solution is to use higher resolution of the input, which, however, will require much more computation. We opt for feature map upsampling as in <ref type="bibr" target="#b0">[1]</ref>. We use 2x bilinear upsampling after the last convolution layer in the proposal network. In our implementation, the front-end convolutions only proceed three pooling operations, i.e., 8x downsampling. Therefore, combined with the 2x deconvolution, the feature map fed to the proposal network is 4x downsampled with respect to the bird's eye view input.</p><p>We do 3D box regression by regressing to t = (∆x, ∆y, ∆z, ∆l, ∆w, ∆h), similarly to RPN <ref type="bibr" target="#b17">[18]</ref>. (∆x, ∆y, ∆z) are the center offsets normalized by anchor sizes, and (∆l, ∆w, ∆h) are computed as ∆s = log sGT sanchor , s ∈ {l, w, h}. we use a multi-task loss to simultaneously classify object/background and do 3D box regression. In particular, we use class-entropy for the "objectness" loss and Smooth ℓ 1 <ref type="bibr" target="#b8">[9]</ref> for the 3D box regression loss. Background anchors are ignored when Since LIDAR point cloud is sparse, which results in many empty anchors, we remove all the empty anchors during both training and testing to reduce computation. This can be achieved by computing an integral image over the point occupancy map.</p><p>For each non-empty anchor at each position of the last convolution feature map, the network generates a 3D box. To reduce redundancy, we apply Non-Maximum Suppression (NMS) on the bird's eye view boxes. Different from <ref type="bibr" target="#b21">[22]</ref>, we did not use 3D NMS because objects should occupy different space on the ground plane. We use IoU threshold of 0.7 for NMS. The top 2000 boxes are kept during training, while in testing, we only use 300 boxes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Region-based Fusion Network</head><p>We design a region-based fusion network to effectively combine features from multiple views and jointly classify object proposals and do oriented 3D box regression.</p><p>Multi-View ROI Pooling. Since features from different views/modalities usually have different resolutions, we employ ROI pooling <ref type="bibr" target="#b8">[9]</ref> for each view to obtain feature vectors of the same length. Given the generated 3D proposals, we can project them to any views in the 3D space. In our case, we project them to three views, i.e., bird's eye view (BV), front view (FV), and the image plane (RGB). Given a 3D proposal p 3D , we obtain ROIs on each view via: where T 3D→v denotes the tranformation functions from the LIDAR coordinate system to the bird's eye view, front view, and the image plane, respectively. Given an input feature map x from the front-end network of each view, we obtain fixed-length features f v via ROI pooling:</p><formula xml:id="formula_2">ROI v = T 3D→v (p 3D ), v ∈ {BV, FV, RGB}<label>(2)</label></formula><formula xml:id="formula_3">f v = R(x, ROI v ), v ∈ {BV, FV, RGB}.<label>(3)</label></formula><p>Deep Fusion. To combine information from different features, prior work usually use early fusion <ref type="bibr" target="#b0">[1]</ref> or late fusion <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b11">12]</ref>. Inspired by <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b25">26]</ref>, we employ a deep fusion approach, which fuses multi-view features hierarchically. A comparison of the architectures of our deep fusion network and early/late fusion networks are shown in <ref type="figure" target="#fig_1">Fig. 3</ref>. For a network that has L layers, early fusion combines features {f v } from multiple views in the input stage:</p><formula xml:id="formula_4">f L = H L (H L−1 (· · · H 1 (f BV ⊕ f F V ⊕ f RGB ))) (4)</formula><p>{H l , l = 1, · · · , L} are feature transformation functions and ⊕ is a join operation (e.g., concatenation, summation). In contrast, late fusion uses seperate subnetworks to learn feature transformation independently and combines their outputs in the prediction stage:</p><formula xml:id="formula_5">f L =(H BV L (· · · H BV 1 (f BV )))⊕ (H F V L (· · · H F V 1 (f F V )))⊕ (H RGB L (· · · H RGB 1 (f RGB )))<label>(5)</label></formula><p>To enable more interactions among features of the intermediate layers from different views, we design the following deep fusion process:</p><formula xml:id="formula_6">f 0 =f BV ⊕ f F V ⊕ f RGB f l =H BV l (f l−1 ) ⊕ H F V l (f l−1 ) ⊕ H RGB l (f l−1 ), ∀l = 1, · · · , L<label>(6)</label></formula><p>We use element-wise mean for the join operation for deep fusion since it is more flexible when combined with droppath training <ref type="bibr" target="#b13">[14]</ref>.</p><p>Oriented 3D Box Regression Given the fusion features of the multi-view network, we regress to oriented 3D boxes from 3D proposals. In particular, the regression targets are the 8 corners of 3D boxes: t = (∆x 0 , · · · , ∆x 7 , ∆y 0 , · · · , ∆y 7 , ∆z 0 , · · · , ∆z 7 ). They are encoded as the corner offsets normalized by the diagonal length of the proposal box. Despite such a 24-D vector representation is redundant in representing an oriented 3D box, we found that this encoding approach works better than the centers and sizes encoding approach. Note that our 3D box regression differs from <ref type="bibr" target="#b21">[22]</ref> which regresses to axisaligned 3D boxes. In our model, the object orientations can be computed from the predicted 3D box corners. We use a multi-task loss to jointly predict object categories and oriented 3D boxes. As in the proposal network, the category loss uses cross-entropy and the 3D box loss uses smooth ℓ 1 . During training, the positive/negative ROIs are determined based on the IoU overlap of brid's eye view boxes. A 3D proposal is considered to be positive if the bird's eye view IoU overlap is above 0.5, and negative otherwise. During inference, we apply NMS on the 3D boxes after 3D bounding box regression. We project the 3D boxes to the bird's eye view to compute their IoU overlap. We use IoU threshold of 0.05 to remove redundant boxes, which ensures objects can not occupy the same space in bird's eye view.</p><p>Network Regularization We employ two approaches to regularize the region-based fusion network: drop-path training <ref type="bibr" target="#b13">[14]</ref> and auxiliary losses. For each iteration, we randomly choose to do global drop-path or local drop-path with a probability of 50%. If global drop-path is chosen, we select a single view from the three views with equal probability. If local drop-path is chosen, paths input to each join node are randomly dropped with 50% probability. We ensure that for each join node at least one input path is kept. To further strengthen the representation capability of each view, we add auxiliary paths and losses to the network. As shown in <ref type="figure" target="#fig_2">Fig. 4</ref>, the auxiliary paths have the same number of layers with the main network. Each layer in the auxiliary paths shares weights with the corresponding layer in the  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Implementation</head><p>Network Architecture. In our multi-view network, each view has the same architecture. The base network is built on the 16-layer VGG net <ref type="bibr" target="#b18">[19]</ref> with the following modifications:</p><p>• Channels are reduced to half of the original network.</p><p>• To handle extra-small objects, we use feature approximation to obtain high-resolution feature map. In particular, we insert a 2x bilinear upsampling layer before feeding the last convolution feature map to the 3D Proposal Network. Similarly, we insert a 4x/4x/2x upsampling layer before the ROI pooling layer for the BV/FV/RGB branch.</p><p>• We remove the 4th pooling operation in the original VGG network, thus the convolution parts of our network proceed 8x downsampling.</p><p>• In the muti-view fusion network, we add an extra fully connected layer f c8 in addition to the original f c6 and f c7 layer. We initialize the parameters by sampling weights from the VGG-16 network pretrained on ImageNet. Despite our network has three branches, the number of parameters is about 75% of the VGG-16 network. The inference time of the network for one image is around 0.36s on a Titan X GPU.</p><p>Input Representation. In the case of KITTI, which provides only annotations for objects in the front view (around 90</p><p>• field of view), we use point cloud in the range of [0, 70.4] × [-40, 40] meters. We also remove points that are out of the image boundaries when projected to the image plane. For bird's eye view, the discretization resolution is set to 0.1m, therefore the bird's eye view input has size of 704×800. Since KITTI uses a 64-beam Velodyne laser scanner, we can obtain a 64×512 map for the front view points. The RGB image is up-scaled so that the shortest size is 500.</p><p>Training. The network is trained in an end-to-end fashion. For each mini-batch we use 1 image and sample 128 ROIs, roughly keeping 25% of the ROIs as positive. We train the network using SGD with a learning rate of 0.001 for 100K iterations. Then we reduce the learning rate to 0.0001 and train another 20K iterations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate our MV3D network on the challenging KITTI object detection benchmark <ref type="bibr" target="#b7">[8]</ref>. The dataset provides 7,481 images for training and 7,518 images for testing. As the test server only evaluates 2D detection, we follow <ref type="bibr" target="#b3">[4]</ref> to split the training data into training set and validation set, each containing roughly half of the whole training data. We conduct 3D box evaluation on the validation set. We focus our experiments on the car category as KITTI provides enough car instances for our deep network based approach. Following the KITTI setting, we do evaluation on three difficulty regimes: easy, moderate and hard.</p><p>Metrics. We evaluate 3D object proposals using 3D box recall as the metric. Different from 2D box recall <ref type="bibr" target="#b12">[13]</ref>, we compute the IoU overlap of two cuboids. Note that the cuboids are not necessary to align with the axes, i.e., they could be oriented 3D boxes. In our evaluation, we set the 3D IoU threshold to 0.25 and 0.5, respectively. For the final 3D detection results, we use two metrics to measure the accuracy of 3D localization and 3D bounding box detection. For 3D localization, we project the 3D boxes to the ground plane (i.e., bird's eye view) to obtain oriented bird's eye   <ref type="table">Table 4</ref>: An ablation study of multi-view features: Peformance are evaluated on KITTI validation set.</p><p>view boxes. We compute Average Precision (AP loc ) for the bird's eye view boxes. For 3D bounding box detection, we also use the Average Precision (AP 3D ) metric to evaluate the full 3D bounding boxes. Note that both the bird's eye view boxes and the 3D boxes are oriented, thus object orientations are implicitly considered in these two metrics. We also evaluate the performance of 2D detection by projecting the 3D boxes to the image plane. Average Preicision (AP 2D ) is also used as the metric. Following the KITTI convention, IoU threshold is set to 0.7 for 2D boxes.</p><p>Baslines. As this work aims at 3D object detection, we mainly compare our approach to LIDAR-based methods VeloFCN <ref type="bibr" target="#b15">[16]</ref>, Vote3Deep <ref type="bibr" target="#b5">[6]</ref> and Vote3D <ref type="bibr" target="#b24">[25]</ref>, as well as image-based methods 3DOP <ref type="bibr" target="#b3">[4]</ref> and Mono3D <ref type="bibr" target="#b2">[3]</ref>. For fair comparison, we focus on two variants of our approach, i.e., the purely LIDAR-based variant which uses bird's eye view and front view as input (BV+FV), and the multimodal variant which combines LIDAR and RGB data (BV+FV+RGB). For 3D box evaluation, we compare with VeloFCN, 3DOP and Mono3D since they provide results on the validation set. For Vote3Deep and Vote3D, which have no results publicly available, we only do comparison on 2D detection on the test set.</p><p>3D Proposal Recall. 3D box recall are shown in <ref type="figure" target="#fig_3">Fig. 5</ref>. We plot recall as a function of IoU threshold using 300 proposals. Our approach significantly outperforms 3DOP <ref type="bibr" target="#b3">[4]</ref> and Mono3D <ref type="bibr" target="#b2">[3]</ref> across all the IoU thresholds. <ref type="figure" target="#fig_3">Fig. 5</ref> also shows 3D recall as a function of the proposal numbers under IoU threshold of 0.25 and 0.5, respectively. Using only 300 proposals, our approach obtains 99.1% recall at IoU threshold of 0.25 and 91% recall at IoU of 0.5. In contrast, when using IoU of 0.5, the maximum recall that 3DOP can achieve is only 73.9%. The large margin suggests the advantage of our LIDAR-based approach over image-based methods.  <ref type="table">Table 5</ref>: 2D detection performance: Average Precision (AP 2D ) (in %) for car category on KITTI test set. Methods in the first group optimize 2D boxes directly while the second group optimize 3D boxes. 3D Localization. We use IoU threshold of 0.5 and 0.7 for 3D localization evaluation. <ref type="table">Table 1</ref> shows AP loc on KITTI validation set. As expected, all LIDAR-based approaches performs better than stereo-based method 3DOP <ref type="bibr" target="#b3">[4]</ref> and monocular method Mono3D <ref type="bibr" target="#b2">[3]</ref>. Among LIDAR-based approaches, our method (BV+FV) outperforms VeloFCN <ref type="bibr" target="#b15">[16]</ref> by ∼25% AP loc under IoU threshold of 0.5. When using IoU=0.7 as the criteria, our improvement is even larger, achieving ∼45% higher AP loc across easy, moderate and hard regimes. By combining with RGB images, our approach is further improved. We visualize the localization results of some examples in <ref type="figure">Fig. 6</ref>. 3D Object Detection. For the 3D overlap criteria, we focus on 3D IoU of 0.5 and 0.7 for LIDAR-based methods. As these IoU thresholds are rather strict for image-based methods, we also use IoU of 0.25 for evaluation. As shown in <ref type="table" target="#tab_3">Table 2</ref>, our "BV+FV" method obtains ∼30% higher AP 3D over VeloFCN when using IoU of 0.5, achieving 87.65% 3DOP <ref type="bibr" target="#b3">[4]</ref> VeloFCN <ref type="bibr" target="#b15">[16]</ref> Ours <ref type="figure">Figure 6</ref>: Qualitative comparisons of 3D detection results: 3D Boxes are projected to the bird's eye view and the images. AP 3D in the moderate setting. With criteria of IoU=0.7, our multimodal approach still achieves 71.29% AP 3D on easy data. In the moderate setting, the best AP 3D that can be achieved by 3DOP using IoU=0.25 is 68.82%, while our approach achieves 89.05% AP 3D using IoU=0.5. Some 3D detectioin results are visualized in <ref type="figure">Fig. 6</ref>.</p><p>Ablation Studies. We first compare our deep fusion network with early/late fusion approaches. As commonly used in literature, the join operation is instantiated with concatenation in the early/late fusion schemes. As shown in Table 3, early and late fusion approaches have very similar performance. Without using auxiliary loss, the deep fusion method achieves ∼0.5% improvement over early and late fusion approaches. Adding auxiliary loss further improves deep fusion network by around 1%.</p><p>To study the contributions of the features from different views, we experiment with different combination of the bird's eye view (BV), the front view (FV), and the RGB image (RGB). The 3D proposal network is the same for all the variants. Detailed comparisons are shown in <ref type="table">Table 4</ref>. If using only a single view as input, the bird's eye view feature performs the best while the front view feature the worst. Combining any of the two views can always improve over individual views. This justifies our assumption that features from different views are complementary. The best overal performance can be achieved when fusing features of all three views. 2D Object Detection. We finally evaluate 2D detection performance on KITTI test set. Results are shown in Table 5. Among the LIDAR-based methods, our "BV+FV" approach outperforms the recently proposed Vote3Deep <ref type="bibr" target="#b5">[6]</ref> method by 14.93% AP 2D in the hard setting. In overall, image-based methods usually perform better than LIDARbased methods in terms of 2D detection. This is due to the fact that image-based methods directly optimize 2D boxes while LIDAR-based methods optimize 3D boxes. Note that despite our method optimizes 3D boxes, it also obtains competitive results compared with the state-of-the-art 2D detection methods.</p><p>Qualitative Results. As shown in <ref type="figure">Fig. 6</ref>, our approach obtains much more accurate 3D locations, sizes and orientation of objects compared with stereo-based method 3DOP <ref type="bibr" target="#b3">[4]</ref> and LIDAR-based method VeloFCN <ref type="bibr" target="#b15">[16]</ref>. We refer readers to the supplementary materials for many additional results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have proposed a multi-view sensory-fusion model for 3D object detection in the road scene. Our model takes advantage of both LIDAR point cloud and images. We align different modalities by generating 3D proposals and projecting them to multiple views for feature extraction. A region-based fusion network is presented to deeply fuse multi-view information and do oriented 3D box regression. Our approach significantly outperforms existing LIDARbased and image-based methods on tasks of 3D localization and 3D detection on KITTI benchmark <ref type="bibr" target="#b7">[8]</ref>. Our 2D box results obtained from 3D detections also show competitive performance compared with the state-of-the-art 2D detection methods.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Input features of the MV3D network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Architectures of different fusion schemes: We instantiate the join nodes in early/late fusion with concatenation operation, and deep fusion with element-wise mean operation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Training strategy for the Region-based Fusion Network: During training, the bottom three paths and losses are added to regularize the network. The auxiliary layers share weights with the corresponding layers in the main network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: 3D bounding box Recall: From left to right: Recall vs IoU using 300 proposals, Recall vs #Proposals at IoU threshold of 0.25 and 0.5 respectively. Recall are evaluated on moderate data of KITTI validation set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>3D detection performance: Average Precision (AP 3D ) (in %) of 3D boxes on KITTI validation set.</figDesc><table>main network. We use the same multi-task loss, i.e. classi-
fication loss plus 3D box regression loss, to back-propagate 
each auxiliary path. We weight all the losses including auxi-
liary losses equally. The auxiliary paths are removed during 
inference. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison of different fusion approaches: Peformance are evaluated on KITTI validation set.</figDesc><table>Data 
AP3D (IoU=0.5) 
APloc (IoU=0.5) 
AP2D (IoU=0.7) 
Easy Moderate Hard 
Easy Moderate Hard 
Easy Moderate Hard 
FV 
67.6 
56.30 
49.98 
74.02 
62.18 
57.61 
75.61 
61.60 
54.29 
RGB 
73.68 
68.86 
61.94 
77.30 
71.68 
64.58 
83.80 
76.45 
73.42 
BV 
92.30 
85.50 
78.94 
92.90 
86.98 
86.14 
85.00 
76.21 
74.80 
FV+RGB 
77.41 
71.63 
64.30 
82.57 
75.19 
66.96 
86.34 
77.47 
74.59 
FV+BV 
95.19 
87.65 
80.11 
95.74 
88.57 
88.13 
88.41 
78.97 
78.16 
BV+RGB 
96.09 
88.70 
80.52 
96.45 
89.19 
80.69 
89.61 
87.76 
79.76 
BV+FV+RGB 
96.02 
89.05 
88.38 
96.34 
89.39 
88.67 
95.01 
87.59 
79.90 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>BV+FV+RGB) LIDAR+Mono 89.11 87.67 79.54</figDesc><table>Method 
Data 
Easy Mod. Hard 
Faster R-CNN [18] 
Mono 
86.71 81.84 71.12 
3DOP [4] 
Stereo 
93.04 88.64 79.10 
Mono3D [3] 
Mono 
92.33 88.66 78.96 
SDP+RPN [29, 18] 
Mono 
90.14 88.85 78.38 
MS-CNN [1] 
Mono 
90.03 89.02 76.11 
SubCNN [28] 
Mono 
90.81 89.04 79.27 
Vote3D [25] 
LIDAR 
56.80 47.99 42.57 
VeloFCN [16] 
LIDAR 
71.06 53.59 46.92 
Vote3Deep [6] 
LIDAR 
76.79 68.24 63.23 
Ours (BV+FV) 
LIDAR 
87.00 79.24 78.16 
Ours (</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The work was supported by National Key Basic Research Program of China (No. 2016YFB0100900) and NSFC 61171113.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">A unified multi-scale deep convolutional neural network for fast object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Feris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Cpmc: Automatic object segmentation using constrained parametric min-cuts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="page" from="1312" to="1328" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d object proposals for accurate object class detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Berneshawi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">A continuous occlusion model for road scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dhiman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">H</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Corso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4331" to="4339" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Engelcke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Hay</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.06666</idno>
		<title level="m">Vote3Deep: Fast Object Detection in 3D Point Clouds Using Efficient Convolutional Neural Networks</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A multilevel mixture-ofexperts framework for pedestrian classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Gavrila</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">10</biblScope>
			<biblScope unit="page" from="2967" to="2979" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Fast R-CNN</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Onboard object detection: Multicue, multimodal, and multiview random forest of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gonzalez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Vazquez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lopez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Amores</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Cybernetics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Fusionnet: 3d object classification using multiple data representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zadeh</surname></persName>
		</author>
		<idno>abs/1607.05695</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Learning with side information through modality hallucination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">What makes for effective detection proposals? PAMI</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hosang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Larsson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07648</idno>
		<title level="m">Fractalnet: Ultra-deep neural networks without residuals</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">3d fully convolutional network for vehicle detection in point cloud</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08069</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Vehicle detection from 3d lidar using fully convolutional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Robotics: Science and Systems</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Volumetric and multi-view cnns for object classification on 3d data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">N H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Joint sfm and detection cues for monocular 3d localization in road scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chandraker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3734" to="3742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Sliding shapes for 3d object detection in depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep sliding shapes for amodal 3d object detection in rgb-d images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Segmentation as selective search for object recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Van De Sande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Uijlings</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smeulders</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Voting for voting in online point cloud object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Posner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of Robotics: Science and Systems</title>
		<meeting>Robotics: Science and Systems</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zeng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.07716</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">Deeply-fused nets.</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Data-driven 3d voxel patterns for object category recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Subcategoryaware convolutional neural networks for object proposals and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.04693.2016.7</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Exploit all the layers: Fast and accurate cnn object detector with scale dependent pooling and cascaded rejection classifiers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Choi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Detailed 3d representations for object recognition and modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<publisher>PAMI</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Are cars just 3d boxes? jointly estimating the 3d shape of multiple objects</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">Z</forename><surname>Zia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Schindler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3678" to="3685" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<idno>ECCV. 2014. 3</idno>
		<imprint/>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
