<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peihua</forename><surname>Li</surname></persName>
							<email>peihuali@dlut.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiangtao</forename><surname>Xie</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qilong</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zilin</forename><surname>Gao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Dalian University of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Towards Faster Training of Global Covariance Pooling Networks by Iterative Matrix Square Root Normalization</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Global covariance pooling in convolutional neural networks has achieved impressive improvement over the clas-</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep convolutional neural networks (ConvNets) have made significant progress in the past years, achieving recognition accuracy surpassing human beings in large-scale object recognition <ref type="bibr" target="#b6">[7]</ref>. The ConvNet models pre-trained on ImageNet <ref type="bibr" target="#b4">[5]</ref> have been proven to benefit a multitude of other computer vision tasks, ranging from fine-grained vi- sual categorization (FGVC) <ref type="bibr" target="#b24">[25]</ref>, object detection <ref type="bibr" target="#b27">[28]</ref>, semantic segmentation <ref type="bibr" target="#b25">[26]</ref> to scene parsing <ref type="bibr" target="#b36">[37]</ref>, where labeled data are insufficient for training from scratch. The common layers such as convolution, non-linear rectification, pooling and batch normalization <ref type="bibr" target="#b10">[11]</ref> have become offthe-shelf commodities, widely supported on devices including workstations, PCs and embedded systems.</p><p>Although the architecture of ConvNet has greatly evolved in the past years, its basic layers largely keep unchanged <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b17">18]</ref>. Recently, researchers have shown increasing interests in exploring structured layers to enhance representation capability of networks <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b21">22]</ref>. One particular kind of structured layer is concerned with global covariance pooling after the last convolution layer, which has shown impressive improvement over the classical firstorder pooling, successfully used in FGVC <ref type="bibr" target="#b24">[25]</ref>, visual question answering <ref type="bibr" target="#b14">[15]</ref> and video action recognition <ref type="bibr" target="#b33">[34]</ref>. Very recent works have demonstrated that matrix square root normalization of global covariance pooling plays a key role in achieving state-of-the-art performance in both large-scale visual recognition <ref type="bibr" target="#b20">[21]</ref> and challenging FGVC <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b31">32]</ref>.</p><p>For computing matrix square root, existing methods depend heavily on eigendecomposition (EIG) or singular value decomposition (SVD) <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b23">24]</ref>. However, fast implementation of EIG or SVD on GPU is an open problem, which is limitedly supported on NVIDIA CUDA platform, significantly slower than their CPU counterparts <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b23">24]</ref>. As such, existing methods opt for EIG or SVD on CPU for computing matrix square root. Nevertheless, current implementations of meta-layers depending on CPU are far from ideal, particularly for multi-GPU configuration. Since GPUs with powerful parallel computing ability have to be interrupted and await CPUs with limited parallel ability, their concurrency and throughput are greatly restricted.</p><p>In <ref type="bibr" target="#b23">[24]</ref>, for the purpose of fast forward propagation (FP), Lin and Maji use Newton-Schulz iteration (called modified Denman-Beavers iteration therein) algorithm, which is proposed in <ref type="bibr" target="#b8">[9]</ref>, to compute matrix square-root. Unfortunately, for backward propagation (BP), they compute the gradient through Lyapunov equation solution which depends on the  <ref type="table">Table 1</ref>. Differences between our iSQRT-COV and related methods. The bottleneck operations are marked with red, bold text.</p><p>GPU unfriendly Schur-decomposition (SCHUR) or EIG. Hence, the training in <ref type="bibr" target="#b23">[24]</ref> is expensive though FP which involves only matrix multiplication runs very fast. Inspired by that work, we propose a fast end-to-end training method, called iterative matrix square root normalization of covariance pooling (iSQRT-COV), depending on Newton-Schulz iteration in both forward and backward propagations.</p><p>At the core of iSQRT-COV is a meta-layer with loopembedded directed graph structure, specifically designed for ensuring both convergence of Newton-Schulz iteration and performance of global covariance pooling networks. The meta-layer consists of three consecutive structured layers, performing pre-normalization, coupled matrix iteration and post-compensation, respectively. We derive the gradients associated with the involved non-linear layers based on matrix backpropagation theory <ref type="bibr" target="#b11">[12]</ref>. The design of sandwiching Newton-Schulz iteration using pre-normalization by Frobenius norm or trace and post-compensation is essential, which, as far as we know, did not appear in previous literature (e.g. in <ref type="bibr" target="#b8">[9]</ref> or <ref type="bibr" target="#b23">[24]</ref> ). The pre-normalization guarantees convergence of Newton-Schulz (NS) iteration, while post-compensation plays a key role in achieving state-ofthe-art performance with prevalent deep ConvNet architectures, e.g. ResNet <ref type="bibr" target="#b7">[8]</ref>. The main differences between our method and other related works <ref type="bibr" target="#b0">1</ref> are summarized in Tab. 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>B-CNN is one of the first end-to-end covariance pooling ConvNets <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b11">12]</ref>. It performs element-wise square root normalization followed by ℓ 2 −normalization for covariance matrix, achieving impressive performance in FGVC task. Improved B-CNN <ref type="bibr" target="#b23">[24]</ref> shows that additional matrix square root normalization before element-wise square root and ℓ 2 −normalization can further attain large improvement. In training process, they perform FP using Newton-Schulz iteration or using SVD, and perform BP by solving Lyapunov equation or compute gradients associated with SVD.</p><p>In any case, improved B-CNN suffers from GPU unfriendly SVD, SCHUR or EIG and so network training is expensive. Our iSQRT-COV differs from <ref type="bibr" target="#b23">[24]</ref> in three aspects. First, both FP and BP of our method are based on NewtonSchulz iteration, making network training very efficient as only GPU friendly matrix multiplications are involved. Second, we propose sandwiching Newton-Schulz iteration using pre-normalization and post-compensation which is essential and plays a key role in training extremely deep ConvNets. Finally, we evaluate extensively on both large-scale ImageNet and on three popular fine-grained benchmarks.</p><p>In <ref type="bibr" target="#b20">[21]</ref>, matrix power normalized covariance pooling method (MPN-COV) is proposed for large-scale visual recognition. It achieves impressive improvements over firstorder pooling with AlexNet <ref type="bibr" target="#b17">[18]</ref>, VGG-Net <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b28">29]</ref> and ResNet <ref type="bibr" target="#b7">[8]</ref> architectures. MPN-COV has shown that, given a small number of high-dimensional features, matrix power is consistent with shrinkage principle of robust covariance estimation, and matrix square root can be derived as a robust covariance estimator via a von Neumann regularized maximum likelihood estimation <ref type="bibr" target="#b32">[33]</ref>. It is also shown that matrix power normalization approximately yet effectively exploits geometry of the manifold of covariance matrices, superior to matrix logarithm normalization <ref type="bibr" target="#b11">[12]</ref> for high-dimensional features. All computations of MPN-COV meta-layer are implemented with NVIDIA cuBLAS library running on GPU, except EIG which runs on CPU. G 2 DeNet <ref type="bibr" target="#b31">[32]</ref> is concerned with inserting global Gaussian distributions into ConvNets for end-to-end learning. In G 2 DeNet, each Gaussian is identified as square root of a symmetric positive definite matrix based on Lie group structure of Gaussian manifold <ref type="bibr" target="#b19">[20]</ref>. The matrix square root plays a central role in obtaining the competitive performance <ref type="bibr" target="#b31">[32,</ref><ref type="bibr">Tab. 1 &amp; Tab. 5]</ref>. Compact bilinear pooling (CBP) <ref type="bibr" target="#b5">[6]</ref> clarifies that bilinear pooling is closely related to the second-order polynomial kernel, and presents two compact representations via low-dimensional feature maps for kernel approximation. Kernel pooling <ref type="bibr" target="#b3">[4]</ref> approximates Gaussian RBF kernel to a given order through compact explicit feature maps, aiming to characterize higher order feature interactions. Cai et al. <ref type="bibr" target="#b1">[2]</ref> introduce a polynomial kernel based predictor to model higher-order statistics of convolutional features across multiple layers.  <ref type="figure">Figure 1</ref>. Proposed iterative matrix square root normalization of covariance pooling (iSQRT-COV) network. After the last convolution layer, we perform second-order pooling by estimating a covariance matrix. We design a meta-layer with loop-embedded directed graph structure for computing approximate square root of covariance matrix. The meta-layer consists of three nonlinear structured layers, performing pre-normalization, coupled Newton-Schulz iteration and post-compensation, respectively. See Sec. 3 for notations and details.</p><formula xml:id="formula_0">k k k f − − Y = Y Z 1 1 ( , ) k k k g − − Z = Y Z 1 tr( ) Σ Σ A = 0 Z = I Tensor Covariance Pooling w h d i x Σ X 1 k = N Y k N = Σ tr( ) N Σ C = Y if k N &lt; 1 k k + = C</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed iSQRT-COV Network</head><p>In this section, we first give an overview of the proposed iSQRT-COV network. Then we describe matrix square root computation and its forward propagation. We finally derive the corresponding backward gradients.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview of Method</head><p>The flowchart of the proposed network is shown in <ref type="figure">Fig. 1</ref>. Let output of the last convolutional layer (with ReLU) be a h × w × d tensor with spatial height h, width w and channel d. We reshape the tensor to a feature matrix X consisting of n = wh features of d−dimension. Then we perform second-order pooling by computing the covariance matrix Σ = XĪX T , whereĪ = 1 n (I − 1 n 1), I and 1 are the n × n identity matrix and matrix of all ones, respectively.</p><p>Our meta-layer is designed to have loop-embedded directed graph structure, consisting of three consecutive nonlinear structured layers. The purpose of the first layer (i.e., pre-normalization) is to guarantee the convergence of the following Newton-Schulz iteration, achieved by dividing the covariance matrix by its trace (or Frobenius norm). The second layer is of loop structure, repeating the coupled matrix equations involved in Newton-Schulz iteration a fixed number of times, for computing approximate matrix square root. The pre-normalization nontrivially changes data magnitudes, so we design the third layer (i.e., postcompensation) to counteract the adverse effect by multiplying trace (or Frobenius norm) of the square root of the covariance matrix. As the output of our meta-layer is a symmetric matrix, we concatenate its upper triangular entries forming an d(d + 1)/2-dimensional vector, submitted to the subsequent layer of the ConvNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Matrix Square Root and Forward Propagation</head><p>Square roots of matrices, particularly covariance matrices which are symmetric positive (semi)definite (SPD), find applications in a variety of fields including computer vision, medical imaging <ref type="bibr" target="#b37">[38]</ref> and chemical physics <ref type="bibr" target="#b13">[14]</ref>. It is wellknown any SPD matrix has a unique square root which can be computed accurately by EIG or SVD. Briefly, let A be an SPD matrix and it has EIG A = Udiag(λ i )U T , where U is orthogonal and diag(λ i ) is a diagonal matrix of eigenvalues</p><formula xml:id="formula_1">λ i of A. Then A has a square root Y = Udiag(λ 1/2 i )U T , i.e., Y 2 = A.</formula><p>Unfortunately, both EIG and SVD are not well supported on GPU.</p><p>Newton-Schulz Iteration Higham <ref type="bibr" target="#b8">[9]</ref> studied a class of methods for iteratively computing matrix square root. These methods, termed as Newton-Padé iterations, are developed based on the connection between matrix sign function and matrix square root, together with rational Padé approximation. Specifically, for computing the square root Y of A, given Y 0 = A and Z 0 = I, for k = 1, · · · , N , the coupled iteration takes the following form <ref type="bibr" target="#b8">[9,</ref><ref type="bibr">Chap. 6.7]</ref>:</p><formula xml:id="formula_2">Y k = Y k−1 p lm (Z k−1 Y k−1 )q lm (Z k−1 Y k−1 ) −1 Z k = p lm (Z k−1 Y k−1 )q lm (Z k−1 Y k−1 ) −1 Z k−1 ,<label>(1)</label></formula><p>where p lm and q lm are polynomials, and l and m are non-negative integers. Eqn. (1) converges only locally: if A − I &lt; 1 where · denotes any induced (or consistent) matrix norm, Y k and Z k quadratically converge to Y and Y −1 , respectively. The family of coupled iteration is stable in that small errors in the previous iteration will not be amplified. The case of l = 0, m = 1 called NewtonSchulz iteration fits for our purpose as no GPU unfriendly matrix inverse is involved:</p><formula xml:id="formula_3">Y k = 1 2 Y k−1 (3I − Z k−1 Y k−1 ) Z k = 1 2 (3I − Z k−1 Y k−1 )Z k−1 .<label>(2)</label></formula><p>Clearly Eqn. <ref type="formula" target="#formula_3">(2)</ref>  Pre-normalization and Post-compensation As NewtonSchulz iteration only converges locally, we pre-normalize Σ by trace or Frobenius norm, i.e.,</p><formula xml:id="formula_4">A = 1 tr(Σ) Σ or 1 Σ F Σ.<label>(3)</label></formula><p>Let λ i be eigenvalues of Σ, arranged in nondecreasing order. As tr(Σ) = i λ i and Σ F = i λ 2 i , it is easy to see that Σ − I 2 , which equals to the largest singular value of Σ − I, is 1 −</p><formula xml:id="formula_5">λ1 i λi and 1 − λ1 √ i λ 2 i</formula><p>for the case of trace and Frobenius norm, respectively, both less than 1. Hence, the convergence condition is satisfied.</p><p>The above pre-normalization of covariance matrix nontrivially changes the data magnitudes such that it produces adverse effect on network. Hence, to counteract this change, after the Newton-Schulz iteration, we accordingly perform post-compensation, i.e.,</p><formula xml:id="formula_6">C = tr(Σ)Y N or C = Σ F Y N .<label>(4)</label></formula><p>An alternative scheme to counterbalance the influence incurred by pre-normalization is Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref>. One may even consider without using any post-compensation. However, our experiment on ImageNet has shown that, without post-normalization, prevalent ResNet <ref type="bibr" target="#b7">[8]</ref> fails to converge, while our scheme outperforms BN by about 1% (see 4.3 for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Backward Propagation (BP)</head><p>The gradients associated with the structured layers are derived using matrix backpropagation methodology <ref type="bibr" target="#b12">[13]</ref>, which establishes the chain rule of a general matrix function by first-order Taylor approximation. Below we take pre-normalization by trace as an example, deriving the corresponding gradients. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>BP of Post-compensation Given</head><formula xml:id="formula_7">∂l ∂Y N = tr(Σ) ∂l ∂C ∂l ∂Σ post = 1 2 tr(Σ) tr ∂l ∂C T Y N I.<label>(5)</label></formula><p>BP of Newton-Schulz Iteration Then we are to compute the partial derivatives of the loss function with respect to <ref type="formula" target="#formula_7">(5)</ref> and ∂l ∂Z N = 0. As the covariance matrix Σ is symmetric, it is easy to see from Eqn. <ref type="formula" target="#formula_3">(2)</ref> that Y k and Z k are both symmetric. According to the chain rules (omitted hereafter for simplicity) of matrix backpropagation and after some manipulations, k = N, . . . , 2, we can derive</p><formula xml:id="formula_8">∂l ∂Y k and ∂l ∂Z k , k = N − 1, . . . , 1, given ∂l ∂Y N computed by Eqn.</formula><formula xml:id="formula_9">∂l ∂Y k−1 = 1 2 ∂l ∂Y k 3I − Y k−1 Z k−1 − Z k−1 ∂l ∂Z k Z k−1 − Z k−1 Y k−1 ∂l ∂Y k ∂l ∂Z k−1 = 1 2 3I − Y k−1 Z k−1 ∂l ∂Z k − Y k−1 ∂l ∂Y k Y k−1 − ∂l ∂Z k Z k−1 Y k−1 .<label>(6)</label></formula><p>The final step of this layer is concerned with the partial derivative with respect to ∂l ∂A , which is given by ∂l ∂A = 1 2</p><formula xml:id="formula_10">∂l ∂Y 1 3I − A − ∂l ∂Z 1 − A ∂l ∂Y 1 .<label>(7)</label></formula><p>BP of Pre-normalization Note that here we need to combine the gradient of the loss function l with respect to Σ, backpropagated from the post-compensation layer. As such, by referring to Eqn. (3), we make similar derivations as before and obtain</p><formula xml:id="formula_11">∂l ∂Σ = − 1 (tr(Σ)) 2 tr ∂l ∂A T Σ I + 1 tr(Σ) ∂l ∂A + ∂l ∂Σ post .<label>(8)</label></formula><p>If we adopt pre-normalization by Frobenius norm, the gradients associated with post-compensation become</p><formula xml:id="formula_12">∂l ∂Y N = Σ F ∂l ∂C ∂l ∂Σ post = 1 2 Σ 3/2 F tr ∂l ∂C T Y N Σ,<label>(9)</label></formula><p>and that with respect to pre-normalization is</p><formula xml:id="formula_13">∂l ∂Σ = − 1 Σ 3 F tr ∂l ∂A T Σ Σ + 1 Σ F ∂l ∂A + ∂l ∂Σ post ,<label>(10)</label></formula><p>while the backward gradients of Newton-Schulz iteration (6) keep unchanged. Finally, given ∂l ∂Σ , one can derive the gradient of the loss function l with respect to input matrix X, which takes the following form <ref type="bibr" target="#b20">[21]</ref>:</p><formula xml:id="formula_14">∂l ∂X =ĪX ∂l ∂Σ + ∂l ∂Σ T .<label>(11)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the proposed method on both large-scale image classification and challenging fine-grained visual categorization tasks. We make experiments using two PCs each of which is equipped with a 4-core Intel i7-4790k@4.0GHz CPU, 32G RAM, 512GB Samsung PRO SSD and two Titan Xp GPUs. We implement our networks using MatConvNet <ref type="bibr" target="#b29">[30]</ref> and Matlab2015b, under Ubuntu 14.04.5 LTS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets and Our Meta-layer Implementation</head><p>Datasets For large-scale image classification, we adopt ImageNet LSVRC2012 dataset <ref type="bibr" target="#b4">[5]</ref> with 1,000 object categories. The dataset contains 1.28M images for training, 50K images for validation and 100K images for testing (without published labels). As in <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b7">8]</ref>, we report the results on the validation set. For fine-grained categorization, we use three popular fine-grained benchmarks, i.e., CUB-200-2011(Birds) <ref type="bibr" target="#b30">[31]</ref>, FGVC-aircraft (Aircrafts) <ref type="bibr" target="#b26">[27]</ref> and Stanford cars (Cars) <ref type="bibr" target="#b16">[17]</ref>. The Birds dataset contains 11,788 images from 200 species, with large intra-class variation but small inter-class variation. The Aircrafts dataset includes 100 aircraft classes and a total of 10,000 images with small background noise but higher inter-class similarity. The Cars dataset consists of 16,185 images from 196 classes. For all datasets, we adopt the provided training/test split, using neither bounding boxes nor part annotations.</p><p>Implementation of iSQRT-COV Meta-layer We encapsulate our code in three computational blocks, which implement forward&amp;backward computation of pre-normalization layer, Newton-Schulz iteration layer and post-compensation layer, respectively. The code is written in C++ based on NVIDIA cuBLAS on top of CUDA toolkit 8.0. In addition, we write code in C++ based on cuBLAS for computing covariance matrices. We create MEX files so that the above subroutines can be called in Matlab environment. For AlexNet, we insert our meta-layer after the last convolution layer (with ReLU), which outputs an 13 × 13 × 256 tensor. For ResNet architecture, as suggested <ref type="bibr" target="#b20">[21]</ref>, we do not perform downsampling for the last set of convolutional blocks, and add one 1 × 1 convolution with d = 256 channels after the last sum layer (with ReLU). The added 1×1 convolution layer outputs an 14 × 14 × 256 tensor. Hence, with both architectures, the covariance matrix Σ is of size 256×256 and our meta-layer outputs an d(d + 1)/2 ≈ 32K-dimensional vector as the image representation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation with AlexNet on ImageNet</head><p>In the first part of experiments, we analyze, with AlexNet architecture, the design choices of our iSQRT-COV method, including the number of Newton-Schulz iterations, time and memory usage, and behaviors of different pre-normalization methods. We select AlexNet because it runs faster with shallower depth, and the results can extrapolate to deeper networks which mostly follow its architecture design. We follow <ref type="bibr" target="#b20">[21]</ref> for color augmentation and weight initialization, adopting BN and no dropout. We use SGD with a mini-batch of 128, unless otherwise stated. The momentum is 0.9 and weight decay is 0.0005. We train iSQRT-COV networks from scratch in 20 epochs where learning rate follows exponential decay 10 −1.1 → 10 −5 . All training and test images are uniformly resized with shorter sides of 256. During training we randomly crop a 224×224 patch from each image or its horizontal flip. We make inference on one single 224 × 224 center crop from a test image. <ref type="figure" target="#fig_3">Fig. 2</ref> shows top-1 error rate as a function of number of Newton-Schulz iterations in Eqn. <ref type="bibr" target="#b1">(2)</ref>. Plain-COV indicates simple covariance pooling without any normalization. With one single iteration, our method outperforms Plain-COV by 1.3%. As iteration number grows, the error rate of iSQRT-COV gradually declines. With 3 iterations, iSQRT-COV is comparable to MPN-COV, having only 0.3% higher error rate, while performing marginally better than MPN-COV between 5 and 7 iterations. After N = 7, the error rate consistently increases, indicating growth of iteration number is not helpful for improving accuracy. As larger N incurs higher computational cost, to balance efficiency and accuracy, we set N to 5 in the remaining experiments. Notably, the approximate square root normalization improves a little over the accurate one obtained via EIG. This interesting problem will be discussed in Sec. 4.3, where iSQRT-COV is further evaluated on substantially deeper ResNets.   G 2 DeNet. For improved B-CNN, the forward computation of Newton-Schulz (NS) iteration is much faster than that of SVD, but the total time of two methods is comparable. The authors of improved B-CNN also proposed two other implementations, i.e., FP by NS iteration plus BP by SVD and FP by SVD plus BP by Lyapunov (Lyap.), which take 15.31 (2.09) and 12.21 <ref type="bibr">(11.19)</ref>, respectively. We observe that, in any case, the forward+backward time taken by single meta-layer of improved B-CNN is significant as GPU unfriendly SVD or EIG cannot be avoided, even though the forward computation is very efficient when NS iteration is used. Tab. 2(b) presents running time of EIG and SVD of an 256 × 256 covariance matrix. Matlab (M) builtin CPU functions and GPU functions deliver over 10x and 2.1x speedups over their CUDA counterparts, respectively. Our method needs to store Y k and Z k in Eqn. (2) which will be used in backpropagation, taking up more memory than EIG or SVD based ones. Among all, our iSQRT-COV (N = 5) takes up the largest memory of 1.129MB, which is insignificant compared to 12GB memory on a Titan Xp. Note that for network inference only, our method  takes 0.125MB memory as it is unnecessary to store Y k and Z k .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Impact of Number N of Newton-Schulz Iterations</head><p>Next, we compare in <ref type="figure" target="#fig_5">Fig. 3</ref> speed of network training between MPN-COV and iSQRT-COV with both one-GPU and two-GPU configurations. For one-GPU configuration, the speed gap vs. batch size between the two methods keeps nearly constant. For two-GPU configuration, their speed gap becomes more significant when batch size gets larger. As can be seen, the speed of iSQRT-COV network continuously grows with increase of batch size while that of MPN-COV tends to saturate when batch size is larger than 512. Clearly our iSQRT-COV network can make better use of computing power of multiple GPUs than MPN-COV.</p><p>Pre-normalization by Trace vs. by Frobenius Norm Sec. 3 describes two pre-normalization methods. Here we compare them in Tab. 3 (bottom rows), where iSQRT-COV (trace) indicates pre-normalization by trace. We can see that pre-normalization by trace produces 0.3% lower error rate than that by Frobenius norm, while taking similar time with the latter. Hence, in all the remaining experiments, we adopt trace based pre-normalization method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Comparison with Other Covariance Pooling Methods</head><p>We compare iSQRT-COV with other covariance pooling methods, as shown in Tab. 3. The results of MPN-COV, B-CNN and DeepO 2 P are duplicated from <ref type="bibr" target="#b20">[21]</ref>. We train from scratch G 2 DeNet and improved B-CNN on ImageNet. We use the most efficient implementation of improved B-CNN, i.e., FP by SVD and BP by Lyap., and we mention all implementations of improved B-CNN produce similar results. Our iSQRT-COV using pre-normalization by trace is marginally better than MPN-COV. All matrix square root normalization methods except improved B-CNN outperform B-CNN and DeepO 2 P. Since improved B-CNN is identical to MPN-COV if element-wise square root normalization and ℓ 2 −normalization are neglected, its unsatisfactory performance suggests that, after matrix square root normalization, further element-wise square root normalization and ℓ 2 −normalization hurt large-scale ImageNet classifica-  <ref type="table">Table 4</ref>. Impact of post-compensation on iSQRT-COV with ResNet-50 architecture on ImageNet. tion. This is consistent with the observation in [21, Tab. 1], where after matrix power normalization, additional normalization by Frobenius norm or matrix ℓ 2 −norm makes performance decline.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Results on ImageNet with ResNet Architecture</head><p>This section evaluates iSQRT-COV with ResNet architecture <ref type="bibr" target="#b7">[8]</ref>. We follow <ref type="bibr" target="#b20">[21]</ref> for color augmentation and weight initialization. We rescale each training image with its shorter side randomly sampled on [256, 512] <ref type="bibr" target="#b28">[29]</ref>. The fixed-size 224 × 224 patch is randomly cropped from the rescaled image or its horizontal flip. We rescale each test image with a shorter side of 256 and evaluate a single 224 × 224 center crop for inference. We use SGD with a mini-batch size of 256, a weight decay of 0.0001 and a momentum of 0.9. We train iSQRT-COV networks from scratch in 60 epochs, initializing the learning rate to 10</p><formula xml:id="formula_15">−1.1</formula><p>which is divided by 10 at epoch 30 and 45, respectively.</p><p>Significance of Post-compensation Rather than our postcompensation scheme, one may choose Batch Normalization (BN) <ref type="bibr" target="#b10">[11]</ref> or simply do nothing (i.e., without postcompensation). Tab. 4 summarizes impact of different schemes on iSQRT-COV network with ResNet-50 architecture. Without post-compensation, iSQRT-COV network fails to converge. Careful observations show that in this case the gradients are very small (on the order of 10 −5 ), and largely tuning of learning rate helps little. Option of BN helps the network converge, but producing about 1% higher top-1 error rate than our post-compensation scheme. The comparison above suggests that our post-compensation scheme is essential for achieving state-of-the-art results.  <ref type="table">Table 5</ref>. Error (%) comparison of second-order networks with firstorder ones on ImageNet.</p><p>Fast Convergence of iSQRT-COV Network We compare convergence of iSQRT-COV and MPN-COV with ResNet-50 architecture, as well as the original ResNet-50 <ref type="bibr" target="#b7">[8]</ref> in which global average pooling is performed after the last convolution layer. <ref type="figure" target="#fig_6">Fig. 4</ref> presents the convergence curves. Compared to the original ResNet-50, the convergence of both iSQRT-COV and MPN-COV is significantly faster. We observe that iSQRT-COV can converge well within 60 epochs, achieving top-1 error rate of 22.14%, ∼0.6% lower than MPN-COV. We also trained iSQRT-COV with 90 epochs using same setting with MPN-COV, obtaining top-5 error of 6.12%, slightly lower than that with 60 epochs (6.22%). This indicates iSQRT-COV can converge in less epochs, so further accelerating training, as opposed to MPN-COV. The fast convergence property of iSQRT-COV is appealing. As far as we know, previous networks with ResNet-50 architecture require at least 90 epochs to converge to competitive results.</p><p>Comparison with State-of-the-arts In Tab. 5, we compare our method with other second-order networks, as well as the original ResNets. With ResNet-50 architecture, all the second-order networks improve over the first-order one while our method performing best. MPN-COV and iSQRT-COV, both of which involve square root normalization, are superior to FBN <ref type="bibr" target="#b22">[23]</ref> which uses no normalization and SORT <ref type="bibr" target="#b34">[35]</ref> which introduces dot product transform in the linear sum of two-branch module followed by element-wise normalization. Moreover, our iSQRT-COV outperforms MPN-COV by 0.6% in top-1 error. Note that our 50-layer iSQRT-COV network achieves lower error rate than much deeper ResNet-101 and ResNet-152, while our 101-layer iSQRT-COV network outperforming the original ResNet-101 by 2.4% and ResNet-152 by 1.8%, respectively.</p><p>Why Approximate Square Root Performs Better <ref type="figure" target="#fig_3">Fig. 2</ref> shows that more iterations which lead to more accurate square root is not helpful for iSQRT-COV with AlexNet. From Tab. 5, we observe that iSQRT-COV with ResNet computing approximate square root performs better than MPN-COV which can obtain exact square root by EIG. Recall that, for covariance pooling ConvNets, we face the  problem of small sample of large dimensionality, and matrix square root is consistent with general shrinkage principle of robust covariance estimation <ref type="bibr" target="#b20">[21]</ref>. Hence, we conjuncture that approximate matrix square root may be a better robust covariance estimator than the exact square root. Despite this analysis, we think this problem is worth future research.</p><p>Compactness of iSQRT-COV Our iSQRT-COV outputs 32k-dimensional representation which is high. Here we consider to compress this representation. Compactness by PCA <ref type="bibr" target="#b24">[25]</ref> is not viable since obtaining the principal components on ImageNet is too expensive. CBP <ref type="bibr" target="#b5">[6]</ref> is not applicable to our iSQRT-COV as well, as it does not explicitly estimate the covariance matrix. We propose a simple scheme, which decreases the dimension (dim.) of covariance representation by lowering the number d of channels of 1 × 1 convolutional layer before our covariance pooling. Tab. 6 summarizes results of compact iSQRT-COV. The recognition error increases slightly (↑ 0.64%) when d decreases from 256 to 128 (correspondingly, dim. of image representation 32K → 8K). The error rate is 23.73% if the dimension is compressed to 2K, still outperforming the original ResNet-50 which performs global average pooling.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Fine-grained Visual Categorization (FGVC)</head><p>Finally, we apply iSQRT-COV models pre-trained on ImageNet to FGVC. For fair comparison, we follow <ref type="bibr" target="#b24">[25]</ref> for experimental setting and evaluation protocol. On all datasets, we crop 448 × 448 patches as input images. We replace 1000-way softmax layer of a pre-trained iSQRT-COV model by a k-way softmax layer, where k is number of classes in the fine-grained dataset, and finetune the network using SGD with momentum of 0.9 for 50∼100 epochs with a small learning rate (lr=10 −2.1 ) for all layers except the fully-connected layer, which is set to 5 × lr. We use horizontal flipping as data augmentation. After finetuning, the outputs of iSQRT-COV layer are ℓ 2 −normalized before inputted to train k one-vs-all linear SVMs with hyperparameter C = 1. We predict the label of a test image by averaging SVM scores of the image and its horizontal flip.</p><p>Tab. 7 presents classification results of different methods, where column 3 lists the dimension of the corresponding representation. With ResNet-50 architecture, KP performs much better than CBP, while iSQRT-COV (8K) respectively outperforms KP (14K) by about 2.6%, 3.8%  <ref type="table">Table 7</ref>. Comparison of accuracy (%) on fine-grained benchmarks. Our method uses neither bounding boxes nor part annotations.</p><p>and 0.6% on Birds, Aircrafts and Cars, and iSQRT-COV (32K) further improves accuracy. Note that KP combines first-order up to fourth-order statistics while iSQRT-COV only exploits second-order one. With VGG-D, iSQRT-COV (32k) matches or outperforms state-of-the-art competitors, but inferior to iSQRT-COV (32k) with ResNet-50. On all fine-grained datasets, KP and CBP with 16-layer VGG-D perform better than their counterparts with 50-layer ResNet, despite the fact that ResNet-50 significantly outperforms VGG-D on ImageNet <ref type="bibr" target="#b7">[8]</ref>. The reason may be that the last convolution layer of pre-trained ResNet-50 outputs 2048-dimensional features, much higher than 512-dimensional one of VGG-D, which are not suitable for existing second-or higher-order pooling methods. Different from all existing methods which use models pre-trained on ImageNet with first-order information, our pre-trained models are of second-order. Using pre-trained iSQRT-COV models with ResNet-50, we achieve recognition results superior to all the compared methods, and furthermore, establish state-of-the-art results on three fine-grained benchmarks using iSQRT-COV model with ResNet-101.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We presented an iterative matrix square root normalization of covariance pooling (iSQRT-COV) network which can be trained end-to-end. Compared to existing works depending heavily on GPU unfriendly EIG or SVD, our method, based on coupled Newton-Schulz iteration <ref type="bibr" target="#b8">[9]</ref>, runs much faster as it involves only matrix multiplications, suitable for parallel implementation on GPU. We validated our method on both large-scale ImageNet dataset and challenging fine-grained benchmarks. Given efficiency and promising performance of our iSQRT-COV, we hope global covariance pooling will be a promising alternative to global average pooling in other deep network architectures, e.g., ResNeXt <ref type="bibr" target="#b35">[36]</ref>, Inception <ref type="bibr" target="#b10">[11]</ref> and DenseNet <ref type="bibr" target="#b9">[10]</ref>.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>The work was supported by National Natural Science Foundation of China (No. 61471082). Peihua Li is the corresponding author.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>, where dC denotes varia- tion of C. After some manipulations, we have</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Impact of number N of Newton-Schulz iterations on iSQRT-COV with AlexNet architecture on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>Time and Memory Analysis We compare time and memory consumed by single meta-layer of different meth- ods. We use public code for MPN-COV, G 2 DeNet and improved B-CNN released by the respective authors. As shown in Tab. 2(a), iSQRT-COV (N = 3) and iSQRT- COV (N = 5) are 3.1x faster and 1.8x faster than MPN- COV, respectively. Furthermore, iSQRT-COV (N = 5) is five times more efficient than improved B-CNN and (a) Time of FP+BP (ms) taken and memory (MB) used by single meta-layer. Numbers in parentheses indicate FP time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Images per second (FP+BP) of network training with AlexNet architecture.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Convergence curves of different networks trained with ResNet-50 architecture on ImageNet.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>involves only matrix product, suitable for parallel implementation on GPU. Compared to accu- rate square root computed by EIG, one can only obtain ap- proximate solution with a small number of iterations. We determine the number of iterations N by cross-validation. Interestingly, compared to EIG or SVD based methods, ex- periments on large-scale ImageNet show that we can obtain matching or marginally better performance under AlexNet architecture (Sec. 4.2) and better performance under ResNet architecture (Sec. 4.3), using no more than 5 iterations.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc></figDesc><table>Error rate (%) and time of FP+BP (ms) per image 
of different covariance pooling methods with AlexNet on Ima-
geNet. Numbers in parentheses indicate FP time. 
 *  Following [24], 
improved B-CNN successively performs matrix square root, 
element-wise square root and ℓ2 normalizations. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Pre-normalization Post-compensation Top-1 Err. Top-5 Err.</figDesc><table>Trace 

w/o 
N/A 
N/A 
w/ BN [11] 
23.12 
6.60 
w/ Trace 
22.14 
6.22 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Method d</head><label>Method</label><figDesc>Dim. Top-1 Err. Top-5 Err. Time</figDesc><table>He et al. [8] 
N/A 
2K 
24.7 
7.8 
8.08 (1.93) 

iSQRT-COV 

64 
2K 
23.73 
6.99 
9.86 (2.39) 
128 
8K 
22.78 
6.43 
10.75 (2.67) 
256 32K 
22.14 
6.22 
11.33 (2.89) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head>Table 6 .</head><label>6</label><figDesc>Error rate (%) and time of FP+BP (ms) per image vs. d (or representation dimension) of compact iSQRT-COV with ResNet- 50 on ImageNet. Numbers in parentheses indicate FP time.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">It is worth noting that, after CVPR submission deadline, authors of [24] release code of improved B-CNN together with a scheme similar to ours, in which BP of Newton-Schulz iteration is implemented using Autograd package in PyTorch. We note that (1) that scheme is parallel to our work, and (2) they only provide pieces of code but do not train using BP of Newton-Schulz iteration on any real-world benchmarks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">NetVLAD: CNN architecture for weakly supervised place recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arandjelovic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Gronat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pajdla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Higher-order integration of hierarchical convolutional activations for fine-grained visual categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Return of the devil in the details: Delving deep into convolutional nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Kernel pooling for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">ImageNet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Compact bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Beijbom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on ImageNet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Functions of Matrices: Theory and Computation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">J</forename><surname>Higham</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<pubPlace>SIAM, Philadelphia, PA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Matrix backpropagation for deep networks with structured layers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ionescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vantzos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Sminchisescu</surname></persName>
		</author>
		<idno>abs/1509.07838</idno>
		<title level="m">Training deep networks with structured layers by matrix backpropagation. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Linear-scaling symmetric square-root decomposition of the overlap matrix</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Jansík</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Høst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jørgensen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Olsen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Helgaker</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. of Chemical Physics</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="124104" to="124104" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">An analysis of visual question answering algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kafle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Kanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Low-rank bilinear pooling for finegrained classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Fowlkes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">3D Object representations for fine-grained categorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Stark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">F</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="554" to="561" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">ImageNet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Local Log-Euclidean multivariate Gaussian descriptor and its application to image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Is second-order information helpful for large-scale visual recognition? In ICCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep scene image classification with the MFAFVNet</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Dixit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Vasconcelos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Factorized bilinear models for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Hou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Improved bilinear pooling with CNNs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Bilinear CNN models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Fine-grained visual classification of aircraft. HAL -INRIA</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rahtu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kannala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Blaschko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">YOLO9000: Better, faster, stronger</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">MatConvNet -convolutional neural networks for MATLAB</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Lenc</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD Birds200-2011 Dataset. California Institute of Technology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">G 2 DeNet: Global Gaussian distribution embedding network and its application to visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">RAID-G: Robust estimation of approximate infinite dimensional Gaussian with application to material recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">SORT: Second-order response transform for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Scene parsing through ADE20K dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Puig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Barriuso</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Regularisation, interpolation and visualisation of diffusion tensor images using non-Euclidean statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">L</forename><surname>Dryden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Koloydenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Audenaert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bai</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Applied Statistics</title>
		<imprint>
			<biblScope unit="volume">43</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="943" to="978" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
