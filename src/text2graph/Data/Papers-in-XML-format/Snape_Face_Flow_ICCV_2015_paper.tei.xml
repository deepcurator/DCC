<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Face Flow</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Patrick</forename><surname>Snape</surname></persName>
							<email>p.snape@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>180 Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anastasios</forename><surname>Roussos</surname></persName>
							<email>troussos@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>180 Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yannis</forename><surname>Panagakis</surname></persName>
							<email>i.panagakis@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>180 Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefanos</forename><surname>Zafeiriou</surname></persName>
							<email>s.zafeiriou@imperial.ac.uk</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Computing</orgName>
								<orgName type="institution">Imperial College London</orgName>
								<address>
									<addrLine>180 Queens Gate</addrLine>
									<postCode>SW7 2AZ</postCode>
									<settlement>London</settlement>
									<country key="GB">U.K</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Face Flow</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we propose a method for the robust and efficient computation of multi-frame optical flow in an expressive sequence of facial images. We formulate a novel energy minimisation problem for establishing dense correspondences between a neutral template and every frame of a sequence. We exploit the highly correlated nature of human expressions by representing dense facial motion using a deformation basis. Furthermore, we exploit the even higher correlation between deformations in a given input sequence by imposing a low-rank prior on the coefficients of the deformation basis, yielding temporally consistent optical flow. Our proposed model-based formulation, in conjunction with the inverse compositional strategy and lowrank matrix optimisation that we adopt, leads to a highly efficient algorithm for calculating facial flow. As experimental evaluation, we show quantitative experiments on a challenging novel benchmark of face sequences, with dense ground truth optical flow provided by motion capture data. We also provide qualitative results on a real sequence displaying fast motion and occlusions. Extensive quantitative and qualitative comparisons demonstrate that the proposed method outperforms state-of-the-art optical flow and dense non-rigid registration techniques, whilst running an order of magnitude faster.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Computing optical flow in the presence of non-rigid deformations is an important and challenging task. It plays a significant role in a wide variety of problems such as medical imaging, dense non-rigid 3D reconstruction, dense 3D mesh registration, motion segmentation, video re-texturing and super-resolution. Broadly, optical flow methods describe procedures to relate pixels in one image to pixels in another image of the same object. They establish a displacement field that can be thought of as a sampling, or warping, of the input image back onto the reference image. Traditionally, optical flow is applied on a pair of consecutive frames of a sequence, treating one of the frames as the template. However, in terms of revealing the dynamics of a non-rigid scene, it is much more useful to estimate the optical flow between every frame of a long sequence and a common template. In this scenario, long-term dense 2D tracks across the sequence are established. We tackle the problem of multiframe optical flow by focusing on a specific deformable object, the human face.</p><p>The most straight-forward way to estimate a multi-frame optical flow is to apply an algorithm that solves the traditional two-frame optical flow problem between every frame and the template independently. However, the fact that we have to deal with long sequences poses major difficulties. For example, the point displacements between the template and a frame can be substantially large and severe occlusions of parts of the template can occur in some frames. Even state-of-the-art two-frame optical flow methods that are especially designed to deal with large displacements or occlusions, e.g. <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b26">27]</ref>, are prone to fail. This is because they lack any additional cues that could help them estimate an appropriate initialisation or to disambiguate severe occlusions.</p><p>An alternative solution to the multi-frame optical flow problem, also based on two-frame optical flow, is to estimate flow between consecutive frames and then combine the various solutions. A simple integration of the solutions to obtain long-term 2D tracks is prone to drift due to error accumulation <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b8">9]</ref>. This can be improved by the automatic detection of occlusions, gross errors, and other ambiguities <ref type="bibr" target="#b31">[32,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b28">29,</ref><ref type="bibr" target="#b24">25]</ref>, but any such solution is still limited by the accuracy of the initial two-frame optical flow estimations that are completely local in time and do not exploit any temporal cues.</p><p>Several recent methods solve the multi-frame optical flow problem directly, by implicitly taking into account the rich temporal information that is present in non-rigid scenes <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b13">14]</ref>. For example, the long-term 2D trajectories of points on a surface undergoing non-rigid deformation are highly correlated and can be compactly described via a linear combination of a low-rank trajectory basis. This basis is typically learnt from the input sequence itself. In this way, these methods are more robust to occlusions and yield a temporally coherent result. However, they rely only on some generic spatial and temporal regularisation priors, applicable to any deformable object and do not utilise any prior knowledge about the specific object observed in the scene. This makes them fail in more challenging conditions that often occur in real-world scenes, such as severe occlusions or significant illumination changes, which cannot be disambiguated by temporal regularisation alone. Furthermore, the memory and runtime efficiency of all existing multi-frame optical flow methods is limited by the fact that they have to estimate a very large number of parameters, i.e. a set of parameters for every pixel of the template. Specially designed parallelisable algorithms, such as primal-dual optimisation schemes <ref type="bibr" target="#b36">[37,</ref><ref type="bibr" target="#b13">14]</ref> can be adopted. However, they are only efficient on recent GPU hardware.</p><p>In this paper, we overcome the aforementioned limitations by incorporating a face-specific deformation model into the multi-frame optical flow estimation. We assume a learnt deformation basis, rather than one calculated directly from the sequence itself. We focus on human faces which are a very commonly considered object in computer vision, and dense face correspondences are required in many research areas and applications. That is, the establishment of dense correspondences of deformable faces is the first step towards high-performance facial expression recognition <ref type="bibr" target="#b17">[18]</ref>, facial motion capture <ref type="bibr" target="#b7">[8]</ref> and 3D face reconstruction <ref type="bibr" target="#b12">[13]</ref>. Nevertheless, computing dense face correspondences has received limited attention <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b39">40]</ref>. This is attributed to the difficulty of developing a statistical model for dense facial flow due to the in-ability of humans to densely annotate sequences and the limited robustness of the optical flow techniques <ref type="bibr" target="#b11">[12]</ref>. Hence, the research community has focused on developing statistical facial models built on a sparse set of landmarks <ref type="bibr" target="#b38">[39]</ref>, which provide limited accuracy to the recognition of subtle expressions <ref type="bibr" target="#b18">[19]</ref>. In this paper, we build the first, to the best of our knowledge, statistical models of dense facial flow by capitalising on the success of recent optical flow techniques applied to densely tracking image sequences <ref type="bibr" target="#b13">[14]</ref>. Due to the use of the statistical low-rank model, and in contrast to existing approaches, our method is able to deal with particularly challenging conditions such as severe occlusions of the face and strong illumination changes. Furthermore, the introduction of a known deformation basis drastically reduces the dimensionality of the multi-frame optical flow problem and leads to a very efficient algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Contributions</head><p>We formulate a novel energy minimisation problem for the robust estimation of multi-frame optical flow in an expressive sequence of facial images. Given that the range of motion expressible by a human face is limited, and that faces themselves are well known to be highly correlated and compactly described by a low-dimensional subspace, we build a dense deformation basis for faces.</p><p>Furthermore, we exploit the even higher correlation between face deformations in a specific input sequence by imposing a low-rank prior on the coefficients of the deformation basis. This acts as a data-specific regularisation term leading to temporally consistent optical flow. We also incorporate a sparse landmark prior term to guide the flow estimation in sparse point locations that are accurately predicted by a state-of-the-art face alignment method <ref type="bibr" target="#b15">[16]</ref>. Finally, we formulate the photometric cost by utilising a state-of-the-art dense feature descriptor that offers robustness even with the usage of a simple quadratic penaliser. Our proposed model-based problem formulation, in conjunction with the inverse compositional strategy and lowrank matrix optimisation that we adopt, leads to a highly efficient algorithm for calculating optical flow across a facial sequence. For experimental evaluation, we show quantitative experiments on a very challenging novel benchmark of face sequences with dense ground truth optical flow based on motion capture data. We also provide qualitative results on a real sequence displaying fast motion and natural occlusions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Further Related Work</head><p>There is a very large body of work on facial alignment that largely revolves around the concept of identifying a set of sparse target landmarks within an image. The most relevant algorithm to our proposed method is that of the Active Appearance Model (AAM) <ref type="bibr" target="#b9">[10]</ref>, particularly the variation by Baker and Matthews <ref type="bibr" target="#b23">[24]</ref> that relates AAMs to the Lucas-Kanade <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b6">7]</ref> optical flow literature. However, our method does not incorporate an appearance model and relies on a single given template image and is thus closer in nature to the original Lucas-Kanade algorithm. Our algorithm also places a low-rank constraint on the shape model coefficients enforcing a form of temporal consistency, which has not been previously considered.</p><p>It is also important to note that, other than a couple of examples <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref>, the AAM literature has focused on the recovery of sparse landmarks, not a dense motion field as in this work. Even in the cases of <ref type="bibr" target="#b25">[26,</ref><ref type="bibr" target="#b3">4]</ref>, the warping of the input images is achieved via an interpolation method such as piecewise affine or thin-plate splines. In this work, our warping method is derived directly from the deformation basis itself and thus recovers dense correspondences. As we show in Section 4, the linear nature of our warp allows us to derive a very efficient optimisation strategy, based on the Inverse Compositional algorithm proposed by Baker and Matthews <ref type="bibr" target="#b6">[7]</ref>.</p><p>Finally, the work of Kemelmacher-Schlizerman et al.</p><p>[17] is relevant as it considers learning deformation fields between images of faces. However, <ref type="bibr" target="#b16">[17]</ref> considers an optical flow method as a key component of the method and does not propose a novel optical formulation, in contrast to our proposed model-based algorithm.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Face Flow Representation</head><p>Let us assume that the input face video contains F frames. Let M ⊂ R 2 be a 2D domain that corresponds to the region of a mean face in neutral expression that will be used as a template. We seek to estimate a function u(x; f ) : M × {1, . . . , F } → R 2 that represents the optical flow from this domain to every frame of the input sequence. More precisely, this function establishes the correspondence between every facial point x in the domain M and its location at every frame index f , which is given by the warping function W f (x) = x + u(x; f ). This warping function registers the f -th frame to the domain M .</p><p>Exploiting prior knowledge about the warping functions that are yielded from facial deformations, we adopt a linear model for every W f (x) as follows:</p><formula xml:id="formula_0">W f (x) = W (x; c f ) = B(x), c f , x ∈ M,<label>(1)</label></formula><p>where B : M → R D is a learnt basis of facial deformations that contains D basis vector elements and is common to all frames. Also, c f ∈ R D is the coefficient vector for the f -th frame. B(x) is constructed a priori during a training process and therefore, for an input face video, we transform the multi-frame optical flow estimation to the estimation of the following D × F matrix:</p><formula xml:id="formula_1">C = [c 1 · · · c f · · · c F ] ,<label>(2)</label></formula><p>The f -th column of this matrix contains the coefficients that yield the warping function (and thus define the optical flow) for the f -th frame of the video. Following AAMs <ref type="bibr" target="#b9">[10]</ref>, the first 4 components of these coefficients, which correspond to the first 4 rows of the coefficient matrix C, control the similarity transformation that rigidly-aligns the template to every frame. The remaining components control the nonrigid deformations. Therefore, we decompose C into the following sub-matrices:</p><formula xml:id="formula_2">C = C s C nr ,<label>(3)</label></formula><p>where C s and C nr correspond to the similarity and nonrigid part of the facial deformations respectively. C s is a 4×F matrix and C nr is a K ×F matrix, where</p><formula xml:id="formula_3">K = D −4</formula><p>is the rank of non-rigid deformations of the model.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Proposed Energy</head><p>Let I(x; f ) : Ω × {1, . . . , F } → R Nc be the N c -channel sequence of frames of the input video, where Ω is the rectangular image domain that corresponds to this video. The channels of the input frames originate from the application of some appropriate feature descriptor.</p><p>As a preprocessing step, a frame of the sequence which is as close as possible to a frontal pose and a neutral expression is selected as reference. This frame is warped to the template domain M in order to match a mean face. This selection and warping estimation can be easily done automatically by fitting the face with an automatic facial alignment method <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b23">24]</ref>. In this case, we assume that there is a known correspondence between the sparse points found by the alignment method and the reference frame of our basis. Once the sparse landmarks have been acquired, it is simple to warp the image into the reference frame using a warping function such as piecewise affine. This procedure is identical to the one performed when building Active Appearance Models, with the exception of only being performed on a single template image. In short, this warped reference frame defines the template image T (x) : M → R Nc . We also consider the case where, as further preprocessing, a sparse set of facial landmarks is localised and tracked in the video. Let L be the total number of landmarks and ℓ i,f ∈ R 2 the position of the i-th landmark on the f -th frame. In addition, letl i ∈ R 2 be the position of the i-th landmark on the template image, which is computed by applying the warping function on the corresponding landmark of the reference frame.</p><p>We propose to estimate the face flow through the minimisation of the following energy:</p><formula xml:id="formula_4">E(C) = E img (C) + βE land (C) ,<label>(4)</label></formula><p>under the low-rank constraint:</p><formula xml:id="formula_5">rank(C nr ) ≤ λ,<label>(5)</label></formula><p>where C nr is the non-rigid part of C (3). E img is an image data term and E land is a landmark term. The positive weight β controls the balance between these terms, whereas the integer 0 ≤ λ ≤ K is the imposed maximum rank of nonrigid deformations for the input sequence. We now define and explain the different parts of this minimisation problem. The first term (E img ) enforces consistency of the feature descriptor values of every point of the template over all frames:</p><formula xml:id="formula_6">E img = F f =1 M T (x) − I (W (x; c f ) ; f ) 2 dx,<label>(6)</label></formula><p>In general, such an image data term could be grossly affected by artifacts in the image, such as illumination variation and external occlusions. Therefore, it is common to use a robust penaliser rather than the quadratic term shown in <ref type="bibr" target="#b5">(6)</ref>. However, we act on recent advancements in facial alignment algorithms <ref type="bibr" target="#b4">[5]</ref> that suggest that densely sampled feature descriptors can vastly improve the performance of alignment algorithms without sacrificing the efficiency of a quadratic optimisation. The use of dense descriptors is similar to SIFTFlow <ref type="bibr" target="#b19">[20]</ref>, where SIFT <ref type="bibr" target="#b20">[21]</ref> features are densely sampled at every pixel in order to improve optical flow. The second term (E land ) is a quadratic prior that ensures that the estimated face flow is in accordance with the landmark information on the corresponding sparse points in every frame:</p><formula xml:id="formula_7">E land = F f =1 L i=1 W (l i ; c f ) − ℓ i,f 2 ,<label>(7)</label></formula><p>Regarding the low-rank constraint (5), it is natural to assume that the deformations of the face over time are highly correlated and thus lie in a low-dimensional subspace. However, the similarity transformations describing the face motion are, in general, not sufficiently correlated with the non-rigid deformations that the face undergoes. For example, the similarity transformations often originate from camera motion. Consequently, we penalise the number of independent components needed to describe the non-rigid face deformations of the specific input sequence and thus impose C nr to be of low-rank.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Estimation of the Sparse Landmarks</head><p>In order to estimate sparse landmarks, we can make use of state-of-the-art, extremely efficient facial alignment methods <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b1">2]</ref>. State-of-the-art methods, such as that by Kazemi et al. <ref type="bibr" target="#b15">[16]</ref>, execute in under a millisecond, and can provide landmark localisation errors of within 3 pixels on average for extremely challenging unconstrained images. In this paper, when we consider estimating landmarks, we use the method of <ref type="bibr" target="#b15">[16]</ref> in conjunction with a robust face detector <ref type="bibr" target="#b41">[42]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Learning the Deformation Basis</head><p>Learning the deformation basis is a very challenging issue and most likely the reason why there is little research in building dense facial deformation models. However, inspired by the performance of recent optical methods, such as that of Garg et al. <ref type="bibr" target="#b13">[14]</ref>, we chose to build our basis using the output of optical flow methods. To realise this, we chose the optical flow method of Garg et al. <ref type="bibr" target="#b13">[14]</ref>, augmented with an additional quadratic penalty involving automatically estimated sparse landmarks. This additional penalty was found to significantly improve the performance of <ref type="bibr" target="#b13">[14]</ref> in expressive sequences, such as wide openings of the mouth.</p><p>We propose to learn a set of trajectories over a number of sequences, each with a differing reference frame. We are thus faced with the problem of achieving correspondence between these reference frames for the construction of the deformation basis. Given that each frame contains estimated sparse landmarks, we calculate the mean position of each landmark and define the area of spatial support for our deformation basis, M , to be the pixels that are situated inside the convex hull of the these positions. Once the reference frame is constructed, each set of trajectories is converted into endpoints for each frame, analogous to dense landmarks for the image, and sampled into the reference frame using a thin-plate splines warp parametrised by the automatically estimated landmarks. Finally, given that we have a set of dense landmarks in correspondence, we perform a Procrustes alignment in order to normalise any scale issues that may be present.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Optimisation of the Proposed Energy</head><p>The image data term is highly non-convex, therefore we have to adopt an iterative linearisation scheme. For this scheme to be computationally efficient, we consider an inverse compositional (IC) strategy. In every iteration, we seek to update the current estimateC = [c 1 · · ·c F ] of the coefficient matrix. In addition, we consider a spatial discretisation of E img on a regular pixel grid with unary steps. Let x 1 , . . . , x P be the 2D locations of the P pixels that lie within the domain M .</p><p>The IC algorithm, as proposed by <ref type="bibr" target="#b6">[7]</ref>, is a very efficient method of solving a parametrised image alignment problem, which corresponds to minimising solely the image data term E img of the proposed energy for every frame of the sequence. Given a single template image T and a single input image I, the classical Lucas-Kanade <ref type="bibr" target="#b21">[22]</ref> problem is given by</p><formula xml:id="formula_8">P p=1 T (W (x p ; ∆c)) − I (W (x p ; c)) 2 ,<label>(8)</label></formula><p>which is minimised for ∆c, the parameters of the warp for a single image. Here, T (W (x p ; ∆c)) denotes the template warped around the current linearised estimate of ∆c. The IC algorithm is so efficient because we assume that we linearise (8) around ∆c and thus the template is fixed and does not require warping during the updates. To update the parameters c, a compositional update is performed:</p><formula xml:id="formula_9">W (x p ; ∆c) ← W (x p ; ∆c) · W (x p ; ∆c) −1</formula><p>. This update ensures that the derivative with respect to the warp is also fixed and therefore we arrive at the extremely efficient update for c:</p><formula xml:id="formula_10">∆c = (J T J ) −1 P p=1 J T [I (W (x p ; c)) − T (x p )],<label>(9)</label></formula><p>where J = ∇T ∂W ∂c , and the derivative ∂W ∂c is taken around (x p ; 0) = B(x p ). Therefore, the entire Hessian term H = J T J , does not depend on c and can be precomputed. Unlike in most previous works in the area, our motion model is completely translational and thus does not involve a complicated compositional update. In fact, it can be shown that our compositional update has the form c ← c − ∆c and is thus equivalent to the additive parameter update scheme <ref type="bibr" target="#b2">[3]</ref>.</p><p>Returning to the optimisation of the proposed energy, the image data term can be approximated as (after the IC strategy and the spatial discretisation):</p><formula xml:id="formula_11">E img ≈ F f =1 P p=1 T (W (x p ; ∆c f )) − I (W (x p ;c f ) ; f ) 2 ,</formula><p>(10) where ∆c f are the additive warp parameters for frame f . Note that ∆c f = c f −c f , a relation that we use since in our formulation, in contrast to the traditional IC algorithm, we incorporate terms that depend directly on c f . By considering linearisations of the template in the above equation and rewriting the terms using compact matrix notation over all pixels and frames, the total proposed energy becomes:</p><formula xml:id="formula_12">E(C) ≈ R + J (C −C) 2 F + β B ℓ C − L loc 2 F ,<label>(11)</label></formula><p>where R is a (P N c )×F matrix that contains the error residuals T (x p ))−I (W (x p ;c f ) ; f ) for all pixels p and frames f . Also, J is a (P N c ) × D matrix that contains the template Jacobian ∇T (x p )B(x p ) for all pixels. Finally, B ℓ is a 2L × D matrix consisting of the deformation basis evaluated at the locations of the landmarks on the template and L loc is a 2L × F matrix with the coordinates of the landmarks in all frames:</p><formula xml:id="formula_13">B ℓ =    B(l 1 ) . . . B(l L )    , L loc =    ℓ 1,1 · · · ℓ 1,F . . . . . . ℓ L,1 · · · ℓ L,F    ,<label>(12)</label></formula><p>Using the decomposition of C in a similarity and a nonrigid part (3), the energy (11) is written as:</p><formula xml:id="formula_14">E(C s , C nr ) ≈ R + J nr (C nr −C nr ) + J s (C s −C s ) 2 F +β B ℓnr C nr − B ℓs C s − L loc 2 F ,</formula><p>Consequently, we propose to solve the following rank constraint optimisation problem:</p><formula xml:id="formula_15">min Cs,Cnr E(C s , C nr ) s.t. rank(C nr ) ≤ λ,<label>(13)</label></formula><p>Although <ref type="formula" target="#formula_0">(13)</ref> is a non-convex problem, it can be solved efficiently by employing a block-coordinate descent (BCD) scheme. Let t be the iteration index. The iteration of BCD for (13) reads as follows:</p><formula xml:id="formula_16">C s [t + 1] = min Cs[t] E(C s [t], C nr [t]),<label>(14)</label></formula><formula xml:id="formula_17">C nr [t + 1] = min Cnr[t+1] E(C s [t + 1], C nr [t]), s.t. rank(C nr ) ≤ λ.<label>(15)</label></formula><p>The sub-problem <ref type="formula" target="#formula_0">(14)</ref> is a least-squares problem admitting a closed-form solution.</p><p>The sub-problem <ref type="formula" target="#formula_0">(15)</ref> is also solved in closed-form. First, let us define the matrices</p><formula xml:id="formula_18">A = R + J s (C s −C s ) B ℓs C s − L loc , Q = J nr B ℓnr ,<label>(16)</label></formula><p>with Q = U ΣV being the Thin Singular Value Decomposition of Q, Q † denoting the pseudo-inverse of Q, Ξ = U U T A, and Ξ (λ) being the λ-rank approximation of Ξ. Then by using <ref type="bibr" target="#b15">(16)</ref>, <ref type="formula" target="#formula_0">(15)</ref> is written as:</p><formula xml:id="formula_19">min Cnr A − QC nr 2 F s.t. rank(C nr ) ≤ λ,<label>(17)</label></formula><p>The closed form solution of <ref type="formula" target="#formula_0">(17)</ref> is given by <ref type="bibr" target="#b32">[33]</ref>:</p><formula xml:id="formula_20">C nr = Q † Ξ (λ) .<label>(18)</label></formula><p>The convergence of the proposed BCD algorithm is guaranteed since the objective function is differentiable and involves two blocks of variables <ref type="bibr" target="#b22">[23]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Experiments</head><p>In this section, we describe the set of qualitative and quantitative experiments that we conducted in order to demonstrate the effectiveness of our proposed algorithm, Face Flow. In order to verify that our method is competitive with the state-of-the-art, we compared against the methods of Garg et al. <ref type="bibr" target="#b13">[14]</ref> (denoted MFSF), Revaud et al. <ref type="bibr" target="#b26">[27]</ref> (denoted EPICFlow), Liu et al. <ref type="bibr" target="#b19">[20]</ref> (denoted SIFTFlow) and the large displacement optical method of Brox et al. <ref type="bibr" target="#b8">[9]</ref> (denoted LDOF). We also provide two formulations of our method, one which enforces the low-rank constraint on the coefficients and one which does not. The latter corresponds to the choice of λ = k. We denote these two methods Face Flow Low-Rank (LR) and Face Flow Full-Rank (FR). This self evaluation is particularly useful for demonstrating the importance and effectiveness of the low-rank constraint for multi-frame facial flow.</p><p>To effectively evaluate Face Flow, we propose a novel ground truth dataset formed from facial motion capture data <ref type="bibr" target="#b42">[43]</ref>. The sequence that we evaluate must be in correspondence and ideally contain an interesting sequence of deformations. Performance capture data is ideal for this purpose, as it is necessarily in correspondence and often deals with </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.1.">Practical Deformation Basis Construction</head><p>Our facial deformation basis is built by applying MFSF multi-frame optical flow method 1 of Garg et al. <ref type="bibr" target="#b13">[14]</ref> on the facial expression database BU4D <ref type="bibr" target="#b40">[41]</ref>. We chose this database due to the large range of expression present and the fact that is captured at a high frame rate, which is ideal for the technique of <ref type="bibr" target="#b13">[14]</ref>. However, in order to further improve the performance of <ref type="bibr" target="#b13">[14]</ref>, we augmented the energy to include an extra quadratic landmark constraint. This landmark constraint takes a similar form to the landmark constraint proposed in this paper, and was found to improve the results considerably in sequences that displayed particularly expressive emotion, such as surprise.</p><p>The BU4D database consists of 102 subjects displaying 6 canonical expression, from neutral to the apex of the emo- <ref type="bibr" target="#b0">1</ref> Code publicly available at https://bitbucket.org/troussos/mfsf/ tion. We selected a neutral frame for every sequence and used this as the reference frame for the method of <ref type="bibr" target="#b13">[14]</ref>. After computing trajectories for each sequence, we constructed a reference frame for our deformation basis using the mean of the neutral images we selected previously. We then applied principal component analysis (PCA) to learn the linear deformation model as described in Section 5.2. Experimentally, we found that k = 20 principal components of non-rigid deformation accounts for 95% of the variance. We note that the BU4D data shows frontal faces and thus our model does not capture out-of-plane rotation. However, there is no practical reason that our PCA basis could not capture this kind of deformation.</p><p>In order to improve the robustness of our algorithm, we adopt a pseudo coarse-to-fine strategy for basis construction and create three bases of increasing scale. This is also commonly employed within optical algorithms to improve robustness. In all of the following experiments, the feature descriptor employed is the dense SIFT feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.2.">Motion Capture Data</head><p>In this experiment, we use the performance capture dataset provided by Zhang et al. <ref type="bibr" target="#b42">[43]</ref> to generate three novel ground truth sequences consisting of 280 frames. This ground truth is provided by rendering the sequence of meshes in a fixed pose, which yields both a texture and a set of vertices in the scene. These vertices can then be used in order simply calculate flow for the face throughout the image.</p><p>As mentioned, we rendered three sets of texture, all with the same underlying geometry, to provide evidence of the robustness of Face Flow to challenging conditions. In the first sequence, we rendered unmodified textures. This is a baseline in order to show the performance of state-of-theart methods for facial data. Since our basis is trained using the output from <ref type="bibr" target="#b13">[14]</ref>, we do not expect to outperform other optical flow techniques on this sequence. The second sequence was rendered by rendering a periodically moving light source around the face. This is challenging for the data term and helps to demonstrate the robustness of our chosen feature descriptor. The final sequence is highly challenging. It contains the periodic illumination variation from the previous sequence and also an artificial occlusion in the form of a smoothly translating hand.</p><p>In order to initialise our Face Flow algorithm, we manually annotated the first frame as the reference frame. To obtain an initial estimate of the coefficient matrix C, the landmark constraint quadratic term is solved, which provides a reasonable estimate of the initial shape. Once solved in the reference frame, this initialisation was propagated across every frame in the sequence. The landmark constraint was otherwise not utilised in this experiment.</p><p>To evaluate the performance of the methods, we computed the root mean squared error of the endpoints (RMSE), shown in <ref type="table">Table 2</ref>. As expected Face Flow does not outperform state-of-the-art methods in the original un-tampered sequence. However, in the more interesting case of the illumination variation and occlusion sequences, our Face Flow method with low-rank constraint (Face Flow LR), performs the best. We also note that the low-rank method of our technique significantly outperforms the full-rank version, particularly in the occluded sequence. An example set of endpoints is given in <ref type="figure" target="#fig_0">Figure 1</ref>. Note how the face deformation is well localised and unlikely to undergo any gross deformations due to being constrained by a statistical basis.</p><p>Finally, to provide evidence as to the stability of our algorithm, and the the effect of the low-rank constraint on the outcome of the sequence, we present <ref type="figure">Figure 2</ref>. This figure shows the mean per-frame endpoint error across the sequence. The Face Flow methods are given by the blue and green lines. Note how stable the Face Flow LR method is, particularly when compared to the Face Flow FR method. We believe this effectively demonstrates the positive effect enforcing soft temporal consistency can have. <ref type="bibr" target="#b26">[27]</ref> SF <ref type="bibr" target="#b19">[20]</ref> 1.4 0.5 20 15 50 15  <ref type="table">Table 2</ref>: RMSE and 95% average endpoint error for the synthetic data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>FF LR FF FR MFSF LDOF EF</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.3.">Real Sequence</head><p>For this experiment, we provide results on a real sequence consisting of 150 frames of a young woman watching a video. In this sequence, she reacts negatively and clearly presents discomfort by touching her face and shifting in her seat. This amounts to challenging variation in the sequence, including occlusions from her hand and motion blur from movement. In this sequence, we also provide evidence as to the benefit of incorporating the landmark constraint. The sequence was automatically landmarked using <ref type="bibr" target="#b15">[16]</ref>, then our method was initialised with these landmarks for every frame. We also used the landmarks for the quadratic penalizer term.</p><p>As <ref type="figure">Figure 3</ref> shows, Face Flow LR performs very well in this sequence. In particular, we note that our method is very robust to the presence of occlusions, aided by the sparse landmarks provided by <ref type="bibr" target="#b15">[16]</ref>. We also provide <ref type="table" target="#tab_0">Table 1</ref> which demonstrates that Face Flow is an order of magnitude more efficient than the other considered methods for this sequence.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="8.">Conclusion</head><p>We presented an optical flow method that incorporates a dense basis of facial shape. We evaluated our method on a ground truth motion capture sequence and demonstrated that our proposed algorithm, Face Flow, outperforms other state-of-the-art optical flow methods. In particular, the introduction of a low-rank constraint yields a robust multiframe optical flow technique. As future work, we intend to investigate a more complex dataset that would enable Face Flow to model out-of-plane rotations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Example endpoint results for the synthetic sequence including illumination variation. Each row shows a different frame of the 280 frame synthetic sequence (Frame 16, 54, 139 and 233 from top to bottom)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :Figure 3 :</head><label>23</label><figDesc>Figure 2: The average endpoint error calculated for each frame of the mocap sequence. Vertical axis is average endpoint error, horizontal is frame number. Top row is the illumination sequence, bottom row is illumination + occlusion.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Run times per frame (in seconds) for the 150 frame sequence shown in Figure 3. Images are 640 × 480. Per- formed on an Intel Xeon E5-1650 3.20GHz (32GB RAM). All times are approximate and averaged over multiple runs. EPICFlow times are dominated by DEEPMatching [38] ( 48s).SF [20] 2.65 5.15 4.89 11.81 11.82 23.05</figDesc><table>Original 
Illum. 
Ilum.+Occ. 

RMSE 
AE95 
RMSE 
AE95 
RMSE 
AE95 

FF LR 
2.95 5.52 3.56 
6.63 
4.48 
8.47 
FF FR 
3.24 6.01 3.76 
7.02 
5.83 11.50 
MFSF 
1.73 3.20 6.33 13.68 
8.25 17.30 
LDOF 
1.56 2.79 4.84 
9.98 
6.54 11.44 
EF [27] 1.66 3.25 4.02 
9.61 
5.15 11.61 
</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>Patrick Snape is funded by a DTA from Imperial College London and by a Qualcomm Innovation Fellowship. Yannis Panagakis was funded by the ERC under the FP7 Marie Curie Intra-European Fellowship. Stefanos Zafeiriou is partially supported by the EPSRC project EP/J017787/1 (4D-FAB) and is also partially supported by the EPSRC project EP/L026813/1 Adaptive Facial Deformable Models for Tracking (ADAManT).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Menpo: A comprehensive platform for parametric image alignment and visual deformable models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Booth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Snape</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Bayesian active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3438" to="3445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">On compositional image alignment, with an application to active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Amberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Vetter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1714" to="1721" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Using bounded diameter minimum spanning trees to build dense active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Stenger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">110</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="48" to="57" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Feature-based lucas-kanade and active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Antonakos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Alabort-I-Medina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TIP</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="2617" to="2632" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">4</biblScope>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">High-quality passive facial performance capture using anchor frames</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Beeler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Hahn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bradley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bickel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Beardsley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gotsman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">W</forename><surname>Sumner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TOG</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">75</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Large displacement optical flow: Descriptor matching in variational motion estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="500" to="513" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">F</forename><surname>Cootes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">J</forename><surname>Edwards</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Taylor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">A facs valid 3d dynamic action unit database with applications to 3d dynamic morphable facial modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cosker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Krumhuber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hilton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2296" to="2303" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Optical flow constraints on deformable models with applications to face tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Decarlo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Metaxas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="99" to="127" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Dense variational reconstruction of non-rigid surfaces from monocular video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1272" to="1279" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">A variational approach to video registration with subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roussos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Agapito</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">104</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Multi-frame correspondence estimation using subspace constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">48</biblScope>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">One millisecond face alignment with an ensemble of regression trees</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kazemi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1867" to="1874" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Collection flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kemelmacher-Shlizerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1792" to="1799" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dynamic texturebased approach to recognition of facial actions and their temporal models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Koelstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Patras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="1940" to="1954" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">A spontaneous micro-expression database: Inducement, collection and baseline</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pfister</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pietikainen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Sift flow: Dense correspondence across scenes and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page">7</biblScope>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Distinctive image features from scale-invariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCV</title>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="91" to="100" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<editor>AI</editor>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">On the convergence of the coordinate descent method for convex differentiable minimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tseng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Optimization Theory and Applications</title>
		<imprint>
			<biblScope unit="volume">72</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="7" to="35" />
			<date type="published" when="1992" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Active appearance models revisited. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Generalised scalable robust principal component analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papamakarios</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Panagakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Increasing the density of active appearance models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ramnath</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">EpicFlow: Edge-Preserving Interpolation of Correspondences for Optical Flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Dense lagrangian motion estimation with occlusions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1800" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simultaneous compaction and factorization of sparse image motion matrices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="456" to="469" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Video motion for every visible point</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ricco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tomasi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2464" to="2471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards longer long-range motion trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rubinstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>Freeman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Particle video: Long-range motion estimation using point trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Teller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IJCV</title>
		<imprint>
			<biblScope unit="volume">80</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="72" to="91" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Best approximate solutions to matrix equations under rank restrictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sondermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistische Hefte</title>
		<imprint>
			<date type="published" when="1986" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="57" to="66" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Dense point trajectories by gpu-accelerated large displacement optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sundaram</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Keutzer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Space-time tracking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis and Processing</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2002" />
			<biblScope unit="page" from="801" to="812" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Tracking and modeling non-rigid objects with rank constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">J</forename><surname>Alexander</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
		<idno>pages -I-500</idno>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">An improved algorithm for TV-L1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Wedel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Statistical and Geometrical Approaches to Visual Motion Analysis</title>
		<meeting><address><addrLine>Berlin</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="23" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1385" to="1392" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Recognizing human facial expressions from long image sequences using optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yacoob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">S</forename><surname>Davis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE T-PAMI</title>
		<imprint>
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="636" to="642" />
			<date type="published" when="1996" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A 3d facial expression database for facial behavior research</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rosato</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">FG</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page" from="211" to="216" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
		<title level="m" type="main">A survey on face detection in the wild: Past, present and future</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">138</biblScope>
			<biblScope unit="page" from="1" to="24" />
		</imprint>
		<respStmt>
			<orgName>CVIU</orgName>
		</respStmt>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Spacetime faces: High-resolution capture for modeling and animation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Snavely</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">M</forename><surname>Seitz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Data-Driven 3D Facial Animation</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
