<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:10+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Recognizing Human Actions as the Evolution of Pose Estimation Maps</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mengyuan</forename><surname>Liu</surname></persName>
							<email>liumengyuan@ntu.edu.sgjsyuan@buffalo.edu</email>
							<affiliation key="aff0">
								<orgName type="department">School of Electrical and Electronic Engineering</orgName>
								<orgName type="institution">Nanyang Technological University</orgName>
								<address>
									<postCode>639798</postCode>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junsong</forename><surname>Yuan</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Department of CSE</orgName>
								<orgName type="institution">SUNY at Buffalo</orgName>
								<address>
									<postCode>14260</postCode>
									<settlement>Buffalo</settlement>
									<region>NY</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Recognizing Human Actions as the Evolution of Pose Estimation Maps</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Most video-based action recognition approaches choose to extract features from the whole video to recognize actions. The cluttered background and non-action motions limit the performances of these methods, since they lack the explicit modeling of human body movements. With recent advances of human pose estimation, this work presents a novel method to recognize human action as the evolution of pose estimation maps. Instead of relying on the inaccurate human poses estimated from videos, we observe that pose estimation maps, the byproduct of pose estimation, preserve richer cues of human body to benefit action recognition. Specifically, the evolution of pose estimation maps can be decomposed as an evolution of heatmaps, e.g., probabilistic maps, and an evolution of estimated 2D human poses, which denote the changes of body shape and body pose, respectively. Considering the sparse property of heatmap, we develop spatial rank pooling to aggregate the evolution of heatmaps as a body shape evolution image. As body shape evolution image does not differentiate body parts, we design body guided sampling to aggregate the evolution of poses as a body pose evolution image. The complementary properties between both types of images are explored by deep convolutional neural networks to predict action label. Experiments on NTU RGB+D, UTD-MHAD and PennAction datasets verify the effectiveness of our method, which outperforms most state-of-the-art methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>1. Introduction</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.1.">Motivation and Objective</head><p>Human action recognition from videos has been researched for decades, since this task enjoys various applications in intelligent surveillance, human-robot interaction and content-based video retrieval. The intrinsic property of existing methods <ref type="bibr" target="#b29">[22,</ref><ref type="bibr" target="#b50">43,</ref><ref type="bibr" target="#b44">37,</ref><ref type="bibr" target="#b31">24,</ref><ref type="bibr" target="#b8">1]</ref> is to learn mapping * Corresponding author functions which transform videos to action labels. Since they do not directly distinguish human body from videos, these methods are easily affected by clutters and non-action motions from backgrounds. To address this limitation, an alternative solution is to detect human <ref type="bibr" target="#b46">[39]</ref> and estimate the body pose in each frame. This approach works well in the field of human action recognition from depth videos, e.g., Microsoft Kinect <ref type="bibr" target="#b62">[55,</ref><ref type="bibr" target="#b34">27]</ref>. By detecting 3D pose from each depth frame with an accurate body pose estimation method <ref type="bibr" target="#b43">[36]</ref>, human movements in depth videos can be simplified as 3D pose sequences <ref type="bibr" target="#b59">[52]</ref>. Recent deep learning models, e.g., CNN <ref type="bibr" target="#b24">[17,</ref><ref type="bibr" target="#b27">20]</ref>, RNN <ref type="bibr" target="#b16">[9]</ref> and LSTM <ref type="bibr" target="#b33">[26,</ref><ref type="bibr" target="#b32">25]</ref>, have achieved high performances on the extracted 3D poses, which outperform methods <ref type="bibr" target="#b39">[32,</ref><ref type="bibr" target="#b57">50]</ref> that rely on raw depth video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Ground Truth Prediction</head><p>The success of 3D human pose inspires us to estimate 2D human poses from videos for action recognition. However, despite the significant advances of 2D pose estima-tion in images and videos <ref type="bibr" target="#b58">[51,</ref><ref type="bibr" target="#b12">5,</ref><ref type="bibr" target="#b53">46,</ref><ref type="bibr">2,</ref><ref type="bibr" target="#b11">4]</ref>, the performance is still inferior to the 3D pose estimation in depth videos. <ref type="figure" target="#fig_0">Fig. 1</ref> illustrates the estimated poses from video frames by a state-of-the-art pose estimation method <ref type="bibr" target="#b11">[4]</ref>. Due to complex background and self-occlusion of human body parts, the estimated poses are not fully reliable and may misinterpret the configuration of human body. In the first row of <ref type="figure" target="#fig_0">Fig. 1 (b)</ref>, the multi-modal pose estimation map in the white bounding box indicates the location of the person's hand. The map contains two peaks, where the ground truth location does not correspond to the highest peak, thus provides a wrong estimation of the hand's location.</p><p>To better utilize the pose estimation maps, instead of relying on the inaccurate 2D pose estimated from the pose estimation maps, we propose to directly model the evolution of pose estimation maps for action recognition. In <ref type="figure" target="#fig_0">Fig.  1</ref> (c), heatmaps (averaged pose estimation maps) provide richer information to reflect human body shape.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.2.">Method Overview and Contributions</head><p>Our method is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. Given each frame of a video, we use convolutional pose machines to predict pose estimation map for each body part. The goal of representing these pose estimation maps is to preserve both global cues, which reflect whole shapes that suffer less from the noise and local cues, which detail the locations of body parts.</p><p>To this end, we average pose estimation maps of all body parts to generate an averaged pose estimation map (heatmap) for each frame. The temporal evolution of heatmaps can reflect the movements of body shape. Different from the original RGB image, the heatmap is sparse. Considering the huge spatial redundancy, we develop a spatial rank pooling method to compress the heatmap as a compact yet informative feature vector. The merit of spatial rank pooling is that it can effectively suppress spatial redundancy, without significantly losing spatial distribution information of the heatmap. The temporal concatenation of feature vectors constructs a 2D body shape evolution image, which reflects the temporal evolution of body shapes.</p><p>As body shape evolution image cannot differentiate body parts, we further predict joint location from pose estimation map of each body part, generating a pose for each frame. Since the number of estimated pose joints is limited, we use body structure to guide the sampling of more abundant pose joints to represent human body. The temporal concatenation of all pose joints constructs a body pose evolution image, which reflects the temporal evolution of body parts. Intuitively, the body shape evolution image and body pose evolution image benefit the recognition of general movements of body shape and elaborate movements of body parts. Thereby, both images are explored by CNNs to generate discriminative features, which are late fused to predict action label. Generally, our contributions are three-fold.</p><p>• Given inaccurate 2D poses estimated from videos, we boost the performance of human action recognition by recognizing actions as the evolution of pose estimation maps instead of the unreliable 2D body poses.</p><p>• The evolution of pose estimation maps are described as body shape evolution image and body pose evolution image, which capture the movements of both whole body and specific body parts in a compact way.</p><p>• With CNNs and late fusion scheme, our method achieves state-of-the-art performances on NTU RG-B+D, UTD-MHAD and PennAction datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">3D Pose-based Action Recognition</head><p>3D pose provides direct physical interpretation for human actions from depth videos. Hand-crafted features <ref type="bibr" target="#b49">[42,</ref><ref type="bibr" target="#b54">47,</ref><ref type="bibr" target="#b20">13]</ref> were designed for describing evolution of 3D poses. Recently, deep neural networks were introduced to model the spatial structures and temporal dynamics of poses. For example, Du et al. <ref type="bibr" target="#b16">[9]</ref> firstly used hierarchical RNN for pose-based action recognition. Liu et al. <ref type="bibr" target="#b32">[25]</ref> extended this idea and proposed spatio-temporal LSTM to learning spatial and temporal domains. To enhance the attention capability of LSTM, Global Context-Aware Attention LSTM <ref type="bibr" target="#b33">[26]</ref> was developed with the assistance of global context.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Video-based Action Recognition</head><p>Local features are motion-related and are robust to cluttered background to some extent. Spatial temporal interest points (STIPs) <ref type="bibr" target="#b29">[22]</ref> and dense trajectory <ref type="bibr" target="#b50">[43]</ref> were applied to extract and describe local spatial temporal patterns. Based on these basic features, multi-feature max-margin hierarchical Bayesian model <ref type="bibr" target="#b56">[49]</ref> and a novel feature enhancing technique called Multi-skIp Feature Stacking <ref type="bibr" target="#b28">[21]</ref> were proposed to learn more distinctive features. Since local features ignore global relationships, holistic features were encoded by two-stream convolutional network <ref type="bibr" target="#b44">[37]</ref>, which learns spatial-temporal features by fusing convolutional networks spatially and temporally. Based on this network, the relationships between the spatial and temporal structures were further explored <ref type="bibr" target="#b18">[11,</ref><ref type="bibr" target="#b52">45]</ref>. Different from two-stream network, the spatial and temporal information of actions can be fused before they are input to CNNs. Fernando et al. <ref type="bibr" target="#b19">[12]</ref> proposed rank pooling method to aggregate all video frames to a compact representation. Bilen et al. <ref type="bibr" target="#b8">[1]</ref> deeply merged rank pooling method with CNN to generate an efficient dynamic image network.</p><p>Human actions are inherently structured patterns of body movements. Recent studies <ref type="bibr" target="#b63">[56,</ref><ref type="bibr" target="#b38">31,</ref><ref type="bibr" target="#b21">14,</ref><ref type="bibr" target="#b45">38,</ref><ref type="bibr" target="#b37">30]</ref>   For each frame, pose estimation maps are aggregated to form a heatmap and a pose. c) Spatial rank pooling is proposed to describe the evolution of heatmaps as a body shape evolution image, which contains one channel. d) Body guided sampling is proposed to describe the evolution of poses as a body pose evolution image, which contains two channels. e) Deep features are extracted from both types of images and the late fusion result predicts action label. Note that both images are normalized to fix-sized color images to facilitate transfer learning.</p><p>pose estimation tasks have been integrated to extract pose guided features for recognition. Wang et al. <ref type="bibr" target="#b48">[41]</ref> improved an existing pose estimation method, and then designed pose features to capture both spatial and temporal configurations of body parts. Xiaohan et al. <ref type="bibr" target="#b55">[48]</ref> proposed a framework to integrate training and testing of action recognition and pose estimation. They decomposed actions into poses which are further divided to mid-level ST-parts and then parts. Most recently, Du et al. <ref type="bibr" target="#b15">[8]</ref> proposed an end-to-end recurrent network which can exploit important spatial-temporal evolutions of human pose to assist action recognition in a unified framework. Different from pose features <ref type="bibr" target="#b48">[41]</ref> or poseguided color features <ref type="bibr" target="#b55">[48,</ref><ref type="bibr" target="#b15">8]</ref>, this paper recognizes human actions from only pose estimation maps, which have not been explored for action recognition task before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Generation of Pose Estimation Maps</head><p>This section predicts pose estimation maps from each frame of a video ( <ref type="figure" target="#fig_3">Fig. 3 (a)</ref>), and then generates a heatmap ( <ref type="figure" target="#fig_3">Fig. 3 (b)</ref>) and a pose ( <ref type="figure" target="#fig_3">Fig. 3 (c)</ref>) to denote each frame.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Pose Estimation Maps:</head><p>The task of human pose estimation from a single image can be modeled as a structure prediction problem. In <ref type="bibr" target="#b41">[34]</ref>, a pose machine is proposed to sequentially predict pose estimation maps for body joints, where previous predicted pose estimation maps iteratively improve the estimates in following stages. Let Y k ∈{x, y} denote the set of coordinates from body joint k. The structural output can be formulated as</p><formula xml:id="formula_0">Y = {Y 1 , ..., Y k , ..., Y K },</formula><p>where K is the total number of body joints. Multi-class classifier g k t is trained to predict the k th body joint in the t th stage. For an image position z, the pose estimation map for assigning it to the k th body joint is formulated as:</p><formula xml:id="formula_1">B k t (Y k = z)=g k t fz; i=1,...,K ψ(z, B i t−1 ) ,<label>(1)</label></formula><p>where f z is the color feature at position z, B i t−1 is the pose estimation map predicted by g i t−1 , ∪ is the operator for vector concatenation, ψ is the feature function for computing contextual features from previous pose estimation maps. After T stages, the generated pose estimation maps are used to predict locations of body joints. The pose machine <ref type="bibr" target="#b41">[34]</ref> uses boosted classifier with random forests for the weak learners. Instead, this paper applies the convolutional pose machine <ref type="bibr" target="#b53">[46,</ref><ref type="bibr" target="#b11">4]</ref> to combine pose machine with convolutional architectures, which does not need graphical-model style inference and boosts the performances of pose machine.</p><p>Heatmaps &amp; Poses: For the n th frame of a video, K types of pose estimation maps, namely {B 1,n T , ..., B K,n T }, are generated. To reduce the redundancy of pose estimation maps, we describe them as a heatmap G n and a pose L n . The heatmap G n can be expressed as:</p><formula xml:id="formula_2">Gn = 1 K K k=1 B k,n T ,<label>(2)</label></formula><p>which reflects the global body shape. The pose L n can be expressed as {z</p><formula xml:id="formula_3">k,n } K k=1</formula><p>, where z k,n is often estimated via Maximum A Posterior (MAP) criterion <ref type="bibr" target="#b11">[4]</ref>:</p><formula xml:id="formula_4">z k,n = arg max z∈Z B k,n T (Y k = z) ,<label>(3)</label></formula><p>where Z∈R 2 denote all positions on the image. Till now, each frame of a video is described as a heatmap and a pose. In other words, the video is converted to the evolution of heatmaps and the evolution of poses.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Evolution of Pose Estimation Maps</head><p>This section describes the evolution of heatmaps as a body shape evolution image using temporal rank pooling ( <ref type="figure" target="#fig_3">Fig. 3 (d)</ref>), based on which spatial rank pooling ( <ref type="figure" target="#fig_3">Fig. 3 (e)</ref>) is developed. Further, body guided sampling is developed to describe the evolution of poses as a body pose evolution image ( <ref type="figure" target="#fig_3">Fig. 3 (f)</ref>). The complementary properties between two images are learned by CNNs.</p><p>Temporal Rank Pooling: As a robust and compact video representation method, temporal rank pooling <ref type="bibr" target="#b19">[12,</ref><ref type="bibr" target="#b8">1]</ref> has the ability to aggregate the temporal relevant information throughout a video via a learning to rank approach. The encoded temporal information denotes the temporal order among frames, which is a robust feature showing less sensitive to different types of input data. As heatmaps are distinct from natural images, we treat heatmaps as a new type of data and apply temporal rank pooling to encode the evolution of heatmaps. Suppose a sequence V G = {G 1 , ..., G n , ..., G N } contains N heatmaps, and G n ∈ R P ×Q denotes the n th frame with P rows and Q columns. G 1:n can be mapped to a vector defined as:</p><formula xml:id="formula_5">v n = V 1 n n i=1 G i ,<label>(4)</label></formula><p>where the function V reshapes a matrix into a vector and</p><formula xml:id="formula_6">v n ∈ R (P ·Q)×1 . Let v n+1 ≻ v n denote the temporal order- ing relationship between v n+1 and v n . A natural constraint among frames is v N ≻ ... ≻ v n+1 ≻ v n ≻ ... ≻ v 1 .</formula><p>Temporal rank pooling (t-rk) <ref type="bibr" target="#b19">[12]</ref> optimizes parameters u ∈ R (P ·Q)×1 of a linear function ψ(v; u) to ensure that</p><formula xml:id="formula_7">∀n i ,n j , v ni ≻ v nj ⇔ u T · v ni &gt; u T · v nj .</formula><p>The parameter u is used as the representation of temporal rank pooling method, as it implicitly encodes the appearance evolution information of the sequence.</p><p>Spatial Rank Pooling: We reshape u as the same size of input frame to facilitate the observation. As shown in <ref type="figure" target="#fig_3">Fig. 3 (d)</ref>, the temporal rank pooling method mainly preserves spatial information while ignores most of the temporal information. To improve this, we propose a novel spatial rank pooling method (s-rk) which takes both spatial and temporal information into account. The observation is that there exists huge spatial redundancy in each heatmap. Therefore, we take advantage of the learning to rank method to reduce each heatmap to a compact feature, which has the ability of preserving spatial order. Concatenating all feature vectors according to the temporal order will generate a body shape evolution image, which can preserve both spatial and temporal information of heatmaps in a compact way. The pipeline of generating body shape evolution image is shown in <ref type="figure" target="#fig_4">Fig. 4</ref>. Specifically, we partition the n th frame of the sequence V into P rows, i.e., </p><formula xml:id="formula_8">G n =[ ( p 1 ) T , ..., (p s ) T , ..., (p P ) T ] T ,</formula><formula xml:id="formula_9">arg min u η 1 2 u η 2 + W ∀i,j v η s i ≻v η s j ij , s.t. (u η ) T · (v η s i − v η s j ) ≥ 1 − ij ij ≥ 0<label>(5)</label></formula><p>where η ∈{ p, q}, u p ∈ R Q and u q ∈ R P . For all N frames, we catenate vectors according to the temporal order, and obtain U p ∈ R Q×N and U q ∈ R P ×N . The final matrix U via spatial rank pooling is defined as (U p ) T , (U q ) T T , where U is called body shape evolution image.</p><p>Body Guided Sampling: Since body shape evolution image only considers the shape of accumulated pose estimation map while does not differentiate different joints, we need body pose evolution image to consider the information from each specific joint. To this end, this section builds body pose evolution image from joints, which are densely sampled by body guided sampling to denote the specific pose of human body. Suppose a sequence</p><formula xml:id="formula_10">V L = {L 1 , ..., L n , ..., L N } contains N poses, where L n = {z k,n } K k=1</formula><p>and z k,n =( x k,n ,y k,n ), which denote the horizontal and vertical coordinates of the k th joint. When  <ref type="table" target="#tab_3">595  620  572  548  475  668  691  620  572  572  594  644  620  620   595  620  572  548  475  548  572  620  572  572  594  572  572  620  644  620  620  620  644  620  668  691  620  691  668</ref>  To encode the spatial and temporal evolution of pose joints in a compact way, we represent the pre-processed pose sequence V L as a body pose evolution image. Since original estimated pose joints are too sparse to represent the human body, we sort the order of joint labels according to the body structure, and use linear interpolation to sample abundant points from pose limbs. The pipeline of building one channel of body pose evolution image from horizontal coordinates is shown in <ref type="figure">Fig. 5</ref>. Another channel of body pose evolution image from vertical coordinates can be similarly built. Mathematically, let</p><formula xml:id="formula_11">x n =[x 1,n , ..., x K ′ ,n ] T and y n =[ y 1,n , ..., y K ′ ,n ]</formula><p>T denote the coordinate vector of the generated joints, where K ′ is the total number of sampled joints. In our experiments, the pose limbs can be roughly denoted by sampling five joints on each limb. Suppose , denoting horizontal and vertical coordinates, respectively. Both channels reflect the temporal evolution of joints, which are densely sampled to denote the pose of human body.</p><p>Late Fusion: A video I c has been denoted as a body shape evolution image and a body pose evolution image, where c means the c th sample from a batch that is used for training. As CNN has achieved success in image classification task, we use CNN model that is pre-trained on Imagenet <ref type="bibr" target="#b14">[7]</ref> for transfer learning. Since these two images contain significantly different spatial structure, we use separate CNN to explore deep features from them. To accommodate existing CNN models, the single channel of body shape evolution image is repeated three times to form a 3-channel image, and two channels of body pose evolution image are combined with a zero-valued channel to form a 3-channel image. Let {I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Datasets and Settings</head><p>PennAction dataset <ref type="bibr" target="#b61">[54]</ref> contains 15 action categories and 2326 sequences in total. Since all sequences are collected from the internet, complex body occlusions, large appearance and motion variations make it challenging for  <ref type="formula" target="#formula_1">4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60</ref>  pose-related action recognition <ref type="bibr" target="#b55">[48,</ref><ref type="bibr" target="#b15">8]</ref>. We follow <ref type="bibr" target="#b55">[48]</ref> to split the data into half and half for training and testing. Snaps with estimated poses 1 are shown in <ref type="figure" target="#fig_6">Fig. 6</ref>. NTU RGB+D dataset <ref type="bibr" target="#b42">[35]</ref> contains 60 actions performed by 40 subjects from various views, generating 56880 sequences. Following the cross subject protocol in <ref type="bibr" target="#b42">[35]</ref>, we split the 40 subjects into training and testing groups. Each group contains samples captured from different views performed by 20 subjects. For this evaluation, the training and testing sets have 40320 and 16560 samples, respectively. Following the cross view protocol in <ref type="bibr" target="#b42">[35]</ref>, we use all the samples of camera 1 for testing and samples of cameras 2, 3 for training. The training and testing sets have 37920 and 18960 samples, respectively.</p><p>UTD-MHAD dataset <ref type="bibr" target="#b13">[6]</ref> was collected using a Microsoft Kinect sensor and a wearable inertial sensor in an indoor environment. It contains 27 actions performed by 8 subjects. Each subject repeated each action 4 times, generating 861 sequences. We use this dataset to compare the performances of methods using different data modalities. Cross subject protocol <ref type="bibr" target="#b13">[6]</ref> is used for the evaluation.</p><p>Implementing details: In our model, each CNN contains five convolutional layers and three fc layers. The first and second fc layers contain 4096 neurons, and the number of neurons in the third one is equal to the total number of action classes. Filter sizes are set to 11 × 11, 5 × 5, 3 × 3, 3 × 3 and 3 × 3, respectively. Local Response Normaliza-1 These poses are generated by pose estimation method <ref type="bibr" target="#b11">[4]</ref>, which can be found from https://github.com/tensorboy/pytorch Realtime MultiPerson Pose Estimation. Four pose joints on the head are not used since they are redundant for denoting actions. For scenes with multi-person, the coordinates of joints are averaged for feature extraction.</p><p>tion (LRN), max pooling and ReLU neuron are adopted and the dropout regularization ratio is set to 0.5. The network weights are learned using the mini-batch stochastic gradient descent with the momentum value set to 0.9 and weight decay set to 0.00005. Learning rate is set to 0.001 and the maximum training cycle is set to 60. When the CNN model achieves 99% accuracy on the training set, the training procedure is stopped beforehand. In each cycle, a mini-batch of C samples is constructed by randomly sampling images from training set. For NTU RGB+D dataset, UTD-MHAD dataset and PenAction dataset, C is set to 64, 16 and 16, considering the size of training set. To reduce the effect of random parameter initialization and random sampling, we repeat the training of CNN model for five times and report the average results. The implementation is based on PyTorch with one TITAN X card and 16G RAM.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Discussions</head><p>2D pose &amp; 3D pose from depth video: As poses estimated from videos lose depth cues, we begin with evaluating the significance of depth information for pose-based human action recognition. In <ref type="table" target="#tab_3">Table 1</ref>, a 3D pose sequence extracted from a depth video is described as a three channel body pose evolution image, which is further encoded by CNN to predict action label. This method is called "S2". By setting all values of depth channel to zero, 2D pose sequences from depth videos are used instead. This method is called "S1". Without using depth channel, the accuracy drops from 89.44% to 85.53% on UTD-MHAD dataset. While, accuracies drop by only 1.86% from 82.38% to 80.52% for cross subject setting and 0.90% from 86.65% <ref type="bibr" target="#b53">46</ref>   to 85.75% for cross view setting on NTU RGB+D dataset. These results show that depth information can improve the recognition, but the influence of depth channel drops when large scale training data is used, as well-trained CNN model may infer depth cues from 2D pose. These results support the potential of estimating 2D poses for action recognition from videos, where depth cues are not directly available.</p><p>2D pose from video: With accurate pose estimation method and additional depth cues, 3D poses from depth videos are more reliable than 2D poses from videos. This part evaluates the performance of noisy 2D poses from videos by comparing with 2D poses and 3D poses from depth videos. "H1" denotes our proposed method using body pose evolution image. On NTU RGB+D dataset, "H1" performs worse than "S1". The reason is that this dataset contains multi-view samples, which brings more ambiguities to 2D pose from video than 3D pose from depth video. The performance of "H1" is comparable with "S1" on UTD-MHAD dataset. The reason is that this dataset contains samples observed from single view, which helps the pose estimation from both RGB and depth videos. Generally,  2D pose from video can only compete with that from depth video in simple scenes; 2D pose from video can barely achieve the performance of 3D pose from depth video. Heatmap from video: Instead of using sole 2D pose, we evaluate the performance of combining heatmap with 2D pose for recognition from video. The method called "H3" describes heatmap as body shape evolution image using spatial rank pooling. For comparisons, the method called "H2" is implemented by temporal rank pooling. "H3" outperforms "H2" by more than 15% on both NTU RGB+D and UTD-MHAD datasets, which verifies the advantage of spatial rank pooling method in preserving both spatial and temporal cues. The method called "H1 + H3" denotes the combination of both 2D pose and heatmap. "H1 + H3" outperforms at least 5% than "H1". Detailed improvements on NTU RGB+D dataset using cross subject protocol are shown in <ref type="figure" target="#fig_8">Fig. 7</ref>. These results indicate the complementary property between 2D pose and heatmap. In <ref type="figure" target="#fig_9">Fig. 8</ref>, we analyze the confusion matrices among 10 types of actions. The red boxes highlight the improvement on action "use a fan (with hand or paper)/feeling warm (49)", by combining 2D pose and heatmap. As shown in <ref type="figure">Fig. 9</ref>, body pose evolution image only captures 2D pose joints, which are noisy due to occlusions. However, body shape evolution image, which captures global shape of heatmap, can provide cues for inferring accurate locations of 2D pose joints.  <ref type="figure">Figure 9</ref>: The visualization of 2D poses and heatmaps estimated from the action "use a fan (with hand or paper)/feeling warm (49)". We show the pose estimation maps of the 8-th, 5-th, 1-st joint. On these pose estimation maps, green arrow points out the estimated position of joint, which is inaccurate. Meanwhile, pink arrow points to the region of heatmap which covers the ground truth of the joint position. We claim that heatmaps contain richer cues for inferring locations of joints when estimated 2D poses are inaccurate. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Comparisons with State-of-the-arts</head><p>Ours versus 3D Pose-based methods: Even with inaccurate 2D pose estimation method, our method outperforms 3D pose, which is extracted by 3D pose estimation method from depth sensors. In <ref type="table" target="#tab_5">Table 2 and Table 3</ref>, "H1 + H3" outperforms most state-of-the-art methods using 3D poses. Specifically, "H1 + H3" achieves 78.80% and 84.21% on the currently largest NTU RGB+D dataset. "H1 + H3" also outperforms LSTM-based method, i.e., GCA-LSTM <ref type="bibr" target="#b33">[26]</ref>. Although slightly worse, "H1 + H3" approaches to similar performance of the most recent CNN-based method, i.e., View-invariant <ref type="bibr" target="#b35">[28]</ref>. On UTD-MHAD dataset, "H1 + H3" outperforms all 3D pose-based methods, e.g., 3DHOT-MBC <ref type="bibr" target="#b60">[53]</ref> and JDM <ref type="bibr" target="#b30">[23]</ref>. Instead of using 2D poses, it is interesting to combine heatmap with more accurate poses, namely 2D poses from depth video. <ref type="table" target="#tab_3">Table 1</ref> shows that "S1+H3" outperforms "H1+H3". With additional depth information, "S2+H3" achieves the best performances. These results verify that our proposed heatmaps benefit both 2D poses from videos and 3D poses from depth videos.</p><p>Ours versus video-based methods: Among approaches using videos, our method is compared with most related 2D pose-based action recognition methods. In <ref type="table" target="#tab_7">Table 4</ref>, poselet detected by Action Bank <ref type="bibr" target="#b61">[54]</ref> achieves accuracy of 83.90% on PennAction dataset. AOG <ref type="bibr" target="#b55">[48]</ref> and Pose + IDT-FV <ref type="bibr" target="#b26">[19]</ref> benefit from treating pose estimation and action recognition as a uniform framework, and achieve accuracy of 85.50% and 92.00%. RPAN <ref type="bibr" target="#b15">[8]</ref>  alleviate the effect of noisy 2D poses. We further select one frame for each video and use CNN to extract deep features. Also, we encode annotated poses, which are provided by original dataset. Fused with these additional information, our method "(H1 + H3)*" achieves accuracy of 98.22% and outperforms all recent methods. The confusion matrix of our method is shown in <ref type="figure" target="#fig_0">Fig. 10</ref>, where most of ambiguities among actions are suppressed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>This paper recognizes actions from videos as the evolution of pose estimation maps. Compared with unreliable estimated 2D poses, pose estimation maps provide richer cues for inferring body parts and their movements. By describing the evolution of pose estimation maps as compact body shape evolution image and body pose evolution image, our method can effectively capture movements of both body shape and body parts, thereby outperforming all 2D pose or 3D pose-based methods on benchmark datasets. It is noted that our features only rely on the estimated pose estimation maps rather than original videos, from which the pose estimation maps are estimated. This property indicates the generalization ability of our method by estimating pose estimation maps from various types of input video, e.g., depth or infrared video, for action recognition task.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Acknowledgement</head><p>This work is supported in part by Singapore Ministry of Education Academic Research Fund Tier 2 MOE2015-T2-2-114 and start-up funds of University at Buffalo.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: An illustration of the complementary property between poses and heatmaps (averaged pose estimation maps), which are both estimated from video frames. (a) An action "baseball pitch" from PennAction dataset [54] is simplified as two frames. The red circle and red star denote the hand and foot, respectively. (b) With inaccurate pose estimation, the estimated poses cannot accurately annotate human body parts. For example, we show the pose estimation map of the hand, where the multiple peaks lead to false prediction. (c) Although heatmaps cannot differentiate body parts, they provide richer information to reflect human body shape.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The overview of the proposed method. a) Convolutional pose machines predict pose estimation map of each body part. b) For each frame, pose estimation maps are aggregated to form a heatmap and a pose. c) Spatial rank pooling is proposed to describe the evolution of heatmaps as a body shape evolution image, which contains one channel. d) Body guided sampling is proposed to describe the evolution of poses as a body pose evolution image, which contains two channels. e) Deep features are extracted from both types of images and the late fusion result predicts action label. Note that both images are normalized to fix-sized color images to facilitate transfer learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Frame</head><label></label><figDesc></figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The comparison between features extracted from two videos. (a) Left video denotes action "wave" and right video denotes action "throw". (b) The evolution of heatmaps. Each heatmap is a gray scale image. To facilitate observation, we use colormap to highlight heatmap according to gray scale values. (c) The evolution of poses. Each joint is shown using specific color. To facilitate the observation, we also show the limbs which are colored in green. (d) Body shape evolution image implemented by temporal rank pooling (t-rk). (e) body shape evolution image implemented by spatial rank pooling (s-rk). (f) Body pose evolution image implemented by body guided sampling.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Generation of body shape evolution image via proposed spatial rank pooling</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>620 595 Figure 5 :</head><label>5955</label><figDesc>Figure 5: Building one channel of body pose evolution image from horizontal coordinates via body guided sampling recording pose sequences, the distances between human bodies and the depth video are not strictly the same. In other words, different pose sequences have their own scales, which bring intra-varieties to same types of actions. To this end, {{x k,n } K k=1 } N n=1 and {{y k,n } K k=1 } N n=1 are normalized to change from 0 to 1, respectively. To encode the spatial and temporal evolution of pose joints in a compact way, we represent the pre-processed pose sequence V L as a body pose evolution image. Since original estimated pose joints are too sparse to represent the human body, we sort the order of joint labels according to the body structure, and use linear interpolation to sample abundant points from pose limbs. The pipeline of building one channel of body pose evolution image from horizontal coordinates is shown in Fig. 5. Another channel of body pose evolution image from vertical coordinates can be similarly built. Mathematically, let x n =[x 1,n , ..., x</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Snaps selected from PennAction dataset K =1 4and the number of limb is K − 1, then the K ′ can be calculated as K +(K − 1) × 5=7 9 . Two channels of body pose evolution image are formed as [x 1 , ..., x N ] and [y 1 , ..., y N ], denoting horizontal and vertical coordinates, respectively. Both channels reflect the temporal evolution of joints, which are densely sampled to denote the pose of human body. Late Fusion: A video I c has been denoted as a body shape evolution image and a body pose evolution image, where c means the c th sample from a batch that is used for training. As CNN has achieved success in image classification task, we use CNN model that is pre-trained on Imagenet [7] for transfer learning. Since these two images contain significantly different spatial structure, we use separate CNN to explore deep features from them. To accommodate existing CNN models, the single channel of body shape evolution image is repeated three times to form a 3-channel image, and two channels of body pose evolution image are combined with a zero-valued channel to form a 3-channel image. Let {I</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>denote these two images. Mean removal is adopted for input images to improve the convergence speed. Then, each color image is processed by a CNN. For the image I c m , the output Υ m of the last fully- connected (fc) layer is normalized by the softmax func- tion to obtain the posterior probability: prob(r | Ito the r-th action class. R is the number of total action classes. The objective function of our mod- el is to minimize the maximum-likelihood loss function L(I m )(r − s c ) prob(r | I c m ), where function δ equals to one if r = s c and equals to zero other- wise, s c is the ground truth label of I c m , and C is the batch size. For a sequence I, its final class score is the average of the two posteriors: score(r |I)prob(r | I m ), where prob(r | I m ) is the probability of I m belonging to the r th action class.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: The complementary between body shape evolution image and body pose evolution image</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Confusion matrices of body pose evolution image-based method (first row), body shape evolution image-based method (second row), body pose and body shape evolution images-based method (third row) on NTU RGB+D dataset using cross subject protocol. Confusion matrices of ten actions are enlarged. These ten actions are: touch back (backache) (46), touch neck (neckache) (47), nausea or vomiting condition (48), use a fan (with hand or paper)/feeling warm (49), punching/slapping other person (50), kicking other person (51), pushing other person (52), pat on back of other person (53), point finger at the other person (54), hugging other person (55). Red boxes verify that the body pose and body shape evolution images compensate for each other and improve the recognition of action (49).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Confusion matrix of our method on PennAction dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>extracted w- hole human body or body parts instead of whole video for action analysis. Meanwhile, human action recognition and</figDesc><table>xy 
x 

y 

1 
8 
5 
11 
14 

... 

... 

... 

... 

... 

... 

Action Label 

Rank pooling 
Spatial concatenation 
Temporal concatenation 
Coordinate concatenation 
Linear interpolation 

(a) 
(b) 
(b) 
(c) 
(d) 
(e) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>o rQ columns, i.e., G n =[ q 1 , ..., q s , ..., q Q ]. Similar to Eq. 4, function V is applied to map (p 1:s )Using the structural risk minimization and max-margin framework, the objective is defined as:</figDesc><table>T and q 1:s to v 

p 

s and v 

q 

s , respective-
ly. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 1 :</head><label>1</label><figDesc>The evaluation of body shape evolution image and body pose evolution image on NTU RGB+D, UTD-MHAD and PennAction datasets. BPI is short for body pose evolution image. BSI is short for body shape evolution image. BSI (t-rk) is short for implementing BSI with temporal rank pooling. BSI (s-rk) is short for implementing BSI with spatial rank pooling. To accelerate the computations, approximate rank pooling [1] is used to implement rank pooling method.</figDesc><table>NTU RGB+D 
UTD-MHAD PennAction 
Method Sensor 
Data 
Feature 
CS 
CV 
CS 
half / half 
S1 
Kinect 
2D Pose 
BPI 
80.52% 85.75% 
85.53% 
-
S2 
Kinect 
3D Pose 
BPI 
82.38% 86.65% 
89.44% 
-
H1 
RGB 
2D Pose 
BPI 
72.96% 77.21% 
85.63% 
84.08% 
H2 
RGB 
Heatmap 
BSI (t-rk) 
53.91% 54.10% 
58.88% 
84.61% 
H3 
RGB 
Heatmap 
BSI (s-rk) 
72.75% 78.35% 
74.88% 
87.02% 
H1+H3 
RGB 
2D Pose + Heatmap BPI + BSI (s-rk) 78.80% 84.21% 
92.51% 
91.39% 
S1+H3 Kinect 2D Pose + Heatmap BPI + BSI (s-rk) 90.90% 94.54% 
92.84% 
-
S2+H3 Kinect 3D Pose + Heatmap BPI + BSI (s-rk) 91.71% 95.26% 
94.51% 
-

1 2 3 </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Comparisons between our proposed method and state-of-the-art methods on NTU RGB+D dataset</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparisons between our proposed method and state-of-the-art methods on UTD-MHAD dataset</figDesc><table>Sensor 
Method 
Year 
CS 
Kinect 
Cov3DJ [18] 
2013 85.58% 
Kinect 
Kinect [6] 
2015 66.10% 
Inertial 
Inertial [6] 
2015 67.20% 
Kinect + Inertial 
Kinect&amp;Inertial [6] 
2015 79.10% 
Kinect 
JTM [44] 
2016 85.81% 
Kinect 
Optical Spectra [15] 
2016 86.97% 
Kinect 
3DHOT-MBC [53] 
2017 84.40% 
Kinect 
JDM [23] 
2017 88.10% 
RGB 
Proposed Method:H1+H3 
-
92.84% 
Kinect 
Proposed Method:S2+H3 
-
94.51% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Comparisons between our proposed method and state-of-the-art methods on PennAction dataset</figDesc><table>Sensor 
Method 
Year half / half 
RGB 
Action Bank [54] 
2013 
83.90% 
RGB 
AOG [48] 
2015 
85.50% 
RGB 
C3D [3] 
2016 
86.00% 
RGB 
JDD [3] 
2016 
87.40% 
RGB 
Pose + IDT-FV [19] 
2017 
92.00% 
RGB 
RPAN [8] 
2017 
97.40% 
RGB 
Proposed Method:H1+H3 
-
91.39% 
RGB 
Proposed Method: (H1 + H3)* 
-
98.22% 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Super Normal Vector</title>
		<idno>50] 2014 31.82% 13.61%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hbrnn-L</surname></persName>
		</author>
		<idno>59.07% 63.97%</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title/>
		<idno>16] 2015 60.23% 65.22%</idno>
	</analytic>
	<monogr>
		<title level="j">FTP Dynamic Skeletons</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rnn</forename><surname>Deep</surname></persName>
		</author>
		<idno>35] 2016 59.29% 64.09%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lstm</forename><surname>Deep</surname></persName>
		</author>
		<idno>35] 2016 60.69% 67.29%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P-Lstm</forename><surname>Layer</surname></persName>
		</author>
		<idno>35] 2016 62.93% 70.27%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>St-Lstm + Trust</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gate</surname></persName>
		</author>
		<idno>25] 2016 69.20% 77.70%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gca-Lstm</forename><surname>Network</surname></persName>
		</author>
		<idno>26] 2017 74.40% 82.80%</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Dynamic image networks for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bilen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3034" to="3042" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Human pose estimation via convolutional part heatmap regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Tzimiropoulos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="717" to="732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Action recognition with joints-pooled 3D deep convolutional descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3324" to="3330" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Realtime multiperson 2D pose estimation using part affinity fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Human pose estimation with iterative error feedback</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Fragkiadaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4733" to="4742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">UTD-MHAD: A multimodal dataset for human action recognition utilizing a depth camera and a wearable inertial sensor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jafari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kehtarnavaz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Image Processing (ICIP)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="168" to="172" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">RPAN: An end-to-end recurrent pose-attention network for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
	<note>IC-CV</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Hierarchical recurrent neural network for skeleton based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Du</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1110" to="1118" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Skeletal quads: Human action recognition using joint quadruples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Evangelidis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Horaud</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IAPR International Conference on Pattern Recognition (ICPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4513" to="4518" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Spatiotemporal multiplier networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4768" to="4777" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Modeling video evolution for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Fernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Gavves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Oramas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ghodrati</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tuytelaars</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5378" to="5387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Transition forests: Learning discriminative temporal transitions for action recognition and detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Garcia-Hernando</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-K</forename><surname>Kim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="432" to="440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Finding action tubes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Gkioxari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="759" to="768" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Skeleton optical spectra based action recognition using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Circuits and Systems for Video Technology</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly learning heterogeneous features for RGB-D activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-F</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-S</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5344" to="5352" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Deep learning on lie groups for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Probst</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="6099" to="6108" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Human action recognition using a temporal hierarchy of covariance descriptors on 3D joint locations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">E</forename><surname>Hussein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Torki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">A</forename><surname>Gowayyed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>El-Saban</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Joint Conferences on Artificial Intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="page" from="2466" to="2472" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Pose for action-action for pose</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Iqbal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Garbade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Automatic Face &amp; Gesture Recognition (FG)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="438" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">A new representation of skeleton sequences for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>An</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Sohel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Boussaid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Beyond gaussian pyramid: Multi-skip feature stacking for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="204" to="212" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">On space-time interest points. International Journal of Computer Vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">64</biblScope>
			<biblScope unit="page" from="107" to="123" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Joint distance maps based action recognition with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Signal Processing Letters</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="624" to="628" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Learning directional cooccurrence for human action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1235" to="1239" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Spatio-temporal LSTM with trust gates for 3D human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="816" to="833" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Global context-aware attention LSTM networks for 3D action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Kot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Depth Context: A new descriptor for human activity recognition by using sole depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neurocomputing</title>
		<imprint>
			<biblScope unit="volume">175</biblScope>
			<biblScope unit="page" from="747" to="758" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Enhanced skeleton visualization for view invariant human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="volume">68</biblScope>
			<biblScope unit="page" from="346" to="362" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Unsupervised learning of long-term motion dynamics for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Going deeper into firstperson activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Kitani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1894" to="1903" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Space-time tree ensemble for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sigal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sclaroff</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="5024" to="5032" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Hon4D: Histogram of oriented 4D normals for activity recognition from depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Oreifej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="716" to="723" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Learning action recognition model from depth and skeleton videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Rahmani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bennamoun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Pose machines: Articulated pose estimation via inference machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Munoz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Bagnell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="33" to="47" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">NTU RG-B+D: A large scale dataset for 3D human activity analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Shahroudy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-T</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1010" to="1019" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Real-time human pose recognition in parts from single depth images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shotton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sharp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kipman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fitzgibbon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Finocchio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cook</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Moore</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications of the ACM</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="116" to="124" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="568" to="576" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">First person action recognition using deep learned descriptors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jawahar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2620" to="2628" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<monogr>
		<title level="m" type="main">Multi-stream CNN: Learning representations based on human-related regions for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Qin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poppe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">C</forename><surname>Veltkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<biblScope unit="volume">79</biblScope>
			<biblScope unit="page" from="32" to="43" />
		</imprint>
	</monogr>
	<note>Pattern Recognition</note>
</biblStruct>

<biblStruct xml:id="b47">
	<analytic>
		<title level="a" type="main">Human action recognition by representing 3d skeletons as points in a lie group</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vemulapalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Arrate</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="588" to="595" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">An approach to posebased action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="915" to="922" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">Mining 3D key-posemotifs for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2639" to="2647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">Action recognition by dense trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b51">
	<analytic>
		<title level="a" type="main">Action recognition based on joint trajectory maps using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM on Multimedia Conference (ACM MM)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="102" to="106" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b52">
	<analytic>
		<title level="a" type="main">Spatiotemporal pyramid network for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">S</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="1529" to="1538" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b53">
	<analytic>
		<title level="a" type="main">Convolutional pose machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-E</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4724" to="4732" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b54">
	<analytic>
		<title level="a" type="main">Spatio-Temporal NaiveBayes Nearest-Neighbor (ST-NBNN) for skeleton-based action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Weng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4171" to="4180" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b55">
	<analytic>
		<title level="a" type="main">Joint action recognition and pose estimation from video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-C</forename><surname>Zhu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1293" to="1301" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b56">
	<analytic>
		<title level="a" type="main">Multi-feature max-margin hierarchical bayesian model for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1610" to="1618" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b57">
	<analytic>
		<title level="a" type="main">Super normal vector for activity recognition using depth sequences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="804" to="811" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b58">
	<analytic>
		<title level="a" type="main">Articulated human detection with flexible mixtures of parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2878" to="2890" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b59">
	<analytic>
		<title level="a" type="main">Discriminative orderlet mining for real-time recognition of human-object interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yuan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision (ACCV)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="50" to="65" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b60">
	<analytic>
		<title level="a" type="main">Action recognition using 3D histograms of texture and a multi-class boosting classifier</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Han</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Shao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="4648" to="4660" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b61">
	<analytic>
		<title level="a" type="main">From actemes to action: A strongly-supervised representation for detailed action understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">G</forename><surname>Derpanis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2248" to="2255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b62">
	<analytic>
		<title level="a" type="main">Microsoft kinect sensor and its effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="4" to="10" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b63">
	<analytic>
		<title level="a" type="main">A key volume mining deep framework for action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1991" to="1999" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
