<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:19+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Confident Multiple Choice Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kimin</forename><surname>Lee</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Changho</forename><surname>Hwang</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kyoungsoo</forename><surname>Park</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jinwoo</forename><surname>Shin</surname></persName>
						</author>
						<title level="a" type="main">Confident Multiple Choice Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Ensemble methods are arguably the most trustworthy techniques for boosting the performance of machine learning models. Popular independent ensembles (IE) relying on na√Øve averaging/voting scheme have been of typical choice for most applications involving deep neural networks, but they do not consider advanced collaboration among ensemble models. In this paper, we propose new ensemble methods specialized for deep neural networks, called confident multiple choice learning (CMCL): it is a variant of multiple choice learning (MCL) via addressing its overconfidence issue. In particular, the proposed major components of CMCL beyond the original MCL scheme are (i) new loss, i.e., confident oracle loss, (ii) new architecture, i.e., feature sharing and (iii) new training method, i.e., stochastic labeling. We demonstrate the effect of CMCL via experiments on the image classification on CIFAR and SVHN, and the foregroundbackground segmentation on the iCoseg. In particular, CMCL using 5 residual networks provides 14.05% and 6.60% relative reductions in the top-1 error rates from the corresponding IE scheme for the classification task on CIFAR and SVHN, respectively.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Ensemble methods have played a critical role in the machine learning community to obtain better predictive performance than what could be obtained from any of the constituent learning models alone, e.g., Bayesian model/parameter averaging <ref type="bibr" target="#b6">(Domingos, 2000)</ref>, boosting <ref type="bibr" target="#b8">(Freund et al., 1999)</ref> and bagging <ref type="bibr" target="#b2">(Breiman, 1996)</ref>. Recently, they have been successfully applied to enhancing the power of many deep neural networks, e.g., 80% of top-5 best-performing teams on ILSVRC challenge 2016 <ref type="bibr" target="#b19">(Krizhevsky et al., 2012)</ref> employ ensemble methods. They are easy and trustworthy to apply for most scenarios. While there exists a long history on ensemble methods, the progress on developing more advanced ensembles specialized for deep neural networks has been slow. Despite continued efforts that apply various ensemble methods such as bagging and boosting to deep models, it has been observed that traditional independent ensembles (IE) which train models independently with random initialization achieve the best performance <ref type="bibr" target="#b3">(Ciregan et al., 2012;</ref><ref type="bibr" target="#b21">Lee et al., 2015)</ref>. In this paper, we focus on developing more advanced ensembles for deep models utilizing the concept of multiple choice learning (MCL).</p><p>The MCL concept was originally proposed in <ref type="bibr">(GuzmanRivera et al., 2012)</ref> under the scenario when inference procedures are cascaded:</p><p>(a) First, generate a set of plausible outputs.</p><p>(b) Then, pick the correct solution form the set.</p><p>For example, <ref type="bibr" target="#b29">(Park &amp; Ramanan, 2011;</ref><ref type="bibr" target="#b1">Batra et al., 2012)</ref> proposed human-pose estimation methods which produce multiple predictions and then refine them by employing a temporal model, and <ref type="bibr" target="#b4">(Collins &amp; Koo, 2005)</ref> proposed a sentence parsing method which re-ranks the output of an initial system which produces a set of plausible outputs <ref type="bibr" target="#b12">(Huang &amp; Chiang, 2005)</ref>. In such scenarios, the goal of the first stage (a) is generating a set of plausible outputs such that at least one of them is correct for the second stage (b), e.g., human operators. Under this motivation, MCL has been studied <ref type="bibr" target="#b10">(Guzman-Rivera et al., 2014;</ref><ref type="bibr" target="#b22">Lee et al., 2016)</ref>, where various applications have been demonstrated, e.g., image classification <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009</ref>), semantic segmentation <ref type="bibr" target="#b7">(Everingham et al., 2010)</ref> and image captioning <ref type="bibr" target="#b24">(Lin et al., 2014b)</ref>. It trains an ensemble of multiple models by minimizing the so-called oracle loss, only focusing on the most accurate prediction produced by them. Consequently, it makes each model specialized for a certain subset of data, not for the entire one similarly as mixture-of-expert schemes <ref type="bibr" target="#b15">(Jacobs et al., 1991)</ref>.</p><p>Although MCL focuses on the first stage (a) in cascaded scenarios and thus can produce diverse/plausible outputs, it might be not useful if one does not have a good scheme for the second stage (b). One can use a certain average/voting scheme of the predictions made by models for (b), but MCL using deep neural networks often fails to make a correct decision since each network tends to be overconfident in its prediction. Namely, the oracle error/loss of MCL is low, but its top-1 error rate might be very high.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Contribution.</head><p>To address the issue, we develop the concept of confident MCL (CMCL) that does not lose any benefit of the original MCL, while its target loss and architecture are redesigned for making the second stage (b) easier. Specifically, it targets to generate a set of diverse/plausible confident predictions from which one can pick the correct one using a simple average/voting scheme. To this end, we first propose a new loss function, called confident oracle loss, for relaxing the overconfidence issue of MCL. Our key idea is to additionally minimize the Kullback-Leibler divergence from a predictive distribution to the uniform one in order to give confidence to non-specialized models. Then, CMCL that minimizes the new loss can be efficiently trained like the original MCL for certain classes of models including neural networks, via stochastic alternating minimization <ref type="bibr" target="#b22">(Lee et al., 2016)</ref>. Furthermore, when CMCL is applied to deep models, we propose two additional regularization techniques for boosting its performance: feature sharing and stochastic labeling. Despite the new components, we note that the training complexity of CMCL is almost same to that of MCL or IE.</p><p>We apply the new ensemble model trained by the new training scheme for several convolutional neural networks (CNNs) including VGGNet <ref type="bibr" target="#b33">(Simonyan &amp; Zisserman, 2015)</ref>, GoogLeNet , and ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref> for image classification on the CIFAR <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009)</ref> and SVHN <ref type="bibr" target="#b27">(Netzer et al., 2011)</ref> datasets, and fully-convolutional neural networks (FCNs) <ref type="bibr" target="#b25">(Long et al., 2015)</ref> for foreground-background segmentation on the iCoseg dataset <ref type="bibr" target="#b0">(Batra et al., 2010)</ref>. First, for the image classification task, CMCL outperforms all baselines, i.e., the traditional IE and the original MCL, in top-1 error rates. In particular, CMCL of 5 ResNet with 20 layers provides 14.05% and 6.60% relative reductions in the top-1 error rates from the corresponding IE on CIFAR-10 and SVHN, respectively. Second, for the foreground-background segmentation task, CMCL using multiple FCNs with 4 layers also outperforms all baselines in top-1 error rates. Each model trained by CMCL generates high-quality solutions by specializing for specific images while each model trained by IE does not. We believe that our new approach should be of broader interest for many deep learning tasks requiring high accuracy.</p><p>Organization. In Section 2, we introduce necessary backgrounds for multiple choice learning and the corresponding loss function. We describe the proposed loss and the corresponding training scheme in Section 3. Section 4 provides additional techniques for the proposed ensemble model. Experimental results are reported in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Preliminaries</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Multiple Choice Learning</head><p>In this section, we describe the basic concept of multiple choice learning (MCL) <ref type="bibr" target="#b10">(Guzman-Rivera et al., 2014;</ref>. Throughout this paper, we denote the set {1, . . . , n} by [n] for positive integer n. The MCL scheme is a type of ensemble learning that produces diverse outputs of high quality. Formally, given a training dataset</p><formula xml:id="formula_0">D = {(x i , y i ) | i ‚àà [N ], x i ‚àà X , y i ‚àà Y},</formula><p>we consider an ensemble of M models f , i.e., (f 1 , . . . , f M ). For some task-specific loss function (y, f (x)), the oracle loss over the dataset D is defined as follows:</p><formula xml:id="formula_1">L O (D) = N i=1 min m‚àà[M ] (y i , f m (x i )) ,<label>(1)</label></formula><p>while the traditional independent ensemble (IE) loss is</p><formula xml:id="formula_2">L E (D) = N i=1 m‚àà[M ] (y i , f m (x i )) .<label>(2)</label></formula><p>If all models have the same capacity and one can obtain the (global) optimum of the IE loss with respect to the model parameters, then all trained models should produce the same outputs, i.e., f 1 = . . . = f M . On the other hand, the oracle loss makes the most accurate model optimize the loss function (y, f (x)) for each data x. Therefore, MCL produces diverse outputs of high quality by forcing each model to be specialized on a part of the entire dataset.</p><p>Minimizing the oracle loss (1) is harder than minimizing the independent ensemble loss (2) since the min function is a non-continuous function. To address the issue, <ref type="bibr">(GuzmanRivera et al., 2012)</ref> proposed an iterative block coordinate decent algorithm and <ref type="bibr" target="#b5">(Dey et al., 2015)</ref> reformulated this problem as a submodular optimization task in which ensemble models are trained sequentially in a boosting-like manner. However, when one considers an ensemble of deep neural networks, it is challenging to apply these methods since they require either costly retraining or sequential training. Recently, <ref type="bibr" target="#b22">(Lee et al., 2016)</ref> overcame this issue by proposing a stochastic gradient descent (SGD) based algorithm. Throughout this paper, we primarily focus on ensembles of deep neural networks and use the SGD algorithm for optimizing the oracle loss (1) or its variants.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Oracle Loss for Top-1 Choice</head><p>The oracle loss (1) used for MCL is useful for producing diverse/plausible outputs, but it is often inappropriate for ap-  (c) Independent ensemble (IE) <ref type="figure">Figure 1</ref>. Class-wise test set accuracy of each ensemble model trained by various ensemble methods. One can observe that most models trained by MCL and CMCL become specialists for certain classes while they are generalized in case of traditional IE.</p><p>plications requiring a single choice, i.e., top-1 error. This is because ensembles of deep neural networks tend to be overconfident in their predictions, and it is hard to judge a better solution from their outputs. To explain this in more detail, we evaluate the performance of ensembles of convolutional neural networks (CNNs) for the image classification task on the CIFAR-10 dataset <ref type="bibr" target="#b18">(Krizhevsky &amp; Hinton, 2009</ref>). We train ensembles of 5 CNNs (two convolutional layers followed by a fully-connected layer) using MCL. We also train the models using traditional IE which trains each model independently under different random initializations. <ref type="figure">Figure 1</ref> summarizes the class-wise test set accuracy of each ensemble member. In the case of MCL, most models become specialists for certain classes (see <ref type="figure">Figure  1</ref>(a)), while they are generalized in the case of traditional IE as shown in <ref type="figure">Figure 1</ref>(c). However, as expected, each model trained by MCL significantly outperforms for its specialized classes than that trained by IE. For choosing a single output, similar to <ref type="bibr" target="#b36">(Wan et al., 2013;</ref><ref type="bibr" target="#b3">Ciregan et al., 2012)</ref>, one can average the output probabilities from ensemble members trained by MCL, but the corresponding top-1 classification error rate is often very high (e.g., see <ref type="table">Table 1</ref> in Section 5). This is because each model trained by MCL is overconfident for its non-specialized classes.</p><p>To quantify this, we also compute the entropy of the predictive distribution on the test data and use this to evaluate the quality of confidence/uncertainty level. <ref type="figure" target="#fig_2">Figure 2</ref>(a) reports the entropy extracted from the predictive distribution of one of ensemble models trained by MCL. One can observe that it has low entropy as expected for its specialized classes (i.e., classes that the model has a test accuracy higher than 90%). However, even for non-specialized classes, it also has low entropy. Due to this, with respect to top-1 error rates, simple averaging of models trained by MCL performs much worse than that of IE. Such issue typically occurs in deep neural networks since it is well known that they are poor at quantifying predictive uncertainties, and tend to be easily overconfident <ref type="bibr" target="#b28">(Nguyen et al., 2015)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Confident Multiple Choice Learning</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Confident Oracle Loss</head><p>In this section, we propose a modified oracle loss for relaxing the issue of MCL described in the previous section. Suppose that the m-th model outputs the predictive distribution P Œ∏m (y | x) given input x, where Œ∏ m denotes the model parameters. Then, we define the confident oracle loss as the following integer programming variant of <ref type="formula" target="#formula_1">(1)</ref>:</p><formula xml:id="formula_3">L C (D) = min v m i N i=1 M m=1 v m i (y i , P Œ∏m (y | x i )) + Œ≤ (1 ‚àí v m i ) D KL (U (y) P Œ∏m (y | x i )) (3a) subject to M m=1 v m i = 1, ‚àÄi,<label>(3b)</label></formula><formula xml:id="formula_4">v m i ‚àà {0, 1}, ‚àÄi, m<label>(3c)</label></formula><p>where D KL denotes the Kullback-Leibler (KL) divergence, U (y) is the uniform distribution, Œ≤ is a penalty parameter, and v m i is a flag variable to decide the assignment of x i to the m-th model. By minimizing the KL divergence from the predictive distribution to the uniform one, the new loss forces the predictive distribution to be closer to the uniform one, i.e., zero confidence, on non-specialized data, while those for specialized data still follow the correct one. For example, for classification tasks, the most accurate model for each data is allowed to optimize the classification loss, while others are forced to give less confident predictions by minimizing the KL divergence. We remark that although we optimize the KL divergence only for non-specialized data, one can also do it even for specialized data to regularize each model <ref type="bibr" target="#b30">(Pereyra et al., 2017)</ref>. In the case of MCL and CMCL, we separate the classes of CIFAR-10 into specialized (i.e., classes that model has a class-wise test accuracy higher than 90%) and non-specialized (others) classes. In the case of IE, we follow the proposed method by <ref type="bibr" target="#b20">(Lakshminarayanan et al., 2016)</ref>: train an ensemble of 5 models with adversarial training (AT) and measure the entropy using the averaged probability, i.e., averaging output probabilities from 5 models. (d) Detailed view of feature sharing between two models. Grey units indicate that they are currently dropped. Masked features passed to a model are all added to generate the shared features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Stochastic Alternating Minimization for Training</head><p>In order to minimize the confident oracle loss <ref type="formula">(3)</ref>  The above scheme iteratively assigns each data to a particular model and then independently trains each model only using its assigned data. Even though it monotonically decreases the objective, it is still highly inefficient since it requires training each model multiple times until assignments {v m i } converge. To address the issue, we propose deciding assignments and update model parameters to the gradient directions once per each batch, similarly to <ref type="bibr" target="#b22">(Lee et al., 2016)</ref>. In other words, we perform a single gradientupdate on parameters in Step 2, without waiting for their convergence to a (local) optimum. In fact, <ref type="bibr" target="#b22">(Lee et al., 2016)</ref> show that such stochastic alternating minimization works well for the oracle loss (1). We formally describe a detailed training procedure as the 'version 0' of Algorithm 1, and we will introduce the alternative 'version 1' later. This direction is complementary to ours, and we do not explore in this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 Confident MCL (CMCL)</head><p>Input: </p><formula xml:id="formula_5">Dataset D = {(x i , y i ) | x i ‚àà X</formula><formula xml:id="formula_6">L m i ‚ÜêŒ≤ m =m D KL (U (y) P Œ∏ m (y | x i )) + (y i , P Œ∏m (y i | x i )) , ‚àÄ(x i , y i ) ‚àà B</formula><p>end for for m = 1 to M do for i = 1 to |B| do if the m-th model has the lowest loss then Compute the gradient of the training loss (y i , P Œ∏m (y i | x i )) w.r.t Œ∏ m else / * version 0: exact gradient * / Compute the gradient of the KL divergence Œ≤D KL (U (y) P Œ∏m (y | x i )) w.r.t Œ∏ m / * version 1: stochastic labeling * / Compute the gradient of the cross entropy loss ‚àíŒ≤ log P Œ∏m ( y i | x i ) using y i w.r.t Œ∏ m where y i ‚àº U (y) end if end for Update the model parameters end for until convergence</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Effect of Confident Oracle Loss</head><p>Similar to Section 2.2, we evaluate the performance of the proposed training scheme using 5 CNNs for image classification on the CIFAR-10 dataset. As shown in <ref type="figure">Figure  1</ref>(b), ensemble models trained by CMCL using the exact gradient (i.e., version 0 of Algorithm 1) become specialists for certain classes. For specialized classes, they show the similar performance compared to the models trained by MCL, i.e., minimizing the oracle loss (1), which considers only specialization (see <ref type="figure">Figure 1(a)</ref>). For non-specialized classes, ensemble members of CMCL are not overconfident, which makes it easy to pick a correct output via simple voting/averaging. We indeed confirm that each model trained by CMCL has not only low entropy for its specialized classes, but also exhibits high entropy for nonspecialized classes as shown in <ref type="figure" target="#fig_2">Figure 2</ref>(b).</p><p>We also evaluate the quality of confidence/uncertainty level on unseen data using SVHN <ref type="bibr" target="#b27">(Netzer et al., 2011)</ref>. Somewhat surprisingly, each model trained by CMCL only using CIFAR-10 training data exhibits high entropy for SVHN test data, whereas models trained by MCL and IE are overconfident on it (see <ref type="figure" target="#fig_2">Figure 2</ref>(a) and 2(c)). We emphasize that our method can produce confident predictions significantly better than the proposed method by <ref type="bibr" target="#b20">(Lakshminarayanan et al., 2016)</ref>, which uses the averaged probability of ensemble models trained by IE to obtain high quality uncertainty estimates (see <ref type="figure" target="#fig_2">Figure 2(c)</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Regularization Techniques</head><p>In this section, we introduce advanced techniques for reducing the overconfidence and improving the performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Feature Sharing</head><p>We first propose a feature sharing scheme that stochastically shares the features among member models of CMCL to further address the overconfidence issue. The primary reason why deep learning models are overconfident is that they do not always extract general features from data. For examples, assume that some deep model only trains frogs and roses for classifying them. Although there might exist many kinds of features on their images, the model might make a decision based only on some specific features, e.g., colors. In this case, 'red' apples can be classified as rose with high confidence. Such an issue might be more severe in CMCL (and MCL) compared to IE since members of CMCL are specialized to certain data. To address the issue, we suggest the feature ensemble approach that encourages each model to generate meaningful abstractions from rich features extracted from other models.</p><p>Formally, consider an ensemble of M neural networks with L hidden layers. We denote the weight matrix for layer of model m ‚àà [M ] and -th hidden feature of model m by W m and h m , respectively. Instead of sharing the whole units of a hidden feature, we introduce random binary masks determining which units to be shared with other models. We denote the mask for layer from model n to m as œÉ œÉ œÉ nm ‚àº Bernoulli(Œª), which has the same dimension with h n (we use Œª = 0.7 in all experiments). Then, the -th hidden feature of model m with sharing ( ‚àí 1)-th hidden features is defined as follows:</p><formula xml:id="formula_7">h m (x) = œÜ Ô£´ Ô£≠ W m Ô£´ Ô£≠ h ‚àí1 m (x) + n =m œÉ œÉ œÉ nm h ‚àí1 n (x) Ô£∂ Ô£∏ Ô£∂ Ô£∏ ,</formula><p>where denotes element-wise multiplication and œÜ is the activation function. <ref type="figure" target="#fig_2">Figure 2(d)</ref> illustrates the proposed feature sharing scheme in an ensemble of deep neural networks. It makes each model learn more generalized features by sharing the features among them. However, one might expect that it might make each model overfitted due to the increased number of parameters that induces a single prediction, i.e., the statistical dependencies among outputs of models increase, which would hurt the ensemble effect. In order to handle this issue, we introduce the randomness in sharing across models in a similar manner to DropOut <ref type="bibr" target="#b34">(Srivastava et al., 2014)</ref> using the random binary masks œÉ œÉ œÉ. In addition, we propose sharing features at lower layers since sharing the higher layers might overfit the overall networks more. For example, in all experiments with CNNs in this paper, we commonly apply feature sharing for hidden features just before the first pooling layer. We also remark that such feature sharing strategies for better generalization have also been investigated in the literature for different purposes <ref type="bibr" target="#b26">(Misra et al., 2016;</ref><ref type="bibr" target="#b32">Rusu et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Stochastic Labeling</head><p>For more efficiency in minimizing the confident oracle loss, we also consider a noisy unbiased estimator of gradients of the KL divergence with Monte Carlo samples from the uniform distribution. The KL divergence from the predictive distribution to the uniform distribution can be written as follows:</p><formula xml:id="formula_8">D KL (U (y) P Œ∏ (y | x)) = y U (y) log U (y) P Œ∏ (y | x) = y U (y) log U (y) ‚àí y U (y) log P Œ∏ (y | x).</formula><p>Hence, the gradient of the above KL divergence with respect to the model parameter Œ∏ becomes</p><formula xml:id="formula_9">Œ∏ D KL (U (y) P Œ∏ (y | x)) = ‚àíE U (y) [ Œ∏ log P Œ∏ (y | x)].</formula><p>From the above, we induce the following noisy unbiased estimator of gradients with Monte Carlo samples from the uniform distribution:</p><formula xml:id="formula_10">‚àí E U (y) [ Œ∏ log P Œ∏ (y | x)] ‚àí 1 S s Œ∏ log P Œ∏ (y s | x),</formula><p>where y s ‚àº U (y) and S is the number of samples. This random estimator takes samples from the uniform distribution U (y) and constructs estimates of the gradient using them. In other words, Œ∏ log P Œ∏ (y s | x) is the gradient of the cross entropy loss under assigning a random label to x. This stochastic labeling provides efficiency in implementation/computation and stochastic regularization effects. We formally describe detailed procedures, as the version 1 of Algorithm 1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluate our algorithm for both classification and foreground-background segmentation tasks using CIFAR-10 ( <ref type="bibr" target="#b18">Krizhevsky &amp; Hinton, 2009)</ref>, SVHN <ref type="bibr" target="#b27">(Netzer et al., 2011)</ref> and iCoseg <ref type="bibr" target="#b0">(Batra et al., 2010)</ref> datasets. In all experiments, we compare the performance of CMCL with those of traditional IE and MCL using deep models. We provide the more detailed experimental setups including model architectures in the supplementary material. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Image Classification</head><p>Setup. The CIFAR-10 dataset consists of 50,000 training and 10,000 test images with 10 image classes where each image consists of 32 √ó 32 RGB pixels. The SVHN dataset consists of 73,257 training and 26,032 test images.</p><p>2 We pre-process the images with global contrast normalization and ZCA whitening following (Ian J. <ref type="bibr" target="#b13">Goodfellow &amp; Bengio, 2013;</ref><ref type="bibr" target="#b37">Zagoruyko &amp; Komodakis, 2016)</ref>, and do not use any data augmentation. Using these datasets, we train various CNNs, e.g., VGGNet <ref type="bibr" target="#b33">(Simonyan &amp; Zisserman, 2015)</ref>, GoogLeNet , and ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref>. Similar to <ref type="bibr" target="#b37">(Zagoruyko &amp; Komodakis, 2016)</ref>, we use the softmax classifier, and train each model by minimizing the cross-entropy loss using the stochastic gradient descent method with Nesterov momentum.</p><p>For evaluation, we measure the top-1 and oracle error rates on the test dataset. The top-1 error rate is calculated by averaging output probabilities from all models and predicting the class of the highest probability. The oracle error rate is the rate of classification failure over all outputs of individual ensemble members for a given input, i.e., it measures whether none of the members predict the correct class for 1 Our code is available at https://github.com/ chhwang/cmcl. <ref type="bibr">2</ref> We do not use the extra SVHN dataset for training. 14.78% <ref type="table">Table 1</ref>. Classification test set error rates on CIFAR-10 using various ensemble methods.</p><p>an input. While a lower oracle error rate suggests higher diversity, a lower oracle error rate does not always bring a higher top-1 accuracy as this metric does not reveal the level of overconfidence of each model. By collectively measuring the top-1 and oracle error rates, one can grasp the level of specialization and confidence of a model.</p><p>Contribution by each technique. <ref type="table">Table 1</ref> validates contributions of our suggested techniques under comparison with other ensemble methods IE and MCL. We evaluate an ensemble of five simple CNN models where each model has two convolutional layers followed by a fully-connected layer. We incrementally apply our optimizations to gauge the stepwise improvement by each component. One can note that CMCL significantly outperforms MCL in the top-1 error rate even without feature sharing or stochastic labeling while it still provides a comparable oracle error rate. By sharing the 1st ReLU activated features, the top-1 error rates are improved compared to those that employ only confident oracle loss. Stochastic labeling further improves both error rates. This implies that stochastic labeling not only reduces computational burdens but also provides regularization effects.</p><p>Overlapping. As a natural extension of CMCL, we also consider picking K specialized models instead of having only one specialized model, which was investigated for original MCL <ref type="bibr" target="#b22">Lee et al., 2016)</ref>. This is easily achieved by modifying the constraint (3b) as</p><formula xml:id="formula_11">M m=1 v m i = K,</formula><p>where K is an overlap parameter that controls training data overlap between the models. This simple but natural scheme brings extra gain in top-1 performance by generalizing each model better. <ref type="table" target="#tab_3">Table 2</ref> compares the performance of various ensemble methods with varying values of K. Under the choice of K = 4, CMCL of 10 CNNs provides 9.13% relative reduction in the top-1 error rates from the corresponding IE. Somewhat interestingly, IE has similar error rates on ensembles of both 5 and 10 CNNs, which implies that the performance of CMCL might be impossible to achieve using IE even if one increases the number of models in IE.</p><p>Large-scale CNNs. We now evaluate the performance of our ensemble method when it is applied to larger-scale CNN models for image classification tasks on CIFAR-10   and SVHN datasets. Specifically, we test VGGNet <ref type="bibr" target="#b33">(Simonyan &amp; Zisserman, 2015)</ref>, GoogLeNet , and ResNet <ref type="bibr" target="#b11">(He et al., 2016)</ref>. We share the nonlinear activated features right before the first pooling layer, i.e., the 6th, 2nd, and 1st ReLU activations for ResNet with 20 layers, VGGNet with 17 layers, and GoogLeNet with 18 layers, respectively. This choice is for maximizing the regularization effect of feature sharing while minimizing the statistical dependencies among the ensemble models. For all models, we choose the best hyperparameters for confident oracle loss among the penalty parameter Œ≤ ‚àà {0.5, 0.75, 1, 1.25, 1.5} and the overlapping parameter K ‚àà {2, 3, 4}. <ref type="table" target="#tab_4">Table 3</ref> shows that CMCL consistently outperforms all baselines with respect to the top-1 error rate while producing comparable oracle error rates to those of MCL. We also apply the feature sharing to IE as reported in <ref type="figure" target="#fig_6">Figure 4</ref>(a). Even though the feature sharing also improves the performance of IE, CMCL still outperforms IE: CMCL provides 6.11% relative reduction of the top-1 error rate from the IE with feature sharing under the choice of M = 10. We also remark that IE with feature sharing has similar error rates as the ensemble size increases, while CMCL does not (i.e., the gain is more significant for CMCL). This implies that feature sharing is more effectively working for CMCL.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Foreground-Background Segmentation</head><p>In this section, we evaluate if ensemble models trained with CMCL produce high-quality segmentation of foreground and background of an image with the iCoseg dataset. The foreground-background segmentation is formulated as a pixel-level classification problem with 2 classes, i.e., 0 (background) or 1 (foreground). To tackle the problem, we design fully convolutional networks (FCNs) model <ref type="bibr" target="#b25">(Long et al., 2015)</ref> based on the decoder architecture presented in <ref type="bibr" target="#b31">(Radford et al., 2016</ref> Top-1 error rate (%) ages that are larger than 300 √ó 500 pixels. For each class, we randomly split 80% and 20% of the data into training and test sets, respectively. We train on 75 √ó 125 resized images using the bicubic interpolation <ref type="bibr" target="#b16">(Keys, 1981)</ref>. Similar to <ref type="bibr" target="#b22">Lee et al., 2016)</ref>, we initialize the parameters of FCNs with those trained by IE for MCL and CMCL. For all experiments, CMCL is used with both feature sharing and stochastic labeling.</p><p>Similar to , we define the percentage of incorrectly labeled pixels as prediction error rate. We measure the oracle error rate (i.e., the lowest error rate over all models for a given input) and the top-1 error rate. The top-1 error rate is measured by following the predictions of the member model that has a lower pixel-wise entropy, i.e., picking the output of a more confident model. For each ensemble method, we vary the number of ensemble models and measure the oracle error rate and test error rate. <ref type="figure" target="#fig_6">Figure 4</ref>(b) and 4(c) show both top-1 and oracle error rates for all ensemble methods. We remark that the ensemble models trained by CMCL consistently improves the top-1 error rate over baselines. In an ensemble of 5 models, we find that CMCL achieve up to 6.77% relative reduction in the top-1 error rate from the corresponding IE. As shown in <ref type="figure">Figure 3</ref>, an individual model trained by CMCL generates high-quality solutions by specializing itself in specific images (e.g., model 1 is specialized for 'lobster' while model 2 is specialized for 'duck') while each model trained by IE does not.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>This paper proposes CMCL, a novel ensemble method of deep neural networks that produces diverse/plausible confident prediction of high quality. To this end, we address the over-confidence issues of MCL, and propose a new loss, architecture and training method. In our experiments, CMCL outperforms not only the known MCL, but also the traditional IE, with respect to the top-1 error rates in classification and segmentation tasks. The recent trend in the deep learning community tends to make models bigger and wider. We believe that our new ensemble approach brings a refreshing angle for developing advanced large-scale deep neural networks in many related applications.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>Proceedings of the 34 th International Conference on Machine Learning, Sydney, Australia, PMLR 70, 2017. Copyright 2017 by the author(s).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Histogram of the predictive entropy of model trained by (a) MCL (b) CMCL and (c) IE on CIFAR-10 and SVHN test data. In the case of MCL and CMCL, we separate the classes of CIFAR-10 into specialized (i.e., classes that model has a class-wise test accuracy higher than 90%) and non-specialized (others) classes. In the case of IE, we follow the proposed method by (Lakshminarayanan et al., 2016): train an ensemble of 5 models with adversarial training (AT) and measure the entropy using the averaged probability, i.e., averaging output probabilities from 5 models. (d) Detailed view of feature sharing between two models. Grey units indicate that they are currently dropped. Masked features passed to a model are all added to generate the shared features.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>effi- ciently, we use the following procedure (Guzman-Rivera et al., 2012), which optimizes model parameters {Œ∏ m } and assignment variables {v m i } alternatively: 1. Fix {Œ∏ m } and optimize {v m i }. Under fixed model parameters {Œ∏ m }, the objective (3a) is decomposable with respect to assignments {v m i } and it is easy to find optimal {v m i }. 2. Fix {v m i } and optimize {Œ∏ m }. Under fixed assignments {v m i }, the objective (3a) is decomposable with respect to model parameters {Œ∏ m }, and it requires each model to be trained inde- pendently.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>, y i ‚àà Y} and penalty parameter Œ≤ Output: Ensemble of M trained models repeat Let U (y) be a uniform distribution Sample random batch B ‚äÇ D for m = 1 to M do Compute the loss of the m-th model:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. (a) Top-1 error rate on CIFAR-10. We train an ensemble of M ResNets with 20 layers, and apply feature sharing (FS) to IE and CMCL. (b) Top-1 error rate and (c) oracle error rate on iCoseg by varying the ensemble sizes. The ensemble models trained by CMCL consistently improves the top-1 error rate over baselines.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Classification</figDesc><table>test set error rates on CIFAR-10 with varying values of the overlap parameter K explained in Section 5.1. We 
use CMCL with both feature sharing and stochastic labeling. Boldface values in parentheses represent the relative reductions from the 
best results of MCL and IE. 

Model Name 
Ensemble 
Method 

CIFAR-10 
SVHN 
Oracle Error Rate Top-1 Error Rate 
Oracle Error Rate Top-1 Error Rate 

VGGNet-17 

-(single) 
10.65% 
10.65% 
5.22% 
5.22% 
IE 
3.27% 
8.21% 
1.99% 
4.10% 
MCL 
2.52% 
45.58% 
1.45% 
45.30% 
CMCL 
2.95% 
7.83% (-4.63%) 
1.65% 
3.92% (-4.39%) 

GoogLeNet-18 

-(single) 
10.15% 
10.15% 
4.59% 
4.59% 
IE 
3.37% 
7.97% 
1.78% 
3.60% 
MCL 
2.41% 
52.03% 
1.39% 
37.92% 
CMCL 
2.78% 
7.51% (-5.77%) 
1.36% 
3.44% (-4.44%) 

ResNet-20 

-(single) 
14.03% 
14.03% 
5.31% 
5.31% 
IE 
3.83% 
10.18% 
1.82% 
3.94% 
MCL 
2.47% 
53.37% 
1.29% 
40.91% 
CMCL 
2.79% 
8.75% (-14.05%) 
1.42% 
3.68% (-6.60%) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Classification test set error rates on CIFAR-10 and SVHN for various large-scale CNN models. We train an ensemble of 5 models, and use CMCL with both feature sharing and stochastic labeling. Boldface values in parentheses indicate relative error rate reductions from the best results of MCL and IE.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>). The dataset consists of 38 groups of related images with pixel-level ground truth on foreground- background segmentation of each image. We only use im-Figure 3. Prediction results of foreground-background segmentation for a few sample images. A test error rate is shown below each prediction. The ensemble models trained by CMCL and MCL generate high-quality predictions specialized for certain images.</figDesc><table>23.81 % 
8.34 % 
10.28 % 
10.99 % 

Input 
Ground truth 
IE model 1 
CMCL model 1 

Prediction error rate: 

6.78 % 
34.12 % 
8.96 % 
9.79 % 

Prediction error rate: 

7.82 % 
33.39 % 

38.17 % 
8.71 % 

IE model 2 
CMCL model 2 MCL model 1 MCL model 2 

CMCL 
IE with FS 
IE without FS 

8.75 

8.45 

9.02 
9 

10.2 

9.72 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">School of Electrical Engineering, Korea Advanced Institute of Science and Technology (KAIST), Daejeon, Repulic of Korea. Correspondence to: Jinwoo Shin &lt;jinwoos@kaist.ac.kr&gt;.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>This work was supported in part by the ICT R&amp;D Program of MSIP/IITP, Korea, under [2016-0-00563 </p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">icoseg: Interactive co-segmentation with intelligent scribble guidance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kowdle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adarsh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Devi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiebo</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsuhan</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3169" to="3176" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Diverse m-best solutions in markov random fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yadollahpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Payman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abner</forename><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gregory</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="16" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Bagging predictors. Machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Leo</forename><surname>Breiman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1996" />
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="123" to="140" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dan</forename><surname>Ciregan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ueli</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J√ºrgen</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Discriminative reranking for natural language parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Collins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Terry</forename><surname>Koo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computational Linguistics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="25" to="70" />
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Predicting multiple structured visual interpretations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Debadeepta</forename><surname>Dey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramakrishna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Varun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Hebert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bagnell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2947" to="2955" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Bayesian averaging of classifiers and the overfitting problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pedro</forename><surname>Domingos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="page" from="223" to="230" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The pascal visual object classes (voc) challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mark</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Van</forename><surname>Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">I</forename><surname>Christopher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="303" to="338" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">A short introduction to boosting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoav</forename><surname>Freund</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Schapire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abe</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal-Japanese Society For Artificial Intelligence</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page">1612</biblScope>
			<date type="published" when="1999" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Multiple choice learning: Learning to produce multiple structured outputs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kohli</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1799" to="1807" />
		</imprint>
	</monogr>
	<note>Pushmeet</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Efficiently enforcing diversity in multi-output structured prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guzman-Rivera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pushmeet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><forename type="middle">A</forename><surname>Rutenbar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Intelligence and Statistics (AIS-TATS)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kaiming</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Xiangyu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shaoqing</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Better k-best parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth International Workshop on Parsing Technology</title>
		<meeting>the Ninth International Workshop on Parsing Technology</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="53" to="64" />
		</imprint>
	</monogr>
	<note>Association for Computational Linguistics</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Maxout networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ian</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aaron</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yoshua</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1319" to="1327" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Adaptive mixtures of local experts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><forename type="middle">A</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Nowlan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Steven</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural computation</title>
		<imprint>
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Cubic convolution interpolation for digital image processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robert</forename><surname>Keys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE transactions on acoustics, speech, and signal processing</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="1153" to="1160" />
			<date type="published" when="1981" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diederik</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jimmy</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning multiple layers of features from tiny images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Simple and scalable predictive uncertainty estimation using deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lakshminarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Balaji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alexander</forename><surname>Pritzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Blundell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Bayesian Deep Learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Why m heads are better than one: Training a diverse ensemble of deep networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Purushwalkam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Senthil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06314</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Stochastic multiple choice learning for training diverse deep ensembles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Senthil</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Purushwalkam Shiva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Cogswell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Viresh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Crandall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dhruv</forename><surname>Batra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2119" to="2127" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Network in network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Min</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qiang</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tsung-Yi</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maire</forename><surname>Michael</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Serge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pietro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Deva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Doll√°r</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lawrence</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Evan</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Trevor</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Cross-stitch networks for multi-task learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Misra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ishan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Shrivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Abhinav</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Abhinav</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3994" to="4003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Reading digits in natural images with unsupervised feature learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuval</forename><surname>Netzer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Coates</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Adam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><surname>Bissacco</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning and Unsupervised Feature Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Anh</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jeff</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">N-best maximal decoders for part models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dennis</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Deva</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2627" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Regularizing neural networks by penalizing confident output distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Pereyra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>George</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Chorowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">≈Åukasz</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Luke</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soumith</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Neil</forename><forename type="middle">C</forename><surname>Rabinowitz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hubert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kirkpatrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>James</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Koray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.04671</idno>
		<title level="m">Progressive neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Karen</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Dropout: a simple way to prevent neural networks from overfitting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nitish</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Geoffrey</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ruslan</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">15</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1929" to="1958" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Yangqing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Pierre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Scott</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dragomir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Dumitru</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vincent</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Regularization of neural networks using dropconnect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Li</forename><surname>Wan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><forename type="middle">D</forename><surname>Zeiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Sixin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning (ICML)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1058" to="1066" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Wide residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Zagoruyko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nikos</forename><surname>Komodakis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">British Machine Vision Conference (BMVC)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
