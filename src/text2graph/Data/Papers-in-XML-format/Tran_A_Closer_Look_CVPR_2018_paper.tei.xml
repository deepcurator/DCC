<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:13+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Du</forename><surname>Tran</surname></persName>
							<email>trandu@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heng</forename><surname>Wang</surname></persName>
							<email>hengwang@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lorenzo</forename><surname>Torresani</surname></persName>
							<email>torresani@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Dartmouth College</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jamie</forename><surname>Ray</surname></persName>
							<email>jamieray@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yann</forename><surname>Lecun</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Manohar</forename><surname>Paluri</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook Research</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">A Closer Look at Spatiotemporal Convolutions for Action Recognition</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Since the introduction of AlexNet <ref type="bibr" target="#b18">[19]</ref>, deep learning has galvanized the field of still-image recognition with a steady sequence of remarkable advances driven by insightful design innovations, such as smaller spatial filters <ref type="bibr" target="#b29">[30]</ref>, multi-scale convolutions <ref type="bibr" target="#b33">[34]</ref>, residual learning <ref type="bibr" target="#b12">[13]</ref>, and dense connections <ref type="bibr" target="#b13">[14]</ref>. Conversely, it may be argued that the video domain has not yet witnessed its "AlexNet moment." While a deep network (I3D <ref type="bibr" target="#b3">[4]</ref>) does currently hold the best results in action recognition, the margin of improvement over the best hand-crafted approach (iDT <ref type="bibr" target="#b37">[38]</ref>) is not as impressive as in the case of image recognition. Furthermore, an image-based 2D CNN (ResNet-152 <ref type="bibr" target="#b24">[25]</ref>) operating on individual frames of the video achieves performance remarkably close to the state-of-the-art on the challenging Sports-1M benchmark. This result is both surprising and frustrating, given that 2D CNNs are unable to model temporal information and motion patterns, which one would deem to be critical aspects for video analysis. Based on such results, one may postulate that temporal reasoning is not essential for accurate action recognition, because of the strong action class information already contained in the static frames of a sequence.</p><p>In this work, we challenge this view and revisit the role of temporal reasoning in action recognition by means of 3D CNNs, i.e., networks that perform 3D convolutions over the spatiotemporal video volume. While 3D CNNs have been widely explored in the setting of action recognition <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b34">35,</ref><ref type="bibr" target="#b35">36,</ref><ref type="bibr" target="#b3">4]</ref>, here we reconsider them within the framework of residual learning, which has been shown to be a powerful tool in the field of still-image recognition. We demonstrate that 3D ResNets significantly outperform 2D ResNets for the same depth when trained and evaluated on large-scale, challenging action recognition benchmarks such as Sports-1M <ref type="bibr" target="#b15">[16]</ref> and Kinetics <ref type="bibr" target="#b16">[17]</ref>.</p><p>Inspired by these results, we introduce two new forms of spatiotemporal convolution that can be viewed as middle grounds between the extremes of 2D (spatial convolution) and full 3D. The first formulation is named mixed convolution (MC) and consists in employing 3D convolutions only in the early layers of the network, with 2D convolutions in the top layers. The rationale behind this design is that motion modeling is a low/mid-level operation that can be implemented via 3D convolutions in the early layers of a network, and spatial reasoning over these mid-level motion features (implemented by 2D convolutions in the top layers) leads to accurate action recognition. We show that MC ResNets yield roughly a 3-4% gain in clip-level accuracy over 2D ResNets of comparable capacity and they match the performance of 3D ResNets, which have 3 times as many parameters. The second spatiotemporal variant is a "(2+1)D" convolutional block, which explicitly factorizes 3D convolution into two separate and successive operations, a 2D spatial convolution and a 1D temporal convolution. What do we gain from such a decomposition? The first advantage is an additional nonlinear rectification between these two operations. This effectively doubles the number of nonlinearities compared to a network using full 3D convolutions for the same number of parameters, thus rendering the model capable of representing more complex functions. The second potential benefit is that the decomposition facilitates the optimization, yielding in practice both a lower training loss and a lower testing loss. In other words we find that, compared to full 3D filters where appearance and dy-namics are jointly intertwined, the (2+1)D blocks (with factorized spatial and temporal components) are easier to optimize. Our experiments demonstrate that ResNets adopting (2+1)D blocks homogeneously in all layers achieve stateof-the-art performance on both Kinetics and Sports-1M.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Video understanding is one of the core computer vision problems and has been studied for decades. Many research contributions in video understanding have focused on developing spatiotemporal features for video analysis. Some proposed video representations include spatiotemporal interest points (STIPs) <ref type="bibr" target="#b20">[21]</ref>, SIFT-3D <ref type="bibr" target="#b26">[27]</ref>, HOG3D <ref type="bibr" target="#b17">[18]</ref>, Motion Boundary Histogram <ref type="bibr" target="#b4">[5]</ref>, Cuboids <ref type="bibr" target="#b5">[6]</ref>, and ActionBank <ref type="bibr" target="#b25">[26]</ref>. These representations are hand-designed and use different feature encoding schemes such as those based on histograms or pyramids. Among these hand-crafted representations, improved Dense Trajectories (iDT) <ref type="bibr" target="#b37">[38]</ref> is widely considered the state-of-the-art, thanks to its strong results on video classification.</p><p>After the breakthrough of deep learning in still-image recognition originated by the introduction of the AlexNet model <ref type="bibr" target="#b18">[19]</ref>, there has been active research devoted to the design of deep networks for video. Many attempts in this genre leverage CNNs trained on images to extract features from the individual frames and then perform temporal integration of such features into a fixed-size descriptor using pooling, high-dimensional feature encoding <ref type="bibr" target="#b40">[41,</ref><ref type="bibr" target="#b10">11]</ref>, or recurrent neural networks <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b1">2]</ref>. Karpathy et al. <ref type="bibr" target="#b15">[16]</ref> presented a thorough study on how to fuse temporal information in CNNs and proposed a "slow fusion" model that extends the connectivity of all convolutional layers in time and computes activations though temporal convolutions in addition to spatial convolutions. However, they found that the networks operating on individual frames performed on par with the networks processing the entire spatiotemporal volume of the video. 3D CNNs using temporal convolutions for recognizing human actions in video were arguably first proposed by Baccouche et al. <ref type="bibr" target="#b0">[1]</ref> and by Ji et al. <ref type="bibr" target="#b14">[15]</ref>. But 3D convolutions were also studied in parallel for unsupervised spatiotemporal feature learning with Restricted Boltzmann Machines <ref type="bibr" target="#b34">[35]</ref> and stacked ISA <ref type="bibr" target="#b21">[22]</ref>. More recently, 3D CNNs were shown to lead to strong action recognition results when trained on large-scale datasets <ref type="bibr" target="#b35">[36]</ref>. 3D CNNs features were also demonstrated to generalize well to other tasks, including action detection <ref type="bibr" target="#b27">[28]</ref>, video captioning <ref type="bibr" target="#b23">[24]</ref>, and hand gesture detection <ref type="bibr" target="#b22">[23]</ref>.</p><p>Another influential approach to CNN-based video modeling is represented by the two-stream framework introduced by Simonyan and Zisserman <ref type="bibr" target="#b28">[29]</ref>, who proposed to fuse deep features extracted from optical flow with the more traditional deep CNN activations computed from color RGB input. Feichtenhofer et al. enhanced these two-stream networks with ResNet architectures <ref type="bibr" target="#b12">[13]</ref> and additional connections between streams <ref type="bibr" target="#b8">[9]</ref>. Additional two-stream approaches include Temporal Segment Networks <ref type="bibr" target="#b38">[39]</ref>, Action Transformations <ref type="bibr" target="#b39">[40]</ref>, and Convolutional Fusion <ref type="bibr" target="#b9">[10]</ref>. Notably, Carreira and Zisserman recently introduced a model (I3D) that combines two-stream processing and 3D convolutions. I3D currently holds the best action recognition results on the large-scale Kinetics dataset.</p><p>Our work revisits many of the aforementioned approaches (specifically 3D CNNs, two-stream networks, and ResNets) in the context of an empirical analysis deeply focused on understanding the effects of different types of spatiotemporal convolutions on action recognition performance. We include in this study 2D convolution over frames, 2D convolution over clips, 3D convolution, interleaved (mixed) 3D-2D convolutions, as well as a decomposition of 3D convolution into a 2D spatial convolution followed by 1D temporal convolution, which we name (2+1)D convolution. We show that when used within a ResNet architecture <ref type="bibr" target="#b12">[13]</ref>, (2+1)D convolutions lead to state-of-theart results on 4 different benchmarks in action recognition. Our architecture, called R(2+1)D, is related to Factorized Spatio-Temporal Convolutional Networks <ref type="bibr" target="#b32">[33]</ref> (F ST CN ) in the way of factorizing spatiotemporal convolutions into spatial and temporal ones. However, F ST CN focuses on network factorization, e.g. F ST CN is implemented by several spatial layers at the lower layers and two parallel temporal layers on its top. On the other hand, R(2+1)D focuses on layer factorization, i.e. factorizing each spatiotemporal convolution into a block of a spatial convolution and a temporal convolution. As a result, R(2+1)D is alternating between spatial and temporal convolutions across the network. R(2+1)D is also closely related to the Pseudo-3D network (P3D) <ref type="bibr" target="#b24">[25]</ref>, which includes three different residual blocks that adapt the bottleneck block of 2D ResNets to video. The blocks implement different forms of spatiotemporal convolution: spatial followed by temporal, spatial and temporal in parallel, and spatial followed by temporal with skip connection from spatial convolution to the output of the block, respectively. The P3D model is formed by interleaving these three blocks in sequence through the depth of the network. In contrast, our R(2+1)D model uses a single type of spatiotemporal residual block homogeneously in all layers and it does not include bottlenecks. Instead, we show that by making a careful choice of dimensionality for the spatial-temporal decomposition in each block we can obtain a model that is compact in size and that yet leads to state-of-the-art action recognition accuracy. For example, on Sports-1M using RGB as input, R(2+1)D outperforms P3D by a margin of 9.1% in Clip@1 accuracy (57.0% vs 47.9%), despite the fact that P3D uses a 152-layer ResNet, while our model has only 34 layers.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Convolutional residual blocks for video</head><p>In this section we discuss several spatiotemporal convolutional variants within the framework of residual learning. Let x denote the input clip of size 3 × L × H × W , where L is the number of frames in the clip, H and W are the frame height and width, and 3 refers to the RGB channels. Let z i be the tensor computed by the i-th convolutional block in the residual network. In this work we consider only "vanilla" residual blocks (i.e., without bottlenecks) <ref type="bibr" target="#b12">[13]</ref>, with each block consisting of two convolutional layers with a ReLU activation function after each layer. Then the output of the i-th residual block is given by</p><formula xml:id="formula_0">z i = z i−1 + F(z i−1 ; θ i )<label>(1)</label></formula><p>where F(; θ i ) implements the composition of two convolutions parameterized by weights θ i and the application of the ReLU functions. In this work we consider networks where the sequence of convolutional residual blocks culminates into a top layer performing global average pooling over the entire spatiotemporal volume and a fully-connected layer responsible for the final classification prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">R2D: 2D convolutions over the entire clip</head><p>2D CNNs for video <ref type="bibr" target="#b28">[29]</ref> ignore the temporal ordering in the video and treat the L frames analogously to channels. Thus, we can think of these models as reshaping the input 4D tensor x into a 3D tensor of size 3L × H × W . The output z i of the i-th residual block is also a 3D tensor. Its size is N i × H i × W i where N i denotes the number of convolutional filters applied in the i-th block, and H i , W i are the spatial dimensions, which may be smaller than the original input frame due to pooling or striding. Each filter is 3D and it has size N i−1 × d × d, where d denotes the spatial width and height. Note that although the filter is 3-dimensional, it is convolved only in 2D over the spatial dimensions of the preceding tensor z i−1 . Each filter yields a single-channel output. Thus, the very first convolutional layer in R2D collapses the entire temporal information of the video in single-channel feature maps, which prevent any temporal reasoning to happen in subsequent layers. This type of CNN architecture is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(a). Note that since the feature maps have no temporal meaning, we do not perform temporal striding for this network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">f-R2D: 2D convolutions over frames</head><p>Another 2D CNN approach involves processing independently the L frames via a series of 2D convolutional residual block. The same filters are applied to all L frames. In this case, no temporal modeling is performed in the convolutional layers and the global spatiotemporal pooling layer at the top simply fuses the information extracted independently from the L frames. We refer to this architecture variant as f-R2D (frame-based R2D).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">R3D: 3D convolutions</head><p>3D CNNs <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b35">36]</ref> preserve temporal information and propagate it through the layers of the network. The tensor z i is in this case 4D and has size N i × L × H i × W i , where N i is the number of filters used in the i-th block. Each filter is 4-dimensional and it has size N i−1 × t × d × d where t denotes the temporal extent of the filter (in this work, we use t = 3, as in <ref type="bibr" target="#b35">[36,</ref><ref type="bibr" target="#b3">4]</ref>). The filters are convolved in 3D, i.e., over both time and space dimensions. This type of CNN architecture is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(d).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">MCx and rMCx: mixed 3D-2D convolutions</head><p>One hypothesis is that motion modeling (i.e., 3D convolutions) may be particularly useful in the early layers, while at higher levels of semantic abstraction (late layers), motion or temporal modeling is not necessary. Thus a plausible architecture may start with 3D convolutions and switch to convolutional block splits the computation into a spatial 2D convolution followed by a temporal 1D convolution. We choose the numbers of 2D filters (Mi) so that the number of parameters in our (2+1)D block matches that of the full 3D convolutional block.</p><p>using 2D convolutions in the top layers. Since in this work we consider 3D ResNets (R3D) having 5 groups of convolutions (see <ref type="table">Table 1</ref>), our first variant consists in replacing all 3D convolutions in group 5 with 2D convolutions. We denote this variant with MC5 (Mixed Convolutions). We design a second variant that uses 2D convolutions in group 4 and 5, and name this model MC4 (meaning from group 4 and deeper layers all convolutions are 2D). Following this pattern, we also create MC3 and MC2 variations. We omit to consider MC1 since it is equivalent to a 2D ResNet (f-R2D) applied to clip inputs. This type of CNN architectures is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(b). An alternative hypothesis is that temporal modeling may be more beneficial in the deep layers, with early capturing appearance information via 2D convolutions. To account for such possibility, we also experiment with "Reversed" Mixed Convolutions.</p><p>Following the naming convention of MC models, we denote these models as rMC2, rMC3, rMC4, and rMC5. Thus, rMC3 would include 2D convolutions in block 1 and 2, and 3D convolutions in group 3 and deeper groups. This type of CNN architecture is illustrated in <ref type="figure" target="#fig_1">Figure 1</ref>(c).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">R(2+1)D: (2+1)D convolutions</head><p>Another possible theory is that full 3D convolutions may be more conveniently approximated by a 2D convolution followed by a 1D convolution, decomposing spatial and temporal modeling into two separate steps. We thus design a network architecture named R(2+1)D, where we replace the N i 3D convolutional filters of size  the temporal convolutions. We choose</p><formula xml:id="formula_1">N i−1 × t × d × d with a (2+1)D block consisting of M i 2D convolutional fil- ters of size N i−1 × 1 × d × d and N i temporal convolu- tional filters of size M i × t × 1 × 1.</formula><formula xml:id="formula_2">M i = ⌊ td 2 Ni−1Ni d 2 Ni−1+tNi</formula><p>⌋ so that the number of parameters in the (2+1)D block is approximately equal to that implementing full 3D convolution. We note that this spatiotemporal decomposition can be applied to any 3D convolutional layer. An illustration of this decomposition is given in <ref type="figure" target="#fig_2">Figure 2</ref> for the simplified setting where the input tensor z i−1 contains a single channel (i.e., N i−1 = 1). If the 3D convolution has spatial or temporal striding (implementing downsampling), the striding is correspondingly decomposed into its spatial or temporal dimensions. This architecture is illustrated in <ref type="figure" target="#fig_1">Figure 1(e)</ref>.</p><p>Compared to full 3D convolution, our (2+1)D decomposition offers two advantages. First, despite not changing the number of parameters, it doubles the number of nonlinearities in the network due to the additional ReLU between the 2D and 1D convolution in each block. Increasing the number of nonlinearities increases the complexity of functions that can be represented, as also noted in VGG networks <ref type="bibr" target="#b29">[30]</ref> which approximate the effect of a big filter by applying multiple smaller filters with additional nonlinearities in between. The second benefit is that forcing the 3D convolution into separate spatial and temporal components renders the optimization easier. This is manifested in lower training error compared to 3D convolutional networks of the same capacity. This is illustrated in <ref type="figure" target="#fig_4">Figure 3</ref> which shows training and testing errors for R3D and R(2+1)D having 18 (left) and 34 (right) layers. It can be seen that, for the same number of layers (and parameters), R(2+1)D yields not only lower testing error but also lower training error compared to R3D. This is an indication that optimization becomes easier when spatiotemporal filters are factorized. The gap in the training losses is particularly large for the nets having 34 layers, which suggests that the facilitation in optimization increases as the depth becomes larger.</p><p>We note that our factorization is closely related to Pseudo-3D blocks (P3D) <ref type="bibr" target="#b24">[25]</ref>, which were proposed to adapt the bottleneck block of R2D to video classification. Three different pseudo-3D blocks were introduced: P3D-A, P3D-B, and P3D-C. The blocks implement different orders of convolution: spatial followed by temporal, spatial and temporal in parallel, and spatial followed by temporal with skip connection from spatial convolution to the output of the block, respectively. Our (2+1)D convolution is most closely related to the P3D-A block, which however contains bottlenecks. Furthermore, the final P3D architecture is composed by interleaving these three blocks in sequence throughout the network, with the exception of the first layer where 2D convolution is used. We propose instead a homogeneous architecture where the same (2+1)-decomposition is used in all blocks. Another difference is that P3D-A is not purposely designed to match the number of parameters with the 3D convolutions. Despite the fact that R(2+1)D is very simple and homogenous in its architecture, our experiments show that it significantly outperforms R3D, R2D, and P3D on Sports-1M (see <ref type="table">Table 4</ref>).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>In this section we present a study of action recognition accuracy for the different spatiotemporal convolutions presented in the previous section. We use Kinetics <ref type="bibr" target="#b3">[4]</ref> and Sports-1M <ref type="bibr" target="#b15">[16]</ref> as the primary benchmarks, as they are large enough to enable training of deep models from scratch. Since a good video model must also support effective transfer learning to other datasets, we include results obtained by pretraining our models on Sports-1M and Kinetics, and finetuning them on UCF101 <ref type="bibr" target="#b30">[31]</ref> and HMDB51 <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental setup</head><p>Network architectures. We constrain our experiments to deep residual networks <ref type="bibr" target="#b12">[13]</ref> owing to their good performance and simplicity. <ref type="table">Table 1</ref> provides the specifications of the two R3D architectures (3D ResNets) considered in our experiments. The first has 18 layers, while the second variant has 34 layers. Each network takes clips consisting of L RGB frames with the size of 112 × 112 as input. We use one spatial downsampling at conv1 implemented by convolutional striding of 1 × 2 × 2, and three spatiotemporal downsampling at conv3 1, conv4 1, and conv5 1 with convolutional striding of 2 × 2 × 2. From these R3D models we obtain architectures R2D, MCx, rMCx, and R(2+1)D by replacing the 3D convolutions with 2D convolutions, mixed convolutions, reversed mixed convolutions, and (2+1)D convolutions, respectively. Since our spatiotemporal downsampling is implemented by 3D convolutional striding, when 3D convolutions are replaced by 2D ones, e.g., as in MCx and rMCx, spatiotemporal downsampling becomes only spatial. This difference yields activation tensors of different temporal sizes in the last convolutional layer. For example, for f-R2D the output of the last convolution layer is L × 7 × 7, since no temporal striding is applied. Conversely, for R3D and R(2+1)D the last convolutional tensor has size</p><formula xml:id="formula_3">L 8 × 7 × 7.</formula><p>MCx and rMCx models will yield different sizes in the time dimension, depending on how many times temporal striding is applied (as shown in the <ref type="table">Table 1</ref>). Regardless of the size of output produced by the last convolutional layer, each network applies global spatiotemporal average pooling to the final convolutional tensor, followed by a fully-connected (fc) layer performing the final classification (the output dimension of the fc layer matches the number of classes, e.g., 400 for Kinetics). Training and evaluation. We perform our initial evaluation on Kinetics, using the training split for training and the validation split for testing. For a fair comparison, we set all of the networks to have 18 layers and we train them from scratch on the same input. Video frames are scaled to the size of 128 × 171 and then each clip is generated by randomly cropping windows of size 112 × 112. We randomly sample L consecutive frames from the video with temporal jittering while training. In this comparison, we experiment with two settings: models are trained on 8-frame clips (L = 8) and 16-frame clips (L = 16). Batch normalization is applied to all convolutional layers. We use a mini-batch size of 32 clips per GPU. Although Kinetics has only about 240k training videos, we set epoch size to be 1M for temporal jittering. The initial learning rate is set to 0.01 and divided by 10 every 10 epochs. We use the first 10 epochs for warm-up <ref type="bibr" target="#b11">[12]</ref> in our distributed training. Training is done in 45 epochs. We report clip top-1 accuracy and video top-1 accuracy. For video top-1, we use center crops of 10 clips uniformly sampled from the video and average these 10 clip predictions to obtain the video prediction. Training  is done with synchronous distributed SGD on GPU clusters using caffe2 <ref type="bibr" target="#b2">[3]</ref>. <ref type="table">Table 2</ref> reports the clip top-1 and video top-1 action classification accuracy on the Kinetics validation set. There are a few findings that can be inferred from these results. First, there is a noticeable gap between the performance of 2D ResNets (f-R2D and R2D) and that of R3D or mixed convolutional models (MCx and rMCx). This gap is 1.3 − 4% in the 8-frame input setting and becomes bigger (i.e. 1.8 − 6.7%) when models are trained on 16-frame clips as input. This suggests that motion modeling is important for action recognition. Note that all models (within the same setting) see the same input and process all frames in each clip (either 8 or 16 frames). The difference is that, compared to 3D or MCx models which perform temporal reasoning through the clip, R2D collapses and eliminates temporal information after the first residual block, while f-R2D computes stillimage features from the individual frames. Among the different 3D convolutional models, R(2+1)D clearly performs the best. It is 2.1−3.4% better than MCx, rMCx, R3D in the 8-frame setting, and 3.1 − 4.7% better in the 16-frame input setting. This indicates that decomposing 3D convolutions in separate spatial and temporal convolutions is better than modeling spatiotemporal information jointly or via mixed 3D-2D convolutions. It also outperforms 2D ResNets (R2D and f-R2D) by 4.7 − 6.1% in the 8-frame setting and by 6.3 − 9.8% in the 16-frame input setting. <ref type="figure" target="#fig_7">Figure 4</ref> shows video top-1 accuracy on Kinetics validation set versus computational complexity (FLOPs) for different models. <ref type="figure" target="#fig_7">Figure 4</ref>(a) plots the models trained on 8-frame clips while <ref type="figure" target="#fig_7">Figure 4</ref>(b) shows models with 16-frame clip input. The most efficient network is R2D but it has the poorest accuracy. In fact, R2D is about 7x faster than f-R2D because it collapses the temporal dimension after conv1.  <ref type="table" target="#tab_5">-R2D  R3D  MC2  MC3  MC4  MC5  rMC2  rMC3  rMC4  rMC5   05  10  15  20  25  30  35  40  45</ref> FLOPs <ref type="formula">(</ref> In terms of accuracy, R2D gets similar performance to f-R2D when trained on 8-frame clips, while it is 1.6% worse than f-R2D in the 16-frame input setting. This is because R2D performs temporal modeling only in the conv1 layer and thus it handles poorly longer clip inputs. Interestingly, rMC3 is more efficient than f-R2D since it performs temporal striding in conv3 1, which yields smaller activation tensors in all subsequent 2D convolutional layers. Conversely, f-R2D processes all frames independently and does not perform any temporal striding. rMC2 is more costly than rMC3, as it uses 2D convolutions in group 2, and does not perform temporal striding in group 3. R(2+1)D has roughly the same computational cost as R3D but it yields higher accuracy. We note that the relative ranking between different architectures is consistent across the two input settings (8 vs 16 frame-clips). However, the gaps are bigger for the 16-frame input setting. This indicates that temporal modeling is more beneficial on longer clip inputs. Why are (2+1)D convolutions better than 3D? <ref type="figure" target="#fig_4">Figure 3</ref> presents the training and testing errors on Kinetics for R3D and R(2+1)D, using 18-layers (left) and 34 layers (right). We already know that R(2+1)D gives lower testing error than R3D but the interesting message in this plot is that R(2+1)D yields also lower training error. The reduction in training error for R(2+1)D compared to R3D is particularly accentuated for the architecture having 34 layers. This suggests that the spatiotemporal decomposition of R(2+1)D renders the optimization easier compared to R3D, especially as depth is increased.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Comparison of spatiotemporal convolutions</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Revisiting practices for video-level prediction</head><p>Varol et. al. <ref type="bibr" target="#b36">[37]</ref> showed that accuracy gains can be obtained by training video CNNs on longer input clips (e.g. with 100 frames) using long-term convolutions (LTC). Here we revisit this idea and evaluate this practice on Kinetics using R(2+1)D of 18 layers with varying input clip lengths: 8, 16, 24, 32, 40, and 48 frames. The outputs of the last convolution layer for these networks have different temporal sizes, but once again we use a global spatiotemporal average pooling to generate a fixed-size representation which is  fed to the fully-connected layer. Note that these networks have the same number of parameters (since pooling involves no learnable parameters). They simply see inputs of different lengths. <ref type="figure" target="#fig_8">Figure 5</ref> a) plots the clip-level and video-level accuracy on Kinetics validation set with respect to different input lengths. Note that video-level prediction is done by averaging the clip-level predictions obtained for 10 clips evenly spaced in the video. One interesting finding is that, although clip accuracy continues to increase when we add more frames, video accuracy peaks at 32 frames.</p><p>Since all these models have the same numbers of parameters, it is natural to ask "what causes the differences in video-level accuracies?" In order to address this question, we conduct two experiments. In the first experiment, we take the model trained on 8-frame clips and test it using 32-frame clips as input. We found that this causes a drop of 1.2% in clip accuracy and 5.8% in video accuracy compared to testing on 8-frame clips. In the second experiment, we finetuned the 32-frame model using as initialization the parameters of the 8-frame model. In this case, the net achieves results that are almost as good as when learning from scratch on 32-frame clips (59.8% vs 60.1%) and produces a gain of 7% over the 8-frame model. The advantage, however, is that finetuning the 32-frame model from the 8-frame net shortens considerably the total training time versus learning from scratch, since the 8-frame model is 7.3x faster than the 32-frame model in terms of FLOPs. These two experiments suggest that training on longer clips yields different (better) clip-level models, as the filters learn longer-term temporal patterns. This improve-  <ref type="table">Table 4</ref>. Comparison with the state-of-the-art on Sports-1M. R(2+1)D outperforms C3D by 10.9%, and P3D by 9.1% and it achieves the best reported accuracy on this benchmark to date. *These baseline numbers are taken from <ref type="bibr" target="#b24">[25]</ref>.</p><p>ment cannot be obtained "for free" by simply lengthening the clip input at test time. This is consistent with the findings in <ref type="bibr" target="#b36">[37]</ref>. <ref type="table">Table 3</ref> reports the total training time and accuracy of R(2+1)D with 18 layers trained and evaluated on clips of varying length. How many clips are needed for accurate video-level prediction? <ref type="figure" target="#fig_8">Figure 5</ref> b) plots the video top-1 accuracy of R(2+1)D with 18 layers trained on 32-frame clips when we vary the number of clips sampled from each video. Using 20 crops is only about 0.5% worse than using 100 crops, but the prediction is 5x faster.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Action recognition with a 34-layer R(2+1)D net</head><p>In this section we report results using a 34-layer version of R(2+1)D, which we denote as R(2+1)D-34. The architecture is the same as that shown in the right column of <ref type="table">Table 1</ref>, but with 3D convolutions decomposed spatiotemporally in (2+1)D. We train our R(2+1)D architecture on both RGB and optical flow inputs and fuse the prediction scores by averaging, as proposed in the two-stream framework <ref type="bibr" target="#b28">[29]</ref> and subsequent work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b8">9]</ref>. We use Farneback's method <ref type="bibr" target="#b7">[8]</ref> to compute optical flow because of its efficiency. Datasets. We evaluate our proposed R(2+1)D architecture on four public benchmarks. Sports-1M is a large-scale dataset for classification of sport videos <ref type="bibr" target="#b15">[16]</ref>. It includes 1.1M videos of 487 fine-grained sport categories. It is provided with a train/test split. Kinetics has about 300K videos of 400 human actions. We report results on the validation set as the annotations on the testing set is not public available. UCF101 and HMDB51 are well-established benchmarks for action recognitions. UCF101 has about 13K videos of 101 categories, whereas HMDB51 is slightly smaller with 6K videos of 51 classes. Both UCF101 and HMDB51 are provided with 3 splits for training and testing. We report the accuracy by averaging over all 3 splits. Results on Sports-1M. We train R(2+1)D-34 on Sports-1M <ref type="bibr" target="#b15">[16]</ref>   ing is done with a setup similar to that described in section 4.1. We also train a R3D-34 baseline for comparison. Videos in Sports-1M are very long, over 5 minutes on average. Thus, we uniformly sample 100 clips per video (instead of 10 clips on Kinetics) for computing the video top-1 accuracy. Average pooling is used to aggregate the predictions over the 100 clips to obtain the video-level predictions. <ref type="table">Table 4</ref> shows the results on Sports-1M. Our R(2+1)D model trained on RGB performs the best among the methods in this comparison. In clip-level accuracy, it outperforms C3D by 10.9% and P3D by 9.1%. R(2+1)D also outperforms 2D ResNet by 10.5%. We note that the 2D ResNet and P3D have 152 layers while R(2+1)D has only 34 layers (or 67 if we count the spatiotemporal decomposition as producing two layers). The R3D baseline is also inferior to R(2+1)D (by 2.3%) when the input is 8 RGB frames, which confirms the benefits of our (2+1)D decomposition. R(2+1)D achieves a video-level accuracy of 73.3% which, to our knowledge, is the best published result on Sports-1M. Results on Kinetics. We assess the performance of R(2+1)D-34 on Kinetics, both when training from scratch on the Kinetics training split, as well as when finetuning the model pretrained on Sports-1M. When training from scratch, we use the same setup as in section 4.1. When finetuning, we start with a base learning rate that is 10 times smaller (i.e., 0.001), and reduce it by a factor of 10 every 4 epochs. Finetuning is completed at 15 epochs. Table 5 reports the results on Kinetics. R(2+1)D outperforms I3D by 4.5% when both models are trained from scratch on RGB input. This indicates that our R(2+1)D is a competitive architecture for action recognition. Our R(2+1)D pretrained on Sports-1M also outperforms I3D pretrained on ImageNet by 2.2% when using RGB as input and by 3.2% when trained on optical flow. However, it is slightly worse than I3D (by 0.3%) when fusing the two streams. Transferring models to UCF101 and HMDB51. We also experiment with finetuning R(2+1)D on UCF101 <ref type="bibr" target="#b30">[31]</ref> and HMDB51 <ref type="bibr" target="#b19">[20]</ref>   <ref type="table">Table 6</ref>. Comparison with the state-of-the-art on UCF101 and HMDB51. Our R(2+1)D finetuned from Kinetics is nearly on par with I3D which, however, uses ImageNet in addition to Kinetics for pretraining. We found that Kinetics is better than Sports-1M for pretraining our models.</p><p>Kinetics. For the models based on Kinetics pretraining, we use the models learned from scratch on Kinetics (not those finetuned from Sports-1M) in order to understand the effects of pretraining on different datasets. <ref type="table">Table 6</ref> reports results of R(2+1)D compared to prior methods. R(2+1)D outperforms all methods in this comparison, except for I3D which, however, used ImageNet in addition to Kinetics for pretraining. R(2+1)D (with Kinetics pretraining) is comparable to I3D when trained on RGB but it is slightly worse than I3D when trained on optical flow. This can be explained by noting that R(2+1)D uses Farneback's optical flow, while I3D uses a more accurate flow, TV-L1 <ref type="bibr" target="#b42">[43]</ref> which is, however, one order of magnitude slower than Farneback's method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>We have presented an empirical study of the effects of different spatiotemporal convolutions for action recognition in video. Our proposed architecture R(2+1)D achieves results comparable or superior to the state of the art on Sports-1M, Kinetics, UCF101, and HMDB51. We hope that our analysis will inspire new network designs harnessing the potential efficacy and modeling flexibility of spatiotemporal convolutions. While our study was focused on a single type of network (ResNet) and a homogenous use of our (2+1)D spatiotemporal decomposition, future work will be devoted to searching more suitable architectures for our approach.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Residual network architectures for video classification considered in this work. (a) R2D are 2D ResNets; (b) MCx are ResNets with mixed convolutions (MC3 is presented in this figure); (c) rMCx use reversed mixed convolutions (rMC3 is shown here); (d) R3D are 3D ResNets; and (e) R(2+1)D are ResNets with (2+1)D convolutions. For interpretability, residual connections are omitted.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. (2+1)D vs 3D convolution. The illustration is given for the simplified setting where the input consists of a spatiotemporal volume with a single feature channel. (a) Full 3D convolution is carried out using a filter of size t × d × d where t denotes the temporal extent and d is the spatial width and height. (b) A (2+1)D convolutional block splits the computation into a spatial 2D convolution followed by a temporal 1D convolution. We choose the numbers of 2D filters (Mi) so that the number of parameters in our (2+1)D block matches that of the full 3D convolutional block.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>The hyperparameter M i determines the dimensionality of the intermediate sub- space where the signal is projected between the spatial and</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Training and testing errors for R(2+1)D and R3D. Results are reported for ResNets of 18 layers (left) and 34 layers (right). It can be observed that the training error (thin lines) is smaller for R(2+1)D compared to R3D, particularly for the network with larger depth (right). This suggests that the the spatialtemporal decomposition implemented by R(2+1)D eases the optimization, especially as depth is increased.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Accuracy vs computational complexity for different types of convolution on Kinetics. Different models are trained on 8-frame clips (left) and 16-frame clips (right). R(2+1)D achieves the highest accuracy, producing about 3−3.8% accuracy gain over R3D for the same computational cost.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Video-level accuracy on Kinetics. a) Clip and video accuracy of a 18-layer R(2+1)D trained on clips of different lengths. b) Video top-1 accuracy obtained by averaging over different number of clip predictions using the same model with 32 frames.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>with both 8-frame and 32-frame clip inputs. Train-</figDesc><table>method 

pretraining dataset top1 top5 
I3D-RGB [4] 
none 
67.5 87.2 
I3D-RGB [4] 
ImageNet 
72.1 90.3 
I3D-Flow [4] 
ImageNet 
65.3 86.2 
I3D-Two-Stream [4] 
ImageNet 
75.7 92.0 
R(2+1)D-RGB 
none 
72.0 90.0 
R(2+1)D-Flow 
none 
67.5 87.2 
R(2+1)D-Two-Stream 
none 
73.9 90.9 
R(2+1)D-RGB 
Sports-1M 
74.3 91.4 
R(2+1)D-Flow 
Sports-1M 
68.5 88.1 
R(2+1)D-Two-Stream 
Sports-1M 
75.4 91.9 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Comparison with the state-of-the-art on Kinetics. R(2+1)D outperforms I3D by 4.5% when trained from scratch on RGB. R(2+1)D pretrained on Sports-1M outperforms I3D pre- trained on ImageNet, for both RGB and optical flow. However, it is slightly worse than I3D (0.3%) when fusing the two streams.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head></head><label></label><figDesc>using models pretrained on Sports-1M and</figDesc><table>method 
pretraining dataset UCF101 HMDB51 
Two-Stream [29] 
ImageNet 
88.0 
59.4 
Action Transf. [40] 
ImageNet 
92.4 
62.0 
Conv Pooling [42] 
Sports-1M 
88.6 
-
F ST CN [33] 
ImageNet 
88.1 
59.1 
Two-Stream Fusion [10] 
ImageNet 
92.5 
65.4 
Spatiotemp. ResNet [9] 
ImageNet 
93.4 
66.4 
Temp. Segm. Net [39] 
ImageNet 
94.2 
69.4 
P3D [25] 
ImageNet+Sports1M 
88.6 
-
I3D-RGB [4] 
ImageNet+Kinetics 
95.6 
74.8 
I3D-Flow [4] 
ImageNet+Kinetics 
96.7 
77.1 
I3D-Two-Stream [4] 
ImageNet+Kinetics 
98.0 
80.7 
R(2+1)D-RGB 
Sports1M 
93.6 
66.6 
R(2+1)D-Flow 
Sports1M 
93.3 
70.1 
R(2+1)D-TwoStream 
Sports1M 
95.0 
72.7 
R(2+1)D-RGB 
Kinetics 
96.8 
74.5 
R(2+1)D-Flow 
Kinetics 
95.5 
76.4 
R(2+1)D-TwoStream 
Kinetics 
97.3 
78.7 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. The authors would like to thank Ahmed Taei, Aarti Basant, Aapo Kyrola, and the Facebook Caffe2 team for their help in implementing NDconvolution, in optimizing video I/O, and in providing support for distributed training. We are grateful to Joao Carreira for sharing I3D results on the Kinetics validation set.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Baccouche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Mamalet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wolf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Baskurt</surname></persName>
		</author>
		<title level="m">Sequential Deep Learning for Human Action Recognition</title>
		<meeting><address><addrLine>Berlin Heidelberg; Berlin, Heidelberg</addrLine></address></meeting>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="29" to="39" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Delving deeper into convolutional networks for learning video representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ballas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Pal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06432</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Caffe2: A new lightweight, modular, and scalable deep learning framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Caffe2-Team</surname></persName>
		</author>
		<ptr target="https://caffe2.ai/.6" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Quo vadis, action recognition? a new model and the kinetics dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Human Detection Using Oriented Histograms of Flow and Appearance</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV &apos;06)</title>
		<editor>A. Leonardis, H. Bischof, and A. Pinz</editor>
		<meeting><address><addrLine>Graz, Austria</addrLine></address></meeting>
		<imprint>
			<publisher>Springer-Verlag</publisher>
			<date type="published" when="2006-05" />
			<biblScope unit="volume">3952</biblScope>
			<biblScope unit="page" from="428" to="441" />
		</imprint>
	</monogr>
	<note>LNCS)</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Behavior recognition via sparse spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rabaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Cottrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV VS-PETS</title>
		<meeting>ICCV VS-PETS</meeting>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2625" to="2634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Two-frame motion estimation based on polynomial expansion</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Farnebäck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Image Analysis, 13th Scandinavian Conference, SCIA 2003</title>
		<meeting><address><addrLine>Halmstad, Sweden</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2003-07-02" />
			<biblScope unit="page" from="363" to="370" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatiotemporal residual networks for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Wildes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional two-stream network fusion for video action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Feichtenhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Pinz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Russell. Actionvlad: Learning spatio-temporal aggregation for action classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girdhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sivic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Noordhuis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wesolowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kyrola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tulloch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02677</idno>
		<title level="m">Accurate, large minibatch sgd: training imagenet in 1 hour</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">3d convolutional neural networks for human action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE TPAMI</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Large-scale video classification with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Toderici</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Shetty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Sukthankar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">The kinetics human action video dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Carreira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Hillier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Back</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Natsev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>abs/1705.06950</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A spatio-temporal descriptor based on 3d-gradients</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kläser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Marszałek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">HMDB: a large video database for human motion recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kuehne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Jhuang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garrote</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Serre</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Space-time interest points</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lindeberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Learning hierarchical invariant spatio-temporal features for action recognition with independent subspace analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Online detection and classification of dynamic hand gestures with recurrent 3d convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Molchanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Jointly modeling embedding and translation to bridge video and language</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Rui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning spatio-temporal representation with pseudo-3d residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Qiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Action bank: A high-level representation of activity in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sadanand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Corso</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">A 3-dimensional sift descriptor and its application to action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Scovanner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ali</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ACM MM</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Temporal action localization in untrimmed videos via multi-stage cnns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Shou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-F</forename><surname>Chang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Two-stream convolutional networks for action recognition in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">UCF101: A dataset of 101 human action classes from videos in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Soomro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Zamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Shah</surname></persName>
		</author>
		<idno>CRCV-TR-12-01</idno>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Unsupervised learning of video representations using lstms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Mansimov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="843" to="852" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Human action recognition using factorized spatio-temporal convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-Y</forename><surname>Yeung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">E</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Convolutional learning of spatio-temporal features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename><surname>Taylor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bregler</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="140" to="153" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Learning spatiotemporal features with 3d convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Long-term Temporal Convolutions for Action Recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Varol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laptev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Action recognition with improved trajectories</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Temporal segment networks: Towards good practices for deep action recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Actions˜transforma-tions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">A discriminative cnn video representation for event detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1798" to="1807" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Beyond short snippets: Deep networks for video classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yue-Hei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hausknecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vijayanarasimhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Toderici</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="4694" to="4702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">A duality based approach for realtime tv-l 1 optical flow</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Pock</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Bischof</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Pattern Recognition</title>
		<imprint>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="214" to="223" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
