<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:18+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Dynamic Conditional Networks for Few-Shot Learning</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fang</forename><surname>Zhao</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Zhao</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">National University of Defense Technology</orgName>
								<address>
									<settlement>Hunan</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">−</forename><surname>756x] * †</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shuicheng</forename><surname>Yan</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">National University of Singapore</orgName>
								<address>
									<settlement>Singapore</settlement>
									<country key="SG">Singapore</country>
								</address>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Qihoo 360 AI Institute</orgName>
								<address>
									<settlement>Beijing</settlement>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiashi</forename><surname>Feng</surname></persName>
						</author>
						<title level="a" type="main">Dynamic Conditional Networks for Few-Shot Learning</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>1[0000−0001−6843−0064]</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>Conditional Model · Few-Shot Learning · Deep Learning · Dynamic Convolution · Filter Bank</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. This paper proposes a novel Dynamic Conditional Convolutional Network (DCCN) to handle conditional few-shot learning, i.e, only a few training samples are available for each condition. DCCN consists of dual subnets: DyConvNet contains a dynamic convolutional layer with a bank of basis filters; CondiNet predicts a set of adaptive weights from conditional inputs to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input. The filter bank is shared between all conditions thus only a low-dimension weight vector needs to be learned. This significantly facilitates the parameter learning across different conditions when training data are limited. We evaluate DCCN on four tasks which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. Extensive experiments demonstrate the superiority of the proposed model in the conditional few-shot learning setting.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>A conditional model is a significant machine learning framework which can be exploited in many tasks, such as multi-modal learning and conditional generative models. It usually contains two inputs. One is interest of task, and the other one is conditional input and provides additional information of specific situation. Recently deep conditional models have attracted much attention since deep neural networks have achieved unprecedented advances in many important fields, such as computer vision <ref type="bibr" target="#b13">[13,</ref><ref type="bibr" target="#b15">15]</ref>, natural language processing <ref type="bibr" target="#b37">[37,</ref><ref type="bibr" target="#b19">19]</ref> and speech recognition <ref type="bibr" target="#b26">[26,</ref><ref type="bibr" target="#b0">1]</ref>. However, they generally suffer performance decline in the challenging conditional few-shot learning scenario, where training samples for each condition are limited due to the high dimension of the condition space although the total number of training samples can be large.</p><p>Deep learning based methods typically require a huge amount of labelled data for training as well as specialized computational platform and optimization strategies to achieve satisfactory performance. Their performance usually drops severely for learning problems with small training sample size due to severe over-fitting issues. In contrast, humans, even children can grasp a new concept (e.g., a "giraffe") remarkably fast, "sample efficiently" and generalize to novel cases reasonably from just a short exposure to few examples (e.g., pictures in a book) <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b20">20]</ref>. This phenomenon motivates the research on the problem of fewshot learning, i.e., the task to learn a new concept on the fly, from a few or even a single annotated example for each category <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b36">36]</ref>.</p><p>Few-shot learning is of great significance both academically and industrially, since 1) models excelling at this task would help alleviate expensive and labourintensive data collection and labeling as they would not require massive labelled training data to achieve reasonable performance; 2) the target data in practice usually have a large number of different categories but very few examples per category. For instance, when operating in natural environments, robots are supposed to recognize many unfamiliar objects after seeing only few examples for each <ref type="bibr" target="#b17">[17]</ref>. The ability of generalizing in such scenarios would be beneficial to modeling the practical data distribution more effectively.</p><p>In this paper, we mainly focus on improving two kinds of models in the conditional few-shot learning scenario, i.e., the discriminative one and the generative one. The discriminative models often resort to hand-crafted features with huge human-engineering efforts and then adopt metric learning algorithms or datadriven deep learning solutions from ample labelled data. However, such datadriven methods are too computationally complex to meet practical applications. Moreover, massive labelled training data covering all underlying variations are usually expensive and unavailable. The generative models often leverage data generative models, e.g., Generative Adversarial Networks (GANs) <ref type="bibr" target="#b10">[10]</ref>, Conditional Generative Adversarial Networks (Conditional-GANs) <ref type="bibr" target="#b24">[24]</ref>, Boundary Equilibrium Generative Adversarial Networks (BE-GANs) <ref type="bibr" target="#b1">[2]</ref>, etc., for synthesizing auxiliary training data for data augmentation. However, among current generative methods, the quality of synthesized data is still far from being satisfactory to perform practical analysis tasks.</p><p>In order to address the challenging and realistic conditional few-shot learning problems, we explore a novel approach to learn a deep conditional model from a few labeled examples of each condition, which can generalize well to other cases of the same condition. The conditions could be based on category labels, on some part of data, or even on data from different modalities. Moreover, to enable on-the-fly computation with high efficiency, we embody this conditional few-shot learning problem into learning dual subnets jointly in an end-to-end way. One subnet is called DyConNet, which contains a Dynamic Convolutional layer with a bank of trainable basis filters. Given any Conditional input, the other subnet, called CondiNet, predicts a set of adaptive weights to linearly combine the basis filters. In this manner, a specific convolutional kernel can be dynamically obtained for each conditional input, as illustrated in <ref type="figure">Fig. 1</ref>  <ref type="figure">Fig. 1</ref>. Dynamic convolutional layer of DyConvNet. It has a filter bank consisting of several basis filters. A set of adaptive weights W = {w1, w2, · · · , wn} is predicted by the embedding of CondiNet from conditional inputs Y to perform the linear combination on the basis filters, which produces the convolution filters applied on feature maps X ing optimization, the filter bank is shared between all conditionals thus only a low-dimension weight vector needs to be leaned for each condition, which significantly compensates the limited information in few-shot setting and facilitates the sample-efficient parameter learning across conditions. We term this model as Dynamic Conditional Convolutional Network (DCCN). We evaluate DCCN on four distinct tasks, all of which can be formulated as conditional model learning, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation. The proposed DCCN outperforms other discriminative and generative conditional models for all the tasks.</p><p>Our contributions in this paper are summarized as follows. <ref type="formula" target="#formula_0">(1)</ref> We present a novel and effective deep architecture, which contains a Dynamic Convolutional subNet (DyConvNet) and a Conditional subNet (CondiNet) that jointly perform learning to learn in an end-to-end way. This deep architecture provides a unified framework for efficient conditional few-shot learning. (2) The dynamic convolution is achieved through linearly combining the basis filters of the filter bank in the DyConvNet with a set of adaptive weights predicted by the CondiNet from conditional inputs, which is different from existing conditional learning approaches that combine the two inputs through direct concatenation. (3) Our architecture is general and works well for multiple distinct conditional model learning tasks. The source codes as well as the trained models of our deep architecture will be made available to the community.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related Works</head><p>Our work is related to several others in the literature. However, we believe to be the first to look at methods that can learn the parameters of deep conditional models in the few-shot setting.</p><p>Since its inception, few-shot learning has been widely studied in the context of generative approaches. The real annotated data covering all variations are ex-pensive to achieve, even impossible, thus synthesizing realistic data is beneficial for more efficiently training deep models for few-shot learning, by augmenting the number of samples with desired variations and avoiding costly annotation work <ref type="bibr" target="#b40">[40,</ref><ref type="bibr" target="#b39">39]</ref>. Successful generation from limited labelled training samples usually requires carefully tuned inductive biases using additional available information due to the high dimensionality of the feature space <ref type="bibr" target="#b14">[14]</ref>. Such additional information can be accessed through various ways. For instance, 1) more samples of categories of interest can be obtained from huge amount of unlabelled data as in semi-supervised learning <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b5">6]</ref>; 2) the available labelled training data can be augmented using simple transformations, such as jittering, noise injection, etc., as commonly used in deep learning <ref type="bibr" target="#b7">[7,</ref><ref type="bibr" target="#b8">8,</ref><ref type="bibr" target="#b18">18]</ref>; 3) samples from other relevant categories can be utilized through transfer learning to assist parameter learning <ref type="bibr" target="#b21">[21]</ref>; 4) new virtual samples can be synthesized, either rendered explicitly with GAN-based techniques <ref type="bibr" target="#b10">[10,</ref><ref type="bibr" target="#b24">24,</ref><ref type="bibr" target="#b1">2]</ref> or created implicitly through compositional representations <ref type="bibr" target="#b25">[25,</ref><ref type="bibr" target="#b41">41]</ref>. Recently, Mehrotra et al. <ref type="bibr" target="#b23">[23]</ref> argued that having a learnable and more expressive similarity objective is an essential missing component, and proposed a network design inspired by deep residual networks that allows the efficient computation of this more expressive pairwise similarity objective. These approaches can significantly advance the performance of few-shot learning if a generative model that accounts for the underlying data distribution is known. However, such a model is usually unavilable and the generation of additional real or synthesized samples often requires substantial efforts.</p><p>A different trend of approaches to few-shot learning is to learn a discriminative embedding space, which is typically done with a siamese network <ref type="bibr" target="#b4">[5]</ref>. Given an exemplar of a novel category, recognition is performed in the embedding space by a simple rule such as nearest-neighbor. Training is usually performed by classifying pairs according to distance <ref type="bibr" target="#b9">[9]</ref>, or by enforcing a distance ranking with a triplet loss <ref type="bibr" target="#b27">[27]</ref>. A variant is to combine embeddings using the outer-product, which yields a bilinear classification rule <ref type="bibr" target="#b22">[22]</ref>. Built on the advances made by the siamese architecture, Vinyals et al. <ref type="bibr" target="#b33">[33]</ref> employed ideas from metric learning based on deep neural features and from recent advances that augment neural networks with external memories. They proposed a framework which learns a network that maps a small labelled support set and an unlabelled example to its label, obviating the need for fine-tuning to adapt to new class types. Ravi and Larochelle <ref type="bibr" target="#b31">[31]</ref> proposed an Long Short Term Memory (LSTM) based metalearner model to learn the exact optimization algorithm used to train another learner neural network classifier in the few-shot regime. The parametrization of their model allows it to learn appropriate parameter updates specifically for the scenario where a set amount of updates will be made, while also learning a general initialization of the learner (classifier) network that allows for quick convergence of training. However, these methods did not consider conditional model learning and are usually computational expensive for effectively and efficiently solving the few-shot learning problems.</p><p>Compared with previous attempts, our proposed method is conceptually simple yet powerful for conditional few-shot learning, which allows learning all pa-rameters from scratch, generalizing across different tasks, and can be seen as a network that effectively "learns to learn". Detailed comparisons with gernerative and discriminative counterparts on various tasks are provided in Sec. 4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Dynamic Conditional Parameter Prediction</head><p>Despite the recent success of deep neural networks, it remains challenging to accommodate such models to an extremely large number of categories with limited samples for each, as in the scenario of few-shot learning. Many works to date have mainly focused on learning one-to-one mappings from input to output. However, many interesting problems are more naturally considered as a probabilistic one-to-many mapping. For instance, in the case of image labelling, there may be many different tags that could appropriately be applied to a given image, and different data annotators may use different terms to describe the same image. One way to help address the issue is to leverage additional information from other modalities and to use a conditional model, taking as input small samples and conditional variables, and the one-to-many mapping is instantiated as a conditional predictive distribution.</p><p>Since we consider few-shot learning in a conditional modeling task, we start with formulating the standard conditional model learning. It aims to find the parameter W that minimizes the loss L of a predictor function h(X|Y ; W ), averaged over N samples x i and corresponding conditions y i :</p><formula xml:id="formula_0">min W 1 N N Σ i=1 L(h(xi|yi; W )),<label>(1)</label></formula><p>where the model can be a discriminative one to learn a classifier or a generative one to learn a conditional distribution over X and Y . In the case when the dimension of the condition space is too high, the training samples are still scarce for each conditional state even though there are massive training data in total, and the goal is to learn W from small samples with the condition y of interest, called conditional few-shot learning. The main challenge in conditional few-shot learning is to find a mechanism to incorporate domainspecific information into the network. Another challenge, which is of practical importance in applications of few-shot learning, is to enhance efficiency of optimization for Eqn. <ref type="bibr" target="#b0">(1)</ref>.</p><p>We propose to address both challenges by learning the parameter W of the predictor from small samples with the conditions y using a meta-learning process, i.e., a non-iterative feed-forward function ϕ (meta learner) that maps (y; W ′ ) to an optimal W of the predictor (base learner). We parameterize this function using a neural network model and we call it a CondiNet. The CondiNet output depends on the condition y which is a representative of the condition of interest, and contains parameter W ′ of its own. We train the CondiNet as follows such that it can produce suitable W for different tasks. We optimize the CondiNet using the following objective function. The feed-forward CondiNet evaluation is much faster than solving the optimization problem of Eqn. <ref type="bibr" target="#b0">(1)</ref>.</p><formula xml:id="formula_1">min ϕ 1 N N Σ i=1 L(h(xi; ϕ(yi; W ′ ))).<label>(2)</label></formula><p>Importantly, the parameters of the original W of Eqn.</p><p>(1) now adapt dynamically to each conditional input y. Note that the training scheme is reminiscent of that of siamese networks <ref type="bibr" target="#b4">[5]</ref> which also employ dual subnets. However, siamese networks adopt the same network architecture with shared weights, and compute the inner-product of their outputs to produce a similarity score:</p><formula xml:id="formula_2">min W 1 N N Σ i=1 L( h(xi; W ), h(yi; W ) ).<label>(3)</label></formula><p>There are two key differences with our model: 1) we treat h(·) and ϕ(·) asymmetrically, which results in a different objective function; 2) more importantly, the output of ϕ(y; W ′ ) is used to parametrize convolutional layers that determine the intermediate representations in the network h(·) dynamically. This is significantly different from siamese networks <ref type="bibr" target="#b4">[5]</ref> and bilinear networks <ref type="bibr" target="#b22">[22]</ref>, as well as traditional conditional networks <ref type="bibr" target="#b24">[24]</ref> based on the conditional probability p(X|Y ; W ). Now, we explain the implementation of the CondiNet ϕ(·) and the main predictor h(·) formally. Given an input tensor x ∈ R p×q×c , weights W ∈ R k×k×c×d (where k is the kernel size), and biases b ∈ R d , the output f ∈ R p ′ ×q ′ ×d of a convolutional layer is given by</p><formula xml:id="formula_3">f = W * x + b,<label>(4)</label></formula><p>where * denotes convolution operation, and the biases b are applied to each of the d channels.</p><p>We propose to formulate the weights and biases as functions of y, W (y) and b(y), to represent the dynamic conditional parameters given the conditional input y ∈ R:</p><formula xml:id="formula_4">f = W (y) * x + b(y).<label>(5)</label></formula><p>While Eqn. (5) seems to be a straightforward drop-in replacement for convolutional layers, careful analysis reveals that it scales extremely poorly. The main reason is the typically high dimensionality of the output space of the CondiNet ϕ(·) : R → R k×k×c×d . Since k is usually small and so is k 2 , for a comparable number of input and output channels in a convolutional layer (c ≃ d), the output space of the CondiNet grows quadratically with the number of channels. Overfitting issues, memory and time costs make learning such a regressor difficult in few-shot learning settings.</p><p>In order to address the above-mentioned issue when learning a conditional model in few shot, we herein propose a simple yet effective method to reduce the output space by considering a decomposition as below (we drop the bias term b for simplification), The filter bank is shared between all conditional states and only the weights of basis filters are specific for each conditional state. Both w and w ′ contain trainable parameters, but they are modest in size compared to the case discussed in Eqn. <ref type="bibr" target="#b4">(5)</ref>. Importantly, the CondiNet ϕ(·) now only needs to predict a set of adaptive weights, so its output space grows linearly with the number of basis filters in the filter bank (i.e., ϕ(·) : R → R n ). Since the resulted convolutional kernel (w ′ i · w i ) in Eqn. (6) is dynamically changed, depending on the prediction of the CondiNet ϕ(·) and the filter bank of the main predictor h(·), we construct h(·) as another subnet -DyConvNet. The dual subnets operate cooperatively for jointly learning parameters of a deep conditional model with conventional chain rules and Back Propagation (BP) algorithms in few shot.</p><formula xml:id="formula_5">f = n Σ i=1 (w ′ i(y) · wi) * x,<label>(6)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments</head><p>We evaluate our model on four conditional few-shot learning problems to verify the effectiveness of dynamically combining the basis filters, including specific object counting, multi-modal image classification, phrase grounding and identity based face generation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Specific Object Counting</head><p>The specific object counting task is from Amazon Bin Image Dataset (ABID) Challenge, which is to predict the quantity of the object in a bin, given an image and the target category. When the maximal quantity of an object in a bin is set to a constant (here is 5), we formulate this task as a conditional classification model by viewing the object category as a conditional input. As shown in <ref type="figure" target="#fig_0">Fig. 2  (b)</ref>, one network is used to extract image features, and the other network is used to embed the object ID. Finally, our dynamic conditional layer is used to combine the last layers of the two networks to output the quantity of the object. Here we use a plain conditional network ( <ref type="figure" target="#fig_0">Fig. 2 (a)</ref>) as a baseline, i.e., directly concatenating the last layers and substituting our dynamic conditional layer with a fully-connected layer.</p><p>Dataset and evaluation metric. We evaluate our model on two subtasks, i.e., object quantity verification and identification. The former is to verify whether the given object quantity is correct for a bin image. The latter is to directly count the objects in a bin image. The dataset contains 535,234 bin images and is divided into two subsets, 481,711 images for training and the remaining images for test. For the object quantity verification, we test on triplets of image, object ID and quantity. The accuracy is used to measure the performance of both the tasks.</p><p>Architecture and training. Similar with the model architecture settings provided by the dataset website, we use the ResNet-34 network to extract image features. The embedding dimension of the object ID in the plain conditional network is 512. The dimension of the dynamic conditional layer is set to 4, 8 and 16 respectively to investigate effects of using different numbers of basis filters. All images are resized into 224x224 for convenient training and comparison. Because it is actually a classification task, the Softmax loss is adopted to optimize the entire network. We train for 30 epochs. The initial learning rate is 0.1 and it is dropped by a factor of 10 every 10 epochs.</p><p>Results and analysis. <ref type="table" target="#tab_1">Table 1</ref> reports accuracies of our method under various dimensions of the dynamic convolutional layer and the plain conditional network. One can see that our network using 8-D dynamic layer achieves the best accuracy. The plain conditional network performs not well because the set of object IDs is too large and each ID is only associated with few training examples (one example for most IDs). It is hard to learn a conditional network for each ID when the embedding dimension is too high, and the network coditional output would not be discriminative enough if the embedding dimension is low. In contrast, the proposed DCCN makes different IDs share a filter bank. Only a lowdimension vector is needed to learn to combine the set of filters as a convolutional kernel. Through applying this kernel on the top layer of the feature network, the spatially local correlation of image and object ID can be learned to make the conditional output more discriminative for different IDs. Note that as the dimension of dynamic layer continuously increases, such as 16, the performance decreases instead due to overfitting. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Multi-modal Image Classification</head><p>Multi-modal classification can be formulated as a typical conditional model consisting of two networks. <ref type="figure" target="#fig_1">Fig. 3 (a)</ref> shows a general framework of multi-modal classification. The inputs of the two network are an image and text describing the image, respectively. The texts are usually transformed into bag-of-words vectors at first. The outputs of the networks are then combined into a feature vector through a fully connected layer. We use this plain conditional network as a baseline. <ref type="figure" target="#fig_1">Fig. 3 (b)</ref> illustrates the proposed dynamic convolution used for multi-modal classification, where we substitute the fully connected layer with the dynamic conditional layer. Dataset and evaluation metric. We evaluate our model on the MIRFlickr-25K dataset <ref type="bibr" target="#b16">[16]</ref> which consists of 25,000 images downloaded from the social website Flickr. Each image associates with some of 20,000 tags. 38 class labels including various scenes and objects, such as sunset, car and bird, are used to annotate these images and an image may belong to multiple class labels. We randomly sample 20,000 images for training and the rest for testing. The multilabel classification performance is measured by the Intersection over Union (IoU) in the multi-label setting, which is defined as the number of correctly predicted labels divided by the union of predicted and ground-truth labels.</p><p>Architecture and training. The base network for both the baseline and our method is the ResNet-34 network. The embedding dimension of the text in the baseline is 512. We set the dimensions of the dynamic conditional layer to 32, 64 and 128 respectively. To deal with the multiple labels in one image, here we adopt the cross entropy loss to learn the conditional networks. The training epoch is 90. Beginning with 0.1, the learning rate is dropped by a factor of 10 every 30 epochs.</p><p>Results and analysis. <ref type="table" target="#tab_2">Table 2</ref> reports the results of different methods on the MIR-Flickr25K dataset. It can be observed that when the dimension of the dynamic layer is 64, our dynamic conditional network outperforms the baseline under the condition of various numbers of tags. We argue that although the total training images are sufficient, there are only a few ones for each tag. That is to say, it is still a few-shot learning for each tag. Thus the dynamic layer which  reduces the parameters of the conditional network is able to address the problem of overfitting effectively for this kind of few-shot conditional learning, and the filter bank shared by tags can be learned easily by using all training images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Phrase Grounding</head><p>The task of phrase grounding is to localize objects or scenes described by text phrases in images <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b29">29]</ref>. This task can also be modeled as a conditional model. A typical framework of phrase grounding is illustrated in <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>. One convolutional neural network is used to produce a spatial feature map of an input image, and Long Short-Term Memory network (LSTM) <ref type="bibr" target="#b11">[11]</ref> is used to embed an input phrase into a vector with fixed length. Then the features of a set of region proposals (i.e.,Edge Boxes <ref type="bibr" target="#b43">[43]</ref>) are extracted by applying the ROI pooling on the spatial feature map. Finally, the proposal features are concatenated with the phrase vector respectively to compute correlation scores by two fully connected layers. <ref type="figure" target="#fig_2">Fig. 4 (b)</ref> shows the proposed dynamic convolution based phrase grounding. We firstly use the dynamic conditional layer to combine the phrase vector with the image feature map to obtain a correlative feature map. Then the ROI pooling is applied to obtain the correlative feature map for each region proposal, which is fed into the average pooling and a fully connected layer sequentially to compute the correlation score. Dataset and evaluation metric. The Flickr30k Entities dataset <ref type="bibr" target="#b30">[30]</ref> is used to evaluate our model for phrase grounding, which is an extension of the Flickr30K dataset <ref type="bibr" target="#b38">[38]</ref>. It consists of 31,000 images and their captions which are associated with 276,000 manually annotated bounding boxes. We use 2,000 images for testing and the remaining images for training. Following <ref type="bibr" target="#b30">[30]</ref>, if a single phrase (e.g., rainbow flags) has multiple ground truth bounding boxes, the union of the boxes is used to represent the phrase. If the IoU of an image region predicted for a phrase and the ground truth bounding box is larger than 0.5, the predicted region is deemed correct for the phrase.</p><p>Architecture and training. The same with <ref type="bibr" target="#b32">[32]</ref>, we adopt the VGG-16 network to extract the image feature map, which is pretrained on the PascalVOC dataset for object detection and then is fixed when training the entire conditional model. Both the numbers of the hidden and input units of LSTM are 512. The dimension of the dynamic layer is set to 8, 16 and 32, respectively. 100 region proposals generated by Edge Boxes are used as candidate bounding boxes. We employ the Softmax loss to learn the model to maximize the correlation score of the input phrase with the correct region proposal. We train for 90 epochs. The initial learning rate is 0.01 and every 30 epochs it is dropped by a factor of 10.</p><p>Results and analysis. <ref type="table" target="#tab_3">Table 3</ref> reports the accuracy of phrase grounding for different methods under the condition of IoU &gt; 0.5 on the Flickr30k Entities dataset. One can see that our dynamic conditional network achieves the best accuracy compared with the state-of-the-art methods when the dimension of the dynamic layer is 16. NonlinearSP <ref type="bibr" target="#b34">[34]</ref> and GroundR <ref type="bibr" target="#b32">[32]</ref> have similar frameworks with <ref type="figure" target="#fig_2">Fig. 4 (a)</ref>, i.e., using fully connected layers to combine the features of the image region and phrase. SMPL <ref type="bibr" target="#b35">[35]</ref> utilizes a bipartite matching to compute their correlation score. However, all of these methods do not consider that only a few training images are available for each phrase although there are a large number of training images in this dataset. In this sense, this task can be viewed as a conditional few-shot learning problem which can be solved better by our dynamic conditional layer. <ref type="table">Table 4</ref> reports the accuracy of phrase grounding for different types of phrases. Our method has better performance than other methods for most phrase types.</p><p>Although there are some phrase grounding methods which have better performance than our method, e.g., RtP <ref type="bibr" target="#b28">[28]</ref> and SPC+PPC <ref type="bibr" target="#b29">[29]</ref>, we argue that these methods employ additional cues to improve correlation learning of image region and phrase, such as region-phrase compatibility, candidate position and size. Actually, our model is mostly like a proof-of-concept and applied on the task of phrase grounding to verify its effectiveness on the conditional few-shot learning. It is orthogonal to many technical improvements found in the phrase grounding literature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Identity Based Face Generation</head><p>The proposed DCCN can also be used to improve conditional generative models. Here we test DCCN on the task of identity based face generation. <ref type="figure" target="#fig_3">Fig. 5 (a)</ref> shows a general framework based on conditional generative adversarial nets (GAN) <ref type="bibr" target="#b24">[24]</ref>, which consists of a generative model G and a discriminative model D. In G, the prior input noise and the face ID are combined through a fully connected layer to obtain a joint hidden representation. Then the representation is fed into a deconvolutional neural network to generate a face image of the input ID. In D, a convolutional neural network is employed to extract the features of the faces generated by G and the real faces. Then the feature and the embedding vector of the face ID are concatenated and fed into a classifier, which judges whether the face is real or not for this ID. The proposed dynamic convolution based conditional GAN is illustrated in <ref type="figure" target="#fig_3">Fig. 5 (b)</ref>. We use the dynamic conditional layer to integrate the face ID with the noise in D and the image feature in G, respectively. Dataset and evaluation metric. We evaluate our model on the MS-Celeb-1M dataset <ref type="bibr" target="#b12">[12]</ref> which contains about 10M face images for 100K subjects. For the training set, we randomly sample 100 subjects and 10 face images for each subject to simulate the conditional few-shot setting. In testing, given a generated face image, a pretrained face recognition model is used to predict which one of the 100 subjects it belongs to. 50 images are generated for each subject. The precision-coverage (PC) curve and the cumulative match characteristic (CMC) curve are used to measure the performance of face identification.</p><p>Architecture and training. We use five-layer fully convolutional and deconvolutional network in the generative and discriminative models, respectively. To learn the conditional GAN, we optimize the generative model G and the discriminative model D alternatively. D is trained to minimize the classification loss under the condition of the input ID, and G is trained to maximize the loss under the same condition, i.e., G trying to generate face images which can confuse G.</p><p>Results and analysis. <ref type="figure" target="#fig_4">Fig. 6</ref> illustrates the PR and CMC curves of the face identification for generated face images. <ref type="table" target="#tab_4">Table 5</ref> reports the precision when Coverage=0.99 and 0.95 and the accuracies of rank 1 and 5. It can be observed that dynamic conditional GAN achieves better performance than the plain conditional GAN in terms of all the metrics. The dynamic layer can effectively incorporate the information of conditional input through sharing filter bank across conditions when limited training data are available for each condition. Some examples of generated faces are shown in <ref type="figure">Fig. 7</ref>. Each row of faces corresponds to one subject. The faces generated by the dynamic conditional GAN are obviously more similar with the real face of the subject.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion</head><p>This paper addressed the problem of conditional few-shot learning. A Dynamic Conditional Convolutional Network is presented to incorporate conditional input in a deep model when only a few training samples are available for each condition. In this model, a set of adaptive weights from conditional inputs is predicted to linearly combine the basis filters of a filter bank shared by all conditions. Then a dynamic convolutional kernel can be obtained according to different conditional inputs. Finally the dynamic kernel is applied on the top layer of the other network to provide conditional output. Qualitative and quantitative experiments on four tasks demonstrate that the proposed model achieves better performance compared with other conditional learning models.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 2 .</head><label>2</label><figDesc>Fig. 2. Flow charts of object counting based on specific object ID</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Fig. 3 .</head><label>3</label><figDesc>Fig. 3. Flow charts of multi-modal image classification</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 4 .</head><label>4</label><figDesc>Fig. 4. Flow charts of phrase grounding</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 5 .</head><label>5</label><figDesc>Fig. 5. Flow charts of identity based face generation</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 6 .</head><label>6</label><figDesc>Fig. 6. Precision-Coverage and cumulative match characteristic curves of face identification for identity based face generation.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>. Dur-</figDesc><table>Embedding 
Y 

… 
+ 
+ 
+ 
w 1 * 
w 2 * 
w n * 

Basis filters 
X 

W 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Accuracies of object quantity verification and identification on the Amazon Bin Image dataset</figDesc><table>Methods 
Dynamic 
Plain 
4-D 
8-D 
16-D 
Identification 
76.60% 
76.81% 
75.66% 
74.48% 
Verification 
85.39% 
85.48% 
84.81% 
84.87% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="true"><head>Table 2 .</head><label>2</label><figDesc>IoU of multi-modal multi-label classification on the MIR-Flickr25K dataset</figDesc><table>Methods 
Dynamic 
Plain 
32-D 
64-D 
128-D 

Tags 

5k 
0.6517 
0.6553 
0.6520 
0.6489 
10k 
0.6513 
0.6606 
0.6560 
0.6516 
20k 
0.6549 
0.6577 
0.6543 
0.6490 

LSTM 
"yellow shorts" 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 .</head><label>3</label><figDesc>Accuracy</figDesc><table>(IoU &gt; 0.5) of phrase grounding on the Flickr30k Entities dataset 

Methods 
Dynamic 
SMPL 
NonlinearSP GroundeR 
8-D 
16-D 
32-D 
Accuracy 
50.18 
50.65 
50.52 
42.08 
43.89 
47.81 

Table 4. Accuracy (IoU &gt; 0.5) of phrase grounding for various phrase types on the 
Flickr30k Entities dataset 

Methods People Clothing Body parts Animals Vehicles Instruments Scene Other 
SMPL 
57.89 
34.61 
15.87 
55.98 
52.25 
23.46 
34.22 26.23 
GroundeR 61.00 
38.12 
10.33 
62.55 
68.75 
36.42 
58.18 29.08 
Dynamic 67.37 38.12 
18.22 
69.93 
56.04 
37.57 
54.05 32.59 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 5 .</head><label>5</label><figDesc>Accuracy and Precision@Coverage of face identification for generated facesFig. 7. Examples of identity based face generation</figDesc><table>Methods 
Accuracy 
Pricison@Coverage 
Rank 1 
Rank 5 
P@C = 0.99 P@C = 0.95 
Plain 
0.457 
0.550 
0.05 
0.18 
Dynamic 
0.688 
0.748 
0.21 
0.71 

Conditional GAN 
Dynamic Convolution Based 
Conditional GAN 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgments Jian Zhao was partially supported by China Scholarship Council (CSC) grant 201503170248. Jiashi Feng was partially supported by NUS IDS R-263-000-C67-646, ECRA R-263-000-C87-133 and MOE Tier-II R-263-000-D17-112.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Deep speech 2: End-toend speech recognition in english and mandarin</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Amodei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ananthanarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Anubhai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Battenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Case</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Casper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="173" to="182" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">Began: Boundary equilibrium generative adversarial networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Learning feedforward one-shot learners</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bertinetto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">F</forename><surname>Henriques</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Valmadre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Torr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="523" to="531" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">How children learn the meanings of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Bloom</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2000" />
			<publisher>The MIT Press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Signature verification using a&quot; siamese&quot; time delay neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bromley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Guyon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Säckinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Shah</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="1994" />
			<biblScope unit="page" from="737" to="744" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semi-supervised learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Chapelle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Scholkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zien</surname></persName>
		</author>
		<editor>chapelle, o. et al.</editor>
		<imprint>
			<date type="published" when="2006" />
		</imprint>
	</monogr>
	<note>book reviews</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title/>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="542" to="542" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Discriminative unsupervised feature learning with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Springenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="766" to="774" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Doudou</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1403.2802</idno>
		<title level="m">Learning deep face representation</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Farrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">MS-Celeb-1M: A dataset and benchmark for large scale face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Guo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning a kernel function for classification with small training samples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hertz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">B</forename><surname>Hillel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Weinshall</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on Machine learning</title>
		<meeting>the international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="401" to="408" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">The mir flickr retrieval evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Huiskes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Lew</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia Information Retrieval</title>
		<meeting>the ACM International Conference on Multimedia Information Retrieval</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Learning to recognize novel objects in one shot through human-robot interactions in natural language dialogues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">A</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Zillich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Scheutz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">One-shot learning by inverting a compositional causal process</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2526" to="2534" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Transfer learning by borrowing examples for multiclass object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="118" to="126" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mehrotra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dukkipati</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08033</idno>
		<title level="m">Generative adversarial residual pairwise networks for one shot learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Osindero</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1411.1784</idno>
		<title level="m">Conditional generative adversarial nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Ontological supervision for fine grained classification of street view storefronts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Stumpe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Shet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arnoud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yatziv</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1693" to="1702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">Wavenet: A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">M</forename><surname>Parkhi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<title level="m">Deep face recognition</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer imageto-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">123</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="74" to="93" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Phrase localization and visual relationship detection with comprehensive imagelanguage cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mallya</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Flickr30k entities: Collecting region-to-phrase correspondences for richer image-to-sentence models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">A</forename><surname>Plummer1</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Cervantes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Caicedo</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Optimization as a model for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ravi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Larochelle</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Grounding of textual phrases in images by reconstruction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3630" to="3638" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">earning deep structure preserving image-text embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lazebnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Structured matching for phrase localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Azab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kojima</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mihalcea</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning to learn: Model regression networks for easy small sample learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="616" to="634" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Norouzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Macherey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Krikun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Macherey</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.08144</idno>
		<title level="m">Google&apos;s neural machine translation system: Bridging the gap between human and machine translation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">From image descriptions to visual denotations: new similarity metrics for semantic inference over event descriptions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Young</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hodosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hockenmaier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Transactions of the Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Age progression/regression by conditional adversarial autoencoder</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Qi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Dual-agent gans for photorealistic and identity preserving profile face synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Jayashree</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="66" to="76" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Do we need more training data?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Fowlkes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">119</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="76" to="92" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Semi-supervised learning literature survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
