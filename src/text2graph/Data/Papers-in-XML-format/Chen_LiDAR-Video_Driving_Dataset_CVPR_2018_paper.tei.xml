<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:57+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">LiDAR-Video Driving Dataset: Learning Driving Policies Effectively</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yiping</forename><surname>Chen</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jingkang</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao</orgName>
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonathan</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="institution">University of Waterloo</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cewu</forename><forename type="middle">Lu</forename><surname>#2</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao</orgName>
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhipeng</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Han</forename><surname>Xue</surname></persName>
							<affiliation key="aff1">
								<orgName type="department">Shanghai Jiao</orgName>
								<orgName type="institution">Tong University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cheng</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Fujian Key Laboratory of Sensing and Computing for Smart Cities</orgName>
								<orgName type="institution">Xiamen University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">LiDAR-Video Driving Dataset: Learning Driving Policies Effectively</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Learning autonomous-driving policies is one of the most challenging but promising tasks for computer vision. Most researchers believe that future research and applications should combine cameras, video recorders and laser scanners to obtain comprehensive semantic understanding of real traffic. However, current approaches only learn from large-scale videos, due to the lack of benchmarks that consist of precise laser-scanner data. In this paper, we are the first to propose a LiDAR-Video dataset, which provides large-scale high-quality point clouds scanned by a Velodyne laser, videos recorded by a dashboard camera and standard drivers' behaviors. Extensive experiments demonstrate that extra depth information help networks to determine driving policies indeed.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Driving policy learning is a core problem in autonomous driving research. Computer vision is expected to play an important role in this challenging task, since driving planning and perception together run as a closed loop. Therefore, some computer vision researchers <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b22">21,</ref><ref type="bibr" target="#b28">27]</ref> attempt to model it as a perception-action model, which is an end-to-end system that maps from pixels to actuation. It opens a new direction in the autonomous driving field.</p><p>However, current research and dataset neglect an important cue, namely, depth information. On the one hand, biological experiments <ref type="bibr" target="#b4">[4,</ref><ref type="bibr" target="#b20">19]</ref> show that monocular people can not drive nicely. For instance, monocular drivers in experiments did worse in parking and lane changing tasks for the lack of stereoscopic depth perception. It verifies depth * indicates equal contributions. # the corresponding authors are Jonathan Li and Cewu Lu. E-mails: junli@uwaterloo.ca and lucewu@sjtu.edu.cn. Cewu Lu is also member of AI research institute at SJTU. information is necessary, though drivers have perfect recognition ability. Most of the people believe that depth information should be a necessary cue in real-world auto-driving due to the consideration of safety. On the other hand, many high-quality depth sensors would be cheap and widely affordable. For example, the cost of Velodyne comes to hundreds of dollars, that is, it will be ready to be equipped in most autonomous cars.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>LiVi-Set</head><p>In consequence, computer vision researchers should pay more attention to perception-action model with depth. Whereas, we found it still misses out both research roadmaps and datasets. Thus, this paper aims to fundamentally study this problem. We offer a large-scale dataset that includes both driving videos with depth and corresponding driving behaviors. Our dataset is largely different from previous ones for vision-based auto-driving research. On the one hand, the depth data sampled by a LiDAR camera is provided, which misses in <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b28">27]</ref>. On the other hand, some datasets like KITTI <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b13">12]</ref> provide depth information, however, driving behavior is not included, which makes them fail to be a benchmark for policy learning. In short, the proposed dataset is the first driving policy learning dataset that includes depth information. Our dataset involves many features: (1) large-scale: our dataset consists of more than 10k frames of real street scenes and the amount of data exceeds 1TB in total. (2) diversity: we record continuous but varied scenes in real traffic, such as seaside roads, school areas and even mountain roads, which include a number of crossroads, pedestrians and traffic signs. (3) high-quality: point clouds, videos and drivers' behaviors in our dataset are all acquired by high-resolution sensors, which provides distinct recovery of real driving conditions.</p><p>Apart from the dataset, this paper attempts to throughly study how important depth information is for auto-driving and fully discuss what we can achieve if current techniques are used. First, we produce an analysis that why depth is necessary for autonomous vehicles. Second, we answer the question that how to leverage current techniques, if depth is given. Finally, we draw a conclusion that depth information would benefit learning driving policies and it has a large room to improve techniques in terms of how to use depth. It again verifies that a qualified dataset is crucial for advancing this topic.</p><p>In conclusion, the key contributions of our work in this paper are mainly two aspects: First, we propose a dataset which is the first policy learning benchmark composed of driving videos, LiDAR data and corresponding driving behaviors. Second, we conduct complete analysis on how important depth information is, how to leverage depth information and what we can achieve by utilizing current techniques.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>The ultimate goal in autonomous vehicle navigation is to learn driving policies. In this section, we investigate driving policy learning methods and existing driving datasets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Driving Policy Learning</head><p>Because of the complexity of real street scenes, deep learning techniques such as neural network are expected the most promising methods to solve this problem. Pomerleau et al. <ref type="bibr" target="#b22">[21]</ref> was the pioneer to use neural networks for lane following and obstacles avoiding. There are now two mainstream ways for this promising task.</p><p>End-to-end learning: This line of works employed end-toend systems mapping pixels directly to policies. <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b22">21]</ref> demonstrated that autonomous vehicles are capable to learn driving policies nicely in simple scenarios, such as highway. NVIDIA <ref type="bibr" target="#b6">[5]</ref> group did excellent attempts to map directly from images by utilizing multi-layer convolution neural network and successfully self-drive in real roads. Recently, <ref type="bibr" target="#b28">[27]</ref> broadened video scenes and illustrated that it is feasible for vehicles to drive in multiple complex situations.</p><p>Learning affordable rules: Rather than directly obtain driving policies, these works learned some affordable information in advance which is helpful for decision making. <ref type="bibr" target="#b8">[7]</ref> proposed to learn some pre-defined low-level measures such as depth information. Whereas, more works <ref type="bibr" target="#b0">[1,</ref><ref type="bibr" target="#b3">3,</ref><ref type="bibr" target="#b30">29]</ref> used neural networks to solve relevant helpful problems such as semantic segmentation based on monocular images. <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b7">6,</ref><ref type="bibr" target="#b9">8,</ref><ref type="bibr" target="#b19">18]</ref> attempt to perform 3D object detection or segmentation leveraging LiDAR information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Existing Driving Datasets</head><p>Large-scale datasets have contributed greatly to the development of machine learning and computer vision. As for the autonomous driving area, research relies much on some benchmarks <ref type="bibr" target="#b10">[9,</ref><ref type="bibr" target="#b13">12,</ref><ref type="bibr" target="#b18">17,</ref><ref type="bibr" target="#b25">24,</ref><ref type="bibr" target="#b28">27]</ref>. These datasets have different features and hierarchies. We conducted a comprehensive survey on existing driving datasets in the view of policy learning challenge.</p><p>Comma.ai <ref type="bibr" target="#b25">[24]</ref> proposed their novel architecture for policy learning with their dataset published, which contains around 7.25-hour highway driving data divided into 11 videos. The released video frames are 160 × 320 pixels in the middle of the captured screen. Besides, the vehicle is equipped with several sensors that were measured with different frequencies and interpolated to 100Hz. Example data coming from sensors are the car speed, steering angle, GPS, gyroscope, IMU, etc. However, this dataset only concentrates on highway driving scenarios, which is not suitable for generic driving policy learning. In addition, it only consists of 2D vision information, that is, only images are used for making decisions.</p><p>KITTI <ref type="bibr" target="#b12">[11,</ref><ref type="bibr" target="#b13">12]</ref>  transforming Time Registration <ref type="figure">Figure 3</ref>. The pipeline of data preprocessing when constructing dataset. Multiple perception are equipped for acquiring high-resolution data. Videos, point clouds and driving behaviors are preprocessed jointly in figure. Finally, we register the corresponding time for three types of data and obtain our benchmark.</p><p>mans, and more suburb scenes, which results in lack of diversity. Moreover, the vehicles do not fix multiple sensors, so there is no standard drivers' behaviors. On the whole, this benchmark is not designed for learning driving policies, but for other affordable tasks.</p><p>Cityscapes Cityscapes <ref type="bibr" target="#b10">[9]</ref> is a large-scale, diverse set of stereo video sequences recorded in streets from 50 different cities. It mainly provides images and a small number of videos. In particular, 5000 of these images have highquality pixel-level annotations and 20000 additional images have coarse annotations to enable methods that leverage large volumes of weakly-labeled data. The data set is designed to capture the high variability of outdoor street scenes and was acquired from a moving vehicle during several months, covering spring, summer, and fall in 50 cities, primarily in Germany but also in neighboring countries. Although this benchmark did well in the diversity of scenarios, the shortage of 3D perception such as LIDAR and driving status data makes it not so appropriate to learn driving policies.</p><p>Oxford The data was collected by the Oxford RobotCar platform <ref type="bibr" target="#b25">[24]</ref>, an autonomous Nissan LEAF. It includes over 1000km of recorded driving with almost 20 million images collected from 6 mounted cameras, along with LI-DAR, GPS and INS ground truth. In addition, it was collected in all weather conditions, including heavy rain, night, direct sunlight and snow. Road and building works over the period of a year significantly changed sections of the route from the beginning to the end of data collection. Same as KITTI and Cityscapes, it omits drivers' behaviors, which is of great significance for the decision prediction.</p><p>BDDV Berkley DeepDrive Video dataset <ref type="bibr" target="#b28">[27]</ref> (unpublished completely) is a benchmark that is intended for driving predictions, which provides more than 10k-hour dash-cam videos in different periods of multiple cities with varied weather conditions. From the paper, it is at least two orders larger than other public datasets for vision-based autonomous driving. It also contains labels including steering angles and vehicle speeds like Comma.ai. Due to focus on end-to-end generic driving model training, it neglects specific car annotations. Unfortunately, it only concerns 2D vision. In another word, it misses 3D stereoscopic depth perception information such as point clouds or meshes, which is an essential cue for future vehicle driving.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Dataset</head><p>Our dataset is intended for driving policy learning and largely different from previous ones for its novel hierarchy and excellent properties. In this section, we firstly introduce our collection platform system in Section 3.1. Then the pipeline of preprocessing LiDAR data is given in Section 3.2. Finally, in Section 3.3, we compare our dataset with existing benchmark and display features of LiVi-Set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Platform and Data Collection</head><p>As is shown in <ref type="figure" target="#fig_1">Figure 2</ref>, the dataset was acquired by our collection system in a multi-functional road information acquisition vehicle. The vehicle we used is a Buick GL8 loaded with multiple perception scanners and sensors. We collected three types of signals, namely, point clouds, videos and driving behaviors.</p><p>Point Cloud We equipped the vehicle with a pair of Velodyne scanners, including one HDL-32E and one VLP-16 laser scanner. In our experiment, we mainly exploited HDL-32E to collect point cloud data. The HDL-32E is always used for high-precision and wide-range point clouds acquisition, of which frequency is 10 Hz with 32 laser beams and depth range is from 1m to 70m with a resolution of 2cm. The range of scanning angle is from +10.67 to −30.67 degrees view in vertical and 360 degrees view in horizontal.</p><formula xml:id="formula_0">Datasets Video/Image LiDAR Behaviors KITTI × Cityscape × × Oxford × Comma.ai × BDDV × LiVi-Set(ours) ✔ ✔ ✔</formula><p>The density is about 700,000 points per second. Velodyne laser scanners are installed on the top front of our vehicle.</p><p>Video A color dashboard camera with real-time update system is placed on the top right of the front glass, which captures the video frame with 30 frames per second, of which the resolution is up to 1920 × 1080. Moreover, 128G memory space makes it possible to record 20-hour continuous 1080P videos maximally.</p><p>Driving Behavior A recording software is wirelessly connected to vehicle controller to get velocity from sensors equipped. Its resolution is up to 0.1km/h. The driver steering angle meter acquires the orientation data, whose resolution is 1 degree. When the steering wheel has a left (right) rotation with regard to standard center angle, angle meter records a negative (positive) value. Using our platform, we totally obtained seven sets with different test scenarios. Every set contains three types of data including point clouds, videos and driver behaviors. The amount of all point clouds is around 1TB and traffic videos are about 15GB. In addition, collected data includes a variety of traffic conditions such as boulevard, primary road, mountain road, school area, narrow road and even tourist special route.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Data Preprocessing</head><p>In this section, we only introduce the preprocessing of point clouds. The processing of videos and driving polices is given in Section 4.4.</p><p>On the whole, there are three major aspects of point cloud processing (middle pipeline in <ref type="figure">Figure 3</ref>).</p><p>Frames Fusion Every 200 frames of raw point clouds, one of which captures small part of real scenes, were fused into one scene. One scene is corresponding to one video frame and one pair of driving behaviors. After that, each test scenario (set) owns around 600 scenes. For the point clouds obtained by our fusion algorithm are stored in PCD format, we employed a standard software to transform data into LAS format, which is an industry standard for LiDAR data.</p><p>Synchronization LiDAR scanners and speed sensors with video system are synchronized in advance to obtain valid data for driving policy learning. It is worthy to mention that synchronization is essential step before data collecting and we try the best to keep it precise (The bias is lower than 0.1 second).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Addressing Errors</head><p>We triple-checked acquired data comprehensively and insured that videos, point clouds and driving behaviors are synchronous. Some unexpected errors were corrected after we manually re-calibrated to produce high-quality data in those time sections.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Data Structure</head><p>Our LiDVR-Video Driving dataset (LiVi-Set) is a benchmark comprised of real driving videos, point clouds and standard driving behaviors. The data structure of dataset is illustrated in <ref type="figure">Figure 3</ref>. Compared with existing benchmark datasets, LiVi-Set benchmark has combined 2D and 3D vision and moves the first attempt to leverage depth information (point clouds) to make the driving predictions. More details are shown in <ref type="table" target="#tab_1">Table 1</ref>.</p><p>In consequence, the dataset is largely different from previous benchmarks for vision-based autonomous driving. To the best of our knowledge, it is the first benchmark for autonomous driving policy prediction combined with 2D and 3D information.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Features and Statistics</head><p>Our dataset has a list of excellent features illustrated in <ref type="figure" target="#fig_3">Figure 4</ref>, which are beneficial to policy learning. We have performed an in-depth analysis on properties of our dataset. Scale We have used two kinds of LiDAR scanners to collect point clouds. They produced more than 1TB point clouds covering more than 100km distance, which is twice larger than previous KITTI. To the best of our knowledge, it is the largest public LiDAR data with corresponding vehicle status (speed and angle). Diversity Our dataset contains a variety of traffic conditions, including local route, boulevard, primary road, mountain road, school areas, which contains a number of crossroads, urban overpasses, ramp ways and hairpin bends. So our benchmark covers light, normal and heavy traffic situations. In addition, it also meets scenes with different numbers of pedestrians. For instance, there are many pedestrians in school areas but few in highway. More specifically, our dataset contains more than 1500 cars, 500 road signs, 160  traffic lights and 363 crossroads and 32 footbridges. The diversity of real road scenes meets the real requirement for autonomous driving practice and makes our models more generic to operate in real scenarios.</p><p>Quality We use the Velodyne HDL-32E scanner to acquire 3D point clouds. HDL-32E can produce accurate depth information in mobile platforms. The depth range is 70 meters and it can achieve 2cm resolution. Additionally, the density of points is approximately 34,000 points per second so that abundant information is included in our point clouds data. We can clearly see buildings, trees, road lines traffic lights and even pedestrians. As for digital videos, vehicle's dashboard camera produces 1920 × 1080 resolution videos with minor distortion while the vehicle moving at high speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Evaluation</head><p>This section answers how to leverage depth information and what we can achieve if current state-of-the-art techniques are used. Section 4.1 and 4.2 define prediction tasks in our experiment and evaluation metrics. Then representative approaches tested in our dataset are displayed in Section 4.3 and more details of the training process are supplemented in Section 4.4. Finally, we give experimental results and discussion of our methods in Section 4.5 and 4.6. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Tasks</head><p>Driving behavior prediction tasks can be classified into two categories, discrete and continuous prediction.</p><p>Discrete action prediction It is to predict current probability distribution over all possible actions. The limitation of discrete prediction is that autonomous vehicle can only make decisions among limited predefined actions. For example, <ref type="bibr" target="#b28">[27]</ref> defines four actions: straight, stop, left turn and right turn and policy decision becomes classification task. Obviously, the discrete task is not suitable for real driving, since it is too coarse to guide the vehicle driving.</p><p>Continuous prediction It is to predict current states of vehicles such as wheel angle and vehicle speed, which is a regression task. If driving policies on all real-world states can be predicted correctly, vehicles are expected to be driven successfully by trained model. Therefore, we model driving process as a continuous prediction task. Our task is to train a model that receives multiple perception information including video frames and point clouds, thus predict correct steering angles and vehicle speeds.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metric</head><p>To evaluate the performance of driving behavior prediction, we investigated previous evaluation metrics. In <ref type="bibr" target="#b28">[27]</ref>, Xu et al. proposed a driving perplexity metric which is inspired by representative Markov model in linguistics. The action perplexity is defined as the exponent of the sum of entropy in sequential prediction events. Perplexity metric is a positive number smaller than one and the smaller score indicates the more accurate prediction.</p><p>Nevertheless, many researchers do not consider it as an effective metric. It is because that they do not give it realworld meaning and they believe perplexity value is more suitable for working as loss function in training process. For example, people do not know whether their models are effective enough or not, when the perplexity is 0.1 (seemingly low). Accuracy metric is more intuitive in comparison to perplexity. More importantly, accuracy metric has been widely adopted <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b21">20,</ref><ref type="bibr" target="#b25">24]</ref> and applied to realistic scenarios <ref type="bibr" target="#b6">[5]</ref>. If vehicles can always be very close to ground truth behavior, they will self-drive smoothly and safely.</p><p>Threshold In accuracy computing, we need to count how many predictions are correct. Therefore, a tolerance threshold is required. When the bias between prediction and ground truth is smaller than tolerance threshold, we count this prediction are a correct case. In fact, human drivers also have minor biases in driving, but it can be tolerated.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Representative Approaches</head><p>To demonstrate the effectiveness of depth information, we explore how well prediction models can achieve if current techniques are utilized. As before, we should introduce some learning tools and depth representation as prior knowledge. In the end, two current mainstream frameworks are presented.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.1">Learning Tools</head><p>DNN. DNN has been built as a powerful class of models for extracting image features. In this paper, we adopt Resnet <ref type="bibr" target="#b14">[13]</ref> and Inception-v4 <ref type="bibr" target="#b26">[25]</ref>, which are all the state-of-the-art approaches for extracting image features. These two models are pretrained on ImageNet <ref type="bibr" target="#b24">[23]</ref> and fine-tuned in our experiments. Besides, we also use NVIDIA architecture <ref type="bibr" target="#b6">[5]</ref> which is much smaller than networks mentioned above but has been tested well in real practice such as highway lane following.</p><p>LSTM. Driving policy prediction based on one frame (or small frame batch) only may miss information in the temporal domain. Therefore, we make use of long short-term memory (LSTM) <ref type="bibr" target="#b15">[14]</ref> recurrent neural network to capture temporal events. LSTM is a well-improved recurrent neural network by introducing memory gates. It avoids gradient vanishing and is capable of learning long-term dependencies. Actually, LSTM is widely used in state-of-theart frameworks for predicting driving behaviors. In <ref type="bibr" target="#b11">[10]</ref>, LSTM-based framework is proposed for video classification. The championship <ref type="bibr" target="#b17">[16]</ref> in Udacity Self-Driving Challenge 2 also adopts this architecture.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Depth Representation</head><p>To leverage point clouds information effectively, we should seek a powerful depth representation. We have attempted different lines of techniques including point clouds reshaping, point clouds mapping and PointNet.</p><p>Point Clouds Mapping (PCM). We adopt the algorithm proposed in <ref type="bibr" target="#b29">[28]</ref> to preserve geometrical information from raw point clouds. As shown in <ref type="figure">Figure 7</ref>, we firstly divides mobile LiDAR points into h × w grids on XOZ plane, where h and w are 600 and 1080 in our paper respectively. Each grids is represented by a single value to form a h × w feature map. The feature values of different grids are calculated using the algorithm in <ref type="bibr" target="#b29">[28]</ref>. Intuitive idea behind it is to get the nearest points of Y coordinate in each grid. In short, feature map nicely extracts geometry information in point clouds. <ref type="figure" target="#fig_5">Figure 6</ref> demonstrates some samples of feature maps and their jet color visualization in our dataset and <ref type="figure">Figure 7</ref> depicts pipeline of this process.</p><p>PointNet. A novel PointNet architecture is put forward in <ref type="bibr" target="#b23">[22]</ref> and opens a new direction for directly utilizing disordered point clouds. It directly takes disorder points as the input of neural networks and finally output the representation features. Currently, this distinct architecture outperforms other shape representation methods and achieves high accuracy.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">Two Mainstream Frameworks</head><p>As is illustrated in <ref type="figure">Figure 8</ref>, inspired by plentiful previous works <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b11">10,</ref><ref type="bibr" target="#b17">16,</ref><ref type="bibr" target="#b25">24]</ref>, we decide to adopt two representative mainstream frameworks for policy prediction tasks, namely "DNN-only" and "DNN-LSTM". DNN-only. <ref type="bibr" target="#b6">[5,</ref><ref type="bibr" target="#b25">24]</ref>  to-end system that receives one (or a mini-batch) frame input and predicts driving behavior (seeing <ref type="figure">Figure 8 (a)</ref>). We employ three representative DNNs (NVIDIA, Resnet152 and Inception-v4) to extract features of RGB frames and 2D depth maps by PCM. The feature of point cloud is also extracted through PointNet. Thus, we concatenate two features (IO + PCM or IO + PointNet) as the input vector of one 1024 layer. This hidden layer is fully connected to fusion network, which outputs final driving behavior prediction.</p><p>DNN-LSTM. <ref type="bibr" target="#b11">[10,</ref><ref type="bibr" target="#b17">16]</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Details of Training</head><p>The training samples are time-registered data including videos, point clouds, feature maps and driving behaviors. The captured videos are down-sampled to 1 fps. Frames are reshaped to different sizes which are suitable for three DNNs (NVIDIA: 66 × 200, Resnet: 224 × 224 and Inception: 299 × 299). Besides, point clouds are down-sampled to 16384 points (16384 × 3) while adopting PointNet. Original point clouds which contain millions of points in each scene are used to generate feature maps directly to maintain enough information.</p><p>Our loss objective is a root-mean-square deviation (RMSD) to represent the sample standard deviation of the differences between predicted values and ground truth values. Vehicle speed and steering angle prediction models are trained individually. We attempt to train them jointly, but the performance is slightly worse than the cases where they are trained individually. We use a 80-20 training-testing split in our experiment and Adam optimizer <ref type="bibr" target="#b16">[15]</ref> to minimize the loss function. <ref type="table" target="#tab_3">Table 2</ref> shows the accuracy of two aforementioned mainstream frameworks. Each setting is measured with the accuracy of predictions on wheel angles and vehicle speeds. Furthermore, we adopt three network structures to extract features of video frames and depth maps. The tolerance thresholds of vehicle speed and wheel angle are 5km/h and 6</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Results</head><p>• , respectively. More results under different tolerance thresholds are available in the supplementary file. In <ref type="figure" target="#fig_4">Figure 5</ref>, we display the trends of accuracy in IO model that adopted  NVIDIA architecture with tolerance threshold increasing.</p><p>Overall, "DNN-LSTM" outperforms "DNN-only" setting, which means that feeding videos-frames in the sequence to networks helps autonomous vehicles to make decisions. It is because that independent image neglects important long-term event information.</p><p>More importantly, it is fascinating that utilizing depth information improves the accuracy of prediction greatly in comparison to the use of video frames only (IO in <ref type="table" target="#tab_3">Table 2</ref>). It again verifies the importance of depth information for driving behavior prediction and also shows the great potentials to improve driving prediction task by designing advanced depth representations and effective ways of extracting point features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Discussion</head><p>Firstly, regarding autonomous driving prediction process as a temporally sequential model keeps more essential information and gets better results. The system that holds memory in sequence is suitable for deciding future trends.</p><p>Secondly, depth information contributes to more reliable results and it helps vehicles learn driving polices more effectively. In consequence, we believe that future autonomous vehicles are likely to equip with 3D-scanners in order to gain comprehensive perception like the human.</p><p>Thirdly, although we use powerful DNNs such as Resnet to extract features, the improvement is still minor, which means we may meet an upper-board for 2D vision.</p><p>Finally, the large gap among different ways of using depth information tells us current depth representation is still an open problem that is not fully resolved. Even though our paper has attempted various depth representation and seems to produce good results, we still believe that there are huge potentials for depth utilization.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion and Future Work</head><p>In this paper, we have proposed a LiDAR-Video Driving benchmark dataset, which is among the first attempts to utilize point clouds to help driving policy learning. We have performed an in-depth analysis of how important depth information is, how to leverage depth information and what we can achieve by leveraging current representative techniques. From preliminary experiment results, we found that the utilization of depth information had resulted in considerable promotion in prediction performance. However, it still has a large room to improve the usage of point cloud information. We believe our benchmark dataset will open one door to study policy learning by providing extra but significant point clouds.</p><p>Our paper has attempted varied ways to take the advantages of point clouds in the benchmark. Even though these methods has helped networks to learn driving policies, they are far from optimal solutions for insufficient utilization of point clouds. How to make the best of these information remains to be further studied.</p><p>Moreover, although the supervised end to end segmentation may improved performance greatly, it may be too expensive to annotate the training data. Unlike plane 2D images or videos, point clouds contains rich depth information and geometrical features. In consequence, it is feasible to segment point clouds in unsupervised ways <ref type="bibr" target="#b2">[2,</ref><ref type="bibr" target="#b27">26]</ref>. We believe that affordable weakly-supervised or unsupervised coarse segmentation will help generate quantities of annotated data and learn driving policies.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. LiDAR-Video Driving dataset: a benchmark for polices learning in autonomous driving.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Our data collection platform with multiple sensors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Driving Behavior Distribution, Object Occurrence and Scenarios Statistics of our Dataset. This figure demonstrates (from left to right and top to bottom): the vehicle and accelerated speed distribution, the wheel angle and angular acceleration distribution, different types of objects occurring and different traffic conditions (road types) in our sequence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. Prediction accuracy variation trends for trained model.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Examples of gray and jet feature maps. First row of this figure is gray feature maps and second row is the corresponding colored feature maps using jet color map. Depth information and spatial information (pedestrians, vehicles, trees, traffic lights, bridges, buildings and so on) can be obtained implicitly from maps.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .Figure 8 .</head><label>78</label><figDesc>Figure 7. The pipeline of extracting feature maps from raw point clouds. Firstly, split XOZ plain into small grids, one of which is corresponding to specific one pixel in feature maps. Secondly, group raw points by projecting points into grids in XOZ. Then calculate feature values (F-values in figure) in each grid. Finally, generate feature maps by visualizing feature matrix and rendering with jet color map.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head></head><label></label><figDesc>utilize this line of framework in their research. Different from "DNN-only", we replace fusion network with stacked LSTM nets in "DNN-LSTM" frame- work. (seeing Figure 8 (b)) More specifically, two features of input data are ex- tracted and concatenated like "DNN-only" framework. Then the concatenated vectors are sent into stacked LSTMs to get predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>established a benchmark which comprises 389 stereo and optical flow image pairs, stereo visual odom- etry sequences of 39.2 km length, and over 200k 3D ob- ject annotations captured in cluttered scenarios. It provides instance-level annotations for humans and vehicles in real scenes, which is intended for object detection and segmen- tation tasks. However, KITTI is only composed of less busy suburban traffic scenes. In other words, KITTI ex- hibits significantly fewer flat ground structures, fewer hu-</figDesc><table>Recording Platform 

LiDAR 
Scanners 

PCAP 

merging 

PCD/LAS 

Digital Vidio 
Recorder 

Videos (1920 x 1080) 

Frames (1fps) 
(66 x 200) 
Frames (1fps) 
(224 x 224) 

Mulitple Sensors 

angle 

Driving Behaviors 

speed 

Dataset Structure 

Frames (1fps) 
(299 x 299) 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparison with existing public driving datasets. Our dataset is first to combine 2D and 3D vision with labeled drivers' behaviors.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>DNN Architecture Metric prediction accuracy of steering angle and vehicle speed</figDesc><table>DNN only 
DNN-LSTM 
IO 
PM 
PN 
IO 
PM 
PN 

NVIDIA 
angle 
63.0% 67.1% 71.1% 
77.9% 83.5% 81.6% 
speed 
70.1 % 69.2% 66.1% 
70.9% 73.8% 76.8% 

Resnet-152 
angle 
65.3% 70.8% 68.6% 
78.4% 84.2% 82.7% 
speed 
71.4% 72.6% 69.4% 
71.9% 74.3% 78.3% 

Inception-v4 
angle 
70.5% 71.1% 73.2% 
78.3% 83.7% 84.8% 
speed 
68.5% 70.3% 69.3% 
70.3% 76.4% 77.3% 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Performance of different combinations of basic networks with and without depth information. IO represents feeding images only into networks. PM denotes plain images plus feature maps (PCM). PN denotes plain networks combined with PointNet architecture. The accuracies are measured within 6 • or 5 km/h biases.</figDesc><table></table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgement</head><p>We thank the anonymous reviewers for their valuable comments. This work is supported in part by the National Natural Science Foundation of China under Grants 61601392 and 61772332.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Road Scene Segmentation from a Single Image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Alvarez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Gevers</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">M</forename><surname>Lopez</surname></persName>
		</author>
		<imprint>
			<biblScope unit="page" from="376" to="389" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heidelberg</forename><surname>Springer Berlin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<pubPlace>Berlin, Heidelberg</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title level="m" type="main">Urban 3d segmentation and modelling from street view images and lidar point clouds. Machine Vision and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Babahajiani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-K</forename><surname>Kämäräinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gabbouj</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017-05" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoder-decoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<idno>abs/1511.00561</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">The relevance of stereopsis for motorists: a pilot study</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Dietz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kolling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Schiefer</surname></persName>
		</author>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Graefe&apos;s Archive for</title>
	</analytic>
	<monogr>
		<title level="j">Clinical and Experimental Ophthalmology</title>
		<imprint>
			<biblScope unit="volume">239</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="400" to="406" />
			<date type="published" when="2001-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">End to end learning for self-driving cars</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bojarski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Testa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dworakowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Firner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">D</forename><surname>Jackel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Monfort</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Zieba</surname></persName>
		</author>
		<idno>abs/1604.07316</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Instant object detection in lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Brcs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Nagy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Benedek</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Geoscience and Remote Sensing Letters</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page" from="992" to="996" />
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Deepdriving: Learning affordance for direct perception in autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Seff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kornhauser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15</title>
		<meeting>the 2015 IEEE International Conference on Computer Vision (ICCV), ICCV &apos;15<address><addrLine>Washington, DC, USA</addrLine></address></meeting>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2722" to="2730" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Monocular 3d object detection for autonomous driving</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kundu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="2147" to="2156" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">The cityscapes dataset for semantic urban scene understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cordts</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Omran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Rehfeld</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Enzweiler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Benenson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Franke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Roth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<idno>abs/1604.01685</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Long-term recurrent convolutional networks for visual recognition and description</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Hendricks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Venugopalan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1411.4389</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Vision meets robotics: The kitti dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Stiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Robotics Research</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Are we ready for autonomous driving? the kitti vision benchmark suite</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lenz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno>abs/1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Comput</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1735" to="1780" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno>abs/1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Udacity self driving car challenge 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kormanda</surname></persName>
		</author>
		<ptr target="https://github.com/udacity/self-driving-car/.Ac-cessed" />
		<imprint>
			<biblScope unit="page" from="2017" to="2028" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">1 Year, 1000km: The Oxford RobotCar Dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Maddern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Pascoe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Linegar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Newman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The International Journal of Robotics Research (IJRR)</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="3" to="15" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">3d all the way: Semantic segmentation of urban scenes from start to end in 3d</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Martinovi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Knopp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Riemenschneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">V</forename><surname>Gool</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2015-06" />
			<biblScope unit="page" from="4456" to="4465" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">The visual and driving performance of monocular and binocular heavy-duty truck drivers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mcknight</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Shinar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hilburn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Accident Analysis and Prevention</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="225" to="237" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Off-road obstacle avoidance through end-to-end learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Muller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ben</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Cosatto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Flepp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">L</forename><surname>Cun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Y. Weiss, P. B. Schölkopf, and J. C. Platt</editor>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">18</biblScope>
			<biblScope unit="page" from="739" to="746" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Advances in neural information processing systems 1. chapter ALVINN: An Autonomous Land Vehicle in a Neural Network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Pomerleau</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1989" />
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<biblScope unit="page" from="305" to="313" />
			<pubPlace>San Francisco, CA, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno>abs/1612.00593</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. CoRR, abs/1409.0575</note>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Learning a driving simulator</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Santana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hotz</surname></persName>
		</author>
		<idno>abs/1608.01230</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">Inception-v4, inception-resnet and the impact of residual connections on learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<idno>abs/1602.07261</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Semantic instance annotation of street scenes by 3d to 2d label transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kiefel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno>abs/1511.03240</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">End-to-end learning of driving models from large-scale video datasets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<idno>abs/1612.01079</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Automated extraction of street-scene objects from mobile lidar point clouds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Remote Sensing</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">18</biblScope>
			<biblScope unit="page" from="5839" to="5861" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Instance-level segmentation with deep densely connected mrfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<idno>abs/1512.06735</idno>
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
