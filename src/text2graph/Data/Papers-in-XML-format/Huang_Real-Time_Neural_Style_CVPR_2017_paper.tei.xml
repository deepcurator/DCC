<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:34+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Real-Time Neural Style Transfer for Videos</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haozhi</forename><surname>Huang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hao</forename><surname>Wang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhan</forename><surname>Luo</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lin</forename><surname>Ma</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenhao</forename><surname>Jiang</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaolong</forename><surname>Zhu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhifeng</forename><surname>Li</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wei</forename><surname>Liu</surname></persName>
							<affiliation key="aff1">
								<orgName type="laboratory">Tencent AI Lab</orgName>
							</affiliation>
						</author>
						<author>
							<affiliation key="aff0">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Real-Time Neural Style Transfer for Videos</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>* Correspondence:</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recently, great progress has been achieved by applying deep convolutional neural networks (CNNs) to image transformation tasks, where a feed-forward CNN receives an input image, possibly equipped with some auxiliary information, and transforms it into a desired output image. This kind of tasks includes style transfer <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b24">27]</ref>, semantic segmentation <ref type="bibr" target="#b18">[19]</ref>, super-resolution <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b6">7]</ref>, colorization <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b28">31]</ref>, etc.</p><p>A natural way to extend image processing techniques to videos is to perform a certain image transformation frame by frame. However, this scheme inevitably brings temporal inconsistencies and thus causes severe flicker artifacts. The second row in <ref type="figure">Fig. 1</ref> shows an example of directly applying the feed-forward network based image style transfer method Style Image <ref type="figure">Figure 1</ref>: Video style transfer without and with temporal consistency. The first row displays two consecutive input frames and a given style image. The second row shows the stylized results generated by the method of Johnson et al. <ref type="bibr" target="#b11">[12]</ref>. The zoom-in regions in the middle show that the stylized patterns are of different appearances between the consecutive frames, which creates flicker artifacts. The third row shows the stylized results of our method, where the stylized patterns maintain the same appearance.</p><p>of Johnson et al. <ref type="bibr" target="#b11">[12]</ref> to videos. It can be observed that the zoom-in content marked by white rectangles is stylized into different appearances between two consecutive frames, therefore creating flicker artifacts. The reason is that slight variations between adjacent video frames may be amplified by the frame-based feed-forward network and thus result in obviously different stylized frames. In the literature, one solution to retain temporal coherence after video transformation is to explicitly consider temporal consistency during the frame generation or optimization process <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b21">22]</ref>. While effective, they are case-specific methods and thus cannot be easily generalized to other problems. Among them, the method of Ruder et al. <ref type="bibr" target="#b21">[22]</ref> is specifically designed for video style transfer. However, it relies on timeconsuming optimization on the fly, and takes about three minutes to process a single frame even with pre-computed optical flows. Another solution to maintaining temporal consistency is to apply post-processing <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b1">2]</ref>. A draw-back of post-processing is that it can only deal with image transformations whose edited results have pixel-wise correspondences to their inputs, which is not the case that style transfer obeys. Moreover, both solutions need to compute optical flows for new input video sequences, which prevents their usage for real-time video style transfer.</p><p>In view of the efficacy of feed-forward networks on image transformation tasks, a natural thinking will be whether a feed-forward network can be adapted to video transformation tasks by including temporal consistency. In this paper, we testify this idea on the problem of video style transfer. We demonstrate that a feed-forward network can not only capture content and style information in the spatial domain, but also encourage consistency in the temporal domain. We propose to use a hybrid loss in the training stage that combines the losses in the spatial and temporal domains together. Aided by the supervision of the spatial loss, our proposed video style transfer model can well preserve high-level abstract contents of the input frames and introduce new colors and patterns from a given style image. Meanwhile, the introduced temporal loss, guided by pre-computed optical flows, enables our feed-forward network to capture the temporal consistency property between consecutive video frames, therefore enforcing our model to yield temporally consistent outputs. To enable the calculation of the temporal loss during the training stage, a novel two-frame synergic training method is proposed. After training, no more optical flow computation is needed during the inference process. Our extensive experiments verify that our method generates much more temporally consistent stylized videos than the method of Johnson et al. <ref type="bibr" target="#b11">[12]</ref>. An example result of our method is shown in the last row of <ref type="figure">Fig. 1</ref>, from which we can see that the stylized patterns incur no more flicker artifacts. The experiments also corroborate that our method is able to create stylized videos at a real-time frame rate, while the previous video style transfer method <ref type="bibr" target="#b21">[22]</ref> needs about three minutes for processing a single frame. This makes us to believe that a well-posed feedforward network technique has a great potential for avoiding large computational costs of traditional video transformation methods.</p><p>The main contributions of this paper are two-fold:</p><p>• A novel real-time style transfer method for videos is proposed, which is solely based on a feed-forward convolutional neural network and avoids computing optical flows on the fly.</p><p>• We demonstrate that a feed-forward convolutional neural network supervised by a hybrid loss can not only stylize each video frame well, but also maintain the temporal consistency. Our proposed novel twoframe synergic training method incorporates the temporal consistency into the network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Style transfer aims to transfer the style of a reference image/video to an input image/video. It is different from color transfer in the sense that it transfers not only colors but also strokes and textures of the reference. Image analogy is the first classic style transfer method for images <ref type="bibr" target="#b9">[10]</ref>, which learns a mapping between image patches. As an extension to image analogy, Lee et al. <ref type="bibr" target="#b15">[16]</ref> further incorporated edge orientation to enforce gradient alignment. Recently, Gatys et al. <ref type="bibr" target="#b8">[9]</ref> proposed to perform style transfer in an optimization manner by running back-propagation with a perceptual loss defined on high-level features of the pretrained VGG-19 network <ref type="bibr" target="#b22">[23]</ref>. Though impressive stylized results are achieved, Gatys et al.'s method takes quite a long time to infer the stylized image. Afterwards, Johnson et al. <ref type="bibr" target="#b11">[12]</ref> proposed to train a feed-forward CNN using a similar perceptual loss defined on the VGG-16 network <ref type="bibr" target="#b22">[23]</ref> to replace the time-consuming optimization process, which enables real-time style transfer for images. Some follow-up work was conducted to further improve the feed-forward C-NN based image style transfer method. Li and Wand <ref type="bibr" target="#b16">[17]</ref> proposed to use patches of neural feature maps to compute a style loss to transfer photo-realistic styles. Ulyanov et al. <ref type="bibr" target="#b25">[28]</ref> suggested instance normalization in lieu of batch normalization, which gives more pleasant stylized results. Dumolin et al. <ref type="bibr" target="#b7">[8]</ref> demonstrated that a feed-forward CNN can be trained to capture multiple different styles by introducing conditional instance normalization.</p><p>Simply treating each video frame as an independent image, the aforementioned image style transfer methods can be directly extended to videos. However, without considering temporal consistency, those methods will inevitably bring flicker artifacts to generated stylized videos. In order to suppress flicker artifacts and enforce temporal consistency, a number of approaches have been investigated and exploited for different tasks <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b27">30,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b21">22]</ref>. Specifically, Ruder et al. <ref type="bibr" target="#b21">[22]</ref> used a temporal loss guided by optical flows for video style transfer. On one hand, Ruder et al.'s approach depends on an optimization process which is much slower than a forward pass through a feed-forward network. On the other hand, the on-the-fly computation of optical flows makes this approach even slower. In this paper, we show that temporal consistency and style transfer can be simultaneously learned by a feed-forward CNN, which avoids computing optical flows in the inference stage and thus enables real-time style transfer for videos.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>Our style transfer model consists of two parts: a stylizing network and a loss network, as shown in <ref type="figure">Fig. 2</ref>. The stylizing network takes one frame as input and produces its corresponding stylized output. The loss network, pre-trained on <ref type="figure">Figure 2</ref>: An overview of our proposed model. It consists of two parts: a stylizing network and a loss network. Black, green and red rectangles represent an input frame, an output frame and a given style image, respectively. A hybrid loss function including spatial and temporal components is defined on the loss network. Specifically, the spatial loss is computed separately for each of two consecutive frames and the temporal loss is computed based on both of them. This hybrid loss is used to train the stylizing network.</p><p>the ImageNet classification task <ref type="bibr" target="#b5">[6]</ref>, first extracts the features of the stylized output frames and then computes the losses, which are leveraged for training the stylizing network. On one hand, these features are used to compute a spatial loss in order to evaluate the style transfer quality in the spatial domain, which is a weighted sum of a content loss and a style loss. The content loss evaluates how close the high-level contents of the input and the stylized output are. The style loss measures how close the style features of the given style image and the stylized output are. On the other hand, a novel term, namely temporal loss, is introduced in our model to enforce temporal consistency between the stylized outputs. During the training process, two stylized output framesx t−1 andx t of two consecutive input frames x t−1 and x t are fed to the loss network to compute the temporal loss, which measures the Euclidian color distance between the corresponding pixels referring to precomputed optical flows.</p><p>The stylizing network and loss network are fully coupled during the training process. The spatio-temporal loss computed by the loss network is employed to train the stylizing network. With sufficient training, the stylizing network, although taking one single frame as input, has encoded the temporal coherence learned from a video dataset and can thus generate temporally consistent stylized video frames. Given a new input video sequence, the stylized frames are yielded by executing a feed-forward process through the stylizing network, so real-time style transfer performance is achieved.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Stylizing Network</head><p>The stylizing network accounts for transforming a single video frame to a stylized one. The architecture of the stylizing network is outlined in <ref type="table" target="#tab_0">Table 1</ref>. After three convolutional blocks, the resolution of the feature map is reduced to a quarter of the input. Then five residual blocks are subsequently followed, leading to fast convergence. Finally, af- ter two deconvolutional blocks and one more convolutional block, we obtain a stylized output frame which is of the same resolution as the input frame. Compared to the existing feed-forward network for image style transfer <ref type="bibr" target="#b11">[12]</ref>, an important benefit of our network is that it uses a smaller number of channels to reduce the model size, which turns out to infer faster without a noticeable loss in the stylization quality. More discussions on the model size can be found in Sec. 4.5.2. Moreover, instance normalization <ref type="bibr" target="#b25">[28]</ref> is adopted in our stylizing network in lieu of batch normalization for achieving better stylization quality. Although with a similar architecture, the most distinguishable difference of our network from <ref type="bibr" target="#b11">[12]</ref> is that the temporal coherence among video frames has been encoded into our stylizing network. As such, the stylizing network can simultaneously perform style transfer and preserve temporal consistency. As will be demonstrated in Sec. 4, our stylizing network yields much more temporally consistent stylized video sequences.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Loss Network</head><p>For training the stylizing network, reliable and meaningful features of the original frame, the stylized frame, and the style image need to be extracted for computing the spatial and temporal losses. In this paper, we use VGG-19 as our loss network, which has demonstrated its effectiveness on the image content and style representations <ref type="bibr" target="#b8">[9]</ref>. Other loss network architectures for image style transfer have been investigated by Nikulin et al. <ref type="bibr" target="#b20">[21]</ref>, which is beyond the focus of this paper.</p><p>Different from the image style transfer approaches <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b25">28]</ref> which only employ the spatial loss for training, video style transfer is more complicated. Temporal consistency, as one of the most significant perceptual factors for videos, needs to be taken into consideration. Therefore, we define a hybrid loss as follows:</p><formula xml:id="formula_0">L hybrid = i∈{t,t−1} L spatial (x i ,x i , s) spatial loss + λL temporal (x t ,x t−1 ) temporal loss ,<label>(1)</label></formula><p>where x t is the input video frame at time t,x t is the corresponding output video frame, and s is the given style image. The spatial loss mainly resembles the definitions in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>, which ensures that each input frame is transferred into the desired style. The newly introduced temporal loss is leveraged to enforce the adjacent stylized output frames to be temporally consistent. While the spatial loss is computed separately for two consecutive stylized framesx t andx t−1 , the temporal loss is computed based on both of them.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">Spatial Loss</head><p>The spatial loss is tailored to evaluate the style transfer quality in the spatial domain for each frame. It is defined as the weighted sum of a content loss, a style loss, and a total variation regularizer:</p><formula xml:id="formula_1">L spatial (x t ,x t , s) = α l L l content (x t ,x t ) content loss + β l L l style (s,x t ) style loss + γRV η TV regularizer ,<label>(2)</label></formula><p>where l denotes a feature extraction layer in VGG-19. The content loss defined at layer l is the mean square error between the feature maps of an input frame x t and its stylized outputx t :</p><formula xml:id="formula_2">L l content (x t ,x t ) = 1 C l H l W l φ l (x t ) − φ l (x t ) 2 2 ,<label>(3)</label></formula><p>where φ l (x t ) denotes the feature maps at level l, C l × H l × W l is the dimension of the level l feature maps. This content loss is motivated by the observation that high-level features learned by CNNs represent abstract contents, which are what we intend to preserve for the original input in the style transfer task. Therefore, by setting l as a high-level layer, the content loss ensures the abstract information of the input and stylized output to be as similar as possible. In this paper, we use the high-level layer ReLU4 2 to calculate the content loss.</p><p>Besides high-level abstractions preserved from the original frame, for the purpose of style transfer, we also need to stylize the details according to a reference style image. Therefore, a style loss is incorporated to evaluate the style difference between the style image and the stylized frame. In order to well capture the style information, a Gram matrix G l ∈ R C l ×C l at layer l of the loss network is defined as:</p><formula xml:id="formula_3">G l ij (x t ) = 1 H l W l H l h=1 W l w=1 φ l (x t ) h,w,i φ l (x t ) h,w,j .<label>(4)</label></formula><p>Here G l ij is the (i, j)-th element of Gram matrix G l , which is equal to the normalized inner product between the vectorized feature maps of channel i and j at layer l. The Gram matrix G l evaluates which channels tend to activate together, which is shown to be able to capture the style information <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b11">12]</ref>. The style loss is thus defined as the mean square error between the Gram matrixes of the style image s and stylized output framex t :</p><formula xml:id="formula_4">L l style (s,x t ) = 1 C 2 l G l (s) − G l (x t ) 2 F .<label>(5)</label></formula><p>In order to fully capture the style information at different scales, a set of Gram matrixes in different layers of the loss network are used to calculate the overall style loss. We choose ReLU1 2, ReLU2 2, ReLU3 2, ReLU4 2 as the layers for computing the style loss. Other layers could also be used as style layers but will give stylized results of different flavors, which depend on personal tastes. Additionally, to encourage spatial smoothness and suppress checkerboard artifacts in the stylized output frame, we also add a total variation regularizer:</p><formula xml:id="formula_5">RV η = i,j x t i,j+1 −x t i,j 2 + x t i+1,j −x t i,j 2 η 2 , (6)</formula><p>wherex t i,j represents the pixel of stylized framex t at the spatial position (i, j), and η is set to be 1 empirically <ref type="bibr" target="#b19">[20]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Temporal Loss</head><p>As aforementioned, by simply applying an image style transfer method to video frames, flicker artifacts will be inevitably introduced. Therefore, besides the spatial loss that leads to style transfer for each frame, we incorporate a temporal loss to enforce the temporal consistency between adjacent frames. As illustrated in Eq. (1), two consecutive frames are fed simultaneously into the network to measure the temporal consistency. The temporal loss is defined as the mean square error between the stylized output at time t and the warped version of the stylized output at time t − 1, namely the short-term temporal loss defined in <ref type="bibr" target="#b21">[22]</ref>:</p><formula xml:id="formula_6">L temporal (x t ,x t−1 )= 1 D D k=1 c k x t k − f (x t−1 k ) 2 ,<label>(7)</label></formula><p>wherex t andx t−1 are the stylized results of the current frame and previous one, respectively. f (x t−1 k ) is a function that warps the stylized output at time t − 1 to time t according to a pre-computed optical flow. D = H × W × C is the dimension of the output. c ∈ [0, 1] D denotes the per-pixel confidence of the optical flow: 0 in occluded regions and at motion boundaries, and 1 otherwise.</p><p>Unlike the calculations of the content loss and style loss, the output of the stylizing network is directly employed to compute the temporal loss other than the high-level features of the loss network. We have tried using the higher-level feature maps of the loss network to compute the temporal loss, but encounter terrible flicker artifacts. The main reason is that higher-level feature maps only capture abstract information. Hence, we insist on using the stylized output of the stylizing network to compute the temporal loss, which enforces the pixel-wise temporal consistency.</p><p>In our experiments, we use Deepflow <ref type="bibr" target="#b26">[29]</ref> to compute the optical flows needed for the temporal loss computation. Alternative methods for optical flow calculation can also be used. Please note that the optical flows only need to be computed once for our training dataset. The temporal loss based on the optical flows can enforce our stylizing network to create temporally consistent stylized output frames, thus suppressing the flicker artifacts. In the testing (or inference) stage, the optical flow information is no longer required. As the feed-forward stylizing network has already incorporated the temporal consistency, we can apply the network on arbitrary input video sequences to generate temporally coherent stylized video sequences. Besides, we have also trained our stylizing network with non-consecutive frames to encourage long-term temporal consistency <ref type="bibr" target="#b21">[22]</ref>, which gives similar results with longer training time. For more discussions on the long-term temporal consistency, please refer to Sec. 4.5.1.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Implementation Details</head><p>We download 100 videos of different scenes collected from Videvo.net <ref type="bibr">[26]</ref>. 91 videos (about 40,000 frames) are used as the training dataset and 9 videos are used as the validation dataset. All video frames are resized to 640 × 360. Given a style image, we train a feed-forward stylizing network with a batch size of 2 for 80,000 iterations, reaching roughly two epoches over the training dataset. For each batch, the stylizing network simultaneously forwards two consecutive input frames x t−1 and x t , and outputs two stylized framesx t−1 andx t . Thenx t−1 andx t will be fed to the loss network to compute the hybrid loss. And a backpropagation process is performed to update the parameters of the stylizing network according to the gradients of the hybrid loss function. Note that the parameters of the loss network are fixed during the training process. The stochastic gradient descent technique we use in training is Adam <ref type="bibr" target="#b12">[13]</ref>, with a learning rate of 10 −3 . The total variation strength γ is set to 10 −3 to enforce spatial smoothness of the stylized frames. The default content strength α, the style strength β, and the temporal strength λ are set to 1, 10 and 10 4 , respectively. All the hyperparameters are chosen based on the results of the validation set. We implement our video style transfer method using Torch <ref type="bibr" target="#b4">[5]</ref> and cuDNN <ref type="bibr" target="#b3">[4]</ref>. Training a </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Qualitative Results</head><p>We verify our method on 30 testing videos, 10 from the Sintel dataset <ref type="bibr" target="#b2">[3]</ref> and 20 from Videvo.net <ref type="bibr">[26]</ref>, using more than 20 styles. For each style, an individual stylizing network is trained. <ref type="figure" target="#fig_0">Fig. 3</ref> shows the stylized results using 6 exemplar styles, named as Gothic, Candy, Dream, Mosaic, Composition and Starry Night. These results show that: <ref type="formula" target="#formula_0">(1)</ref> semantic contents of the original input videos are preserved; (2) colors and textures of the style images are transferred successfully. Due to the space limit, we cannot include all the stylized results here. Please refer to our supplementary material for more results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison to Methods in the Literature</head><p>We compare our method with two representative methods in the literature. One is the feed-forward CNN based method proposed by Johnson et al. <ref type="bibr" target="#b11">[12]</ref>, which is designed for image style transfer and runs in real time, but does not consider any temporal consistency. The other is the optimization-based method proposed by Ruder et al. <ref type="bibr" target="#b21">[22]</ref>, which is designed for video style transfer and maintains temporal consistency, but needs about 3 minutes for processing one frame. Since the Sintel dataset <ref type="bibr" target="#b2">[3]</ref> provides ground truth optical flows, we use it to quantitatively compare the temporal consistencies of different methods. We defined a term temporal error E temporal over a video sequence to be the average pixel-wise Euclidean color difference between consecutive frames:</p><formula xml:id="formula_7">E temporal = 1 (T − 1) × D T −1 t=1 D k=1 c k x t k − f (x t+1 k ) 2 ,<label>(8)</label></formula><p>where T represents the total number of frames. This formulation is very similar to Eq. <ref type="formula" target="#formula_6">(7)</ref>, except that we sum the temporal loss for all consecutive frame pairs in a video sequence. The function f (x t+1 k ) warps the stylized output at </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Comparison to Commercial Softwares</head><p>There are also some commercial Apps on smartphones, providing style transfer for videos. Artisto <ref type="bibr" target="#b23">[24]</ref> and Prisma <ref type="bibr">[25]</ref> are two representatives. Since we are not able to acquire the exact style images they are using, we select some of the styles that look most similar to ours, and generate results using their Apps directly. Both Artisto and Prisma support only square inputs, so we crop and resize all the input frames to 436 × 436. The examples of the stylized consecutive frames of different methods transferring the style Gothic are shown in <ref type="figure" target="#fig_2">Fig. 5</ref>. The results suggest that the temporal consistency of our results is better. The two columns in the middle show the zoom-in version of white rectangle regions. While the patterns change from frame to frame in Artisto and Prisma's results (see the regions marked by white circles), the patterns that our method creates maintain the same.</p><p>Both Artisto and Prisma create stylized videos at a different frame rate from that of an original input video, so we cannot use the ground truth optical flows of the Sintel dataset to compute temporal error as in Sec. 4.3. The best we can do is carrying out a user study. We invite 20 people, aged from 21 to 35, to participate in our study. 10 of our 30 testing videos are used. We select two styles (Candy and Gothic) which look like the styles provided by both Prisma and Artisto, and create 10 × 2 = 20 testing cases. In this user study, we compare our method with Artisto and Prisma separately. In each testing case, we simultaneously show the original input video, our result, and the result of either Artisto or Prisma on the same screen. The order of the two stylized videos is arranged randomly to avoid participants' laziness. To ensure that a user has enough time to distinguish the difference and make a careful judge, we loop all the videos for three times. In each testing case, we ask the participant which stylized video he/she prefers, or whether the two stylized videos are equally good, especially taking the flicker artifacts into account. The user study results after all testing cases are plotted in <ref type="figure">Fig. 6</ref>, which indicates that users prefer our method to both Artisto and Prisma. Night with a short-term temporal loss. At the top-right corner of the stylized frames, the color has changed from blue to yellow after the woman passed by. We would suspect that the optimization process of frame 30 is initialized with the previous frame 29 and ends up in a different local optimum from that of frame 1. This process makes the stylized results highly unstable. To address this issue, Ruder et al. <ref type="bibr" target="#b21">[22]</ref> considered a long-term temporal loss between the current stylized framex t and a set of previous stylized frames {x t−1 ,x t−10 ,x t−20 ,x t−40 }, which brings a heavier computational burden and slows down the optimization process significantly. For our method, since the training process has already encouraged overall temporal consistencies over a video dataset, the stylized results are highly stable, meaning that similar contents will end up to be stylized similarly. This saves us the effort of including a long-term temporal loss in our proposed loss function. See the second row of <ref type="figure">Fig. 8</ref>, the content of the top-right corner remains the same even after the woman passed by, which reveals that our stylizing network has already included the long-term temporal consistency even trained with only a short-term temporal loss.</p><p>We also carry out an experiment to testify whether including a temporal loss of non-consecutive (long-term) frames will decrease the temporal errors defined in Sec. 4.3. During the training process, we not only consider the frame pairs of consecutive frames (x t−1 and x t ), but also include the frame pairs of non-consecutive frames (x t−2 and x t ). Both the consecutive pairs and non-consecutive pairs are gathered together and disrupted into a random order to formalize a training epoch. We compare our default model and s results without using a long-term temporal loss. The color of the top-right corner changes after the woman passed by. The second row gives our results. Without using a long-term temporal loss, our stylizing network can produce visually similar results as those of <ref type="bibr" target="#b21">[22]</ref> with long-term consistency. the model trained including non-consecutive frame pairs on the videos from the Sintel dataset using style Composition. <ref type="table" target="#tab_3">Table 3</ref> shows the temporal errors of the two cases. There is only a slight drop of the temporal errors observed if we compare these two cases. This leads us to believe that including a long-term temporal loss during the training stage gives a very limited improvement on the temporal consistency, whereas at the cost of double or even more training time. Thus our default model for video style transfer does not include a long-term temporal loss.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.2">Influence of Model Compression</head><p>The stylizing network proposed by Johnson et al. <ref type="bibr" target="#b11">[12]</ref> has 5 residual blocks and each residual block produces 128 feature maps. In this paper, we find that using a smaller number of feature maps gives visually similar results while saving lots of space for storing models and shortening the inference time.</p><p>The design of our model has already been shown in Table 1, which has 5 residual blocks and each residual block </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusions</head><p>In this paper, we proposed a novel neural method for real-time video style transfer. This method relies on training a feed-forward convolutional neural network to simultaneously preserve the high-level abstract contents of input video frames, introduce the colors and patterns from a given style image, and enforce the temporal consistency among the stylized video frames. Moreover, the trained feedforward network, with the relief of on-the-fly optical flow computation, is capable of performing real-time video stylizing. The extensive experimental results clearly demonstrate the efficacy and superiority of our method.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Video style transfer results of six styles. The high-level abstract content of each original input video is kept, while the colors and textures are transferred from each style image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Comparison to two methods in the literature. The first row displays two consecutive input video frames. The following three rows show the error maps of Ruder et al.'s, our and Johnson et al.'s results. Ruder et al.'s method achieves the best temporal consistency, our method comes at the second place, and Johnson et al.'s method is the worst.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Comparison to commercial softwares. The first row shows two consecutive input frames. The second row shows our stylized results using style Gothic. The third row shows Artisto's results. The fourth row shows Prisma's results. The two columns in the middle show the zoom-in version of white rectangle regions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 7 :Figure 8 :</head><label>78</label><figDesc>Figure 7: Stylized results of two model sizes. Both results are stylized similarly.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>The stylizing network architecture. Conv denotes the convolutional block (convolutional layer + instance nor- malization + activation); Res denotes the residual block; Deconv denotes the deconvolutional block (deconvolutional layer + instance normalization + activation).</figDesc><table>Layer 
Size Stride Channel Activation 

Stylizing Network 

Conv 
3 
1 
16 
ReLU 
Conv 
3 
2 
32 
ReLU 
Conv 
3 
2 
48 
ReLU 
Res × 5 
Deconv 
3 
0.5 
32 
ReLU 
Deconv 
3 
0.5 
16 
ReLU 
Conv 
3 
1 
3 
Tanh 

Res 
Conv 
3 
1 
48 
ReLU 
Conv 
3 
1 
48 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Temporal errors of three different methods on five testing videos in the Sintel dataset.At first, the qualitative results of our method are given in Sec. 4.2. Then the quantitative com- parison with existing methods in the literature is presented in Sec. 4.3. We also compare our method against some popular commercial Apps in Sec. 4.4. To explore the in- fluence of long-term consistency and different model sizes, Sec. 4.5 discusses two variants of our method, and gives the quantitative results.</figDesc><table>Method 
Alley 2 Ambush 5 Bandage 2 Market 6 Temple 2 
Ruder et al. [22] 
0.0252 
0.0512 
0.0195 
0.0407 
0.0361 
Johnson et al. [12] 0.0770 
0.0926 
0.0517 
0.0789 
0.0872 
Ours 
0.0439 
0.0675 
0.0304 
0.0553 
0.0513 

single stylizing network takes about 92 hours with a single 
NVIDIA Tesla K80 GPU. 
In the following we present the experimental results from 
three perspectives. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Fig. 8, which shows the stylized output frames of Ruder et al. s method according to style image Starry</figDesc><table>Comparison to Artisto 
Comparison to Prisma 
Prefer ours 
216 
332 
Prefer the other 
91 
49 
Equal 
93 
19 

0 

50 

100 

150 

200 

250 

300 

350 

Votes 

User Preference 

Figure 6: User study results. Comparing with Artisto, our 
method receives 216 votes while Artisto receives 91 votes. 
Comparing with Prisma, our method receives 332 votes 
while Prisma receives 49 votes. 

4.5. Variants of Our Model 

4.5.1 Long-Term Temporal Consistency 

For the optimization-based method of Ruder et al. [22], on-
ly considering a temporal loss between consecutive frames 
may not guarantee long-term temporal consistency. See the 
first row of </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Temporal errors without/with non-consecutive frame pairs.</figDesc><table>Method 
Alley 2 Ambush 5 Bandage 2 Market 6 Temple 2 
(x 
t−1 , x 
t ) 
0.0250 
0.0406 
0.0182 
0.0330 
0.0297 
(x 
t−2 , x 
t ) + (x 
t−1 , x 
t ) 0.0241 
0.0394 
0.0180 
0.0311 
0.0283 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Temporal errors of different model sizes.produces 48 feature maps. This model is termed as 353- Res48. We test another stylizing network that mimics the architecture of Johnson et al. [12], which is obtained by in- creasing the feature map number to 128 and called as 353- Res128. The channel number of other layers is also adjusted accordingly, which is consistent with Johnson et al.'s net- work. Once again, we use the Sintel dataset [3] to calculate the temporal errors defined in Sec. 4.3. The results are col- lected in Table 4, where we can see that 353-Res48 presents a similar temporal error to 353-Res128. The results in Table 4 illustrate that although the number of learnable parameters is reduced, 353-Res48 still creates visually similar results as 353-Res128. By reducing the model size, we can accelerate the inference speed. To perform style transfer for one frame at the resolution of 1024 × 436, our model 353-Res48 takes about 0.041 seconds, while 353-Res128 takes about 0.098 seconds, both with a single NVIDIA Tesla K80 GPU. This enables our method to support higher frame rates or higher resolutions for real-time applications.</figDesc><table>Model 
Alley 2 Ambush 5 Bandage 2 Market 6 Temple 2 
353-Res128 0.0243 
0.0408 
0.0193 
0.0330 
0.0296 
353-Res48 
0.0244 
0.0425 
0.0195 
0.0334 
0.0302 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>When this work was conducted, the first author was an intern at Tencent AI Lab. This work is supported in part by Tsinghua University -Tencent Joint Lab.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Examplebased video color grading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">39</biblScope>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Blind video temporal consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Bonneel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tompkin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sunkavalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Paris</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pfister</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">34</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page">196</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A naturalistic open source movie for optical flow evaluation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Butler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wulff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">B</forename><surname>Stanley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chetlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Woolley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandermersch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Catanzaro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1410.0759</idno>
		<title level="m">Efficient primitives for deep learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Torch7: A matlab-like environment for machine learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Farabet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NIPS 25 Workshop on BigLearn</title>
		<meeting>NIPS 25 Workshop on BigLearn</meeting>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Image superresolution using deep convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on PAMI</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="295" to="307" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">A learned representation for artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.07629</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Image style transfer using convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Gatys</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Ecker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Image analogies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hertzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">E</forename><surname>Jacobs</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Oliver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Curless</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Salesin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Let there be color!: joint end-to-end learning of global and local image priors for automatic image colorization with simultaneous classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Iizuka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simo-Serra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ishikawa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">110</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Perceptual losses for real-time style transfer and super-resolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Alahi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Intrinsic video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">V</forename><surname>Gehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Practical temporal consistency for image-based graphics applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">O</forename><surname>Aydin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smolic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Gross</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">34</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Directional texture transfer</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Seo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ryoo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yoon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 8th International Symposium on Non-Photorealistic Animation and Rendering</title>
		<meeting>the 8th International Symposium on Non-Photorealistic Animation and Rendering</meeting>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Combining markov random fields and convolutional neural networks for image synthesis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wand</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Processing images and video for an impressionist effect</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Litwinowicz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ACM SIGGRAPH</title>
		<meeting>ACM SIGGRAPH</meeting>
		<imprint>
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Understanding deep image representations by inverting them</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mahendran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. CVPR</title>
		<meeting>CVPR</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">Exploring the neural algorithm of artistic style</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Nikulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Novak</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.07188</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Artistic style transfer for videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ruder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dosovitskiy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of German Conference on Pattern Recognition</title>
		<meeting>German Conference on Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno>arX- iv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>My</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Com</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Artisto</surname></persName>
		</author>
		<ptr target="https://artisto.my.com/" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Texture networks: Feed-forward synthesis of textures and stylized images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lebedev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>arX- iv:1607.08022</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepflow: Large displacement optical flow with deep matching</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Weinzaepfel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Revaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Harchaoui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schmid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICCV</title>
		<meeting>ICCV</meeting>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Intrinsic video and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Garces</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Gutierrez</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACM Transactions on Graphics</title>
		<imprint>
			<biblScope unit="volume">33</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page">80</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Colorful image colorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ECCV</title>
		<meeting>ECCV</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
