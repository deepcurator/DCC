<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:50+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Defense against Universal Adversarial Perturbations</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Naveed</forename><surname>Akhtar</surname></persName>
							<email>naveed.akhtar@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jian</forename><surname>Liu</surname></persName>
							<email>jian.liu@research.uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ajmal</forename><surname>Mian</surname></persName>
							<email>ajmal.mian@uwa.edu.au</email>
							<affiliation key="aff0">
								<orgName type="department">Computer Science and Software Engineering</orgName>
								<orgName type="institution">The University of Western</orgName>
								<address>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Defense against Universal Adversarial Perturbations</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
					<note>*The authors contributed equally to this work.</note>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Recent advances in Deep Learning</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Deep Neural Networks are at the heart of the current advancements in Computer Vision and Pattern Recognition, providing state-of-the-art performance on many challenging classification tasks <ref type="bibr" target="#b8">[9]</ref>, <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b13">[14]</ref>, <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b35">[36]</ref>, <ref type="bibr" target="#b36">[37]</ref>. However, Moosavi-Dezfooli et al. <ref type="bibr" target="#b24">[25]</ref> recently showed the possibility of fooling the deep networks to change their prediction about 'any' image that is slightly perturbed with the Universal Adversarial Perturbations. For a given network model, these image-agnostic (hence universal) perturbations can be computed rather easily <ref type="bibr" target="#b24">[25]</ref>, <ref type="bibr" target="#b25">[26]</ref>. The perturbations remain <ref type="figure">Figure 1</ref>. Adding quasi-imperceptible universal adversarial perturbations <ref type="bibr" target="#b24">[25]</ref> can fool neural networks. The proposed framework rectifies the images to restore the network predictions. The patterns removed by rectification are separately analyzed to decide on the presence of adversarial perturbations in images. The shown 'perturbations' and 'removed patterns' are normalized on different scales for better visualization.</p><p>quasi-imperceptible (see <ref type="figure">Fig. 1</ref>), yet the adversarial examples generated by adding the perturbations to the images fool the networks with alarmingly high probabilities <ref type="bibr" target="#b24">[25]</ref>. Furthermore, the fooling is able to generalize well across different network models.</p><p>Being image-agnostic, universal adversarial perturbations can be conveniently exploited to fool models on-thefly on unseen images by using pre-computed perturbations. This even eradicates the need of on-board computational capacity that is needed for generating image-specific perturbations <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b20">[21]</ref>. This fact, along the cross-model generalization of universal perturbations make them particularly relevant to the practical cases where a model is deployed in a possibly hostile environment. Thus, defense against these perturbations is a necessity for the success of Deep Learning in practice. The need for counter-measures against these perturbations becomes even more pronounced considering that the real-world scenes (e.g. sign boards on roads) modified by the adversarial perturbations can also behave as adversarial examples for the networks <ref type="bibr" target="#b16">[17]</ref>.</p><p>This work proposes the first dedicated defense against the universal adversarial perturbations <ref type="bibr" target="#b24">[25]</ref>. The major contributions of this paper are as follows:</p><p>• We propose to learn a Perturbation Rectifying Network (PRN) that is trained as the 'pre-input' of a targeted network model. This allows our framework to provide defense to already deployed networks without the need of modifying them.</p><p>• We propose a method to efficiently compute synthetic image-agnostic adversarial perturbations to effectively train the PRN. The successful generation of these perturbations complements the theoretical findings of Moosavi-Dezfooli <ref type="bibr" target="#b25">[26]</ref>.</p><p>• We also propose a separate perturbation detector that is learned from the Discrete Cosine Transform of the image rectifications performed by the PRN for clean and perturbed examples.</p><p>• Rigorous evaluation is performed by defending the GoogLeNet <ref type="bibr" target="#b36">[37]</ref>, CaffeNet <ref type="bibr" target="#b15">[16]</ref> and VGG-F network <ref type="bibr" target="#b3">[4]</ref> 1 , demonstrating up to 97.5% success rate on unseen images possibly modified with unseen perturbations. Our experiments also show that the proposed PRN generalizes well across different network models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related work</head><p>The robustness of image classifiers against adversarial perturbations has gained significant attention in the last few years <ref type="bibr" target="#b5">[6]</ref>, <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b28">[29]</ref>, <ref type="bibr" target="#b31">[32]</ref>, <ref type="bibr" target="#b33">[34]</ref>, <ref type="bibr" target="#b34">[35]</ref>, <ref type="bibr" target="#b39">[40]</ref>. Deep neural networks became the center of attention in this area after Szegedy et al. <ref type="bibr" target="#b38">[39]</ref> first demonstrated the existence of adversarial perturbations for such networks. See <ref type="bibr" target="#b0">[1]</ref> for a recent review of literature in this direction. Szegedy et al. <ref type="bibr" target="#b38">[39]</ref> computed adversarial examples for the networks by adding quasi-imperceptible perturbations to the images, where the perturbations were estimated by maximizing the network's prediction error. Although these perturbations were imagespecific, it was shown that the same perturbed images were able to fool multiple network models. Szegedy et al. reported encouraging results for improving the model robustness against the adversarial attacks by using adversarial examples for training, a.k.a. adversarial training.</p><p>Goodfellow et al. <ref type="bibr" target="#b9">[10]</ref> built on the findings in <ref type="bibr" target="#b38">[39]</ref> and developed a 'fast gradient sign method' to efficiently generate adversarial examples that can be used for training the networks. They hypothesized that it is the linearity of the deep networks that makes them vulnerable to the adversarial perturbations. However, Tanay and Griffin <ref type="bibr" target="#b40">[41]</ref> later constructed the image classes that do not suffer from the adversarial examples for the linear classifiers. Their arguments about the existence of the adversarial perturbations again point towards the over-fitting phenomena, that can be alleviated by regularization. Nevertheless, it remains unclear how a network should be regularized for robustness against adversarial examples.</p><p>Moosavi-Dezfooli <ref type="bibr" target="#b26">[27]</ref> proposed the DeepFool algorithm to compute image-specific adversarial perturbations by assuming that the loss function of the network is linearizable around the current training sample. In contrast to the one-step perturbation estimation <ref type="bibr" target="#b9">[10]</ref>, their approach computes the perturbation in an iterative manner. They also reported that augmenting training data with adversarial examples significantly increases the robustness of networks against the adversarial perturbations. Baluja and Fischer <ref type="bibr" target="#b1">[2]</ref> trained an Adversarial Transformation Network to generate adversarial examples against a target network. Liu et al. <ref type="bibr" target="#b18">[19]</ref> analyzed the transferability of adversarial examples. They studied this property for both targeted and nontargeted examples, and proposed an ensemble based approach to generate the examples with better transferability.</p><p>The above-mentioned techniques mainly focus on generating adversarial examples, and address the defense against those examples with adversarial training. In-line with our take on the problem, few recent techniques also directly focus on the defense against the adversarial examples. For instance, Lu et al. <ref type="bibr" target="#b21">[22]</ref> mitigate the issues resulting from the adversarial perturbations using foveation. Their main argument is that the neural networks (for ImageNet <ref type="bibr" target="#b32">[33]</ref>) are robust to the foveation-induced scale and translation variations of the images, however, this property does not generalize to the perturbation transformations.</p><p>Papernot et al. <ref type="bibr" target="#b29">[30]</ref> used distillation <ref type="bibr" target="#b12">[13]</ref> to make the neural networks more robust against the adversarial perturbations. However, Carlini and Wagner <ref type="bibr" target="#b2">[3]</ref> later introduced adversarial attacks that can not be defended by the distillation method. Kurakin et al. <ref type="bibr" target="#b17">[18]</ref> specifically studied the adversarial training for making large models (e.g. Inception v3 <ref type="bibr" target="#b37">[38]</ref>) robust to perturbations, and found that the training indeed provides robustness against the perturbations generated by the one-step methods <ref type="bibr" target="#b9">[10]</ref>. However, Tramer et al. <ref type="bibr" target="#b41">[42]</ref> found that this robustness weakens for the adversarial examples learned using different networks i.e. for the black-box attacks <ref type="bibr" target="#b18">[19]</ref>. Hence, ensemble adversarial training was proposed in <ref type="bibr" target="#b41">[42]</ref> that uses adversarial examples generated by multiple networks.</p><p>Dziugaite et al. <ref type="bibr" target="#b4">[5]</ref> studied the effects of JPG compression on adversarial examples and found that the compression can sometimes revert network fooling. Nevertheless, it was concluded that JPG compression alone is insufficient as a defense against adversarial attacks. Prakash et al. <ref type="bibr" target="#b30">[31]</ref> took advantage of localization of the perturbed pixels in their defense. Lu et al. <ref type="bibr" target="#b19">[20]</ref> proposed SafetyNet for detecting and rejecting adversarial examples for the conventional network classifiers (e.g. VGG19 <ref type="bibr" target="#b10">[11]</ref>) that capitalizes on the late stage ReLUs of the network to detect the perturbed examples. Similarly, a proposal of appending the deep neural networks with detector subnetworks was also presented by Metzen et al. <ref type="bibr" target="#b22">[23]</ref>. In addition to the classification, adversarial examples and robustness of the deep networks against them have also been recently investigated for the tasks of semantic segmentation and object detection <ref type="bibr" target="#b7">[8]</ref>, <ref type="bibr" target="#b20">[21]</ref>, <ref type="bibr" target="#b42">[43]</ref>.</p><p>Whereas the central topic of all the above-mentioned literature is the perturbations computed for individual images, Moosavi-Dezfooli <ref type="bibr" target="#b24">[25]</ref> were the first to show the existence of image-agnostic perturbations for neural networks. These perturbations were further analyzed in <ref type="bibr" target="#b25">[26]</ref>, whereas Metzen et al. <ref type="bibr" target="#b23">[24]</ref> also showed their existence for semantic image segmentation. To date, no dedicated technique exists for defending the networks against the universal adversarial perturbations, which is the topic of this paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Problem formulation</head><p>Below, we present the notions of universal adversarial perturbations and the defense against them more formally. Let ℑ c ∈ R d denote the distribution of the (clean) natural images in a d-dimensional space, such that, a class label is associated with its every sample I c ∼ ℑ c . Let C(.) be a classifier (a deep network) that maps an image to its class label, i.e. C(I c ) : I c → ℓ ∈ R. The vector ρ ∈ R d is a universal adversarial perturbation for the classifier, if it satisfies the following constraint:</p><formula xml:id="formula_0">P Ic∼ℑc C(I c ) = C(I c + ρ) ≥ δ s.t. ||ρ|| p ≤ ξ, (1)</formula><p>where P(.) is the probability, ||.|| p denotes the ℓ p -norm of a vector such that p ∈ [1, ∞), δ ∈ (0, 1] denotes the fooling ratio and ξ is a pre-defined constant. In the text to follow, we alternatively refer to ρ as the perturbation for brevity.</p><p>In (1), the perturbations in question are image-agnostic, hence Moosavi-Dezfooli et al. <ref type="bibr" target="#b24">[25]</ref> termed them universal 2 . According to the stated definition, the parameter ξ controls the norm of the perturbation. For the quasi-imperceptible perturbations, the value of this parameter should be very small as compared to the image norm ||I c || p . On the other hand, a larger δ is required for the perturbation to fool the classifier with a higher probability. In this work, we let δ ≥ 0.8 and consider the perturbations constrained by their ℓ 2 and ℓ ∞ norms. For the ℓ 2 -norm, we let ξ = 2, 000, and select ξ = 10 for the ℓ ∞ -norm perturbations. For both types, these values are ∼ 4% of the means of the respective image norms used in our experiments (in Section 5), which is the same as <ref type="bibr" target="#b24">[25]</ref>.</p><p>To defend C(.) against the perturbations, we seek two components of the defense mechanism. <ref type="formula">(1)</ref>  . Moreover, the formulation allows us to compute I such that || I − I c || 2 &gt; 0. We leverage this property to learn R(.) as the pre-input layers of C(.) in an end-to-end fashion.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Proposed approach</head><p>We draw on the insights from the literature reviewed in Section 2 to develop a framework for defending a (possibly) targeted network model against universal adversarial perturbations. <ref type="figure" target="#fig_1">Figure 2</ref> shows the schematics of our approach to learn the 'rectifier' and the 'detector' components of the defense framework. We use the Perturbation Rectifying Network (PRN) as the 'rectifier', whereas a binary classifier is eventually trained to detect the adversarial perturbations in the images. The framework uses both real and synthetic perturbations for training. The constituents of the proposed framework are explained below.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Perturbation Rectifying Network (PRN)</head><p>At the core of our technique is the Perturbation Rectifying Network (PRN), that is trained as pre-input layers to the targeted network classifier. The PRN is attached to the first layer of the classification network and the joint network is trained to minimize the following cost:</p><formula xml:id="formula_1">J (θ p , b p ) = 1 N N i=1 L(ℓ * i , ℓ i ),<label>(2)</label></formula><p>where ℓ * i and ℓ i are the labels predicted by the joint network and the targeted network respectively, such that ℓ i is necessarily computed for the clean image. For the N training examples, L(.) computes the loss, whereas θ p and b p denote the PRN weight and bias parameters.</p><p>In Eq. (2) we define the cost over the parameters of PRN only, which ensures that the (already deployed) targeted net- work does not require any modification for the defense being provided by our framework. This strategy is orthogonal to the existing defense techniques that either update the targeted model using adversarial training to make the networks more robust <ref type="bibr" target="#b17">[18]</ref>, <ref type="bibr" target="#b41">[42]</ref>; or incorporate architectural changes to the targeted network, which may include adding a subnetwork to the model <ref type="bibr" target="#b22">[23]</ref> or tapping into the activations of certain layers to detect the adversarial examples <ref type="bibr" target="#b19">[20]</ref>. Our defense mechanism acts as an external wrapper for the targeted network such that the PRN (and the detector) trained to counter the adversarial attacks can be kept secretive in order refrain from potential counter-counter attacks <ref type="bibr" target="#b2">3</ref> . This is a highly desirable property of defense frameworks in the real-world scenarios. Moosavi-Dezfooli <ref type="bibr" target="#b24">[25]</ref> noted that the universal adversarial perturbations can still exist for a model even after their adversarial training. The proposed framework constitutionally caters for this problem.</p><p>We train the PRN using both clean and adversarial examples to ensure that the image transformation learned by our network is not biased towards the adversarial examples. For training, ℓ i is computed separately with the targeted network for the clean version of the i th training example. The PRN is implemented as 5-ResNet blocks <ref type="bibr" target="#b11">[12]</ref> sandwiched by convolution layers. The 224 × 224 × 3 input image is fed to Conv 3 × 3, stride = 1, feature maps = 64, 'same' convolution; followed by 5 ResNet blocks, where each block consists of two convolution layers with ReLU activations <ref type="bibr" target="#b27">[28]</ref>, resulting in 64 feature maps. The feature maps of the last ResNet block are processed by Conv 3 × 3, stride = 1, feature maps = 16, 'same' convolution; and then Conv 3 × 3, stride = 1, feature maps = 3, 'same' convolution.</p><p>We use the cross-entropy loss <ref type="bibr" target="#b8">[9]</ref> for training the PRN with the help of ADAM optimizer <ref type="bibr" target="#b14">[15]</ref>. The exponential decay rates for the first and the second moment estimates are set to 0.9 and 0.999 respectively. We set the initial learning rate to 0.01, and decay it by 10% after each 1K iterations. We used mini-batch size of 64, and trained the PRN for a given targeted network for at least 5 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Training data</head><p>The PRN is trained using clean images as well as their adversarial counterparts, constructed by adding perturbations to the clean images. We compute the latter by first generating a set of perturbations ρ ∈ P ⊆ R d following Moosavi-Dezfooli et al. <ref type="bibr" target="#b24">[25]</ref>. Their algorithm computes a universal perturbation in an iterative manner. In its inner loop (ran over the training images), the algorithm seeks a minimal norm vector <ref type="bibr" target="#b26">[27]</ref> to fool the network on a given image. The current estimate of ρ is updated by adding to it the sought vector and back-projecting the resultant vector onto the ℓ p ball of radius ξ. The outer loop ensures that the desired fooling ratio is achieved over the complete training set. Generally, the algorithm requires several passes on the training data to achieve an acceptable fooling ratio. We refer to <ref type="bibr" target="#b24">[25]</ref> for further details on the algorithm.</p><p>A PRN trained with more adversarial patterns underlying the training images is expected to perform better. However, it becomes computationally infeasible to generate a large (e.g. &gt; 100) number of perturbations using the abovementioned algorithm. Therefore, we devise a mechanism to efficiently generate synthetic perturbations ρ s ∈ P s ⊆ R d to augment the set of available perturbations for training the PRN. The synthetic perturbations are computed using the set P while capitalizing on the theoretical results of <ref type="bibr" target="#b25">[26]</ref>. To generate the synthetic perturbations, we com-Algorithm 1 ℓ ∞ -norm synthetic perturbation generation Input: Pre-generated perturbation samples P ⊆ R d , number of new samples to be generated η, threshold ξ.</p><formula xml:id="formula_2">Output: Synthetic perturbations P s ⊆ R d 1: set P s = {}; ℓ 2 -threshold = E {||ρ i∈P || 2 } |P| i=1 ; P n = P</formula><note type="other">with ℓ 2 -normalized elements. 2: while |P s | &lt; η do 3: set ρ s = 0 4: while ||ρ s || ∞ &lt; ξ do 5: z ∼ unif(0, 1) ⊙ ξ 6:</note><formula xml:id="formula_3">ρ s = ρ s + (z ⊙ rand ∼ P n ) 7:</formula><p>end while <ref type="bibr">8:</ref> if ||ρ s || 2 ≥ ℓ 2 -threshold then 4 ||ρ s || ∞ ≈ ξ. The procedure for computing the synthetic perturbations that are constrained by their ℓ ∞ -norm is summarized in Algorithm 1. We refer to the supplementary material of the paper for the algorithm to compute the ℓ 2 -norm perturbations.</p><p>To generate a synthetic perturbation, Algorithm 1 searches for ρ s in Ψ + P by taking small random steps in the directions governed by the unit vectors of the elements of P. The random walk continues until the ℓ ∞ -norm of ρ s remains smaller than ξ. The algorithm selects the found ρ s as a valid perturbation if the ℓ 2 -norm of the vector is comparable to the Expected value of the ℓ 2 -norms of the vectors in P. For generating the ℓ 2 -norm perturbations, the corresponding algorithm given in the supplementary material terminates the random walk based on ||ρ s || 2 in line-4, and directly selects the computed ρ s as the desired perturbation. Analyzing the robustness of the deep networks against the universal adversarial perturbations, Moosavi-Dezfooli <ref type="bibr" target="#b25">[26]</ref> showed the existence of shared directions (across different data points) along which a decision boundary induced by a network becomes highly positively curved. Along these vulnerable directions, small universal perturbations exist that can fool the network to change its predictions about the labels of the data points. Our algorithms search for the synthetic perturbations along those directions, whereas the knowledge of the desired directions is borrowed from P. <ref type="figure" target="#fig_3">Fig. 3</ref> exemplifies the typical synthetic perturbations generated by our algorithms for the ℓ 2 and ℓ ∞ norms. It also shows the corresponding closest matches in the set P for the given perturbations. The fooling ratios for the synthetic perturbations is generally not as high as the original ones, nevertheless the values remain in an acceptable range. In our experiments (Section 5), augmenting the training data with the synthetic perturbations consistently helped in early convergence and better performance of the PRN. We note that the acceptable fooling ratios demonstrated by the synthetic perturbations in this work complement the theoretical findings in <ref type="bibr" target="#b25">[26]</ref>. Once the set of synthetic perturbations P s is computed, we construct P * = P P s and use it to perturb the images in our training data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Perturbation detection</head><p>While studying the JPG compression as a mechanism to mitigate the effects of the (image-specific) adversarial perturbations, Dziugaite et al. <ref type="bibr" target="#b4">[5]</ref> also suggested the Discrete Cosine Transform (DCT) as a possible candidate to reduce the effectiveness of the perturbations. Our experiments, reported in supplementary material, show that the DCT based compression can also be exploited to reduce the network fooling ratios under the universal adversarial perturbations. However, it becomes difficult to decide on the required compression rate, especially when it is not known whether the image in question is actually perturbed or not. Unnecessary rectification often leads to degraded performance of the networks on the clean images.</p><p>Instead of using the DCT to remove the perturbations, we exploit it for perturbation detection in our approach. Using the training data that contains both clean and perturbed images, say I </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head><p>We evaluated the performance of our technique by defending CaffeNet <ref type="bibr" target="#b15">[16]</ref>, VGG-F network <ref type="bibr" target="#b3">[4]</ref> and GoogLeNet <ref type="bibr" target="#b36">[37]</ref> against universal adversarial perturbations. The choice of the networks is based on the computational feasibility of generating the perturbations for our experimental protocol. The same framework is applicable to other networks. Following Moosavi-Dezfooli <ref type="bibr" target="#b24">[25]</ref>, we used the ILSVRC 2012 <ref type="bibr" target="#b15">[16]</ref> validation set of 50, 000 images to perform the experiments. Setup: From the available images, we randomly selected 10, 000 samples to generate a total of 50 image-agnostic perturbations for each network, such that 25 of those perturbations were constrained to have ℓ ∞ -norm equal to 10, whereas the ℓ 2 -norm of the remaining 25 was restricted to 2, 000. The fooling ratio of all the perturbations was lowerbounded by 0.8. Moreover, the maximum dot product between any two perturbations of the same type (i.e. ℓ 2 or ℓ ∞ ) was upper bounded by 0.15. This ensured that the constructed perturbations were significantly different from each other, thereby removing any potential bias from our evaluation. From each set of the 25 perturbations, we randomly selected 20 perturbations to be used with the training data, and the remaining 5 were used with the testing data.</p><p>We extended the sets of the training perturbations using the method discussed in Section 4.2, such that there were total 250 perturbations in each extended set, henceforth denoted as P * ∞ and P * 2 . To generate the training data, we first randomly selected 40, 000 samples from the available images and performed 5 corner crops of dimensions 224 × 224 × 3 to generate 200, 000 samples. For creating the adversarial examples with the ℓ 2 -type perturbations, we used the set P * 2 and randomly added perturbations to the images with 0.5 probability. This resulted in ∼ 100, 000 samples each for the clean and the perturbed images, which were used to train the approach for the ℓ 2 -norm perturbations for a given network. We repeated this procedure using the set P * ∞ to separately train it for the ℓ ∞ -type perturbations. Note that, for a given targeted network we performed the training twice to evaluate the performance of our technique for both types of perturbations.</p><p>For a thorough evaluation, two protocols were followed to generate the testing data. Both protocols used the unseen 10, 000 images that were perturbed with the 5 unseen testing perturbations. Notice that the evaluation has been kept doubly-blind to emulate the real-world scenario for a deployed network. For Protocol-A, we used the whole 10, 000 test images and randomly corrupted them with the 5 test perturbations with a 0.5 probability. For the Protocol-B, we chose the subset of the 10, 000 test images that were correctly classified by the targeted network in their clean form, and corrupted that subset with 0.5 probability using the 5 testing perturbations. The existence of both clean and perturbed images with equal probability in our test sets especially ensures a fair evaluation of the detector. Evaluation metric: We used four different metrics for a comprehensive analysis of the performance of our technique. Let I c and I ρ denote the sets containing clean and perturbed test images. Similarly, let I ρ and I ρ/c be the sets containing the test images rectified by PRN, such that all the images in I ρ were perturbed (before passing through the PRN) whereas the images in I ρ/c were similarly perturbed with 0.5 probability, as per our protocol. Let * I be the set comprising the test images such that each image is rectified by the PRN only if it were classified as perturbed by the detector D. Furthermore, let acc(.) be the function computing the prediction accuracy of the target network on a given set of images. The formal definitions of the metrics that we used in our experiments are stated below:</p><formula xml:id="formula_4">1. PRN-gain (%) = acc( Iρ)−acc(Iρ) acc( Iρ)</formula><p>× 100.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">PRN-restoration (%)</head><formula xml:id="formula_5">= acc( I ρ/c ) acc(Ic) × 100.</formula><p>3. Detection rate (%) = Accuracy of D.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Defense rate (%)</head><formula xml:id="formula_6">= acc( * I) acc(Ic) × 100.</formula><p>The names of the metric are in accordance with the semantic notions associated with them. Notice that the PRNrestoration is defined over the rectification of both clean and perturbed images. We do this to account for any loss in the classification accuracy of the targeted network incurred by the rectification of the clean images by the PRN. It was observed in our experiments that unnecessary rectification of the clean images can sometimes lead to a minor (1 -2%) reduction in the classification accuracy of the targeted network. Hence, we used a more strict definition of the restoration by PRN for a more transparent evaluation. This definition is also in-line with our underlying assumption of the practical scenarios where we do not know a prior if the test image is clean or perturbed. Same/Cross-norm evaluation: In <ref type="table" target="#tab_0">Table 1</ref>, we summarize the results of our experiments for defending the GoogLeNet <ref type="bibr" target="#b36">[37]</ref> against the perturbations. The table summarizes two kinds of experiments. For the first kind, we used the same types of perturbations for testing and training. For instance, we used the ℓ 2 -type perturbations for learning the framework components (rectifier + detector) and then  The same conventions will be followed in the similar tables for the other two targeted networks below. Representative examples to visualize the perturbed and rectified images (by the PRN) are shown in <ref type="figure" target="#fig_5">Fig. 4</ref>. Please refer to the supplementary material for more illustrations.</p><p>From <ref type="table" target="#tab_0">Table 1</ref>, we can see that in general, our framework is able to defend the GoogLeNet very successfully against the universal adversarial perturbations that are specifically targeted at this network. The Prot-A captures the performance of our framework when an attacker might have added a perturbation to an unseen image without knowing if the clean image would be correctly classified by the targeted network. The Prot-B represents the case where the perturbation is added to fool the network on an image that it had previously classified correctly. Note that the difference in the performance of our framework for Prot-A and Prot-B is related to the accuracy of the targeted network on clean images. For a network that is 100% accurate on clean images, the results under Prot-A and Prot-B would match exactly. The results would differ more for the less accurate classifiers, as also evident from the subsequent tables.</p><p>In <ref type="table" target="#tab_1">Table 2</ref>, we summarize the performance of our framework for the CaffeNet <ref type="bibr" target="#b15">[16]</ref>. Again, the results demonstrate a good defense against the perturbations. The final Defenserate for the ℓ 2 -type perturbation for Prot-A is 96.4%. Under the used metric definition and the experimental protocol, one interpretation of this value is as follows. With the defense wrapper provided by our framework, the performance of the CaffeNet is expected to be 96.4% of its original performance (in the perfect world of clean images), such that there is an equal chance of every query image to be perturbed or clean <ref type="bibr" target="#b4">5</ref> . Considering that the fooling rate of the network was at least 80% on all the test perturbations used in our experiments, it is a good performance recovery.</p><p>In <ref type="table">Table 3</ref>, the defense summary for the VGG-F network <ref type="bibr" target="#b3">[4]</ref> is reported, which again shows a decent performance of our framework. Interestingly, for both CaffeNet and VGG-F, the existence of the ℓ ∞ -type perturbations in the test images could be detected very accurately by our detector for the 'different test/train perturbation type'. However, it was not the case for the GoogLeNet. We found that for the ℓ ∞ -type perturbations (with ξ = 10) the corresponding ℓ 2 -norm of the perturbations was generally much lower for the GoogLeNet (∼ 2, 400 on avg.) as compared to the CaffeNet and VGG-F (∼ 2, 850 on avg.). This made the detection of the ℓ ∞ -type perturbations more challenging for the GoogLeNet. The dissimilarity in these values indicate that there is a significant difference between the decision boundaries induced by the GoogLeNet and the other two networks, which is governed by the significant architectural differences of the networks. Cross-architecture generalisation: With the above observation, it was anticipated that the cross-network defense performance of our framework would be better for the networks with the (relatively) similar architectures. This prediction was verified by the results of our experiments in Tables 4 and 5. These tables show the performance for ℓ 2 and ℓ ∞ -type perturbations where we used the 'same test/train perturbation type'. The results are reported for protocol A. For the corresponding results under protocol B, we refer to the supplementary material. From these tables, we can conclude that our framework generalizes well across different networks, especially across the networks that have (relatively) similar architectures. We conjecture that the crossnetwork generalization is inherited by our framework from the cross-model generalization of the universal adversarial perturbations. Like our technique, any framework for the defense against these perturbations can be expected to exhibit similar characteristics.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>We presented the first dedicated framework for the defense against universal adversarial perturbations <ref type="bibr" target="#b24">[25]</ref> that not only detects the presence of these perturbations in the images but also rectifies the perturbed images so that the targeted classifier can reliably predict their labels. The proposed framework provides defense to a targeted model without the need of modifying it, which makes our technique highly desirable for the practical cases. Moreover, to prevent the potential counter-counter measures, it provides the flexibility of keeping its 'rectifier' and 'detector' components secretive. We implement the 'rectifier' as a Perturbation Rectifying Network (PRN), whereas the 'detector' is implemented as an SVM trained by exploiting the im- <ref type="table">Table 4</ref>. ℓ2-type cross-network defense (Prot-A): Testing is done using the perturbations generated on the networks in the left-most column. The networks to generate the training perturbations are indicated in the second row.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>PRN-restoration (%)</head><p>VGG-F CaffeNet GoogLeNet VGG-F <ref type="bibr" target="#b3">[4]</ref> 93.2 88.9 81.7 CaffeNet <ref type="bibr" target="#b15">[16]</ref> 91.3 95.1 72.0 GoogLeNet <ref type="bibr" target="#b36">[37]</ref> 84.7 85.9 97.0 Defense rate (%) VGG-F CaffeNet GoogLeNet VGG-F <ref type="bibr" target="#b3">[4]</ref> 95.5 91.5 82.4 CaffeNet <ref type="bibr" target="#b15">[16]</ref> 94.8 96.2 77.3 GoogLeNet <ref type="bibr" target="#b36">[37]</ref> 88.3 87.3 97.4 <ref type="table">Table 5</ref>. ℓ∞-type cross-network defense summary (Prot-A).</p><p>PRN-restoration (%) VGG-F CaffeNet GoogLeNet VGG-F <ref type="bibr" target="#b3">[4]</ref> 90.3 86.9 74.1 CaffeNet <ref type="bibr" target="#b15">[16]</ref> 85.7 93.6 69.3 GoogLeNet <ref type="bibr" target="#b36">[37]</ref> 85.9 83.3 95.6 Defense rate (%) VGG-F CaffeNet GoogLeNet VGG-F <ref type="bibr" target="#b3">[4]</ref> 92.2 88.9 74.8 CaffeNet <ref type="bibr" target="#b15">[16]</ref> 93.5 95.2 73.8 GoogLeNet <ref type="bibr" target="#b36">[37]</ref> 88.4 85.4 96.4</p><p>age transformations performed by the PRN. For an effective training, we also proposed a method to efficiently compute image-agnostic perturbations synthetically. The efficacy of our framework is demonstrated by a successful defense of CaffeNet <ref type="bibr" target="#b15">[16]</ref>, VGG-F network <ref type="bibr" target="#b3">[4]</ref> and GoogLeNet <ref type="bibr" target="#b36">[37]</ref> against the universal adversarial perturbations.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>A perturbation 'detector' D(I ρ/c ) : I ρ/c → [0, 1] and (2) a perturbation 'rectifier' R(I ρ ) : I ρ → I, where I ρ = I c + ρ. The detector determines whether an unseen image I ρ/c is per- turbed or clean. The objective of the rectifier is to com- pute a transformation I of the perturbed image such that P Ic∼ℑc C( I) = C(I c ) ≈ 1. Notice that the rectifier does not seek to improve the prediction of C(.) on the rectified version of the image beyond the classifier's performance on the clean/original image. This ensures stable induction of R(.)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Training schematics: From the clean data, image-agnostic perturbations are computed and augmented with the synthetic perturbations. Both clean and perturbed images are fed to the Perturbation Rectifying Network (PRN). The PRN is learned by attaching it to the first layer of the targeted network such that the parameters of the targeted network are kept frozen during the PRN training. The perturbation detection mechanism extracts discriminative features from the difference between the inputs and outputs of the PRN and learns a binary classifier. To classify an unseen test image I ρ/c , first D(I ρ/c ) = B(F (I ρ/c − R(I ρ/c ))) is computed. If a perturbation is detected then R(I ρ/c ) is used as the input to the classifier C(.) instead of the actual test image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>P</head><label></label><figDesc>= positive orthant of the subspace spanned by the elements of P. (c2) ||ρ s || 2 ≈ E [||ρ|| 2 , ∀ρ ∈ P] and (c3)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Illustration of synthetic perturbations computed for the CaffeNet [16]: The corresponding closest matches in set P are also shown. The dot product between the vectorized perturbations with their closest matches are 0.71 and 0.83 respectively for the ℓ2 and ℓ∞-norm perturbations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>c )) and then learn a binary classifier B(F) → [0, 1] with the data labels denoting the input being 'clean' or 'perturbed'. We implement F(.) to compute the log-absolute values of the 2D-DCT coefficients of the gray-scaled image in the ar-gument, whereas an SVM is learned as B(.). The func- tion D(.) = B(F(.)) forms the detector component of our defense framework. To classify a test image I ρ/c , we first evaluate D(I ρ/c ), and if a perturbation is detected then C(R(I ρ/c )) is evaluated for classification instead of C(I ρ/c ), where C(.) denotes the targeted network classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Representative examples to visualize the perturbed images and their rectified version computed by the PRN. The labels predicted by the networks along the prediction confidence are also given. The examples are provided for the ℓ∞-type perturbations. Please refer to the supplementary material of the paper for more examples.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 .</head><label>1</label><figDesc>Defense summary for the GoogLeNet [37]: The mentioned types of the perturbations (i.e. ℓ2 or ℓ∞) are for the testing data.</figDesc><table>Metric 

Same test/train perturbation type 
Different test/train perturbation type 
ℓ 2 -type 
ℓ ∞ -type 
ℓ 2 -type 
ℓ ∞ -type 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
PRN-gain (%) 
77.0 
77.1 
73.9 
74.2 
76.4 
77.0 
72.6 
73.4 
PRN-restoration (%) 
97.0 
92.4 
95.6 
91.3 
97.1 
92.7 
93.8 
89.3 
Detection rate (%) 
94.6 
94.6 
98.5 
98.4 
92.4 
92.3 
81.3 
81.2 
Defense rate (%) 
97.4 
94.8 
96.4 
93.7 
97.5 
94.9 
94.3 
91.6 

also used the ℓ 2 -type perturbations for testing. The results 
of these experiments are summarized in the left half of the 
table. We performed the 'same test/train perturbation type' 
experiments for both ℓ 2 and ℓ ∞ perturbations, for both test-
ing protocols (denoted as Prot-A and Prot-B in the table). In 
the second kind of experiments, we trained our framework 
on one type of perturbation and tested for the other. The 
right half of the table summarizes the results of those exper-
iments. The mentioned perturbation types in the table are 
for the testing data. </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Defense summary for the CaffeNet [16]: The mentioned types of the perturbations (i.e. ℓ2 or ℓ∞) are for the testing data.Table 3. Defense summary for the VGG-F network [4]: The mentioned types of the perturbations (i.e. ℓ2 or ℓ∞) are for the testing data.</figDesc><table>Metric 

Same test/train perturbation type 
Different test/train perturbation type 
ℓ 2 -type 
ℓ ∞ -type 
ℓ 2 -type 
ℓ ∞ -type 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
PRN-gain (%) 
67.2 
69.0 
78.4 
79.1 
65.3 
66.8 
77.3 
77.7 
PRN-restoration (%) 
95.1 
89.9 
93.6 
88.7 
92.2 
87.1 
91.7 
85.8 
Detection rate (%) 
98.1 
98.0 
97.8 
97.9 
84.2 
84.0 
97.9 
98.0 
Defense rate (%) 
96.4 
93.6 
95.2 
92.5 
93.6 
90.1 
93.2 
90.0 

Metric 

Same test/train perturbation type 
Different test/train perturbation type 
ℓ 2 -type 
ℓ ∞ -type 
ℓ 2 -type 
ℓ ∞ -type 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
Prot-A Prot-B 
PRN-gain (%) 
72.1 
73.3 
84.1 
84.3 
68.3 
69.2 
84.7 
84.8 
PRN-restoration (%) 
93.2 
86.2 
90.3 
83.2 
88.8 
81.2 
91.1 
83.3 
Detection rate (%) 
92.5 
92.5 
98.6 
98.6 
92.5 
92.5 
98.1 
98.1 
Defense rate (%) 
95.5 
91.4 
92.2 
87.9 
90.0 
85.9 
93.7 
89.1 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The choice of the networks is based on the computational feasibility of generating the adversarial perturbations for the evaluation protocol in Section 5. However, our approach is generic in nature.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">A single perturbation that satisfies (1) for any classifier is referred as 'doubly universal' by Moosavi-Dezfooli et al. [25]. We focus on the singly universal perturbations in this work.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">PRN+targeted network are end-to-end differentiable and the joint network can be susceptible to stronger attacks if PRN is not secretive. However, stronger perturbations are also more easily detectable by our detector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">For the perturbations restricted by their ℓ 2 -norm only, this condition is ignored. In that case, (c2) automatically ensures ||ρ s || 2 ≈ ξ.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="5">We emphasize that our evaluation protocols and metrics are carefully designed to analyze the performance in the real-world situations where it is not known apriori whether the query is perturbed or clean.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgement This research was supported by ARC grant DP160101458. The Titan Xp used for this research was donated by NVIDIA Corporation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Threat of adversarial attacks on deep learning in computer vision: A survey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Akhtar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mian</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.00553</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Adversarial transformation networks: Learning to generate adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baluja</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09387</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Towards evaluating the robustness of neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Carlini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wagner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2017 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="39" to="57" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chatfield</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3531</idno>
		<title level="m">Return of the devil in the details: Delving deep into convolutional nets</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">K</forename><surname>Dziugaite</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Roy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.00853</idno>
		<title level="m">A study of the effect of jpg compression on adversarial images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Analysis of classifiers&apos; robustness to adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.02590</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Robustness of classifiers: from adversarial to random noise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1632" to="1640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01101</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Deep learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Delving deep into rectifiers: Surpassing human-level performance on imagenet classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision</title>
		<meeting>the IEEE international conference on computer vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1026" to="1034" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Distilling the knowledge in a neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1503.02531</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.06993</idno>
		<title level="m">Densely connected convolutional networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Adversarial examples in the physical world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.02533</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Delving into transferable adversarial examples and black-box attacks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Song</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02770</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Issaranon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00103</idno>
		<title level="m">Safetynet: Detecting and rejecting adversarial examples robustly</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">No need to worry about adversarial examples in object detection in autonomous vehicles</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sibai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Fabry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.03501</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Foveationbased mechanisms alleviate adversarial examples</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Boix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Roig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Poggio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Zhao</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06292</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Genewein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bischoff</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.04267</idno>
		<title level="m">On detecting adversarial perturbations</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations against semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Metzen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">C</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Fischer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.05712</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Analysis of universal adversarial perturbations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Soatto</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09554</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Deepfool: a simple and accurate method to fool deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S.-M</forename><surname>Moosavi-Dezfooli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fawzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Frossard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2574" to="2582" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th international conference on machine learning (ICML-10)</title>
		<meeting>the 27th international conference on machine learning (ICML-10)</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Deep neural networks are easily fooled: High confidence predictions for unrecognizable images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nguyen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yosinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Clune</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="427" to="436" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Distillation as a defense to adversarial perturbations against deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Swami</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Security and Privacy (SP), 2016 IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="582" to="597" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Deflecting adversarial attacks with pixel deflection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Prakash</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Moran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Garber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Dilillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Storer</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.08926</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Adversarial diversity and hard positive generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rozsa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">M</forename><surname>Rudd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">E</forename><surname>Boult</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="25" to="32" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sabour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Faghri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Fleet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05122</idno>
		<title level="m">Adversarial manipulation of deep representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Accessorize to a crime: Real and stealthy attacks on state-ofthe-art face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Sharif</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhagavatula</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bauer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">K</forename><surname>Reiter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2016 ACM SIGSAC Conference on Computer and Communications Security</title>
		<meeting>the 2016 ACM SIGSAC Conference on Computer and Communications Security</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1528" to="1540" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Rethinking the inception architecture for computer vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wojna</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2818" to="2826" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6199</idno>
		<title level="m">Intriguing properties of neural networks</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Exploring the space of adversarial images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tabacof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valle</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Networks (IJCNN), 2016 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="426" to="433" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Tanay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Griffin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1608.07690</idno>
		<title level="m">A boundary tilting persepective on the phenomenon of adversarial examples</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b41">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b42">
	<monogr>
		<title level="m" type="main">Adversarial examples for semantic segmentation and object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08603</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
