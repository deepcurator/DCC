<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Surface Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ilya</forename><surname>Kostrikov</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhongshi</forename><surname>Jiang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniele</forename><surname>Panozzo</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Zorin</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joan</forename><surname>Bruna</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Courant Institute of Mathematical Sciences</orgName>
								<orgName type="institution" key="instit2">New York University</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="department">Center for Data Science</orgName>
								<orgName type="institution">New York University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Surface Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We study data-driven   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>3D geometry analysis, manipulation and synthesis plays an important role in a variety of applications from engineering to computer animation to medical imaging. Despite the * DP was supported in part by the NSF CAREER award IIS-1652515, a gift from Adobe, and a gift from nTopology.</p><p>† DZ was supported in part by the NSF awards DMS-1436591 and IIS-1320635.</p><p>‡ JB was partially supported by Samsung Electronics (Improving Deep Learning using Latent Structure) and DOA W911NF-17-1-0438. Corresponding author: bruna@cims.nyu.edu vast amount of high-quality 3D geometric data available, data-driven approaches to problems involving complex geometry have yet to become mainstream, in part due to the lack of data representation regularity which is required for traditional convolutional neural network approaches. While in computer vision problems inputs are typically sampled on regular two or three-dimensional grids, surface geometry is represented in a more complex form and, in general, cannot be converted to an image-like format by parametrizing the shape using a single planar chart. Most commonly an irregular triangle mesh is used to represent shapes, capturing its main topological and geometrical properties.</p><p>Similarly to the regular grid case (used for images or videos), we are interested in data-driven representations that strike the right balance between expressive power and sample complexity. In the case of CNNs, this is achieved by exploiting the inductive bias that most computer vision tasks are locally stable to deformations, leading to localized, multiscale, stationary features. In the case of surfaces, we face a fundamental modeling choice between extrinsic versus intrinsic representations. Extrinsic representations rely on the specific embedding of surfaces within a three-dimensional ambient space, whereas intrinsic representations only capture geometric properties specific to the surface, irrespective of its parametrization. Whereas the former offer arbitrary representation power, they are unable to easily exploit inductive priors such as stability to local deformations and invariance to global transformations.</p><p>A particularly simple and popular extrinsic method <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> represents shapes as point clouds in R 3 of variable size, and leverages recent deep learning models that operate on input sets <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>. Despite its advantages in terms of ease of data acquisition (they no longer require a mesh triangulation) and good empirical performance on shape classification and segmentation tasks, one may wonder whether this simplification comes at a loss of precision as one considers more challenging prediction tasks.</p><p>In this paper, we develop an alternative pipeline that applies neural networks directly on triangle meshes, building on geometric deep learning. These models provide datadriven intrinsic graph and manifold representations with inductive biases analogous to CNNs on natural images. Models based on Graph Neural Networks <ref type="bibr" target="#b33">[34]</ref> and their spectral variants <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b21">22]</ref> have been successfully applied to geometry processing tasks such as shape correspondence <ref type="bibr" target="#b26">[27]</ref>. In their basic form, these models learn a deep representation over the discretized surface by combining a latent representation at a given node with a local linear combination of its neighbors' latent representations, and a point-wise nonlinearity. Different models vary in their choice of linear operator and point-wise nonlinearity, which notably includes the graph Laplacian, leading to spectral interpretations of those models.</p><p>Our contributions are three-fold. First, we extend the model to support extrinsic features. More specifically, we exploit the fact that surfaces in R 3 admit a first-order differential operator, the Dirac operator, that is stable to discretization, provides a direct generalization of Laplacianbased propagation models, and is able to detect principal curvature directions <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref>. Next, we prove that the models resulting from either Laplace or Dirac operators are stable to deformations and to discretization, two major sources of variability in practical applications. Last, we introduce a generative model for surfaces based on the variational autoencoder framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>, that is able to exploit nonEuclidean geometric regularity.</p><p>By combining the Dirac operator with input coordinates, we obtain a fully differentiable, end-to-end feature representation that we apply to several challenging tasks. The resulting Surface Networks -using either the Dirac or the Laplacian, inherit the stability and invariance properties of these operators, thus providing data-driven representations with certified stability to deformations. We demonstrate the model efficiency on a temporal prediction task of complex dynamics, based on a physical simulation of elastic shells, which confirms that whenever geometric information (in the form of a mesh) is available, it can be leveraged to significantly outperform point-cloud based models.</p><p>Our main contributions are summarized as follows:</p><p>• We demonstrate that Surface Networks provide accurate temporal prediction of surfaces under complex non-linear dynamics, motivating the use of geometric shape information.</p><p>• We prove that Surface Networks define shape representations that are stable to deformation and to discretization.</p><p>• We introduce a generative model for 3D surfaces based on the variational autoencoder.</p><p>A reference implementation of our algorithm is available at https://github.com/jiangzhongshi/ SurfaceNetworks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Learning end-to-end representations on irregular and non-Euclidean domains is an active and ongoing area of research. <ref type="bibr" target="#b33">[34]</ref> introduced graph neural networks as recursive neural networks on graphs, whose stationary distributions could be trained by backpropagation. Subsequent works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b36">37]</ref> have relaxed the model by untying the recurrent layer weights and proposed several nonlinear updates through gating mechanisms. Graph neural networks are in fact natural generalizations of convolutional networks to non-Euclidean graphs. <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15]</ref> proposed to learn smooth spectral multipliers of the graph Laplacian, albeit with high computational cost, and <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b21">22]</ref> resolved the computational bottleneck by learning polynomials of the graph Laplacian, thus avoiding the computation of eigenvectors and completing the connection with GNNs. We refer the reader to <ref type="bibr" target="#b4">[5]</ref> for an exhaustive literature review on the topic. GNNs are finding application in many different domains. <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b6">7]</ref> develop graph interaction networks that learn pairwise particle interactions and apply them to discrete particle physical dynamics. <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19]</ref> study molecular fingerprints using variants of the GNN architecture, and <ref type="bibr" target="#b11">[12]</ref> further develop the model by combining it with set representations <ref type="bibr" target="#b37">[38]</ref>, showing state-of-the-art results on molecular prediction. The resulting models, so-called Message-Passing Neural Networks, also learn the diffusion operator, which can be seen as generalizations of the Dirac model on general graphs.</p><p>In the context of computer graphics, <ref type="bibr" target="#b24">[25]</ref> developed the first CNN model on meshed surfaces using intrinsic patch representations, and further generalized in <ref type="bibr" target="#b3">[4]</ref> and <ref type="bibr" target="#b26">[27]</ref>. This last work allows for flexible representations via the socalled pseudo-coordinates and obtains state-of-the-art results on 3D shape correspondence, although it does not easily encode first-order differential information. These intrinsic models contrast with Euclidean models such as <ref type="bibr" target="#b41">[42,</ref><ref type="bibr" target="#b39">40]</ref>, that have higher sample complexity, since they need to learn the underlying invariance of the surface embedding. Point-cloud based models are increasingly popular to model 3d objects due to their simplicity and versatility. <ref type="bibr" target="#b29">[30,</ref><ref type="bibr" target="#b30">31]</ref> use set-invariant representations from <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref> to solve shape segmentation and classification tasks. More recently, <ref type="bibr" target="#b23">[24]</ref> proposes to learn surface convolutional network from a canonical representation of planar flat-torus, with excellent performance on shape segmentation and classification, although such canonical representations may introduce exponential scale changes that can introduce instabilities. Finally, <ref type="bibr" target="#b10">[11]</ref> proposes a point-cloud generative model for 3D shapes, that incorporates invariance to point permutations, but does not encode geometrical information as our shape generative model. Learning variational deformations is an important problem for graphics applications, since it enables negligible and fixed per-frame cost <ref type="bibr" target="#b28">[29]</ref>, but it is currently limited to 2D deformations using point handles.</p><p>In constrast, our method easily generalizes to 3D and learns dynamic behaviours.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Surface Networks</head><p>This section presents our surface neural network model and its basic properties. We start by introducing the problem setup and notations using the Laplacian formalism (Section 3.1), and then introduce our model based on the Dirac operator (Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Laplacian Surface Networks</head><p>Our first goal is to define a trainable representation of discrete surfaces. Let M = {V, E, F } be a triangular mesh, where V = (v i ∈ R 3 ) i≤N contains the node coordinates, E = (e i,j ) corresponds to edges, and F is the set of triangular faces. We denote as ∆ the discrete Laplace-Beltrami operator (we use the popular cotangent weights formulation, see <ref type="bibr" target="#b4">[5]</ref> for details).</p><p>This operator can be interpreted as a local, linear highpass filter in M that acts on signals x ∈ R d×|V | defined on the vertices as a simple matrix multiplicationx = ∆x. By combining ∆ with an all-pass filter and learning generic linear combinations followed by a point-wise nonlinearity, we obtain a simple generalization of localized convolutional operators in M that update a feature map from layer k to layer k + 1 using trainable parameters A k and B k :</p><formula xml:id="formula_0">x k+1 = ρ A k ∆x k + B k x k , A k , B k ∈ R d k+1 ×d k . (1)</formula><p>By observing that the Laplacian itself can be written in terms of the graph weight similarity by diagonal renormalization, this model is a specific instance of the graph neural network <ref type="bibr" target="#b33">[34,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b21">22]</ref> and a generalization of the spectrumfree Laplacian networks from <ref type="bibr" target="#b8">[9]</ref>. As shown in these previous works, convolutional-like layers (1) can be combined with graph coarsening or pooling layers.</p><p>In contrast to general graphs, meshes contain a lowdimensional Euclidean embedding that contains potentially useful information in many practical tasks, despite being extrinsic and thus not invariant to the global position of the surface. A simple strategy to strike a good balance between expressivity and invariance is to include the node canonical coordinates as input channels to the network:</p><formula xml:id="formula_1">x 1 := V ∈ R |V |×3</formula><p>. The mean curvature can be computed by applying the Laplace operator to the coordinates of the vertices:</p><formula xml:id="formula_2">∆x 1 = −2Hn ,<label>(2)</label></formula><p>where H is the mean curvature function and n(u) is the normal vector of the surface at point u. As a result, the Laplacian neural model (1) has access to mean curvature and normal information. Feeding Euclidean embedding coordinates into graph neural network models is related to the use of generalized coordinates from <ref type="bibr" target="#b26">[27]</ref>. By cascading K layers of the form (1) we obtain a representation Φ ∆ (M) that contains generic features at each node location. When the number of layers K is of the order of diam(M), the diameter of the graph determined by M, then the network is able to propagate and aggregate information across the whole surface. Equation <ref type="bibr" target="#b1">(2)</ref> illustrates that a Laplacian layer is only able to extract isotropic high-frequency information, corresponding to the mean variations across all directions. Although in general graphs there is no well-defined procedure to recover anisotropic local variations, in the case of surfaces some authors ( <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b26">27]</ref>) have considered anisotropic extensions. We describe next a particularly simple procedure to increase the expressive power of the network using a related operator from quantum mechanics: the Dirac operator, that has been previously used successfully in the context of surface deformation <ref type="bibr" target="#b7">[8]</ref> and shape analysis <ref type="bibr" target="#b15">[16]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Dirac Surface Networks</head><p>The Laplace-Beltrami operator ∆ is a second-order differential operator, constructed as ∆ = −div∇ by combining the gradient (a first-order differential operator) with its adjoint, the divergence operator. In an Euclidean space, one has access to these first-order differential operators separately, enabling oriented high-pass filters.</p><p>For convenience, we embed R 3 to the imaginary quaternion space Im(H) (see Appendix A in the Suppl. Material for details). The Dirac operator is then defined as a matrix D ∈ H |F |×|V | that maps (quaternion) signals on the nodes to signals on the faces. In coordinates,</p><formula xml:id="formula_3">D f,j = −1 2|A f | e j , f ∈ F, j ∈ V ,</formula><p>where e j is the opposing edge vector of node j in the face f , and A f is the area (see Appendix A) using counterclockwise orientations on all faces. To apply the Dirac operator defined in quaternions to signals in vertices and faces defined in real numbers, we write the feature vectors as quaternions by splitting them into chunks of 4 real numbers representing the real and imaginary parts of a quaternion; see Appendix A. Thus, we always work with feature vectors with dimensionalities that are multiples of 4. The Dirac operator provides firstorder differential information and is sensitive to local orientations. Moreover, one can verify <ref type="bibr" target="#b7">[8]</ref> that</p><formula xml:id="formula_4">Re D * D = ∆ ,</formula><p>where D * is the adjoint operator of D in the quaternion space (see Appendix A). The adjoint matrix can be computed as</p><formula xml:id="formula_5">D * = M −1 V D H M F where D</formula><p>H is a conjugate transpose of D and M V , M F are diagonal mass matrices with one third of areas of triangles incident to a vertex and face areas respectively.</p><p>The Dirac operator can be used to define a new neural surface representation that alternates layers with signals defined over nodes with layers defined over faces. Given a d-dimensional feature representation over the nodes x k ∈ R d×|V | , and the faces of the mesh, y k ∈ R d×|F | , we define a d ′ -dimensional mapping to a face representation as</p><formula xml:id="formula_6">y k+1 = ρ C k Dx k + E k y k , C k , E k ∈ R d k+1 ×d k ,<label>(3)</label></formula><p>where C k , E k are trainable parameters. Similarly, we define the adjoint layer that maps back to ad-dimensional signal over nodes as</p><formula xml:id="formula_7">x k+1 = ρ A k D * y k+1 + B k x k , A k , B k ∈ R d k+1 ×d k ,<label>(4)</label></formula><p>where A k , B k are trainable parameters.</p><p>A surface neural network layer is thus determined by parameters {A, B, C, E} using equations <ref type="formula" target="#formula_6">(3)</ref> and <ref type="formula" target="#formula_7">(4)</ref> to define x k+1 ∈ R d k+1 ×|V | . We denote by Φ D (M) the mesh representation resulting from applying K such layers (that we assume fixed for the purpose of exposition).</p><p>The Dirac-based surface network is related to edge feature transforms proposed on general graphs in <ref type="bibr" target="#b11">[12]</ref>, although these edge measurements cannot be associated with derivatives due to lack of proper orientation. In general graphs, there is no notion of square root of ∆ that recovers oriented first-order derivatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Stability of Surface Networks</head><p>Here we describe how Surface Networks are geometrically stable, because surface deformations become additive noise under the model. Given a continuous surface S ⊂ R 3 or a discrete mesh M, and a smooth deformation field τ : R 3 → R 3 , we are particularly interested in two forms of stability:</p><p>• Given a discrete mesh M and a certain non-rigid deformation τ acting on M, we want to certify that</p><formula xml:id="formula_8">Φ(M) − Φ(τ (M)</formula><p>) is small if ∇τ (∇τ ) * − I is small, i.e when the deformation is nearly rigid; see Theorem 4.1.</p><p>• Given two discretizations M 1 and M 2 of the same underlying surface S, we would like to control</p><formula xml:id="formula_9">Φ(M 1 ) − Φ(M 2 )</formula><p>in terms of the resolution of the meshes; see Theorem 4.2.</p><p>These stability properties are important in applications, since most tasks we are interested in are stable to deformation and to discretization. We shall see that the first property is a simple consequence of the fact that the mesh Laplacian and Dirac operators are themselves stable to deformations. The second property will require us to specify under which conditions the discrete mesh Laplacian ∆ M converges to the Laplace-Beltrami operator ∆ S on S. Unless it is clear from the context, in the following ∆ will denote the discrete Laplacian. </p><formula xml:id="formula_10">( · ) is non-expansive (|ρ(z) − ρ(z ′ )| ≤ |z − z ′ |). Then (a) Φ ∆ (M; x) − Φ ∆ (M; x ′ ) ≤ α ∆ x − x ′</formula><p>, where α ∆ depends only on the trained weights and the mesh.  </p><formula xml:id="formula_11">(b) Φ D (M; x) − Φ D (M; x ′ ) ≤ α D x − x ′ ,</formula><formula xml:id="formula_12">→ τ (u). Then Φ ∆ (M; x) − Φ ∆ (τ (M); x) ≤ β ∆ |∇τ | ∞ x ,</formula><p>where β ∆ is independent of τ and x.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(d) Denote by |∇τ |</head><formula xml:id="formula_13">∞ := sup u ∇τ (u) − 1 . Then Φ D (M; x) − Φ D (τ (M); x) ≤ β D |∇τ | ∞ x , where β D is independent of τ and x.</formula><p>Properties (a) and (b) are not specific to surface representations, and are a simple consequence of the non-expansive property of our chosen nonlinearities. The constant α is controlled by the product of ℓ 2 norms of the network weights at each layer and the norm of the discrete Laplacian operator. Properties (c) and (d) are based on the fact that the Laplacian and Dirac operators are themselves stable to deformations, a property that depends on two key aspects: first, the Laplacian/Dirac is localized in space, and next, that it is a high-pass filter and therefore only depends on relative changes in position.</p><p>One caveat of Theorem 4.1 is that the constants appearing in the bounds depend upon a bandwidth parameter given by the reciprocal of triangle areas, which increases as the size of the mesh increases. This corresponds to the fact that the spectral radius of ∆ M diverges as the mesh size N increases.</p><p>In order to overcome this problematic asymptotic behavior, it is necessary to exploit the smoothness of the signals incoming to the surface network. This can be measured with Sobolev norms defined using the spectrum of the Laplacian operator. Given a mesh M of N nodes approximating an underlying surface S, and its associated cotangent Laplacian ∆ M , consider the spectral decomposition of ∆ M (a symmetric, positive definite operator):</p><formula xml:id="formula_14">∆ M = k≤N λ k e k e T k , e k ∈ R N , 0 ≤ λ 1 ≤ λ 2 · · · ≤ λ N .</formula><p>Under normal uniform convergence 1 <ref type="bibr" target="#b38">[39]</ref>, the spectrum of ∆ M converges to the spectrum of the Laplace-Beltrami operator ∆ S of S. If S is bounded, it is known from the Weyl law <ref type="bibr" target="#b40">[41]</ref> that there exists γ &gt; 0 such that k</p><formula xml:id="formula_15">−γ(S) λ −1</formula><p>k , so the eigenvalues λ k do not grow too fast. The smoothness of a signal x ∈ R |V |×d defined in M is captured by how fast its spectral decompositionx(k) = e</p><formula xml:id="formula_16">T k x ∈ R d decays [36]. We define x 2 H := k λ(k) 2 x(k) 2</formula><p>is Sobolev norm, and β(x, S) &gt; 1 as the largest rate such that its spectral decomposition coefficients satisfy</p><formula xml:id="formula_17">x(k) k −β , (k → ∞) .<label>(5)</label></formula><p>If x ∈ R |V |×d is the input to the Laplace Surface Network of R layers, we denote by (β 0 , β 1 , . . . , β R−1 ) the smoothness rates of the feature maps x (r) defined at each layer r ≤ R.  </p><formula xml:id="formula_18">Φ ∆ (x 1 ; M N )−Φ ∆ (x 2 ; M N ) 2 ≤ C(β) x 1 −x 2 h(β) ,<label>(6)</label></formula><formula xml:id="formula_19">Φ ∆ (x; M N ) − Φ ∆ (x; τ (M N )) ≤ C|∇τ | ∞ h(β) ,</formula><p>where C does not depend upon N .</p><formula xml:id="formula_20">(c) Let M and M ′ be N -point discretizations of S, If max(d(M, S), d(M ′ , S)) ≤ ǫ, then Φ ∆ (M; x) − Φ ∆ (M ′ , x ′ ) ≤ Cǫ h(β) , where C is independent of N .</formula><p>This result ensures that if we use as generator of the SN an operator that is consistent as the mesh resolution increases, the resulting surface representation is also consistent. Although our present result only concerns the Laplacian, the Dirac operator also has a well-defined continuous counterpart <ref type="bibr" target="#b7">[8]</ref> that generalizes the gradient operator in quaternion space. Also, our current bounds depend explicitly upon the smoothness of feature maps across different layers, which may be controlled in terms of the original signal if one considers nonlinearities that demodulate the signal, such as ρ(x) = |x| or ρ(x) = ReLU(x). These extensions are left for future work. Finally, a specific setup that we use in experiments is to use as input signal the canonical coordinates of the mesh M. In that case, an immediate application of the previous theorem yields </p><formula xml:id="formula_21">Φ(M)−Φ(τ (M)) ≤ κ max(|∇τ | ∞ , ∇ 2 τ ) h(β) . (7)</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Generative Surface Models</head><p>State-of-the-art generative models for images, such as generative adversarial networks <ref type="bibr" target="#b31">[32]</ref>, pixel autoregressive networks <ref type="bibr" target="#b27">[28]</ref>, or variational autoencoders <ref type="bibr" target="#b20">[21]</ref>, exploit the locality and stationarity of natural images in their probabilistic models, in the sense that the model satisfies p θ (x) ≈ p θ (x τ ) by construction, where x τ is a small deformation of a given input x. This property is obtained via encoders and decoders with a deep convolutional structure. We intend to exploit similar geometric stability priors with SNs, owing to their stability properties described in Section 4. A mesh generative model contains two distinct sources of randomness: on the one hand, the randomness associated with the underlying continuous surface, which corresponds to shape variability; on the other hand, the randomness of the discretization of the surface. Whereas the former contains the essential semantic information, the latter is not informative, and to some extent independent of the shape identity. We focus initially on meshes that can be represented as a depth map over an (irregular) 2D mesh, referred as height-field meshes in the literature. That is, a mesh M = (V, E, F ) is expressed as (M, f (M)), whereM = (Ṽ ,Ẽ,F ) is now a 2D mesh and f :Ṽ → R is a depth-map encoding the original node locations V , as shown in <ref type="figure" target="#fig_4">Figure 1</ref>.</p><p>In this work, we consider the variational autoencoder framework <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b32">33]</ref>. It considers a mixture model of the form</p><formula xml:id="formula_22">p(M) = p θ (M | h)p 0 (h)dh , where h ∈ R</formula><p>| S| is a vector of latent variables. We train this model by optimizing the variational lower bound of the data log-likelihood:</p><formula xml:id="formula_23">min θ,ψ 1 L l≤L −E h∼q ψ (h | M l ) log p θ (M l | h)+DKL(q ψ (h | M l ) || p0(h)) .<label>(8)</label></formula><p>We thus need to specify a conditional generative model p θ (M | h), a prior distribution p 0 (h) and a variational approximation to the posterior q ψ (h | M), where θ and ψ denote respectively generative and variational trainable parameters. Based on the height-field representation, <ref type="figure" target="#fig_2">Figure 2</ref>. A single ResNet-v2 block used for Laplace, Average Pooling (top) and Dirac models (bottom). The green boxes correspond to the linear operators replacing convolutions in regular domains. We consider Exponential Linear Units (ELU) activations (orange), Batch Normalization (blue) and '1 × 1' convolutions (red) containing the trainable parameters; see Eqs (1, 3 and 4). We slightly abuse language and denote by x k+1 the output of this 2-layer block.</p><p>we choose for simplicity a separable model of the form</p><formula xml:id="formula_24">p θ (M | h) = p θ (f | h,M) · p(M) , whereM ∼ p(M)</formula><note type="other">is a homogeneous Poisson point process, and f</note><formula xml:id="formula_25">∼ p θ (f | h,M)</formula><p>is a normal distribution with mean and isotropic covariance parameters given by a SN:</p><formula xml:id="formula_26">p θ (f | h,M) = N (µ(h,M), σ 2 (h,M)1) , with [µ(h,M), σ 2 (h,M)] = Φ D (M ; h)</formula><p>. The generation step thus proceeds as follows. We first sample a 2D mesh M independent of the latent variable h, and then sample a depth field overM conditioned on h from the output of a decoder network Φ D (M ; h). Finally, the variational family q ψ is also a Normal distribution whose parameters are obtained from an encoder Surface Neural Network whose last layer is a global pooling that removes the spatial localization:</p><formula xml:id="formula_27">q ψ (h | M) = N (μ,σ 2 1) , with [μ,σ] =Φ D (M) .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>For experimental evaluation, we compare models built using ResNet-v2 blocks <ref type="bibr" target="#b13">[14]</ref>, where convolutions are replaced with the appropriate operators (see <ref type="figure" target="#fig_2">Fig. 2</ref>): (i) a point cloud based model from <ref type="bibr" target="#b36">[37]</ref> that aggregates global information by averaging features in the intermediate layers and distributing them to all nodes; (ii) a Laplacian Surface network with input canonical coordinates; (iii) a Dirac Surface Network model. We report experiments on generative models using an unstructured variant of MNIST digits (Section 6.1), and on temporal prediction under non-rigid deformation models (Section 6.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">MeshMNIST</head><p>For this task, we construct a MeshMNIST database with only height-field meshes (Sec. 5). First, we sample points on a 2D plane ([0, 27] × [0, 27]) with Poisson disk sampling with r = 1.0, which roughly generates 500 points, and apply a Delaunay triangulation to these points. We then overlay the triangulation with the original MNIST images and assign to each point a z coordinate bilinearly interpolating the grey-scale value. Thus, the procedure allows us to define a sampling process over 3D height-field meshes.</p><p>We used VAE models with decoders and encoders built using 10 ResNet-v2 blocks with 128 features. The encoder converts a mesh into a latent vector by averaging output of the last ResNet-v2 block and applying linear transformations to obtain mean and variance, while the decoder takes a latent vector and a 2D mesh as input (corresponding to a specific 3D mesh) and predicts offsets for the corresponding locations. We keep variance of the decoder as a trainable parameter that does not depend on input data. We trained the model for 75 epochs using Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with learning rate 10 −3 , weight decay 10 −5 and batch size 32. <ref type="figure" target="#fig_5">Figures 3,4</ref> illustrate samples from the model. The geometric encoder is able to leverage the local translation invariance of the data despite the irregular sampling, whereas the geometric decoder automatically adapts to the specific sampled grid, as opposed to set-based generative models.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Spatio-Temporal Predictions</head><p>One specific task we consider is temporal predictions of non-linear dynamics. Given a sequence of frames X = X 1 , X 2 , . . . , X n , the task is to predict the following frames</p><formula xml:id="formula_28">Y = Y 1 = X n+1 , Y 2 , . . . , Y m = X n+m .</formula><p>As in <ref type="bibr" target="#b25">[26]</ref>, we use a simple non-recurrent model that takes a concatenation of input frames X and predicts a concatenation of frames Y . We condition on n = 2 frames and predict the next . m = 40 frames. In order to generate data, we first extracted 10k patches from the MPI-Faust dataset <ref type="bibr" target="#b2">[3]</ref>, by selecting a random point and growing a topological sphere of radius 15 edges (i.e. the 15-ring of the point). For each patch, we generate a sequence of 50 frames by randomly rotating it and letting it fall to the ground. We consider the mesh a thin elastic shell, and we simulate it using the As-Rigid-As-Possible technique <ref type="bibr" target="#b34">[35]</ref>, with additional gravitational forces <ref type="bibr" target="#b16">[17]</ref>. Libigl <ref type="bibr" target="#b17">[18]</ref> has been used for the mesh processing tasks. Sequences with patches from the first 80 subjects were used in training, while the 20 last subjects were used for testing. The dataset and the code are available on request. We restrict our experiments to temporal prediction tasks that are deterministic when conditioned on several initial frames. Thus, we can train models by minimizing smooth-L1 loss <ref type="bibr" target="#b12">[13]</ref> between target frames and output of our models. We used models with 15 ResNet-v2 blocks with 128 output features each. In order to cover larger context for Dirac and Laplace based models, we alternate these blocks with Average Pooling blocks. We predict offsets to the last conditioned frame and use the corresponding Laplace and Dirac operators. Thus, the models take 6-dimensional inputs and produce 120-dimensional outputs. We trained all models using the Adam optimizer <ref type="bibr" target="#b19">[20]</ref> with learning rate 10 −3 , weight decay 10 −5 , and batch size 32. After 60k steps we decreased the learning rate by a factor of 2 every 10k steps. The models were trained for 110k steps in overall. <ref type="table">Table 1</ref>  prediction models at specific frames. The set-to-set model <ref type="bibr" target="#b37">[38,</ref><ref type="bibr" target="#b36">37]</ref>, corresponding to a point-cloud representation used also in <ref type="bibr" target="#b29">[30]</ref>, already performs reasonably well on the task, even if the visual difference is noticeable. Nevertheless, the gap between this model and Laplace-/Dirac-based models is significant, both visually and quantitatively. Diracbased model outperforms Laplace-based model despite the smaller receptive field. Videos comparing the performance of different models are available in the additional material. <ref type="figure" target="#fig_7">Figure 6</ref> illustrates the effect of replacing Laplace by Dirac in the formulation of the SN. Laplacian-based models, since they propagate information using an isotropic operator, have more difficulties at resolving corners and pointy  structures than the Dirac operator, that is sensitive to principal curvature directions. However, the capacity of Laplace models to exploit the extrinsic information only via the input coordinates is remarkable and more computationally efficient than the Dirac counterpart. <ref type="figure" target="#fig_8">Figures 7 and 8</ref> overlay the prediction error and compare Laplace against Dirac and PointCloud against Dirac respectively. They confirm first that SNs outperform the point-cloud based model, which often produce excessive flattening and large deformations, and next that first-order Dirac operators help resolve areas with high directional curvature. We refer to the supplementary material for additional qualitative results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>We have introduced Surface Networks, a deep neural network that is designed to naturally exploit the nonEuclidean geometry of surfaces. We have shown how a first-order differential operator (the Dirac operator) can detect and adapt to geometric features beyond the local mean curvature, the limit of what Laplacian-based methods can exploit. This distinction is important in practice, since areas with high directional curvature are perceptually important, as shown in the experiments. That said, the Dirac operator comes at increased computational cost due to the quaternion calculus, and it would be interesting to instead learn the operator, akin to recent Message-Passing NNs <ref type="bibr" target="#b11">[12]</ref> and explore whether Dirac is recovered.</p><p>Whenever the data contains good-quality meshes, our experiments demonstrate that using intrinsic geometry offers vastly superior performance to point-cloud based models. While there are not many such datasets currently available, we expect them to become common in the next years, as scanning and reconstruction technology advances and 3D sensors are integrated in consumer devices. SNs provide efficient inference, with predictable runtime, which makes them appealing across many areas of computer graphics, where a fixed, per-frame cost is required to ensure a stable framerate, especially in VR applications. Our future plans include applying Surface Networks precisely to having automated, data-driven mesh processing, and generalizing the generative model to arbitrary meshes, which will require an appropriate multi-resolution pipeline.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>where α D depends only on the trained weights and the mesh.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>( c )</head><label>c</label><figDesc>Let |∇τ | ∞ := sup u ∇τ (u)(∇τ (u)) * − 1 , where ∇τ (u) is the Jacobian matrix of u</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Theorem 4. 2</head><label>2</label><figDesc>Consider a surface S and a finite-mesh ap- proximation M N of N points, and Φ ∆ a Laplace Sur- face Network with parameters {(A r , B r )} r≤R . Denote by d(S, M N ) the uniform normal distance, and let x 1 , x 2 be piece-wise polyhedral approximations ofx(t), t ∈ S in M N , with x H(S) &lt; ∞. Assume x (r) H(S) &lt; ∞ for r ≤ R. (a) If x 1 , x 2 are two functions such that the R feature maps x (r) l have rates (β 0 , β 1 , . . . , β R−1 ), then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>2 , and where C(β) does not depend upon N . (b) If τ is a smooth deformation field, then</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. Height-Field Representation of surfaces. A 3D mesh M ⊂ R 3 (right) is expressed in terms of a "sampling" 2D irregular meshM ⊂ R 2 (left) and a depth scalar field f :M → R over M (center).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Samples generated for the same latent variable and different triangulations. The learned representation is independent of discretization/triangulation (Poisson disk sampling with p=1.5).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Meshes from the dataset (first five). And meshes generated by our model (last five).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Dirac-based model visually outperforms Laplace-based models in the regions of high mean curvature.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. From left to right: PointCloud (set2set), ground truth and Dirac based model. Color corresponds to mean squared error between ground truth and prediction: green -smaller error, redlarger error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. From left to right: Laplace, ground truth and Dirac based model. Color corresponds to mean squared error between ground truth and prediction: green -smaller error, red -larger error.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Theorem 4.1 Let M be a N -node mesh and x, x ′ ∈ R |V |×d be input signals defined on the nodes. Assume the nonlinearity ρ</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>Figure 5. Qualitative comparison of different models. We plot 30th predicted frames correspondingly for two sequences in the test set. Boxes indicate distinctive features. For larger crops, see Figure 6</figDesc><table>Model 

Receptive field Number of parameters Smooth L1-loss (mean per sequence (std)) 
MLP 
1 
519672 
64.56 (0.62) 
PointCloud 
-
1018872 
23.64 (0.21) 
Laplace 
16 
1018872 
17.34 (0.52) 
Dirac 
8 
1018872 
16.84 (0.16) 

Table 1. Evaluation of different models on the temporal task 

Ground Truth 
MLP 
PointCloud 
Laplace 
Dirac 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">which controls how the normals of the mesh align with the surface normals; see [39].</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Anisotropic Laplace-Beltrami operators for shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andreux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Aubry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Cremers</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. NORDIA</title>
		<meeting>NORDIA</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Interaction networks for learning about objects, relations and physics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4502" to="4510" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Faust: Dataset and evaluation for 3d mesh registration</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bogo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Romero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Loper</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Black</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="3794" to="3801" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning shape correspondence with anisotropic convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08097</idno>
		<title level="m">Geometric deep learning: going beyond euclidean data</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<title level="m">Spectral networks and locally connected networks on graphs. Proc. ICLR</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A compositional object-based approach to learning physical dynamics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">B</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ullman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ICLR</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Spin transformations of discrete surfaces</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Crane</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Pinkall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Schröder</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In ACM Transactions on Graphics (TOG). ACM</title>
		<imprint>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on graphs with fast localized spectral filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Defferrard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bresson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Convolutional networks on graphs for learning molecular fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Maclaurin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">A point set generation network for 3d object reconstruction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Fan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00603</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gilmer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Schoenholz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">F</forename><surname>Riley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Dahl</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.01212</idno>
		<title level="m">Neural message passing for quantum chemistry</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Henaff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bruna</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1506.05163</idno>
		<title level="m">Deep convolutional networks on graph-structured data</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">A dirac operator for extrinsic shape analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">C</forename><surname>Hsueh-Ti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Derek</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alec</forename><surname>Jacobson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Computer Graphics Forum</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Algorithms and Interfaces for Real-Time Deformation of 2D and 3D Shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
			<pubPlace>ETH, Zürich</pubPlace>
		</imprint>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">A simple C++ geometry processing library</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jacobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Panozzo</surname></persName>
		</author>
		<ptr target="http://libigl.github.io/libigl/.7" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Molecular graph convolutions: moving beyond fingerprints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Kearnes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mccloskey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Berndl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Pande</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Riley</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of computer-aided molecular design</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representation</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.6114</idno>
		<title level="m">Auto-encoding variational bayes</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Semi-supervised classification with graph convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.02907</idno>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Tarlow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Brockschmidt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05493</idno>
		<title level="m">Gated graph sequence neural networks</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Convolutional neural networks on surfaces via seamless toric covers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Maron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Galun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Aigerman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Trope</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dym</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Yumer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">SIGGRAPH</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Geodesic convolutional neural networks on riemannian manifolds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bronstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vandergheynst</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE international conference on computer vision workshops</title>
		<meeting>the IEEE international conference on computer vision workshops</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Deep multi-scale video prediction beyond mean square error</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.05440</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Monti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boscaini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Masci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Rodolà</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Svoboda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Bronstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08402</idno>
		<title level="m">Geometric deep learning on graphs and manifolds using mixture model cnns</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">V</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.06759</idno>
		<title level="m">Pixel recurrent neural networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Simple approximations of planar deformation operators</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Poranne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lipman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ETHZ</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">Technical report</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Pointnet: Deep learning on point sets for 3d classification and segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.00593</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Pointnet++: Deep hierarchical feature learning on point sets in a metric space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">R</forename><surname>Qi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Yi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Guibas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.02413</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1505.05770</idno>
		<title level="m">Variational inference with normalizing flows</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">The graph neural network model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Scarselli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Gori</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Tsoi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hagenbuchner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Monfardini</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">3</biblScope>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">As-rigid-as-possible surface modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Sorkine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Alexa</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Symposium on Geometry processing</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="volume">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Spectral graph theory and its applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">A</forename><surname>Spielman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Foundations of Computer Science, 2007. FOCS&apos;07. 48th Annual IEEE Symposium on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="29" to="38" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Learning multiagent communication with backpropagation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="2244" to="2252" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Order matters: Sequence to sequence for sets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06391</idno>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Convergence of the cotangent formula: An overview</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wardetzky</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Discrete Differential Geometry</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="275" to="286" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Dense human body correspondences using convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ceylan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Vouga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1544" to="1553" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Über die asymptotische verteilung der eigenwerte</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nachrichten von der Gesellschaft der Wissenschaften zu Göttingen, Mathematisch-Physikalische Klasse</title>
		<imprint>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="110" to="117" />
			<date type="published" when="1911" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">3d shapenets: A deep representation for volumetric shapes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1912" to="1920" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
