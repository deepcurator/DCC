<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:42+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Heusel</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">LIT AI Lab &amp; Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Ramsauer</surname></persName>
							<email>ramsauer@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">LIT AI Lab &amp; Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Thomas</forename><surname>Unterthiner</surname></persName>
							<email>unterthiner@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">LIT AI Lab &amp; Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bernhard</forename><surname>Nessler</surname></persName>
							<email>nessler@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">LIT AI Lab &amp; Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sepp</forename><surname>Hochreiter</surname></persName>
							<email>hochreit@bioinf.jku.at</email>
							<affiliation key="aff0">
								<orgName type="department">LIT AI Lab &amp; Institute of Bioinformatics</orgName>
								<orgName type="institution">Johannes Kepler University Linz</orgName>
								<address>
									<postCode>A-4040</postCode>
									<settlement>Linz</settlement>
									<country key="AT">Austria</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GANs Trained by a Two Time-Scale Update Rule Converge to a Local Nash Equilibrium</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Generative Adversarial Networks (GANs) excel at creating realistic images with complex models for which maximum likelihood is infeasible. However, the convergence of GAN training has still not been proved. We propose a two time-scale update rule (TTUR) for training GANs with stochastic gradient descent on arbitrary GAN loss functions. TTUR has an individual learning rate for both the discriminator and the generator. Using the theory of stochastic approximation, we prove that the TTUR converges under mild assumptions to a stationary local Nash equilibrium. The convergence carries over to the popular Adam optimization, for which we prove that it follows the dynamics of a heavy ball with friction and thus prefers flat minima in the objective landscape. For the evaluation of the performance of GANs at image generation, we introduce the 'Fréchet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments, TTUR improves learning for DCGANs and Improved Wasserstein GANs (WGAN-GP) outperforming conventional GAN training on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Generative adversarial networks (GANs) <ref type="bibr" target="#b15">[16]</ref> have achieved outstanding results in generating realistic images <ref type="bibr" target="#b42">[42,</ref><ref type="bibr" target="#b31">31,</ref><ref type="bibr" target="#b25">25,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b3">4]</ref> and producing text <ref type="bibr" target="#b21">[21]</ref>. GANs can learn complex generative models for which maximum likelihood or a variational approximations are infeasible. Instead of the likelihood, a discriminator network serves as objective for the generative model, that is, the generator. GAN learning is a game between the generator, which constructs synthetic data from random variables, and the discriminator, which separates synthetic data from real world data. The generator's goal is to construct data in such a way that the discriminator cannot tell them apart from real world data. Thus, the discriminator tries to minimize the synthetic-real discrimination error while the generator tries to maximize this error. Since training GANs is a game and its solution is a Nash equilibrium, gradient descent may fail to converge <ref type="bibr" target="#b44">[44,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b18">18]</ref>. Only local Nash equilibria are found, because gradient descent is a local optimization method. If there exists a local neighborhood around a point in parameter space where neither the generator nor the discriminator can unilaterally decrease their respective losses, then we call this point a local Nash equilibrium.</p><p>To characterize the convergence properties of training general GANs is still an open challenge <ref type="bibr" target="#b17">[17,</ref><ref type="bibr" target="#b18">18]</ref>. For special GAN variants, convergence can be proved under certain assumptions <ref type="bibr" target="#b34">[34,</ref><ref type="bibr" target="#b20">20,</ref><ref type="bibr" target="#b46">46]</ref>. A 31st Conference on Neural Information Processing Systems (NIPS 2017), Long Beach, CA, USA.  e convergence to a neighborhood is the best we can hereas by using diminishing step sizes, convergence bability one to the optimal points is made possible.</p><p>bility of The Stochastic Algorithm: The Biased Case: at when the gradient estimation error is biased, we ope to obtain almost sure convergence to the optimal . Instead, we have shown that provided that the biased asymptotically uniformly bounded, the iterates return ntraction region" infinitely often. In this example, we hat α s (n) = β (i,j) (n) and are uniformly bounded by a positive value. We also assume that ζ s (n) ∼ N (0, 1)</p><p>) (n) ∼ N (0, 1), for all s and (i, j). lot the iterates (using the relative distance to the points) in <ref type="figure" target="#fig_9">Fig. 4</ref>, which is further "zoomed in" in t can be observed from <ref type="figure" target="#fig_9">Fig. 4</ref> that when the upperon the {α s , β (i,j) } are small, the iterates return to orhood of the optimal solution. However, when the n errors are large, the recurrent behavior of the may not occur, and the iterates may diverge. This ates the theoretical analysis. We can further observe . 5 that the smaller the upper-bound is, the smaller the tion region" A η becomes, indicating that the iterates "closer" to the optimal points.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>V. STOCHASTIC STABILITY OF TWO TIME-SCALE ALGORITHM UNDER NOISY FEEDBACK</head><p>In the previous sections, we have applied the dual decomposition method to Problem (1) and devised the primal-dual algorithm, which is a single time-scale algorithm. As noted in Section I, there are many other decomposition methods. In particular, the primal decomposition method is a useful machinery for problem with coupled variables <ref type="bibr" target="#b31">[31]</ref>; and when some of the variables are fixed, the rest of the problem may decouple into several subproblems. This naturally yields multiple time-scale algorithms. It is also of great interest to examine the stability of the multiple time-scale algorithms in the presence of noisy feedback, and compare with the single time-scale algorithms, in terms of complexity and robustness.</p><p>To get a more concrete sense of the two time-scale algorithms based on primal decomposition, we consider the following NUM problem:</p><formula xml:id="formula_0">Ξ 2 :</formula><p>maximize {ms ≤xs ≤Ms , p}</p><formula xml:id="formula_1">s U s (x s ) subject to s:l∈L(s) x s ≤ c l , ∀l c l = h l (p), ∀l p ∈ H,<label>(39)</label></formula><p>where the link capacities {c l } are functions of specific MAC parameters p (for instance, p can be transmission probabilities  <ref type="bibr" target="#b50">[50]</ref> which shows the distance of the parameter from the optimum for a one time-scale update of a 4 node network flow problem. When the upper bounds on the errors (α, β) are small, the iterates oscillate and repeatedly return to a neighborhood of the optimal solution (cf. Supplement Section 2.3). However, when the upper bounds on the errors are large, the iterates typically diverge.</p><p>prerequisit for many convergence proofs is local stability <ref type="bibr" target="#b30">[30]</ref> which was shown for GANs by Nagarajan and Kolter <ref type="bibr" target="#b39">[39]</ref> for a min-max GAN setting. However, Nagarajan and Kolter require for their proof either rather strong and unrealistic assumptions or a restriction to a linear discriminator. Recent convergence proofs for GANs hold for expectations over training samples or for the number of examples going to infinity <ref type="bibr" target="#b32">[32,</ref><ref type="bibr" target="#b38">38,</ref><ref type="bibr" target="#b35">35,</ref><ref type="bibr" target="#b1">2]</ref>, thus do not consider mini-batch learning which leads to a stochastic gradient <ref type="bibr" target="#b47">[47,</ref><ref type="bibr" target="#b23">23,</ref><ref type="bibr" target="#b36">36,</ref><ref type="bibr" target="#b33">33]</ref>.</p><p>Recently actor-critic learning has been analyzed using stochastic approximation. Prasad et al. <ref type="bibr" target="#b41">[41]</ref> showed that a two time-scale update rule ensures that training reaches a stationary local Nash equilibrium if the critic learns faster than the actor. Convergence was proved via an ordinary differential equation (ODE), whose stable limit points coincide with stationary local Nash equilibria. We follow the same approach. We adopt this approach for GANs and prove that also GANs converge to a local Nash equilibrium when trained by a two time-scale update rule (TTUR), i.e., when discriminator and generator have separate learning rates. This also leads to better results in experiments. The main premise is that the discriminator converges to a local minimum when the generator is fixed. If the generator changes slowly enough, then the discriminator still converges, since the generator perturbations are small. Besides ensuring convergence, the performance may also improve since the discriminator must first learn new patterns before they are transferred to the generator. In contrast, a generator which is overly fast, drives the discriminator steadily into new regions without capturing its gathered information. In recent GAN implementations, the discriminator often learned faster than the generator. A new objective slowed down the generator to prevent it from overtraining on the current discriminator <ref type="bibr" target="#b44">[44]</ref>. The Wasserstein GAN algorithm uses more update steps for the discriminator than for the generator <ref type="bibr" target="#b0">[1]</ref>. We compare TTUR and standard GAN training. <ref type="figure">Fig. 1</ref> shows at the left panel a stochastic gradient example on CelebA for original GAN training (orig), which often leads to oscillations, and the TTUR. On the right panel an example of a 4 node network flow problem of Zhang et al. <ref type="bibr" target="#b50">[50]</ref> is shown. The distance between the actual parameter and its optimum for an one time-scale update rule is shown across iterates. When the upper bounds on the errors are small, the iterates return to a neighborhood of the optimal solution, while for large errors the iterates may diverge (see also Supplement Section 2.3). Our novel contributions in this paper are: (i) the two time-scale update rule for GANs, (ii) the proof that GANs trained with TTUR converge to a stationary local Nash equilibrium, (iii) the description of Adam as heavy ball with friction and the resulting second order differential equation, (iv) the convergence of GANs trained with TTUR and Adam to a stationary local Nash equilibrium, (v) the "Fréchet Inception Distance" (FID) to evaluate GANs, which is more consistent than the Inception Score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Two Time-Scale Update Rule for GANs</head><p>We consider a discriminator D(.; w) with parameter vector w and a generator G(.; θ) with parameter vector θ. Learning is based on a stochastic gradientg(θ, w) of the discriminator's loss function L D and a stochastic gradienth(θ, w) of the generator's loss function L G . The loss functions L D and L G can be the original as introduced in Goodfellow et al. <ref type="bibr" target="#b15">[16]</ref>, its improved versions <ref type="bibr" target="#b18">[18]</ref>, or recently proposed losses for GANs like the Wasserstein GAN <ref type="bibr" target="#b0">[1]</ref>. Our setting is not restricted to min-max GANs, but is also valid for all other, more general GANs for which the discriminator's loss function L D is not necessarily related to the generator's loss function L G . The gradientsg θ, w andh θ, w are stochastic, since they use mini-batches of m real world samples x (i) , 1 i m and m synthetic samples z (i) , 1 i m which are randomly chosen. If the true gradients are</p><formula xml:id="formula_2">g(θ, w) = ∇ w L D and h(θ, w) = ∇ θ L G , then we can defineg(θ, w) = g(θ, w) + M (w) andh(θ, w) = h(θ, w) + M (θ)</formula><p>with random variables M <ref type="bibr">(w)</ref> and M</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>(θ)</head><p>. Thus, the gradientsg θ, w andh θ, w are stochastic approximations to the true gradients. Consequently, we analyze convergence of GANs by two time-scale stochastic approximations algorithms. For a two time-scale update rule (TTUR), we use the learning rates b(n) and a(n) for the discriminator and the generator update, respectively:</p><formula xml:id="formula_3">w n+1 = w n + b(n) g θ n , w n + M (w) n , θ n+1 = θ n + a(n) h θ n , w n + M (θ) n . (1)</formula><p>For more details on the following convergence proof and its assumptions see Supplement Section 2.1. To prove convergence of GANs learned by TTUR, we make the following assumptions (The actual assumption is ended by , the following text are just comments and explanations):</p><p>(A1) The gradients h and g are Lipschitz.</p><p>Consequently, networks with Lipschitz smooth activation functions like ELUs (α = 1) <ref type="bibr" target="#b10">[11]</ref> fulfill the assumption but not ReLU networks.</p><formula xml:id="formula_4">(A2) n a(n) = ∞, n a 2 (n) &lt; ∞, n b(n) = ∞, n b 2 (n) &lt; ∞, a(n) = o(b(n)) (A3) The stochastic gradient errors {M (θ) n } and {M (w)</formula><p>n } are martingale difference sequences w.r.t. the increasing σ-field</p><formula xml:id="formula_5">F n = σ(θ l , w l , M (θ) l , M (w) l , l n), n 0 with E M (θ) n 2 | F (θ) n B 1 and E M (w) n 2 | F (w) n B 2</formula><p>, where B 1 and B 2 are positive deterministic constants. The original Assumption (A3) from Borkar 1997 follows from Lemma 2 in <ref type="bibr" target="#b4">[5]</ref> (see also <ref type="bibr" target="#b43">[43]</ref>). The assumption is fulfilled in the Robbins-Monro setting, where mini-batches are randomly sampled and the gradients are bounded.</p><p>(A4) For each θ, the ODEẇ(t) = g θ, w(t) has a local asymptotically stable attractor λ(θ) within a domain of attraction G θ such that λ is Lipschitz. The ODEθ(t) = h θ(t), λ(θ(t)) has a local asymptotically stable attractor θ * within a domain of attraction. The discriminator must converge to a minimum for fixed generator parameters and the generator, in turn, must converge to a minimum for this fixed discriminator minimum. Borkar 1997 required unique global asymptotically stable equilibria <ref type="bibr" target="#b6">[7]</ref>. The assumption of global attractors was relaxed to local attractors via Assumption (A6) and Theorem 2.7 in Karmakar &amp; Bhatnagar <ref type="bibr" target="#b26">[26]</ref>. See for more details Assumption (A6) in Supplement Section 2.1.3. Here, the GAN objectives may serve as Lyapunov functions. These assumptions of locally stable ODEs can be ensured by an additional weight decay term in the loss function which increases the eigenvalues of the Hessian. Therefore, problems with a region-wise constant discriminator that has zero second order derivatives are avoided. For further discussion see Supplement Section 2.1.1 (C3).</p><p>(A5) sup n θ n &lt; ∞ and sup n w n &lt; ∞. Typically ensured by the objective or a weight decay term.</p><p>The next theorem has been proved in the seminal paper of Borkar 1997 <ref type="bibr" target="#b6">[7]</ref>.</p><p>Theorem 1 <ref type="bibr">(Borkar)</ref>. If the assumptions are satisfied, then the updates Eq.</p><formula xml:id="formula_6">(1) converge to (θ * , λ(θ * )) a.s.</formula><p>The solution (θ * , λ(θ * )) is a stationary local Nash equilibrium <ref type="bibr" target="#b41">[41]</ref>, since θ * as well as λ(θ * ) are local asymptotically stable attractors with g θ * , λ(θ * ) = 0 and h θ * , λ(θ * ) = 0. An alternative approach to the proof of convergence using the Poisson equation for ensuring a solution to the fast update rule can be found in the Supplement Section 2.1.2. This approach assumes a linear update function in the fast update rule which, however, can be a linear approximation to a nonlinear gradient <ref type="bibr" target="#b28">[28,</ref><ref type="bibr" target="#b29">29]</ref>. For the rate of convergence see Supplement Section 2.2, where Section 2.2.1 focuses on linear and Section 2.2.2 on non-linear updates. For equal time-scales it can only be proven that the updates revisit an environment of the solution infinitely often, which, however, can be very large <ref type="bibr" target="#b50">[50,</ref><ref type="bibr" target="#b11">12]</ref>. For more details on the analysis of equal time-scales see Supplement Section 2.3. The main idea of the proof of Borkar <ref type="bibr" target="#b6">[7]</ref> is to use (T, δ) perturbed ODEs according to <ref type="bibr">Hirsch 1989 [22]</ref> (see also Appendix Section C of Bhatnagar, Prasad, &amp; Prashanth 2013 <ref type="bibr" target="#b5">[6]</ref>). The proof relies on the fact that there eventually is a time point when the perturbation of the slow update rule is small enough (given by δ) to allow the fast update rule to converge. For experiments with TTUR, we aim at finding learning rates such that the slow update is small enough to allow the fast to converge. Typically, the slow update is the generator and the fast update the discriminator. We have to adjust the two learning rates such that the generator does not affect discriminator learning in a undesired way and perturb it too much. However, even a larger learning rate for the generator than for the discriminator may ensure that the discriminator has low perturbations. Learning rates cannot be translated directly into perturbation since the perturbation of the discriminator by the generator is different from the perturbation of the generator by the discriminator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Adam Follows an HBF ODE and Ensures TTUR Convergence</head><p>In our experiments, we aim at using Adam stochastic approximation to avoid mode collapsing. GANs suffer from "mode collapsing" where large masses of probability are mapped onto a few modes that cover only small regions. While these regions represent meaningful samples, the variety of the real world data is lost and only few prototype samples are generated. Different methods have been proposed to avoid mode collapsing <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b37">37]</ref>. We obviate mode collapsing by using Adam stochastic approximation <ref type="bibr" target="#b27">[27]</ref>. Adam can be described as Heavy Ball with Friction (HBF) (see below), since it averages over past gradients. This averaging corresponds to a velocity that makes the generator resistant to getting pushed into small regions. Adam as an HBF method typically overshoots small local minima that correspond to mode collapse and can find flat minima which generalize well <ref type="bibr" target="#b24">[24]</ref>.  We recapitulate the Adam update rule at step n, with learning rate a, exponential averaging factors β 1 for the first and β 2 for the second moment of the gradient ∇f (θ n−1 ):</p><formula xml:id="formula_7">g n ←− ∇f (θ n−1 )<label>(2)</label></formula><formula xml:id="formula_8">m n ←− (β 1 /(1 − β n 1 )) m n−1 + ((1 − β 1 )/(1 − β n 1 )) g n v n ←− (β 2 /(1 − β n 2 )) v n−1 + ((1 − β 2 )/(1 − β n 2 )) g n g n θ n ←− θ n−1 − a m n /( √ v n + )</formula><p>, where following operations are meant componentwise: the product , the square root √ ., and the division / in the last line. Instead of learning rate a, we introduce the damping coefficient a(n) with a(n) = an −τ for τ ∈ (0, 1]. Adam has parameters β 1 for averaging the gradient and β 2 parametrized by a positive α for averaging the squared gradient. These parameters can be considered as defining a memory for Adam. To characterize β 1 and β 2 in the following, we define the exponential memory r(n) = r and the polynomial memory r(n) = r/ n l=1 a(l) for some positive constant r. The next theorem describes Adam by a differential equation, which in turn allows to apply the idea of (T, δ) perturbed ODEs to TTUR. Consequently, learning GANs with TTUR and Adam converges.  <ref type="bibr" target="#b40">[40]</ref>, the Heavy Ball with Friction (HBF) <ref type="bibr" target="#b14">[15]</ref>:</p><formula xml:id="formula_9">θ n+1 = θ n − a(n + 1) m n ,<label>(4)</label></formula><formula xml:id="formula_10">m n+1 = 1 − a(n + 1) r(n) m n + a(n + 1) r(n) ∇f (θ n ) + M n+1 .</formula><p>These update rules are the first moment update rules of Adam <ref type="bibr" target="#b27">[27]</ref>. The HBF can be formulated as the differential equation Eq. (3) <ref type="bibr" target="#b14">[15]</ref>. <ref type="bibr">Gadat et al.</ref> showed that the update rules Eq. (4) converge for loss functions f with at most quadratic grow and stated that convergence can be proofed for ∇f that are L-Lipschitz <ref type="bibr" target="#b14">[15]</ref>. Convergence has been proved for continuously differentiable f that is quasiconvex (Theorem 3 in Goudou &amp; Munier <ref type="bibr" target="#b19">[19]</ref>). Convergence has been proved for ∇f that is L-Lipschitz and bounded from below (Theorem 3.1 in Attouch et al. <ref type="bibr" target="#b2">[3]</ref>). Adam normalizes the average m n by the second moments v n of of the gradient g n : v n = E [g n g n ]. m n is componentwise divided by the square root of the components of v n . We assume that the second moments of g n are stationary, i.e., v = E [g n g n ]. In this case the normalization can be considered as additional noise since the normalization factor randomly deviates from its mean. In the HBF interpretation the normalization by √ v corresponds to introducing gravitation. We obtain</p><formula xml:id="formula_11">v n = 1 − β 2 1 − β n 2 n l=1 β n−l 2 g l g l , ∆v n = v n − v = 1 − β 2 1 − β n 2 n l=1 β n−l 2 (g l g l − v) . (5)</formula><p>For a stationary second moment v and β 2 = 1 − αa(n + 1)r(n), we have ∆v n ∝ a(n + 1)r(n). We use a componentwise linear approximation to Adam's second moment normalization</p><formula xml:id="formula_12">1/ √ v + ∆v n ≈ 1/ √ v − (1/(2v √ v)) ∆v n + O(∆ 2 v n )</formula><p>, where all operations are meant componentwise. If we set M</p><formula xml:id="formula_13">(v) n+1 = −(m n ∆v n )/(2v √ va(n + 1)r(n)), then m n / √ v n ≈ m n / √ v + a(n + 1)r(n)M (v) n+1 and E M (v) n+1 = 0, since E [g l g l − v] = 0. For a stationary second moment v, the random variable {M (v)</formula><p>n } is a martingale difference sequence with a bounded second moment. Therefore {M <ref type="bibr">(v)</ref> n+1 } can be subsumed into {M n+1 } in update rules Eq. (4). The factor 1/ √ v can be componentwise incorporated into the gradient g which corresponds to rescaling the parameters without changing the minimum.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>According to Attouch et al. [3] the energy, that is, a Lyapunov function, is E(t) = 1/2|θ(t)|</head><p>2 +f (θ(t)) andĖ(t) = −a |θ(t)| 2 &lt; 0. Since Adam can be expressed as differential equation and has a Lyapunov function, the idea of (T, δ) perturbed ODEs <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b22">22,</ref><ref type="bibr" target="#b7">8]</ref> carries over to Adam. Therefore the convergence of Adam with TTUR can be proved via two time-scale stochastic approximation analysis like in Borkar <ref type="bibr" target="#b6">[7]</ref> for stationary second moments of the gradient.</p><p>In the supplement we further discuss the convergence of two time-scale stochastic approximation algorithms with additive noise, linear update functions depending on Markov chains, nonlinear update functions, and updates depending on controlled Markov processes. Futhermore, the supplement presents work on the rate of convergence for both linear and nonlinear update rules using similar techniques as the local stability analysis of Nagarajan and Kolter <ref type="bibr" target="#b39">[39]</ref>. Finally, we elaborate more on equal time-scale updates, which are investigated for saddle point problems and actor-critic learning.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Experiments</head><p>Performance Measure. Before presenting the experiments, we introduce a quality measure for models learned by GANs. The objective of generative learning is that the model produces data which matches the observed data. Therefore, each distance between the probability of observing real world data p w (.) and the probability of generating model data p(.) can serve as performance measure for generative models. However, defining appropriate performance measures for generative models is difficult <ref type="bibr" target="#b45">[45]</ref>. The best known measure is the likelihood, which can be estimated by annealed importance sampling <ref type="bibr" target="#b49">[49]</ref>. However, the likelihood heavily depends on the noise assumptions for the real data and can be dominated by single samples <ref type="bibr" target="#b45">[45]</ref>. Other approaches like density estimates have drawbacks, too <ref type="bibr" target="#b45">[45]</ref>. A well-performing approach to measure the performance of GANs is the "Inception Score" which correlates with human judgment <ref type="bibr" target="#b44">[44]</ref>. Generated samples are fed into an inception model that was trained on ImageNet. Images with meaningful objects are supposed to have low label (output) entropy, that is, they belong to few object classes. On the other hand, the entropy across images should be high, that is, the variance over the images should be large. Drawback of the Inception Score is that the statistics of real world samples are not used and compared to the statistics of synthetic samples. Next, we improve the Inception Score. The equality p(.) = p w (.) holds except for a non-measurable set if and only if p(.)f (x)dx = p w (.)f (x)dx for a basis f (.) spanning the function space in which p(.) and p w (.) live. These equalities of expectations are used to describe distributions by moments or cumulants, where f (x) are polynomials of the data x. We generalize these polynomials by replacing x by the coding layer of an inception model in order to obtain vision-relevant features. For practical reasons we only consider the first two polynomials, that is, the first two moments: mean and covariance. The Gaussian is the maximum entropy distribution for given mean and covariance, therefore we assume the coding units to follow a multidimensional Gaussian. The difference of two Gaussians (synthetic and real-world images) is measured by the Fréchet distance [14] also known as Wasserstein-2 distance <ref type="bibr" target="#b48">[48]</ref>. We call the Fréchet distance d(., .) between the Gaussian with mean (m, C) obtained from p(.) and the Gaussian with mean (m w , C w ) obtained from p w (.) the "Fréchet Inception Distance" (FID), which is given by <ref type="bibr" target="#b12">[13]</ref>:</p><formula xml:id="formula_14">d 2 ((m, C), (m w , C w )) = m − m w 2 2 + Tr C + C w − 2 CC w 1/2 .</formula><p>Next we show that the FID is consistent with increasing disturbances and human judgment. <ref type="figure" target="#fig_8">Fig. 3</ref> evaluates the FID for Gaussian noise, Gaussian blur, implanted black rectangles, swirled images, salt and pepper noise, and CelebA dataset contaminated by ImageNet images. The FID captures the disturbance level very well. In the experiments we used the FID to evaluate the performance of GANs. For more details and a comparison between FID and Inception Score see Supplement Section 1, where we show that FID is more consistent with the noise level than the Inception Score.</p><p>Model Selection and Evaluation. We compare the two time-scale update rule (TTUR) for GANs with the original GAN training to see whether TTUR improves the convergence speed and performance of GANs. We have selected Adam stochastic optimization to reduce the risk of mode collapsing. The advantage of Adam has been confirmed by MNIST experiments, where Adam indeed considerably reduced the cases for which we observed mode collapsing. Although TTUR ensures that the discriminator converges during learning, practicable learning rates must be found for each experiment. We face a trade-off since the learning rates should be small enough (e.g. for the generator) to ensure convergence but at the same time should be large enough to allow fast learning. For each of the experiments, the learning rates have been optimized to be large while still ensuring stable training which is indicated by a decreasing FID or Jensen-Shannon-divergence (JSD). We further fixed the time point for stopping training to the update step when the FID or Jensen-Shannon-divergence of the best models was no longer decreasing. For some models, we observed that the FID diverges or starts to increase at a certain time point. An example of this behaviour is shown in <ref type="figure">Fig. 5</ref>. The performance of generative models is evaluated via the Fréchet Inception Distance (FID) introduced above. For the One Billion Word experiment, the normalized JSD served as performance measure. For computing the FID, we propagated all images from the training dataset through the pretrained Inception-v3 model following the computation of the Inception Score <ref type="bibr" target="#b44">[44]</ref>, however, we use the last pooling layer as coding layer. For this coding layer, we calculated the mean m w and the covariance matrix C w . Thus, we approximate the first and second central moment of the function given by the Inception coding layer under the real world distribution. To approximate these moments for the model distribution, we generate 50,000 images, propagate them through the Inception-v3 model, and then compute the mean m and the covariance matrix C. For computational efficiency, we evaluate the FID every 1,000 DCGAN mini-batch updates, every 5,000 WGAN-GP outer iterations for the image experiments, and every 100 outer iterations for the WGAN-GP language model. For the one time-scale updates a WGAN-GP outer iteration for the image model consists of five discriminator mini-batches and ten discriminator mini-batches for the language model, where we follow the original implementation. For TTUR however, the discriminator is updated only once per iteration. We repeat the training for each single time-scale (orig) and TTUR learning rate eight times for the image datasets and ten times for the language benchmark. Additionally to the mean FID training progress we show the minimum and maximum FID over all runs at each evaluation time-step. For more details, implementations and further results see Supplement Section 4 and 6.</p><p>Simple Toy Data. We first want to demonstrate the difference between a single time-scale update rule and TTUR on a simple toy min/max problem where a saddle point should be found. The objective f (x, y) = (1 + x 2 )(100 − y 2 ) in <ref type="figure" target="#fig_9">Fig. 4</ref> (left) has a saddle point at (x, y) = (0, 0) and fulfills assumption A4. The norm (x, y) measures the distance of the parameter vector (x, y) to the saddle point. We update (x, y) by gradient descent in x and gradient ascent in y using additive Gaussian noise in order to simulate a stochastic update. The updates should converge to the saddle point (x, y) = (0, 0) with objective value f (0, 0) = 100 and the norm 0. In <ref type="figure" target="#fig_9">Fig. 4 (right)</ref>, the first two rows show one time-scale update rules. The large learning rate in the first row diverges and has large fluctuations. The smaller learning rate in the second row converges but slower than the TTUR in the third row which has slow x-updates. TTUR with slow y-updates in the fourth row also converges but slower. DCGAN on Image Data. We test TTUR for the deep convolutional GAN (DCGAN) <ref type="bibr" target="#b42">[42]</ref> at the CelebA, CIFAR-10, SVHN and LSUN Bedrooms dataset. <ref type="figure">Fig. 5</ref> shows the FID during learning with the original learning method (orig) and with TTUR. The original training method is faster at the beginning, but TTUR eventually achieves better performance. DCGAN trained TTUR reaches constantly a lower FID than the original method and for CelebA and LSUN Bedrooms all one time-scale runs diverge. For DCGAN the learning rate of the generator is larger then that of the discriminator, which, however, does not contradict the TTUR theory (see the Supplement Section 5).</p><p>In <ref type="table" target="#tab_0">Table 1</ref> we report the best FID with TTUR and one time-scale training for optimized number of updates and learning rates. TTUR constantly outperforms standard training and is more stable.</p><p>WGAN-GP on Image Data. We used the WGAN-GP image model <ref type="bibr" target="#b21">[21]</ref> to test TTUR with the CIFAR-10 and LSUN Bedrooms datasets. In contrast to the original code where the discriminator is trained five times for each generator update, TTUR updates the discriminator only once, therefore we align the training progress with wall-clock time. The learning rate for the original training was optimized to be large but leads to stable learning. TTUR can use a higher learning rate for the discriminator since TTUR stabilizes learning.  <ref type="table" target="#tab_0">Table 1</ref> shows the best FID with TTUR and one time-scale training for optimized number of iterations and learning rates. Again TTUR reaches lower FIDs than one time-scale training.</p><p>WGAN-GP on Language Data. Finally the One Billion Word Benchmark <ref type="bibr" target="#b9">[10]</ref> serves to evaluate TTUR on WGAN-GP. The character-level generative language model is a 1D convolutional neural network (CNN) which maps a latent vector to a sequence of one-hot character vectors of dimension 32 given by the maximum of a softmax output. The discriminator is also a 1D CNN applied to sequences of one-hot vectors of 32 characters. Since the FID criterium only works for images, we measured the performance by the Jensen-Shannon-divergence (JSD) between the model and the real world distribution as has been done previously <ref type="bibr" target="#b21">[21]</ref>. In contrast to the original code where the critic is trained ten times for each generator update, TTUR updates the discriminator only once, therefore we align the training progress with wall-clock time. The learning rate for the original training was optimized to be large but leads to stable learning. TTUR can use a higher learning rate for the discriminator since TTUR stabilizes learning. We report for the 4 and 6-gram word evaluation the normalized mean JSD for ten runs for original training and TTUR training in <ref type="figure" target="#fig_11">Fig. 7</ref>. In <ref type="table" target="#tab_0">Table 1</ref> we report the best JSD at an optimal time-step where TTUR outperforms the standard training for both measures. The improvement of TTUR on the 6-gram statistics over original training shows that TTUR enables to learn to generate more subtle pseudo-words which better resembles real words.   </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Conclusion</head><p>For learning GANs, we have introduced the two time-scale update rule (TTUR), which we have proved to converge to a stationary local Nash equilibrium. Then we described Adam stochastic optimization as a heavy ball with friction (HBF) dynamics, which shows that Adam converges and that Adam tends to find flat minima while avoiding small local minima. A second order differential equation describes the learning dynamics of Adam as an HBF system. Via this differential equation, the convergence of GANs trained with TTUR to a stationary local Nash equilibrium can be extended to Adam. Finally, to evaluate GANs, we introduced the 'Fréchet Inception Distance" (FID) which captures the similarity of generated images to real ones better than the Inception Score. In experiments we have compared GANs trained with TTUR to conventional GAN training with a one time-scale update rule on CelebA, CIFAR-10, SVHN, LSUN Bedrooms, and the One Billion Word Benchmark. TTUR outperforms conventional GAN training consistently in all experiments.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>noisy feedback (the unbiased case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>Fig. 4. Convergence under noisy feedback (the biased case).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Fig. 5. "Zoomed-in" convergence behavior of the iterates in Figure 4.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>10 Figure 1 :</head><label>101</label><figDesc>Figure 1: Left: Original vs. TTUR GAN training on CelebA. Right: Figure from Zhang 2007 [50] which shows the distance of the parameter from the optimum for a one time-scale update of a 4 node network flow problem. When the upper bounds on the errors (α, β) are small, the iterates oscillate and repeatedly return to a neighborhood of the optimal solution (cf. Supplement Section 2.3). However, when the upper bounds on the errors are large, the iterates typically diverge.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>Fig. 2 depicts the dynamics of HBF, where the ball settles at a flat minimum. Next, we analyze whether GANs trained with TTUR converge when using Adam. For more details see Supplement Section 3.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Heavy Ball with Friction, where the ball with mass overshoots the local minimum θ + and settles at the flat minimum θ * .</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Theorem 2 .</head><label>2</label><figDesc>If Adam is used with β 1 = 1 − a(n + 1)r(n), β 2 = 1 − αa(n + 1)r(n) and with ∇f as the full gradient of the lower bounded, continuously differentiable objective f , then for stationary second moments of the gradient, Adam follows the differential equation for Heavy Ball with Friction (HBF):θ Proof. Gadat et al. derived a discrete and stochastic version of Polyak's Heavy Ball method</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: FID is evaluated for upper left: Gaussian noise, upper middle: Gaussian blur, upper right: implanted black rectangles, lower left: swirled images, lower middle: salt and pepper noise, and lower right: CelebA dataset contaminated by ImageNet images. The disturbance level rises from zero and increases to the highest level. The FID captures the disturbance level very well by monotonically increasing.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Left: Plot of the objective with a saddle point at (0, 0). Right: Training progress with equal learning rates of 0.01 (first row) and 0.001 (second row)) for x and y, TTUR with a learning rate of 0.0001 for x vs. 0.01 for y (third row) and a larger learning rate of 0.01 for x vs. 0.0001 for y (fourth row). The columns show the function values (left), norms (middle), and (x, y) (right). TTUR (third row) clearly converges faster than with equal time-scale updates and directly moves to the saddle point as shown by the norm and in the (x, y)-plot.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Mean FID (solid line) surrounded by a shaded area bounded by the maximum and the minimum over 8 runs for DCGAN on CelebA, CIFAR-10, SVHN, and LSUN Bedrooms. TTUR learning rates are given for the discriminator b and generator a as: "TTUR b a". Top Left: CelebA. Top Right: CIFAR-10, starting at mini-batch update 10k for better visualisation. Bottom Left: SVHN. Bottom Right: LSUN Bedrooms. Training with TTUR (red) is more stable, has much lower variance, and leads to a better FID.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Performance of WGAN-GP models trained with the original (orig) and our TTUR method on the One Billion Word benchmark. The performance is measured by the normalized JensenShannon-divergence based on 4-gram (left) and 6-gram (right) statistics averaged (solid line) and surrounded by a shaded area bounded by the maximum and the minimum over 10 runs, aligned to wall-clock time and starting at minute 150. TTUR learning (red) clearly outperforms the original one time-scale learning.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="true"><head>Table 1 :</head><label>1</label><figDesc>The performance of DCGAN and WGAN-GP trained with the original one time-scale update rule and with TTUR on CelebA, CIFAR-10, SVHN, LSUN Bedrooms and the One Billion Word Benchmark. During training we compare the performance with respect to the FID and JSD for optimized number of updates. TTUR exhibits consistently a better FID and a better JSD.</figDesc><table>DCGAN Image 
dataset 
method 
b, a 
updates FID method b = a updates FID 
CelebA 
TTUR 
1e-5, 5e-4 
225k 
12.5 orig 
5e-4 
70k 
21.4 
CIFAR-10 TTUR 
1e-4, 5e-4 
75k 
36.9 orig 
1e-4 
100k 
37.7 
SVHN 
TTUR 
1e-5, 1e-4 
165k 
12.5 orig 
5e-5 
185k 
21.4 
LSUN 
TTUR 
1e-5, 1e-4 
340k 
57.5 orig 
5e-5 
70k 
70.</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">t + a(t)θ t + ∇f (θ t ) = 0 . (3) Adam converges for gradients ∇f that are L-Lipschitz.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Wasserstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.07875</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Generalization and equilibrium in generative adversarial nets (GANs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 34th International Conference on Machine Learning, Proceedings of Machine Learning Research</title>
		<editor>D. Precup and Y. W. Teh</editor>
		<meeting>the 34th International Conference on Machine Learning, Machine Learning Research</meeting>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">70</biblScope>
			<biblScope unit="page" from="224" to="232" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">The heavy ball with friction method, I. the continuous dynamical system: Global exploration of the local minima of a real-valued function by asymptotic analysis of a dissipative dynamical system</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Attouch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Goudou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Redont</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Communications in Contemporary Mathematics</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="34" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Berthelot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Schumm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.10717</idno>
		<title level="m">BEGAN: Boundary equilibrium generative adversarial networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Gradient convergence in gradient methods with errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Bertsekas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Optimization</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="627" to="642" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Stochastic Recursive Algorithms for Optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Prashanth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Lecture Notes in Control and Information Sciences</title>
		<imprint>
			<date type="published" when="2013" />
			<publisher>Springer-Verlag</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Stochastic approximation with two time scales</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="291" to="294" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">The O.D.E. method for convergence of stochastic approximation and reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Borkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">P</forename><surname>Meyn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIAM Journal on Control and Optimization</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="447" to="469" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Mode regularized generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Li</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.02136</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">One billion word benchmark for measuring progress in statistical language modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Chelba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Ge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brants</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Koehn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Robinson</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1312.3005</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Fast and accurate deep network learning by exponential linear units (ELUs)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A convergent online single time scale actor critic algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Dicastro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Meir</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">J. Mach. Learn. Res</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="367" to="410" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The Fréchet distance between multivariate normal distributions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Dowson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">V</forename><surname>Landau</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Multivariate Analysis</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="450" to="455" />
			<date type="published" when="1982" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Sur la distance de deux lois de probabilité</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fréchet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">C. R. Acad. Sci</title>
		<imprint>
			<biblScope unit="volume">244</biblScope>
			<biblScope unit="page" from="689" to="692" />
			<date type="published" when="1957" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gadat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Panloup</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Saadane</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04228</idno>
		<title level="m">Stochastic heavy ball. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D</editor>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Lawrence</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Weinberger</surname></persName>
		</author>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="2672" to="2680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">On distinguishability criteria for estimating generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6515</idno>
	</analytic>
	<monogr>
		<title level="m">Workshop at the International Conference on Learning Representations (ICLR)</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.00160</idno>
		<title level="m">NIPS 2016 tutorial: Generative adversarial networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">The gradient and heavy ball with friction dynamical systems: the quasiconvex case</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Goudou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Munier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Mathematical Programming</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
		<title level="m" type="main">An online learning approach to generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Grnarova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Y</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lucchi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krause</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03269</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ahmed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Arjovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Dumoulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.00028</idno>
	</analytic>
	<monogr>
		<title level="m">Improved training of Wasserstein GANs. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Convergent activation dynamics in continuous time networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Hirsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="331" to="349" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">D</forename><surname>Hjelm</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Jacob</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Che</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.08431</idno>
		<title level="m">Boundary-seeking generative adversarial networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Flat minima</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="42" />
			<date type="published" when="1997" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.07004</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Two time-scale stochastic approximation with controlled Markov noise and off-policy temporal-difference learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Karmakar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematics of Operations Research</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2015</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Actor-Critic Algorithms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
		<respStmt>
			<orgName>Department of Electrical Engineering and Computer Science, Massachusetts Institute of Technology</orgName>
		</respStmt>
	</monogr>
<note type="report_type">PhD thesis</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Linear stochastic approximation driven by slowly varying Markov chains</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Konda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">N</forename><surname>Tsitsiklis</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Systems &amp; Control Letters</title>
		<imprint>
			<biblScope unit="volume">50</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="95" to="102" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Stochastic Approximation Algorithms and Recursive Algorithms and Applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Kushner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">G</forename><surname>Yin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Springer-Verlag</publisher>
			<pubPlace>New York</pubPlace>
		</imprint>
	</monogr>
	<note>second edition</note>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Photo-realistic single image super-resolution using a generative adversarial network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Ledig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Huszar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Caballero</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">P</forename><surname>Aitken</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Tejani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Totz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.04802</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
	<note>arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Towards deeper understanding of moment matching network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-L</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-C</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Póczos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08584</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Madry</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peebles</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Schmidt</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.09884</idno>
		<title level="m">Towards understanding the dynamics of generative adversarial networks. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">H</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Geometric</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Gan</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.02894</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Approximation and convergence properties of generative adversarial learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chaudhuri</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.08991</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">The numerics of GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">M</forename><surname>Mescheder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Nowozin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Geiger</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10461</idno>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Unrolled generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Poole</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Pfau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sohl-Dickstein</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.02163</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR</title>
		<meeting>the International Conference on Learning Representations (ICLR</meeting>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Mroueh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Sercu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gan</forename><surname>Fisher</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.09675</idno>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nagarajan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Z</forename><surname>Kolter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.04156</idno>
	</analytic>
	<monogr>
		<title level="m">Gradient descent GAN optimization is locally stable. arXiv e-prints</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">31</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b40">
	<analytic>
		<title level="a" type="main">Some methods of speeding up the convergence of iteration methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">T</forename><surname>Polyak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">USSR Computational Mathematics and Mathematical Physics</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1" to="17" />
			<date type="published" when="1964" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b41">
	<analytic>
		<title level="a" type="main">Two-timescale algorithms for learning Nash equilibria in general-sum stochastic games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">L</forename><surname>Prasad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">A</forename><surname>Prashanth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2015 International Conference on Autonomous Agents and Multiagent Systems (AAMAS &apos;15)</title>
		<meeting>the 2015 International Conference on Autonomous Agents and Multiagent Systems (AAMAS &apos;15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1371" to="1379" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b42">
	<analytic>
		<title level="a" type="main">Unsupervised representation learning with deep convolutional generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Metz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06434</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b43">
	<analytic>
		<title level="a" type="main">Stochastic recursive inclusion in two timescales with an application to the lagrangian dual problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ramaswamy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhatnagar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Stochastics</title>
		<imprint>
			<biblScope unit="volume">88</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1173" to="1187" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b44">
	<analytic>
		<title level="a" type="main">Improved techniques for training GANs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Salimans</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zaremba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Radford</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>D. D. Lee, M. Sugiyama, U. V. Luxburg, I. Guyon, and R. Garnett</editor>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="page" from="2234" to="2242" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b45">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.01844</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations (ICLR)</title>
		<meeting>the International Conference on Learning Representations (ICLR)</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b46">
	<analytic>
		<title level="a" type="main">AdaGAN: Boosting generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Tolstikhin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gelly</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Bousquet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-J</forename><surname>Simon-Gabriel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1701.02386</idno>
	</analytic>
	<monogr>
		<title level="j">Advances in Neural Information Processing Systems</title>
		<imprint>
			<biblScope unit="volume">31</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b47">
	<monogr>
		<title level="m" type="main">MAGAN: margin adaptation for generative adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cully</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">J</forename><surname>Chang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Demiris</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.03817</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv e-prints</note>
</biblStruct>

<biblStruct xml:id="b48">
	<analytic>
		<title level="a" type="main">Markov processes over denumerable products of spaces describing large systems of automata</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">N</forename><surname>Wasserstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Probl. Inform. Transmission</title>
		<imprint>
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page" from="47" to="52" />
			<date type="published" when="1969" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b49">
	<analytic>
		<title level="a" type="main">On the quantitative analysis of decoderbased generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Burda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Grosse</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.04273</idno>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the International Conference on Learning Representations</title>
		<meeting>the International Conference on Learning Representations</meeting>
		<imprint>
			<biblScope unit="page">2017</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b50">
	<analytic>
		<title level="a" type="main">The impact of stochastic noisy feedback on distributed network utility maximization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Chiang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE INFOCOM 2007 -26th IEEE International Conference on Computer Communications</title>
		<imprint>
			<date type="published" when="2007" />
			<biblScope unit="page" from="222" to="230" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
