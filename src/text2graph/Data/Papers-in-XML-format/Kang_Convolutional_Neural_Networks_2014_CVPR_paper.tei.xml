<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:24+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Convolutional Neural Networks for No-Reference Image Quality Assessment</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Le</forename><surname>Kang</surname></persName>
							<email>1lekang@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Ye</surname></persName>
							<email>pengye@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Li</surname></persName>
							<email>2yi.li@cecs.anu.edu.au</email>
							<affiliation key="aff1">
								<orgName type="institution">NICTA and ANU</orgName>
								<address>
									<settlement>Canberra</settlement>
									<country key="AU">Australia</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Doermann</surname></persName>
							<email>doermann@umiacs.umd.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">University of Maryland</orgName>
								<address>
									<settlement>College Park</settlement>
									<region>MD</region>
									<country key="US">USA</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Convolutional Neural Networks for No-Reference Image Quality Assessment</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this work we describe a Convolutional Neural Network (CNN) to accurately predict image quality without a reference image. Taking image patches as input, the CNN works in the spatial domain without using hand-crafted features that are employed by most previous methods. The network consists of one convolutional layer with max and min pooling, two fully connected layers and an output node. Within the network structure, feature learning and regression are integrated into one optimization process, which leads to a more effective model for estimating image quality. This approach achieves state of the art performance on the LIVE dataset and shows excellent generalization ability in cross dataset experiments. Further experiments on images with local distortions demonstrate the local quality estimation ability of our CNN, which is rarely reported in previous literature.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>This paper presents a Convolutional Neural Network (CNN) that can accurately predict the quality of distorted images with respect to human perception. The work focuses on the most challenging category of objective image quality assessment (IQA) tasks: general-purpose No-Reference IQA (NR-IQA), which evaluates the visual quality of digital images without access to reference images and without prior knowledge of the types of distortions present.</p><p>Visual quality is a very complex yet inherent characteristic of an image. In principle, it is the measure of the distortion compared with an ideal imaging model or perfect reference image. When reference images are available, Full Reference (FR) IQA methods <ref type="bibr" target="#b13">[14,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b18">19]</ref> can be ap-</p><p>The partial support of this research by DARPA through BBN/DARPA Award HR0011-08-C-0004 under subcontract 9500009235, the US Government through NSF Awards IIS-0812111 and IIS-1262122 is gratefully acknowledged.</p><p>plied to directly quantify the differences between distorted images and their corresponding ideal versions. State of the art FR measures, such as VIF <ref type="bibr" target="#b13">[14]</ref> and FSIM <ref type="bibr" target="#b21">[22]</ref>, achieve a very high correlation with human perception.</p><p>However, in many practical computer vision applications there do not exist perfect versions of the distorted images, so NR-IQA is required. NR-IQA measures can directly quantify image degradations by exploiting features that are discriminant for image degradations. Most successful approaches use Natural Scene Statistics (NSS) based features. Typically, NSS based features characterize the distributions of certain filter responses. Traditional NSS based features are extracted in image transformation domains using, for example the wavelet transform <ref type="bibr" target="#b9">[10]</ref> or the DCT transform <ref type="bibr" target="#b12">[13]</ref>. These methods are usually very slow due to the use of computationally expensive image transformations. Recent development in NR-IQA methods -CORNIA <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref> and BRISQUE <ref type="bibr" target="#b8">[9]</ref> promote extracting features from the spatial domain, which leads to a significant reduction in computation time. CORNIA demonstrates that it is possible to learn discriminant image features directly from the raw image pixels, instead of using handcrafted features.</p><p>Based on these observations, we explore using a Convolutional Neural Network (CNN) to learn discriminant features for the NR-IQA task. Recently, deep neural networks have gained researchers' attention and achieved great success on various computer vision tasks. Specifically, CNN has shown superior performance on many standard object recognition benchmarks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b3">4]</ref>. One of CNN's advantages is that it can take raw images as input and incorporate feature learning into the training process. With a deep structure, the CNN can effectively learn complicated mappings while requiring minimal domain knowledge.</p><p>To the best of our knowledge, CNN has not been applied to general-purpose NR-IQA. The primary reason is that the original CNN is not designed for capturing image quality features. In the object recognition domain good features generally encode local invariant parts, however, for the NR-IQA task, good features should be able to capture NSS properties. The difference between NR-IQA and object recognition makes the application of CNN nonintuitive. One of our contributions is that we modified the network structure, such that it can learn image quality features more effectively and estimate the image quality more accurately.</p><p>Another contribution of our paper is that we propose a novel framework that allows learning and prediction of image quality on local regions. Previous approaches typically accumulate features over the entire image to obtain statistics for estimating overall quality, and have rarely shown the ability to estimate local quality, except for a simple example in <ref type="bibr" target="#b17">[18]</ref>. By contrast, our method can estimate quality on small patchs (such as 32 × 32). Local quality estimation is important for the image denoising or reconstruction problems, applying enhancement only where required.</p><p>We show experimentally that the proposed method advances the state of the art. On the LIVE dataset our CNN outperforms CORNIA and BRISQUE, and achieves comparable results with state of the art FR measures such as FSIM <ref type="bibr" target="#b21">[22]</ref>. In addition to the superior overall performance, we also show qualitative results that demonstrate the local quality estimation of our method.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Previously researchers have attempted to use neural networks for NR-IQA. Li et al. <ref type="bibr" target="#b7">[8]</ref> applied a general regression neural network that takes as input perceptual features including phase congruency, entropy and the image gradients. Chetouani et al. <ref type="bibr" target="#b2">[3]</ref> used a neural network to combine multiple distortion-specific NR-IQA measures. These methods require pre-extracted handcrafted features and only use neural networks for learning the regression function. Thus they do not have the advantage of learning features and regression models in a holistic way, and these approaches are inferior to the state of the art approaches. In contrast, our method does not require any handcrafted features and directly learns discriminant features from normalized raw image pixels to achieve much better performance.</p><p>The use of convolutional neural networks is partly motivated by the feature learning framework introduced in CORNIA <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21]</ref>. First, the CORNIA features are learned directly from the normalized raw image patches. This implies that it is possible to extract discriminative features from spatial domain without complicated image transformations. Second, supervised CORNIA <ref type="bibr" target="#b20">[21]</ref> employs a twolayer structure which learns the filters and weights in the regression model simultaneously based on an EM like approach. This structure can be viewed as an empirical implementation of a two layer neural network. However, it has not utilized the full power of neural networks.</p><p>Our approach integrates feature learning and regression into the general CNN framework. The advantages are two fold. First, making the network deeper will raise the learning capacity significantly <ref type="bibr" target="#b0">[1]</ref>. In the following sections we will see that with fewer filters/features than CORNIA, we are able to achieve the state of the art results. Second, in the CNN framework, training the network as a whole using a simple method like backpropagation enables the possibility of conveniently incorporating recent techniques designed to improve learning such as dropout <ref type="bibr" target="#b4">[5]</ref> and rectified linear unit <ref type="bibr" target="#b6">[7]</ref>. Furthermore, after we make the bridge between NR-IQA and CNN, the rapid developing deep learning community will be a significant source of novel techniques for advancing the NR-IQA performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">CNN for NR-IQA</head><p>The proposed framework of using CNN for image quality estimation is as follows. Given a gray scale image, we first perform a contrast normalization, then sample nonoverlapping patches from it. We use a CNN to estimate the quality score for each patch and average the patch scores to obtain a quality estimation for the image.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Network Architecture</head><p>The proposed network consists of five layers. <ref type="figure">Figure 1</ref> shows the architecture of our network, which is a 32 × 32 − 26 × 26 × 50 − 2 × 50 − 800 − 800 − 1 structure. The input is locally normalized 32 × 32 image patches. The first layer is a convolutional layer which filters the input with 50 kernels each of size 7 × 7 with a stride of 1 pixel. The convolutional layer produces 50 feature maps each of size 26 × 26, followed by a pooling operation that reduces each feature map to one max and one min. Two fully connected layers of 800 nodes each come after the pooling. The last layer is a simple linear regression with a one dimensional output that gives the score.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Local Normalization</head><p>Previous NR-IQA methods, such as BRISQUE and CORNIA, typically apply a contrast normalization. In this work, we employ a simple local contrast normalization method similar to <ref type="bibr" target="#b8">[9]</ref>. Suppose the intensity value of a pixel at location (i, j) is I(i, j), we compute its normalized valuê I(i, j) as follows:</p><formula xml:id="formula_0">I(i, j) = I(i, j) − µ(i, j) σ(i, j) + C µ(i, j) = p=P p=−P q=Q q=−Q I(i + p, j + q) σ(i, j) = p=P p=−P q=Q q=−Q I(i + p, j + q) − µ(i, j) 2</formula><p>(1) where C is a positive constant that prevents dividing by zero. P and Q are the normalization window sizes. In <ref type="bibr" target="#b8">[9]</ref>, <ref type="figure">Figure 1</ref>: The architecture of our CNN it was shown that a smaller normalization window size improves the performance. In practice we pick P = Q = 3 so the window size is much smaller than the input image patch. Note that with this local normalization each pixel may have a different local mean and variance.</p><p>Local normalization is important. We observe that using larger normalization windows leads to worse performance. Specifically, a uniform normalization, which applies the mean and variance of the entire image patch to each pixel, will cause about a 3% drop on the performance.</p><p>It is worth noting that when using a CNN for object recognition, a global contrast normalization is usually applied to the entire image. The normalization not only alleviates the saturation problem common in early work that used sigmoid neurons, but also makes the network robust to illumination and contrast variation. For the NR-IQA problem, contrast normalization should be applied locally. Additionally, although luminance and contrast change can be considered distortions in some applications, we mainly focus on distortions arising from image degradations, such as blur, compression and additive noise.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Pooling</head><p>In the convolution layer, the locally normalized image patches are convolved with 50 filters and each filter generates a feature map. We then apply pooling on each feature map to reduce the filter responses to a lower dimension. Specifically, each feature map is pooled into one max value and one min value, which is similar to CORNIA. Let R k i,j denote the response at location (i, j) of the feature map obtained by the k-th filter, then the max and min values of u k and v k are given by</p><formula xml:id="formula_1">u k = max i,j R k i,j v k = min i,j R k i,j<label>(2)</label></formula><p>where k = 1, 2, ..., K and K is the number of kernels. The pooling procedure reduces each feature map to a 2 dimensional feature vector. Therefore, each node of the next fully connected layer takes an input of size 2 × K. It is worth noting that although max pooling already works well, introducing min pooling boosts the performance by about 2%.</p><p>In object recognition scenario, pooling is typically performed on every 2 × 2 cell. In that case, selecting a representative filter response from each small cell may keep some location information while achieving robustness to translation. This operation is particularly helpful for object recognition since objects can typically be modeled as multiple parts organized in a certain spatial order. However, for the NR-IQA task we observe that image distortions are often locally (if not globally) homogeneous, i.e. the same level of distortion occurs at all the locations of a 32 × 32 patch, for example. The lack of obvious global spatial structure in image distortions enables pooling without keeping locations to reduce the cost of computation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">ReLU Nonlinearity</head><p>Instead of traditional sigmoid or tanh neurons, we use Rectified Linear Units (ReLUs) <ref type="bibr" target="#b10">[11]</ref> in the two fully connected layers. <ref type="bibr" target="#b6">[7]</ref> demonstrated in a deep CNN that ReLUs enable the network to train several times faster compared to using tanh units. Here we give a brief description of ReLUs. ReLUs take a simple form of nonlinearity by applying a thresholding function to the input, in place of the sigmoid or tanh transform. Let g, w i and a i denote the output of the ReLU, the weights of the ReLU and the output of the previous layer, respectively, then the ReLU can be mathematically described as g = max(0, i w i a i ).</p><p>Note that ReLUs only allow nonnegative signals to pass through. Due to this property, we do not use ReLUs but use linear neurons (identity transform) on the convolutional and pooling layer. The reason is that the min pooling typically produce negative values and we do not want to block the information in these negative pooling outputs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Learning</head><p>We train our network on non-overlapping 32×32 patches taken from large images. For training we assign each patch a quality score as its source image's ground truth score. We can do this because the training images in our experiments have homogeneous distortions. During the test stage, we average the predicted patch scores for each image to obtain the image level quality score. By taking small patches as input, we have a much larger number of training samples compared to using the whole image on a given dataset, which particularly meets the needs of CNNs.</p><p>Let x n and y n denote the input patch and its ground truth score respectively and f (x n ; w) be the predicted score of x n with network weights w. Support Vector Regression (SVR) with -insensitive loss has been successfully applied to learn the regression function for NR-IQA in previous work <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b8">9]</ref>. We adopt a similar objective function as follows:</p><formula xml:id="formula_2">L = 1 N N n=1 f (x n ; w) − y n l1 w = min w L<label>(3)</label></formula><p>Note that the above loss function is equivalent to the loss function used in -SVR with = 0. Stochastic gradient decent (SGD) and backpropagation are used to solve this problem. A validation set is used to select parameters of the trained model and prevent overfitting. In experiments we perform SGD for 40 epochs in training and keep the model parameters that generate the highest Linear Correlation Coefficient (LCC) on the validation set.</p><p>Recently successful neural network methods <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b4">5]</ref> report that dropout and momentum improve learning. In our experiment we also find these two techniques boost the performance.</p><p>Dropout is a technique that prevents overfitting in training neural networks. Typically the outputs of neurons are set to zero with a probability of 0.5 in the training stage and divided by 2 in the test stage. By randomly masking out the neurons, dropout is an efficient approximation of training many different networks with shared weights. In our experiments, since applying dropout to all layers significantly increases the time to reach convergence, we only apply dropout at the second fully connected layer.</p><p>Updating the network weights with momentum is a widely adopted strategy. We update the weights in the following form:</p><formula xml:id="formula_3">∆w t = r t ∆w t−1 − (1 − r t ) t w L w t = w t−1 + ∆w t t = 0 (d) t r t = t T r e + (1 − t T )r s , t &lt; T r e , t T<label>(4)</label></formula><p>where w t is weight at epoch t, 0 = 0.1 is learning rate, d = 0.9 is decay for the learning rate, r s = 0.9 and r e = 0.5 are starting and ending momentums respectively, T = 10 is a threshold to control how the momentum changes with the number of epochs. Note that unlike <ref type="bibr" target="#b4">[5]</ref> where momentum starts off at a value of 0.5 and stays at 0.99, we use a large momentum at the beginning and reduce it as the training progresses. We found through experiments that this setting can achieve better performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Experimental Protocol</head><p>Datasets: The following two datasets are used in our experiments.</p><p>(1) LIVE <ref type="bibr" target="#b14">[15]</ref>: A total of 779 distorted images with five different distortions -JP2k compression (JP2K), JPEG compression (JPEG), White Gaussian (WN), Gaussian blur (BLUR) and Fast Fading (FF) at 7-8 degradation levels derived from 29 reference images. Differential Mean Opinion Scores (DMOS) are provided for each image, roughly in the range [0, 100]. Higher DMOS indicates lower quality.</p><p>(2) TID2008 <ref type="bibr" target="#b11">[12]</ref>: 1700 distorted images with 17 different distortions derived from 25 reference images at 4 degradation levels. In our experiments, we consider only the four common distortions that are shared by the LIVE dataset, i.e. JP2k, JPEG, WN and BLUR. Each image is associated with a Mean Opinion Score (MOS) in the range [0, 9]. Contrary to DMOS, higher MOS indicates higher quality. Evaluation: Two measures are used to evaluate the performance of IQA algorithms: 1) Linear Correlation Coefficient (LCC) and 2) Spearman Rank Order Correlation Coefficient (SROCC). LCC measures the linear dependence between two quantities and SROCC measures how well one quantity can be described as a monotonic function of another quantity. We report results obtained from 100 train-test iterations where in each iteration we randomly select 60% of reference images and their distorted versions as the training set, 20% as the validation set, and the remaining 20% as the test set.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation on LIVE</head><p>On the LIVE dataset, for distortion-specific experiment we train and test on each of the five distortions: JP2K, JPEG, WN, BLUR and FF. For non-distortion-specific experiments, images of all five distortions are trained and tested together without providing a distortion type. <ref type="table" target="#tab_1">Table 1</ref> shows the results of the two experiments compared with previous state of the art NR-IQA methods as well as FR-IQA methods. Results of the best performing NR-IQA systems are in bold. The FR-IQA measures are evaluated by using 80% of the data for fitting a non-linear logistic function, then testing on 20% of the data. We can see from <ref type="table" target="#tab_1">Table 1</ref>    We visually examine the learned convolution kernels, and find only a few kernels present obvious structures related to the type of distortion. <ref type="figure" target="#fig_0">Figure 2</ref> shows the kernels learned on JPEG and all distortions combined respectively. We can see that blockiness patterns are learned from JPEG, and a few blur-like patterns exist for kernels learned from all distortions. It is not surprising that the kernels learned by CNN tend to be noisy patterns instead of presenting strong structure related to certain distortions as shown in CORNIA <ref type="bibr" target="#b19">[20]</ref>. This is because CORNIA's feature learning is unsupervised and belongs to generative model while our CNN is supervisedly trained and learns discriminative features.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Effects of Parameters</head><p>Several parameters are involved in the CNN design. In this section, we examine how these parameters affect the performance of the network on the LIVE dataset.</p><p>Number of kernels <ref type="figure" target="#fig_1">Figure 3</ref> shows how the performance varies with the number of convolution kernels. It is not surprising to find that the number of filters significantly affects the performance. In general, the use of more kernels leads to better performance. But little performance increase is gained when the number of kernels exceeds 40.</p><p>Kernel size We train and test the network with different kernel sizes while fixing the rest of structure. <ref type="table" target="#tab_2">Table 2</ref> shows how the performance changes with the kernel size. We can see from <ref type="figure" target="#fig_0">Figure 2</ref> that all tested kernel sizes show similar performance. The proposed network is not sensitive to kernel size.</p><p>Patch size Since in our experiment the whole image score is simply the average score of all patches sampled, we examine how the patch sampling strategy affects performance. This includes two aspects, patch size and number of patches per image. It is worth noting that if we keep sampling patches in a non-overlapping way, larger patch size leads to fewer patches. For example, if we double the patch size, the number of patches per image will drop to one fourth of the original number. To avoid this situation, we allow overlap sampling and use a fixed sampling stride (32) for different patch sizes. In this way the number of patches per image remains roughly the same (ignoring the border effect) when patch size varies. <ref type="table" target="#tab_3">Table 3</ref> shows the change of performance with respect to patch size. From   <ref type="figure">Figure 4</ref>: SROCC and LCC with respect to the sampling stride from 8 to 48. However larger patches not only lead to more processing time but also reduce spatial quality resolution. Therefore we prefer the smallest patch that yields the state of the art performance. Sampling stride To observe how the number of patches affects the overall performance, we fix the patch size and vary the stride. Changing the stride does not change the structure of the network. For simplicity at each iteration of the 100 iteration experiment, we use the same model trained at stride 32, and test with different different stride values. <ref type="figure">Figure 4</ref> shows the change of performance with respect to the stride. A larger stride generally leads to lower performance since less image information is used for overall estimation. However, it is worth noting that state of the art performance is still maintained even when the stride increases up to 128, which roughly corresponds to 1/16 of the original number of patches. This result is consistent with the fact that the distortions on the LIVE data are roughly homogeneous across entire image, and also indicates that our CNN can accurately predict quality score on small image patches.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Cross Dataset Test</head><p>Tests on TID2008 This set of experiment is designed to test the generalization ability of our method. We follow the protocol of previous work <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b19">20]</ref> to investigate cross dataset performance between the two datasets by training our CNN on LIVE and testing on TID2008 <ref type="bibr" target="#b0">1</ref> . Only the four types of distortions that are shared by LIVE and TID2008 are examined in this experiment. The DMOS scores in LIVE range from 0 to 100, while the MOS scores in TID2008 fall in the range 0 and 9. To make a fair comparison, we adopt the <ref type="bibr" target="#b0">1</ref> We have observed that some images in TID2008 share the same content as images in LIVE. However, their resolutions are different.  same method as <ref type="bibr" target="#b19">[20]</ref> to perform a nonlinear mapping on the predicted scores produced by the model trained on LIVE.</p><p>A nonlinear mapping based on a logistic function is usually applied to FR measures for transforming the quality measure into a certain range. We randomly split the TID2008 into two parts of 80% and 20% 100 times. Each time 80% of data is used for estimating parameters of the logistic function and 20% is used for testing, i.e. evaluating the transformed prediction scores. Results of the cross dataset test are shown in <ref type="table" target="#tab_6">Table 4</ref>. We can see that our CNN outperforms previous state of the art methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Local Quality Estimation</head><p>Our CNN measures the quality on small image patches, so it can be used to detect low/high quality local regions as well as giving a global score for the entire image.</p><p>We select an undistorted reference image from TID 2008 (which is not included in LIVE) and divide it into four vertical parts. We then replace the second to the fourth parts with distorted versions at three different degradation levels. Four synthetic images are generated in this way, one for each types of distortions including WN, BLUR, JPEG and JP2K. We then perform local quality estimation on these synthetic images using our model trained on LIVE. We scan 16 × 16 patches with a stride of 8 and normalize the predicted scores into the range [0, 255] for visualization. <ref type="figure" target="#fig_2">Figure 5</ref> shows estimated quality map on the synthetic images. We can see that our model properly distinguishes the clean and the distorted parts of each synthetic image.</p><p>To better examine the local quality estimation power of our model, we consider several types of distortions in TID2008 that are not used in previous experiments, and find three types that can only affect local regions: JPEG transmission, JPEG2000 transmission and blockwise distortion. Again from TID2008 we pick several images that are not shared by LIVE, and test on their distorted versions with the above three distortions. <ref type="figure" target="#fig_3">Figure 6</ref> shows the local quality estimation results. We find our model locates the distorted regions with reasonable accuracy and the results generally fit human judgement. It is worth noting that our model locates the blockwise distortion very well although this type of distortion is not contained in the training data from LIVE. In the images of the third row in <ref type="figure" target="#fig_3">Figure 6</ref>, the stripes on the window are mistaken as a low quality region. We speculate that it is because the local patterns on the stripes resem-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Computational Cost</head><p>Our CNN is implemented using the Python library Theano <ref type="bibr" target="#b1">[2]</ref>. With Theano we are able to easily run our algorithm on a GPU to speed up the process without much optimization. Our experiments are performed on a PC with 1.8GHz CPU and GTX660 GPU. We measure the processing time on images of size 512 × 768 using our model of 50 kernels with 32×32 input size, and test the model using part of those strides that give the state of the art performance in the experiments on LIVE. <ref type="table">Table 5</ref> shows the average processing time per image under different strides. Note that our implementation is not fully optimized. For example, the normalization process for each image is performed on the CPU in about 0.017 sec, which takes a significant portion of the total time. From <ref type="table">Table 5</ref> we can see that with a sparser sampling pattern (stride greater than 64), real time processing can be achieved while maintaining state of the art performance.  <ref type="table">Table 5</ref>: Time cost under different strides.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>We have developed a CNN for no-reference image quality assessment. Our algorithm combines feature learning and regression as a complete optimization process, which enables us to employ modern training techniques to boost performance. Our algorithm generates image quality predictions well correlated with human perception, and achieves state of the art performance on standard IQA datasets. Furthermore we demonstrated that our algorithm can estimate quality in local regions, which is rarely reported in previous literature and has many potential applications in image reconstruction or enhancement.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Learned convolution kernels on (a) JPEG (b) ALL on LIVE dataset</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: SROCC and LCC with respect to number of convolution kernels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Synthetic examples and local quality estimation results. The first row contains images distorted in (a) WN, (b) BLUR, (c) JPEG (d) JP2K. Each image is divided into four parts and three of them are distorted in different degradation level. The second row shows the local quality estimation results, where brighter pixels indicate lower quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Local quality estimation results on examples of non-global distortion from TID2008. Column 1,3,5 show (a) jpeg transmission errors (b) jpeg2000 transmission errors (c) local blockwise distortion. Column 2,4,6 show the local quality estimation results, where brighter pixels indicate lower quality.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>that our approach works well on each of</figDesc><table>SROCC 

JP2K 
JPEG 
WN 
BLUR 
FF 
ALL 
PSNR 
0.870 
0.885 
0.942 
0.763 
0.874 
0.866 
SSIM 
0.939 
0.946 
0.964 
0.907 
0.941 
0.913 
FSIM 
0.970 
0.981 
0.967 
0.972 
0.949 
0.964 
DIIVINE 
0.913 
0.910 
0.984 
0.921 
0.863 
0.916 
BLIINDS-II 
0.929 
0.942 
0.969 
0.923 
0.889 
0.931 
BRISQUE 
0.914 
0.965 
0.979 
0.951 
0.877 
0.940 
CORNIA 
0.943 
0.955 
0.976 
0.969 
0.906 
0.942 
CNN 
0.952 
0.977 
0.978 
0.962 
0.908 
0.956 

LCC 
JP2K 
JPEG 
WN 
BLUR 
FF 
ALL 
PSNR 
0.873 
0.876 
0.926 
0.779 
0.870 
0.856 
SSIM 
0.921 
0.955 
0.982 
0.893 
0.939 
0.906 
FSIM 
0.910 
0.985 
0.976 
0.978 
0.912 
0.960 
DIIVINE 
0.922 
0.921 
0.988 
0.923 
0.888 
0.917 
BLIINDS-II 
0.935 
0.968 
0.980 
0.938 
0.896 
0.930 
BRISQUE 
0.922 
0.973 
0.985 
0.951 
0.903 
0.942 
CORNIA 
0.951 
0.965 
0.987 
0.968 
0.917 
0.935 
CNN 
0.953 
0.981 
0.984 
0.953 
0.933 
0.953 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>SROCC and LCC on LIVE. Italicized are FR-IQA methods for reference.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>SROCC and LCC under different kernel sizes</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3</head><label>3</label><figDesc>we see that larger patch results in better performance. The performance increases slightly as the patch size increases</figDesc><table>size 

48 
40 
32 
24 
16 
SROCC 
0.959 
0.958 
0.956 
0.950 
0.946 
LCC 
0.957 
0.955 
0.953 
0.947 
0.946 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>SROCC and LCC on different patch size</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="false"><head>Table 4 :</head><label>4</label><figDesc>SROCC and LCC obtained by training on LIVE and testing on TID2008</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Representation learning: A review and new perspectives. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">35</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1798" to="1828" />
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Theano: a CPU and GPU math expression compiler</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bergstra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Breuleux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bastien</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Lamblin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Desjardins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Turian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Python for Scientific Computing Conference (SciPy)</title>
		<meeting>the Python for Scientific Computing Conference (SciPy)</meeting>
		<imprint>
			<date type="published" when="2010-06" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">A novel free reference image quality metric using neural network approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chetouani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Beghdadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mostafaoui</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Int. Workshop Video Process. Qual. Metrics Cons. Electron</title>
		<imprint>
			<date type="published" when="2010-01" />
			<biblScope unit="page" from="1" to="4" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Multi-column deep neural networks for image classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">C</forename><surname>Ciresan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Meier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="3642" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Improving neural networks by preventing co-adaptation of feature detectors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<idno>arxiv:1207.0580</idno>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning convolutional feature hierachies for visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-L</forename><surname>Boureau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mathieu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Blind image quality assessment using a general regression neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Neural Networks</title>
		<imprint>
			<biblScope unit="volume">22</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="793" to="799" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">No-reference image quality assessment in the spatial domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mittal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="4695" to="4708" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: From natural scene statistics to perceptual quality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">K</forename><surname>Moorthy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="3350" to="3364" />
			<date type="published" when="2011-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Rectified linear units improve restricted boltzmann machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="807" to="814" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">TID2008 -a database for evaluation of full-reference visual quality assessment metrics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Ponomarenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Lukin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zelensky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Egiazarian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Carli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Battisti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances of Modern Radio Electronics</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="30" to="45" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Blind image quality assessment: A natural scene statistics approach in the DCT domain</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Saad</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Charrier</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="3339" to="3352" />
			<date type="published" when="2012-08" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An information fidelity criterion for image quality assessment using natural scene statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Veciana</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2117" to="2128" />
			<date type="published" when="2005-12" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">LIVE image quality assessment database release 2</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">R</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Cormack</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Bovik</surname></persName>
		</author>
		<ptr target="http://live.ece.utexas.edu/research/quality" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Image quality assessment: from error visibility to structural similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Sheikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Simoncelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">13</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="600" to="612" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Information content weighting for perceptual image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1185" to="1198" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Learning without human scores for blind image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="995" to="1002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Gradient magnitude similarity deviation: A highly efficient perceptual image quality index. Image Processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bovik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="684" to="695" />
			<date type="published" when="2014-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Unsupervised feature learning framework for no-reference image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1098" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Real-time noreference image quality assessment based on filter learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Doermann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition (CVPR)</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="987" to="994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">FSIM: A feature similarity index for image quality assessment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Mou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Zhang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Image Processing</title>
		<imprint>
			<biblScope unit="volume">20</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="2378" to="2386" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
