<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:58+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">AMNet: Memorability Estimation with Attention</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiri</forename><surname>Fajtl</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kingston University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vasileios</forename><surname>Argyriou</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kingston University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dorothy</forename><surname>Monekosso</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">Leeds Beckett University</orgName>
								<address>
									<settlement>Leeds</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Paolo</forename><surname>Remagnino</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Kingston University</orgName>
								<address>
									<settlement>London</settlement>
									<country key="GB">UK</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">AMNet: Memorability Estimation with Attention</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The ability of man cognition to recall as well as forget visual content after viewing it is very important to the way we acquire new information and interact with our environment. This is becoming increasingly significant as creating and consuming visual content dominates other forms of information exchange. Moreover, low cost, automated image and video capture systems are rapidly surfacing as the norm in the Internet of the Things (IoT) domain, also contributing to the visual information flow.</p><p>To which degree an image is later remembered or forgotten is expressed as image memorability. It is an important cognitive measure to be taken into account while processing visual content, whether for human to human or machine to human communication or for storage.</p><p>Memorability estimation has a large variety of practical applications, such as selecting or designing highly memorable advertising material, organizing and tagging of photos in albums, introducing a real-time, image memorability measure built into consumer digital cameras, helping to make highly memorable presentations and data visualizations, improving memorability of specific parts of a graphical user interface (GUI) or helping to illustrate education material. An application of a great interest is to measure a decline in memory capacity of patients affected by de- Prior research <ref type="bibr" target="#b12">[13]</ref> has shown that image memorability has a stable property, that is, individuals tend to remember the same images with the same probability regardless of delays, and that it can be quantified and measured. This research has led to first attempts to learn and predict memorability with machine learning freameworks, initially with low-level, global image features <ref type="bibr" target="#b11">[12]</ref>, reaching moderate success. To improve such a solution would, however, require the design of new features, which demands a strong domain knowledge not well understood in the specific case of memorability.</p><p>In <ref type="bibr" target="#b36">[37]</ref>, <ref type="bibr" target="#b1">[2]</ref> and recently <ref type="bibr" target="#b13">[14]</ref> has been shown that this problem can be mitigated by applying deep learning techniques to the memorability domain. Deep learning, however, requires large training dataset which was not available until A. Khosla et al. <ref type="bibr" target="#b17">[18]</ref> introduced a large memorability dataset LaMem with 60K images and subsequently used it to train the MemNet, which is based on the AlexNet <ref type="bibr" target="#b20">[21]</ref> initialized on the ImageNet <ref type="bibr" target="#b29">[30]</ref> and Places <ref type="bibr" target="#b38">[39]</ref> datasets. MemNet achieves Spearman's rank correlation ρ = 0.64 compared with the human consistency ρ = 0.68 as measured by <ref type="bibr" target="#b17">[18]</ref>.</p><p>Intuitively, image regions immediately drawing our attention would appear to be linked with highly memorable visual content. Indeed, this assumption was confirmed to be correct in the works of <ref type="bibr" target="#b23">[24]</ref>, <ref type="bibr" target="#b18">[19]</ref> and <ref type="bibr" target="#b12">[13]</ref> who already very early indicated a potential relationship between the visual attention and memorability but did not further investigate their correlation. To that end, we propose the Attention based Memorability estimation Network-AMNet, a novel, deep neural network architecture with a recurrent, visual attention mechanism with the primary goal to improve on the state of the art for the memorability prediction task. We also show advantages of the visualization of the generated attention maps and their connection to the memorability property. Our approach is extensively evaluated on the LaMem <ref type="bibr" target="#b17">[18]</ref> and SUN Memorability <ref type="bibr" target="#b12">[13]</ref> datasets. The main contributions of our work are:</p><p>• AMNet as a generic architecture for regression tasks with deep CNN, visual attention mechanism and recurrent neural network.</p><p>• application of the proposed AMNet to the image memorability estimation.</p><p>• introduction of the incremental memorability estimation with the recurrent network and demonstration of the achieved performance gain.</p><p>• introduction of the visual attention technique for the memorability estimation and presentation of the performance gain.</p><p>• demonstration that transfer learning from deep models, trained for image classification, is particularly beneficial for the memorability estimation.</p><p>The paper is organized as follows: Section 2 provides background material on image memorability, its properties, measurement and prediction. In section 3 we propose the AMNet and discuss the theoretical framework behind this architecture and the training procedure. The performance of AMNet is studied in the section 4, with section 5 concluding this work.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head><p>In a pioneering work on image memorability, Isola et al. <ref type="bibr" target="#b12">[13]</ref>, <ref type="bibr" target="#b10">[11]</ref> demonstrated that the ability of our cognition system to remember certain images and forget other is congruent among independent observers, despite large variability in the image content, concluding memorability is a stable property, intrinsic to images. Based on this premise, Isola et al. <ref type="bibr" target="#b12">[13]</ref> investigated factors that give rise to the image memorability effect, which was then used to predict image memorability scores with a machine learning program, based on global image features GIST <ref type="bibr" target="#b25">[26]</ref>, SIFT <ref type="bibr" target="#b21">[22]</ref>, HOG <ref type="bibr" target="#b4">[5]</ref>, SSIM <ref type="bibr" target="#b32">[33]</ref> and pixel histogram.</p><p>In order to build better computational models to learn and predict memorability, researchers analyzed the relationship between memorability and various visual factors <ref type="bibr" target="#b18">[19]</ref>, image classes <ref type="bibr" target="#b11">[12]</ref> and saliency <ref type="bibr" target="#b6">[7]</ref>. Bylinskii et al. <ref type="bibr" target="#b2">[3]</ref> conducted a number of experiments to better understand the intrinsic and extrinsic effects on image memorability, concluding that the primary substrate of memorability lies in the intrinsic properties of images and all extrinsic effects contribute only marginally.</p><p>Deep learning was first applied to the memorability problem by Baveye et al. <ref type="bibr" target="#b1">[2]</ref> who proposed a MemoNet model based on GoogLeNet <ref type="bibr" target="#b33">[34]</ref> trained on the ImageNet <ref type="bibr" target="#b29">[30]</ref> dataset. <ref type="bibr" target="#b36">[37]</ref> used CNN features with SVR <ref type="bibr" target="#b5">[6]</ref> to predict memorability with accuracy comparable to MemoNet <ref type="bibr" target="#b1">[2]</ref>.</p><p>To achieve higher accuracy with deep learning techniques Khosla et al. <ref type="bibr" target="#b17">[18]</ref> collected a large memorability dataset LaMem with 60K images and introduced MemNet model based on the Hybrid-CNN, which is the AlexNet <ref type="bibr" target="#b20">[21]</ref> CNN pretrained on the ImageNet <ref type="bibr" target="#b29">[30]</ref> and the Places <ref type="bibr" target="#b38">[39]</ref> datasets (∼3.6 million images in total). Researchers also tried to improve memorability prediction by other techniques, such as the adaptive transfer learning from external sources <ref type="bibr" target="#b13">[14]</ref> or predicting image memorability by multiview adaptive regression <ref type="bibr" target="#b26">[27]</ref>, none exceeding the performance of the MemNet <ref type="bibr" target="#b17">[18]</ref>.</p><p>Relationship between the visual attention and memorability was already suggested by Isola et al. <ref type="bibr" target="#b12">[13]</ref> but was not further investigated. Mancas and Le Meur <ref type="bibr" target="#b23">[24]</ref> studied the link between saliency and memorability and found that the most memorable images have uniquely localized regions, while less memorable either do not have precise regions of interest or have several of them. Based on these findings, <ref type="bibr" target="#b23">[24]</ref> devised new attention-related features that improved the memorability prediction by 2% compared to the non attention based models from <ref type="bibr" target="#b12">[13]</ref>. In a similar work, Celikkale et al. <ref type="bibr" target="#b3">[4]</ref> applied an attention driven spatial pooling pipeline based on SIFT <ref type="bibr" target="#b21">[22]</ref> and HOG <ref type="bibr" target="#b4">[5]</ref> features and bottom-up and object-level saliency detectors. Their results, albeit only moderate, still indicate a benefit of the attention based approach. Importance of the memorability regions was explored by Khosla et al. <ref type="bibr" target="#b18">[19]</ref> who introduced the concept of attention maps that relate image regions to memorability. These maps are learnt directly as clusters of gradients, textures and color features with the SVM-Rank solver <ref type="bibr" target="#b14">[15]</ref> with results showing benefits of the attention on memorability prediction.</p><p>In our work we investigate the application of deep learn-ing methods with visual attention and recurrent network to learn and predict image memorability. To our knowledge the presented approach has not been attempted before.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Method</head><p>The idea behind the AMNet architecture is based on four main components a deep CNN trained on large-scale image classification task, a soft attention network, a Long Short Term Memory (LSTM) <ref type="bibr" target="#b8">[9]</ref> recurrent neural network followed by a fully connected neural network for memorability score regression.</p><p>In the following section we introduce the details of the AMNet architecture as shown in <ref type="figure">Figure 2</ref>, starting with the pre-trained CNN (a) for transfer learning. Subsequently we show the working of the visual, soft attention mechanism (b), the LSTM and network for the memorability regression (c) and (d). Finally we outline the training procedure and finish with the data augmentation process.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Transfer Learning for Memorability Estimation</head><p>It is common practice to use a pretrained CNN as a fixed feature extractor or to fine tune it for a similar application <ref type="bibr" target="#b31">[32]</ref>, mainly to reduce training time and overfitting on tasks with small datasets.</p><p>This technique is readily applied to computer vision problems centered around semantic features such as objects detection and segmentation, however little is known about such transfer learning for the image memorability estimation since there is no clear understanding of what visual features trigger the effects of remembering and forgetting.</p><p>Khosla et al. <ref type="bibr" target="#b17">[18]</ref> has already shown the benefits of fine tuning of pretrained CNN for this domain, however we decided to evaluate a much deeper model as a fixed feature extractor. Our results show that the features learnt for image classification are highly suitable for the memorability task. In our work we use ResNet50 <ref type="bibr" target="#b9">[10]</ref> model trained on ImageNet where it achieves the top 1 error 24.7%.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Soft Attention Mechanism</head><p>The ability of a neural network to learn which discrete information elements to focus on within a given training sample was first applied in machine translation by Bahdanau et al. <ref type="bibr" target="#b0">[1]</ref>. This mechanism is called soft attention due to the fact that it produces a probability weight for every information element rather than a hard decision boundary. The benefit of soft attention is that it can be learnt end-to-end with a gradient based optimization method.</p><p>The soft attention mechanism has two components, a network that learns probabilities for each information element within the input data and a gating function that uses these probabilities to weigh data for further processing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">AMNet Details</head><p>The AMNet estimates the image memorability by taking a single image X and generating a memorability score y.</p><formula xml:id="formula_0">y = f (X), y = [0, 1]<label>(1)</label></formula><p>The process of memorability estimation is summarized in algorithm 1.</p><formula xml:id="formula_1">Algorithm 1 AMNet algorithm 1: procedure MEMORABILITY(X) ⊲ y = f (X) 2:</formula><p>x = get cnn features(X) ⊲ ResNet50 fwd pass 3:</p><formula xml:id="formula_2">h 0 = f initc (x) ⊲ Eq. 12</formula><p>4:</p><formula xml:id="formula_3">c 0 = f init h (x) ⊲ Eq. 12 5:</formula><p>lstm init(h 0 , c 0 )</p><p>6:</p><formula xml:id="formula_4">y = 0 7: for t = 0 to T do ⊲ at t = 0 → h t = h 0 8: e = f att (x, h t ) ⊲ Eq. 8 9:</formula><p>α = sof tmax(e) ⊲ Eq. 6</p><p>10:</p><formula xml:id="formula_5">z = [] 11:</formula><p>for i = 0 to L do ⊲ for all locations, Eq. 4</p><p>12:</p><formula xml:id="formula_6">z = z + α i x i ⊲ z ∈ R D 13: h t , c t = lstm step(z, h t , c t ) ⊲ Eq. 3</formula><p>14: </p><formula xml:id="formula_7">y = y + f m (h t ) ⊲ Eq.</formula><formula xml:id="formula_8">x = {x 1 , ..., x L } x i ∈ R D<label>(2)</label></formula><p>All vectors are column vectors, unless stated otherwise. The memorability is estimated with LSTM <ref type="bibr" target="#b8">[9]</ref> over a three steps long sequence T = 3. The LSTM is defined as:</p><formula xml:id="formula_9">h t = φ(h t−1 , z t ) t = [0, T ), h ∈ R B<label>(3)</label></formula><p>where h t is the LSTM state at time t with size B = 1024. The vector z t represents a new image features produced at the step t as a result of the application of the attention weights α t on the input image features x and is calculated as a simple weighted sum such that</p><formula xml:id="formula_10">z t = L i=1 α t,i x i z t ∈ R D<label>(4)</label></formula><p>where α are the attention probabilities conditioned on the entire image feature vector x and previous LSTM hidden state h t−1  <ref type="figure">Figure 2</ref>: A pretrained RestNet50 (a) is followed by the soft attention mechanism (b) with LSTM (c), which over a sequence of three steps T = 3 produces attention maps, each conditioned on the previous LSTM state h t−1 and the entire image feature vector x. Memorability y is then calculated as a sum of discrete memorability scores in the regression network (d).</p><formula xml:id="formula_11">α t ∼ p(α t |x, h t−1 ) α t ∈ R L<label>(5)</label></formula><p>The attention probabilities, as well as other functions are parameterised with neural networks. The attention is then represented as a vector of weights produced by a softmax function</p><formula xml:id="formula_12">α t,i = exp(e t,i ) L k=1 exp(e t,k )<label>(6)</label></formula><p>The attention weights vector e t is a product of the image feature vector x and the LSTM hidden state h t−1</p><formula xml:id="formula_13">e t,i = f att (x i , h t−1 )<label>(7)</label></formula><p>f att () is s simple sum of two affine transformations followed by logistic function</p><formula xml:id="formula_14">f att (x i , h t−1 ) = M i tanh(U h t−1 + Kx i + b) (8)</formula><p>where M L×D , U D×B , K D×D and b D×1 are network weights and biases respectively, estimated together with other parameters of the network during optimization. In order to experiment with the effects of the attention we can conditionally disable it by defining the f att () as a constant function with unit output such that:</p><formula xml:id="formula_15">f att (x i , h t−1 ) = 1<label>(9)</label></formula><p>The results it that all feature vectors in x are considered equally, thus disabling the attention mechanism. At each step t the network produces one discrete memorability score m t calculated as:</p><formula xml:id="formula_16">m t = f m (h t )<label>(10)</label></formula><p>The function f m () maps the LSTM hidden state h t to the memorability score m t = [0, 1]. It is implemented as a two-layer neural network for regression with a single output neuron and linear activation function. Finally, the total image memorability score y is calculated as a sum of the discrete memorabilities m t</p><formula xml:id="formula_17">y = T t m t<label>(11)</label></formula><p>In the first step, the LSTM hidden h 0 and memory c 0 states are initialized from the image feature vector x as follows:</p><formula xml:id="formula_18">c 0 = f initc 1 L L i x i h 0 = f init h 1 L L i x i<label>(12)</label></formula><p>where the f init () functions are single, fully connected neural networks with tanh() activation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Training Procedure</head><p>The AMNet model is trained by minimizing the following loss function:</p><formula xml:id="formula_19">L = (ŷ − y) 2 + λL α<label>(13)</label></formula><p>The first term represents a mean squared error between the ground truthŷ and predicted image memorability y. In order to encourage the attention model to explore all image regions over all time steps, we add a second term λL α which performs a joint ℓ 1 -ℓ 2 penalty as a function of activations of all attention maps in the LSTM sequence T , introduced by Xu et al. <ref type="bibr" target="#b35">[36]</ref>. The hyper-parameter λ specifies the impact of this penalty.</p><formula xml:id="formula_20">L sα = L i s 2 i<label>(14)</label></formula><p>s i represents the ℓ 1 penalty, which enforces sparsity along the sequence dimension T . In other words, it encourages a strong activation for only one of the attention maps at location i.</p><formula xml:id="formula_21">s i = 1 − T t α t,i<label>(15)</label></formula><p>Finally, the ℓ 2 penalty in the form of i s 2 i in Eq. 14 further promotes an even distribution of activations over all locations. The value of the λ parameter was experimentally determined as 10 −4 for which the network achieved the highest performance.</p><p>The entire model if fully differentiable and trained endto-end with the ADAM <ref type="bibr" target="#b19">[20]</ref> optimizer with a fixed learning rate 10 −3 . The input image feature vector x is extracted from the 43 rd layer of the RestNet50 <ref type="bibr" target="#b9">[10]</ref> with dimensions [14 × 14 × 1024]. The ResNet50 is trained for image classification on the ImageNet dataset and its weights are not updated during the AMNet training.</p><p>The AMNet network is heavily regularized with dropout and with small ℓ 2 weights regularization 10 −6 . We found that the dropout was critical to stop the network from overfitting. The training was carried out in minibatches of 256 images and terminated by early stopping when the observed Spearman's rank correlation on the validation dataset reached its maximum, which was between epoch 30 and 50 depending on the split and the training dataset (LaMem or SUN). Training and validation losses as well as the memorability rank correlation on the validation dataset in the LaMem, split 1 is showin in <ref type="figure" target="#fig_2">Figure 3.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Data Preprocessing and Augmentation</head><p>Common augmentation techniques are applied to the images during the training stage to reduce overfitting and improve generalization. A crop of random size of (0.08 to 1.0) of the original size and a random aspect ratio of 3/4 to 4/3 of the original aspect ratio is made and then resized to 224 × 224 and randomly, horizontally flipped. For the evaluation only a center crop 224 × 224 was selected for the input.</p><p>Memorability scores in the LaMem dataset are in the range [0, 1] with distribution shown in <ref type="figure" target="#fig_3">Figure 4</ref>. For the training purpose the memorability scores were zero mean centered and scaled to range [−1, 1].</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experimental Results</head><p>In this sections we evaluate the AMNet on the LaMem <ref type="bibr" target="#b17">[18]</ref> and SUN Memorability <ref type="bibr" target="#b12">[13]</ref> datasets. First we briefly describe the datasets and used evaluation metrics, and then present our qualitative and quantitative results with the comparison against the state of the art.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>Main focus of this research work is on the LaMem <ref type="bibr" target="#b17">[18]</ref> dataset due to its large size which makes it suitable for train- ing deep neural networks. The LaMem is the largest annotated image memorability dataset to this date with total of 58741 images. The images cover a wide range of indoor and outdoor environments, objects and people and were obtained from other labeled datasets such as MIR Flicker, AVA dataset <ref type="bibr" target="#b24">[25]</ref>, affective images dataset <ref type="bibr" target="#b15">[16]</ref>, image saliency datasets <ref type="bibr" target="#b22">[23]</ref>, <ref type="bibr" target="#b28">[29]</ref>, SUN <ref type="bibr" target="#b34">[35]</ref>, image popularity dataset <ref type="bibr" target="#b16">[17]</ref>, Abnormal Objects dataset <ref type="bibr" target="#b30">[31]</ref> and a Pascal dataset <ref type="bibr" target="#b7">[8]</ref>. The memorability scores were collected manually on the Amazon Mechanical Turk (AMT) by means of a memorability game introduced by <ref type="bibr" target="#b12">[13]</ref> and improved by <ref type="bibr" target="#b17">[18]</ref>. Approximately 80 measurements (memorable=yes/no) were collected per image. There are 5 random splits each with 45000 images for training, 3741 for evaluation and 10000 for testing.</p><p>As a second dataset for evaluation we chose the SUN Memorability dataset pioneered by Isola et al. <ref type="bibr" target="#b12">[13]</ref>. There are 2222 images in total, originating from the SUN <ref type="bibr" target="#b34">[35]</ref> dataset with memorability scores collected similarly to the LaMem. There are 25 random splits with equal number of 1111 images for training and testing.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Evaluation Metrics</head><p>Following the previous work, we report on the performance in terms of rank correlation, specifically a Spearman's rank correlation coefficient <ref type="bibr" target="#b27">[28]</ref> ρ and mean squared error MSE.</p><p>The Spearman's rank correlation coefficient measures consistency between the predicted and ground truth ranking, within the range [−1, +1] where zero represents no correlation. Higher ρ values indicate better memorability prediction method: </p><formula xml:id="formula_22">ρ s (r, r) = 1 − 6 N i (r i − r i ) 2 N (N 2 − 1)<label>(16)</label></formula><p>where N is a number of samples,r i is a rank of the i th ground truth memorability score, and r i the i th prediction. MSE is used as a secondary metric, not always presented in previous work. The Spearman's rank correlation shows a monotonic relationships between the reference and observations but does not reflect the absolute numerical errors between them, which is then presented by MSE according to:</p><formula xml:id="formula_23">MSE(ŷ, y) = 1 N N i=1 (ŷ i − y i ) 2<label>(17)</label></formula><p>whereŷ i is the ground truth memorability score, while y i the prediction and N number of tested samples.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Performance Evaluation</head><p>In order to obtain results that are fully comparable with the previous work, we used the same training and evaluation protocol as in the <ref type="bibr" target="#b17">[18]</ref> for the LaMem dataset and <ref type="bibr" target="#b12">[13]</ref> for the SUN memorability dataset.</p><p>Evaluation on the LaMem dataset was performed by training one model on each of the five random splits as suggested by the authors <ref type="bibr" target="#b17">[18]</ref> and then reporting the final memorability rank correlation and MSE , averaged over the results from five corresponding test datasets.</p><p>In <ref type="table">Table 1</ref> we show that the AMNet model with the active attention achieves ρ = 0.677, or a 5.8% improvement over the best known method MemNet <ref type="bibr" target="#b17">[18]</ref>. Even without attention the AMNet outperforms prior work by 3.6% which demonstrates that the pretrained, deep CNN with our recurrent and regression network layers still achieve high</p><formula xml:id="formula_24">Method (LaMem dataset) ρ ↑ MSE ↓ AMNet 0.677 0.0082 AMNet (no attention)</formula><p>0.663 0.0085 MemNet <ref type="bibr" target="#b17">[18]</ref> 0.64 NA CNN-MTLES <ref type="bibr" target="#b13">[14]</ref> (different train/test (50/50) split) 0.5025 NA <ref type="table">Table 1</ref>: Average Spearman's rank correlation ρ and MSE over 5 test splits of the LaMem dataset.</p><p>Method (SUN Memorability dataset) ρ ↑ MSE ↓ Isola <ref type="bibr" target="#b12">[13]</ref> 0.462 0.017 Mancas &amp; Le Meur <ref type="bibr" target="#b23">[24]</ref> 0.479 NA AMNet 0.649 0.011 AMNet (no attention) 0.62 0.012 MemNet <ref type="bibr" target="#b17">[18]</ref> 0.63 NA MemoNet 30k <ref type="bibr" target="#b1">[2]</ref> 0.636 0.012 Hybrid-CNN+SVR <ref type="bibr" target="#b36">[37]</ref> 0.6202 0.013 <ref type="table">Table 2</ref>: Evaluation on the SUN Memorability dataset. All models were trained and tested on the 25 train/val splits.</p><p>accuracy. The comparatively low performance of the CNN-MTLES <ref type="bibr" target="#b13">[14]</ref> method can be attributed to the fact that this model uses various, specifically engineered visual features and features extracted from CNN networks trained on ImageNet <ref type="bibr" target="#b29">[30]</ref> and Places <ref type="bibr" target="#b38">[39]</ref>. Thus it does not leverage the end-to-end deep learning. The CNN-MTLES, however, uses the LaMem dataset, which indicates that even a large dataset does not significantly improve the performance of models based on engineered visual features. To train the deep AMNet model on the rather small SUN dataset we had to increase regularization to avoid overfitting. We found that in this specific case ℓ 2 = 10 −4 weights regularization performed better than a stronger dropout or the combination of both. <ref type="table">Table 2</ref> shows that the AMNet with attention performs 2% better than the current best model. By disabling the attention the performance declined to ρ = 0.62, demonstrating the advantages of visual attention for this task.</p><p>We found that during training MSE on the validation datasets follows a similar trend with the rank correlation ρ, however the ρ peaks after the model starts overfitting as seen in <ref type="figure" target="#fig_2">Figure 3</ref>. It is conceivable to assume that the slightly higher variance at the maximum ρ improves generalization in terms of the predicted and ground truth monotonic relationships, even though MSE starts increasing. For example, during the training on the LaMem split 1, as seen in <ref type="figure" target="#fig_2">Figure  3</ref>, we attained maximum ρ = 0.6721 and MSE = 0.00848 while ρ = 0.6676 for minimum MSE = 0.00844. <ref type="table">Tables 1 and 2</ref> show that the AMNet exhibits the best performance in terms of the Spearman's rank correlation as well as MSE on both, the LaMem and the SUN datasets. The best performance attains ρ = 0.677 on the LaMem dataset, approaching 99.6% of the human performance ρ = 0.68 as measured by Khosla et al. <ref type="bibr" target="#b17">[18]</ref>. Comparison against the state of the art can be seeing in <ref type="figure" target="#fig_6">Figure 7.</ref> </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">The Role of Attention on Memorability</head><p>The significant performance gain is achieved by the fact that the neural network learns to focus its attention to specific regions most relevant to memorability. The improvement is close to 2% on the LaMem and almost 5% on the SUN dataset. AMNet learns to explore the image content by producing three visual attention maps, each conditioned on the image content obtained by exploiting the previous map. We have experimented with 2,3,4,5 and 6 LSTM steps and found that three steps are sufficient to achieve the reported performance.</p><p>In order to interpret the relation between the attention maps and corresponding discrete memorability estimations in each LSTM step, we converted the attention maps to heat maps and visualized them along with the memorability scores. In <ref type="figure">Figure 5</ref> we show selected images from the LaMem, split 2 test dataset. Images As we can see in images (a), (c) and (d) in <ref type="figure">Figure 5</ref>, most of the first attention weights gravitate towards the image center, which is most likely caused by the Center Bias, studied in <ref type="bibr" target="#b15">[16]</ref>, <ref type="bibr" target="#b37">[38]</ref> and attributed primarily to the photographer bias. In the subsequent LSTM steps, however, the attention usually moves to the regions responsible for memorability.</p><p>After a close inspection, we found that the attention maps for low memorability images tend to be sparser with few small peaks, while for higher image memorability, the attention maps display sharper focus covering larger regions around the activation peaks. Core image memorability usually originates in regions with people and human faces as evident in images (c) and (f) in <ref type="figure">Figure 5</ref>.</p><p>Moreover, we found that the estimates of discrete memorabilities m t in Eq. 10 decrease with each LSTM step t for low memorability images, while for high memorability images they grow. This relation is shown in <ref type="figure" target="#fig_5">Figure 6</ref>. This effects is consistent within the LaMem test datasets across all splits and can be seen in <ref type="figure">Figure 5</ref>.</p><p>Initially, we experimented with additional penalty function that would encourage the optimizer to estimate the discrete memorabilities in ascending or descending order, however this always caused a drop in the performance. The   <ref type="table">1  2  3  1  2  3  1  2  3  1  2  3  1  2  3  1  2  3</ref> Figure 5: Examples of attention maps for low and high memorability images from LaMem test dataset split 2. Tested images, their estimated and ground truth memorabilities (in brackets) are shown in the top raw. Bellow each image is a discrete memorability score estimated at the steps t 1 , t 2 and t 3 . Plots at the bottom row show gradients over three LSTM steps.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work we propose AMNet, a novel deep neural network with visual attention component for image memorability estimation. This network consists of a pre-trained, deep CNN followed by a modified visual attention mechanism with a recurrent network and network for memorability regression. By design the AMNet is generic and could be employed for other regression, computer vision tasks.</p><p>We show that a deep CNN, trained on large-scale image classification is beneficial for the memorability estimation task, indicating that the feature hierarchies extracted for the image classification are suitable to express the composition underlying the memorability effect.</p><p>Finally, we demonstrate that our recurrent visual attention network significantly improves performance of the image memorability learning and inference.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: AMNet iteratively generates attention maps linked to the image regions correlated with the memorability. After three iterations the memorability scores are added and presented on the output.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Formally, let the image features, extracted by a CNN, be a tensor with dimensions (W, H, D) where W and H represent the spatial resolution while D a length of feature vectors, one for each location within the (W, H) region. Specifically, in the case of AMNet the feature tensor has di- mensions 14 × 14 × 1024. In general there are L = W × H locations, represented as a vector x:</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Training/validation losses and memorability rank correlation on the validation dataset in the LaMem split1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Histogram of ground truth memorability scores in the LaMem [18] training dataset split1.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head></head><label></label><figDesc>(a), (b) and (c) have low memorability, image (d) a medium one and (e) and (f) high memorability. Images of the attention maps are obtained by taking the output of the softmax function Eq. 6, scaled to range [0, 255] and resized from 14 × 14 to 244 × 244.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Histogram of gradients of discrete memorabilities over the LSTM steps. The gradient is directly proportional to the total image memorability.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Comparison against the state of the art methods. Red depicts deep learning based methods. AMNet, MemNet and CNN-MTLES [14] where trained on the LaMem, the rest on the SUN Memorability dataset.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>The proposed method outperforms previous state of the art work by 5.8% (from ρ = 0.64 to ρ = 0.677) on the Spearman's rank correlation and closely approaches the human performance ρ = 0.68 with a 99.6% consistency. The AMNet implementation in PyTorch, including all trained models, will be made publicly available.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Neural machine translation by jointly learning to align and translate</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Bahdanau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.0473</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Deep learning for image memorability prediction: The emotional bias</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Baveye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cohendet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Perreira Da</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Le</forename><surname>Silva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Callet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the ACM International Conference on Multimedia</title>
		<meeting>the ACM International Conference on Multimedia<address><addrLine>New York, NY, USA</addrLine></address></meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="491" to="495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Intrinsic and extrinsic effects on image memorability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Bainbridge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computational Models of Visual Attention</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="page" from="165" to="178" />
		</imprint>
	</monogr>
	<note>Part B</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Visual attentiondriven spatial pooling for image memorability</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Celikkale</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Erdem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Erdem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition Workshops</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="976" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Computer Vision and Pattern Recognition</title>
		<meeting>the Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Support vector regression machines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Drucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J C</forename><surname>Burges</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaufman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vapnik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems 9</title>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="1997" />
			<biblScope unit="page" from="155" to="161" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">What makes an object memorable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Dubey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Peterson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-H</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Ghanem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1089" to="1097" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Describing objects by their attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Endres</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Forsyth</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="1778" to="1785" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">LSTM: A search space odyssey</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Greff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">K</forename><surname>Srivastava</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Koutn&amp;apos;ik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">R</forename><surname>Steunebrink</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on neural networks and learning systems</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Understanding the intrinsic memorability of images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="2429" to="2437" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">What makes a photograph memorable</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2014-07" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="1469" to="1482" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">What makes an image memorable?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="145" to="152" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Predicting image memorability through adaptive transfer learning from external sources</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Jing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Gu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">19</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="1050" to="1062" />
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Training linear SVMs in linear time</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Joachims</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</title>
		<meeting>the 12th ACM SIGKDD international conference on Knowledge discovery and data mining</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2006" />
			<biblScope unit="page" from="217" to="226" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV)</title>
		<imprint>
			<date type="published" when="2009" />
			<biblScope unit="page" from="2106" to="2113" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">What makes an image popular?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Das</forename><surname>Sarma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hamid</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the international conference on World wide Web</title>
		<meeting>the international conference on World wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="867" to="876" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Understanding and predicting image memorability at a large scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">S</forename><surname>Raju</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2390" to="2398" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Memorability of image regions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="296" to="304" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page">25</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Distinctive image features from ScaleInvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Affective image classification using features inspired by psychology and art theory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Machajdik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Hanbury</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 18th ACM international conference on Multimedia</title>
		<meeting>the 18th ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="83" to="92" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Memorability of natural scenes: The role of attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mancas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><forename type="middle">Le</forename><surname>Meur</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="196" to="200" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">AVA: A largescale database for aesthetic visual analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Marchesotti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Perronnin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="2408" to="2415" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Modeling the shape of the scene: A holistic representation of the spatial envelope</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">42</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="145" to="175" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Predicting image memorability by multi-view adaptive regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Hu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd ACM international conference on Multimedia</title>
		<meeting>the 23rd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1147" to="1150" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Spearman rank correlation coefficient. Encyclopedia of statistical sciences</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Pirie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1988" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">An eye fixation database for saliency detection in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ramanathan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Katti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kankanhalli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-S</forename><surname>Chua</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="30" to="43" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Imagenet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Others</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="211" to="252" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Object-centric anomaly detection by attribute-based reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Saleh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Elgammal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="787" to="794" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">CNN features off-the-shelf: an astounding baseline for recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Sharif Razavian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Azizpour</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sullivan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Carlsson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition workshops</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition workshops</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="806" to="813" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Matching local self-similarities across images and videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shechtman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Irani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Sun database: Large-scale scene recognition from abbey to zoo</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page" from="3485" to="3492" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Show, attend and tell: Neural image caption generation with visual attention</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhudinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2048" to="2057" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Image memorability prediction using deep features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zarezadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rezaeian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">T</forename><surname>Sadeghi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Iranian Conference on Electrical Engineering (ICEE)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="2176" to="2181" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<analytic>
		<title level="a" type="main">Cottrell. SUN: A bayesian framework for saliency using natural statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Tong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Marks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Shan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">W</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of vision</title>
		<imprint>
			<biblScope unit="volume">8</biblScope>
			<biblScope unit="issue">7</biblScope>
			<biblScope unit="page">32</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<analytic>
		<title level="a" type="main">Learning deep features for scene recognition using places database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Lapedriza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14</title>
		<meeting>the 27th International Conference on Neural Information Processing Systems, NIPS&apos;14<address><addrLine>Cambridge, MA, USA</addrLine></address></meeting>
		<imprint>
			<publisher>MIT Press</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="487" to="495" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
