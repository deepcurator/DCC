<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:35+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Inverse Compositional Spatial Transformer Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen-Hsuan</forename><surname>Lin</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Lucey</surname></persName>
							<email>slucey@cs.cmu.edu</email>
							<affiliation key="aff0">
								<orgName type="department">The Robotics Institute</orgName>
								<orgName type="institution">Carnegie Mellon University</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Inverse Compositional Spatial Transformer Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we establish a theoretical connection between the classical Lucas &amp; Kanade (LK)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Recent rapid advances in deep learning are allowing for the learning of complex functions through convolutional neural networks (CNNs), which have achieved stateof-the-art performances in a plethora of computer vision tasks <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b3">4]</ref>. Most networks learn to tolerate spatial variations through: (a) spatial pooling layers and/or (b) data augmentation techniques <ref type="bibr" target="#b15">[16]</ref>; however, these approaches come with several drawbacks. Data augmentation (i.e. the synthetic generation of new training samples through geometric distortion according to a known noise model) is probably the oldest and best known strategy for increasing spatial tolerance within a visual learning system. This is problematic as it can often require an exponential increase in the number of training samples and thus the capacity of the model to be learned. Spatial pooling operations can partially alleviate this problem as they naturally encode spatial invariance within the network architecture and uses subsampling to reduce the capacity of the model. However, they have an intrinsic limited range of tolerance to geometric variation they can provide; furthermore, such pooling operations destroy spatial details within the images that could be crucial to the performance of subsequent tasks.</p><p>Instead of designing a network to solely give tolerance to spatial variation, another option is to have the network solve for some of the geometric misalignment in the input images <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. Such a strategy only makes sense, however, if it has lower capacity and computational cost as well as better performance than traditional spatially invariant CNNs. Spatial Transformer Networks (STNs) <ref type="bibr" target="#b6">[7]</ref> are one of the first notable attempts to integrate low capacity and computationally efficient strategies for resolving -instead of tolerating -misalignment with classical CNNs. Jaderberg et al. presented a novel strategy for integrating image warping within a neural network and showed that such operations are (sub-)differentiable, allowing for the application of canonical backpropagation to an image warping framework.</p><p>The problem of learning a low-capacity relationship between image appearance and geometric distortion is not new in computer vision. Over three and a half decades ago, Lucas &amp; Kanade (LK) <ref type="bibr" target="#b13">[14]</ref> proposed the seminal algorithm for gradient descent image alignment. The LK algorithm can be interpreted as a feed forward network of multiple alignment modules; specifically, each alignment module contains a low-capacity predictor (typically linear) for predicting geometric distortion from relative image appearance, followed by an image resampling/warp operation. The LK algorithm differs fundamentally, however, to STNs in their application: image/object alignment instead of classification.</p><p>Putting applications to one side, the LK and STN frameworks share quite similar characteristics however with a criticial exception. In an STN with multiple feed-forward alignment modules, the output image of the previous alignment module is directly fed into the next. As we will demonstate in this paper, this is problematic as it can create unwanted boundary effects as the number of geometric prediction layers increase. The LK algorithm does not suffer from such problems; instead, it feeds the warp parameters through the network (instead of the warped image) such that each subsequent alignment module in the network resamples the original input source image. Furthermore, the Inverse Compositional (IC) variant of the LK algorithm <ref type="bibr" target="#b1">[2]</ref> has demonstrated to achieve equivalently ef-fective alignment by reusing the same geometric predictor in a compositional update form.</p><p>Inspired by the IC-LK algorithm, we advocate an improved extension to the STN framework that (a) propagates warp parameters, rather than image intensities, through the network, and (b) employs the same geometric predictor that could be reapplied for all alignment modules. We propose Inverse Compositional Spatial Transformer Networks (ICSTNs) and show its superior performance over the original STNs across a myriad of tasks, including pure image alignment and joint alignment/classification problems.</p><p>We organize the paper as follows: we give a general review of efficient image/object alignment in Sec. 2 and an overview of Spatial Transformer Networks in Sec. 3. We describe our proposed IC-STNs in detail in Sec. 4 and show experimental results for different applications in Sec. 5. Finally, we draw to our conclusion in Sec. 6.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Efficient Image &amp; Object Alignment</head><p>In this section, we give a review of nominal approaches to efficient and low-capacity image/object alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">The Lucas &amp; Kanade Algorithm</head><p>The Lucas &amp; Kanade (LK) algorithm <ref type="bibr" target="#b13">[14]</ref> has been a popular approach for tackling dense alignment problems for images and objects. For a given geometric warp function parameterized by the warp parameters p, one can express the LK algorithm as minimizing the sum of squared differences (SSD) objective in the image space,</p><formula xml:id="formula_0">min ∆p I(p + ∆p) − T (0) 2 2 ,<label>(1)</label></formula><p>where I is the source image, T is the template image to align against, and ∆p is the warp update being estimated. Here, we denote I(p) as the image I warped with the parameters p. The LK algorithm assumes a approximate linear relationship between appearance and geometric displacements; specifically, it linearizes (1) by taking the firstorder Taylor approximation as</p><formula xml:id="formula_1">min ∆p I(p) + ∂I(p) ∂p ∆p − T (0) 2 2 .<label>(2)</label></formula><p>The warp parameters are thus additively updated through p ← p + ∆p, which can be regarded as a quasi-Newton update. The term</p><formula xml:id="formula_2">∂I(p)</formula><p>∂p , known as the steepest descent image, is the composition of image gradients and the predefined warp Jacobian, where the image gradients are typically estimated through finite differences. As the true relationship between appearance and geometry is seldom linear, the warp update ∆p must be iteratively estimated and applied until convergence is reached.</p><p>A fundamental problem with the canonical LK formulation, which employs addtive updates of the warp parameters, is that ∂I(p) ∂p must be recomputed on the rewarped images for each iteration, greatly impacting computational efficiency. Baker and Matthews <ref type="bibr" target="#b1">[2]</ref> devised a computationally efficient variant of the LK algorithm, which they referred to as the Inverse Compositional (IC) algorithm. The IC-LK algorithm reformulates (1) to predict the warp update to the template image instead, written as</p><formula xml:id="formula_3">min ∆p I(p) − T (∆p) 2 2 ,<label>(3)</label></formula><p>and the linearized least-squares objective is thus formed as</p><formula xml:id="formula_4">min ∆p I(p) − T (0) − ∂T (0) ∂p ∆p 2 2 .<label>(4)</label></formula><p>The least-squares solution is given by</p><formula xml:id="formula_5">∆p = ∂T (0) ∂p † (I(p) − T (0)) ,<label>(5)</label></formula><p>where the superscript † denotes the Moore-Penrose pseudoinverse operator. This is followed by the inverse compositional update p ← p • (∆p) −1 , where we abbreviate the notation • to be the composition of warp functions parameterized by p, and (∆p) −1 is the parameters of the inverse warp function parameterized by ∆p.</p><p>The solutions of (2) and (4) are in the form of linear regression, which can be more generically expressed as</p><formula xml:id="formula_6">∆p = R · I(p) + b,<label>(6)</label></formula><p>where R is a linear regressor establishing the linear relationship between appearance and geometry, and b is the bias term. Therefore, LK and IC-LK can be interpreted as belonging to the category of cascaded linear regression approaches for image alignment. It has been shown <ref type="bibr" target="#b1">[2]</ref> that the IC form of LK is effectively equivalent to the original form; the advantage of the IC form lies in its efficiency of computing the fixed steepest descent image</p><formula xml:id="formula_7">∂T (0)</formula><p>∂p in the least-squares objective. Specifically, it is evaluated on the static template image T at the identity warp p = 0 and remains constant across iterations, and thus so is the resulting linear regressor R. This gives an important theoretical proof of concept that a fixed predictor of geometric updates can be successfully employed within an iterative image/object alignment strategy, further reducing unnecessary model capacities.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Learning Alignment from Data</head><p>More generally, cascaded regression approaches for alignment can be learned from data given that the distribution of warp displacements is known a priori. A notable example of this kind of approach is the Supervised Descent Method (SDM) <ref type="bibr" target="#b18">[19]</ref>, which aims to learn the series of linear geometric predictors {R, b} from data. The formulation of SDM's learning objective is</p><formula xml:id="formula_8">min R,b N n=1 M j=1 δp n,j − R · I n (p n • δp n,j ) − b 2 2 , (7)</formula><p>where δp is the geometric displacement drawn from a known generating distribution using Monte Carlo sampling, and M is the number of synthetically created examples for each image. Here, the image appearance I is often replaced with a predefined feature extraction function (e.g. SIFT <ref type="bibr" target="#b12">[13]</ref> or HOG <ref type="bibr" target="#b2">[3]</ref>) of the image. This least-squares objective is typically solved with added regularization (e.g. ridge regression) to ensure good matrix condition.</p><p>SDM is learned in a sequential manner, i.e. the training data for learning the next linear model is drawn from the same generating distribution and applied through the previously learned regressors. This has been a popular approach for its simplicity and effectiveness across various alignment tasks, leading to a large number of variants <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b10">11]</ref> of similar frameworks. Like the LK and IC-LK algorithms, SDM is another example of employing multiple low-capacity models to establish the nonlinear relationship between appearance and geometry. We draw the readers' attention to <ref type="bibr" target="#b10">[11]</ref> for a more formally established link between LK and SDM.</p><p>It is a widely agreed that computer vision problems can be solved much more efficiently if misalignment among data is eliminated. Although SDM learns alignment from data and guarantees optimal solutions after each applied linear model, it is not clear whether such alignment learned in a greedy fashion is optimal for the subsequent tasks at hand, e.g. classification. In order to optimize in terms of the final objective, it would be more favorable to paramterize the model as a deep neural network and optimize the entire model using backpropagation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Spatial Transformer Networks</head><p>In the rapidly emerging field of deep learning among with the explosion of available collected data, deep neural networks have enjoyed huge success in various vision problems. Nevertheless, there had not been a principled way of resolving geometric variations in the given data. The recently proposed Spatial Transformer Networks <ref type="bibr" target="#b6">[7]</ref> performs spatial transformations on images or feature maps with a (sub-)differentiable module. It has the effects of reducing geometric variations inside the data and has brought great attention to the deep learning community.</p><p>In the feed-forward sense, a Spatial Transformer warps an image conditioned on the input. This can be mathemati- <ref type="figure">Figure 1</ref>: Network module of Spatial Transformers <ref type="bibr" target="#b6">[7]</ref>. The blue arrows indicate information passing of appearance, and the purple one indicate that of geometry. The yellow 3D trapezoid denotes the geometric predictor, which contains the learnable parameters.</p><p>cally written as</p><formula xml:id="formula_9">I out (0) = I in (p), where p = f (I in (0)).<label>(8)</label></formula><p>Here, the nonlinear function f is parametrized as a learnable geometric predictor (termed the localization network in the original paper), which predicts the warp parameters from the input image. We note that the "grid generator" and the "sampler" from the original paper can be combined to be a single warp function. We can see that for the special case where the geometric predictor consists of a single linear layer, f would consists of a linear regressor R as well as a bias term b, resulting the geometric predictor in an equivalent form of (6). This insight elegantly links the STN and LK/SDM frameworks together. <ref type="figure">Fig. 1</ref> shows the basic architecture of STNs. STNs are of great interest in that transformation predictions can be learned while also showing that grid sampling functions can be (sub-)differentiable, allowing for backpropagation within an end-to-end learning framework.</p><p>Despite the similarities STNs have with classic alignment algorithms, there exist some fundamental drawbacks in comparison to LK/SDM. For one, it attempts to directly predict the optimal geometric transformation with a single geometric predictor and does not take advantage of the employment of multiple lower-capacity models to achieve more efficient alignment before classification. Although it has been demonstrated that multiple Spatial Transformers can be inserted between feature maps, the effectiveness of such employment has on improving performance is not well-understood. In addition, we can observe from <ref type="bibr" target="#b7">(8)</ref> that no information of the geometric warp p is preserved after the output image; this leads to a boundary effect when resampling outside the input source image. A detailed treatment on this part is provided in Sec. 4.1.</p><p>In this work, we aim to improve upon STNs by theoretically connecting it to the LK algorithm. We show that employing multiple low-capacity models as in LK/SDM for learning spatial transformation within a deep network yields </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Inverse Compositional STNs</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Geometry Preservation</head><p>One of the major drawbacks of the original Spatial Transformer architecture <ref type="figure">(Fig. 1)</ref> is that the output image samples only from the cropped input image; pixel information outside the cropped region is discarded, introducing a boundary effect. <ref type="figure" target="#fig_0">Fig. 2</ref> illustrates the phenomenon.</p><p>We can see from <ref type="figure" target="#fig_0">Fig. 2(d)</ref> that such effect is visible for STNs in zoom-out transformations where pixel information outside the bounding box is required. This is due to the fact that geometric information is not preserved after the spatial transformations. In the scenario of iterative alignment, boundary effects are accumulated for each zoom-out transformations. Although this is less of an issue with images with clean background, this is problematic with real images.</p><p>A series of spatial transformations, however, can be composed and described with exact expressions. <ref type="figure" target="#fig_1">Fig. 3</ref> illustrates an improved alignment module, which we refer to as compositional STNs (c-STNs). Here, the geometric transformation is also predicted from a geometric predictor, but the warp parameters p are kept track of, composed, and passed through the network instead of the warped images. It is important to note that if one were to incorporate a cascade of multiple Spatial Transformers, the geometric transforma- tions are implicitly composed through multiple resampling of the images. We advocate that these transformations are able to be and should be explicitly defined and composed. Unlike the Spatial Transformer module in <ref type="figure">Fig. 1</ref>, the geometry is preserved in p instead of being absorbed into the output image. Furthermore, c-STNs allows repeated concatenation, illustrated in <ref type="figure" target="#fig_2">Fig. 4</ref>, where updates to the warp can be iteratively predicted. This eliminates the boundary effect because pixel information outside the cropped image is also preserved until the final transformation.</p><p>The derivative of warp compositions can also be mathematically expressed in closed forms. Consider the input and output warp parameters p in and p out in <ref type="figure" target="#fig_1">Fig. 3</ref>. Taking the case of affine warps for example, the parameters</p><formula xml:id="formula_10">p = [p 1 p 2 p 3 p 4 p 5 p 6 ]</formula><p>⊤ are relatable to transformation matrices in the homogeneous coordinates as</p><formula xml:id="formula_11">M(p) =   1 + p 1 p 2 p 3 p 4 1 + p 5 p 6 0 0 1   .<label>(9)</label></formula><p>From the definition of warp composition, the warp parameters are related to the transformation matrices through</p><formula xml:id="formula_12">M(p out ) = M(∆p) · M(p in ).<label>(10)</label></formula><p>We can thus derive the derivative to be where I is the identity matrix. This allows the gradients to backpropagate into the geometric predictor. It is interesting to note that the expression of ∂pout ∂pin in (11) has a very similar expression as in Residual Networks <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b4">5]</ref>, where the gradients contains the identity matrix I and "residual components". This suggests that the warp parameters from c-STNs are generally insensitive to the vanishing gradient phenomenon given the predicted warp parameters ∆p is small, and that it is possible to repeat the warp/composition operation by a large number of times.</p><formula xml:id="formula_13">∂p out ∂p in = I +         ∆p 1 0 0 ∆p 2 0 0 0 ∆p 1 0 0 ∆p 2 0 0 0 ∆p 1 0 0 ∆p 2 ∆p 4 0 0 ∆p 5 0 0 0 ∆p 4 0 0 ∆p 5 0 0 0 ∆p 4 0 0 ∆p 5         ∂p out ∂∆p = I +         p in,1 p in,4 0 0 0 0 p in,2 p in,5 0 0 0 0 p in,3 p in,6 0 0 0 0 0 0 0 p in,1 p in,4 0 0 0 0 p in,2 p in,5 0 0 0 0 p in,3 p in,6 0         ,<label>(11)</label></formula><p>We also note that c-STNs are highly analogous to classic alignment algorithms. If each geometric predictor consists of a single linear layer, i.e. the appearance-geometry relationship is assumed to be linearly approximated, then it performs equivalent operations as the compositional LK algorithm. It is also related to SDM, where heuristic features such as SIFT are extracted before each regression layer. Therefore, c-STNs can be regarded as a generalization of LK and SDM, differing that the features for predicting the warp updates can be learned from data and incorporated into an end-to-end learning framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Recurrent Spatial Transformations</head><p>Of all variants of the LK algorithm, the IC form <ref type="bibr" target="#b1">[2]</ref> has a very special property in that the linear regressor remains constant across iterations. The steepest descent image <ref type="formula" target="#formula_5">(5)</ref> is independent of the input image and the current estimate of p; therefore, it is only needed to be computed once. In terms of model capacity, IC-LK further reduces the necessary learnable parameters compared to canonical LK, for the same regressor can be applied repeatedly and converges provided a good initialization. The main difference from canonical LK and IC-LK lies in that the warp update ∆p should be compositionally applied in the inverse form. We redirect the readers to <ref type="bibr" target="#b1">[2]</ref> for a full treatment of IC-LK, which is out of scope of this paper.</p><formula xml:id="formula_14">∂T (0) ∂p in</formula><p>This inspires us to propose the Inverse Compositional Spatial Transformer Network (IC-STN). <ref type="figure" target="#fig_3">Fig. 5</ref> illustrates the recurrent module of IC-STN: the warp parameters p is iteratively updated by ∆p, which is predicted from the current warped image with the same geometric predictors. This allows one to recurrently predict spatial transformations on the input image. It is possible due to the close spatial proximity of pixel intensities within natural images: there exists high correlation between pixels in close distances.</p><p>In the IC-LK algorithm, the predicted warp parameters are inversely composed. Since the IC-STN geometric predictor is optimized in an end-to-end learning framework, we can absorb the inversion operation into the geometric predictor without explicitly defining it; in other words, ICSTNs are able to directly predict the inverse parameters. In our experiments, we find that there is negligible difference to explicitly perform an additional inverse operation on the predicted forward parameters, and that implicitly predicting the inverse parameters fits more elegantly in an end-to-end learning framework using backpropagation. We name our proposed method Inverse Compositional nevertheless as IC-LK is where our inspirations are drawn from.</p><p>In practice, IC-STNs can be trained by unfolding the architecture in <ref type="figure" target="#fig_3">Fig. 5</ref> multiple times into the form of c-STNs <ref type="figure" target="#fig_2">(Fig. 4)</ref>, sharing the learnable parameters across all geometric predictors, and backpropagating the gradients as described in Sec. 4.1. This results in a single effective geometric predictor that can be applied multiple times before performing the final warp operation that suits subsequent tasks such as classification.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Planar Image Alignment</head><p>To start with, we explore the efficacy of IC-STN for planar alignment of a single image. We took an example image from the Caffe library <ref type="bibr" target="#b7">[8]</ref> and generated perturbed images with affine warps around the hand-labeled ground truth, shown in <ref type="figure" target="#fig_4">Fig. 6</ref>. We used image samples of size 50× 50 pixels. The perturbed boxes are generated by adding i.i.d. Gaussian noise of standard deviation σ (in pixels) to the four corners of the ground-truth box plus an additional translational noise from the same Gaussian distribution, and finally fitting the box to the initial warp parameters p.</p><p>To demonstrate the effectiveness of iterative alignment under different amount of noise, we consider IC-STNs that consist of a single learnable linear layer with different numbers of learned recurrent transformations. We optimize all networks in terms of L 2 error between warp parameters with stochastic gradient descent and a batch size of 100 perturbed training samples generated on the fly.</p><p>The test error is illustrated in <ref type="table">Table 1</ref>. We see from c-STN-1 (which is equivalent to IC-STN-1 with only one warp operation unfolded) that a single geometric warp pre- dictor has limited ability to directly predict the optimal geometric transformation. Reusing the geometric predictor to incorporating multiple spatial transformations yields better alignment performance given the same model capacity. <ref type="figure" target="#fig_5">Fig. 7</ref> shows the test error over the number of warp operations applied to the learned alignment module. We can see that even when the recurrent spatial transformation is applied more times than trained with, the error continues to decrease until some of point of saturation, which typically does not hold true for classical recurrent neural networks. This implies that IC-STN is able to capture the correlation between appearance and geometry to perform gradient descent on a learned cost surface for successful alignment.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">MNIST Classification</head><p>In this section, we demonstrate how IC-STNs can be utilized in joint alignment/classfication tasks. We choose the MNIST handwritten digit dataset <ref type="bibr" target="#b9">[10]</ref>, and we use a homography warp noise model to perturb the four corners of the image and translate them with Gaussian noise, both with a standard deviation of 3.5 pixels. We train all networks for 200K iterations with a batch size of 100 perturbed samples generated on the fly. We choose a constant learning rate of 10 −2 for the classification networks and 10 −4 for the geometric predictors as we find the geometric predictor sensitive to large changes. We evaluate the classification accuracy on the test set using the same warp noise model.</p><p>We compare IC-STN to several network architectures, including a baseline CNN with no spatial transformations, the original STN from Jaderberg et al., and c-STNs. All networks with spatial transformations employ the same classification network. The results as well as the architectural details are listed in <ref type="table">Table 2</ref>. We can see that classical CNNs do not handle large spatial variations efficiently with data augmentation. In the case where the digits may be occluded,  however, trading off capacity for a single deep predictor of geometric transformation also results in poor performance. Incorporating multiple transformers lead to a significant improvement in classification accuracy; further comparing c-STN-4(a) and IC-STN-4(b), we see that IC-STNs are able to trade little accuracy off for a large reduction of capacity compared to its non-recurrent counterpart. <ref type="figure" target="#fig_7">Fig. 8</ref> shows how IC-STNs learns alignment for classification. In many cases where the handwritten digits are occluded, IC-STN is able to automatically warp the image and reveal the occluded information from the original image. There also exists smooth transitions during the alignment, which confirms with the recurrent spatial transformation concept IC-STN learns. Furthermore, we see that the outcome of the original STN becomes cropped digits due to the boundary effect described in Sec. 4.1.</p><p>We also visualize the overall final alignment performance by taking the mean and variance on the test set appearance before classification, shown in <ref type="figure" target="#fig_8">Fig. 9</ref>. The mean/variance results of the original STN becomes a downscaled version of the original digits, reducing information necessary for better classification. From c-STN-1, we see that a single geometric predictor is poor in directly predicting geometric transformations. The variance among all aligned samples is dramatically decreased when more warp operations are introduced in IC-STN. These results support the fact that elimination of spatial variations within data is crucial to boosting the performance of subsequent tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Traffic Sign Classification</head><p>Here, we show how IC-STNs can be applied to realworld classification problems such as traffic sign recognition. We evaluate our proposed method with the German Traffic Sign Recognition Benchmark <ref type="bibr" target="#b17">[18]</ref>, which consists of 39,209 training and 12,630 test images from 43 classes taken under various conditions. We consider this as a challenging task since many of the images are taken with motion blurs and/or of resolution as low as 15×15 pixels. We    rescale all images and generate perturbed samples of size 36×36 pixels with the same homography warp noise model described in Sec. 5.2. The learning rate is 10 −3 for the classification networks and 10 −5 for the geometric predictors. We set the controlled model capacities to around 200K learnable parameters and perform similar comparisons to the MNIST experiment. <ref type="table" target="#tab_2">Table 3</ref> shows the classification error on the perturbed GTSRB test set. Once again, we see a considerable amount of classification improvement of IC-STN from learning to reuse the same geometric predictor.  We also visualize the aligned mean appearances from each network in <ref type="figure" target="#fig_10">Fig. 11</ref>, and it can be observed that the mean appearance of IC-STN becomes sharper as the number of warp operations increase, once again indicating that good alignment is crucial to the subsequent target tasks. It is also interesting to note that not all traffic signs are aligned to be fit exactly inside the bounding boxes, e.g. the networks finds the optimal alignment for stop signs to be zoomed-in images while excluding the background information outside the octagonal shapes. This suggests that in certain cases, only the pixel information inside the sign shapes are necessary to achieve good alignment for classification.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion</head><p>In this paper, we theoretically connect the core idea of the Lucas &amp; Kanade algorithm with Spatial Transformer Networks. We show that geometric variations within data can be eliminated more efficiently through multiple spatial transformations within an alignment framework. We propose Inverse Compositional Spatial Transformer Networks for predicting recurrent spatial transformations and demonstrate superior alignment and classification results compared to baseline CNNs and the original STN.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Boundary effect of Spatial Transformers on real images. (a) Original image, where the green box indicates the cropped region. (b) Cropped image as the input of the Spatial Transformer. (c) Zoom-in transformation: sampling occurs within the range of the input image. (d)(e) Zoom-out transformation: discarding the information outside the input image introduces a boundary effect (STNs), while it is not the case with geometry preservation (c-STNs). The white dotted box indicates the warp from the original image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: A learnable warping module with geometry preserved, termed as c-STNs. The warp parameters are passed through the network instead of the warped images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Multiple concatenation of c-STNs for an iterative alignment framework.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Illustration of the proposed Inverse Compositional Spatial Transformer Network (IC-STN). The same geometric predictor is learned to predict recurrent spatial transformations that are composed together to warp the input image.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 :</head><label>6</label><figDesc>Figure 6: Visualization of the image and perturbed training samples for the planar image alignment experiment. (a) Original image, where the red box indicates the groundtruth warp and the yellow boxes indicate example generated warps. (b) Examples of the perturbed images (affine warps with σ = 7.5 in this case). Model σ = 2.5 σ = 5 σ = 7.5 σ = 10 c-STN-1 2.699 5.576 9.491 9.218 IC-STN-2 0.615 2.268 5.283 5.502 IC-STN-3 0.434 1.092 2.877 3.020 IC-STN-4 0.292 0.481 1.476 2.287 IC-STN-6 0.027 0.125 0.245 1.305</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Evaluation on trained IC-STNs, where the dot on each curve corresponds to the number of recurrent transformations unfolded during training.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>:</head><label></label><figDesc>×4 → conv(9×9, 3)-FC(10) IC-STN-2(a) 1.905 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×2 → conv(9×9, 3)-FC(10) IC-STN-4(a) 1.230 % 39048 [ conv(7×7, 4)-conv(7×7, 8)-P-FC(48)-FC(8) ]×4 → conv(9×9, 3)-FC(Classification error on the perturbed MNIST test set. The non-recurrent networks have similar numbers of layers and learnable parameters but different numbers of warp operations (bold-faced). The filter dimensions are shown in parentheses, where those of the geometric predictor(s) are in green and those of the subsequent classification network are in blue (P denotes a 2×2 max-pooling operation). Best viewed in color.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Sample alignment results of IC-STN-4(a) on the MNIST test set with homography warp perturbations. The first row of each column shows the initial perturbation; the middle three rows illustrates the alignment process (iterations 1 to 3); the second last row shows the final alignment before feeding into the classification network. The last row shows the alignment from the original STN: the cropped digits are the results of the boundary effect.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 9 :</head><label>9</label><figDesc>Figure 9: Mean/variance of the aligned appearances from the 10 classes of the test set (homography perturbations).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_9"><head>Figure 10 :</head><label>10</label><figDesc>Figure 10: Sample alignment results of IC-STN-4 on the GTSRB test set in comparison to the original STN.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_10"><head>Figure 11 :</head><label>11</label><figDesc>Figure 11: Mean aligned appearances for classification from sampled classes of the GTSRB test set.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_11"><head>Fig</head><label></label><figDesc>Fig. 10 compares the aligned images from IC-STN and the original STN before the classification networks. Again, IC-STNs are able to recover occluded appearances from the input image. Although STN still attempts to center the perturbed images, the missing information from occlusion degrades its subsequent classification performance. We also visualize the aligned mean appearances from each network in Fig. 11, and it can be observed that the mean appearance of IC-STN becomes sharper as the number of warp operations increase, once again indicating that good alignment is crucial to the subsequent target tasks. It is also interesting to note that not all traffic signs are aligned to be fit exactly inside the bounding boxes, e.g. the networks finds the optimal alignment for stop signs to be zoomed-in images while excluding the background information outside the octagonal shapes. This suggests that in certain cases, only the pixel information inside the sign shapes are necessary to achieve good alignment for classification.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 3 :</head><label>3</label><figDesc>Classification error on the perturbed GTSRB test set. The architectural descriptions follow that in Table 2.</figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Incremental face alignment in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Asthana</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zafeiriou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pantic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1859" to="1866" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lucas-kanade 20 years on: A unifying framework</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Baker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Matthews</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">56</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="221" to="255" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Histograms of oriented gradients for human detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dalal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Triggs</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2005 IEEE Computer Society Conference on Computer Vision and Pattern Recognition (CVPR&apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="886" to="893" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1603.05027</idno>
		<title level="m">Identity mappings in deep residual networks</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learned-miller. Learning to align from scratch</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mattar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>F. Pereira, C. J. C. Burges, L. Bottou, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2012" />
			<biblScope unit="volume">25</biblScope>
			<biblScope unit="page" from="764" to="772" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Caffe: Convolutional architecture for fast feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Donahue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Karayev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Guadarrama</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 22nd ACM international conference on Multimedia</title>
		<meeting>the 22nd ACM international conference on Multimedia</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="675" to="678" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1097" to="1105" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">The mnist database of handwritten digits</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Cortes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Burges</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">The conditional lucas &amp; kanade algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-H</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lucey</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision (ECCV)</title>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="793" to="808" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Do convnets learn correspondence?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<editor>Z. Ghahramani, M. Welling, C. Cortes, N. D. Lawrence, and K. Q. Weinberger</editor>
		<imprint>
			<publisher>Curran Associates, Inc</publisher>
			<date type="published" when="2014" />
			<biblScope unit="volume">27</biblScope>
			<biblScope unit="page" from="1601" to="1609" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Distinctive image features from scaleinvariant keypoints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G</forename><surname>Lowe</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International journal of computer vision</title>
		<imprint>
			<biblScope unit="volume">60</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="91" to="110" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">An iterative image registration technique with an application to stereo vision</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">D</forename><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kanade</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IJCAI</title>
		<imprint>
			<date type="published" when="1981" />
			<biblScope unit="volume">81</biblScope>
			<biblScope unit="page" from="674" to="679" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Face alignment at 3000 fps via regressing local binary features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Cao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1685" to="1692" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Best practices for convolutional neural networks applied to visual document analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">Y</forename><surname>Simard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Steinkraus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">C</forename><surname>Platt</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR</title>
		<imprint>
			<date type="published" when="2003" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="958" to="962" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">The german traffic sign recognition benchmark: a multi-class classification competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Stallkamp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schlipsing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Salmen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Igel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The 2011 International Joint Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="1453" to="1460" />
		</imprint>
	</monogr>
	<note>Neural Networks (IJCNN)</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Supervised descent method and its applications to face alignment</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>De La</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Torre</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="532" to="539" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
