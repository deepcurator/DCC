<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Structured Embedding Models for Grouped Data</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maja</forename><surname>Rudolph</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia Univ</orgName>
								<orgName type="institution" key="instit2">Univ. of Cambridge Columbia Univ</orgName>
								<orgName type="institution" key="instit3">Stanford Univ</orgName>
								<orgName type="institution" key="instit4">Columbia Univ</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Francisco</forename><surname>Ruiz</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia Univ</orgName>
								<orgName type="institution" key="instit2">Univ. of Cambridge Columbia Univ</orgName>
								<orgName type="institution" key="instit3">Stanford Univ</orgName>
								<orgName type="institution" key="instit4">Columbia Univ</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Susan</forename><surname>Athey</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia Univ</orgName>
								<orgName type="institution" key="instit2">Univ. of Cambridge Columbia Univ</orgName>
								<orgName type="institution" key="instit3">Stanford Univ</orgName>
								<orgName type="institution" key="instit4">Columbia Univ</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Blei</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution" key="instit1">Columbia Univ</orgName>
								<orgName type="institution" key="instit2">Univ. of Cambridge Columbia Univ</orgName>
								<orgName type="institution" key="instit3">Stanford Univ</orgName>
								<orgName type="institution" key="instit4">Columbia Univ</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Structured Embedding Models for Grouped Data</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Word embeddings are a powerful approach for analyzing language, and exponential family embeddings (EFE) extend them to other types of data. Here we develop structured exponential family embeddings (S-EFE), a method for discovering embeddings that vary across related groups of data. We study how the word usage of U.S. Congressional speeches varies across states and party affiliation, how words are used differently across sections of the ArXiv, and how the copurchase patterns of groceries can vary across seasons. Key to the success of our method is that the groups share statistical information. We develop two sharing strategies: hierarchical modeling and amortization. We demonstrate the benefits of this approach in empirical studies of speeches, abstracts, and shopping baskets. We show how S-EFE enables group-specific interpretation of word usage, and outperforms EFE in predicting held-out data.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Word embeddings <ref type="bibr" target="#b6">(Bengio et al., 2003;</ref><ref type="bibr">Mikolov et al., 2013d,c,a;</ref><ref type="bibr" target="#b28">Pennington et al., 2014;</ref><ref type="bibr" target="#b21">Levy and Goldberg, 2014;</ref><ref type="bibr" target="#b3">Arora et al., 2015)</ref> are unsupervised learning methods for capturing latent semantic structure in language. Word embedding methods analyze text data to learn distributed representations of the vocabulary that capture its co-occurrence statistics. These representations are useful for reasoning about word usage and meaning <ref type="bibr" target="#b13">(Harris, 1954;</ref><ref type="bibr" target="#b33">Rumelhart et al., 1986)</ref>. Word embeddings have also been extended to data beyond text <ref type="bibr" target="#b5">(Barkan and Koenigstein, 2016;</ref><ref type="bibr" target="#b32">Rudolph et al., 2016)</ref>, such as items in a grocery store or neurons in the brain. Exponential family embeddings (EFE) is a probabilistic perspective on embeddings that encompasses many existing methods and opens the door to bringing expressive probabilistic modeling <ref type="bibr" target="#b7">(Bishop, 2006;</ref><ref type="bibr" target="#b27">Murphy, 2012)</ref> to the problem of learning distributed representations.</p><p>We develop structured exponential family embeddings (S-EFE), an extension of EFE for studying how embeddings can vary across groups of related data. We will study several examples: in U.S. Congressional speeches, word usage can vary across states or party affiliations; in scientific literature, the usage patterns of technical terms can vary across fields; in supermarket shopping data, co-purchase patterns of items can vary across seasons of the year. We will see that S-EFE discovers a per-group embedding representation of objects. While the naïve approach of fitting an individual embedding model for each group would typically suffer from lack of data-especially in groups for which fewer observations are available-we develop two methods that can share information across groups. <ref type="figure" target="#fig_2">Figure 1a</ref> illustrates the kind of variation that we can capture. We fit an S-EFE to ArXiv abstracts grouped into different sections, such as computer science (cs), quantitative finance (q-fin), and nonlinear sciences (nlin). S-EFE results in a per-section embedding of each term in the vocabulary.</p><p>Using the fitted embeddings, we illustrate similar words to the word <ref type="bibr">INTELLIGENCE.</ref> We can see that how INTELLIGENCE is used varies by field: in computer science the most similar words include ARTIFICIAL and AI; in finance, similar words include ABILITIES and CONSCIOUSNESS.   v are specific to each group, and the context vectors α v are shared across all categories.</p><p>In more detail, embedding methods posit two representation vectors for each term in the vocabulary; an embedding vector and a context vector. (We use the language of text for concreteness; as we mentioned, EFE extend to other types of data.) The idea is that the conditional probability of each observed word depends on the interaction between the embedding vector and the context vectors of the surrounding words. In S-EFE, we posit a separate set of embedding vectors for each group but a shared set of context vectors; this ensures that the embedding vectors are in the same space.</p><p>We propose two methods to share statistical strength among the embedding vectors. The first approach is based on hierarchical modeling <ref type="bibr" target="#b9">(Gelman et al., 2003)</ref>, which assumes that the groupspecific embedding representations are tied through a global embedding. The second approach is based on amortization <ref type="bibr" target="#b8">(Dayan et al., 1995;</ref><ref type="bibr" target="#b10">Gershman and Goodman, 2014)</ref>, which considers that the individual embeddings are the output of a deterministic function of a global embedding representation. We use stochastic optimization to fit large data sets.</p><p>Our work relates closely to two threads of research in the embedding literature. One is embedding methods that study how language evolves over time <ref type="bibr" target="#b15">(Kim et al., 2014;</ref><ref type="bibr" target="#b20">Kulkarni et al., 2015;</ref><ref type="bibr" target="#b12">Hamilton et al., 2016;</ref><ref type="bibr" target="#b31">Rudolph and Blei, 2017;</ref><ref type="bibr" target="#b4">Bamler and Mandt, 2017;</ref><ref type="bibr" target="#b35">Yao et al., 2017)</ref>. Time can be thought of as a type of "group", though with evolutionary structure that we do not consider. The second thread is multilingual embeddings <ref type="bibr" target="#b18">(Klementiev et al., 2012;</ref><ref type="bibr" target="#b23">Mikolov et al., 2013b;</ref><ref type="bibr" target="#b1">Ammar et al., 2016;</ref><ref type="bibr" target="#b36">Zou et al., 2013)</ref>; our approach is different in that most words appear in all groups and we are interested in the variations of the embeddings across those groups.</p><p>Our contributions are thus as follows. We introduce the S-EFE model, extending EFE to grouped data. We present two techniques to share statistical strength among the embedding vectors, one based on hierarchical modeling and one based on amortization. We carry out a thorough experimental study on two text databases, ArXiv papers by section and U.S. Congressional speeches by home state and political party. Using Poisson embeddings, we study market basket data from a large grocery store, grouped by season. On all three data sets, S-EFE outperforms EFE in terms of held-out log-likelihood. Qualitatively, we demonstrate how S-EFE discovers which words are used most differently across U.S. states and political parties, and show how word usage changes in different ArXiv disciplines.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Model Description</head><p>In this section, we develop structured exponential family embeddings (S-EFE), a model that builds on exponential family embeddings (EFE) <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref> to capture semantic variations across groups of data. In embedding models, we represent each object (e.g., a word in text, or an item in shopping data) using two sets of vectors, an embedding vector and a context vector. In this paper, we are interested in how the embeddings vary across groups of data, and for each object we want to learn a separate embedding vector for each group. Having a separate embedding for each group allows us to study how the usage of a word like INTELLIGENCE varies across categories of the ArXiv, or which words are used most differently by U.S. Senators depending on which state they are from and whether they are Democrats or Republicans.</p><p>The S-EFE model extends EFE to grouped data, by having the embedding vectors be specific for each group, while sharing the context vectors across all groups. We review the EFE model in Section 2.1. We then formalize the idea of sharing the context vectors in Section 2.2, where we present two approaches to build a hierarchical structure over the group-specific embeddings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Background: Exponential Family Embeddings</head><p>In exponential family embeddings, we have a collection of objects, and our goal is to learn a vector representation of these objects based on their co-occurrence patterns.</p><p>Let us consider a dataset represented as a (typically sparse) matrix X, where columns are datapoints and rows are objects. For example, in text, each column corresponds to a location in the text, and each entry x vi is a binary variable that indicates whether word v appears at location i.</p><p>In EFE, we represent each object v with two sets of vectors, embeddings vectors ρ v [i] and context vectors α v [i], and we posit a probability distribution of data entries x vi in which these vectors interact. The definition of the EFE model requires three ingredients: a context, a conditional exponential family, and a parameter sharing structure. We next describe these three components.</p><p>Exponential family embeddings learn the vector representation of objects based on the conditional probability of each observation, conditioned on the observations in its context. The context c vi gives the indices of the observations that appear in the conditional probability distribution of x vi . The definition of the context varies across applications. In text, it corresponds to the set of words in a fixed-size window centered at location i.</p><p>Given the context c vi and the corresponding observations x cvi indexed by c vi , the distribution for x vi is in the exponential family,</p><formula xml:id="formula_0">x vi | x cvi ∼ ExpFam (t(x vi ), η v (x cvi )) ,<label>(1)</label></formula><p>with sufficient statistics t(x vi ) and natural parameter η v (x cvi ). The parameter vectors interact in the conditional probability distributions of each observation x vi as follows. The embedding vectors ρ v [i] and the context vectors α v [i] are combined to form the natural parameter,</p><formula xml:id="formula_1">η v (x cvi ) = g   ρ v [i] (v ,i )∈cvi α v [i ]x v i   ,<label>(2)</label></formula><p>where g(·) is the link function. Exponential family embeddings can be understood as a bank of generalized linear models (GLMs). The context vectors are combined to give the covariates, and the "regression coefficients" are the embedding vectors. In Eq. 2, the link function g(·) plays the same role as in GLMs and is a modeling choice. We use the identity link function.</p><p>The third ingredient of the EFE model is the parameter sharing structure, which indicates how the embedding vectors are shared across observations. In the standard EFE model, we use</p><formula xml:id="formula_2">ρ v [i] ≡ ρ v and α v [i] ≡ α v for all columns of X.</formula><p>That is, each unique object v has a shared representation across all instances.</p><p>The objective function. In EFE, we maximize the objective function, which is given by the sum of the log-conditional likelihoods in Eq. 1. In addition, we add an 2 -regularization term (we use the notation of the log Gaussian pdf) over the embedding and context vectors, yielding</p><formula xml:id="formula_3">L = log p(α) + log p(ρ) + v,i log p x vi x cvi ; α, ρ ,<label>(3)</label></formula><p>Note that maximizing the regularized conditional likelihood is not equivalent to maximum a posteriori. Rather, it is similar to maximization of the pseudo-likelihood in conditionally specified models <ref type="bibr" target="#b2">(Arnold et al., 2001;</ref><ref type="bibr" target="#b32">Rudolph et al., 2016)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2">Structured Exponential Family Embeddings</head><p>Here, we describe the S-EFE model for grouped data. In text, some examples of grouped data are Congressional speeches grouped into political parties or scientific documents grouped by discipline. Our goal is to learn group-specific embeddings from data partitioned into S groups, i.e., each instance i is associated with a group s i ∈ {1, . . . , S}. The S-EFE model extends EFE to learn a separate set of embedding vectors for each group.</p><p>To build the S-EFE model, we impose a particular parameter sharing structure over the set of embedding and context vectors. We posit a structured model in which the context vectors are shared across groups, i.e., α v [i] ≡ α v (as in the standard EFE model), but the embedding vectors are only shared at the group level, i.e., for an observation i belonging to group</p><formula xml:id="formula_4">s i , ρ v [i] ≡ ρ (si) v . Here, ρ (s) v</formula><p>denotes the embedding vector corresponding to group s. We show a graphical representation of the S-EFE in <ref type="figure" target="#fig_2">Figure 1b</ref>.</p><p>Sharing the context vectors α v has two advantages. First, the shared structure reduces the number of parameters, while the resulting S-EFE model is still flexible to capture how differently words are used across different groups, as ρ</p><formula xml:id="formula_5">(s)</formula><p>v is allowed to vary. 1 Second, it has the important effect of uniting all embedding parameters in the same space, as the group-specific vectors ρ (s)</p><p>v need to agree with the components of α v . While one could learn a separate embedding model for each group, as has been done for text grouped into time slices <ref type="bibr" target="#b15">(Kim et al., 2014;</ref><ref type="bibr" target="#b20">Kulkarni et al., 2015;</ref><ref type="bibr" target="#b12">Hamilton et al., 2016)</ref>, this approach would require ad-hoc postprocessing steps to align the embeddings. When there are S groups, the S-EFE model has S times as many embedding vectors than the standard embedding model. This may complicate inferences about the group-specific vectors, especially for groups with less data. Additionally, an object v may appear with very low frequency in a particular group. Thus, the naïve approach for building the S-EFE model without additional structure may be detrimental for the quality of the embeddings, especially for small-sized groups. To address this problem, we propose two different methods to tie the individual ρ (s) v together, sharing statistical strength among them. The first approach consists in a hierarchical embedding structure. The second approach is based on amortization. In both methods, we introduce a set of global embedding vectors ρ (0) v , and impose a particular structure to generate ρ</p><formula xml:id="formula_6">(s) v from ρ (0) v .</formula><p>Hierarchical embedding structure. Here, we impose a hierarchical structure that allows sharing statistical strength among the per-group variables. For that, we assume that each ρ</p><formula xml:id="formula_7">(s) v ∼ N (ρ (0) v , σ 2 ρ I),</formula><p>where σ 2 ρ is a fixed hyperparameter. Thus, we replace the EFE objective function in Eq. 3 with</p><formula xml:id="formula_8">L hier = log p(α) + log p(ρ (0) ) + s log p(ρ (s) | ρ (0) ) + v,i log p x vi x cvi ; α, ρ .<label>(4)</label></formula><p>where the 2 -regularization term now applies only on α v and the global vectors ρ</p><formula xml:id="formula_9">(0) v .</formula><p>Fitting the hierarchical model involves maximizing Eq. 4 with respect to α v , ρ</p><p>v , and ρ (s)</p><p>v . We note that we have not reduced the number of parameters to be inferred; rather, we tie them together through a common prior distribution. We use stochastic gradient ascent to maximize Eq. 4.</p><p>Amortization. The idea of amortization has been applied in the literature to develop amortized inference algorithms <ref type="bibr" target="#b8">(Dayan et al., 1995;</ref><ref type="bibr" target="#b10">Gershman and Goodman, 2014)</ref>. The main insight behind amortization is to reuse inferences about past experiences when presented with a new task, leveraging the accumulated knowledge to quickly solve the new problem. Here, we use amortization to control the number of parameters of the S-EFE model. In particular, we set the per-group embeddings ρ (s) v to be the output of a deterministic function of the global embedding vectors, ρ</p><formula xml:id="formula_11">(s) v = f s (ρ (0) v ).</formula><p>We use a different function f s (·) for each group s, and we parameterize them using neural networks, similarly to other works on amortized inference <ref type="bibr" target="#b19">(Korattikara et al., 2015;</ref><ref type="bibr" target="#b17">Kingma and Welling, 2014;</ref><ref type="bibr" target="#b29">Rezende et al., 2014;</ref><ref type="bibr" target="#b26">Mnih and Gregor, 2014)</ref>. Unlike standard uses of amortized inference, in S-EFE the input to the functions f s (·) is unobserved and must be estimated together with the parameters of the functions f s (·).</p><p>Depending on the architecture of the neural networks, the amortization can significantly reduce the number of parameters in the model (as compared to the non-amortized model), while still having the flexibility to model different embedding vectors for each group. The number of parameters in the S-EFE model is KL(S + 1), where S is the number of groups, K is the dimensionality of the embedding vectors, and L is the number of objects (e.g., the vocabulary size). With amortization, we reduce the number of parameters to 2KL + SP , where P is the number of parameters of the neural network. Since typically L P , this corresponds to a significant reduction in the number of parameters, even when P scales linearly with K.</p><p>In the amortized S-EFE model, we need to introduce a new set of parameters φ (s) ∈ R P for each group s, corresponding to the neural network parameters. Given these, the group-specific embedding vectors ρ</p><formula xml:id="formula_12">(s) v are obtained as ρ (s) v = f s (ρ (0) v ) = f (ρ (0) v ; φ (s) ).<label>(5)</label></formula><p>We compare two architectures for the function f s (·): fully connected feed-forward neural networks and residual networks <ref type="bibr" target="#b14">(He et al., 2016)</ref>. For both, we consider one hidden layer with H units. Hence, the network parameters φ (s) are two weight matrices,</p><formula xml:id="formula_13">φ (s) = {W (s) 1 ∈ R H×K , W (s) 2 ∈ R K×H },<label>(6)</label></formula><p>i.e., P = 2KH parameters. The neural network takes as input the global embedding vector ρ</p><p>v , and it outputs the group-specific embedding vectors ρ v for a feed-forward neural network and a residual network is respectively given by</p><formula xml:id="formula_15">ρ (s) v = f ffnet (ρ (0) v ; φ (s) ) = W (s) 2 tanh W (s) 1 ρ (0) v ,<label>(7)</label></formula><formula xml:id="formula_16">ρ (s) v = f resnet (ρ (0) v ; φ (s) ) = ρ (0) v + W (s) 2 tanh W (s) 1 ρ (0) v ,<label>(8)</label></formula><p>where we have considered the hyperbolic tangent nonlinearity. The main difference between both network architectures is that the residual network focuses on modeling how the group-specific embedding vectors ρ The objective function under amortization is given by</p><formula xml:id="formula_17">L amortiz = log p(α) + log p(ρ (0) ) + v,i log p x vi x cvi ; α, ρ (0) , φ .<label>(9)</label></formula><p>We maximize this objective with respect to α v , ρ</p><p>v , and φ (s) using stochastic gradient ascent. We implement the hierarchical and amortized S-EFE models in TensorFlow <ref type="bibr" target="#b0">(Abadi et al., 2015)</ref>, which allows us to leverage automatic differentiation. Example: structured Bernoulli embeddings for grouped text data. Here, we consider a set of documents broken down into groups, such as political affiliations or scientific disciplines. We can represent the data as a binary matrix X and a set of group indicators s i . Since only one word can appear in a certain position i, the matrix X contains one non-zero element per column. In embedding models, we ignore this one-hot constraint for computational efficiency, and consider that the observations are generated following a set of conditional Bernoulli distributions <ref type="bibr" target="#b24">(Mikolov et al., 2013c;</ref><ref type="bibr" target="#b32">Rudolph et al., 2016)</ref>. Given that most of the entries in X are zero, embedding models typically downweigh the contribution of the zeros to the objective function. <ref type="bibr" target="#b24">Mikolov et al. (2013c)</ref> use negative sampling, which consists in randomly choosing a subset of the zero observations. This corresponds to a biased estimate of the gradient in a Bernoulli exponential family embedding model <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref>.</p><p>The context c vi is given at each position i by the set of surrounding words in the document, according to a fixed-size window.  <ref type="table">Table 1</ref>: Group structure and size of the three corpora analyzed in Section 3.</p><p>Example: structured Poisson embeddings for grouped shopping data. EFE and S-EFE extend to applications beyond text and we use S-EFE to model supermarket purchases broken down by month. For each market basket i, we have access to the month s i in which that shopping trip happened. Now, the rows of the data matrix X index items, while columns index shopping trips. Each element x vi denotes the number of units of item v purchased at trip i. Unlike text, each column of X may contain more than one non-zero element. The context c vi corresponds to the set of items purchased in trip i, excluding v.</p><p>In this case, we use the Poisson conditional distribution, which is more appropriate for count data. In Poisson S-EFE, we also downweigh the contribution of the zeros in the objective function, which provides better results because it allows the inference to focus on the positive signal of the actual purchases <ref type="bibr" target="#b32">(Rudolph et al., 2016;</ref><ref type="bibr" target="#b24">Mikolov et al., 2013c)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Empirical Study</head><p>In this section, we describe the experimental study. We fit the S-EFE model on three datasets and compare it against the EFE <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref>. Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements.</p><p>Data. We apply the S-EFE on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in <ref type="table">Table 1</ref>.</p><p>ArXiv papers: This dataset contains the abstracts of papers published on the ArXiv under the 19 different tags between April 2007 and June 2015. We treat each tag as a group and fit S-EFE with the goal of uncovering which words have the strongest shift in usage. We split the abstracts into training, validation, and test sets, with proportions of 80%, 10%, and 10%, respectively.</p><p>Senate speeches: This dataset contains U.S. Senate speeches from 1994 to mid 2009. In contrast to the ArXiv collection, it is a transcript of spoken language. We group the data into state of origin of the speaker and his or her party affiliation. Only affiliations with the Republican and Democratic Party are considered. As a result, there are 83 groups (Republicans from Alabama, Democrats from Alabama, Republicans from Arkansas, etc.). Some of the state/party combinations are not available in the data, as some of the 50 states have only had Senators with the same party affiliation. We split the speeches into training (80%), validation (10%), and testing (10%).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Grocery shopping data:</head><p>This dataset contains the purchases of 3, 206 customers. The data covers a period of 97 weeks. After removing low-frequency items, the data contains 5, 590 unique items at the UPC (Universal Product Code) level. We split the data into a training, test, and validation sets, with proportions of 90%, 5%, and 5%, respectively. The training data contains 515, 867 shopping trips and 5, 370, 623 purchases in total.</p><p>For the text corpora, we fix the vocabulary to the 15k most frequent terms and remove all words that are not in the vocabulary. Following <ref type="bibr" target="#b24">Mikolov et al. (2013c)</ref>, we additionally remove each word with probability 1 − 10 −5 /f v , where f v is the word frequency. This downsamples especially the frequent words and speeds up training. (Sizes reported in <ref type="table">Table 1</ref> are the number of words remaining after preprocessing.)</p><p>Models. Our goal is to fit the S-EFE model on these datasets. For the text data, we use the Bernoulli distribution as the conditional exponential family, while for the shopping data we use the Poisson distribution, which is more appropriate for count data.</p><p>On each dataset, we compare four approaches based on S-EFE with two EFE <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref> baselines. All are fit using stochastic gradient descent (SGD) <ref type="bibr" target="#b30">(Robbins and Monro, 1951)</ref>. In particular, we compare the following methods:</p><p>• A global EFE model, which cannot capture group structure.</p><p>• Separate EFE models, fitted independently on each group.</p><p>• (this paper) S-EFE without hierarchical structure or amortization.</p><p>• (this paper) S-EFE with hierarchical group structure.</p><p>• (this paper) S-EFE, amortized with a feed-forward neural network (Eq. 7).</p><p>• (this paper) S-EFE, amortized using a residual network (Eq. 8).</p><p>Experimental setup and hyperparameters. For text we set the dimension of the embeddings to K = 100, the number of hidden units to H = 25, and we experiment with two context sizes, 2 and 8.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>4</head><p>In the shopping data, we use K = 50 and H = 20, and we randomly truncate the context of baskets larger than 20 to reduce their size to 20. For both methods, we use 20 negative samples.</p><p>For all methods, we subsample minibatches of data in the same manner. Each minibatch contains subsampled observations from all groups and each group is subsampled proportionally to its size. For text, the words subsampled from within a group are consecutive, and for shopping data the observations are sampled at the shopping trip level. This sampling scheme reduces the bias from imbalanced group sizes. For text, we use a minibatch size of N/10000, where N is the size of the corpus, and we run 5 passes over the data; for the shopping data we use N/100 and run 50 passes. We use the default learning rate setting of TensorFlow for Adam 5 <ref type="bibr" target="#b16">(Kingma and Ba, 2015)</ref>.</p><p>We use the standard initialization schemes for the neural network parameters. The weights are drawn from a uniform distribution bounded at ± √ 6/ √ K + H <ref type="bibr" target="#b11">(Glorot and Bengio, 2010)</ref>. For the embeddings, we try 3 initialization schemes and choose the best one based on validation error. In particular, these schemes are: (1) all embeddings are drawn from the Gaussian prior implied by the regularizer; (2) the embeddings are initialized from a global embedding; (3) the context vectors are initialized from a global embedding and held constant, while the embeddings vectors are drawn randomly from the prior. Finally, for each method we choose the regularization variance from the set {100, 10, 1, 0.1}, also based on validation error.</p><p>Runtime. We implemented all methods in Tensorflow. On the Senate speeches, the runtime of S-EFE is 4.3 times slower than the runtime of global EFE, hierarchical EFE is 4.6 times slower than the runtime of global EFE, and amortized S-EFE is 3.3 times slower than the runtime of global EFE. (The Senate speeches have the most groups and hence the largest difference in runtime between methods.) Evaluation metric. We evaluate the fits by held-out pseudo (log-)likelihood. For each model, we compute the test pseudo log-likelihood, according to the exponential family distribution used (Bernoulli or Poisson). For each test entry, a better model will assign higher probability to the observed word or item, and lower probability to the negative samples. This is a fair metric because the competing methods all produce conditional likelihoods from the same exponential family. <ref type="bibr">6</ref> To make results comparable, we train and evaluate all methods with the same number of negative samples <ref type="bibr">(20)</ref>. The reported held out likelihoods give equal weight to the positive and negative samples.</p><p>Quantitative results. We show the test pseudo log-likelihood of all methods in <ref type="table" target="#tab_2">Table 2</ref> and report that our method outperforms the baseline in all experiments. We find that S-EFE with either hierarchical structure or amortization outperforms the competing methods based on standard EFE highlighted in bold. This is because the global EFE ignores per-group variations, whereas the separate EFE cannot share information across groups. The results of the global EFE baseline are better than fitting separate EFE (the other baseline), but unlike the other methods the global EFE cannot be used for the exploratory analysis of variations across groups. Our results show that using a hierarchical S-EFE is always better than using the simple S-EFE model or fitting a separate EFE on each group. The hierarchical structure helps, especially for the Senate speeches, where the data is divided into many</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>ArXiv papers</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Senate speeches Shopping data</head><p>Global EFE <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref> −2.176 ± 0.005 −2.239 ± 0.002 −0.772 ± 0.000 Separated EFE <ref type="bibr" target="#b32">(Rudolph et al., 2016)</ref> −2.500 ± 0.012 −2.915 ± 0.004 −0.807 ± 0.002  groups. Among the amortized S-EFE models we developed, at least amortization with residual networks outperforms the base S-EFE. The advantage of residual networks over feed-forward neural networks is consistent with the results reported by <ref type="bibr" target="#b14">(He et al., 2016)</ref>.</p><formula xml:id="formula_19">S-</formula><p>While both hierarchical S-EFE and amortized S-EFE share information about the embedding of a particular word across groups (through the global embedding ρ</p><p>v ), amortization additionally ties the embeddings of all words within a group (through learning the neural network of that group). We hypothesize that for the Senate speeches, which are split into many groups, this is a strong modeling constraint, while it helps for all other experiments.</p><p>Structured embeddings reveal a spectrum of word usage. We have motivated S-EFE with the example that the usage of INTELLIGENCE varies by ArXiv category <ref type="figure" target="#fig_2">(Figure 1a)</ref>. We now explain how for each term the per-group embeddings place the groups on a spectrum. For a specific term v we take its embeddings vectors {ρ v } for all groups s, and project them onto a one-dimensional space using the first component of principal component analysis (PCA). This is a one-dimensional summary of how close the embeddings of v are across groups. Such comparison is posible because the S-EFE shares the context vectors, which grounds the embedding vectors in a joint space.</p><p>The spectrum for the word INTELLIGENCE along its first principal component is the horizontal axis in <ref type="figure" target="#fig_2">Figure 1a</ref>. The dots are the projections of the group-specific embeddings for that word. (The embeddings come from a fitted S-EFE with feed-forward amortization.) We can see that in an unsupervised manner, the method has placed together groups related to physics on one end on the spectrum, while computer science, statistics and math are on the other end of the spectrum.</p><p>To give additional intuition of what the usage of INTELLIGENCE is at different locations on the spectrum, we have listed the 8 most similar words for the groups computer science (cs), quantitative finance (q-fin), math (math), statistics (stat), and nonlinear sciences (nlin). Word similarities are computed using cosine distance in the embedding space. Eventhough their embeddings are relatively close to each other on the spectrum, the model has the flexibility to capture high variabilty in the lists of similar words.</p><p>Exploring group variations with structured embeddings. The result of the S-EFE also allows us to investigate which words have the highest deviation from their average usage for each group. For example, in the Congressional speeches, there are many terms that we do not expect the Senators to use differently (e.g., most stopwords). We might however want to ask a question like "which words do Republicans from Texas use most differently from other Senators?" By suggesting an answer, our method can guide an exploratory data analysis. For each group s (state/party combination), we compute the top 3 words in argsort v ||ρ</p><formula xml:id="formula_21">(s) v − 1 S S t=1 ρ (t)</formula><p>v || from within the top 1k words.  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Discussion</head><p>We have presented several structured extensions of EFE for modeling grouped data. Hierarchical S-EFE can capture variations in word usage across groups while sharing statistical strength between them through a hierarchical prior. Amortization is an effective way to reduce the number of parameters in the hierarchical model. The amortized S-EFE model leverages the expressive power of neural networks to reduce the number of parameters, while still having the flexibility to capture variations between the embeddings of each group. Below are practical guidelines for choosing a S-EFE.</p><p>How can I fit embeddings that vary across groups of data? To capture variations across groups, never fit a separate embedding model for each group. We recommend at least sharing the context vectors, as all the S-EFE models do. This ensures that the latent dimensions of the embeddings are aligned across groups. In addition to sharing context vectors, we also recommend sharing statistical strength between the embedding vectors. In this paper we have presented two ways to do so, hierarchical modeling and amortization.</p><p>Should I use a hierarchical prior or amortization? The answer depends on how many groups the data contain. In our experiments, the hierarchical S-EFE works better when there are many groups. With less groups, the amortized S-EFE works better.</p><p>The advantage of the amortized S-EFE is that it has fewer parameters than the hierarchical model, while still having the flexibility to capture across-group variations. The global embeddings in an amortized S-EFE have two roles. They capture the semantic similarities of the words, and they also serve as the input into the amortization networks. Thus, the global embeddings of words with similar pattern of across-group variation need to be in regions of the embedding space that lead to similar modifications by the amortization network. As the number of groups in the data increases, these two roles become harder to balance. We hypothesize that this is why the amortized S-EFE has stronger performance when there are fewer groups.</p><p>Should I use feed-forward or residual networks? To amortize a S-EFE we recommend residual networks. They perform better than the feed-forward networks in all of our experiments. While the feed-forward network has to output the entire meaning of a word in the group-specific embedding, the residual network only needs the capacity to model how the group-specific embedding differs from the global embedding.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head></head><label></label><figDesc>a) S-EFE uncover variations in the usage of the word INTELLIGENCE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>Graphical repres. of S-EFE.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: (a) INTELLIGENCE is used differently across the ArXiv sections. Words with the closest embedding to the query are listed for 5 sections. (The embeddings were obtained by fitting an amortized S-EFE.) The method automatically orders the sections along the horizontal axis by their similarity in the usage of INTELLIGENCE. See Section 3 additional for details. (b) Graphical representation of S-EFE for data in S categories. The embedding vectors ρ (s)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>v</head><label></label><figDesc>. That is, if all weights were set to 0, the feed-forward network would output 0, while the residual network would output the global vector ρ (0) v for all groups.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc>Test</figDesc><table>log-likelihood on the three considered datasets. S-EFE consistently achieves the 
highest held-out likelihood. The competing methods are the global EFE, which can not capture group 
variations, and the separate EFE, which cannot share information across groups. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 3</head><label>3</label><figDesc>shows a summary of our findings (the full table is in the Appendix). According to the S-EFE (with residual network amortization), Republican Senators from Texas use BORDER and the phrase OUR COUNTRY in different contexts than other Senators. Some of these variations are probably influenced by term frequency, as we expect Democrats from Washington to talk about WASHINGTON more frequently than other states. But we argue that our method provides more insights than a frequency based analysis, as it is also sensitive to the context in which a word appears. For example, WASHINGTON might in some groups be used more often in</figDesc><table>TEXAS 

FLORIDA 
IOWA 
WASHINGTON 

border 
medicaid 
bankruptcy 
agriculture prescription 
washington 
our country 
prescription 
water 
farmers 
drug 
energy 
iraq 
medicare 
waste 
food 
drugs 
oil 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table 3 :</head><label>3</label><figDesc>List of the three most different words for different groups for the Congressional speeches. S-EFE uncovers which words are used most differently by Republican Senators (red) and Democratic Senators (blue) from different states. The complete table is in the Appendix. the context of PRESIDENT and GEORGE, while in others it might appear in the context of DC and CAPITAL, or it may refer to the state.</figDesc><table></table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Alternatively, we could share the embedding vectors ρv and have group-specific context vectors α (s) v . We did not explore that avenue and leave it for future work. 2 Another potential advantage of the proposed parameter sharing structure is that, when the context vectors are held fixed, the resulting objective function is convex, by the convexity properties of exponential families (Wainwright and Jordan, 2008).</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">Code is available at https://github.com/mariru/structured_embeddings</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">To save space we report results for context size 8 only. Context size 2 shows the same relative performance. 5 Adam needs to track a history of the gradients for each parameter that is being optimized. One advantage from reducing the number of parameters with amortization is that it results in a reduced computational overhead for Adam (as well as for other adaptive stepsize schedules). 6 Since we hold out chunks of consecutive words usually both a word and its context are held out. For all methods we have to use the words in the context to compute the conditional likelihoods.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We thank Elliott Ash and Suresh Naidu for the helpful discussions and for sharing the Senate speeches. This work is supported by NSF IIS-1247664, ONR N00014-11-1-0651, DARPA PPAML FA8750-14-2-0009, DARPA SIMPLEX N66001-15-C-4032, the Alfred P. Sloan Foundation, and the John Simon Guggenheim Foundation. Francisco J. R. Ruiz is supported by the EU H2020 programme (Marie Skłodowska-Curie grant agreement 706760).</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ammar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Mulcaire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tsvetkov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">A</forename><surname>Smith</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01925</idno>
		<title level="m">Massively multilingual word embeddings</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Conditionally specified distributions: an introduction (with comments and a rejoinder by the authors)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">C</forename><surname>Arnold</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Sarabia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Statistical Science</title>
		<imprint>
			<biblScope unit="volume">16</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="249" to="274" />
			<date type="published" when="2001" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Risteski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.03520</idno>
		<title level="m">Rand-walk: A latent variable model approach to word embeddings</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Bamler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<title level="m">Dynamic word embeddings. International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Item2vec: neural item embedding for collaborative filtering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Barkan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Koenigstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2016 IEEE 26th International Workshop on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
	<note>Machine Learning for Signal Processing</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A neural probabilistic language model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ducharme</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Vincent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Jauvin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of machine learning research</title>
		<imprint>
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1137" to="1155" />
			<date type="published" when="2003-02" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Machine learning and pattern recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">M</forename><surname>Bishop</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Information Science and Statistics</title>
		<imprint>
			<date type="published" when="2006" />
			<publisher>Springer</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">The Helmholtz machine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dayan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">M</forename><surname>Neal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computation</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="issue">5</biblScope>
			<biblScope unit="page" from="889" to="904" />
			<date type="published" when="1995" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Bayesian data analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gelman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Carlin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">S</forename><surname>Stern</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">B</forename><surname>Rubin</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2003" />
			<publisher>Chapman and Hall/CRC</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Amortized inference in probabilistic reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Gershman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">D</forename><surname>Goodman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirty-Sixth Annual Conference of the Cognitive Science Society</title>
		<meeting>the Thirty-Sixth Annual Conference of the Cognitive Science Society</meeting>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In Aistats</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page" from="249" to="256" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Diachronic word embeddings reveal statistical laws of semantic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jurafsky</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename></persName>
		</author>
		<idno type="arXiv">arXiv:1605.09096</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><forename type="middle">S</forename><surname>Harris</surname></persName>
		</author>
		<title level="m">Distributional structure. Word</title>
		<imprint>
			<date type="published" when="1954" />
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="page" from="146" to="162" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE Conference on Computer Vision and Pattern Recognition</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-I</forename><surname>Chiu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hanaki</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hegde</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Petrov</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1405.3515</idno>
		<title level="m">Temporal analysis of language through neural language models</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Auto-encoding variational Bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Inducing crosslingual distributed representations of words</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Klementiev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Titov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Bhattarai</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Bayesian dark knowledge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Korattikara</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Statistically significant detection of linguistic change</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Kulkarni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Al-Rfou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Perozzi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 24th International Conference on World Wide Web</title>
		<meeting>the 24th International Conference on World Wide Web</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="625" to="635" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Neural word embedding as implicit matrix factorization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Levy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goldberg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="2177" to="2185" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Efficient estimation of word representations in vector space</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1301.3781</idno>
	</analytic>
	<monogr>
		<title level="m">ICLR Workshop Proceedings</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Exploiting similarities among languages for machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><forename type="middle">V</forename><surname>Le</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1309.4168</idno>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distributed representations of words and phrases and their compositionality</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3111" to="3119" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Linguistic regularities in continuous space word representations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Mikolov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W.-T</forename><forename type="middle">A</forename><surname>Yih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Zweig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">HLT-NAACL</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="746" to="751" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Neural variational inference and learning in belief networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
		<title level="m" type="main">Machine learning: a probabilistic perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">P</forename><surname>Murphy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>MIT press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Glove: Global vectors for word representation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pennington</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Conference on Empirical Methods on Natural Language Processing</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">14</biblScope>
			<biblScope unit="page" from="1532" to="1543" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Stochastic backpropagation and approximate inference in deep generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">A stochastic approximation method. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Robbins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Monro</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1951" />
			<biblScope unit="page" from="400" to="407" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Dynamic bernoulli embeddings for language evolution</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08052</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint at</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Exponential family embeddings</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rudolph</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ruiz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mandt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="478" to="486" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Learning representations by back-propagating errors</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">E</forename><surname>Rumelhart</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hintont</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">J</forename><surname>Williams</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">323</biblScope>
			<biblScope unit="page">9</biblScope>
			<date type="published" when="1986" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Graphical models, exponential families, and variational inference. Foundations and Trends in Machine Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1" to="305" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<monogr>
		<title level="m" type="main">Discovery of evolving semantics through dynamic word embedding learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ding</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Rao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xiong</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.00607</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Bilingual word embeddings for phrase-based machine translation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">Y</forename><surname>Zou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">M</forename><surname>Cer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="1393" to="1398" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
