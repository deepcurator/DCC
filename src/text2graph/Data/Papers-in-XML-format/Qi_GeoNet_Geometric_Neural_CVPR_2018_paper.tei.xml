<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaojuan</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Uber Advanced Technologies Group ♭ YouTu Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ University of Toronto</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Renjie</forename><surname>Liao</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Uber Advanced Technologies Group ♭ YouTu Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ University of Toronto</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhengzhe</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Uber Advanced Technologies Group ♭ YouTu Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ University of Toronto</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raquel</forename><surname>Urtasun</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Uber Advanced Technologies Group ♭ YouTu Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ University of Toronto</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaya</forename><surname>Jia</surname></persName>
							<affiliation key="aff0">
								<orgName type="laboratory">Uber Advanced Technologies Group ♭ YouTu Lab</orgName>
								<orgName type="institution">The Chinese University of Hong Kong ‡ University of Toronto</orgName>
								<address>
									<region>Tencent</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">GeoNet: Geometric Neural Network for Joint Depth and Surface Normal Estimation</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we propose Geometric Neural Network (GeoNet) to jointly predict depth and surface normal maps from a single image. Building on top of two-stream CNNs, our GeoNet incorporates geometric relation between depth and surface normal via the new depth-to-normal and normalto-depth networks. Depth-to-normal network exploits the least square solution of surface normal from depth and improves its quality with a residual module. Normal-to-depth network, contrarily, refines the depth map based on the constraints from the surface normal through a kernel regression module, which has no parameter to learn. These two networks enforce the underlying model to efficiently predict depth and surface normal for high consistency and corresponding accuracy. Our experiments on NYU v2 dataset verify that our GeoNet is able to predict geometrically consistent depth and normal maps. It achieves top performance on surface normal estimation and is on par with state-of-theart depth estimation methods.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>We tackle the important problem of joint estimation of depth and surface normal from a single RGB image. The 2.5D geometric information is beneficial to various computer vision tasks, including structure from motion (SfM), 3D reconstruction, pose estimation, object recognition, and scene classification.</p><p>There exist a large amount of methods on depth estimation <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b6">7,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b17">18]</ref> and surface normal estimation <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b32">33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b1">2,</ref><ref type="bibr" target="#b17">18</ref>] from a single image. Among them, deep-neural-network-based methods achieve very promising results.</p><p>Challenges Albeit the great advancement in this field, we notice that most previous methods deal with depth and normal estimation independently, which possibly make their prediction inconsistent without considering the close underlying geometry relationship. For example, as demonstrated in <ref type="bibr" target="#b31">[32]</ref>, the predicted depth map could be distorted in planar regions. It is thus intriguing to ask what if one considers the fact that surface normal does not change much in planar regions. This thought motivates us to design new models, which are exactly based on above simple fact and yet potentially show a vital direction in this field, to exploit the inevitable geometric relationship between depth and surface normal for more accurate estimation.</p><p>We use the example in <ref type="figure" target="#fig_0">Fig. 1</ref> to illustrate the commonknowledge relation. On the one hand, surface normal is determined by local surface tangent plane of 3D points, which can be estimated from depth; on the other hand, depth is constrained by the local surface tangent plane determined by surface normal. Although it looks straightforward, it is not trivial to design neural networks to properly make use of these geometric conditions.</p><p>We note incorporating geometric relationship into traditional models via hand-crafted feature is already feasible, as explained in <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b3">4]</ref>. However, there is no much research to make it happen in neural networks. One possible design is to build a convolutional neural network (CNN) to directly learn such geometric relationship from data. However, our experiments in section 4.2 demonstrate that even with the common successful CNN architectures, e.g., VGG-16, we cannot obtain any reasonable normal results from depth, not even close. It is found that training always converges to very poor local minima given carefully tuned architectures and hyper-parameters.</p><p>These extensive experiments manifest that current classification CNN architectures do not have the necessary ability to learn such geometric relationship from data. This finding motivates us to design specialized architecture to explicitly incorporate and enforce geometric conditions. Our Contributions We in this paper propose the Geometric Neural Networks (GeoNet) to infer depth and surface normal in one unified system. The architecture of GeoNet involves a two-stream CNN, which predicts depth and surface normal from a single image respectively. The two networks manage the two streams to model the depth-to-normal and normal-to-depth mapping.</p><p>In particular, relying on least-square and residual modules, the depth-to-normal network effectively captures the geometric relationship. Normal-to-depth network updates estimates of depth via a kernel regression module; it does not require any parameters that should be learned. With these coupled networks, our GeoNet enforces the final prediction of depth and surface normal to follow the underlying conditions. Further, these two networks are computationally efficient since they do not have many parameters to learn.</p><p>Experimental results on NYU v2 dataset show that our GeoNet achieves state-of-the-art performance in terms of most of the evaluation metrics and is more efficient than other alternatives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>2.5D geometry estimation from a single image has been intensively studied in past years. Previous work can be roughly divided into two categories.</p><p>Traditional methods did not use deep neural networks, and mainly focused on exploiting low-level image cues and geometric constraints. For example, the method of <ref type="bibr" target="#b29">[30]</ref> estimates mean depth of the scene by recognizing the structures presented in the image, and inferring the scale of the scene. Based on Markov random fields (MRF), Saxena et al. <ref type="bibr" target="#b24">[25]</ref> predicts a depth map given the hand-crafted features of a single image. Vanishing points and lines are utilized in <ref type="bibr" target="#b11">[12]</ref> for recovering the surface layout.</p><p>Besides, Liu et al. <ref type="bibr" target="#b18">[19]</ref> leveraged predicted labels of semantic segmentation to incorporate geometry constraints. A scale-dependent classifier was proposed in <ref type="bibr" target="#b14">[15]</ref> to jointly learn semantic segmentation and depth estimation. Shi et al. <ref type="bibr" target="#b26">[27]</ref> showed that estimating the defocus blur is beneficial for recovering the depth map. In <ref type="bibr" target="#b3">[4]</ref>, a unified optimization problem was formed, which aims at recovering the intrinsic scene property, e.g., shape, illumination, and reflectance from shading. Relying on specially designed features, above methods directly incorporate geometric constraints. However, their model capacity and generality may be unsatisfactory to deal with different types of images.</p><p>With deep learning, many methods were recently proposed for single-image depth or/and surface normal prediction. Eigen et al. <ref type="bibr" target="#b7">[8]</ref> directly predicted the depth map by feeding the image to CNNs. Shelhamer et al. <ref type="bibr" target="#b25">[26]</ref> proposed a fully convolutional network (FCN) based solution to learn the full intrinsic decomposition of a single image, which involves inferring the depth map as the first intermediate step. In <ref type="bibr" target="#b6">[7]</ref>, a unified coarse-to-fine hierarchical network was adopted for depth/normal prediction.</p><p>For predicting single-image surface normal, Wang et al. <ref type="bibr" target="#b32">[33]</ref> incorporated local, global, and vanishing point information in designing the network architecture. In <ref type="bibr" target="#b19">[20]</ref>, a continuous conditional random field (CRF) was built on top of CNN to smooth super-pixel-based depth prediction. There is also a skip-connected architecture <ref type="bibr" target="#b2">[3]</ref> to fuse hidden representations of different layers for surface normal estimation.</p><p>All these methods regard depth and surface normal prediction as independent tasks, thus ignoring their basic geometric relationship. The most related work to ours is that of <ref type="bibr" target="#b31">[32]</ref>, which designed a CRF with a 4-stream CNN, considering the consistency of predicted depth and surface normal in planar regions. Nevertheless, it may fail when planar regions are uncommon in images. In comparison, our GeoNet exploits the geometric relationship between depth and surface normal for general situations without making any planar or curvature assumptions. It is not limited to particular types of regions, and is computationally efficient.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Geometric Neural Networks</head><p>In this section, we first introduce the depth-to-normal network, which refines the surface normal from the given depth map. Then we explain the normal-to-depth network to update depth from the given surface normal map. It is followed by the overall architecture of our GeoNet, which utilizes these new modules.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Depth-to-Normal Network</head><p>As aforementioned, learning geometrically consistent surface normal from depth via directly applying neural networks is surprisingly hard. Inspired from the geometry-based solution <ref type="bibr" target="#b8">[9]</ref>, we propose a novel neural network architecture, which takes initial surface normal and depth maps as input and predicts a better surface normal. We start with introducing the geometric model, which can be viewed as a fixweight neural network. Then we explain the residual module that aims at smoothing and combining the predictions of surface normal. Pinhole Camera Model As a common practice, the pinhole camera model is adopted. We denote (u i , v i ) as the location of pixel i in the 2D image. Its corresponding location in 3D space is (x i , y i , z i ), where z i is the depth. Based on the geometry of perspective projection, we obtain</p><formula xml:id="formula_0">x i = (u i − c x ) * z i /f x , y i = (v i − c y ) * z i /f y ,<label>(1)</label></formula><p>where f x and f y are the focal length along the x and y directions respectively. c x and c y are coordinates of the principal points.</p><p>Least Square Module Following <ref type="bibr" target="#b8">[9]</ref>, we formulate inference of surface normal from the depth map as a least square problem. Specifically, for any pixel i, given its depth z i , we first compute its 3D coordinates (x i , y i , z i ) from its 2D coordinates (u i , v i ) relying on the pinhole camera model. In order to compute the surface normal of pixel i, we need to determine the tangent plane, which crosses pixel i in 3D space. We follow traditional assumption that pixels within a local neighborhood of pixel i lie on the same tangent plane. In particular, we define the set of neighboring pixels, including pixel i itself, as</p><formula xml:id="formula_1">N i = {(x j , y j , z j ) ||u i − u j | &lt; β,<label>(2)</label></formula><formula xml:id="formula_2">|v i − v j | &lt; β, |z i − z j | &lt; γz i },</formula><p>where β and γ are hyper-parameters controlling the size of neighborhood along x-y and depth axes respectively. With these pixels on the tangent plane, the surface normal estimate n = n x , n y , n z should satisfy the over-determined linear system of</p><formula xml:id="formula_3">An = b, subject to n 2 2 = 1.<label>(3)</label></formula><p>where</p><formula xml:id="formula_4">A =      x 1 y 1 z 1 x 2 y 2 z 2 . . . . . . . . . x K y K z K      ∈ R K×3 ,<label>(4)</label></formula><p>and b ∈ R K×1 is a constant vector. K is the size of N i . The least square solution of this problem, which minimizes An − b 2 has the closed form of</p><formula xml:id="formula_5">n = (A ⊤ A) −1 A ⊤ 1 (A ⊤ A) −1 A ⊤ 1 2 ,<label>(5)</label></formula><p>where 1 ∈ R k is a vector with all 1 elements. It is not surprising that Eq. (5) can be regarded as a fix-weight neural network, which predicts surface normal given the depth map.</p><p>Residual Module This least square module occasionally produces noisy estimate of surface normal due to noise and other image issues. A rough normal map is shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. To improve accuracy, we propose a residual module, which consists of a 3-layer CNN with skip-connection and 1 × 1 convolutional layer, as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. The goal is to smooth out noise and combine the initial guess of surface normal to further enhance the quality. In particular, before fed to the 1 × 1 convolution, the output of this CNN is concatenated with initial estimation of surface normal, which could be output of another network.</p><p>The architecture of this depth-to-normal network is illustrated in the bottom row of <ref type="figure" target="#fig_1">Fig. 2</ref>. By explicitly leveraging the geometric relationship between depth and surface normal, our network circumvents the aforementioned difficulty in learning geometrically consistent surface normal. It is computationally efficient since the least-square module is just a fix-weight layer. The extra important benefit stems from using ground-truth depth as the input to pre-train the network. It permits concatenation and joint fine-tuning with other networks, which predict depth maps from raw images.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Normal-to-Depth Network</head><p>Now we turn to the normal-to-depth network. For any pixel i, given its surface normal (n ix , n iy , n iz ) and an initial estimate of depth z i , the goal is to refine depth.</p><p>First, note that given the 3D point (x i , y i , z i ) and its surface normal (n ix , n iy , n iz ), we can uniquely determine the tangent plane P i , which satisfies the equation of</p><formula xml:id="formula_6">n ix (x − x i ) + n iy (y − y i ) + n iz (z − z i ) = 0. (6)</formula><p>As explained in section 3.1, we can still assume that pixels within a small neighborhood of pixel i lie on this tangent plane P i , as shown in <ref type="figure" target="#fig_1">Fig. 2</ref> (bottom row). This neighborhood M i is defined as</p><formula xml:id="formula_7">Mi = {(xj, yj, zj)|n ⊤ j ni &gt; α, |ui − uj| &lt; β, |vi − vj| &lt; β},</formula><p>where β is the hyper-parameter to control the size of neighborhood along x − y axes. α is a threshold to rule out spatially close points, which are not approximately coplanar. (u i , v i ) are the coordinates of pixel i in the 2D image.</p><p>For any pixel j ∈ M i , if we assume its depth z j is accurate, we can compute the depth estimate of pixel i as z ′ ji relying on Eqs. <ref type="formula" target="#formula_0">(1)</ref> and <ref type="bibr" target="#b5">(6)</ref>. It is expressed as</p><formula xml:id="formula_8">z ′ ji = n ix x j + n ix y j + n iz z j (u i − c x )n ix /f x + (v i − c y )n iy /f y + n iz .<label>(7)</label></formula><p>After getting it, to refine depth of pixel i, we use kernel regression to aggregate estimation from all pixels in the neighborhood aŝ</p><formula xml:id="formula_9">z i = j∈Mi K(n j , n i )z ′ ji j∈Mi K(n j , n i ) ,<label>(8)</label></formula><p>whereẑ i is the refined depth, n i = n ix , n iy , n iz and K is the kernel function. We use linear kernel due to its simplicity, i.e., K(n j , n i ) = n ⊤ j n i . In this case, the smaller the angle between normals n i and n j is, which means higher probability that pixels i and j are in the same tangent plane, the more accurate and important the estimate z ′ ji is. The above process is illustrated in the upper row of <ref type="figure" target="#fig_1">Fig. 2</ref>. It can be viewed as a voting process where every pixel j ∈ M i gives a "vote" to determine the depth of pixel i. By utilizing the geometric relationship between surface normal and depth, we efficiently improve the quality of depth estimate without the need to learn any weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">GeoNet</head><p>Full Architecture With above two networks, we now explain our full model illustrated in <ref type="figure" target="#fig_2">Fig. 3</ref>. We first use twostream CNNs to predict the initial depth and surface normal maps, as shown in <ref type="figure" target="#fig_2">Fig. 3(a) and (b)</ref> respectively. The fundamental structures we adopted are (1) VGG-16 <ref type="bibr" target="#b28">[29]</ref> and (2) ResNet-50 <ref type="bibr" target="#b10">[11]</ref>.</p><p>Based on the initial depth map predicted by one CNN, we apply the depth-to-normal network explained in Section 3.1 to refine normal as shown in <ref type="figure" target="#fig_2">Fig. 3(c)</ref>. Similarly, as shown in <ref type="figure" target="#fig_2">Fig. 3(d)</ref>, given the surface normal estimate, we refine depth using the normal-to-depth network described in Section 3.2. We pre-train the depth-to-normal network taking groundtruth depth as input. For the normal-to-depth network, we do not need to learn any weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Loss Functions</head><p>We now explain the loss functions associated with our GeoNet. For pixel i, we denote the initial, refined and ground-truth depth as z i ,ẑ i and z gt i respectively. Similarly, we have these classes of surface normal as n i ,n i and n gt i respectively. The total number of pixels is M . The overall loss function is the summation of two terms, i.e., L = l depth + l normal . The depth loss l depth is expressed as</p><formula xml:id="formula_10">l depth = 1 M i z i − z gt i 2 2 + η i ẑ i − z gt i 2 2 .</formula><p>The surface normal loss l normal is</p><formula xml:id="formula_11">l normal = 1 M i n i − n gt i 2 2 + λ i n i − n gt i 2 2</formula><p>.</p><p>Here λ and η are hyper-parameters to balance contribution of different terms. The final predictions of our GeoNet are the optimized depth and surface normal estimates. GeoNet is trained by back-propagation in an end-to-end manner.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We evaluate the effectiveness of our method on the NYU v2 dataset <ref type="bibr" target="#b27">[28]</ref>. It contains 464 video sequences of indoor scenes, which are further divided into 249 sequences for training and 215 for testing. We sample 30, 816 frames from the training video sequences as the training data. Note that the methods of <ref type="bibr" target="#b6">[7]</ref>, <ref type="bibr" target="#b33">[34]</ref> and <ref type="bibr" target="#b15">[16]</ref> used 120K, 90K and 95K data for training, which are all significantly more than ours.</p><p>For the training set, we use the inpainting method of <ref type="bibr" target="#b16">[17]</ref> to fill in invalid or missing pixels in the ground-truth depth maps. Then we generate ground-truth surface normal maps following the procedure of <ref type="bibr" target="#b8">[9]</ref>. Our GeoNet is implemented in TensorFlow.</p><p>We initialize the two-stream CNNs with networks pretrained on ImageNet. In particular, we try two different choices. The first is a modified VGG-16 network based on FCN <ref type="bibr" target="#b22">[23]</ref> with dilated convolutions <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b34">35]</ref> and global pooling <ref type="bibr" target="#b21">[22]</ref>. The second is a ResNet-50 following the model of <ref type="bibr" target="#b15">[16]</ref>. We use Adam <ref type="bibr" target="#b13">[14]</ref> to optimize the network and clip the norm of gradients so that they are no larger than 5. The  initial learning rate is 1e −4 and is adjusted following the polynomial decay strategy with the power parameter 0.9. Random horizontal flip is utilized to augment training data. While flipping images, we multiply the corresponding xdirection of surface normal maps with −1.</p><p>The whole system is trained with batch-size 4 for 40, 000 iterations. Hyper-parameters {α, β, γ, λ, η} are set to {0.95, 9, 0.05, 0.01, 0.5} according to validation on a 5% randomly split training data. λ is set to a small value due to numerical instability when computing the matrix inverse in the least square module -gradient of Eq. (5) needs inverse of matrix A T A, which might be erroneous if the condition number is small. Setting λ = 0.01 mitigates this effect.</p><p>Following <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b33">34]</ref>, we adopt four metrics to evaluate resulting depth map quantitatively. They are root mean square error (rmse), mean log 10 error (log 10), mean relative error (rel), and pixel accuracy as percentage of pixels with</p><formula xml:id="formula_12">max(z i /z gt i , z gt i /z i ) &lt; δ for δ ∈ [1.25, 1.25 2 , 1.25 3 ]</formula><p>. The evaluation metrics for surface normal prediction <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b2">3,</ref><ref type="bibr" target="#b6">7]</ref> are mean of angle error (mean), medians of the angle error (median), root mean square error (rmse), and pixel accuracy as percentage of pixels with angle error below threshold t where t ∈ [11.25</p><p>• , 22.5</p><formula xml:id="formula_13">• , 30 • ].</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Comparison with State-of-the-Art</head><p>In this section, we compare our GeoNet with existing methods in terms of depth and/or surface normal prediction.</p><p>Surface Normal Prediction For surface normal prediction, the results are listed in <ref type="table">Table 1</ref>. Our GeoNet consistently outperforms previous approaches regarding all different metrics. Note that since we use the same backbone network architecture VGG-16, the improvement stems from our depth-to-normal network, which effectively correct errors during estimation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Depth Prediction</head><p>In the task of depth prediction, since most state-of-the-art methods adopt either backbone network between VGG-16 and ResNet-50, we thus conduct experiments under both settings. The complete results are shown (a) Image (b) Deep3D <ref type="bibr" target="#b32">[33]</ref> (c) Multi-scale CNN <ref type="bibr" target="#b6">[7]</ref> (d) SkipNet <ref type="bibr" target="#b2">[3]</ref> (e) Ours (f) GT <ref type="figure">Figure 6</ref>: Visual comparison on surface normal prediction with VGG-16 being the backbone architecture. GT stands for "ground truth".</p><p>in <ref type="table">Table 2</ref>. Our GeoNet performs again better than stateof-the-art methods on 4 out of total 6 evaluation metrics. It performs comparably on the remaining two. Among all these methods, SURGE <ref type="bibr" target="#b31">[32]</ref> is the only one, which shares the same objective -that is, jointly predicting depth and surface normal. It builds CRFs on top of a VGG-16 network. Using the same backbone network, as summarized in the table, our GeoNet significantly outperforms it. It is because our model does not impose special assumptions on surface shape and underlying geometry.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Visual Comparisons</head><p>We show visual examples of predicted depth and surface normal maps. First, in <ref type="figure" target="#fig_4">Fig. 5</ref>  <ref type="table">Table 1</ref>: Performance of surface normal prediction on NYU v2 test set. "Baseline" refers to using VGG-16 network with global pooling to directly predict surface normal from raw images. "SkipNet <ref type="bibr" target="#b2">[3]</ref>+GeoNet" means building GeoNet on top of the normal result of <ref type="bibr" target="#b2">[3]</ref>.</p><p>Error Accuracy rmse log 10 rel δ &lt; 1.25 δ &lt;  <ref type="table">Table 2</ref>: Performance of depth prediction on NYU v2 test set. "Baseline" means using VGG-16 to directly predict depth from raw images. VGG and ResNet are short for VGG-16 and ResNet-50 respectively.</p><p>objects on the table in the 2nd and 3rd rows respectively. We also show the corresponding predictions of surface normal to verify that our GeoNet takes the advantage of surface normal to improve depth. The usefulness is illustrated regarding the whiteboard in the 1st row. 3D visualization of our depth prediction is shown in <ref type="figure" target="#fig_5">Fig. 7</ref>. The wall region of our prediction is much smoother than previous state-of-theart FRCN <ref type="bibr" target="#b15">[16]</ref>, manifesting the necessity of incorporating geometric consistency.</p><p>Moreover, we compare results with those of other methods, including Deep3D <ref type="bibr" target="#b32">[33]</ref>, Multi-scale CNN <ref type="bibr" target="#b6">[7]</ref> and SkipNet <ref type="bibr" target="#b2">[3]</ref> on surface normal prediction in <ref type="figure">Fig. 6</ref>. GeoNet actually can produce results with better details on, for ex-(a) FCRN <ref type="bibr" target="#b15">[16]</ref> (b) Our (c) GT ample, the chair, washbasin and wall from the 1st, 2nd, 3rd rows respectively. More results of joint prediction are shown in <ref type="figure" target="#fig_3">Fig. 4</ref>. From these figures, it is clear that our GeoNet does a much better job in terms of geometry estimation compared with the baseline VGG-16 network, which was not designed for this task in the first place.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Running-time Comparison</head><p>We test our GeoNet on a PC with Intel i7-6950 CPU and a single TitanX GPU. When taking VGG-16 as the backbone network, our GeoNet obtains both surface normal and depth using 0.87s for an image with size 480 × 640. In comparison, Local Network <ref type="bibr" target="#b4">[5]</ref> takes around 24s to predict the depth map of the same-sized image; SURGE <ref type="bibr" target="#b31">[32]</ref> 1 also takes a lot of time due to the fact that it has to go through the forward-pass 10 times on the same VGG-16 network and it needs the inference of CRFs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">CNNs and Geometric Conditions</head><p>In this section, we verify our motivation through experiments and evaluate if previous CNNs can directly learn a mapping from depth to surface normal, implicitly following the geometric relationship.</p><p>To this end, we train CNNs, which take ground-truth depth and surface normal maps as input and supervision respectively. We tried different architectures, which include Error Accuracy mean median rmse 11.25</p><p>• 22.  <ref type="table">Table 3</ref>: Performance evaluation of depth-to-normal on NYU v2 test set. VGG stands for VGG-16 network. LS means our least square module. D-N is our depth-to-normal network without the last 1 × 1 convolution layer. Ground-truth depth maps are used as input.</p><p>the first 4 layers of VGG-16, the first 7 layers of VGG-16, and full VGG-16 network. Before fed it to networks, the depth map is transformed into a 3-channel image encoding {x, y, z} coordinates respectively. We provide the test performance on NYU v2 dataset in <ref type="table">Table 3</ref>. All alternatives converge to very poor local minima. For fair comparison and clear illustration, we provide the test performance of surface normal predicted by our depth-tonormal network without combining the initial surface normal estimation. In particular, since the depth-to-normal network contains least-square and residual modules, we also show the surface normal map predicted by the least square module only, denoting as "LS". The table reveals that LS module is already significantly better than the vanilla CNN baselines in all aspects. Moreover, with the residual module, our depthto-normal network accomplishes superior results compared to using the least-square module alone.</p><p>These experiments preliminarily lead us to the following important findings.</p><p>1. Learning a mapping from depth to normal directly via vanilla CNNs hardly respects the underlying geometric relationship.</p><p>2. Despite its simplicity, the least square module is very effective in incorporating geometric conditions into neural networks, thus leading to better performance.</p><p>3. Our overall depth-to-normal network further improves the quality of normal prediction compared to the single least-square module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Geometric Consistency</head><p>In this section, we verify if the predictions of depth and surface normal maps made by our GeoNet are consistent. To this end, we first pre-trained our depth-to-normal network without the last 1 × 1 convolution layer using ground-truth depth and surface normal maps and regard it as an accurate Error Accuracy mean median rmse 11.25</p><p>• 22.  <ref type="table">Table 4</ref>: Depth-to-normal consistency evaluation on the NYU v2 test set. "Pred" means that we transform predicted depth to surface normal and compare it with the predicted surface normal. "GT" means that we transform predicted depth to surface normal and compare it with the ground-truth surface normal. "Baseline" and "GeoNet" indicate that predictions are from baseline and our model respectively. The backbone network of baseline is VGG-16.</p><p>transformation. Given the predicted depth map, we compute the transformed surface normal map using the pre-trained network.</p><p>With these preparations, we compare error and accuracy under the following 4 settings. (1) Metrics between transformed and predicted normal (depth and surface normals generated by baseline CNNs). (2) Metrics between transformed and predicted normal (depth and surface normals generated by our GeoNet). (3) Metrics between transformed and ground-truth normal (depth generated by baseline CNNs). (4) Metrics between transformed and ground-truth normal (depth generated by our GeoNet). Here we also use the VGG-16 network as the baseline CNN.</p><p>The results are shown in <ref type="table">Table 4</ref>. The "Pred" columns of the table show that our GeoNet can generate predictions of depth and surface normal more consistent than those of the baseline CNNs. From the "GT" columns of the table, it is also obvious that, compared to the baseline CNN, the predictions yielded from our GeoNet are consistently closer to the ground truth.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose Geometric Neural Networks (GeoNet) to jointly predict depth and surface normal from a single image. Our GeoNet involves depth-to-normal and normal-to-depth networks. It effectively enforces the geometric conditions that computation should obey regarding depth and surface normal. They make the final prediction geometrically consistent and more accurate. Our extensive experiments show that GeoNet achieves state-of-the-art performance.</p><p>In the future, we would like to apply our GeoNet to tasks with inherent lighting and color constraints, such as intrinsic image decomposition and 3D reconstruction.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Geometric relationship of depth and surface normal. Surface normal can be estimated from 3D point cloud; depth is inferred from surface normal by solving linear equations.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Upper row: normal-to-depth network. Bottom row: depth-to-normal network.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overall framework of our Geometric Neural Networks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Visual comparison on joint prediction with VGG-16 as backbone architecture. GT stands for "ground truth".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Visual comparison on depth prediction with ResNet-50 as backbone architecture. GT stands for "ground truth".</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: 3D visulization of point cloud with depth from FCRN [16], our prediction and ground truth. Each row shows the point cloud observed from one viewpoint.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>Pred-GeoNet 34.9 31.4 41.4 15.3 35.0 47.7 GT-Baseline 47.8 47.3 52.1 2.8 11.8 20.7 GT-GeoNet 36.8 32.1 44.5 15.0 34.5 46.7</figDesc><table>5 
• 30 

• 

Pred-Baseline 42.2 39.8 48.9 9.8 
25.2 35.9 
</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">We do not have exact time without available public code.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Coupled depth learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Baig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Torresani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Ramanan</surname></persName>
		</author>
		<title level="m">Representation of the pixels, by the pixels, and for the pixels. arXiv</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Marr revisited: 2d-3d alignment via surface normal prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bansal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Shape, illumination, and reflectance from shading</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Malik</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="1670" to="1687" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Depth from a single image by harmonizing overcomplete local network predictions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Shakhnarovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Semantic image segmentation with deep convolutional nets and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Predicting depth, surface normals and semantic labels with a common multi-scale convolutional architecture</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Depth map prediction from a single image using a multi-scale deep network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Eigen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Puhrsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Data-driven 3d primitives for single image understanding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Unfolding an indoor origami world</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="687" to="702" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Recovering surface layout from an image. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hebert</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Depth extraction from video using non-parametric sampling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Karsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">B</forename><surname>Kang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="775" to="788" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization. arXiv</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Pulling things out of perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Ladicky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Deeper depth prediction with fully convolutional residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Laina</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rupprecht</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Belagiannis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tombari</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Navab</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">3DV</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Colorization using optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Levin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lischinski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Weiss</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ToG</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Depth and surface normal estimation from monocular images using regression on deep features and hierarchical crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Single image depth estimation from predicted semantic labels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Koller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1253" to="1260" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning depth from single monocular images using deep convolutional neural fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Reid</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Discrete-continuous depth estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
		<title level="m">Parsenet: Looking wider to see better. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Monocular depth estimation using neural regression forest</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Todorovic</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning depth from single monocular images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Saxena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">H</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">Y</forename><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="1161" to="1168" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Scene intrinsics and depth from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">T</forename><surname>Barron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV Workshops</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="37" to="44" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Break ames room illusion: depth from general single images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Jia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">SIGRAPH</title>
		<imprint>
			<biblScope unit="issue">2</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Indoor segmentation and support inference from rgbd images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silberman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ECCV</title>
		<imprint>
			<biblScope unit="issue">4</biblScope>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv</note>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Depth estimation from image structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2002" />
			<publisher>PAMI</publisher>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="page" from="1226" to="1238" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Towards unified depth and semantic prediction from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">Surge: Surface regularized geometry estimation from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Russell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Cohen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Price</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Designing deep networks for surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Fouhey</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<monogr>
		<title level="m" type="main">Multiscale continuous crfs as sequential deep networks for monocular depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Ricci</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Ouyang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Sebe</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
		<title level="m">Multi-scale context aggregation by dilated convolutions. arXiv</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Discriminatively trained dense surface normal estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Zeisl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pollefeys</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="468" to="484" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Indoor scene structure analysis for single image depth estimation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Zhuo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Salzmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="614" to="622" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
