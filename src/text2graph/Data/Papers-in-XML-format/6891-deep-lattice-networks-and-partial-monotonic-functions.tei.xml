<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep Lattice Networks and Partial Monotonic Functions</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Seungil</forename><surname>You</surname></persName>
							<email>siyou@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Ding</surname></persName>
							<email>dwding@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Kevin</forename><surname>Canini</surname></persName>
							<email>canini@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Pfeifer</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Maya</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
							<email>mayagupta@google.com</email>
							<affiliation key="aff0">
								<orgName type="institution">Google Research</orgName>
								<address>
									<addrLine>1600 Amphitheatre Parkway</addrLine>
									<postCode>94043</postCode>
									<settlement>Mountain View</settlement>
									<region>CA</region>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep Lattice Networks and Partial Monotonic Functions</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>We propose learning deep models that are monotonic with respect to a userspecified set of inputs by alternating layers of linear embeddings, ensembles of lattices, and calibrators (piecewise linear functions), with appropriate constraints for monotonicity, and jointly training the resulting network. We implement the layers and projections with new computational graph nodes in TensorFlow and use the Adam optimizer and batched stochastic gradients. Experiments on benchmark and real-world datasets show that six-layer monotonic deep lattice networks achieve state-of-the art performance for classification and regression with monotonicity guarantees.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>We propose building models with multiple layers of lattices, which we refer to as deep lattice networks (DLNs). While we hypothesize that DLNs may generally be useful, we focus on the challenge of learning flexible partially-monotonic functions, that is, models that are guaranteed to be monotonic with respect to a user-specified subset of the inputs. For example, if one is predicting whether to give someone else a loan, we expect and would like to constrain the prediction to be monotonically increasing with respect to the applicant's income, if all other features are unchanged. Imposing monotonicity acts as a regularizer, improves generalization to test data, and makes the end-to-end model more interpretable, debuggable, and trustworthy.</p><p>To learn more flexible partial monotonic functions, we propose architectures that alternate three kinds of layers: linear embeddings, calibrators, and ensembles of lattices, each of which is trained discriminatively to optimize a structural risk objective and obey any given monotonicity constraints. See <ref type="figure">Fig. 2</ref> for an example DLN with nine such layers.</p><p>Lattices are interpolated look-up tables, as shown in <ref type="figure">Fig. 1</ref>. Lattices have been shown to be an efficient nonlinear function class that can be constrained to be monotonic by adding appropriate sparse linear inequalities on the parameters <ref type="bibr" target="#b0">[1]</ref>, and can be trained in a standard empirical risk minimization framework <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b0">1]</ref>. Recent work showed lattices could be jointly trained as an ensemble to learn flexible monotonic functions for an arbitrary number of inputs <ref type="bibr" target="#b2">[3]</ref>.</p><p>Calibrators are one-dimensional lattices, which nonlinearly transform a single input <ref type="bibr" target="#b0">[1]</ref>; see <ref type="figure">Fig. 1</ref> for an example. They have been used to pre-process inputs in two-layer models: calibrators-then-linear models <ref type="bibr" target="#b3">[4]</ref>, calibrators-then-lattice models <ref type="bibr" target="#b0">[1]</ref>, and calibrators-then-ensemble-of-lattices model <ref type="bibr" target="#b2">[3]</ref>. Here, we extend their use to discriminatively normalize between other layers of the deep model, as well as act as a pre-processing layer. We also find that using a calibrator for a last layer can help nonlinearly transform the outputs to better match the labels.</p><p>We first describe the proposed DLN layers in detail in Section 2. In Section 3, we review more related work in learning flexible partial monotonic functions. We provide theoretical results characterizing the flexibility of the DLN in Section 4, followed by details on our open-source TensorFlow imple-  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Deep Lattice Network Layers</head><p>We describe in detail the three types of layers we propose for learning flexible functions that can be constrained to be monotonic with respect to any subset of the inputs. Without loss of generality, we assume monotonic means monotonic non-decreasing (one can flip the sign of an input if nonincreasing monotonicity is desired). Let x t ∈ R Dt be the input vector to the tth layer, with D t inputs, and let x t [d] denote the dth input for d = 1, . . . , D t . <ref type="table" target="#tab_2">Table 1</ref> summarizes the parameters and hyperparameters for each layer. For notational simplicity, in some places we drop the notation t if it is clear in the context. We also denote as x m t the subset of x t that are to be monotonically constrained, and as x  (1) The output of the linear embedding layer is:</p><formula xml:id="formula_0">x t+1 = x m t+1 x n t+1 = W m t x m t W n t x n t + b t</formula><p>Only the first D m t+1 coordinates of x t+1 needs to be a monotonic input to the t + 1 layer. These two linear embedding matrices and bias vector are discriminatively trained.</p><p>Calibration Layer: Each calibration layer consists of a separate one-dimensional piecewise linear transform for each input at that layer, c t,</p><formula xml:id="formula_1">d (x t [d]) that maps R to [0, 1], so that x t+1 := [c t,1 (x t [1]) c t,2 (x t [2]) · · · c t,Dt (x t [D t ])]</formula><p>T .</p><p>Here each c t,d is a 1D lattice with K key-value pairs (a ∈ R K , b ∈ R K ), and the function for each input is linearly interpolated between the two b values corresponding to the input's surrounding a values. An example is shown on the left in <ref type="figure">Fig. 1</ref>.</p><p>Each 1D calibration function is equivalent to a sum of weighted-and-shifted Rectified linear units (ReLU), that is, a calibrator function c(x[d]; a, b) can be equivalently expressed as</p><formula xml:id="formula_2">c(x[d]; a, b) = K k=1 α[k]ReLU(x − a[k]) + b[1],<label>(2)</label></formula><p>where</p><formula xml:id="formula_3">α[k] :=      b[k+1]−b[k] a[k+1]−a[k] − b[k]−b[k−1] a[k]−a[k−1] for k = 2, · · · , K − 1 b[2]−b[1] a[2]−a[1] for k = 1 − b[K]−b[K−1] a[K]−a[K−1]</formula><p>for k = K However, enforcing monotonicity and boundedness constraints for the calibrator output is much simpler with the (a, b) parameterization of each keypoint's input-output values, as we discuss shortly.</p><p>Before training the DLN, we fix the input range for each calibrator to [a min , a max ], and we fix the K keypoints a ∈ R K to be uniformly-spaced over [a min , a max ]. Inputs that fall outside [a min , a max ] are clipped to that range. The calibrator output parameters b ∈ [0, 1] K are discriminatively trained.</p><p>For monotonic inputs, we can constrain the calibrator functions to be monotonic by constraining the calibrator parameters b ∈ [0, 1] K to be monotonic, by adding the linear inequality constraints</p><formula xml:id="formula_4">b[k] ≤ b[k + 1] for k = 1, . . . , K − 1<label>(3)</label></formula><p>into the training objective <ref type="bibr" target="#b2">[3]</ref>. We also experimented with constraining all calibrators to be monotonic (even for non-monotonic inputs) for more stable/regularized training. , specifying the lattice's output for each of the 2 S vertices of the unit hypercube. Inputs in-between the vertices are linearly interpolated, which forms a smooth but nonlinear function over the unit hypercube. Two interpolation methods have been used, multilinear interpolation and simplex interpolation <ref type="bibr" target="#b0">[1]</ref> (also known as Lovász extension <ref type="bibr" target="#b4">[5]</ref>). We use multilinear interpolation for all our experiments, which can be expressed ψ(x)</p><p>T θ where the</p><formula xml:id="formula_5">non-linear feature transformation ψ(x) : [0, 1] S → [0, 1] 2 S</formula><p>are the 2 S linear interpolation weights that input x puts on each of the 2 S parameters θ such that the interpolated value for x is ψ(x) T θ, and</p><formula xml:id="formula_6">ψ(x)[j] = Π S d=1 x[d] vj [d] (1−x[d]) 1−vj [d]</formula><p>, where v j [·] ∈ 0, 1 is the coordinate vector of the jth vertex of the unit hypercube, and</p><formula xml:id="formula_7">j = 1, · · · , 2 D . For example, when S = 2, v 1 = (0, 0), v 2 = (0, 1), v 3 = (1, 0), v 4 = (1, 1) and ψ(x) = ((1 − x[1])(1 − x[2]), (1 − x[1])x[2], x[1](1 − x[2]), x[1]x[2]).</formula><p>The ensemble of lattices layer produces G outputs, one per lattice. When initializing the DLN, if the t + 1th layer is an ensemble of lattices, we randomly permute the outputs of the previous layer </p><formula xml:id="formula_8">∈ R Dt+1 , W m t ∈ R D m t+1 ×D m t , D t+1 W n t ∈ R (Dt+1−D m t+1 )×(Dt−D m t ) Calibrators B t ∈ R Dt×K K ∈ N + keypoints, input range [ , u] Lattice Ensemble θ t,g ∈ R 2</formula><p>S t for g = 1, . . . , G t G t lattices S t inputs per lattice to be assigned to the G t+1 × S t+1 inputs of the ensemble. If a lattice has at least one monotonic input, then that lattice's output is constrained to be a monotonic input to the next layer to guarantee end-to-end monotonicity. Each lattice is constrained to be monotonic by enforcing monotonicity constraints on each pair of lattice parameters that are adjacent in the monotonic directions; for details see Gupta et al. <ref type="bibr" target="#b0">[1]</ref>.</p><p>End-to-end monotonicity: The DLN is constructed to preserve end-to-end monotonicity with respect to a user-specified subset of the inputs. As we described, the parameters for each component (matrix, calibrator, lattice) can be constrained to be monotonic with respect to a subset of inputs by satisfying certain linear inequality constraints <ref type="bibr" target="#b0">[1]</ref>. Also if a component has a monotonic input, then the output of that component is treated as a monotonic input to the following layer. Because the composition of monotonic functions is monotonic, the constructed DLN belongs to the partial monotonic function class. The arrows in <ref type="figure">Figure 2</ref> illustrate this construction, i.e., how the tth layer output becomes a monotonic input to t + 1th layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1">Hyperparameters</head><p>We detail the hyperparameters for each type of DLN layer in <ref type="table" target="#tab_2">Table 1</ref>. Some of these hyperparameters constrain each other since the number of outputs from each layer must be equal to the number of inputs to the next layer; for example, if you have a linear embedding layer with D t+1 = 1000 outputs, then there are 1000 inputs to the next layer, and if that next layer is a lattice ensemble, its hyperparameters must obey G t × S t = 1000.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Related Work</head><p>Low-dimensional monotonic models have a long history in statistics, where they are called shape constraints, and often use isotonic regression <ref type="bibr" target="#b5">[6]</ref>. Learning monotonic single-layer neural nets by constraining the neural net weights to be positive dates back to Archer and Wang in 1993 <ref type="bibr" target="#b6">[7]</ref>, and that basic idea has been re-visited by others <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>, but with some negative results about the obtainable flexibility, even with multiple hidden layers <ref type="bibr" target="#b11">[12]</ref>. Sill <ref type="bibr" target="#b12">[13]</ref> proposed a three-layer monotonic network that used monotonic linear embedding and max-and-min-pooling. Daniels and Velikova <ref type="bibr" target="#b11">[12]</ref> extended Sill's result to learn a partial monotonic function by combining min-maxpooling, also known as adaptive logic networks <ref type="bibr" target="#b13">[14]</ref>, with partial monotonic linear embedding, and showed that their proposed architecture is a universal approximator for partial monotone functions. None of these prior neural networks were demonstrated on problems with more than D = 10 features, nor trained on more than a few thousand examples. For our experiments we implemented a positive neural network and a min-max-pooling network <ref type="bibr" target="#b11">[12]</ref> with TensorFlow.</p><p>This paper extends recent work in learning multidimensional flexible partial monotonic 2-layer networks consisting of a layer of calibrators followed by an ensemble of lattices <ref type="bibr" target="#b2">[3]</ref>, with parameters appropriately constrained for monotonicity, which built on earlier work of Gupta et al. <ref type="bibr" target="#b0">[1]</ref>. This work differs in three key regards.</p><p>First, we alternate layers to form a deeper, and hence potentially more flexible, network. Second, a key question addressed in Canini et al. <ref type="bibr" target="#b2">[3]</ref> is how to decide which features should be put together in each lattice in their ensemble. They found that random assignment worked well, but required large ensembles. They showed that smaller (and hence faster) models with the same accuracy could be trained by using a heuristic pre-processing step they proposed (crystals) to identify which features interact nonlinearly. This pre-processing step requires training a lattice for each pair of inputs to judge that pair's strength of interaction, which scales as O(D 2 ), and we found it can be a large fraction of overall training time for D &gt; 50.</p><p>We solve this problem of determining which inputs should interact in each lattice by using a linear embedding layer before an ensemble of lattices layer to discriminatively and adaptively learn during training how to map the features to the first ensemble-layer lattices' inputs. This strategy also means each input to a lattice can be a linear combination of the features. This use of a jointly trained linear embedding is the second key difference to that prior work <ref type="bibr" target="#b2">[3]</ref>.</p><p>The third difference is that in previous work <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b0">1,</ref><ref type="bibr" target="#b2">3]</ref>, the calibrator keypoint values were fixed a priori based on the quantiles of the features, which is challenging to do for the calibration layers mid-DLN, because the quantiles of their inputs are evolving during training. Instead, we fix the keypoint values uniformly over the bounded calibrator domain.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Function Class of Deep Lattice Networks</head><p>We offer some results and hypotheses about the function class of deep lattice networks, depending on whether the lattices are interpolated with multilinear interpolation (which forms multilinear polynomials), or simplex interpolation (which forms locally linear surfaces).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Cascaded multilinear lookup tables</head><p>We show that a deep lattice network made up only of cascaded layers of lattices (without intervening layers of calibrators or linear embeddings) is equivalent to a single lattice defined on the D input features if multilinear interpolation is used. It is easy to construct counter-examples showing that this result does not hold for simplex-interpolated lattices. Lemma 1. Suppose that a lattice has L inputs that can each be expressed in the form θ Proof. Each input i of the lattice can be expressed in the following form:</p><formula xml:id="formula_9">f i = θ T i ψ(x[s i ]) = 2 |s i | k=1 θ i [v ik ] d∈si x[d] v ik [d] (1 − x[d]) 1−v ik [d]</formula><p>This is a multilinear polynomial on x[s i ]. The output can be expressed in the following form:</p><formula xml:id="formula_10">F = 2 L j=1 θ i [v j ] L i=1 f vj [i] i (1 − f i ) 1−vj [i]</formula><p>Note the product in the expression: f i and 1 − f i are both multilinear polynomials, but within each term of the product, only one is present, since one of the two has exponent 0 and the other has exponent 1. Furthermore, since each f i is a function of a different subset of x, we conclude that the entire product is a multilinear polynomial. Since the sum of multilinear polynomials is still a multilinear polynomial, we conclude that F is a multilinear polynomial. Any multilinear polynomial on k variables can be converted to a k-dimensional multilinear lookup table, which concludes the proof.</p><p>Lemma 1 can be applied inductively to every layer of cascaded lattices down to the final output F (x). We have shown that cascaded lattices using multilinear interpolation is equivalent to a single multilinear lattice defined on all D features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Universal approximation of partial monotone functions</head><p>Theorem 4.1 in <ref type="bibr" target="#b11">[12]</ref> states that partial monotone linear embedding followed by min and max pooling can approximate any partial monotone functions on the hypercube up to arbitrary precision given sufficiently high embedding dimension. We show in the next lemma that simplex-interpolated lattices can represent min or max pooling. Thus one can use a DLN constructed with a linear embedding layer followed by two cascaded simplex-interpolated lattice layers to approximate any partial monotone function on the hypercube.</p><formula xml:id="formula_11">Lemma 2. Let θ min = (0, 0, · · · , 0, 1) ∈ R 2 n and θ max = (1, 0, · · · , 0) ∈ R 2</formula><p>n , and ψ simplex be the simplex interpolation weights. Then</p><formula xml:id="formula_12">min(x[0], x[1], · · · , x[n]) = ψ simplex (x) T θ min max(x[0], x[1], · · · , x[n]) = ψ simplex (x) T θ max Proof. From the definition of simplex interpolation [1], ψ simplex (x) T θ = θ[1]x[π[1]] + · · · + θ[2 n ]x[π[n]]</formula><p>, where π is the sorted order such that</p><formula xml:id="formula_13">x[π[1]] ≥ · · · ≥ x[π[n]]</formula><p>, and due to sparsity, θ min and θ max selects the min and the max.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Locally linear functions</head><p>If simplex interpolation <ref type="bibr" target="#b0">[1]</ref> (aka the Lovász extension) is used, the deep lattice network produces a locally linear function, because each layer is locally linear, and compositions of locally linear functions are locally linear. Note that a D input lattice interpolated with simplex interpolation has D! linear pieces <ref type="bibr" target="#b0">[1]</ref>. If one cascades an ensemble of D lattices into a lattice, then the number of possible locally linear pieces is of the order O((D!)!).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Numerical Optimization Details for the DLN</head><p>Operators: We implemented 1D calibrators and multilinear interpolation over a lattice as new C++ operators in TensorFlow <ref type="bibr" target="#b14">[15]</ref> and express each layer as a computational graph node using these new and existing TensorFlow operators. Our implementation is open sourced and can be found in https://github.com/tensorflow/lattice. We use the Adam optimizer <ref type="bibr" target="#b15">[16]</ref> and batched stochastic gradients to update model parameters. After each batched gradient update, we project parameters to satisfy their monotonicity constraints. The linear embedding layer's constraints are element-wise non-negativity constraints, so its projection clips each negative component to zero. This projection can be done in O(# of elements in a monotonic linear embedding matrix). Projection for each calibrator is isotonic regression with chain ordering, which we implement with the pooladjacent-violator algorithm <ref type="bibr" target="#b16">[17]</ref> for each calibrator. This can be done in O(# of calibration keypoints). Projection for each lattice is isotonic regression with partial ordering that imposes O(S2 S ) linear constraints for each lattice <ref type="bibr" target="#b0">[1]</ref>. We solved it with consensus optimization and alternating direction method of multipliers <ref type="bibr" target="#b17">[18]</ref> to parallelize the projection computations with a convergence criterion of = 10 −7 . This can be done in O(S2 S log(1/ )).</p><p>Initialization: For linear embedding layers, we initialize each component in the linear embedding matrix with IID Gaussian noise N (2, 1). The initial mean of 2 is to bias the initial parameters to be positive so that they are not clipped to zero by the first monotonicity projection. However, because the calibration layer before the linear embedding outputs in [0, 1] and thus is expected to have output E[x t ] = 0.5, initializing the linear embedding with a mean of 2 introduces an initial bias:</p><formula xml:id="formula_14">E[x t+1 ] = E[W t x t ] = D t .</formula><p>To counteract that we initialize each component of the bias vector, b t , to −D t , so that the initial expected output of the linear layer is E[</p><formula xml:id="formula_15">x t+1 ] = E[W t x t + b t ] = 0.</formula><p>We initialize each lattice's parameters to be a linear function spanning [0, 1], and add IID Gaussian noise N (0, 1 S 2 ) to each parameter, where S is the number of input to a lattice. We initialize each calibrator to be a linear function that maps [x min , x max ] to [0, 1] (and did not add any noise).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6">Experiments</head><p>We present results on the same benchmark dataset (Adult) with the same monotonic features as in Canini et al. <ref type="bibr" target="#b2">[3]</ref>, and for three problems from Google where the monotonicity constraints were specified by product groups. For each experiment, every model considered is trained with monotonicity guarantees on the same set of inputs. See <ref type="table" target="#tab_3">Table 2</ref> for a summary of the datasets.  For classification problems, we used logistic loss, and for the regression, we used squared error. For each problem, we used a validation set to optimize the hyperparameters for each model architecture: the learning rate, the number of training steps, etc. For an ensemble of lattices, we tune the number of lattices, G, and number of inputs to each lattice, S. All calibrators for all models used a fixed number of 100 keypoints, and set [−100, 100] as an input range.</p><p>In all experiments, we use the six-layer DLN architecture: Calibrators → Linear Embedding → Calibrators → Ensemble of Lattices → Calibrators → Linear Embedding, and validate the number of lattices in the ensemble G, number of inputs to each lattice, S, the Adam stepsize and number of loops.</p><p>For crystals <ref type="bibr" target="#b2">[3]</ref> we validated the number of ensembles, G, and number of inputs to each lattice, S, as well as Adam stepsize and number of loops. For min-max net <ref type="bibr" target="#b11">[12]</ref>, we validated the number of groups, G, and dimension of each group S, as well as Adam stepsize and number of loops.</p><p>For datasets where all features are monotonic, we also train a deep neural network with a non-negative weight matrix and ReLU as an activation unit with a final fully connected layer with non-negative weight matrix, which we call monotonic DNN, akin to the proposals of <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b7">8,</ref><ref type="bibr" target="#b8">9,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b10">11]</ref>. We tune the depth of hidden layers, G, and the activation units in each layer S.</p><p>All the result tables are sorted by their validation accuracy, and contain an additional column for chosen hyperparameters; 2 × 5D means G = 2 and S = 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1">User Intent Case Study (Classification)</head><p>For this real-world Google problem, the problem is to classify the user intent. This experiment is set-up to test generalization ability to non-IID test data. The train and validation examples are collected from the U.S., and the test set is collected from 20 other countries, and as a result of this difference between the train/validation and test distributions, there is a notable difference between the validation and the test accuracy. The results in <ref type="table" target="#tab_4">Table 3</ref> show a 0.5% gain in test accuracy for the DLN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2">Adult Benchmark Dataset (Classification)</head><p>We compare accuracy on the benchmark Adult dataset <ref type="bibr" target="#b18">[19]</ref>, where a model predicts whether a person's income is at least $50,000 or not. Following Canini et al. <ref type="bibr" target="#b0">[1]</ref>, we require all models to be monotonically increasing in capital-gain, weekly hours of work and education level, and the gender wage gap. We used one-hot encoding for the other categorical features, for 90 features in total. We randomly split the usual train set <ref type="bibr" target="#b18">[19]</ref> 80-20 and trained over the 80%, and validated over the 20%. Results in <ref type="table" target="#tab_5">Table 4</ref> show the DLN provides better validation and test accuracy than the min-max network or crystals.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3">Rater Score Prediction Case Study (Regression)</head><p>For this real-world Google problem, we train a model to predict a rater score for a candidate result, where each rater score is averaged over 1-5 raters, and takes on 5-25 possible real values. All 10 monotonic features are required to be monotonic. Results in <ref type="table" target="#tab_6">Table 5</ref> show the DLN has very test MSE than the two-layer crystals model, and much better MSE than the other monotonic networks. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.4">Usefulness Case Study (Classifier)</head><p>For this real-world Google problem, we train a model to predict whether a candidate result adds useful information given the presence of another result. All 9 features are required to be monotonic. <ref type="table" target="#tab_7">Table 6</ref> shows the DLN has slightly better validation and test accuracy than crystals, and both are notably better than the min-max network or positive-weight DNN. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7">Conclusions</head><p>In this paper, we proposed combining three types of layers, (1) calibrators, (2) linear embeddings, and (3) multidimensional lattices, to produce a new class of models we call deep lattice networks that combines the flexibility of deep networks with the regularization, interpretability and debuggability advantages that come with being able to impose monotonicity constraints on some inputs.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>n t the subset of x t that are non-monotonic. Linear Embedding Layer: Each linear embedding layer consists of two linear matrices, one matrix W m t ∈ R</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head></head><label></label><figDesc>that linearly embeds non-monotonic inputs x n t , and one bias vector b t . To preserve monotonicity on the embedded vector Wi, j] ≥ 0 for all (i, j).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>x[s i ]), where the s i are mutually disjoint and ψ represents multilinear interpolation weights. Then the output can be expressed in the formθ Tψ (x[∪s i ]). That is, the lattice preserves the functional form of its inputs, changing only the values of the coefficients θ and the linear interpolation weights ψ.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>Figure 1: Left: Example calibrator (1-d lattice) with fixed input range [−10, 10] and five fixed uniformly-spaced keypoints and corresponding discriminatively-trained outputs (look-up table values values). Middle: Example lattice on three inputs in fixed input range [0, 1] 3 , with 8 discriminatively- trained parameters (shown as gray-values), each corresponding to one of the 2 3 vertices of the unit hypercube. The parameters are linearly interpolated for any input [0, 1] 3 to form the lattice function's output. If the parameters are increasing in any direction, then the function is monotonic increasing in that direction. In this example, the gray-value parameters get lighter in all three directions, so the function is monotonic increasing in all three inputs. Right: Three examples of lattice values are shown in italics, each interpolated from the 8 lattice parameters.</figDesc><table>(1,1,1) 

(1,0,1) 

(0,1,1) 

(.7,0,.8) 

(.2,0,.4) 

(.5,5,1) 
(0,0,1) 

1d calibrator 
monotonic 

Monotonic 
inputs 

W 
m ≥ 0 

Non-
monotonic 
inputs 

W 

n 

multi-d lattice 
non-monotonic 

Figure 2: Illustration of a nine-layer DLN: calibrators, linear embedding, calibrators, ensemble of 
lattices, calibrators, ensemble of lattices, calibrators, lattice, calibrator. 

mentation and numerical optimization choices in Section 5. Experimental results demonstrate the 
potential on benchmark and real-world scenarios in Section 6. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 :</head><label>1</label><figDesc>DLN layers and hyperparameters Layer t Parameters Hyperparameters Linear Embedding b t</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 2 :</head><label>2</label><figDesc>Dataset Summary</figDesc><table>Dataset 
Type 
# Features (# Monotonic) # Training # Validation 
# Test 

Adult 
Classify 
90 (4) 
26,065 
6,496 
16,281 
User Intent Classify 
49 (19) 
241,325 
60,412 176,792 
Rater Score Regress 
10 (10) 1,565,468 
195,530 195,748 
Usefulness 
Classify 
9 (9) 
62,220 
7,764 
7,919 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>User Intent Case Study Results</figDesc><table>Validation 
Test # Parameters 
G × S 
Accuracy Accuracy 

DLN 
74.39% 
72.48% 
27,903 30 × 5D 
Crystals 
74.24% 
72.01% 
15,840 80 × 7D 
Min-Max network 
73.89% 
72.02% 
31,500 90 × 7D 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Adult Results</figDesc><table>Validation 
Test # Parameters 
G × S 
Accuracy Accuracy 

DLN 
86.50% 
86.08% 
40,549 70 × 5D 
Crystals 
86.02% 
85.87% 
3,360 60 × 4D 
Min-Max network 
85.28% 
84.63% 
57,330 70 × 9D 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 5 :</head><label>5</label><figDesc>Rater Score Prediction (Monotonic Features Only) Results Validation MSE Test MSE # Parameters G × S</figDesc><table>DLN 
1.2078 
1.2096 
81,601 
50 × 9D 
Crystals 
1.2101 
1.2109 
1,980 
10 × 7D 
Min-Max network 
1.3474 
1.3447 
5,500 
100 × 5D 
Monotonic DNN 
1.3920 
1.3939 
2,341 20 × 100D 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="true"><head>Table 6 :</head><label>6</label><figDesc>Usefulness Results</figDesc><table>Validation 
Test # Parameters 
G × S 
Accuracy Accuracy 

DLN 
66.08% 
65.26% 
81,051 
50 × 9D 
Crystals 
65.45% 
65.13% 
9,920 
80 × 6D 
Min-Max network 
64.62% 
63.65% 
4,200 
70 × 6D 
Monotonic DNN 
64.27% 
62.88% 
2,012 1 × 1000D 

</table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Monotonic calibrated interpolated look-up tables</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pfeifer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Voevodski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Mangylov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Moczydlowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Esbroeck</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">109</biblScope>
			<biblScope unit="page" from="1" to="47" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Lattice regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">K</forename><surname>Garcia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast and flexible monotonic functions with ensembles of lattices</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Canini</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Cotter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Fard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">R</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pfeifer</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Learning monotonic transformations for classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Howard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Jebara</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Submodular functions and convexity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lovász</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Mathematical Programming The State of the Art</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="1983" />
			<biblScope unit="page" from="235" to="257" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title level="m" type="main">Nonparametric estimation under shape constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Groeneboom</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Jongbloed</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<publisher>Cambridge Press</publisher>
			<pubPlace>New York, USA</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Application of the back propagation neural network algorithm with monotonicity constraints for two-group classification problems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">P</forename><surname>Archer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Decision Sciences</title>
		<imprint>
			<biblScope unit="volume">24</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="60" to="75" />
			<date type="published" when="1993" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">A neural network method of density estimation for univariate unimodal data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Computing &amp; Applications</title>
		<imprint>
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="160" to="167" />
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Estimating monotonic functions and their bounds</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">H</forename><surname>Ungar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AIChE Journal</title>
		<imprint>
			<biblScope unit="volume">46</biblScope>
			<biblScope unit="issue">12</biblScope>
			<biblScope unit="page" from="2426" to="2434" />
			<date type="published" when="2000" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Incorporating functional knowledge in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dugas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Bélisle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Nadeau</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Garcia</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal Machine Learning Research</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Comparison of universal approximators incorporating partial monotonicity by structure</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Minin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velikova</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Lang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniels</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural Networks</title>
		<imprint>
			<biblScope unit="volume">23</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="471" to="475" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Monotone and partially monotone neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Daniels</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Velikova</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Trans. Neural Networks</title>
		<imprint>
			<biblScope unit="volume">21</biblScope>
			<biblScope unit="issue">6</biblScope>
			<biblScope unit="page" from="906" to="917" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Monotonic networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sill</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems (NIPS)</title>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Adaptive logic networks. Handbook of Neural Computation, Section C1</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">W</forename><surname>Armstrong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">M</forename><surname>Thomas</surname></persName>
		</author>
		<idno>ISBN 0 7503 0312</idno>
		<imprint>
			<date type="published" when="1996" />
			<publisher>IOP Publishing and Oxford U. Press</publisher>
			<biblScope unit="volume">8</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">TensorFlow: Large-scale machine learning on heterogeneous systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Abadi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agarwal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Barham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Brevdo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Citro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">S</forename><surname>Corrado</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Devin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghemawat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Harp</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Irving</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Isard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Jozefowicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Kaiser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Kudlur</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Levenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Mané</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Monga</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Moore</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Murray</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Steiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Talwar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Tucker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vasudevan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Viégas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Warden</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wattenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wicke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zheng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Software available from tensorflow.org</note>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
		<title level="m" type="main">An empirical distribution function for sampling with incomplete information. The annals of mathematical statistics</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Ayer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">D</forename><surname>Brunk</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">M</forename><surname>Ewing</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">T</forename><surname>William</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Silverman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1955" />
			<biblScope unit="volume">26</biblScope>
			<biblScope unit="page" from="641" to="647" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Distributed optimization and statistical learning via the alternating direction method of multipliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Boyd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Parikh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Chu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Peleato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Eckstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page" from="1" to="122" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">UCI repository of machine learning databases</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">J</forename><surname>Merz</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
