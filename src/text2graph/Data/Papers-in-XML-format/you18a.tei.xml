<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:38+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiaxuan</forename><surname>You</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rex</forename><surname>Ying</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiang</forename><surname>Ren</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">William</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jure</forename><surname>Leskovec</surname></persName>
						</author>
						<title level="a" type="main">GraphRNN: Generating Realistic Graphs with Deep Auto-regressive Models</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Modeling and generating graphs is fundamental for studying networks in biology, engineering, and social sciences. However, modeling complex distributions over graphs and then efficiently sampling from these distributions is challenging due to the non-unique, high-dimensional nature of graphs and the complex, non-local dependencies that exist between edges in a given graph. Here we propose GraphRNN, a deep autoregressive model that addresses the above challenges and approximates any distribution of graphs with minimal assumptions about their structure. GraphRNN learns to generate graphs by training on a representative set of graphs and decomposes the graph generation process into a sequence of node and edge formations, conditioned on the graph structure generated so far. In order to quantitatively evaluate the performance of GraphRNN, we introduce a benchmark suite of datasets, baselines and novel evaluation metrics based on Maximum Mean Discrepancy, which measure distances between sets of graphs. Our experiments show that GraphRNN significantly outperforms all baselines, learning to generate diverse graphs that match the structural characteristics of a target set, while also scaling to graphs 50× larger than previous deep models.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction and Related Work</head><p>Generative models for real-world graphs have important applications in many domains, including modeling physical and social interactions, discovering new chemical and molecular structures, and constructing knowledge graphs. Development of generative graph models has a rich history, and many methods have been proposed that can generate graphs based on a priori structural assumptions <ref type="bibr" target="#b21">(Newman, 2010)</ref>. However, a key open challenge in this area is developing methods that can directly learn generative models from an observed set of graphs. Developing generative models that can learn directly from data is an important step towards improving the fidelity of generated graphs, and paves a way for new kinds of applications, such as discovering new graph structures and completing evolving graphs.</p><p>In contrast, traditional generative models for graphs (e.g., Barabási-Albert model, Kronecker graphs, exponential random graphs, and stochastic block models) <ref type="bibr" target="#b6">(Erdős &amp; Rényi, 1959;</ref><ref type="bibr" target="#b18">Leskovec et al., 2010;</ref><ref type="bibr" target="#b1">Albert &amp; Barabási, 2002;</ref><ref type="bibr" target="#b0">Airoldi et al., 2008;</ref><ref type="bibr" target="#b17">Leskovec et al., 2007;</ref><ref type="bibr" target="#b26">Robins et al., 2007)</ref> are hand-engineered to model a particular family of graphs, and thus do not have the capacity to directly learn the generative model from observed data. For example, the Barabási-Albert model is carefully designed to capture the scale-free nature of empirical degree distributions, but fails to capture many other aspects of real-world graphs, such as community structure.</p><p>Recent advances in deep generative models, such as variational autoencoders (VAE) <ref type="bibr" target="#b15">(Kingma &amp; Welling, 2014)</ref> and generative adversarial networks (GAN) <ref type="bibr" target="#b8">(Goodfellow et al., 2014)</ref>, have made important progress towards generative modeling for complex domains, such as image and text data. Building on these approaches a number of deep learning models for generating graphs have been proposed <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Grover et al., 2017;</ref><ref type="bibr" target="#b29">Simonovsky &amp; Komodakis, 2018;</ref><ref type="bibr" target="#b19">Li et al., 2018)</ref>. For example, Simonovsky &amp; Komodakis 2018 propose a VAE-based approach, while <ref type="bibr" target="#b19">Li et al. 2018</ref> propose a framework based upon graph neural networks. However, these recently proposed deep models are either limited to learning from a single graph <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Grover et al., 2017)</ref> or generating small graphs with 40 or fewer nodes <ref type="bibr" target="#b19">(Li et al., 2018;</ref><ref type="bibr" target="#b29">Simonovsky &amp; Komodakis, 2018)</ref>-limitations that stem from three fundamental challenges in the graph generation problem:</p><p>• Large and variable output spaces: To generate a graph with n nodes the generative model has to output n 2 values to fully specify its structure. Also, the number of nodes n and edges m varies between different graphs and a generative model needs to accommodate such complexity and variability in the output space.</p><p>• Non-unique representations: In the general graph generation problem studied here, we want distributions over possible graph structures without assuming a fixed set of nodes (e.g., to generate candidate molecules of varying sizes). In this general setting, a graph with n nodes can be represented by up to n! equivalent adjacency matrices, each corresponding to a different, arbitrary node ordering/numbering. Such high representation complexity is challenging to model and makes it expensive to compute and then optimize objective functions, like reconstruction error, during training. For example, GraphVAE (Simonovsky &amp; Komodakis, 2018) uses approximate graph matching to address this issue, requiring O(n 4 ) operations in the worst case .</p><p>• Complex dependencies: Edge formation in graphs involves complex structural dependencies. For example, in many real-world graphs two nodes are more likely to be connected if they share common neighbors <ref type="bibr" target="#b21">(Newman, 2010)</ref>. Therefore, edges cannot be modeled as a sequence of independent events, but rather need to be generated jointly, where each next edge depends on the previously generated edges. <ref type="bibr" target="#b19">Li et al. 2018</ref> address this problem using graph neural networks to perform a form of "message passing"; however, while expressive, this approach takes O(mn 2 diam(G)) operations to generate a graph with m edges, n nodes and diameter diam(G).</p><p>Present work. Here we address the above challenges and present Graph Recurrent Neural Networks (GraphRNN), a scalable framework for learning generative models of graphs. GraphRNN models a graph in an autoregressive (or recurrent) manner-as a sequence of additions of new nodes and edges-to capture the complex joint probability of all nodes and edges in the graph. In particular, GraphRNN can be viewed as a hierarchical model, where a graph-level RNN maintains the state of the graph and generates new nodes, while an edge-level RNN generates the edges for each newly generated node. Due to its autoregressive structure, GraphRNN can naturally accommodate variable-sized graphs, and we introduce a breadth-first-search (BFS) nodeordering scheme to drastically improve scalability. This BFS approach alleviates the fact that graphs have non-unique representations-by collapsing distinct representations to unique BFS trees-and the tree-structure induced by BFS allows us to limit the number of edge predictions made for each node during training. Our approach requires O(n 2 ) operations on worst-case (i.e., complete) graphs, but we prove that our BFS ordering scheme permits sub-quadratic complexity in many cases.</p><p>In addition to the novel GraphRNN framework, we also introduce a comprehensive suite of benchmark tasks and baselines for the graph generation problem, with all code made publicly available 1 . A key challenge for the graph generation problem is quantitative evaluation of the quality of generated graphs. Whereas prior studies have mainly relied on visual inspection or first-order moment statistics for evaluation, we provide a comprehensive evaluation setup by comparing graph statistics such as the degree distribution, clustering coefficient distribution and motif counts for two sets of graphs based on variants of the Maximum Mean Discrepancy (MMD) <ref type="bibr" target="#b9">(Gretton et al., 2012)</ref>. This quantitative evaluation approach can compare higher order moments of graph-statistic distributions and provides a more rigorous evaluation than simply comparing mean values.</p><p>Extensive experiments on synthetic and real-world graphs of varying size demonstrate the significant improvement GraphRNN provides over baseline approaches, including the most recent deep graph generative models as well as traditional models. Compared to traditional baselines (e.g., stochastic block models), GraphRNN is able to generate high-quality graphs on all benchmark datasets, while the traditional models are only able to achieve good performance on specific datasets that exhibit special structures. Compared to other state-of-the-art deep graph generative models, GraphRNN is able to achieve superior quantitative performance-in terms of the MMD distance between the generated and test set graphs-while also scaling to graphs that are 50× larger than what these previous approaches can handle. Overall, GraphRNN reduces MMD by 80%-90% over the baselines on average across all datasets and effectively generalizes, achieving comparatively high log-likelihood scores on held-out data.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Proposed Approach</head><p>We first describe the background and notation for building generative models of graphs, and then describe our autoregressive framework, GraphRNN.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Notations and Problem Definition</head><p>An undirected graph 2 G = (V, E) is defined by its node set</p><formula xml:id="formula_0">V = {v 1 , ..., v n } and edge set E = {(v i , v j )|v i , v j ∈ V }.</formula><p>One common way to represent a graph is using an adjacency matrix, which requires a node ordering π that maps nodes to rows/columns of the adjacency matrix. More precisely, π is a permutation function over V (i.e., (π(v 1 ), ..., π(v n )) is a permutation of (v 1 , ..., v n )). We define Π as the set of all n! possible node permutations. Under a node ordering π, a graph G can then be represented by the adjacency matrix A π ∈ R n×n , where</p><formula xml:id="formula_1">A π i,j = 1[(π(v i ), π(v j )) ∈ E].</formula><p>1 The code is available in https://github.com/ snap-stanford/GraphRNN, the appendix is available in https://arxiv.org/abs/1802.08773.</p><p>2 We focus on undirected graphs. Extensions to directed graphs and graphs with features are discussed in the Appendix.</p><p>Note that elements in the set of adjacency matrices A Π = {A π |π ∈ Π} all correspond to the same underlying graph.</p><p>The goal of learning generative models of graphs is to learn a distribution p model (G) over graphs, based on a set of observed graphs G = {G 1 , ..., G s } sampled from data distribution p(G), where each graph G i may have a different number of nodes and edges. When representing G ∈ G, we further assume that we may observe any node ordering π with equal probability, i.e., p(π) = 1 n! , ∀π ∈ Π. Thus, the generative model needs to be capable of generating graphs where each graph could have exponentially many representations, which is distinct from previous generative models for images, text, and time series.</p><p>Finally, note that traditional graph generative models (surveyed in the introduction) usually assume a single input training graph. Our approach is more general and can be applied to a single as well as multiple input training graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">A Brief Survey of Possible Approaches</head><p>We start by surveying some general alternative approaches for modeling p(G), in order to highlight the limitations of existing non-autoregressive approaches and motivate our proposed autoregressive architecture.</p><p>Vector-representation based models. One naïve approach would be to represent G by flattening A π into a vector in R n 2 , which is then used as input to any off-the-shelf generative model, such as a VAE or GAN. However, this approach suffers from serious drawbacks: it cannot naturally generalize to graphs of varying size, and requires training on all possible node permutations or specifying a canonical permutation, both of which require O(n!) time in general.</p><p>Node-embedding based models. There have been recent successes in encoding a graph's structural properties into node embeddings <ref type="bibr" target="#b11">(Hamilton et al., 2017)</ref>, and one approach to graph generation could be to define a generative model that decodes edge probabilities based on pairwise relationships between learned node embeddings (as in <ref type="bibr" target="#b16">Kipf &amp; Welling 2016)</ref>. However, this approach is only well-defined when given a fixed-set of nodes, limiting its utility for the general graph generation problem, and approaches based on this idea are limited to learning from a single input graph <ref type="bibr" target="#b16">(Kipf &amp; Welling, 2016;</ref><ref type="bibr" target="#b10">Grover et al., 2017)</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">GraphRNN: Deep Generative Models for Graphs</head><p>The key idea of our approach is to represent graphs under different node orderings as sequences, and then to build an autoregressive generative model on these sequences. As we will show, this approach does not suffer from the drawbacks common to other general approaches (c.f., Section 2.2), allowing us to model graphs of varying size with complex edge dependencies, and we introduce a BFS node ordering scheme to drastically reduce the complexity of learning over all possible node sequences (Section 2.3.4). In this autoregressive framework, the model complexity is greatly reduced by weight sharing with recurrent neural networks (RNNs). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates our GraphRNN approach, where the main idea is that we decompose graph generation into a process that generates a sequence of nodes (via a graph-level RNN), and another process that then generates a sequence of edges for each newly added node (via an edge-level RNN).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.1.">MODELING GRAPHS AS SEQUENCES</head><p>We first define a mapping f S from graphs to sequences, where for a graph G ∼ p(G) with n nodes under node ordering π, we have</p><formula xml:id="formula_2">S π = f S (G, π) = (S π 1 , ..., S π n ),<label>(1)</label></formula><p>where each element S π i ∈ {0, 1} i−1 , i ∈ {1, ..., n} is an adjacency vector representing the edges between node π(v i ) and the previous nodes π(v j ), j ∈ {1, ..., i − 1} already in the graph:</p><formula xml:id="formula_3">3 S π i = (A π 1,i , ..., A π i−1,i ) T , ∀i ∈ {2, ..., n}.<label>(2)</label></formula><p>For undirected graphs, S π determines a unique graph G, and we write the mapping as</p><formula xml:id="formula_4">f G (·) where f G (S π ) = G.</formula><p>Thus, instead of learning p(G), whose sample space cannot be easily characterized, we sample the auxiliary π to get the observations of S π and learn p(S π ), which can be modeled autoregressively due to the sequential nature of S π . At inference time, we can sample G without explicitly computing p(G) by sampling S π , which maps to G via f G .</p><p>Given the above definitions, we can write p(G) as the marginal distribution of the joint distribution p(G, S π ):</p><formula xml:id="formula_5">p(G) = S π p(S π ) 1[f G (S π ) = G],<label>(3)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Algorithm 1 GraphRNN inference algorithm</head><p>Input: RNN-based transition module f trans , output module f out , probability distribution P θi parameterized by θ i , start token SOS, end token EOS, empty graph state h Output:</p><formula xml:id="formula_6">Graph sequence S π S π 1 = SOS, h 1 = h , i = 1 repeat i = i + 1 h i = f trans (h i−1 , S π i−1 ) {update graph state} θ i = f out (h i ) S π i ∼ P θi {sample node i's edge connections} until S π i is EOS Return S π = (S π 1 , ..., S π i )</formula><p>where p(S π ) is the distribution that we want to learn using a generative model. Due to the sequential nature of S π , we further decompose p(S π ) as the product of conditional distributions over the elements:</p><formula xml:id="formula_7">p(S π ) = n+1 i=1 p(S π i |S π 1 , ..., S π i−1 )<label>(4)</label></formula><p>where we set S π n+1 as the end of sequence token EOS, to represent sequences with variable lengths. We simplify</p><formula xml:id="formula_8">p(S π i |S π 1 , ..., S π i−1 ) as p(S π i |S π &lt;i ) in further discussions.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.2.">THE GRAPHRNN FRAMEWORK</head><p>So far we have transformed the modeling of p(G) to modeling p(S π ), which we further decomposed into the product of conditional probabilities p(S</p><formula xml:id="formula_9">π i |S π &lt;i ). Note that p(S π i |S π &lt;i )</formula><p>is highly complex as it has to capture how node π(v i ) links to previous nodes based on how previous nodes are interconnected among each other. Here we propose to parameterize p(S π i |S π &lt;i ) using expressive neural networks to model the complex distribution. To achieve scalable modeling, we let the neural networks share weights across all time steps i. In particular, we use an RNN that consists of a state-transition function and an output function:</p><formula xml:id="formula_10">h i = f trans (h i−1 , S π i−1 ),<label>(5)</label></formula><formula xml:id="formula_11">θ i = f out (h i ),<label>(6)</label></formula><p>where h i ∈ R d is a vector that encodes the state of the graph generated so far, S π i−1 is the adjacency vector for the most recently generated node i − 1, and θ i specifies the distribution of next node's adjacency vector (i.e., S π i ∼ P θi ). In general, f trans and f out can be arbitrary neural networks, and P θi can be an arbitrary distribution over binary vectors. This general framework is summarized in Algorithm 1.</p><p>Note that the proposed problem formulation is fully general; we discuss and present some specific variants with implementation details in the next section. Note also that RNNs require fixed-size input vectors, while we previously defined S π i as having varying dimensions depending on i; we describe an efficient and flexible scheme to address this issue in Section 2.3.4.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.3.">GRAPHRNN VARIANTS</head><p>Different variants of the GraphRNN model correspond to different assumptions about p(S π i |S π &lt;i ). Recall that each dimension of S π i is a binary value that models existence of an edge between the new node π(v i ) and a previous node π(v j ), j ∈ {1, ..., i − 1}. We propose two variants of GraphRNN, both of which implement the transition function f trans (i.e., the graph-level RNN) as a Gated Recurrent Unit (GRU) <ref type="bibr" target="#b3">(Chung et al., 2014)</ref> but differ in the implementation of f out (i.e., the edge-level model). Both variants are trained using stochastic gradient descent with a maximum likelihood loss over S π -i.e., we optimize the parameters of the neural networks to optimize p model (S π ) over all observed graph sequences.</p><p>Multivariate Bernoulli. First we present a simple baseline variant of our GraphRNN approach, which we term GraphRNN-S ("S" for "simplified"). In this variant, we model p(S π i |S π &lt;i ) as a multivariate Bernoulli distribution, parameterized by the θ i ∈ R i−1 vector that is output by f out . In particular, we implement f out as single layer multilayer perceptron (MLP) with sigmoid activation function, that shares weights across all time steps. The output of f out is a vector θ i , whose element θ i [j] can be interpreted as a probability of edge (i, j). We then sample edges in S </p><formula xml:id="formula_12">p(S π i |S π &lt;i ) = i−1 j=1 p(S π i,j |S π i,&lt;j , S π &lt;i ),<label>(7)</label></formula><p>where S π i,j denotes a binary scalar that is 1 if node π(v i+1 ) is connected to node π(v j ) (under ordering π). In this variant, each distribution in the product is approximated by an another RNN. Conceptually, we have a hierarchical RNN, where the first (i.e., the graph-level) RNN generates the nodes and maintains the state of the graph, while the second (i.e., the edge-level) RNN generates the edges of a given node (as illustrated in <ref type="figure" target="#fig_0">Figure 1</ref>). In our implementation, the edge-level RNN is a GRU model, where the hidden state is initialized via the graph-level hidden state h i and where the output at each step is mapped by a MLP to a scalar indicating the probability of having an edge. S π i,j is sampled from this distribution specified by the jth output of the ith edge-level RNN, and is fed into the j +1th input of the same RNN. All edge-level RNNs share the same parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.4.">TRACTABILITY VIA BREADTH-FIRST SEARCH</head><p>A crucial insight in our approach is that rather than learning to generate graphs under any possible node permutation, we learn to generate graphs using breadth-first-search (BFS) node orderings, without a loss of generality. Formally, we modify Equation <ref type="formula" target="#formula_2">(1)</ref> to</p><formula xml:id="formula_13">S π = f S (G, BFS(G, π)),<label>(8)</label></formula><p>where BFS(·) denotes the deterministic BFS function. In particular, this BFS function takes a random permutation π as input, picks π(v 1 ) as the starting node and appends the neighbors of a node into the BFS queue in the order defined by π. Note that the BFS function is many-to-one, i.e., multiple permutations can map to the same ordering after applying the BFS function.</p><p>Using BFS to specify the node ordering during generation has two essential benefits. The first is that we only need to train on all possible BFS orderings, rather than all possible node permutations, i.e., multiple node permutations map to the same BFS ordering, providing a reduction in the overall number of sequences we need to consider. <ref type="bibr">4</ref> The second is that the BFS ordering makes learning easier by reducing the number of edge predictions we need to make in the edgelevel RNN; in particular, when we are adding a new node under a BFS ordering, the only possible edges for this new node are those connecting to nodes that are in the "frontier" of the BFS (i.e., nodes that are still in the BFS queue)-a notion formalized by Proposition 1 (proof in the Appendix): Proposition 1. Suppose v 1 , . . . , v n is a BFS ordering of n nodes in graph G, and</p><formula xml:id="formula_14">(v i , v j−1 ) ∈ E but (v i , v j ) ∈ E for some i &lt; j ≤ n, then (v i , v j ) ∈ E, ∀1 ≤ i ≤ i and j ≤ j &lt; n.</formula><p>Importantly, this insight allows us to redefine the variable size S π i vector as a fixed M -dimensional vector, representing the connectivity between node π(v i ) and nodes in the current BFS queue with maximum size M :</p><formula xml:id="formula_15">S π i = (A π max(1,i−M ),i , ..., A π i−1,i ) T , i ∈ {2, ..., n}. (9)</formula><p>As a consequence of Proposition 1, we can bound M as follows: Corollary 1. With a BFS ordering the maximum number of entries that GraphRNN model needs to predict for</p><formula xml:id="formula_16">S π i , ∀1 ≤ i ≤ n is O max diam(G) d=1 |{v i |dist(v i , v 1 ) = d}| ,</formula><p>where dist denotes the shortest-path-distance between vertices.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>The overall time complexity of GraphRNN is thus O(M n).</head><p>In practice, we estimate an empirical upper bound for M (see the Appendix for details).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">GraphRNN Model Capacity</head><p>In this section we analyze the representational capacity of GraphRNN, illustrating how it is able to capture complex edge dependencies. In particular, we discuss two very different cases on how GraphRNN can learn to generate graphs with a global community structure as well as graphs with a very regular geometric structure. For simplicity, we assume that h i (the hidden state of the graph-level RNN) can exactly encode S π &lt;i , and that the edge-level RNN can encode S π i,&lt;j . That is, we assume that our RNNs can maintain memory of the decisions they make and elucidate the models capacity in this ideal case. We similarly rely on the universal approximation theorem of neural networks <ref type="bibr" target="#b13">(Hornik, 1991)</ref>.</p><p>Graphs with community structure. GraphRNN can model structures that are specified by a given probabilistic model. This is because the posterior of a new edge probability can be expressed as a function of the outcomes of previous nodes. For instance, suppose that the training set contains graphs generated from the following distribution p com (G): half of the nodes are in community A, and half of the nodes are in community B (in expectation), and nodes are connected with probability p s within each community and probability p d between communities. Given such a model, we have the following key (inductive) observation: (by Bayes' rule and p com 's definition, with an analogous result for p(π(v i ) ∈ B)). Finally, GraphRNN can handle the base case of the induction in Observation 1, i.e., S i,1 , simply by sampling according to 0.5p s + 0.5p d at the first step of the edge-level RNN (i.e., 0.5 probability i is in same community as node π(v 1 )).</p><p>Graphs with regular structure. GraphRNN can also naturally learn to generate regular structures, due to its ability to learn functions that only activate for S π i,j where v j has specific degree. For example, suppose that the training set consists of ladder graphs <ref type="bibr" target="#b22">(Noy &amp; Ribó, 2004)</ref>. To generate a ladder graph, the edge-level RNN must handle three key cases: if </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We compare GraphRNN to state-of-the-art baselines, demonstrating its robustness and ability to generate highquality graphs in diverse settings.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We perform experiments on both synthetic and real datasets, with drastically varying sizes and characteristics. The sizes of graphs vary from |V | = 10 to |V | = 2025.</p><p>Community. 500 two-community graphs with 60 ≤ |V | ≤ 160. Each community is generated by the Erdős-Rényi model (E-R) <ref type="bibr" target="#b6">(Erdős &amp; Rényi, 1959)</ref> with n = |V |/2 nodes and p = 0.3. We then add 0.05|V | inter-community edges with uniform probability.</p><p>Grid. 100 standard 2D grid graphs with 100 ≤ |V | ≤ 400. We also run our models on 100 standard 2D grid graphs with 1296 ≤ |V | ≤ 2025, and achieve comparable results.</p><p>B-A. 500 graphs with 100 ≤ |V | ≤ 200 that are generated using the Barabási-Albert model. During generation, each node is connected to 4 existing nodes.</p><p>Protein. 918 protein graphs <ref type="bibr" target="#b5">(Dobson &amp; Doig, 2003)</ref> with 100 ≤ |V | ≤ 500. Each protein is represented by a graph, where nodes are amino acids and two nodes are connected if they are less than 6 Angstroms apart.</p><p>Ego. 757 3-hop ego networks extracted from the Citeseer network <ref type="bibr" target="#b28">(Sen et al., 2008)</ref> with 50 ≤ |V | ≤ 399. Nodes represent documents and edges represent citation relationships.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Experimental Setup</head><p>We compare the performance of our model against various traditional generative models for graphs, as well as some recent deep graph generative models.</p><p>Traditional baselines. Following <ref type="bibr" target="#b19">Li et al. 2018</ref> we compare against the Erdős-Rényi model (E-R) <ref type="bibr" target="#b6">(Erdős &amp; Rényi, 1959)</ref> and the Barabási-Albert (B-A) model <ref type="bibr" target="#b1">(Albert &amp; Barabási, 2002)</ref>. In addition, we compare against popular generative models that include learnable parameters: Kronecker graph models <ref type="bibr" target="#b18">(Leskovec et al., 2010)</ref> and mixed-membership stochastic block models (MMSB) <ref type="bibr" target="#b0">(Airoldi et al., 2008)</ref>.</p><p>Deep learning baselines. We compare against the recent methods of <ref type="bibr">Simonovsky &amp; Komodakis 2018 (GraphVAE)</ref> and <ref type="bibr">Li et al. 2018 (DeepGMG)</ref>. We provide reference implementations for these methods (which do not currently have associated public code), and we adapt GraphVAE to our problem setting by using one-hot indicator vectors as node features for the graph convolutional network encoder.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>5</head><p>Experiment settings. We use 80% of the graphs in each dataset for training and test on the rest. We set the hyperparameters for baseline methods based on recommendations made in their respective papers. The hyperparameter settings for GraphRNN were fixed after development tests on data that was not used in follow-up evaluations (further details in the Appendix). Note that all the traditional methods are only designed to learn from a single graph, therefore we train a separate model for each training graph in order to compare with these methods. In addition, both deep learning baselines suffer from aforementioned scalability issues, so we only compare to these baselines on a small version of the community dataset with 12 ≤ |V | ≤ 20 (Community-small) and 200 ego graphs with 4 ≤ |V | ≤ 18 (Ego-small).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Evaluating the Generated Graphs</head><p>Evaluating the sample quality of generative models is a challenging task in general <ref type="bibr" target="#b30">(Theis et al., 2016)</ref>, and in our case, this evaluation requires a comparison between two sets of graphs (the generated graphs and the test sets). Whereas previous works relied on qualitative visual inspection (Simonovsky &amp; Komodakis, 2018) or simple comparisons of average statistics between the two sets <ref type="bibr" target="#b18">(Leskovec et al., 2010)</ref>, we propose novel evaluation metrics that compare all moments of their empirical distributions.</p><p>Our proposed metrics are based on Maximum Mean Discrepancy (MMD) measures. Suppose that a unit ball in a reproducing kernel Hilbert space (RKHS) H is used as its function class F, and k is the associated kernel, the squared MMD between two sets of samples from distributions p and q can be derived as <ref type="bibr" target="#b9">(Gretton et al., 2012</ref>)</p><formula xml:id="formula_17">MMD 2 (p||q) = E x,y∼p [k(x, y)] + E x,y∼q [k(x, y)] − 2E x∼p,y∼q [k(x, y)].<label>(10)</label></formula><p>Proper distance metrics over graphs are in general computationally intractable <ref type="bibr" target="#b20">(Lin, 1994)</ref>. Thus, we compute MMD using a set of graph statistics M = {M 1 , ..., M k }, where each M i (G) is a univariate distribution over R, such as the degree distribution or clustering coefficient distribution. We then use the first Wasserstein distance as an efficient distance metric between two distributions p and q:</p><formula xml:id="formula_18">W (p, q) = inf γ∈Π(p,q) E (x,y)∼γ [||x − y||],<label>(11)</label></formula><p>where Π(p, q) is the set of all distributions whose marginals <ref type="bibr">5</ref> We also attempted using degree and clustering coefficients as features for nodes, but did not achieve better performance.   In experiments, we show this derived MMD score for degree and clustering coefficient distributions, as well as average orbit counts statistics, i.e., the number of occurrences of all orbits with 4 nodes (to capture higher-level motifs) <ref type="bibr" target="#b12">(Hočevar &amp; Demšar, 2014)</ref>. We use the RBF kernel to compute distances between count vectors.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Generating High Quality Graphs</head><p>Our experiments demonstrate that GraphRNN can generate graphs that match the characteristics of the ground truth graphs in a variety of metrics.</p><p>Graph visualization. <ref type="figure" target="#fig_4">Figure 2</ref> visualizes the graphs generated by GraphRNN and various baselines, showing that GraphRNN can capture the structure of datasets with vastly differing characteristics-being able to effectively learn regular structures like grids as well as more natural structures like ego networks. Specifically, we found that grids generated by GraphRNN do not appear in the training set, i.e., it learns to generalize to unseen grid widths/heights.</p><p>Evaluation with graph statistics. We use three graph statistics-based on degrees, clustering coefficients and orbit counts-to further quantitatively evaluate the generated graphs. <ref type="figure" target="#fig_6">Figure 3</ref> shows the average graph statistics in the test vs. generated graphs, which demonstrates that even from hundreds of graphs with diverse sizes, GraphRNN can still learn to capture the underlying graph statistics very well, with the generated average statistics closely matching the overall test set distribution. <ref type="table" target="#tab_0">Tables 1 and 2</ref> summarize MMD evaluations on the full datasets and small versions, respectively. Note that we train all the models with a fixed number of steps, and report the test set performance at the step with the lowest training error. <ref type="bibr">6</ref> GraphRNN variants achieve the best performance on all datasets, with 80% decrease of MMD on average compared with traditional baselines, and 90% decrease of MMD compared with deep learning baselines. Interestingly, on the protein dataset, our simpler GraphRNN-S model performs very well, which is likely due to the fact that the protein dataset is a nearest neighbor graph over Euclidean space and thus does not involve highly complex edge dependencies. Note that even though some baseline models perform well on specific datasets (e.g., MMSB on the community dataset), they fail to generalize across other types of input graphs.  Generalization ability. <ref type="table" target="#tab_1">Table 2</ref> also shows negative loglikelihoods (NLLs) on the training and test sets. We report the average p(S π ) in our model, and report the likelihood in baseline methods as defined in their papers. A model with good generalization ability should have small NLL gap between training and test graphs. We found that our model can generalize well, with 22% smaller average NLL gap. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Robustness</head><p>Finally, we also investigate the robustness of our model by interpolating between Barabási-Albert (B-A) and Erdős-Rényi (E-R) graphs. We randomly perturb [0%, 20%, ..., 100%] edges of B-A graphs with 100 nodes. With 0% edges perturbed, the graphs are E-R graphs; with 100% edges perturbed, the graphs are B-A graphs. <ref type="figure" target="#fig_8">Figure  4</ref> shows the MMD scores for degree and clustering coefficient distributions for the 6 sets of graphs. Both B-A and E-R perform well when graphs are generated from their respective distributions, but their performance degrades significantly once noise is introduced. In contrast, GraphRNN maintains strong performance as we interpolate between these structures, indicating high robustness and versatility.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Further Related Work</head><p>In addition to the deep graph generative approaches and traditional graph generation approaches surveyed previously, our framework also builds off a variety of other methods.</p><p>Molecule and parse-tree generation. There has been related domain-specific work on generating candidate <ref type="bibr">7</ref> The average likelihood is ill-defined for the traditional models. molecules and parse trees in natural language processing. Most previous work on discovering molecule structures make use of a expert-crafted sequence representations of molecular graph structures (SMILES) <ref type="bibr" target="#b23">(Olivecrona et al., 2017;</ref><ref type="bibr" target="#b27">Segler et al., 2017;</ref><ref type="bibr" target="#b7">Gómez-Bombarelli et al., 2016)</ref>. Most recently, SD-VAE <ref type="bibr" target="#b4">(Dai et al., 2018</ref>) introduced a grammar-based approach to generate structured data, including molecules and parse trees. In contrast to these works, we consider the fully general graph generation setting without assuming features or special structures of graphs.</p><p>Deep autoregressive models. Deep autoregressive models decompose joint probability distributions as a product of conditionals, a general idea that has achieved striking successes in the image <ref type="bibr" target="#b25">(Oord et al., 2016b)</ref> and audio <ref type="bibr" target="#b24">(Oord et al., 2016a)</ref> domains. Our approach extends these successes to the domain of generating graphs. Note that the DeepGMG algorithm <ref type="bibr" target="#b19">(Li et al., 2018)</ref> and the related prior work of Johnson 2017 can also be viewed as deep autoregressive models of graphs. However, unlike these methods, we focus on providing a scalable (i.e., O(n 2 )) algorithm that can generate general graphs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusion and Future Work</head><p>We proposed GraphRNN, an autoregressive generative model for graph-structured data, along with a comprehensive evaluation suite for the graph generation problem, which we used to show that GraphRNN achieves significantly better performance compared to previous state-ofthe-art models, while being scalable and robust to noise. However, significant challenges remain in this space, such as scaling to even larger graphs and developing models that are capable of doing efficient conditional graph generation.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. GraphRNN at inference time. Green arrows denote the graph-level RNN that encodes the "graph state" vector hi in its hidden state, updated by the predicted adjacency vector S π i for node π(vi). Blue arrows represent the edge-level RNN, whose hidden state is initialized by the graph-level RNN, that is used to predict the adjacency vector S π i for node π(vi).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>according to a multivariate Bernoulli distribution parametrized by θ i . Dependent Bernoulli sequence. To fully capture complex edge dependencies, in the full GraphRNN model we further decompose p(S π i |S π &lt;i ) into a product of conditionals,</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Observation 1 .</head><label>1</label><figDesc>Assume there exists a parameter setting for GraphRNN such that it can generate S π &lt;i and S π i,&lt;j accord- ing to the distribution over S π implied by p com (G), then there also exists a parameter setting for GraphRNN such that it can output paccording to p com (G). This observation follows from three facts: First, we know that pcan be expressed as a function of p s , p d , and p(π(v j ) ∈ A), p(π(v j ) ∈ B) ∀1 ≤ j ≤ i (which holds by p com 's definition). Second, by our earlier assumptions on the RNN memory, S π &lt;i can be encoded into the initial state of the edge-level RNN, and the edge-level RNN can also encode the outcomes of S π i,&lt;j . Third, we know that p(π(v i ) ∈ A) is computable from S</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>j = 0, then the new node should only connect to the degree 1 node or else any degree 2 node; if&lt;j . The appendix contains visual illustrations and further discussions on this example.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Visualization of graphs from grid dataset (Left group), community dataset (Middle group) and Ego dataset (Right group). Within each group, graphs from training set (First row), graphs generated by GraphRNN(Second row) and graphs generated by Kronecker, MMSB and B-A baselines respectively (Third row) are shown. Different visualization layouts are used for different datasets.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>and q respectively, and γ is a valid transport plan. To capture high-order moments, we use the following ker- nel, whose Taylor expansion is a linear combination of all moments (proof in the Appendix): Proposition 2. The kernel function defined by k W (p, q) = exp W (p,q) 2σ 2 induces a unique RKHS.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Average degree (Left) and clustering coefficient (Right) distributions of graphs from test set and graphs generated by GraphRNN and baseline models.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_8"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. MMD performance of different approaches on degree (Left) and clustering coefficient (Right) under different noise level.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Comparison of GraphRNN to traditional graph generative models using MMD. (max(|V |), max(|E|)) of each dataset is shown.</figDesc><table>Community (160,1945) 
Ego (399,1071) 
Grid (361,684) 
Protein (500,1575) 

Deg. 
Clus. Orbit Deg. 
Clus. Orbit Deg. 
Clus. Orbit 
Deg. 
Clus. Orbit 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>GraphRNN compared to state-of-the-art deep graph generative models on small graph datasets using MMD and negative log-likelihood (NLL). (max(|V |), max(|E|)) of each dataset is shown. (DeepVAE and GraphVAE cannot scale to the graphs in Table 1.)Degree Clustering Orbit Train NLL Test NLL Degree Clustering Orbit Train NLL Test NLL</figDesc><table>Community-small (20,83) 
Ego-small (18,69) 

GraphVAE 
0.35 
0.98 
0.54 
13.55 
25.48 
0.13 
0.17 
0.05 
12.45 
14.28 
DeepGMG 
0.22 
0.95 
0.40 
106.09 
112.19 
0.04 
0.10 
0.02 
21.17 
22.40 
GraphRNN-S 0.02 
0.15 
0.01 
31.24 
35.94 
0.002 
0.05 
0.0009 8.51 
9.88 
GraphRNN 
0.03 
0.03 
0.01 
28.95 
35.10 
0.0003 0.05 
0.0009 9.05 
10.61 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="3">We prohibit self-loops and S π 1 is defined as an empty vector.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="4">In the worst case (e.g., star graphs), the number of BFS orderings is n!, but we observe substantial reductions on many realworld graphs.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="6">Using the training set or a validation set to evaluate MMD gave analogous results, so we used the train set for early stopping.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>The authors thank Ethan Steinberg, Bowen Liu, Marinka Zitnik and Srijan Kumar for their helpful discussions and comments on the paper. This research has been supported in part by DARPA SIMPLEX, ARO MURI, Stanford Data Science Initiative, Huawei, JD, and Chan Zuckerberg Biohub. W.L.H. was also supported by the SAP Stanford Graduate Fellowship and an NSERC PGS-D grant.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Mixed membership stochastic blockmodels</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Airoldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Blei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fienberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Xing</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2008" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Statistical mechanics of complex networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Albert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Barabási</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Reviews of Modern Physics</title>
		<imprint>
			<biblScope unit="volume">74</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">47</biblScope>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Finding matches in a haystack: A max-pooling strategy for graph matching in the presence of outliers</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Duchenne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ponce</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Empirical evaluation of gated recurrent neural networks on sequence modeling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Chung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gulcehre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Cho</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Workshop on Deep Learning</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Syntaxdirected variational autoencoder for structured data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Skiena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Distinguishing enzyme structures from non-enzymes without alignments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dobson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Doig</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Molecular Biology</title>
		<imprint>
			<biblScope unit="volume">330</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="771" to="783" />
			<date type="published" when="2003" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">On random graphs I</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Erdős</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rényi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Publicationes Mathematicae (Debrecen)</title>
		<imprint>
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="290" to="297" />
			<date type="published" when="1959" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Automatic chemical design using a data-driven continuous representation of molecules</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Gómez-Bombarelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Duvenaud</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Hernández-Lobato</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Sánchez-Lengeling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Sheberla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Aguilera-Iparraguirre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">D</forename><surname>Hirzel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">P</forename><surname>Adams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Aspuru-Guzik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pouget-Abadie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Warde-Farley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ozair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Courville</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">A kernel two-sample test</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gretton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">M</forename><surname>Borgwardt</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Rasch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2012" />
			<publisher>JMLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Graphite: Iterative generative modeling of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Grover</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zweig</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ermon</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Representation learning on graphs: Methods and applications</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">L</forename><surname>Hamilton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ying</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Data Engineering Bulletin</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">A combinatorial approach to graphlet counting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Hočevar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Demšar</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Bioinformatics</title>
		<imprint>
			<biblScope unit="volume">30</biblScope>
			<biblScope unit="issue">4</biblScope>
			<biblScope unit="page" from="559" to="565" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Approximation capabilities of multilayer feedforward networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Hornik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Neural networks</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="251" to="257" />
			<date type="published" when="1991" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning graphical state transitions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">D</forename><surname>Johnson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Auto-encoding variational bayes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Variational graph auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">N</forename><surname>Kipf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Welling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Bayesian Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Graph evolution: Densification and shrinking diameters</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">TKDD</title>
		<imprint>
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="issue">1</biblScope>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Kronecker graphs: An approach to modeling networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Leskovec</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chakrabarti</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kleinberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Faloutsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Ghahramani</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">JMRL</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Learning deep generative models of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Dyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=Hy1d-ebAb" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Hardness of approximating graph transformation problem</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Lin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Symposium on Algorithms and Computation</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Networks: an introduction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Newman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
			<publisher>Oxford university press</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Recursively constructible families of graphs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Noy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ribó</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Advances in Applied Mathematics</title>
		<imprint>
			<biblScope unit="volume">32</biblScope>
			<biblScope unit="issue">1-2</biblScope>
			<biblScope unit="page" from="350" to="363" />
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Molecular de novo design through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Olivecrona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Blaschke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Engkvist</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chen</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Cheminformatics</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page">48</biblScope>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Dieleman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Senior</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Wavenet</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.03499</idno>
		<title level="m">A generative model for raw audio</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Pixel recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Kalchbrenner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Kavukcuoglu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">An introduction to exponential random graph (p*) models for social networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Robins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Pattison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Kalish</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lusher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Social Networks</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="173" to="191" />
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Generating focussed molecule libraries for drug discovery with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Segler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Kogej</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Tyrchan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Waller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">ACS Central Science</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Collective classification in network data</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Namata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bilgic</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Getoor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Galligher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Eliassi-Rad</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AI Magazine</title>
		<imprint>
			<biblScope unit="volume">29</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">93</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">GraphVAE: Towards generation of small graphs using variational autoencoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simonovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Komodakis</surname></persName>
		</author>
		<ptr target="https://openreview.net/forum?id=SJlhPMWAW" />
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">A note on the evaluation of generative models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Theis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Oord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bethge</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
