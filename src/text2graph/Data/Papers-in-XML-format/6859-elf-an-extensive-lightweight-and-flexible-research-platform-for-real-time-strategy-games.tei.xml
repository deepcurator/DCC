<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-05T11:40+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
							<email>1yuandong@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2 Oculus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qucheng</forename><surname>Gong</surname></persName>
							<email>qucheng@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2 Oculus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wenling</forename><surname>Shang</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2 Oculus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
							<email>yuxinwu@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2 Oculus</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">Lawrence</forename><surname>Zitnick</surname></persName>
							<email>zitnick@fb.com</email>
							<affiliation key="aff0">
								<orgName type="department">Facebook AI Research 2 Oculus</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">ELF: An Extensive, Lightweight and Flexible Research Platform for Real-time Strategy Games</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>In this paper, we propose ELF, an Extensive, Lightweight and Flexible platform for fundamental reinforcement learning research. Using ELF, we implement a highly customizable real-time strategy (RTS) engine with three game environments (Mini-RTS, Capture the Flag and Tower Defense). Mini-RTS, as a miniature version of StarCraft, captures key game dynamics and runs at 40K frameper-second (FPS) per core on a laptop. When coupled with modern reinforcement learning methods, the system can train a full-game bot against built-in AIs endto-end in one day with 6 CPUs and 1 GPU. In addition, our platform is flexible in terms of environment-agent communication topologies, choices of RL methods, changes in game parameters, and can host existing C/C++-based game environments like ALE <ref type="bibr" target="#b3">[4]</ref>. Using ELF, we thoroughly explore training parameters and show that a network with Leaky ReLU [17] and Batch Normalization [11] coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in the full game of Mini-RTS. Strong performance is also achieved on the other two games. In game replays, we show our agents learn interesting strategies. ELF, along with its RL platform, is open sourced at https://github.com/facebookresearch/ELF.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Game environments are commonly used for research in Reinforcement Learning (RL), i.e. how to train intelligent agents to behave properly from sparse rewards <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b28">29]</ref>. Compared to the real world, game environments offer an infinite amount of highly controllable, fully reproducible, and automatically labeled data. Ideally, a game environment for fundamental RL research is:</p><p>• Extensive: The environment should capture many diverse aspects of the real world, such as rich dynamics, partial information, delayed/long-term rewards, concurrent actions with different granularity, etc. Having an extensive set of features and properties increases the potential for trained agents to generalize to diverse real-world scenarios.</p><p>• Lightweight: A platform should be fast and capable of generating samples hundreds or thousands of times faster than real-time with minimal computational resources (e.g., a single machine). Lightweight and efficient platforms help accelerate academic research of RL algorithms, particularly for methods which are heavily data-dependent.</p><p>• Flexible: A platform that is easily customizable at different levels, including rich choices of environment content, easy manipulation of game parameters, accessibility of internal variables, and flexibility of training architectures. All are important for fast exploration of different algorithms. For example, changing environment parameters <ref type="bibr" target="#b33">[35]</ref>, as well as using internal data <ref type="bibr" target="#b14">[15,</ref><ref type="bibr" target="#b18">19]</ref> have been shown to substantially accelerate training.</p><p>To our knowledge, no current game platforms satisfy all criteria. Modern commercial games (e.g., StarCraft I/II, GTA V) are extremely realistic, but are not customizable and require significant resources for complex visual effects and for computational costs related to platform-shifting (e.g., a virtual machine to host Windows-only SC I on Linux). Old games and their wrappers <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b4">5,</ref><ref type="bibr" target="#b13">14]</ref>) are substantially faster, but are less realistic with limited customizability. On the other hand, games designed for research purpose (e.g., MazeBase <ref type="bibr" target="#b28">[29]</ref>, µRTS <ref type="bibr" target="#b22">[23]</ref>) are efficient and highly customizable, but are not very extensive in their capabilities. Furthermore, none of the environments consider simulation concurrency, and thus have limited flexibility when different training architectures are applied. For instance, the interplay between RL methods and environments during training is often limited to providing simplistic interfaces (e.g., one interface for one game) in scripting languages like Python.</p><p>In this paper, we propose ELF, a research-oriented platform that offers games with diverse properties, efficient simulation, and highly customizable environment settings. The platform allows for both game parameter changes and new game additions. The training of RL methods is deeply and flexibly integrated into the environment, with an emphasis on concurrent simulations. On ELF, we build a real-time strategy (RTS) game engine that includes three initial environments including Mini-RTS, Capture the Flag and Tower Defense. Mini-RTS is a miniature custom-made RTS game that captures all the basic dynamics of StarCraft (fog-of-war, resource gathering, troop building, defense/attack with troops, etc). Mini-RTS runs at 165K FPS on a 4 core laptop, which is faster than existing environments by an order of magnitude. This enables us for the first time to train end-toend a full-game bot against built-in AIs. Moreover, training is accomplished in only one day using 6 CPUs and 1 GPU. The other two games can be trained with similar (or higher) efficiency.</p><p>Many real-world scenarios and complex games (e.g. StarCraft) are hierarchical in nature. Our RTS engine has full access to the game data and has a built-in hierarchical command system, which allows training at any level of the command hierarchy. As we demonstrate, this allows us to train a full-game bot that acts on the top-level strategy in the hierarchy while lower-level commands are handled using build-in tactics. Previously, most research on RTS games focused only on lower-level scenarios such as tactical battles <ref type="bibr" target="#b32">[34,</ref><ref type="bibr" target="#b24">25]</ref>. The full access to the game data also allows for supervised training with small-scale internal data.</p><p>ELF is resilient to changes in the topology of the environment-actor communication used for training, thanks to its hybrid C++/Python framework. These include one-to-one, many-to-one and oneto-many mappings. In contrast, existing environments (e.g., OpenAI Gym <ref type="bibr" target="#b5">[6]</ref> and Universe [33]) wrap one game in one Python interface, which makes it cumbersome to change topologies. Parallelism is implemented in C++, which is essential for simulation acceleration. Finally, ELF is capable of hosting any existing game written in C/C++, including Atari games (e.g., ALE <ref type="bibr" target="#b3">[4]</ref>), board games (e.g. Chess and Go <ref type="bibr" target="#b31">[32]</ref>), physics engines (e.g., Bullet <ref type="bibr" target="#b9">[10]</ref>), etc, by writing a simple adaptor.</p><p>Equipped with a flexible RL backend powered by PyTorch, we experiment with numerous baselines, and highlight effective techniques used in training. We show the first demonstration of end-toend trained AIs for real-time strategy games with partial information. We use the Asynchronous Advantagous Actor-Critic (A3C) model <ref type="bibr" target="#b20">[21]</ref> and explore extensive design choices including frameskip, temporal horizon, network structure, curriculum training, etc. We show that a network with Leaky ReLU <ref type="bibr" target="#b16">[17]</ref> and Batch Normalization <ref type="bibr" target="#b10">[11]</ref> coupled with long-horizon training and progressive curriculum beats the rule-based built-in AI more than 70% of the time in full-game Mini-RTS. We also show stronger performance in others games. ELF and its RL platform, is open-sourced at https://github.com/facebookresearch/ELF.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Architecture</head><p>ELF follows a canonical and simple producer-consumer paradigm <ref type="figure" target="#fig_0">(Fig. 1)</ref>. The producer plays N games, each in a single C++ thread. When a batch of M current game states are ready (M &lt; N ), the corresponding games are blocked and the batch are sent to the Python side via the daemon. The consumers (e.g., actor, optimizer, etc) get batched experience with history information via a Python/C++ interface and send back the replies to the blocked batch of the games, which are waiting for the next action and/or values, so that they can proceed. For simplicity, the producer and consumers are in the same process. However, they can also live in different processes, or even on different machines. Before the training (or evaluation) starts, different consumers register themselves for batches with different history length. For example, an actor might need a batch with short history, while an optimizer (e.g., T -step actor-critic) needs a batch with longer history. During training, the consumers use the batch in various ways. For example, the actor takes the batch and returns the probabilties of actions (and values), then the actions are sampled from the distribution and sent back. The batch received by the optimizer already contains the sampled actions from the previous steps, and can be used to drive reinforcement learning algorithms such as A3C. Here is a sample usage of ELF: </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Main loop of ELF</head><p>Parallelism using C++ threads. Modern reinforcement learning methods often require heavy parallelism to obtain diverse experiences <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b21">22]</ref>. Most existing RL environments (OpenAI Gym <ref type="bibr" target="#b5">[6]</ref> and Universe [33], RLE <ref type="bibr" target="#b4">[5]</ref>, Atari <ref type="bibr" target="#b3">[4]</ref>, Doom <ref type="bibr" target="#b13">[14]</ref>) provide Python interfaces which wrap only single game instances. As a result, parallelism needs to be built in Python when applying modern RL methods. However, thread-level parallelism in Python can only poorly utilize multi-core processors, due to the Global Interpreter Lock (GIL) <ref type="bibr" target="#b0">1</ref> . Process-level parallelism will also introduce extra data exchange overhead between processes and increase complexity to framework design. In contrast, our parallelism is achieved with C++ threads for better scaling on multi-core CPUs.</p><p>Flexible Environment-Model Configurations. In ELF, one or multiple consumers can be used. Each consumer knows the game environment identities of samples from received batches, and typically contains one neural network model. The models of different consumers may or may not share parameters, might update the weights, might reside in different processes or even on different machines. This architecture offers flexibility for switching topologies between game environments and models. We can assign one model to each game environment, or one-to-one (e.g, vanilla A3C <ref type="bibr" target="#b20">[21]</ref>), in which each agent follows and updates its own copy of the model. Similarly, multiple environments can be assigned to a single model, or many-to-one (e.g., BatchA3C <ref type="bibr" target="#b33">[35]</ref> or GA3C <ref type="bibr" target="#b0">[1]</ref>), where the model can perform batched forward prediction to better utilize GPUs. We have also incorporated forward-planning methods (e.g., Monte-Carlo Tree Search (MCTS) <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b31">32,</ref><ref type="bibr" target="#b26">27]</ref>) and Self-Play <ref type="bibr" target="#b26">[27]</ref>, in which a single environment might emit multiple states processed by multiple models, or one-tomany. Using ELF, these training configurations can be tested with minimal changes.</p><p>Highly customizable and unified interface. Games implemented with our RTS engine can be trained using raw pixel data or lower-dimensional internal game data. Using internal game data is In the current repository (https://github.com/ facebookresearch/ELF, master branch), there are board games (e.g., Go <ref type="bibr" target="#b31">[32]</ref>), Atari learning environment <ref type="bibr" target="#b3">[4]</ref>, and a customized RTS engine that contains three simple games. typically more convenient for research focusing on reasoning tasks rather than perceptual ones. Note that web-based visual renderings is also supported (e.g., <ref type="figure" target="#fig_2">Fig. 3(a)</ref>) for case-by-case debugging.</p><p>ELF allows for a unified interface capable of hosting any existing game written in C/C++, including Atari games (e.g., ALE <ref type="bibr" target="#b3">[4]</ref>), board games (e.g. Go <ref type="bibr" target="#b31">[32]</ref>), and a customized RTS engine, with a simple adaptor <ref type="figure" target="#fig_1">(Fig. 2)</ref>. This enables easy multi-threaded training and evaluation using existing RL methods. Besides, we also provide three concrete simple games based on RTS engine (Sec. 3).</p><p>Reinforcement Learning backend. We propose a Python-based RL backend. It has a flexible design that decouples RL methods from models. Multiple baseline methods (e.g., A3C <ref type="bibr" target="#b20">[21]</ref>, Policy Gradient <ref type="bibr" target="#b29">[30]</ref>, Q-learning <ref type="bibr" target="#b19">[20]</ref>, Trust Region Policy Optimization <ref type="bibr" target="#b25">[26]</ref>, etc) are implemented, mostly with very few lines of Python codes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3">Real-time strategy Games</head><p>Real-time strategy (RTS) games are considered to be one of the next grand AI challenges after Chess and Go <ref type="bibr" target="#b26">[27]</ref>. In RTS games, players commonly gather resources, build units (facilities, troops, etc), and explore the environment in the fog-of-war (i.e., regions outside the sight of units are invisible) to invade/defend the enemy, until one player wins. RTS games are known for their exponential and changing action space (e.g., 5 10 possible actions for 10 units with 5 choices each, and units of each player can be built/destroyed when game advances), subtle game situations, incomplete information due to limited sight and long-delayed rewards. Typically professional players take 200-300 actions per minute, and the game lasts for 20-30 minutes.</p><p>Very few existing RTS engines can be used directly for research. Commercial RTS games (e.g., StarCraft I/II) have sophisticated dynamics, interactions and graphics. The game play strategies have been long proven to be complex. Moreover, they are close-source with unknown internal states, and cannot be easily utilized for research. Open-source RTS games like Spring <ref type="bibr" target="#b11">[12]</ref>, OpenRA <ref type="bibr" target="#b23">[24]</ref> and Warzone 2100 <ref type="bibr" target="#b27">[28]</ref> focus on complex graphics and effects, convenient user interface, stable network play, flexible map editors and plug-and-play mods (i.e., game extensions). Most of them use rule-based AIs, do not intend to run faster than real-time, and offer no straightforward interface with modern machine learning architectures. ORTS <ref type="bibr" target="#b7">[8]</ref>, BattleCode <ref type="bibr" target="#b1">[2]</ref> and RoboCup Simulation League <ref type="bibr" target="#b15">[16]</ref> are designed for coding competitions and focused on rule-based AIs. Research-oriented platforms (e.g., µRTS <ref type="bibr" target="#b22">[23]</ref>, MazeBase <ref type="bibr" target="#b28">[29]</ref>  <ref type="table">Table 2</ref>: Frame rate comparison. Note that Mini-RTS does not render frames, but save game information into a C structure which is used in Python without copying. For DeepMind Lab, FPS is 287 (CPU) and 866 (GPU) on single 6CPU+1GPU machine. Other numbers are in 1CPU core.</p><p>but often with much simpler dynamics than RTS games. Recently, TorchCraft <ref type="bibr" target="#b30">[31]</ref> provides APIs for StarCraft I to access its internal game states. However, due to platform incompatibility, one docker is used to host one StarCraft engine, and is resource-consuming. Tbl. 1 summarizes the difference.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Our approach</head><p>Many popular RTS games and its variants (e.g., StarCraft, DoTA, Leagues of Legends, Tower Defense) share the same structure: a few units are controlled by a player, to move, attack, gather or cast special spells, to influence their own or an enemy's army. With our command hierarchy, a new game can be created by changing (1) available commands (2) available units, and (3) how each unit emits commands triggered by certain scenarios. For this, we offer simple yet effective tools. Researchers can change these variables either by adding commands in C++, or by writing game scripts (e.g., Lua). All derived games share the mechanism of hierarchical commands, replay, etc. Rule-based AIs can also be extended similarly. We provide the following three games: Mini-RTS, Capture the Flag and Tower Defense <ref type="figure" target="#fig_2">(Fig. 3(b)</ref>). These games share the following properties:</p><p>Gameplay. Units in each game move with real coordinates, have dimensions and collision checks, and perform durative actions. The RTS engine is tick-driven. At each tick, AIs make decisions by sending commands to units based on observed information. Then commands are executed, the game's state changes, and the game continues. Despite a fair complicated game mechanism, Mini-RTS is able to run 40K frames-per-second per core on a laptop, an order of magnitude faster than most existing environments. Therefore, bots can be trained in a day on a single machine.</p><p>Built-in hierarchical command levels. An agent could issue strategic commands (e.g., more aggressive expansion), tactical commands (e.g., hit and run), or micro-command (e.g., move a particular unit backward to avoid damage). Ideally strong agents master all levels; in practice, they may focus on a certain level of command hierarchy, and leave others to be covered by hard-coded rules. For this, our RTS engine uses a hierarchical command system that offers different levels of controls over the game. A high-level command may affect all units, by issuing low-level commands. A low-level, unit-specific durative command lasts a few ticks until completion during which per-tick immediate commands are issued.</p><p>Built-in rule-based AIs. We have designed rule-based AIs along with the environment. These AIs have access to all the information of the map and follow fixed strategies (e.g., build 5 tanks and attack the opponent base). These AIs act by sending high-level commands which are then translated to low-level ones and then executed.</p><p>With ELF, for the first time, we are able to train full-game bots for real-time strategy games and achieve stronger performance than built-in rule-based AIs. In contrast, existing RTS AIs are either rule-based or focused on tactics (e.g., 5 units vs. 5 units). We run experiments on the three games to justify the usability of our platform. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiments 4.1 Benchmarking ELF</head><p>We run ELF on a single server with a different number of CPU cores to test the efficiency of parallelism. <ref type="figure">Fig. 4(a)</ref> shows the results when running Mini-RTS. We can see that ELF scales well with the number of CPU cores used to run the environments. We also embed Atari emulator <ref type="bibr" target="#b3">[4]</ref> into our platform and check the speed difference between a single-threaded ALE and paralleled ALE per core <ref type="figure">(Fig. 4(b)</ref>). While a single-threaded engine gives around 5.8K FPS on Pong, our paralleled ALE runs comparable speed (5.1K FPS per core) with up to 16 cores, while OpenAI Gym (with Python threads) runs 3x slower (1.7K FPS per core) with 16 cores 1024 threads, and degrades with more cores. Number of threads matters for training since they determine how diverse the experiences could be, with the same number of CPUs. Apart from this, we observed that Python multiprocessing with Gym is even slower, due to heavy communication of game frames among processes. Note that we used no hyperthreading for all experiments.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Baselines on Real-time Strategy Games</head><p>We focus on 1-vs-1 full games between trained AIs and built-in AIs. Built-in AIs have access to full information (e.g., number of opponent's tanks), while trained AIs know partial information in the fog of war, i.e., game environment within the sight of its own units. There are exceptions: in Mini-RTS, the location of the opponent's base is known so that the trained AI can attack; in Capture the Flag, the flag location is known to all; Tower Defense is a game of complete information.</p><p>Details of Built-in AI. For Mini-RTS there are two rule-based AIs: SIMPLE gathers, builds five tanks and then attacks the opponent base. HIT N RUN often harasses, builds and attacks. For Capture the Flag, we have one built-in AI. For Tower Defense (TD), no AI is needed. We tested our built-in AIs against a human player and find they are strong in combat but exploitable. For example, SIMPLE is vulnerable to hit-and-run style harass. As a result, a human player has a win rate of 90% and 50% against SIMPLE and HIT N RUN, respectively, in 20 games.</p><p>Action Space. For simplicity, we use 9 strategic (and thus global) actions with hard-coded execution details. For example, AI may issue BUILD BARRACKS, which automatically picks a worker to build barracks at an empty location, if the player can afford. Although this setting is simple, detailed commands (e.g., command per unit) can be easily set up, which bear more resemblance to StarCraft. Similar setting applies to Capture the Flag and Tower Defense. Please check Appendix for detailed descriptions.</p><p>Rewards. For Mini-RTS, the agent only receives a reward when the game ends (±1 for win/loss). An average game of Mini-RTS lasts for around 4000 ticks, which results in 80 decisions for a frame skip of 50, showing that the game is indeed delayed in reward. For Capturing the Flag, we give intermediate rewards when the flag moves towards player's own base (one score when the flag "touches down"). In Tower Defense, intermediate penalty is given if enemy units are leaked.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.1">A3C baseline</head><p>Next, we describe our baselines and their variants. Note that while we refer to these as baseline, we are the first to demonstrate end-to-end trained AIs for real-time strategy (RTS) games with partial information. For all games, we randomize the initial game states for more diverse experience and   <ref type="table">Table 4</ref>: Win rate in % of A3C models using different network architectures. Frame skip of both sides are 50 ticks. The fact that the medians are better than the means shows that different instances of A3C could converge to very different solutions.</p><p>use A3C <ref type="bibr" target="#b20">[21]</ref> to train AIs to play the full game. We run all experiments 5 times and report mean and standard deviation. We use simple convolutional networks with two heads, one for actions and the other for values. The input features are composed of spatially structured (20-by-20) abstractions of the current game environment with multiple channels. At each (rounded) 2D location, the type and hit point of the unit at that location is quantized and written to their corresponding channels. For Mini-RTS, we also add an additional constant channel filled with current resource of the player. The input feature only contains the units within the sight of one player, respecting the properties of fog-of-war. For Capture the Flag, immediate action is required at specific situations (e.g., when the opponent just gets the flag) and A3C does not give good performance. Therefore we use frame skip 10 for trained AI and 50 for the opponent to give trained AI a bit advantage. All models are trained from scratch with curriculum training (Sec. 4.2.2).</p><p>Note that there are several factors affecting the AI performance.</p><p>Frame-skip. A frame skip of 50 means that the AI acts every 50 ticks, etc. Against an opponent with low frame skip (fast-acting), A3C's performance is generally lower <ref type="figure" target="#fig_2">(Fig. 3)</ref>. When the opponent has high frame skip (e.g., 50 ticks), the trained agent is able to find a strategy that exploits the longdelayed nature of the opponent. For example, in Mini-RTS it will send two tanks to the opponent's base. When one tank is destroyed, the opponent does not attack the other tank until the next 50-divisible tick comes. Interestingly, the trained model could be adaptive to different frame-rates and learn to develop different strategies for faster acting opponents. For Capture the Flag, the trained bot learns to win 60% over built-in AI, with an advantage in frame skip. For even frame skip, trained AI performance is low.</p><p>Network Architectures. Since the input is sparse and heterogeneous, we experiment on CNN architectures with Batch Normalization <ref type="bibr" target="#b10">[11]</ref> and Leaky ReLU <ref type="bibr" target="#b17">[18]</ref>. BatchNorm stabilizes the gradient flow by normalizing the outputs of each filter. Leaky ReLU preserves the signal of negative linear responses, which is important in scenarios when the input features are sparse. Tbl. 4 shows that these two modifications both improve and stabilize the performance. Furthermore, they are complimentary to each other when combined.</p><p>History length. History length T affects the convergence speed, as well as the final performance of A3C <ref type="figure">(Fig. 5)</ref>. While Vanilla A3C <ref type="bibr" target="#b20">[21]</ref> uses T = 5 for Atari games, the reward in Mini-RTS is more delayed (∼ 80 actions before a reward). In this case, the T -step estimation of reward</p><formula xml:id="formula_0">R 1 = T t=1 γ t−1 r t + γ T V (s T ) used in A3C</formula><p>does not yield a good estimation of the true reward if V (s T ) is inaccurate, in particular for small T . For other experiments we use T = 6.</p><p>Interesting behaviors The trained AI learns to act promptly and use sophisticated strategies ( </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.2">Curriculum Training</head><p>We find that curriculum training plays an important role in training AIs. All AIs shown in Tbl. 3 and Tbl. 4 are trained with curriculum training. For Mini-RTS, we let the built-in AI play the first k ticks, where k ∼ Uniform(0, 1000), then switch to the AI to be trained. This (1) reduces the difficulty of the game initially and (2) gives diverse situations for training to avoid local minima. During training, the aid of the built-in AIs is gradually reduced until no aid is given. All reported win rates are obtained by running the trained agents alone with greedy policy.</p><p>We list the comparison with and without curriculum training in Tbl. 6. It is clear that the performance improves with curriculum training. Similarly, when fine-tuning models pre-trained with one type of opponent towards a mixture of opponents (e.g., 50%SIMPLE + 50%HIT N RUN), curriculum training is critical for better performance (Tbl. 5). Tbl. 5 shows that AIs trained with one built-in AI cannot do very well against another built-in AI in the same game. This demonstrates that training with diverse agents is important for training AIs with low-exploitability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.3">Monte-Carlo Tree Search</head><p>Monte-Carlo Tree Search (MCTS) can be used for planning when complete information about the game is known. This includes the complete state s without fog-of-war, and the precise forward model s = s (s, a). Rooted at the current game state, MCTS builds a game tree that is biased towards paths with high win rate. Leaves are expanded with all candidate moves and the win rate estimation is computed by random self-play until the game ends. We use 8 threads, each with 100 rollouts. We use root parallelization <ref type="bibr" target="#b8">[9]</ref> in which each thread independently expands a tree, and are combined to get the most visited action. As shown in Tbl. 7, MCTS achieves a comparable win rate to models trained with RL. Note that the win rates of the two methods are not directly comparable, since RL methods have no knowledge of game dynamics, and its state knowledge is reduced by the limits introduced by the fog-of-war. Also, MCTS runs much slower (2-3sec per move) than the trained RL AI (≤ 1msec per move).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusion and Future Work</head><p>In this paper, we propose ELF, a research-oriented platform for concurrent game simulation which offers an extensive set of game play options, a lightweight game simulator, and a flexible environment. Based on ELF, we build a RTS game engine and three initial environments (Mini-RTS, Capture the Flag and Tower Defense) that run 40KFPS per core on a laptop. As a result, a fullgame bot in these games can be trained end-to-end in one day using a single machine. In addition to the platform, we provide throughput benchmarks of ELF, and extensive baseline results using state-of-the-art RL methods (e.g, A3C <ref type="bibr" target="#b20">[21]</ref>) on Mini-RTS and show interesting learnt behaviors.</p><p>ELF opens up many possibilities for future research. With this lightweight and flexible platform, RL methods on RTS games can be explored in an efficient way, including forward modeling, hierarchical RL, planning under uncertainty, RL with complicated action space, and so on. Furthermore, the exploration can be done with an affordable amount of resources. As future work, we will continue improving the platform and build a library of maps and bots to compete with.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: Overview of ELF.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Hierarchical layout of ELF. In the current repository (https://github.com/ facebookresearch/ELF, master branch), there are board games (e.g., Go [32]), Atari learning environment [4], and a customized RTS engine that contains three simple games.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Overview of Real-time strategy engine. (a) Visualization of current game state. (b) The three different game environments and their descriptions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Win rate in Mini-RTS with respect to the amount of experience at different steps T in A3C. Note that one sample (with history) in T = 2 is equivalent to two samples in T = 1. Longer T shows superior performance to small step counterparts, even if their samples are more expensive.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>) are fast and simple, often coming with various baselines,</figDesc><table>Realistic Code Resource Rule AIs Data AIs RL backend 
StarCraft I/II 
High 
No 
High 
Yes 
No 
No 
TorchCraft 
High 
Yes 
High 
Yes 
Yes 
No 
ORTS, BattleCode 
Mid 
Yes 
Low 
Yes 
No 
No 
µRTS, MazeBase 
Low 
Yes 
Low 
Yes 
Yes 
No 
Mini-RTS 
Mid 
Yes 
Low 
Yes 
Yes 
Yes 
Table 1: Comparison between different RTS engines. 

Platform 
ALE [4] 
RLE [5] 
Universe [33] 
Malmo [13] 
Frame per second 
6000 
530 
60 
120 
Platform 
DeepMind Lab [3] VizDoom [14] 
TorchCraft [31] 
Mini-RTS 
Frame per second 
287(C)/866(G) 
∼ 7,000 
2,000 (frameskip=50) 
40,000 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>KFPS per CPU core for Mini-RTS</figDesc><table>70 

60 

50 

40 

30 

20 

10 

0 

1 core 
2 cores 
4 cores 
8 cores 
16 cores 

64 threads 
128 threads 
256 threads 
512 threads 1024 threads 

6 

5 

4 

3 

2 

1 

0 

KFPS per CPU core for Pong (Atari) 

64 threads 
128 threads 
256 threads 
512 threads 
1024 threads 

1 core 
2 cores 
4 cores 
8 cores 
16 cores 

OpenAI Gym 

ELF 

Figure 4: Frame-per-second per CPU core (no hyper-threading) with respect to CPUs/threads. ELF 
(light-shaded) is 3x faster than OpenAI Gym [6] (dark-shaded) with 1024 threads. CPU involved in 
testing: Intel E5-2680@2.50GHz. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Win rate of A3C models competing with built-in AIs over 10k games. Left: Mini-RTS. Frame skip of the trained AI is 50. Right: For Capture the Flag, frame skip of trained AI is 10, while the opponent is 50. For Tower Defense the frame skip of trained AI is 50, no opponent AI.</figDesc><table>Game 
Mini-RTS SIMPLE 
Mini-RTS HIT N RUN 
Median Mean (± std) Median Mean (± std) 
ReLU 
52.8 
54.7 (± 4.2) 
60.4 
57.0 (± 6.8) 
Leaky ReLU 
59.8 
61.0 (± 2.6) 
60.2 
60.3 (± 3.3) 
BN 
61.0 
64.4 (± 7.4 ) 
55.6 
57.5 (± 6.8) 
Leaky ReLU + BN 
72.2 
68.4 (± 4.3) 
65.5 
63.6 (± 7.9) 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head></head><label></label><figDesc>: Training with a specific/combined AIs. Frame skip of both sides is 50. When against combined AIs (50%SIMPLE + 50%HIT N RUN), curriculum training is particularly important.Mini-RTS SIMPLE Mini-RTS HIT N RUN Capture the Flag: Win rate of A3C models with and without curriculum training. Mini-RTS: Frame skip of both sides are 50 ticks. Capture the Flag: Frame skip of trained AI is 10, while the opponent is 50. The standard deviation of win rates are large due to instability of A3C training. For example in Capture the Flag, highest win rate reaches 70% while lowest win rate is only 27%.: Win rate using MCTS over 1000 games. Both players use a frameskip of 50.</figDesc><table>Game 
Mini-RTS 
SIMPLE 
HIT N RUN 
Combined 
SIMPLE 
68.4 (±4.3) 
26.6(±7.6) 
47.5(±5.1) 
HIT N RUN 
34.6(±13.1) 63.6 (±7.9) 49.1(±10.5) 
Combined(No curriculum) 49.4(±10.0) 46.0(±15.3) 47.7(±11.0) 
Combined 
51.8(±10.6) 54.7(±11.2) 53.2(±8.5) 
Table 5Game 

no curriculum training 
66.0(±2.4) 
54.4(±15.9) 
54.2(±20.0) 
with curriculum training 
68.4 (±4.3) 
63.6 (±7.9) 
59.9 (±7.4) 
Table 6Game 
Mini-RTS SIMPLE Mini-RTS HIT N RUN 
Random 
24.2(±3.9) 
25.9(±0.6) 
MCTS 
73.2(±0.6) 
62.7(±2.0) 
Table 7</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">The GIL in Python forbids simultaneous interpretations of multiple statements even on multi-core CPUs.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Reinforcement learning through asynchronous advantage actor</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mohammad</forename><surname>Babaeizadeh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Iuri</forename><surname>Frosio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Tyree</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jason</forename><surname>Clemons</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jan</forename><surname>Kautz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">mit&apos;s ai programming competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battlecode</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Battlecode</surname></persName>
		</author>
		<ptr target="https://www.battlecode.org/.2000.URLhttps://www.battlecode.org/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><forename type="middle">Z</forename><surname>Leibo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Denis</forename><surname>Teplyashin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tom</forename><surname>Ward</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marcus</forename><surname>Wainwright</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Heinrich</forename><surname>Küttler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><surname>Lefrancq</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Green</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Víctor</forename><surname>Valdés</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Amir</forename><surname>Sadik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Schrittwieser</surname></persName>
		</author>
		<idno>abs/1612.03801</idno>
		<ptr target="http://arxiv.org/abs/1612.03801" />
		<editor>Demis Hassabis, Shane Legg, and Stig Petersen. Deepmind lab. CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
			<pubPlace>Keith Anderson, Sarah York, Max Cant, Adam Cain, Adrian Bolton, Stephen Gaffney, Helen King</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">The arcade learning environment: An evaluation platform for general agents. CoRR, abs/1207</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yavar</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Naddaf</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Bowling</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1207.4708" />
		<imprint>
			<date type="published" when="2012" />
			<biblScope unit="volume">4708</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Playing SNES in the retro learning environment. CoRR, abs/1611.02205</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nadav</forename><surname>Bhonker</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shai</forename><surname>Rozenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Itay</forename><surname>Hubara</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1611.02205" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Greg</forename><surname>Brockman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vicki</forename><surname>Cheung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ludwig</forename><surname>Pettersson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jonas</forename><surname>Schneider</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<idno>abs/1606.01540</idno>
		<ptr target="http://arxiv.org/abs/1606.01540" />
		<editor>Jie Tang, and Wojciech Zaremba. Openai gym. CoRR</editor>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A survey of monte carlo tree search methods</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Cameron</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Edward</forename><surname>Browne</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Daniel</forename><surname>Powley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Whitehouse</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Simon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lucas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Peter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Cowling</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stephen</forename><surname>Rohlfshagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Diego</forename><surname>Tavener</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Spyridon</forename><surname>Perez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Simon</forename><surname>Samothrakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Colton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Computational Intelligence and AI in games</title>
		<imprint>
			<biblScope unit="volume">4</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="43" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">On the development of a free rts game engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michael</forename><surname>Buro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothy</forename><surname>Furtak</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">GameOnNA Conference</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="23" to="27" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Parallel monte-carlo tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mj-B</forename><surname>Guillaume</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Chaslot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">M</forename><surname>Mark</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Winands</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Jaap Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Herik</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computers and Games</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="60" to="71" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Bullet physics engine</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Erwin</forename><surname>Coumans</surname></persName>
		</author>
		<ptr target="http://bulletphysics.org" />
	</analytic>
	<monogr>
		<title level="j">Open Source Software</title>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift. ICML</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Christian</forename><surname>Szegedy</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stefan</forename><surname>Johansson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Robin</forename><forename type="middle">Westberg</forename><surname>Spring</surname></persName>
		</author>
		<ptr target="https://springrts.com/.2008.URLhttps://springrts.com/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">The malmo platform for artificial intelligence experimentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Katja</forename><surname>Hofmann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Hutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Bignell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International joint conference on artificial intelligence (IJCAI)</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page">4246</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Vizdoom: A doom-based ai research platform for visual reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michał</forename><surname>Kempka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marek</forename><surname>Wydmuch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Grzegorz</forename><surname>Runc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jakub</forename><surname>Toczek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Wojciech</forename><surname>Jaśkowski</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1605.02097</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Playing fps games with deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guillaume</forename><surname>Lample</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Devendra</forename><surname>Singh Chaplot</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1609.05521</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title/>
		<ptr target="https://en.wikipedia.org/wiki/robocupsimulationleague.1995.URLhttps" />
	</analytic>
	<monogr>
		<title level="j">RoboCup Simulation League. Robocup simulation</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. ICML</title>
		<meeting>ICML</meeting>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="volume">30</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Rectifier nonlinearities improve neural network acoustic models</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Maas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Awni</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew Y</forename><surname>Hannun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ng</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learning to navigate in complex environments</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Piotr</forename><surname>Mirowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Razvan</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Fabio</forename><surname>Viola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hubert</forename><surname>Soyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrew</forename><forename type="middle">J</forename><surname>Ballard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrea</forename><surname>Banino</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Misha</forename><surname>Denil</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ross</forename><surname>Goroshin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Dharshan</forename><surname>Kumaran</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Raia</forename><surname>Hadsell</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Human-level control through deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andrei</forename><forename type="middle">A</forename><surname>Rusu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Joel</forename><surname>Veness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Marc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Bellemare</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Martin</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><forename type="middle">K</forename><surname>Riedmiller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Georg</forename><surname>Fidjeland</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ostrovski</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">518</biblScope>
			<biblScope unit="issue">7540</biblScope>
			<biblScope unit="page" from="529" to="533" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adria</forename><forename type="middle">Puigdomenech</forename><surname>Badia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mehdi</forename><surname>Mirza</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Timothy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tim</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Harley</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Kavukcuoglu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1602.01783</idno>
		<title level="m">Asynchronous methods for deep reinforcement learning</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">Massively parallel methods for deep reinforcement learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arun</forename><surname>Nair</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Praveen</forename><surname>Srinivasan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sam</forename><surname>Blackwell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Cagdas</forename><surname>Alcicek</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rory</forename><surname>Fearon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alessandro</forename><forename type="middle">De</forename><surname>Maria</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Vedavyas</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Mustafa</forename><surname>Suleyman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Charles</forename><surname>Beattie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Stig</forename><surname>Petersen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Shane</forename><surname>Legg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Volodymyr</forename><surname>Mnih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Koray</forename><surname>Kavukcuoglu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<idno>abs/1507.04296</idno>
		<ptr target="http://arxiv.org/abs/1507.04296" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">The combinatorial multi-armed bandit problem and its application to realtime strategy games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Santiago</forename><surname>Ontanón</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</title>
		<meeting>the Ninth AAAI Conference on Artificial Intelligence and Interactive Digital Entertainment</meeting>
		<imprint>
			<publisher>AAAI Press</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="58" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Openra</surname></persName>
		</author>
		<ptr target="http://www.openra.net/.2007.URLhttp://www.openra.net/" />
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Multiagent bidirectionally-coordinated nets for learning to play starcraft combat games. CoRR, abs/1703.10069</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Peng</forename><surname>Peng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Quan</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ying</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yaodong</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhenkun</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Haitao</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jun</forename><surname>Wang</surname></persName>
		</author>
		<ptr target="http://arxiv.org/abs/1703.10069" />
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Trust region policy optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">John</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sergey</forename><surname>Levine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Pieter</forename><surname>Abbeel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Philipp</forename><surname>Michael I Jordan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Moritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1889" to="1897" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Mastering the game of go with deep neural networks and tree search</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><surname>Silver</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aja</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Chris</forename><forename type="middle">J</forename><surname>Maddison</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Guez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Laurent</forename><surname>Sifre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">George</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Julian</forename><surname>Driessche</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Ioannis</forename><surname>Schrittwieser</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Veda</forename><surname>Antonoglou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Marc</forename><surname>Panneershelvam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Lanctot</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Nature</title>
		<imprint>
			<biblScope unit="volume">529</biblScope>
			<biblScope unit="issue">7587</biblScope>
			<biblScope unit="page" from="484" to="489" />
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title/>
		<ptr target="https://wz2100.net/.1999.URLhttps://wz2100.net/" />
	</analytic>
	<monogr>
		<title level="j">Pumpkin Studios. Warzone</title>
		<imprint>
			<biblScope unit="volume">2100</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Mazebase: A sandbox for learning from games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Sainbayar</forename><surname>Sukhbaatar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Arthur</forename><surname>Szlam</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rob</forename><surname>Fergus</surname></persName>
		</author>
		<idno>abs/1511.07401</idno>
		<ptr target="http://arxiv.org/abs/1511.07401" />
		<imprint>
			<date type="published" when="2015" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Policy gradient methods for reinforcement learning with function approximation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Richard</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">A</forename><surname>Sutton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mcallester</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Satinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yishay</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Mansour</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1999" />
			<biblScope unit="volume">99</biblScope>
			<biblScope unit="page" from="1057" to="1063" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
		<title level="m" type="main">Torchcraft: a library for machine learning research on real-time strategy games</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nantas</forename><surname>Nardelli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Alex</forename><surname>Auvolat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Timothée</forename><surname>Lacroix</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Florian</forename><surname>Richoux</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<idno>abs/1611.00625</idno>
		<ptr target="http://arxiv.org/abs/1611.00625" />
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Better computer go player with neural network and long-term prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yan</forename><surname>Zhu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.06410</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<monogr>
		<title level="m" type="main">Episodic exploration for deep deterministic policies: An application to starcraft micromanagement tasks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Nicolas</forename><surname>Usunier</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Gabriel</forename><surname>Synnaeve</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zeming</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soumith</forename><surname>Chintala</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Training agent for first-person shooter game with actor-critic curriculum learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuxin</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yuandong</forename><surname>Tian</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
