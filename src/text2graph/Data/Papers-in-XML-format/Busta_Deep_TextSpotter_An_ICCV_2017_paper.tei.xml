<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:28+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Michal</forename><surname>Bušta</surname></persName>
							<email>bustam@fel.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics</orgName>
								<orgName type="laboratory">Centre for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Lukáš</forename><surname>Neumann</surname></persName>
							<email>neumalu1@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics</orgName>
								<orgName type="laboratory">Centre for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Jiří</forename><surname>Matas</surname></persName>
							<email>matas@cmp.felk.cvut.cz</email>
							<affiliation key="aff0">
								<orgName type="department">Department of Cybernetics</orgName>
								<orgName type="laboratory">Centre for Machine Perception</orgName>
								<orgName type="institution">Czech Technical University</orgName>
								<address>
									<settlement>Prague</settlement>
									<country key="CZ">Czech Republic</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Deep TextSpotter: An End-to-End Trainable Scene Text Localization and Recognition Framework</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>A method for scene text localization and recognition is proposed. The novelties include: training of both text detection and recognition in a single end-to-end pass, the structure of the recognition CNN and the geometry of its input layer that preserves the aspect of the text and adapts its resolution to the data. The proposed method achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets</head> -ICDAR 2013 and ICDAR 2015, whilst  <p>being an order of magnitude faster than competing methods -the whole pipeline runs at 10 frames per second on an NVidia K80 GPU.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Scene text localization and recognition, a.k.a. text spotting, text-in-the-wild problem or photo OCR, in an open problem with many practical applications, ranging from tools for helping visually impaired or text translation, to use as a part of a larger integrated system, e.g. in robotics, indoor navigation or autonomous driving.</p><p>Like many areas of computer vision, the scene text field has greatly benefited from deep learning techniques and accuracy of methods has significantly improved <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b5">6]</ref>. Most work however focuses either solely on text localization (detection) <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b5">6,</ref><ref type="bibr" target="#b14">15]</ref> or on recognition of manually cropped-out words <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b23">24]</ref>. The problem of scene text recognition has been so far always approached ad-hoc, by connecting the detection module to an existing independent recognition method <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b7">8]</ref>.</p><p>In this paper, we propose a novel end-to-end framework which simultaneously detects and recognizes text in scene images. As the first contribution, we present a model which is trained for both text detection and recognition in a single learning framework, and we show that such joint model outperforms the combination of state-of-the-art localization <ref type="figure">Figure 1</ref>. The proposed method detects and recognizes text in scene images at 10fps on an NVidia K80 GPU. Ground truth in green, model output in red. The image is taken from the ICDAR 2013 dataset <ref type="bibr" target="#b12">[13]</ref> and state-of-the-art recognition methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4]</ref>.</p><p>As the second contribution, we show how the stateof-the-art object detection methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> can be extended for text detection and recognition, taking into account specifics of text such as the exponential number of classes (given an alphabet A, there are up to A L possible classes, where L denotes maximum text length) and the sensitivity to hidden parameters such as text aspect and rotation.</p><p>The method achieves state-of-the-art results on the standard ICDAR 2013 <ref type="bibr" target="#b12">[13]</ref> and ICDAR 2015 <ref type="bibr" target="#b11">[12]</ref> datasets and the pipeline runs end-to-end at 10 frames per second on a NVidia K80 GPU, which is more than 10 times faster than the fastest methods.</p><p>The rest of the paper is structured as follows. In Section 2, previous work is reviewed. In Section 3, the proposed method is described and in Section 4 evaluated. The paper is concluded in Section 5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Previous Work</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.1.">Scene Text Localization</head><p>Jaderberg et al. <ref type="bibr" target="#b9">[10]</ref> train a character-centric CNN <ref type="bibr" target="#b13">[14]</ref>, which takes a 24 × 24 image patch and predicts a text/notext score, a character and a bigram class. The input image  <ref type="table">-RR-I-VE-R-S--III-D--E-------------------------------------------------------------W-AA-LLLK--------</ref>  <ref type="bibr" target="#b0">[1]</ref> classifier to reduce the number of false positives and its position and size is further refined by a CNN regressor, to obtain a more suitable cropping of the detected word image. Gupta et al. <ref type="bibr" target="#b5">[6]</ref> propose a fully-convolutional regression network, drawing inspiration from the YOLO object detection pipeline <ref type="bibr" target="#b20">[21]</ref>. An image is divided into a fixed number of cells (14 × 14 in the highest resolution), where each cell is associated with 7 values directly predicting the position, rotation and confidence of text. The values are estimated by translation-invariant predictors built on top of the first 9 convolutional layers of the popular VGG-16 architecture <ref type="bibr" target="#b24">[25]</ref>, trained on synthetic data.</p><p>Tian et al. <ref type="bibr" target="#b25">[26]</ref> adapt the Faster R-CNN architecture <ref type="bibr" target="#b22">[23]</ref> by horizontally sliding a 3 × 3 window on the last convolutional layer of the VGG-16 <ref type="bibr" target="#b24">[25]</ref> and applying a Recurrent Neural Network to jointly predict the text/non-text score, the y-axis coordinates and the anchor side-refinement. Similarly, Liao et al. <ref type="bibr" target="#b14">[15]</ref> adapt the SSD object detector <ref type="bibr" target="#b16">[17]</ref> to detect horizontal bounding boxes.</p><p>Ma et al. <ref type="bibr" target="#b17">[18]</ref> adapt the Faster R-CNN architecture and extend it to detect text of different orientations by adding anchor boxes of 6 hand-crafted rotations and 3 aspects. This is in contrast to our work, where the rotation is a continuous parameter and the optimal anchor boxes dimensions are found on the training set.</p><p>All the aforementioned methods only localize text, but do not provide text recognition. The end-to-end scene text recognition results, where present, are achieved by simply connecting the particular localization method to one of the cropped-word recognition methods (see Section 2.2).</p><p>Last but not least, the methods are significantly slower than the proposed method, the missing recognition stage notwithstanding.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.2.">Scene Text Recognition</head><p>Jaderberg et al. <ref type="bibr" target="#b7">[8]</ref> take a cropped image of a single word, resize it to a fixed size of 32 × 100 pixels and classify it as one of the words in a dictionary. In their setup, the dictionary contains 90 000 English words and words of the training and testing set. The classifier is trained on a dataset of 9 million synthetic word images uniformly sampled from this dictionary.</p><p>Shi et al. <ref type="bibr" target="#b23">[24]</ref> train a fully-convolutional network with a bidirectional LSTM using the Connectionist Temporal Classification (CTC), which was first introduced by Graves et al. <ref type="bibr" target="#b4">[5]</ref> for speech recognition to eliminate the need for pre-segmented data. Unlike the proposed method, Shi et al. <ref type="bibr" target="#b23">[24]</ref> only recognize a single word per image (i.e. the output is always just one sequence of characters), they resize the source image to a fixed-sized matrix of 100 × 32 pixels regardless of how many characters it contains and the method is significantly slower because of the LSTM layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.3.">Image Captioning</head><p>Johnson et al. <ref type="bibr" target="#b10">[11]</ref> introduce a Fully Convolutional Localization Network (FCLN) that combines the Faster R-CNN approach of Ren et al. <ref type="bibr" target="#b22">[23]</ref> based on full VGG-16 <ref type="bibr" target="#b24">[25]</ref> with bilinear sampling <ref type="bibr" target="#b8">[9]</ref> to generate features for LSTM that produces captions for detected objects. In our method, we use YOLOv2 architecture <ref type="bibr" target="#b21">[22]</ref> for its lower complexity, we use the bilinear sampling to produce tensors of variable width to deal with character sequence recognition and we employ a different (and significantly faster) classification stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Proposed Method</head><p>The proposed model localizes text regions in a given scene image and provides text transcription as a sequence of characters for all regions with text (see <ref type="figure">Figure 2)</ref>. The model is jointly optimized for both text localization and recognition in an end-to-end training framework.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Fully Convolutional Network</head><p>We adapt the YOLOv2 architecture <ref type="bibr" target="#b21">[22]</ref> for its accuracy and significantly lower complexity than the standard VGG-16 architecture <ref type="bibr" target="#b24">[25,</ref><ref type="bibr" target="#b10">11]</ref>, as the full VGG-16 architecture requires 30 billion operations just to process a 224×224 (0.05 Mpx) image <ref type="bibr" target="#b21">[22]</ref>. Using YOLOv2 architecture allows us to process images with higher resolution, which is a crucial ability for text recognition -processing at higher resolution is required because a 1Mpx scene image may contain text which is 10 pixels high <ref type="bibr" target="#b11">[12]</ref>, so scaling down the source image would make the text unreadable.</p><p>The proposed method uses the first 18 convolutional and 5 max pool layers from the YOLOv2 architecture, which is based on 3 × 3 convolutional filters, doubling the number of channels after every pooling step and adding 1 × 1 filters to compress the representations between the 3 × 3 filters <ref type="bibr" target="#b21">[22]</ref>. We remove the fully-connected layers to make the network fully convolutional, so our model final layer has the dimension of</p><formula xml:id="formula_0">W 32 × H 32 × 1024,</formula><p>where W a H denote source image width and height <ref type="bibr" target="#b21">[22]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Region Proposals</head><p>Similarly to Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> and YOLOv2 <ref type="bibr" target="#b21">[22]</ref>, we use a Region Proposal Network (RPN) to generate region proposals, but we add rotation r θ which is crucial for a successful text recognition. At each position of the last convolutional layer, the model predicts k rotated bounding boxes, where for each bounding box r we predict 6 features -its position r x , r y , its dimensions r w , r h , its rotation r θ and its score r p , which captures the probability that the region contains text.</p><p>The bounding box position and dimension is encoded with respect to predefined anchor boxes using the logistic activation function, so the actual bounding box position (x, y) and dimension (w, h) in the source image is given as</p><formula xml:id="formula_1">x = σ(r x ) + c x (1) y = σ(r y ) + c y (2) w = a w exp(r w ) (3) h = a h exp(r h ) (4) θ = r θ<label>(5)</label></formula><p>where c x and c y denote the offset of the cell in the last convolutional layer and a w and a h denote the predefined height and width of the anchor box a. The rotation θ ∈ (− π 2 , π 2 ) of the bounding box is predicted directly by r θ .</p><p>We followed the approach of Redmon et al. <ref type="bibr" target="#b21">[22]</ref> and found suitable anchor box scales and aspects by kmeans clustering on the aggregated training set (see Section 3.5). Requiring the anchor boxes to have at least 60% intersection-over-union with the ground truth led to k = 14 different anchor boxes dimensions (see <ref type="figure" target="#fig_1">Figure 3)</ref>.</p><p>For every image, the RPN produces</p><formula xml:id="formula_2">W 32 × H 32 × 6k</formula><p>boxes, where k is the number of anchor boxes in every location and 6 is the number of predicted parameters (x, y, w, h, θ and the text score).</p><p>In the training stage, we use the YOLOv2 approach <ref type="bibr" target="#b21">[22]</ref> by taking all positive and negative samples in the source image, where every 20 batches we randomly change the input dimension size into one of {352, 416, 480, 544, 608}. A positive sample is the region with the highest intersection over union with the ground truth, the other intersecting regions are negatives.</p><p>At runtime, we found the best approach is to take all regions with the score r p above a certain threshold p min and to postpone the non-maxima suppression after the recognition stage, because regions with very similar r p scores could produce very different transcriptions, and therefore selecting the region with the highest r p at this stage would not always correspond to the correct transcription (for example, in some cases a region containing letters "TALY" may have slightly higher score r p than a region containing the full word "ITALY"). We empirically found the value p min = 0.1 to be a reasonable trade-off between accuracy and speed.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Bilinear Sampling</head><p>Each region detected in the previous stage has a different size and rotation and it is therefore necessary to map the features into a tensor of canonical dimensions, which can be used in recognition.</p><p>Faster R-CNN <ref type="bibr" target="#b22">[23]</ref> uses the RoI pooling approach of Girshick <ref type="bibr" target="#b2">[3]</ref>, where a w × h × C region is mapped onto a fixed-sized W ′ × H ′ × C grid (7 × 7 × 1024 in their implementation), where each cell takes the maximum activation of the w W × h H cells in the underlying feature layer. In our model, we instead use bilinear sampling <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b10">11]</ref> to map a w × h × C region from the source image into a fixed-height  scale, but at the same to persist the aspect and positioning of individual characters, which is crucial for text recognition accuracy (see Section 3.4).</p><formula xml:id="formula_3">wH ′ h × H ′ × C tensor (H ′ = 32</formula><note type="other">64 3 × 3 leaky ReLU BatchNorm recurrent conv 64 3 × 3 leaky ReLU maxpool 2 × 2/2 W /4 × 8 conv 128 3 × 3 leaky ReLU BatchNorm recurrent conv 128 3 × 3 leaky ReLU maxpool 2 × 2/2 × 1 W /4 × 4 conv 256 3 × 3 leaky ReLU BatchNorm recurrent conv 256 3 × 3 leaky ReLU maxpool 2 × 2/2 × 1 W /4 × 2 conv 512 3 × 2 leaky ReLU conv 512 5 × 1 leaky ReLU conv |Â| 7 × 1 W /4 × 1 log softmax</note><p>Given the detected region features U ∈ R w×h×C , they are mapped into a fixed-height tensor</p><formula xml:id="formula_4">V ∈ R wH ′ h ×H ′ ×C as V c x ′ ,y ′ = w x=1 h y=1 U c x,y κ(x − T x (x ′ ))κ(y − T y (y ′ )) (6)</formula><p>where κ is the bilinear sampling kernel κ(v) = max(0, 1 − |v|) and T is a point-wise coordinate transformation, which projects co-ordinates x ′ and y ′ of the fixed-sized tensor V to the co-ordinates x and y in the detected region features tensor U.</p><p>The transformation allows for shift and scaling in x-and y-axes and rotation and its parameters are taken directly from the region parameters (see Section 3.2).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Text Recognition</head><p>Given the normalized region from the source image, each region is associated with a sequence of characters or rejected as not text in the following process.</p><p>The main problem one has to address in this step is the fact, that text regions of different sizes have to be mapped to character sequences of different lengths. Traditionally, the issue is solved by resizing the input to a fixed-sized matrix (typically 100×32 <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b23">24]</ref>) and the input is then classified by either making every possible character sequence (i.e. every word) a separate class of its own <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b5">6]</ref>, thus requiring a list of all possible outputs in the training stage, or by having × |Â| as the most probable class at given column (middle) and the resulting sequence (bottom) multiple independent classifiers, where each classifier predicts the character at a predefined position <ref type="bibr" target="#b6">[7]</ref>.</p><p>Our model exploits a novel fully-convolutional network (see <ref type="table" target="#tab_2">Table 1</ref>), which takes a variable-width feature tensor</p><formula xml:id="formula_5">W × H ′ × C as an input (W = wH ′ h</formula><p>) and outputs a matrix W 4 × |Â|, where A is the alphabet (e.g. all English characters). The matrix height is fixed (it's the number of character classes), but its width grows with the width of the source region and therefore with the length of the expected character sequence.</p><p>As a result, a single classifier is used regardless of the position of the character in the word (in contrast to Jaderberg et al. <ref type="bibr" target="#b6">[7]</ref>, where there is an independent classifier for the character "A" as the first character in the word, an independent classifier for the character "A" as the second character in the word, etc). The model also does not require prior knowledge of all words to be detected in the training stage, in contrast to the separate class per character sequence formulation <ref type="bibr" target="#b7">[8]</ref>.</p><p>The model uses Connectionist Temporal Classification (CTC) <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b23">24]</ref> to transform variable-width feature tensor into a conditional probability distribution over label sequences. The distribution is then used to select the most probable labelling sequence for the text region (see <ref type="figure" target="#fig_2">Figure 4)</ref>.</p><p>Let y = y 1 , y 2 , · · · , y n denote the vector of network outputs of length n from an alphabet A extended with a blank symbol "-".</p><p>The probability of a path π is then given as</p><formula xml:id="formula_6">p(π|y) = n i=1 y i πi , π ∈Â n (7) A = A ∪ {−}</formula><p>where y i πi denotes the output probability of the network predicting the label π i at the position i (i.e. the output of the final softmax layer in <ref type="table" target="#tab_2">Table 1</ref>).</p><p>Let us further define a many-to-one mapping B :Â n → A ≤n , whereÂ ≤n is the set of all sequences of shorter or equal in length. The mapping B removes all blanks and repeated labels, which corresponds to outputting a new label every time the label prediction changes. For example,</p><formula xml:id="formula_7">B(−ww − al − k) = B(wwaaa − l − k−) = walk B(−f − oo − o − −d) = B(ffoo − ooo − d) = food</formula><p>The conditional probability of observing the output sequence w is then given as</p><formula xml:id="formula_8">p(w|y) = π: B(π)=w p(π|y), w ∈ A ≤n (8)</formula><p>In training, an objective function that maximizes the log likelihood of target labellings p(w|y) is used <ref type="bibr" target="#b4">[5]</ref>. In every training step, the probability p(w gt |y) of every text region in the mini-batch is efficiently calculated using a forwardbackward algorithm similar to HMMs training <ref type="bibr" target="#b19">[20]</ref> and the objective function derivatives are used to update network weights, using the standard back-propagation algorithm (w gt denotes the ground truth transcription of the text region).</p><p>At test time, the classification output w * should be given by the most probable path p(w|y), which unfortunately is not tractable, and therefore we adapt the approximate approach <ref type="bibr" target="#b4">[5]</ref> of taking the most probable labelling</p><formula xml:id="formula_9">w * ≈ B argmax p(π|y)<label>(9)</label></formula><p>At the end of this process, each text region in the image has an associated content in the form of a character sequence, or it is rejected as not text when all the labels are blank.</p><p>The model typically produces many different boxes for a single text area in the image, we therefore suppress overlapping boxes by a standard non-maxima suppression algorithm based on the text recognition confidence, which is the p(w * |y) normalized by the text length.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Training</head><p>We pre-train the detection CNN using the SynthText dataset <ref type="bibr" target="#b5">[6]</ref> (800, 000 synthetic scene images with multiple words per image) for 3 epochs, with weights initialized from ImageNet <ref type="bibr" target="#b21">[22]</ref>. The recognition CNN is pre-trained on the Synthetic Word dataset <ref type="bibr" target="#b6">[7]</ref> (9 million synthetic cropped word images) for 3 epochs, with weights randomly initialized from the N (0, 1) distribution.</p><p>As the final step, we train both networks simultaneously for 3 epochs on a combined dataset consisting of the SynthText dataset, the Synthetic Word dataset, the ICDAR 2013 Training dataset <ref type="bibr" target="#b12">[13]</ref> (229 scene images captured by a professional camera) and the ICDAR 2015 Training Note that in some cases (e.g. top-right) text is correctly recognized even though the bounding IoU with the ground truth is less than 80%, which would be required by the text localization protocol <ref type="bibr" target="#b12">[13]</ref>. Best viewed zoomed in color dataset <ref type="bibr" target="#b11">[12]</ref> (1000 scene images captured by Google Glass). For every image, we randomly crop up to 30% of its width and height. We use standard Stochastic Gradient Descent with momentum 0.9 and learning rate 10 −3 , divided by 10 after each epoch. One mini-batch takes about 500ms on a NVidia K80 GPU.  <ref type="table">Table 2</ref>. ICDAR 2013 dataset -End-to-end scene text recognition accuracy (f-measure), depending on the lexicon size and whether digits are excluded from the evaluation (denoted as word spotting). Methods running on a GPU marked with an asterisk end-to-end word spotting speed strong weak generic strong weak generic fps TextSpotter <ref type="bibr" target="#b18">[19]</ref> 0  <ref type="table">Table 3</ref>. ICDAR 2015 dataset -End-to-end scene text recognition accuracy (f-measure). Methods running on a GPU marked with an asterisk</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We trained our model once <ref type="bibr" target="#b0">1</ref> and then evaluated its accuracy on three standard datasets. We evaluate the model in an end-to-end set up, where the objective is to localize and recognize all words in the image in a single step, using the standard evaluation protocol associated with each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">ICDAR 2013 dataset</head><p>In the ICDAR evaluation schema <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b11">12]</ref>, each image in the test set is associated with a list of words (lexicon), which contains the words that the method should localize and recognize, as well as an increasing number of random "distractor" words. There are three sizes of lists provided with each image, depending how heavily contextualized their content is to the specific image:</p><p>• strongly contextualized -100 words specific to each image, contains all words in the image and the remaining words are "distractors"</p><p>• weakly contextualized -all words in the testing set, same list for every image</p><p>• generic -all words in the testing set plus 90k English words A word is considered as correctly recognized, when its Intersection-over-Union (IoU) with the ground truth is above 0.5 and the transcription is identical, using caseinsensitive comparison <ref type="bibr" target="#b11">[12]</ref>.</p><p>The ICDAR 2013 Dataset <ref type="bibr" target="#b12">[13]</ref> is the most-frequently cited dataset for scene text evaluation. It consists of 255 testing images with 716 annotated words, the images were taken by a professional camera so text is typically horizontal and the camera is almost always aimed at it. The dataset is sometimes referred to as the Focused Scene Text dataset.</p><p>The proposed model achieves state-of-the-art text recognition accuracy (see <ref type="table">Table 2</ref>) for all 3 lexicon sizes. In the end-to-end set up, where all lexicon words plus all digits in an image should be recognized, the maximal f-measure it achieves is 0.89/0.86/0.77 for strongly, weakly and generally contextualized lexicons respectively. Each image is first resized to 544×544 pixels, the average processing time is 100ms per image on a NVidia K80 GPU for the whole pipeline.</p><p>While training on the same training data, our model outperforms the combination of the state-of-the-art localization method of Gupta et al. <ref type="bibr" target="#b5">[6]</ref> with the state-of-the-art recognition method of Jaderberg et al. <ref type="bibr" target="#b7">[8]</ref> by at least 3 per cent points on every measure, thus demonstrating the advantage of the joint training for the end-to-end task of our model. It is also more than 20 times faster than the method of Gupta et al. <ref type="bibr" target="#b5">[6]</ref>.</p><p>Let us further note that our model would not be considered as a state-of-the-art text localization method according to the text localization evaluation protocol, because the standard DetEval tool used for evaluation is based on a series of thresholds which require at least a 80% intersection-overunion with bounding boxes created by human annotators. Our method in contrast does not always achieve the required 80% overlap, but it is still mostly able to recognize the text correctly even when the overlap is lower (see <ref type="figure" target="#fig_3">Figure 5)</ref>.</p><p>We argue that evaluating methods purely on text localization accuracy without subsequent recognition is not very informative, because the text localization "accuracy" only aims to fit the way human annotators create bounding boxes around text, but it does not give any estimates on how well a text recognition phase would read text post a successful  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">ICDAR 2015 dataset</head><p>The ICDAR 2015 dataset was introduced in the ICDAR 2015 Robust Reading Competition <ref type="bibr" target="#b11">[12]</ref> and it uses the same evaluation protocol as the ICDAR 2013 dataset in the previous section. The dataset consists of 500 test images, which were collected by people wearing Google Glass devices and walking in Singapore. Subsequently, all images with text were selected and annotated. The images in the dataset were taken "not having text in mind", therefore text is much smaller and the images contain a high variability of text fonts and sizes. They also include many realistic effects -e.g. occlusion, perspective distortion, blur or noise, so as a result the dataset is significantly more challenging than the ICDAR 2013 dataset (Section 4.1), which contains typically large horizontal text.</p><p>The proposed model achieves state-of-the-art end-to-end text recognition accuracy (see <ref type="table">Table 3</ref> and <ref type="figure" target="#fig_4">Figure 6</ref>) for all 3 lexicon sizes. In our experiments, the average processing time was 110ms per image on a NVidia K80 GPU (the image is first resized to 608 × 608 pixels), which makes the proposed model 45 times faster than currently the best published method of Gomez et al. <ref type="bibr" target="#b3">[4]</ref> The main failure mode of the proposed method is blurry  or noisy text (see <ref type="figure" target="#fig_6">Figure 8</ref>), which are effects not present in the training set (Section 3.5). The method also often fails to detect small text (less than 15 pixels high), which again is due to the lack of such samples in the training stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">COCO-Text dataset</head><p>The COCO-Text dataset <ref type="bibr" target="#b26">[27]</ref> was created by annotating the standard MS COCO dataset <ref type="bibr" target="#b15">[16]</ref>, which captures images of complex everyday scenes. As a result, the dataset contains 63,686 images with 173,589 labeled text regions, so it is two orders of magnitude larger than any other scene text dataset. Unlike the ICDAR datasets, there is no lexicon used in the evaluation, so methods have to recognize text without any prior knowledge.</p><p>The proposed model demonstrates competitive results in the text recognition accuracy (see <ref type="table">Table 4</ref> and <ref type="figure" target="#fig_7">Figure 9</ref>), being only surpassed by Method A 2 .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>A novel framework for scene text localization and recognition was proposed. The model is trained for both text detection and recognition in a single training framework.</p><p>The proposed model achieves state-of-the-art accuracy in the end-to-end text recognition on two standard datasets <ref type="bibr">(ICDAR 2013 and</ref><ref type="bibr">ICDAR 2015)</ref>, whilst being an order of magnitude faster than the previous methods -the whole pipeline runs at 10 frames per second on a NVidia K80 GPU. Our model showed that the state-of-the-art object detection methods <ref type="bibr" target="#b21">[22,</ref><ref type="bibr" target="#b22">23]</ref> can be extended for text detection and recognition, taking into account specifics of text, and still maintaining a low computational complexity.</p><p>We also demonstrated the advantage of the joint training for the end-to-end task, by outperforming the ad-hoc combination of the state-of-the-art localization and state-of-theart recognition methods <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b3">4,</ref><ref type="bibr" target="#b7">8]</ref>, while exploiting the same training data.</p><p>Last but not least, we showed that optimizing localization accuracy on human-annotated bounding boxes might not improve performance of an end-to-end system, as there is not a clear link between how well a method fits the bounding boxes created by a human annotator and how well a method reads text. Future work includes extending the training set with more realistic effects, single characters and digits.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Anchor box widths and heights, or equivalently scales and aspects, were obtained by k-means clustering on the training set. Requiring that each ground truth box had intersection-overunion of at least 60% with one anchor box led to k = 14 boxes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Text recognition using Connectionist Temporal Classification. Input W × 32 region (top), CTC output W 4</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. End-to-end scene text recognition samples from the ICDAR 2013 dataset. Model output in red, ground truth in green. Note that in some cases (e.g. top-right) text is correctly recognized even though the bounding IoU with the ground truth is less than 80%, which would be required by the text localization protocol [13]. Best viewed zoomed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. End-to-end scene text recognition samples from the ICDAR 2015 dataset. Model output in red, ground truth in green. Best viewed zoomed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. All the images of the ICDAR 2013 Testing set where the proposed method fails to correctly recognize any text (i.e. images with 0% recall)</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Main failure modes on the ICDAR 2015 dataset. Blurred and noisy text (top), vertical text (top) and small text (bottom). Best viewed zoomed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 9 .</head><label>9</label><figDesc>Figure 9. End-to-end scene text recognition samples from the COCO-Text dataset. Model output in red, ground truth in green. Best viewed zoomed in color</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_0" validated="false"><head></head><label></label><figDesc>-</figDesc><table>Source Image 
Fully Convolutional Net 
Region Proposals 
Region Scores 

Best Proposals 
Geometric Normalization 
CTC 
Region Transcription 

CNN CNN 
CNN CNN 

Figure 2. Method overview. Text region proposals are generated by a Region Proposal Network [22]. Each region with a sufficient text 
confidence is then normalized to a variable-width feature tensor by bilinear sampling. Finally, each region is associated with a sequence of 
characters or rejected as not text. 

is scanned by the trained network in 16 scales and a text 
saliency map is obtained by taking the text/no-text output 
of the network. Given the saliency maps, word bounding 
boxes are then obtained by the run length smoothing algo-
rithm. The method is further improved in [8], where a word-
centric approach is introduced. First, horizontal bounding-
box proposals are detected by aggregating the output of 
the standard Edge Boxes [29] and Aggregate Channel Fea-
ture [2] detectors. Each proposal is then classified by a Ran-
dom Forest </table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>). This feature representation has a key advantage over the standard RoI approach as it allows the network to normalize rotation and</figDesc><table>Type 

Channels Size/Stride 
Dim/Act 
input 
C 
-
W × 32 
conv 
32 
3 × 3 
leaky ReLU 
conv 
32 
3 × 3 
leaky ReLU 
maxpool 
2 × 2/2 
W /2 × 16 
conv 
</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 1 .</head><label>1</label><figDesc>Fully-Convolutional Network for Text Recognition</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>Table 4. COCO-Text dataset -End to End text recognition</figDesc><table>). 
Best viewed zoomed in color 

recall precision f-measure 
Method A [27] 
28.33 
68.42 
40.07 
Method B [27] 
9.97 
54.46 
16.85 
Method C [27] 
1.66 
4.15 
2.37 
Deep TextSpotter 16.75 
31.43 
21.85 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Full source code and the trained model are publicly available at https://github.com/MichalBusta/DeepTextSpotter</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="2">Method A [27] was authored by Google and neither the training data nor the algorithm is published.</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Image classification using random forests and ferns</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bosch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Muoz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision, 2007. ICCV 2007. IEEE 11th International Conference on</title>
		<imprint>
			<date type="published" when="2002" />
			<biblScope unit="page" from="1" to="8" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast feature pyramids for object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Appel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="issue">8</biblScope>
			<biblScope unit="page" from="1532" to="1545" />
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1440" to="1448" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<monogr>
		<title level="m" type="main">Textproposals: A textspecific selective search algorithm for word spotting in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.02619</idno>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Connectionist temporal classification: labelling unsegmented sequence data with recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fernández</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Gomez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schmidhuber</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 23rd international conference on Machine learning</title>
		<meeting>the 23rd international conference on Machine learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Synthetic data for text localisation in natural images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Gupta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Synthetic data and artificial neural networks for natural scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2005" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Reading text in the wild with convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">116</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="1" to="20" />
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep features for text spotting</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="512" to="528" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Densecap: Fully convolutional localization networks for dense captioning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">ICDAR 2015 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Gomez-Bigorda</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Nicolaou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ghosh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Andrew</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">R</forename><surname>Chandrasekhar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Valveny</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2015</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">ICDAR 2013 robust reading competition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Karatzas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Shafait</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Uchida</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iwamura</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">R</forename><surname>Mestre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">F</forename><surname>Mota</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">A</forename><surname>Almazan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">P</forename><surname>De Las Heras</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICDAR 2013</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="volume">5</biblScope>
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Gradientbased learning applied to document recognition. Proceedings of the IEEE</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bottou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Haffner</surname></persName>
		</author>
		<imprint>
			<date type="published" when="1998" />
			<biblScope unit="volume">86</biblScope>
			<biblScope unit="page" from="2278" to="2324" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Liao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.06779</idno>
		<title level="m">Textboxes: A fast text detector with a single deep neural network</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Microsoft coco: Common objects in context</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European conference on computer vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Ssd: Single shot multibox detector</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Fu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">C</forename><surname>Berg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="21" to="37" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Arbitrary-oriented scene text detection via rotation proposals</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ye</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xue</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.01086</idno>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Real-time lexicon-free scene text localization and recognition. Pattern Analysis and Machine Intelligence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on</title>
		<imprint>
			<biblScope unit="volume">38</biblScope>
			<biblScope unit="issue">9</biblScope>
			<biblScope unit="page" from="1872" to="1885" />
			<date type="published" when="2016-09" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">A tutorial on hidden markov models and selected applications in speech recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">R</forename><surname>Rabiner</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Proceedings of the IEEE</title>
		<imprint>
			<biblScope unit="volume">77</biblScope>
			<biblScope unit="issue">2</biblScope>
			<biblScope unit="page" from="257" to="286" />
			<date type="published" when="1989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.08242</idno>
		<title level="m">Yolo9000: Better, faster, stronger</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="91" to="99" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">An end-to-end trainable neural network for image-based sequence recognition and its application to scene text recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Shi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Bai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Pattern Analysis and Machine Intelligence</title>
		<imprint>
			<date type="published" when="2004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Detecting text in natural image with connectionist text proposal network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Qiao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="56" to="72" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Veit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Matera</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Neumann</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Matas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1601.07140</idno>
		<title level="m">Coco-text: Dataset and benchmark for text detection and recognition in natural images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Robust text detection in natural scene images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X.-C</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Yin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H.-W</forename><surname>Hao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE transactions on pattern analysis and machine intelligence</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">36</biblScope>
			<biblScope unit="page" from="970" to="983" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Edge boxes: Locating object proposals from edges</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="391" to="405" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
