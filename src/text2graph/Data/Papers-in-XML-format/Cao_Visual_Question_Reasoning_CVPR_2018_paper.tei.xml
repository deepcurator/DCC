<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:51+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Visual Question Reasoning on General Dependency Tree</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Qingxing</forename><surname>Cao</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Xiaodan</forename><surname>Liang</surname></persName>
							<email>xdliang328@gmail.com</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bailin</forename><surname>Li</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Guanbin</forename><surname>Li</surname></persName>
							<email>liguanbin@mail.sysu.edu.cn</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Lin</surname></persName>
							<email>linliang@ieee.org</email>
							<affiliation key="aff0">
								<orgName type="department">School of Data and Computer Science</orgName>
								<orgName type="institution">Sun Yat-sen University</orgName>
								<address>
									<country key="CN">China</country>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Visual Question Reasoning on General Dependency Tree</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"> <ref type="figure">Figure 1</ref><p>: Illustration of our Adversarial Composition Module Network (ACMN) that sequentially performs reasoning over a dependency tree parsed from the question. Conditioning on preceding word nodes, our ACMN alternatively mines visual evidence for nodes with modifier relations via an adversarial attention module and integrates features of child nodes of nodes with clausal predicate relation via a residual composition module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>The task of Visual Question Answering (VQA) is to predict the correct answer given an image and a textual question. The key to this task is the capability of coreasoning over both image and language domains. However, most of the previous methods <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b15">16]</ref> work more like a black-box manner, i.e., simply mapping the visual content to the textual words by crafting neural networks. The main drawback of these methods is the lack of interpreting ability to the results, i.e., why these answers are produced? Moreover, it has been shown that their accu-racy may be achieved by over-fitting the data bias in the VQA benchmark <ref type="bibr" target="#b8">[9]</ref>, and the absence of explicitly exploiting structures of text and image leads to unsatisfying performance on relational reasoning <ref type="bibr" target="#b13">[14]</ref>. Very recently, a few pioneering works <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15,</ref><ref type="bibr" target="#b25">26]</ref> take advantage of the structure inherently contained in text and image, which parses the question-image input into a tree or graph layout and assembles local features of nodes to predict the answer. For example, layout "more(find(ball),find(yellow))" means the module should locate the ball and the yellow object on the image first, then compose the two results to answer whether there are more balls than yellow objects. However, these methods would either rely on hand-designed rules for understanding questions or train a layout parser from scratch which suffers large decay in performance. We argue those limitations severely prohibit their application potentials in understanding general image-question pairs that may contain diverse and open-ended question styles.</p><p>To achieve a general and powerful reasoning system with the ability to enable reasoning over any dependency trees of questions rather than fixed layouts in prior works, we propose a novel Adversarial Composition Modular Network (ACMN) that designs two collaborative modules to perform tailored reasoning operations for addressing two most common word relations in the questions. As shown in <ref type="figure">Figure 1</ref>, given a specific dependency tree of each question by an offthe-shelf dependency parser, we construct a reasoning route following the dependency layout that is a tree-structure composed of clausal predicate relation and modifier relation. Our module network then alternatively performs two collaborative modules on each word node for global reasoning: 1) exploit local visual evidence of each word guided by exploited regions of its child nodes in an adversarial way in terms of nodes with modifier relations; 2) integrate the hidden representations of child nodes via residual composition with respect to nodes with clausal predicate relation. Notably, in contrast to previous methods, our ACMN aims at a general and interpretable reasoning VQA framework that does not require any complicated handcrafted rules or ground-truth annotation to obtain a specific layout.</p><p>Specifically, we observe that the frequently used types of dependency relations can be categorize in two sets: whether the head is a predicate that describes the relation of its children (e.g. color ← is, is→nose), or a word decorated by its child (e.g. furthest→object). We refer the first set as clausal predicate relation and the second is modifier relation. Thus our ACMN designs adversarial attention modules for encoding modifier relation and residual composition modules for clausal predicate relations.</p><p>Firstly, for child nodes with modifier relations, we apply the adversarial attention mechanism similar to <ref type="bibr" target="#b27">[28]</ref>. To enable effectively mining all visual evidence, we enforce each parent node explore new regions by masking out attentive regions of its child nodes at each step. More specifically, we sum up the attention maps from child nodes and mask out features weighted by the mined attention map in a soft manner. We then perform attention operation on manipulated hidden representations to extract new local visual evidence for the parent node. Secondly, for those with clausal predicate relation, our residual composition module integrates the hidden representations weighted by attention maps of its child nodes using bilinear fusion. In order to retain the information from the child nodes and deal with an arbitrary number of child nodes, the module learns a residual that will be added to the input on the sum of child nodes to modify their hidden representations. Finally, the final hidden representation of the root node will go through a multi-layer perceptron to predict the final answer.</p><p>Extensive experiments show that our model can achieve state-of-art VQA performance on both natural image VQA benchmark VQAv2 dataset and CLEVR relational dataset.</p><p>And qualitative results further demonstrate the interpretable capability of our ACMN on collaborative reasoning over image and language domains.</p><p>Our contributions summarized as follows: 1) We present a general and interpretable reasoning VQA system following a general dependency layout composed by modifier relations and clausal predicate relations. 2) a novel adversarial attention module is proposed to enforce efficient visual evidence mining for modifier relations while a residual composition module for integrating knowledge of child nodes for clausal predicate relations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>Visual question answering The visual question answering task requires co-reasoning over both image and text to infer the correct answer.</p><p>The baseline methods proposed in VQA dataset <ref type="bibr" target="#b3">[4]</ref> to solve this task using a CNN-LSTM based architecture, which consists of a convolution neural network to extract image features, and an LSTM to encode the question features. It combines these two features to predict the final answer. Recent years, a large number of works followed this pipeline and have achieved substantial improvements over baseline model. Among these works, the attention mechanism <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b30">31,</ref><ref type="bibr" target="#b19">20]</ref> and the joint embedding of image and question representation <ref type="bibr" target="#b7">[8,</ref><ref type="bibr" target="#b15">16]</ref> have been widely studied. Attention mechanism learns to focus on the most discriminate sub-region instead of the whole image, provide a certain extent of reasoning to the answer. Different attention methods such as stacked attention <ref type="bibr" target="#b30">[31]</ref> and co-attention between question and image on different levels <ref type="bibr" target="#b19">[20]</ref> constantly improve the performance of the VQA task. As for the multi-modal joint embedding, Fukui et al. <ref type="bibr" target="#b7">[8]</ref>, Kim et al. <ref type="bibr" target="#b15">[16]</ref> and Hedi et al. <ref type="bibr" target="#b4">[5]</ref> exploited the compact bilinear method to fuse the embedding of image and question and in- The modules in our ACMN: a) each ACMN module that is composed by an adversarial attention module and residual composition module; b) adversarial attention module; c) residual composition module. The blue arrows indicate the modifier relation and the yellow arrows represent the clausal predicate relation. Each node receives the output attention maps and the hidden features from its children, as well as the image feature and word encoding. The adversarial attention module is employed to generate a new attention map conditioned on image feature, word encoding and previous attended regions given by modifier-dependent children. The residual composition module is learned to evolve higher-level representation by integrating features of its children and local visual evidence. corporated the attention mechanism to further improve the performance.</p><p>However, some recently proposed works <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b12">13]</ref> showed that the promising performance of these deep models might be achieved by exploiting the dataset bias. It is possible to perform equally well by memorizing the QA-pairs or encode the question with the bag-of-words method. To address this concern, newer datasets were released in the very recent. The VQAv2 dataset <ref type="bibr" target="#b8">[9]</ref> was proposed to eliminate the data biases through balancing question-answer pairs. The CLEVR <ref type="bibr" target="#b13">[14]</ref> dataset consists of synthetic images, and provides more complex questions that involve multiple objects. It also has balanced answer distribution to suppresses the data bias.</p><p>Reasoning model There exist prior works that tried to explicitly incorporate the knowledge into the network structure. <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b26">27]</ref> encoded both image and question into discrete vectors such as image attributes or database queries. These vectors enable their model to query the external data source for common senses and basic factual knowledge to answer the question. <ref type="bibr" target="#b34">[35]</ref> actively acquires pre-defined types of evidence to obtain external information and predicted the answer. Other recent works proposed modular network to handle the composition reasoning. <ref type="bibr" target="#b28">[29]</ref> augmented a differentiable memory and encoding long-term knowledge to infer the answer.</p><p>Neural modular network The recently proposed neural modular network provides a framework to address compositional visual reasoning. Instead of using a fixed structure to predict the answer to every question, this line of works assembles a structure layout for different question into pre-defined sub-tasks. Then a set of neural modules is designed to solve a particular sub-task respectively. Earlier works <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b1">2]</ref> generated their layouts based on dependency parse. Later, <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b14">15]</ref> use sequence-to-sequence RNN to predict the post-order of layout tree, and jointly train the RNN and the neural module using RL or EM manner.</p><p>However, it is difficult to jointly train the RNN and the modular network from scratch. On the other hand, existing neural module will propagate its error through the rest of modular network, thus these methods heavily depend on the correctness of the structured layout. Our method modifies the structure of neural modules to avoid the error propagation and takes advantage of the information lay on the type of dependency. This substantially improves the prediction accuracy while preserving the compositional reasoning ability.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Adversarial Composition Modular Network</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Overview</head><p>Given the free-form questions Q and images I, our proposed ACMN model learns to predict the answers y and their corresponding explainable attention maps. Specifically, we first generate the structure layout given the input question Q by parsing it into a tree structure using an off-the-shelf universal Stanford Parser <ref type="bibr" target="#b5">[6]</ref>. To reduce the computational complexity, we prune the leaf-nodes that are not noun, then categorize the labels of dependency relations such as "nominal modifier" (e.g. (left, object)), "nominal subject"(e.g. (is, color) into two classes: the modifier relation M and the clausal predicate relation P .</p><p>Our ACMN model is constituted by a set of network modules f on each word node in the layout from bottom to top. Suppose a node is x, and its n children {x . It outputs a new attentive region att out , and a hidden feature h out , which are generated by the adversarial attention module f a and residual composition module f h respectively, as shown in <ref type="figure" target="#fig_0">Figure 2a</ref>.</p><p>The spatial feature v is extracted for each image via any pre-trained convolution neural network on ImageNet (e.g. conv5 features from ResNet-152 <ref type="bibr" target="#b9">[10]</ref> or conv4 features from ResNet-101 <ref type="bibr" target="#b9">[10]</ref>). The word embedding vector w is obtained with a Bi-LSTM <ref type="bibr" target="#b22">[23]</ref>. Specifically, each word in the question is first embedded as a 300 dimension vector, then the question is feed into a bidirectional LSTM. The final word embedding w is the hidden vector of Bi-LSTM at its corresponding position.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Adversarial Attention Module</head><p>Specifically, as shown in <ref type="figure" target="#fig_0">Figure 2b</ref>, we first filter the child nodes whose relation is modifier M and perform adversarial attention module on the parent node x. The input attention map att in of each node x is first obtained by summing attention maps {att c i } of its modifier-dependent child nodes x i . The adversarial mask is generated by subtracting att in by 1 followed by a ReLU layer to keep the results nonnegative. Then the mask is used to softly weights the spatial feature v via a multiplication operation. Finally, the adversarial module f a outputs a new attention map att out conditioned on the input word embedding w and weighted spatial features. We further apply Softmax to regularize the resulting attention map into the range of [0, 1]. The visual representation h ′ of the node x is then generated by the weighted sum of each grid features in v given the attended weight att out .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Residual Composition Module</head><p>As shown in <ref type="figure" target="#fig_0">Figure 2c</ref>, the residual composition module f h first sums the hidden features {h c i } of its children with clausal predicate relation P into h in , and then concatenate h in with extracted local evidence h ′ , and finally combine with word encoding w to generate a new hidden feature h out . A fully connected layer is applied to project both the concatenated hidden [h in , h ′ ] and word encoding w feature to 2048 dimension feature vector. Then we perform element-wise multiplication on two features, project it to 128 dimension vector, and add it with all of its children's hidden feature {h c i } as the output hidden representation h out . </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">The proposed ACMN model</head><p>Given the tree-structured layout of the dependency tree, our ACMN module is sequentially used on each word node to mine visual evidence and integrate features of its child nodes from bottom to top, and then predict the final answer at the root of the tree. Formally, each ACMN module can be written as:</p><formula xml:id="formula_0">att in = (x,x c i )∈M att c i , h in = (x,x c i )∈P h c i , att out = f a (att in , v, w), h ′ = att out * v, h out = f h ([h in , h ′ ], w) + i h c i ,<label>(1)</label></formula><p>Where (x, x c i ) represents the relation of node x and its child x c i . Because the nodes with modifier relations M can modify their parent node by referring to a more specific object, we thus generate a more precise attention map as att out . On the other hand, the clausal predicate relation P suggests the parent node is a predicate of child nodes, we thus integrate features of child nodes to enhance the representation given the predicate word.</p><p>After propagating through all word nodes with a sequence of adversarial attention module and residual composition module, the output features of the root node h root are passed through a three Multi-Layer Perceptron to predict the final answer y. Our model that is stacked by a list of adversarial attention modules and residual composition mod-ules following a tree-structured layout. Weights are share across modules with same height in order to learn different levels of semantic representation. The whole model can be trained end-to-end with only the supervision signal y.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Modifier Relation and Clausal Predicate Relation</head><p>A dependency-based parser is to draw directed edges from head words to dependent words in a sentence. It also labels the head-dependent relations to provide an approximation to the relationship between predicates and their arguments. One of the most widely used head-dependent relation sets is the Universal Dependencies(UD) <ref type="bibr" target="#b6">[7]</ref>. It has a total of 42 relations that can be clustered into 9 categories. But the frequently used relations concentrate on only two of them: the core dependents of clausal predicate and the noun dependents, as shown in <ref type="figure" target="#fig_2">Figure 3</ref>. In this work, we make some small modification to noun dependents sets and refer these two kinds of relationships as clausal predicate relation P and modifier relation M . The details of both sets are shown on <ref type="table">Table 1</ref>. For those relations that belong to neither of the two sets, we will pass both the attention map and the hidden representation to the parent nodes.</p><p>Dependents of clausal predicate relation P describe syntactic roles with respect to a predicate that often describes how to compose its children. For example, in question What color is the nose of the plane?, word is is the head of color and nose, and their relations are (is,color) = direct object and (is,nose) = nominal subject. So the word is tells us how to compose word color and nose, such as using a function "describes(color, nose)" in a modular network <ref type="bibr" target="#b2">[3]</ref>. Thus, our residual composition module learns to compose features {h c i } of its children nodes with clausal predicate relation P conditioned on current word embedding w of a parent node.</p><p>The modifier relations M categorize the ways words that can modify their parents. For example, the modifier relation M of the question What size is the cylinder that is left of the brown metal thing from CLEVR dataset <ref type="bibr" target="#b13">[14]</ref> can be the relation (left, brown metal thing) = nominal modifier. The reason is that the word left indicates the region related to brown metal thing instead of cylinder, which is similar to "transform(left, thing)" relation in the modular network <ref type="bibr" target="#b2">[3]</ref>. Thus we can obtain a modified attention map for the part node according to attention maps {att c i } of its children given the current word encoding w via our adversarial attention module.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiment</head><p>We validate the effectiveness and interpretation capability of our models on both two synthetic datasets (i.e., CLEVR and Sort-of-CLEVR) that mainly focus on relation reasoning and one natural dataset (i.e., VQAv2) with diverse image-question pairs in the wild. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>The CLEVR <ref type="bibr" target="#b13">[14]</ref> is a synthesized dataset with 100, 000 images and 853, 554 questions. The images are photorealistic rendered images with objects of random shapes, colors, materials and sizes. The questions are generated using sets of functional programs, which consists of functions that can filter certain color, shape, or compare two objects. Thus, the reasoning routes required to answer each question can be precisely determined by its underlying function program. Unlike natural image dataset, it requires model capable of reasoning on relations to answer the questions.</p><p>The Sort-of-CLEVR <ref type="bibr" target="#b21">[22]</ref> consists of synthesized images of 2D colored shapes. Each image has exactly 6 objects that can be unambiguously identified by 6 colors, and the objects have random shapes(square or circle) and positions. Each image is associated with 20 questions asking about the shape or position of a certain object, 10 of which is non-relational questions that query the object by its unambiguous colors and another 10 are relational questions that query the object with furthest or closest relation to another unambiguous colored object. It is visually simpler than the CLEVR, but also requires the model capable of relational reasoning. Since the original dataset is not released, we generate a set following their detailed description, including 9800 images for training and 200 for testing.</p><p>The VQAv2 <ref type="bibr" target="#b8">[9]</ref> contains 204, 721 natural images from COCO <ref type="bibr" target="#b18">[19]</ref> and 1, 105, 904 free-form questions. Compared with its first version <ref type="bibr" target="#b3">[4]</ref>, this dataset focuses on reducing dataset biases through balanced pairs: for each question, there are pair of images which the answers to that question are different.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Implementation details</head><p>For the CLEVR dataset, we employ the same setting used in <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b13">14]</ref> to extract the image feature and words encoding. We first resize all images to 224 × 224, then ex-   tract the conv4 feature from ResNet-101 pre-trained on ImageNet. The resulting 1024 × 14 × 14 feature maps are concatenated with a 2-channel coordinate map. It is further fed into a single 3 × 3 convolution layer. The resulting 128 × 14 × 14 feature maps are passed through our ACMN module network. We encode the questions using a bidirectional LSTM <ref type="bibr" target="#b22">[23]</ref> with 1024-d hidden states for both directions. The hidden vector of Bi-LSTM at corresponding word position is considered as this word's encoding w. The maximum tree height of this dataset is 13, thus there a to-   The image features for VQAv2 are extracted by bottomup attention network <ref type="bibr" target="#b0">[1]</ref>, which is trained to detect objects on the Visual Genome dataset. The word vectors are also extracted from a 150-d bidirectional LSTM at corresponding positions. The maximum height of dependency parse trees in this dataset is 11. The hidden representation h of these 11 module instances are 1024-d vectors.</p><p>For VQAv2, We train our model on the training and validation split. For CLEVR and Sort-of-CLEVR dataset, only the training split is used. The model is trained with Adam optimizer <ref type="bibr" target="#b16">[17]</ref>. The base learning rate is 0.0001 and the batch-size is 32. The weight decay, β 1 and β 2 are 0, 0.9, 0.999 respectively, which are the default settings for Adam optimizer. <ref type="table" target="#tab_2">Table 2</ref> shows the performances of different works on CLEVR test set. The previous End-to-End modular network <ref type="bibr" target="#b10">[11]</ref> and Program Execution Engine <ref type="bibr" target="#b14">[15]</ref> are shorten as N2NMN and PE respectively. They both use the functional programs as groundtruth layout, and train their question parser with a sequence-to-sequence manner with strong supervision. They also have variants that are trained using semi or none supervision signals. The "N2NMN scratch" indicating the end-to-end modular network without layout supervision and the "N2NMN cloning expert" show the results of their model trained with full supervision. The "N2NMN policy search" gives this model's best results if it further trains the parser from "N2NMN cloning expert" with RL. It can be seen that our model outperforms all of these previous models by a large margin without using any dataset-specific layout, showing the good generalization capability of our ACMN. Our ACMN also beats the Program Execution Engine <ref type="bibr" target="#b14">[15]</ref> variant trained with semi-supervision (as "PE-semi-9K"). The PE-Strong <ref type="bibr" target="#b14">[15]</ref> used all program layouts as additional supervision signals, and the RN <ref type="bibr" target="#b21">[22]</ref> is a black-box model that lacks interpreting ability. Although our ACMN only obtains comparable results with Program Execution Engine <ref type="bibr" target="#b14">[15]</ref> with fully-supervision (as "PE-Strong") and Relation Network (as "RN") <ref type="bibr" target="#b21">[22]</ref>, our ACMN can provide more explicit reasoning results without layout supervision. <ref type="figure" target="#fig_3">Figure 4</ref> shows the promising intermediate reasoning results achieved by our ACMN. The images and the dependency parse trees are shown on the left. We highlight the regions with high attention weights, and slightly brighter the image if our model equally attends all of the regions. The first example shows that our model can first locate the "purple object", while the phase "same material" alone does not correspond to any object, our model doesn't focus on any specific region. Later, our model attends all object except the purple object given phase "any other things", and "are there" locate the objects that have the same material and predict the answer "yes". The second example illustrates the process of locating the "big thing" and the "metal sphere". Then our model composes their visual features to answer whether these two objects have the same color. <ref type="table">Table 3</ref> shows the comparisons among our model, its variants and prior works on Sort-of-CLEVR dataset. As described in <ref type="bibr" target="#b21">[22]</ref>, since the visual elements in this dataset are quite simple, a simple CNN+MLP baseline model can achieve over 94% accuracy for non-relational questions but fail for relational questions. We thus mainly focus on comparing results for relational questions. The results of two baselines (i.e. "CNN+MLP <ref type="bibr" target="#b21">[22]</ref>" and "CNN+RN <ref type="bibr" target="#b21">[22]</ref>" are originally reported in <ref type="bibr" target="#b21">[22]</ref>. The actual accuracy number for non-relational questions are not reported since both models achieve nearly 100%. We can see that our ACMN achieves superior results over two previous methods for answering relational questions that require the model has strong capability in relation reasoning rather than overfitting the dataset bias as previous works. <ref type="figure" target="#fig_5">Figure 5</ref> shows the resulting attention regions following the general dependency tree for the questions achieved by our ACMN, which clearly demonstrates its promising interpretable ability. The first example locates the "gray object", then it transforms its attention regions to its "furthest" ob-  <ref type="figure" target="#fig_3">Figure 4</ref>, the edges in dependency tree is drawn from head words to dependent words. The attended regions are highlighted for different nodes. jects. Our model successfully attends the correct objects in last steps to answer the question. The second example also attends the "closet" area of "blue object", and then correctly locate the gray circle object to answer the question.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Comparison with State-of-the-arts 4.3.1 CLEVR dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.2">Sort-of-CLEVR dataset</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.3">VQAv2 dataset</head><p>The results on test-std and test-dev of the VQAv2 dataset are shown in <ref type="table">Table 4</ref>. We compare our model with the first place method of the VQAv2 challenge. The first place method obtained their best with an ensemble of 30 networks, and their results are denoted as "1st ensemble <ref type="bibr" target="#b24">[25]</ref>" in <ref type="table">Table 4</ref>. The "1st single <ref type="bibr" target="#b24">[25]</ref>" show its performance of single network with exact same network architecture and hyper-parameters. Since they used the image features extracted by bottom-up attention network <ref type="bibr" target="#b0">[1]</ref>, we also use features provided by <ref type="bibr" target="#b0">[1]</ref> for fair comparison. Specifically, we use features of the top-36 proposal with highest object score as visual inputs and generate a 36-D attention vector. Our results are slightly lower than the best method on VQAv2. Note that we haven't applied tricks such as data augmentation, pretrained classifier, as described in <ref type="bibr" target="#b24">[25]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Ablation Studies</head><p>We show the accuracy of our model and its variants on Sort-of-CLEVR dataset in <ref type="table">Table 3</ref>.</p><p>Residual composition module By removing connection in our residual composition module, the accuracy drops 2.7% on relational question answering by comparing  <ref type="table">Table 4</ref>: The question answering accuracy on VQAv2 testdev and test-std. The "1st ensemble" and "1st single" denotes the first place method of the 2017 VQA Challenge with and without ensemble respectively.</p><p>"Ours-w/o residual" with "ours". Furthermore, we combine the residual and dense connection to form a dual-path treestructured network, resulting in a variant "Ours-DualPath". This network has more parameters and exploits previous nodes' knowledge in a more direct way. Specifically, we concatenate all previous hidden representation h and use an extra fully connected layer to project them into a 256-d feature vector. "Ours-DualPath" achieves 91.1% accuracy, indicates that the extra fully connected layer hurts the performance since nodes in a general dependency parse tree may contain duplicate information. Our residual connection can handle these trivial nodes, demonstrate the effectiveness of our residual composition. Adversarial attention module We also evaluate the results of other attention modules to demonstrate the effectiveness of our adversarial attention module. One commonly used attention module is the Relocate module in <ref type="bibr" target="#b10">[11]</ref> which used the soft-attention encoding applied in <ref type="bibr" target="#b10">[11]</ref>, resulting in our variant "Ours-relocate". Another option for attention module is to directly concatenate image features with the input attention maps att in instead of using an adversarial mask, that is "Ours-concat". The proposed adversarial attention module is demonstrated to obtain better question answering performance over these two attention alternatives, benefiting from the adversarial-mask driven exploration of unseen regions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this paper, we propose a novel ACMN module network equipped with an adversarial attention module and a residual composition module for visual question reasoning. In contrast to previous works that rely on the annotations or hand-crafted rules to obtain valid layouts, our ACMN model can automatically perform interpretable reasoning process over a general dependency parse tree from the question, which can largely broaden its application fields. The adversarial attention module encourages the model to attend the local visual evidence for each modifier relation while the residual composition module can learn to compose representations of children for the clausal predicate relation while retaining the information flow from its indirect child nodes. Experiments show that our model outperforms previous modular networks without using any specified groundtruth layouts or complicated hand-crafted rules.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: The modules in our ACMN: a) each ACMN module that is composed by an adversarial attention module and residual composition module; b) adversarial attention module; c) residual composition module. The blue arrows indicate the modifier relation and the yellow arrows represent the clausal predicate relation. Each node receives the output attention maps and the hidden features from its children, as well as the image feature and word encoding. The adversarial attention module is employed to generate a new attention map conditioned on image feature, word encoding and previous attended regions given by modifier-dependent children. The residual composition module is learned to evolve higher-level representation by integrating features of its children and local visual evidence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head></head><label></label><figDesc>The module f has three inputs: the image feature v, the word encoding w, and its children's outputs [</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: The statistic of clausal predicate relation and modifier relation in the questions of VQAv2 [9] dataset training split and CLEVR dataset [14] training split.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Two examples of the dependency trees of questions and corresponding regions attended by our model at each step on CLEVR dataset. The question is shown on the bottom. The image and dependency parse tree are shown on the left. The arrows in the dependency tree are drawn from the head words to the dependent words, The blue arrows indicate the modifier relation M , and the yellow arrows indicate the clausal Predicate relation P . The curved arrows point to the pruned leaf words that are not a noun. Thus word "there" and "have" is the root node for each example respectively. The regions with high attention weight are shown as bright areas in the images on the right. Those nodes without obvious bright region indicate our model equally attend all regions of the image, thus no specific salient regions correspond to this node.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>3 :</head><label>3</label><figDesc>Comparisons in terms of question answering accu- racy on the Sort-of-CLEVR dataset. tal of 13 node module instances. The hidden representation h of each module is a 256-d vector. The three-layer MLP have output sizes of 512, 1024 and 29 respectively. The size of images in Sort-of-CLEVR is 75×75, we used a four-layer CNN and each layer has a 3 × 3 kernel and 24- channel outputs to extract the image features. The resulting feature maps have the size of 24 × 8 × 8, so the output h ′ of the adversarial attention module is a 24-d vector. The out- put h of the residual composition module is a 256-d vector, and there are 5 instances of modules for each level of de- pendency tree. The words in a question are first embedded as a 300-d vector and then the whole question is encoded by a bidirectional LSTM [23], which has 150-d hidden units in both directions. The word encoding vector is represented by the LSTM hidden vector at its corresponding position.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :</head><label>5</label><figDesc>Figure 5: Examples of parse trees and corresponding regions attended by our ACMN on Sort-of-CLEVR dataset. Same with Figure 4, the edges in dependency tree is drawn from head words to dependent words. The attended regions are highlighted for different nodes.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Comparisons in terms of question answering accuracy on the CLEVR dataset. The performance of question types 
Exist, emphCount, Compare Integer, Query, Compare are reported on each column. LBP-SIG [33] and RN [22] only report 
total accuracy of question types Compare Integer, Query, Compare, their performance on these types are mereged. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="false"><head>Table</head><label></label><figDesc></figDesc><table></table></figure>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Buehler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Gould</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.07998</idno>
		<title level="m">Bottom-up and top-down attention for image captioning and visual question answering</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning to compose neural networks for question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The Association for Computational Linguistics</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1545" to="1554" />
		</imprint>
	</monogr>
	<note>HLT-NAACL</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Neural module networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Klein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="39" to="48" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">VQA: Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Antol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Agrawal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Mitchell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Mutan: Multimodal tucker fusion for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Ben-Younes</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cadene</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cord</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Thome</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">A fast and accurate dependency parser using neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">In EMNLP</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Universal stanford dependencies: A cross-linguistic typology</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>De Marneffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Dozat</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Silveira</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Haverinen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Ginter</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Nivre</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Manning</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">LREC</title>
		<imprint>
			<publisher>ELRA</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="4585" to="4592" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Multimodal compact bilinear pooling for visual question answering and visual grounding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fukui</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">EMNLP</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="457" to="468" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Making the V in VQA matter: Elevating the role of image understanding in Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Goyal</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Khot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Summers-Stay</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<publisher>IEEE Computer Society</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Learning to reason: End-to-end module networks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Andreas</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">A focused dynamic attention model for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Ilievski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<idno>abs/1604.01485</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Revisiting visual question answering baselines</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Jabri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Joulin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ECCV</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="volume">9912</biblScope>
			<biblScope unit="page" from="727" to="739" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">CLEVR: A diagnostic dataset for compositional language and elementary visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Inferring and executing programs for visual reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Johnson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hoffman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">B</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<monogr>
		<title level="m" type="main">Hadamard product for low-rank bilinear pooling</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J.-H</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">W</forename><surname>On</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Kim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ha</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B.-T</forename><surname>Zhang</surname></persName>
		</author>
		<idno>abs/1610.04325</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICLR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Ask me anything: Dynamic memory networks for natural language processing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kumar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Irsoy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Ondruska</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Iyyer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bradbury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Gulrajani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Paulus</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1378" to="1387" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Maire</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Hays</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ramanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">L</forename><surname>Zitnick</surname></persName>
		</author>
		<title level="m">Microsoft COCO: Common Objects in Context</title>
		<meeting><address><addrLine>Cham</addrLine></address></meeting>
		<imprint>
			<publisher>Springer International Publishing</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="740" to="755" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Hierarchical question-image co-attention for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Batra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="289" to="297" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Ask your neurons: A neural-based approach to answering questions about images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Rohrbach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
		<title level="m" type="main">A simple neural network module for relational reasoning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Santoro</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Raposo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">G T</forename><surname>Barrett</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Malinowski</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Pascanu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">P</forename><surname>Lillicrap</surname></persName>
		</author>
		<idno>abs/1706.01427</idno>
		<imprint>
			<date type="published" when="2017" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Bidirectional recurrent neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Schuster</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Paliwal</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Trans. Sig. Proc</title>
		<imprint>
			<biblScope unit="volume">45</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2673" to="2681" />
			<date type="published" when="1997-11" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Where to look: Focus regions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">J</forename><surname>Shih</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Hoiem</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<monogr>
		<title level="m" type="main">Tips and tricks for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Anderson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
		<idno>abs/1708.02711</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Graph-structured representations for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Teney</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Hengel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<monogr>
		<title level="m" type="main">FVQA: fact-based visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Van Den Hengel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">R</forename><surname>Dick</surname></persName>
		</author>
		<idno>abs/1606.05433</idno>
		<imprint>
			<date type="published" when="2016" />
			<publisher>CoRR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Object region mining with adversarial erasing: A simple classification to semantic segmentation approach</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M.-M</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017-07" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b28">
	<analytic>
		<title level="a" type="main">Dynamic memory networks for visual and textual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xiong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Merity</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="2397" to="2406" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<monogr>
		<title level="m" type="main">Attend and Answer: Exploring Question-Guided Spatial Attention for Visual Question Answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Saenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Ask</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer International Publishing</publisher>
			<biblScope unit="page" from="451" to="466" />
			<pubPlace>Cham</pubPlace>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">Stacked attention networks for image question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Gao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Smola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016-06" />
			<biblScope unit="page" from="21" to="29" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Yunpeng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Jianan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huaxin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Xiaojie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Shuicheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Jiashi</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1707.01629</idno>
		<title level="m">Dual path networks</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Structured attentions for visual question answering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Ma</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017-10" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Visual7w: Grounded question answering in images</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Groth</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">S</forename><surname>Bernstein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="4995" to="5004" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Knowledge Acquisition for Visual Question Answering via Iterative Querying</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">J</forename><surname>Lim</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
