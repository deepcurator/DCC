<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:59+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Decoupled Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Weiyang</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhen</forename><surname>Liu</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhiding</forename><surname>Yu</surname></persName>
							<affiliation key="aff1">
								<orgName type="institution">NVIDIA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Bo</forename><surname>Dai</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Rongmei</forename><surname>Lin</surname></persName>
							<affiliation key="aff2">
								<orgName type="institution">Emory University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yisen</forename><surname>Wang</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff3">
								<orgName type="institution">Tsinghua University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">James</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><surname>Le Song</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">Georgia Institute of Technology</orgName>
							</affiliation>
							<affiliation key="aff4">
								<orgName type="department">Ant Financial</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Decoupled Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head><p>Inner product-based convolution has been a central component of convolutional neural networks (CNNs)   </p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Convolutional neural networks have pushed the boundaries on a wide variety of vision tasks, including object recognition <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b20">21,</ref><ref type="bibr" target="#b3">4]</ref>, object detection <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b17">18]</ref>, semantic segmentation <ref type="bibr" target="#b14">[15]</ref>, etc. A significant portion of recent studies on CNNs focused on increasing network depth and representation ability via improved architectures such as shortcut connections <ref type="bibr" target="#b3">[4,</ref><ref type="bibr" target="#b6">7]</ref> and multi-branch convolution <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b24">25]</ref>. Despite these advances, understanding how convolution naturally leads to discriminative representation and good generalization remains an interesting problem.</p><p>Current CNNs often encode the similarity between a patch x and a kernel w via inner product. The formulation of inner product w, x = w ⊤ x couples the semantic difference (i.e., inter-class variation) and the intra-class variation in one unified measure. As a result, when the inner product between two samples is large, one can not tell * Equal contributions. Email:{wyliu,liuzhen1994}@gatech.edu whether the two samples have large semantic/label difference or have large intra-class variation. In order to better study the properties of CNN representation and further improve existing frameworks, we propose to explicitly decouple semantic difference and intra-class variation <ref type="bibr" target="#b0">1</ref> . Specifically, we reparametrize the inner product with the norms and the angle, i.e., w 2 x 2 cos(θ (w,x) ). Our direct intuition comes from the the observation in <ref type="figure" target="#fig_0">Fig. 1</ref> where angle accounts for semantic/label difference and feature norm accounts for intra-class variation. The larger the feature norm, the more confident the prediction. Such naturally decoupled phenomenon inspires us to propose the decoupled convolution operators. We hope that decoupling norm and angle in inner product can better model the intra-class variation and the semantic difference in deep networks.</p><p>On top of the idea to decouple the norm and the angle in an inner product, we propose a novel decoupled network (DCNet) by generalizing traditional inner productbased convolution operators ( w x cos(θ (w,x) )) to decoupled operators. To this end, we define such operator as multiplication of a function of norms h( w , x ) and a function of angle g(θ (w,x) ). The decoupled operator provides a generic framework to better model the intraclass variation and the semantic difference, and the original CNNs are equivalent to setting h( w , x ) as w x and g(θ (w,x) ) as cos(θ <ref type="bibr">(w,x)</ref> ). The magnitude function h( w , x ) models the intra-class variation while the angular function g(θ (w,x) ) models the semantic difference.</p><p>From the decoupling point of view, the original CNNs make a strong assumption that the intra-class variation can be linearly modeled via the multiplication of norms and the semantic difference is described by the cosine of the angle. However, this modeling approach is not necessarily optimal for all tasks. With the decoupled learning framework, we can either design the decoupled operators based on the task itself or learn them directly from data. The advantages of DCNets are in four aspects. First, DCNets not only allow us to use some alternative functions to better model the intraclass variation and the semantic difference, but they also enable us to directly learn these functions rather than fixing them. Second, with bounded magnitude functions, DCNets can improve the problem conditioning as analyzed in <ref type="bibr" target="#b12">[13]</ref>, and therefore DCNets can converge faster while achieving comparable or even better accuracy than the original CNNs. Third, some instances of DCNets can have stronger robustness against adversarial attacks. We can squeeze the feature space of each class with a bounded h(·), which can bring certain robustness. Last, the decoupled operators are very flexible and architecture-agnostic. They could be easily adapted to any kind of architectures such as VGG <ref type="bibr" target="#b19">[20]</ref>, GoogleNet <ref type="bibr" target="#b20">[21]</ref> and ResNet <ref type="bibr" target="#b3">[4]</ref>. Specifically, we propose two different types of decoupled convolution operators: bounded operators and unbounded operators. We present multiple instances for each type of decoupled operators. Empirically, the bounded operators may yield faster convergence and better robustness against adversarial attacks, and the unbounded operators may have better representational power. These decoupled operators can also be either smooth or non-smooth, which can yield different behaviors. Moreover, we introduce a novel concept -operator radius for the decoupled operators. The operator radius describes the critical change of the derivative of the magnitude function h(·) with respect to the input x . By jointly learning the operator radius via back-propagation, we further propose learnable decoupled operators. Moreover, we show some alternative ways to optimize these operators that improve upon standard backpropagation. Our contributions can be summarized as:</p><p>• Inspired by the observation that CNN-learned features are naturally decoupled, we propose an explicitly decoupled framework to study neural networks.</p><p>• We show that CNNs make a strong assumption to model the intra-class and inter-class variation, which may not be optimal. By decoupling the inner product, we are able to design more effective magnitude and angular functions rather than the original convolution for different tasks.</p><p>• In comparison to standard CNNs, DCNets have easier convergence, better accuracy and stronger robustness.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Works</head><p>There are an increasing number of works <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b10">11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b23">24,</ref><ref type="bibr" target="#b25">26,</ref><ref type="bibr" target="#b8">9]</ref> that focus on improving the classification layer in order to increase the discriminativeness of learned features. <ref type="bibr" target="#b11">[12]</ref> models the angular function for each class differently and defines a more difficult task than classification, improving the network generalization. Built upon <ref type="bibr" target="#b11">[12]</ref>, <ref type="bibr" target="#b10">[11]</ref> further normalizes the weights of the last fully connected layer (i.e., classification layer) and reported improved results on face recognition. <ref type="bibr" target="#b22">[23,</ref><ref type="bibr" target="#b16">17,</ref><ref type="bibr" target="#b23">24]</ref> normalize the input features before entering the last fully connected layer, achieving promising performance on face recognition. However, these existing works can be viewed as heuristic modifications and are often restricted to the last fully connected layer. In contrast, the decoupled learning provides a more general and systematic way to study the CNNs. In our framework, the previous work can be viewed as proposing a new magnitude function h( w , x ) or angular function g(θ (w,x) ) for the last fully connected layer. For example, normalizing the weights is to let h( w , x ) be x and normalizing the input is equivalent to h( w , x ) = w .</p><p>[13] proposes a deep hyperspherical learning framework which directly makes h( w , x ) equal to 1 such that all the activation outputs only depend on g(θ (w,x) ). The framework provides faster convergence compared to the original CNNs, but is somehow restricted in the sense that h( w , x ) is only allowed to be 1, and therefore can be sub-optimal in some cases. From the decoupling perspective, hyperspherical learning only cares about the semantic difference and aims to compress the intra-class variation to a space that is as small as possible, while the decoupled framework focuses on both. As a non-trivial generalization of <ref type="bibr" target="#b12">[13]</ref>, our decoupled network is a more generic and unified framework to model both intra-class variation and semantic difference, providing the flexibility to design or learn both magnitude function h(·) and angular function g(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Decoupled Networks</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Reparametrizing Convolution via Decoupling</head><p>For a conventional convolution operator f (·, ·), the output is calculated by the inner product of the input patch x and the filter w (both x and w are vectorized into columns):</p><formula xml:id="formula_0">f (w, x) = w, x = w ⊤ x<label>(1)</label></formula><p>which can be further formulated as a decoupled form that separates the norm and the angle:</p><formula xml:id="formula_1">f (w, x) = w x cos(θ (w,x) )<label>(2)</label></formula><p>where θ (w,x) is the angle between x and w. Our proposed decoupled convolution operator takes the general form of</p><formula xml:id="formula_2">f d (w, x) = h( w , x ) · g(θ (w,x) )<label>(3)</label></formula><p>which explicitly decouples the norm of w, x and the angle between them. We define h( w , x ) as the magnitude function and g(θ (w,x) ) as the angular activation function. It is easy to see that the decoupled convolution operator includes the original convolution operator as a special case.</p><p>As illustrated in <ref type="figure" target="#fig_0">Fig. 1</ref>, the semantic difference and intraclass variation are usually decoupled and very suitable for this formulation. Based on the decoupled operator, we propose several alternative ways to model the semantic difference and intra-class variation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Decoupled Convolution Operators</head><p>We discuss how to better model the intra-class variation, and then give a few instances of the decoupled operator.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.1">On Better Modeling of the Intra-class Variation</head><p>Hyperspherical learning <ref type="bibr" target="#b12">[13]</ref> has discussed the modeling of the inter-class variation (i.e., the angular function). The design of angular function g(·) is relatively easy but restricted, because it only takes the angle as input. In contrast, the magnitude function h(·) takes the norm of w and the norm of x as two inputs, and therefore it is more complicated to design. w is the intrinsic property of a kernel itself, corresponding to the importance of the kernel rather than the intra-class variation of the inputs. Therefore, we tend not to include w into the magnitude function h(·). Moreover, removing w from h(·) indicates that all kernels (or operators) are assigned with equal importance, which encourages the network to make decision based on as many kernels as possible and therefore may make the network generalize better. However, incorporating the kernel importance to the network learning can improve the representational power and may be useful when dealing with a large-scale dataset with numerous categories. By combining w back to h(·), the operators become weighted decoupled operators. There are multiple ways of incorporating w back to the magnitude function. We will discuss and empirically evaluate these variants later.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.2">Bounded Decoupled Operators</head><p>The output of the bounded operators must be bounded by a finite constant regardless of its input and kernel, namely |f d (w, x)| ≤ c where c is a positive constant. For simplicity, we first consider the decoupled operator without the norm of the weights (i.e., w is not included in h(·)). Hyperspherical Convolution. If we let h( w , x ) = α, we will have the hyperspherical convolution (SphereConv) with the following decoupled form:</p><formula xml:id="formula_3">in [−α, α]</formula><p>. Usually, we can use α = 1, which reduces to SphereConv <ref type="bibr" target="#b12">[13]</ref> in this case. Geometrically, SphereConv can be viewed as projecting w and x to a hypersphere and then performing inner product (if g(θ) = cos(θ)). Based on <ref type="bibr" target="#b12">[13]</ref>, SphereConv improves the problem conditioning in neural networks, making the network converge better. Hyperball Convolution. The hyperball convolution (BallConv) uses h( w , x ) = α min( x , ρ)/ρ as its magnitude function. The specific form of the BallConv is</p><formula xml:id="formula_4">f d (w, x) = α · min( x , ρ) ρ · g(θ (w,x) )<label>(5)</label></formula><p>where ρ controls the saturation threshold for the input norm x and α scales the output range. When x is larger than ρ, then the magnitude function will be saturate and output α. When x is smaller than ρ, the magnitude function grows linearly with x . Geometrically, BallConv can be viewed as projecting w to a hypersphere and projecting the input x to a hyperball, and then performing the inner product (if g(θ) = cos(θ)). Intuitively, BallConv is more robust and flexible than SphereConv in the sense that SphereConv may amplify the x with very small x , because x with small x and the same direction as w could still produce the maximum output. It makes SphereConv sensitive to perturbations to x with small norm. In contrast, BallConv will not have such a problem, because the multiplicative factor x can help to decrease the output if x is small. Moreover, small x indicates that the local patch is not informative and should not be emphasized. In this sense, BallConv is better than SphereConv. In terms of convergence, the BallConv can still help the network convergence because its output is bounded with the same range as SphereConv. Hyperbolic Tangent Convolution. We present a smooth decoupled operator with bounded output called hyperbolic tangent convolution (TanhConv). The TanhConv uses a hyperbolic tangent function to replace the step function in the BallConv and can be formulated as</p><formula xml:id="formula_5">f d (w, x) = α tanh x ρ · g(θ (w,x) )<label>(6)</label></formula><p>where tanh(·) denotes the hyperbolic tangent function and ρ is parameter controlling the decay curve. The TanhConv can be viewed as a smooth version of BallConv, which not only shares the same advantages as BallConv but also has more convergence gain due to its smoothness <ref type="bibr" target="#b0">[1]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.3">Unbounded Decoupled Operators</head><p>Linear Convolution. One of the simplest unbounded decoupled operators is the linear convolution (LinearConv):</p><formula xml:id="formula_6">f d (w, x) = α x · g(θ (w,x) )<label>(7)</label></formula><p>where α controls the output scale. LinearConv differs the original convolution in the sense that it projects the weights to a hypersphere and has a parameter to control the slope.  Segmented Convolution. We propose a segmented convolution (SegConv) which takes the following form:</p><formula xml:id="formula_7">f d (w, x) = α x · g(θ (w,x) ), 0 ≤ x ≤ ρ (β x + αρ − βρ) · g(θ (w,x) ), ρ &lt; x<label>(8)</label></formula><p>where α controls the slope when x ≤ ρ and β controls the slope when x &gt; ρ. ρ is the change point of the gradient of the magnitude function w.r.t. x . SegConv is a flexible multi-range linear function corresponding to x . Both LinearConv and BallConv are special cases of SegConv. Logarithm Convolution. We present another smooth decoupled operator with unbounded output, logarithm convolution (LogConv). LogConv uses a Logarithm function for the norm of the input x and can be formulated as</p><formula xml:id="formula_8">f d (w, x) = α log(1 + x ) · g(θ (w,x) )<label>(9)</label></formula><p>where α controls the base of logarithm and is used to adjust the curvature of the logarithm function. Mixed Convolution. Mixed convolution (MixConv) combines multiple decoupled convolution operators and enjoys better flexibility. Because the mixed convolution has many possible combinations, we only consider the additive combination of LinearConv and LogConv as an example:</p><formula xml:id="formula_9">f d (w, x) = α x + β log(1 + x ) · g(θ (w,x) )<label>(10)</label></formula><p>which combines LogConv and LinearConv, becoming more flexible than both original operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.4">Properties of Decoupled Operators</head><p>Operator Radius. Operator radius is defined to describe the gradient change point of the magnitude function. Operator radius differentiates two stages of the magnitude function. The two stages usually have different gradient ranges and therefore behave differently during optimization. We let ρ denote the operator radius in each decoupled operator. For BallConv, when x is smaller than ρ, the magnitude function will be activated and it will grow with x linearly. When x is larger than ρ, then the magnitude function will be deactivated and output a constant. For SegConv, x = ρ is the change point of the magnitude function's slope. The operator radius of some decoupled operators (SphereConv, LinearConv, LogConv) is defined to be zero, indicating that they have no operator radius. The decoupled operator with non-zero operator radius is similar to a gated operator where x = ρ serves as the switch. Boundedness. The Boundedness of a decoupled operator may affect its convergence speed and robustness. <ref type="bibr" target="#b12">[13]</ref> shows that using a bounded operator can improve the convergence due to two reasons. First, bounded operators lead to better problem conditioning in training a deep network via stochastic gradient descent. Second, bounded operators make the variance of the output small and partially address the internal covariate shift problem. The bounded operators can also constrain the Lipschitz constant of a neural network, making the entire network more smooth. The Lipschitz constant of a neural network is shown to be closely related to its robustness against adversarial perturbation <ref type="bibr" target="#b5">[6]</ref>. In contrast, the unbounded operators may have stronger approximation power and flexibility than the bounded ones.</p><p>Smoothness. The smoothness of the magnitude function is closely related to the approximation ability and the convergence behavior. In general, using a smooth magnitude function could have better approximation rate <ref type="bibr" target="#b15">[16]</ref> and may also lead to more stable and faster convergence <ref type="bibr" target="#b0">[1]</ref>. However, a smooth magnitude function may also be more computationally expensive, since it could be more difficult to approximate a smooth function with polynomials.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Geometric Interpretations</head><p>All the decoupled convolution operators have very clear geometric interpretations, as illustrated in <ref type="figure" target="#fig_1">Fig. 2</ref>. Because all decoupled operators normalize the kernel weights, all the weights are already on the unit hypersphere. SphereConv also projects the input vector x on the unit hypersphere and then computes the similarity between w and x based on the geodesic distance on the hypersphere (multiplied by a scaling factor α). Therefore, its output is bounded from −α to α and only depends on the directions of w and x (suppose g <ref type="figure">(θ (w,x)</ref> ) is in the range of [−1, 1]).</p><p>BallConv first projects the input vector x to a hyperball and then computes the similarity based on the projected x inside the hyperball and the normalized w on surface of the hyperball. Specifically, BallConv projects x to the hypersphere if x &gt; ρ. TanhConv is a smoothed BallConv and has similar geometric interpretation, but TanhConv is differentible everywhere and has soft boundary around the operator radius x = ρ. TanhConv can be viewed as performing projection to a soft hyperball.</p><p>SegConv is more flexible than both SphereConv and BallConv. By using certain parameters, SegConv can reduce to either SphereConv or BallConv. SegConv essentially adjusts the norm of the input x with a multi-range lin- ear multiplicative factor. Geometrically, such a factor will either push the vector close to the hypersphere or away from the hypersphere depending on the selection of α and β. For example, we consider the case where α = 1 and 0 &lt; β &lt; 1. When x ≤ ρ, the magnitude function h(·) in SegConv will directly output x . When x &gt; ρ, h(·) in SegConv will output a value smaller than x , as shown in <ref type="figure" target="#fig_1">Fig. 2</ref>. LinearConv is the simplest unbounded operator and its magnitude function grows linearly with x . When α = 1, the magnitude function h(·) in LinearConv simply outputs x , which does not perform any projection.</p><p>LogConv use a logarithm function to transform the norm of the input x. After such nonlinear transformation on x, LogConv computes similarity based on the transformed input x and the normalized weights on a hypersphere.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Design of the Angular Activation Function</head><p>The design of the angular function g(θ (w,x) ) mostly follows the deep hyperspherical learning <ref type="bibr" target="#b12">[13]</ref>. We use four different types of g(θ (w,x) ) in this paper. The linear angular activation is defined as</p><formula xml:id="formula_10">g(θ (w,x) ) = − 2 π θ (w,x) + 1,<label>(11)</label></formula><p>whose output grows linearly with the angle θ (w,x) . The cosine angular activation is defined as</p><formula xml:id="formula_11">g(θ (w,x) ) = cos(θ (w,x) ),<label>(12)</label></formula><p>which is also used by the original convolution operator. Moreover, the sigmoid angular activation is defined as</p><formula xml:id="formula_12">g(θ (w,x) ) = 1 + exp(− π 2k ) 1 − exp(− π 2k ) · 1 − exp( θ (w,x) k − π 2k ) 1 + exp( θ (w,x) k − π 2k ) ,<label>(13)</label></formula><p>where k controls the curvature. Additionally, we also propose a square cosine angular activation function:</p><formula xml:id="formula_13">g(θ (w,x) ) = sign(cos(θ)) · cos 2 (θ),<label>(14)</label></formula><p>which can encourage a degree of angular margin near the decision boundary and may improve network generalization. In addition to fixing these angular activations prior to training, we can also jointly learn the parameter k in the sigmoid activation using back-propagation, which is a learnable angular activation <ref type="bibr" target="#b12">[13]</ref>. <ref type="figure" target="#fig_2">Fig. 3</ref> shows the curves of these angular activation functions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5.">Weighted Decoupled Operators</head><p>All the decoupled operators we have discussed normalize the kernel weights w and the magnitude functions do not take the weights into consideration. Although empirically we find that the standard decoupled operators work better than the weighted ones in most cases, we still consider weighted decoupled operators, which incorporate w into the magnitude function, in order to improve the operator's flexibility. We propose two straightforward ways to combine w : linear and nonlinear. Linearly Weighted Decoupled Operator. Similar to the original inner produce-based convolution, we can directly multiply the norm of weights into the magnitude function, which makes the decoupled operators linearly weighted. For example, SphereConv will become f d (w, x) = α w · g(θ (w,x) ). Notably, linearly weighted LinearConv will become the original inner produce-based convolution. Nonlinearly Weighted Decoupled Operator. Compared to linearly weighted decoupled operators, the norm of the weights are incorporated into the magnitude function in a nonlinear way. Taking TanhConv as an example, we could formulate the nonlinearly weighted TanhConv as</p><formula xml:id="formula_14">f d (w, x) = α tanh( 1 ρ x · w ) · g(θ (w,x) ).<label>(15)</label></formula><p>We can also formulate the nonlinearly weighted TanhConv in an alternative way:</p><formula xml:id="formula_15">f d (w, x) = α tanh( 1 ρ w ) · tanh( 1 ρ x ) · g(θ (w,x) ).<label>(16)</label></formula><p>The first nonlinearly weighted formulation couples x and w by multiplication and then perform a nonlinear transformation, while the second one performs nonlinear transformations separately for x and w , and then multiplies them. In practice, the linearly weighted operators are favored over nonlinearly weighted ones due to the simplicity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.6.">Learnable Decoupled Operators</head><p>Because our decoupled operators usually have hyperparameters, we usually need to do cross-validation in order to choose suitable parameters, which is time-consuming and sub-optimal. To address this, we can learn these parameters jointly with network weight training via back-propagation. We propose learnable decoupled operators which perform hyperparameter learning with h(·) and g(θ <ref type="figure">(w,x)</ref> ). For example, <ref type="bibr" target="#b12">[13]</ref> proposed to learn the hyperparameters of sigmoid angular function. By making both h( w , x ) and g(θ <ref type="figure">(w,x)</ref> ) learnable, we can greatly enhance the representational power and flexibility.</p><p>However, making the decoupled operators too flexible (i.e., too many learnable parameters) may require a prohibitive amount of training data to achieve good generalization. In order to achieve an effective trade-off, we only investigate learning the operator radius ρ via backpropagation during the network training.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Improving the Optimization for DCNets</head><p>We propose several tricks to improve the optimization of DCNets and enable DCNets to converge to a better local minima. More analysis and discussion of weight projection and weight gradients are provided in Appendix G.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Weight Projection</head><p>The forward pass of DCNets is not dependent on the norm of the weights w , because the decoupled operators take the normalized weights as input. However, w will significantly affect the backward pass. Taking SphereConv as an example, we compute the gradient w.r.t. w:</p><formula xml:id="formula_16">∂ ∂w ŵ ⊤x =x −ŵ ⊤x ·ŵ w<label>(17)</label></formula><p>whereŵ = w/ w andx = x/ x . In comparison, w will not affect the gradient w.r.t w in inner product. From Eq. <ref type="formula" target="#formula_0">(17)</ref>, large w can make the gradients very small so that the backward pass is not able to update the weights effectively. To address this issue, we propose weight projection to control the norm of the weights. Weight projection performs w ← s ·ŵ every certain number of iterations where ← denotes the replacement operation. s is a positive constant which controls the norm of the gradient (we use s = 1 in our experiments). In general, larger s leads to smaller gradients. Note that, weight projection cannot be used in the weighted decoupled operators, because w will affect the forward pass. We can only apply weight projection to our standard decoupled operators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weighted Gradients</head><p>From Eq. <ref type="formula" target="#formula_0">(17)</ref>, we observe that we could simply multiply w to Eq. <ref type="bibr" target="#b16">(17)</ref> to eliminate the effect of w on the backward pass. We update the weights with the following:</p><formula xml:id="formula_17">∆w = w · ∂ ∂w ŵ ⊤x =x −ŵ ⊤x ·ŵ<label>(18)</label></formula><p>which does not depend on w and is called weighted gradients. Using the proposed weighted gradients for backpropagation, we can also prevent the gradients from being affected by the norm of the weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Pretraining as a Better Initialization</head><p>We find that DCNets may sometimes be trapped into a bad local minima and yield a less competitive accuracy while trained on large-scale datasets (e.g., ImageNet). Because the decoupled operators have stronger nonlinearity, its loss landscape may be more complex than the original convolution. The most straightforward way to improve the optimization is to use a better initialization. To this end, we use a CNN model that has the same structure and is pretrained on the same training set to initialize the DCNet.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Discussions</head><p>Why Decoupling? Decoupling the intra-class and interclass variation gives us the flexibility to design better models that are more suitable for a given task. Inner productbased convolution is computationally attractive but not necessarily optimal. The original convolution makes an assumption that the intra-class and inter-class variations are modeled by h( w , x ) = w x and g(θ) = cos(θ), respectively. Such assumptions may not be optimal. h(·) and g(·) can be task-driven in our novel decoupled framework. Flexibility of Decoupled Operators. There are numerous design options for the magnitude and angular function. The original convolution can be viewed as a special decoupled operator. Moreover, we can parametrize a decoupled operator with a few learnable parameters and learn them via back-propagation. However, there is a delicate tradeoff between the size of the training data, the generalization of the network and the flexibility of the decoupled operator. Generally, given a large enough dataset, the network generalization improves with more learnable parameters. A Unified Learning Framework for CNNs. The decoupled formulation provides a unified learning framework for CNNs. Consider a standard CNN with ReLU, we write the convolution and ReLU as max(0, w x cos(θ)) which can be written as w x · max(0, cos(θ)). Such formulation can be viewed as a decoupled operator where h( w , x ) = w x and g(θ) = max(0, cos(θ)). We can jointly consider the convolution operator and nonlinear activation in the decoupled framework. It is possible to learn one single function g(·) that represents both angular activation and the nonlinearity, which is why the square cosine angular activation works well without ReLU. Network Regularization. In most instances of DCNets, the ℓ 2 weight decay is no longer suitable. <ref type="bibr" target="#b12">[13]</ref> uses an orthonormal constraint W ⊤ W − I 2 F to regularize the network, where W is the weight matrix whose columns are the kernel weights and I is identity matrix. We also propose an orthogonal constraint</p><formula xml:id="formula_18">W ⊤ W − diag(W ⊤ W ) 2</formula><p>F . Network Architecture. Due to the non-linear nature of DCNet, the performance of specific h(·) and g(·) is dependent on the choice of architecture. An interesting challenge for future work is to inverstigate the link between the architecture and the choise of h(·) and g(·).</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments and Results</head><p>General. We evaluate both accuracy and robustness of DCNets on objection recognition. For all decoupled operators, we use the standard softmax loss if not otherwise specified. Training. The architecture for each task and the training details are given in Appendix A. For CIFAR-10 and CIFAR-100, the network is trained by ADAM with 128 batch size. The learning rate starts from 0.001. For ImageNet-2012, we use SGD with momentum 0.9 to train the network. The batch size is 40. The learning rate starts from 0.1. For the adversarial attacks, the network is trained via ADAM. All learning rates are divided by 10 when the error plateaus. Implementation Details. For all decoupled operators that have non-zero operator radius (i.e., ρ = 0), we will learn the operator radius from the training data via back-propagation. More details are provided in Appendix B.</p><p>6.1. Object Recognition 6.1.1 CIFAR-10 and CIFAR-100 Weighted Decoupled Operators. We first compare the weighted decoupled operators and the standard ones. Because the weights are incorporated into the forward pass in the weighted decoupled operators, the optimization tricks like weight projection and weighted gradients are not applicable. Therefore, the weighted operators simply use the conventional gradients to perform back-propagation. For standard decoupled operators, we show the results using standard optimization, weight projection and weight gradients. From the results of TanhConv in <ref type="table">Table 1</ref>  Learning without Batch Normalization. Batch Normalization (BN) <ref type="bibr" target="#b7">[8]</ref> is usually crucial for training a wellperforming CNN, but the results in <ref type="table" target="#tab_2">Table 2</ref> show that our decoupled operators can perform much better than the original convolution even without BN. Learning without ReLU. Our decoupled operators naturally have strong nonlinearity, because our decoupled convolution is no longer a linear matrix multiplication. In <ref type="table">Table 3</ref>, square cosine angular activation works extremely well in plain CNN-9, even better than the networks with ReLU. The results show that using suitable h(·) and g(·) can lead  <ref type="table">Table 3</ref>: Testing error rate (%) of plain CNN-9 on CIFAR-100. Note that, BN is used in all compared models. Baseline is the original plain CNN-9.</p><p>Comparison among Different Decoupled Operators. We compare different decoupled operators on both plain CNN-9 and ResNet-32. All the compared decoupled operators are unweighted and use weight projection during optimization. The standard softmax loss and BN are used in all networks. For plain CNN-9, we compare the case with and without ReLU. The results in <ref type="table">Table 3</ref> show that DCNets significantly outperform the baseline. In particular, our DCNet with SegConv and square cosine can achieve 24.29% even without ReLU, which is even better than the networks with ReLU. For ResNet-32, our DCNets also consistently outperform the baseline with a considerable margin. The results further verify that the intra-class and inter-class variation assumptions of the original CNN are not optimal.   Convergence. We also evaluate the convergence of DCNets using the architecture of ResNet-32. The convergence curves in <ref type="figure" target="#fig_3">Fig. 4</ref> show that the decoupled operators are able to converge and generalize better than original convolution operators on CIFAR-100 dataset.  Comparison to the state-of-the-art. <ref type="table" target="#tab_7">Table 5</ref> shows that our DCNet-32 has very competitive accuracy compared to ResNet-1001. In order to achieve best accuracy, we use the weight-normalized softmax loss <ref type="bibr" target="#b12">[13]</ref>. We also find that using SGD further improves the accuracy of DCNets. Experiments on SGD-trained models are provided in Appendix F.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.2">ImageNet-2012</head><p>Standard ResNet. We first evaluate the DCNets with the standard ResNet-18. All presented decoupled operators use the cosine angular activation. <ref type="bibr" target="#b12">[13]</ref> shows that SphereConv can perform comparably to the baseline on ImageNet only when the network is wide enough. Using the weight projection and pretrained model initialization, SphereConv is comparable to the baseline even on narrow networks. Most importantly, TanhConv and LinearConv achieve better accuracy than the baseline ResNet. The learned filters of DCNets on ImageNet are also provided in Appendix E. Modified ResNet. We also evaluate decoupled operators with a modified ResNet, similar to SphereFace networks <ref type="bibr" target="#b10">[11]</ref>, to better show the advantages of decoupled operators. DCNets can be trained from scratch and outperform the baseline by 1%. Moreover, DCNets can converge stably in very challenging scenarios. From <ref type="table" target="#tab_9">Table 6</ref>  </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Robustness against Adversarial attacks</head><p>We evaluate the robustness of DCNets. DCNets in this subsection use standard gradients and are trained without any optimization trick. Fast gradient sign method (FGSM) <ref type="bibr" target="#b2">[3]</ref> and basic iterative method (BIM) <ref type="bibr" target="#b9">[10]</ref> are used to attack the networks. Experimental details and more experiments are given in Appendix A and Appendix C, respectively.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.1">White-box Adversarial Attacks</head><p>We run white-box attacks on both naturally trained models and FGSM-trained models on CIFAR-10 (results shown in <ref type="table" target="#tab_10">Table 7</ref>). "None" attacks mean that all the testing samples are normal. For naturally trained models, all DCNet variants show significantly better robustness over the baseline, with naturally trained TanhConv being most resistant. With adversarial training, while DCNets achieve the best robustness, SphereConv is particularly resistant against BIM attack. We speculate that the tight spherical constraint strongly twists the data manifold so that the adversarial gradient updates can only result in small gains.  <ref type="table">Table 8</ref>: Black-box attacks on CIFAR-10. Performance is measured in accuracy (%). The first three rows are results of naturally trained models, and the last three rows are results of adversarially trained models.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.2">Black-box Adversarial Attacks</head><p>We run black-box attacks on naturally-trained and FGSM-trained models on CIFAR-10 (see <ref type="table">Table 8</ref>). With natural training, it is surprising that BallConv and TanhConv do not show an advantage over the baseline, while SphereConv performs the best. The strongly nonlinear landscape of BallConv and TanhConv may be too difficult to be optimized without adversarial training. SphereConv, with a tighter geometric constraint, is able to withstand adversarial attacks without adversarial training. With adversarial training, SphereConv is less resistant against adversarial attacks than the baseline. BallConv and TanhConv, however, show significant advantage over the baseline. Our observation that adversarial training compromises the robustness of SphereConv matches the conclusion made by <ref type="bibr" target="#b21">[22]</ref>. Since SphereConv enforces a tight constraint of output vectors, the landscape around some data points will be dramatically changed during adversarial training. BallConv and TanhConv are less constrained and thus can fit adversarial examples without detrimental changes in the landscapes.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Concluding Remarks</head><p>This paper proposes a decoupled framework for learning neural networks. The decoupled formulation enables us to design or learn better decoupled operators than the original convolution. We argue that standard CNNs do not constitute an optimal decoupled design in general.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 :</head><label>1</label><figDesc>Figure 1: CNN learned features are naturally decoupled. These 2D features are output directly from the CNN by setting the feature dimension as 2.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: Geometric interpretations for decoupled convolution operators. Green denotes the original vectors, and red denotes the projected vectors.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Magnitude function (ρ = 1) and angular activation function.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Convergence.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head></head><label></label><figDesc>, weighted decoupled operators do not show obvious advantages.</figDesc><table>Method 
Error 

Linearly Weighted Decoupled Operator 
22.95 
Nonlinearly Weighted Decoupled Operator (Eq. (15)) 
23.03 
Nonlinearly Weighted Decoupled Operator (Eq. (16)) 
23.38 
Decoupled Operator (Standard Gradients) 
23.09 
Decoupled Operator (Weight Projection) 
21.17 
Decoupled Operator (Weighted Gradients) 
21.45 

Table 1: Evaluation of weighted operators (TanhConv) on CIFAR-100. 

Optimization Tricks. We propose weight projection and 
weighted gradients to facilitate the optimization of DCNets. 
These two tricks essentially amplify the original gradient 
and make the backward update more effective. From Ta-
ble 1, we observe that both weight projection and weighted 
gradients work much better than the competing methods. 

Method 
Linear Cosine Sq. Cosine 

CNN Baseline 
-
35.30 
-
LinearConv 
33.39 
31.76 
N/C 
TanhConv 
32.88 
31.88 
34.26 
SegConv 
34.69 
30.34 
N/C 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Testing error (%) of plain CNN-9 without BN on CIFAR-100. 
"N/C" indicates that the model can not converge. "-" denotes no result. 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head></head><label></label><figDesc>to significantly better accuracy than the baseline CNN with ReLU, even if our DCNet does not use ReLU at all.</figDesc><table>Method 
Cosine 
w/o ReLU 

Sq. Cosine 
w/o ReLU 

Cosine 
w/ ReLU 

Sq. Cosine 
w/ ReLU 

Baseline 
58.24 
-
26.01 
-
SphereConv 
33.31 
25.90 
26.00 
26.97 
BallConv 
31.81 
25.43 
25.18 
26.48 
TanhConv 
32.27 
25.27 
25.15 
26.94 
LinearConv 
36.49 
24.36 
24.81 
25.14 
SegConv 
33.57 
24.29 
24.96 
25.04 
LogConv 
33.62 
24.91 
25.17 
25.85 
MixConv 
33.46 
24.93 
25.27 
25.77 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head>Table 4 :</head><label>4</label><figDesc>Testing error rate (%) of ResNet-32 on CIFAR-100.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Comparison to the state-of-the-art on CIFAR-10 and CIFAR-100.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_8" validated="false"><head></head><label></label><figDesc>, we observe that DCNets can converge to a decent accuracy without BN, while the baseline model fails to converge without BN.</figDesc><table>Method 

Standard 
ResNet-18 
w/ BN 

Modified 
ResNet-18 
w/ BN 

Modified 
ResNet-18 
w/o BN 

Baseline 
12.63 
12.10 
N/C 
SphereConv 
12.68* 
11.55 
13.30 
LinearConv 
11.99* 
11.50 
N/C 
TanhConv 
12.47* 
11.10 
12.79 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_9" validated="false"><head>Table 6 :</head><label>6</label><figDesc>Center-crop Top-5 error (%) of standard ResNet-18 and modified ResNet-18 on ImageNet-2012. * indicates we use the pretrained model of original CNN on ImageNet-2012 as initialization (see Section 4.3).</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_10" validated="false"><head>Table 7 :</head><label>7</label><figDesc>White-box attacks on CIFAR-10. Performance is measured in accuracy (%). The first three rows are results of naturally trained models, and the last three rows are results of adversarially trained models.</figDesc><table>Target models 
Attack 
Baseline 
SphereConv BallConv TanhConv 

None 
85.35 
88.58 
91.13 
91.45 
FGSM 
18.82 
43.64 
50.47 
52.60 
BIM 
8.67 
8.89 
7.74 
10.18 

None 
83.70 
87.41 
87.47 
87.54 
FGSM 
78.96 
85.98 
82.20 
81.46 
BIM 
7.96 
35.07 
17.38 
19.86 

Target models 
Attack 
Baseline 
SphereConv BallConv TanhConv 

None 
85.35 
88.58 
91.13 
91.45 
FGSM 
50.90 
56.71 
49.50 
50.61 
BIM 
36.22 
43.10 
27.48 
29.06 

None 
83.70 
87.41 
87.47 
87.54 
FGSM 
77.57 
76.29 
78.67 
80.38 
BIM 
78.55 
77.79 
80.59 
82.47 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">Although the concepts of semantic difference and intra-class variation often refer to classification, they are extended to convolutions in this paper. Specifically, semantic difference means the pattern similarity between local patch x and kernel w, while intra-class variation refers to the energy of local patch x and kernel w.</note>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot">f d (w, x) = α · g(θ (w,x) ) (4) where α &gt; 0 controls the output scale. g(θ (w,x) ) depends on the geodesic distance on the unit hypersphere and typically outputs value from −1 to 1, so the final output is</note>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Fast and accurate deep network learning by exponential linear units (elus)</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D.-A</forename><surname>Clevert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Unterthiner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Hochreiter</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1511.07289</idno>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">4</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Fast r-cnn</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">J</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6572</idno>
		<title level="m">Explaining and harnessing adversarial examples</title>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Identity mappings in deep residual networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="630" to="645" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Formal guarantees on the robustness of a classifier against adversarial manipulation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hein</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Andriushchenko</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Densely connected convolutional networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Improving face verification and person re-identification accuracy using hyperplane similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jones</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kobori</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Adversarial machine learning at scale</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Goodfellow</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.01236</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Sphereface: Deep hypersphere embedding for face recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Raj</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Large-margin softmax loss for convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Yang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Deep hyperspherical learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y.-M</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Song</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Rethinking feature discrimination and polymerization for large-scale recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1710.00870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">How to choose an activation function</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">N</forename><surname>Mhaskar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">A</forename><surname>Micchelli</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="1994" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Ranjan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">D</forename><surname>Castillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Chellappa</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.09507</idno>
		<title level="m">L2-constrained softmax loss for discriminative face verification</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">You only look once: Unified, real-time object detection</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Redmon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Divvala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Farhadi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Faster r-cnn: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Tramèr</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kurakin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Papernot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Boneh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Mcdaniel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.07204</idno>
		<title level="m">Ensemble adversarial training: Attacks and defenses</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">Additive margin softmax for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1801.05599</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Normface: l 2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1704.06369</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Aggregated residual transformations for deep neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Xie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollár</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Tu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Feature incay for representation regularization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yuan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Zhang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1705.10284</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
