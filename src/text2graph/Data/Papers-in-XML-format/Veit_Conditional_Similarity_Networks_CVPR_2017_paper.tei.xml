<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T22:37+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Conditional Similarity Networks</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Andreas</forename><surname>Veit</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Cornell Tech</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Serge</forename><surname>Belongie</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">Department of Computer Science &amp; Cornell Tech</orgName>
								<orgName type="institution">Cornell University</orgName>
							</affiliation>
						</author>
						<author role="corresp">
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Theofanis</forename><surname>Karaletsos</surname></persName>
							<email>theofanis.karaletsos@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution">Uber AI Labs</orgName>
							</affiliation>
							<affiliation key="aff2">
								<orgName type="department">Computational Biology</orgName>
								<orgName type="institution">Sloan Kettering Institute</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Conditional Similarity Networks</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Understanding visual similarities between images is a key problem in computer vision. To measure the similarity between images, they are embedded in a feature-vector space, in which their distances preserve the relative dissimilarity. Commonly, convolutional neural networks are trained to transform images into respective feature-vectors. We refer to these as Similarity Networks. When learning such networks from pairwise or triplet (dis-)similarity constraints, the simplifying assumption is commonly made that objects are compared according to one unique measure of similarity. However, objects have various attributes and can be compared according to a multitude of semantic aspects. similar category similar color similar occasion <ref type="figure">Figure 1</ref>. Example illustrating how objects can be compared according to multiple notions of similarity. Here, we demonstrate three intuitive concepts, which are challenging to combine for a machine vision algorithm that has to embed objects in a feature space where distances preserve the relative dissimilarity: shoes are of the same category; red objects are more similar in terms of color; sneakers and t-shirts are stylistically closer.</p><p>An illustrative example to consider is the comparison of coloured geometric shapes, a task toddlers are regularly exposed to with benefits to concept learning. Consider, that a red triangle and a red circle are very similar in terms of color, more so than a red triangle and a blue triangle. However, the triangles are more similar to one another in terms of shape than the triangle and the circle.</p><p>An optimal embedding should minimize distances between perceptually similar objects. In the example above and also in the practical example in <ref type="figure">Figure 1</ref> this creates a situation where the same two objects are semantically repelled and drawn to each other at the same time. A standard triplet embedding ignores the sources of similarity and cannot jointly satisfy the competing semantic aspects. Thus, a successful embedding necessarily needs to take the visual concept into account that objects are compared to.</p><p>One way to address this issue is to learn separate triplet networks for each aspect of similarity. However, the idea is wasteful in terms of parameters needed, redundancy of parameters, as well as the associated need for training data.  <ref type="figure">Figure 2</ref>. The proposed Conditional Similarity Network consists of three key components: First, a learned convolutional neural network as feature extractor that learns the disentangled embedding, i.e., different dimensions encode features for specific notions of similarity. Second, a condition that encodes according to which visual concept images should be compared. Third, a learned masking operation that, given the condition, selects the relevant embedding dimensions that induce a subspace which encodes the queried visual concept.</p><p>In this work, we introduce Conditional Similarity Networks (CSNs) a joint architecture to learn a nonlinear embeddings that gracefully deals with multiple notions of similarity within a shared embedding using a shared feature extractor. Different aspects of similarity are incorporated by assigning responsibility weights to each embedding dimension with respect to each aspect of similarity. This can be achieved through a masking operation leading to separate semantic subspaces. <ref type="figure">Figure 2</ref> provides an overview of the proposed framework. Images are passed through a convolutional network and projected into a nonlinear embedding such that different dimensions encode features for specific notions of similarity. Subsequent masks indicate which dimensions of the embedding are responsible for separate aspects of similarity. We can then compare objects according to various notions of similarity by selecting an appropriate masked subspace. In the proposed approach the convolutional network that learns the disentangled embedding as well as the masks that learn to select relevant dimensions are trained jointly.</p><p>In our experiments we evaluate the quality of the learned embeddings by their ability to embed unseen triplets. We demonstrate that CSNs clearly outperform single triplet networks, and even sets of specialist triplet networks where a lot more parameters are available and each network is trained towards one single similarity notion. Further we show CSNs make the representation interpretable by encoding different similarities in separate dimensions.</p><p>Our contributions are a) formulating Conditional Similarity Networks, an approach that allows to to learn nonlinear embeddings that incorporate multiple aspects of similarity within a shared embedding using a shared feature extractor, b) demonstrating that the proposed approach outperforms standard triplet networks and even sets of specialist triplet networks in a variety of hard predictive visual tasks and c) demonstrating that our approach successfully disentangles the embedding features into meaningful dimensions so as to make the representation interpretable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Similarity based learning has emerged as a broad field of interest in modern computer vision and has been used in many contexts. Disconnected from the input image, triplet based similarity embeddings, can be learned using crowdkernels <ref type="bibr" target="#b23">[24]</ref>. Further, Tamuz et al. <ref type="bibr" target="#b20">[21]</ref> introduce a probabilistic treatment for triplets and learn an adaptive crowd kernel. Similar work has been generalized to multipleviews and clustering settings by Amid and Ukkonen <ref type="bibr" target="#b0">[1]</ref> as well as Van der Maaten and Hinton <ref type="bibr" target="#b22">[23]</ref>. A combination of triplet embeddings with input kernels was presented by Wilber et al. <ref type="bibr" target="#b26">[27]</ref>, but this work did not include joint feature and embedding learning. An early approach to connect input features with embeddings has been to learn image similarity functions through ranking <ref type="bibr" target="#b3">[4]</ref>.</p><p>A foundational line of work combining similarities with neural network models to learn visual features from similarities revolves around Siamese networks <ref type="bibr" target="#b5">[6,</ref><ref type="bibr" target="#b9">10]</ref>, which use pairwise distances to learn embeddings discriminatively. In contrast to pairwise comparisons, triplets have a key advantage due to their flexibility in capturing a variety of higher-order similarity constraints rather than the binary similar/dissimilar statement for pairs. Neural networks to learn visual features from triplet based similarities have been used by Wang et al. <ref type="bibr" target="#b24">[25]</ref> and Schroff et al. <ref type="bibr" target="#b16">[17]</ref> for face verification and fine-grained visual categorization. A key insight from these works is that semantics as captured by triplet embeddings are a natural way to represent complex class-structures when dealing with problems of highdimensional categorization and greatly boost the ability of models to share information between classes.</p><p>Disentangling representations is a major topic in the recent machine learning literature and has for example been tackled using Boltzmann Machines by Reed et al. <ref type="bibr" target="#b15">[16]</ref>. Chen et al. <ref type="bibr" target="#b4">[5]</ref> propose information theoretical factorizations to improve unsupervised adversarial networks. Within this stream of research, the work closest to ours is that of Karaletsos et al. <ref type="bibr" target="#b11">[12]</ref> on representation learning which introduces a joint generative model over inputs and triplets to learn a factorized latent space. However, the focus of that work is the generative aspect of disentangling representations and proof of concept applications to low-dimensional data. Our work introduces a convolutional embedding architecture that forgoes the generative pathway in favor of exploring applications to embed high-dimensional image data. We thus demonstrate that the generative interpretation is not required to reap the benefits of Conditional Similarity Networks and demonstrate in particular their use in common computer vision tasks.</p><p>A theme in our work is the goal of modeling separate similarity measures within the same system by factorizing (or disentangling) latent spaces. We note the relation of these goals to a variety of approaches used in representation learning. Multi-view learning <ref type="bibr" target="#b19">[20,</ref><ref type="bibr" target="#b25">26]</ref> has been used for 3d shape inference and shown to generically be a good way to learn factorized latent spaces. Multiple kernel learning <ref type="bibr" target="#b2">[3,</ref><ref type="bibr" target="#b18">19]</ref> employs information encoded in different kernels to provide predictions using the synthesized complex feature space and has also been used for similarity-based learning by McFee and Lanckriet <ref type="bibr" target="#b14">[15]</ref>. Multi-task learning approaches <ref type="bibr" target="#b6">[7]</ref> are used when information from disparate sources or using differing assumptions can be combined beneficially for a final prediction task. Indeed, our gating mechanism can be interpreted as an architectural novelty in neural networks for multi-task triplet learning. Similar to our work, multiliniear networks <ref type="bibr" target="#b13">[14]</ref> also strive to factorize representations, but differ in that they ignore weak additional information. An interesting link also exists to multiple similarity learning <ref type="bibr" target="#b1">[2]</ref>, where category specific similarities are used to approximate a fine-grained global embedding. Our global factorized embeddings can be thought of as an approach to capture similar information in a shared space directly through feature learning.</p><p>We also discuss the notion of attention in our work, by employing gates to attend to the mentioned subspaces of the inferred embeddings when focusing on particular visual tasks. This term may be confused with spatial attention such as used in the DRAW model <ref type="bibr" target="#b8">[9]</ref>, but bears similarity insofar as it shows that the ability to gate the focus of the model on relevant dimensions (in our case in latent space rather than observed space) is beneficial both to the semantics and to the quantitative performance of our model.  <ref type="figure">Figure 3</ref>. The masking operation selects relevant embedding dimensions, given a condition index. Masking can be seen as a soft gating function, to attend to a particular concept.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Conditional Similarity Networks</head><p>Our goal is to learn a nonlinear feature embedding f (x), from an image x into a feature space R d , such that for a pair of images x 1 and x 2 , the Euclidean distance between f (x 1 ) and f (x 2 ) reflects their semantic dis-similarity. In particular, we strive for the distance between images of semantically similar objects to be small, and the distance between images of semantically different objects to be large. This relationship should hold independent of imaging conditions.</p><p>We consider y = f (x) to be an embedding of observed images x into coordinates in a feature space y. Here,</p><formula xml:id="formula_0">f (x) = W g(x)</formula><p>clarifies that the embedding function is a composition of an arbitrarily nonlinear function g(·) and a linear projection W , for W ∈ R d×b , where d denotes the dimensions of the embedding and b stands for the dimensions of the output of the nonlinear function g(·). In general, we denote the parameters of function f (x) by θ, denoting all the filters and weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Conditional Similarity Triplets</head><p>Apart from observing images x, we are also given a set of triplet constraints sampled from an oracle such as a crowd. We define triplet constraints in the following.</p><p>Given an unknown conditional similarity function s c (·, ·), an oracle such as a crowd can compare images x 1 , x 2 and x 3 according to condition c. A condition is defined as a certain notion of similarity according to which images can be compared. <ref type="figure">Figure 1</ref> gives a few example notions according to which images of fashion products can be compared. The condition c serves as a switch between attented visual concepts and can effectively gate between different similarity functions s c . Using image x 1 as reference, the oracle can apply s c (x 1 , x 2 ) and s c (x 1 , x 3 ) and decide whether x 1 is more similar to x 2 or to x 3 conditioned on c. The oracle then returns an ordering over these two distances, which we call a triplet t. A triplet is defined as the set of indices {reference image, more distant image, closer image}, e.g.</p><formula xml:id="formula_1">{1, 3, 2} if s c (x 1 , x 3 ) is larger than s c (x 1 , x 2 ).</formula><p>We define the set of all triplets related to condition C as:</p><formula xml:id="formula_2">T ⌋ = {(i, j, l; c) | s c (x i , x j ) &gt; s c (x i , x l )}.<label>(1)</label></formula><p>We do not have access to the exhaustive set T ⌋ , but can sample K-times from it using the oracle to yield a finite sample</p><formula xml:id="formula_3">T ⌋ K = {t k } K k=1 .</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Learning From Triplets</head><p>The feature space spanned by our model is given by function f (·). To learn this nonlinear embedding and to be consistent with the observed triplets, we define a loss function L T (·) over triplets to model the similarity structure over images. The triplet loss commonly used is</p><formula xml:id="formula_4">L T (x i , x j , x l ) = max{0, D(x i , x j ) − D(x i , x l ) + h}<label>(2)</label></formula><p>where</p><formula xml:id="formula_5">D(x i , x j ) = f (x i ; θ)−f (x j ; θ) 2 .</formula><p>is the Euclidean distance between the representations of images x i and x j . The scalar margin h helps to prevent trivial solutions. The generic triplet loss is not capable of capturing the structure induced by multiple notions of similarities.</p><p>To be able to model conditional similarities, we introduce masks m over the embedding with m ∈ R d×nc where n c is the number of possible notions of similarities. We define a set of parameters β m of the same dimension as m such that m = σ(β), with σ denoting a rectified linear unit so that σ(β) = max{0, β}. As such, we denote m c to be the selection of the c-th mask column of dimension d (in pseudocode m c = m[:, c]). The mask plays the role of an element-wise gating function selecting the relevant dimensions of the embedding required to attend to a particular concept. The role of the masking operation is visually sketched in <ref type="figure">Figure 3</ref>. The masked distance function between two images x i and x j is given by</p><formula xml:id="formula_6">D(x i , x j ; m c , θ) = f (x i ; θ)m c − f (x j ; θ)m c 2 . (3)</formula><p>While appearing to be a small technical change, the inclusion of a masking mechanism for the triplet-loss has a highly non-trivial effect. The mask induces a subspace over the relevant embedding dimensions, effectively attending only to the relevant dimensions for the visual concept being queried. In the loss function above, that translates into a modulated cost phasing out Euclidean distances between irrelevant feature-dimensions while preserving the loss-structure of the relevant ones.</p><p>Given an triplet t = {i, j, l} defined over indices of the observed images and a corresponding condition-index c, the final triplet loss function L T (·) is given by: </p><formula xml:id="formula_7">L T (x i , x j , x l , c; m, θ) = max{0, D(x i , x j ; m c , θ) − D(x i , x l ; m c , θ) + h}<label>(4)</label></formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3.">Encouraging Regular Embeddings</head><p>We want to encourage embeddings to be drawn from a unit ball to maintain regularity in the latent space. We encode this in an embedding loss function L W given by:</p><formula xml:id="formula_8">L W (x; θ) = f (x; θ) 2 2 = y 2 2 (5)</formula><p>The separate subspaces are computed as f (x)m c . To prevent the masks from expanding the embedding and to encourage sparse masks, we add a loss to regulate the masks:</p><formula xml:id="formula_9">L M (m) = m 1<label>(6)</label></formula><p>Without these terms, an optimization scheme may choose to inflate embeddings to create space for new data points instead of learning appropriate parameters to encode the semantic structure.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4.">Joint Formulation For Convolutional CSNs</head><p>We define a loss-function L CSN for training CSNs by putting together the defined loss functions. Given images x, triplet constraints with associated condition {t, c} as well as parameters for the masks m and the embedding function θ, the CSN loss is defined as</p><formula xml:id="formula_10">L CSN (x, {t, c}; m, θ) = L T (x t0 , x t1 , x t2 , c; m, θ) + λ 1 L W (x, θ) + λ 2 L M (m)<label>(7)</label></formula><p>The parameters λ 1 and λ 2 weight the contributions of the triplet terms against the regular embedding terms. In our paper, the nonlinear embedding function is defined as f (x) = W g(x), where g(x) is a convolutional neural network. In the masked learning procedure the masks learn to select specific dimensions in the embedding that are associated with a given notion of similarity. At the same time, f (·) learns to encode the visual features such that different dimensions in the embedding encode features associated to specific semantic notions of similarity. Then, during test time each image can be mapped into this embedding by f (·). By looking at the different dimensions of the image's representation, one can reason about the different semantic notions of similarity. We call a feature space spanned by a function with this property disentangled , as it preserves the separation of the similarity notions through test time.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Experiments</head><p>We focus our experiments on evaluating the semantic structure of the learned embeddings and their subspaces as well as the underlying convolutional filters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Datasets</head><p>We perform experiments on two different datasets. First, for illustrative purposes we use a dataset of fonts 1 collected by Bernhardsson. The dataset contains 3.1 million images of single characters in gray scale with a size of 64 by 64 pixels each. The dataset exhibits variations according to <ref type="bibr" target="#b0">1</ref> http://erikbern.com/2016/01/21/analyzing-50k-fonts-using-deepneural-networks/ font style and character type. In particular, it contains 62 different characters in 50,000 fonts, from which we use the first 1,000. Second, we use the Zappos50k shoe dataset <ref type="bibr" target="#b27">[28]</ref> collected by Yu and Grauman. The dataset contains 50,000 images of individual richly annotated shoes, with a size of 136 by 102 pixels each, which we resize to 112 by 112. The images exhibit multiple complex variations. In particular, we are looking into four different characteristics: the type of the shoes (i.e., shoes, boots, sandals or slippers), the suggested gender of the shoes (i.e., for women, men, girls or boys), the height of the shoes' heels (numerical measurements from 0 to 5 inches) and the closing mechanism of the shoes (buckle, pull on, slip on, hook and loop or laced up). We also use the shoes' brand information to perform a finegrained classification test.</p><p>To supervise and evaluate the triplet networks, we sample triplet constraints from the annotations of the datasets. For the font dataset, we sample triplets such that two characters are of the same type or font and one is different. For the Zappos dataset, we sample triplets in an analogous way for the three categorical attributes. For the heel heights we have numerical measurements so that for each triplet we pick two shoes with similar height and one with different height. First, we split the images into three parts: 70% for training, 10% for validation and 20% in the test set. Then, we sample triplets within each set. For each attribute we collect 200k train, 20k validation and 40k test triplets.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Baselines and Model Variants</head><p>As initial model for our experiments we use a ConvNet pre-trained on ImageNet. All model variants are fine-tuned on the same set of triplets and only differ in the way they are trained. We compare four different approaches, which are schematically illustrated in <ref type="figure" target="#fig_4">Figure 6</ref>. Standard Triplet Network: The common approach to learn from triplet constrains is a single Convolutional Network where the embedding layer receives supervision from the triplet loss defined in Equation 2. As such, it aims to learn from all available triplets jointly as if they come from a single measure of similarity. Set of Task Specific Triplet Networks: Second, we compare to a set of n c separate triplet network experts, each of which is trained on a single notion of similarity. This overcomes the simplifying assumption that all comparisons come from a single measure of similarity. However, this comes at the cost of significantly more parameters. This is the best model achievable with currently available methods.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Similarity Networks -fixed disjoint masks:</head><p>We compare two variants of Conditional Similarity Networks. Both extend a standard triplet network with a masking operation on the embedding vector and supervise the network with the loss defined in Equation <ref type="bibr" target="#b3">4</ref>. The first variant learns the convolutional filters and the embedding. The masks are pre-defined to be disjoint between the different notions of similarity. This ensures the learned embedding is fully disentangled, because each dimension must encode features that describe a specific notion of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Conditional Similarity Networks -learned masks:</head><p>The second variant learns the convolutional filters, the embedding and the mask parameters together. This allows the model to learn unique features for the subspaces as well as features shared across tasks. This variant has the additional benefit that the learned masks can provide interesting insight in how different similarity notions are related.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Training Details</head><p>We train different convolutional networks for the two datasets. For the font dataset, we use a variant of the VGG architecture <ref type="bibr" target="#b17">[18]</ref> with 9 layers of 3 by 3 convolutions and two fully connected layers, which we train from scratch. For the Zappos dataset we fine-tune an 18 layer deep residual network <ref type="bibr" target="#b10">[11]</ref> that is pre-trained on Imagenet <ref type="bibr" target="#b7">[8]</ref>. We remove one downsampling module to adjust for the smaller image size. We train the networks with a mini-batch size of 256 and optimize using ADAM <ref type="bibr" target="#b12">[13]</ref> with α = 5E-5, β 1 = 0.1 and β 2 = 0.001. For all our experiments we use an embedding dimension of 64 and the weights for the embedding losses are λ 1 = 5E-3 and λ 2 = 5E-4. In each minibatch we sample triplets uniformly and for each condition in equal proportions. We train each model for 200 epochs and perform early stopping in that we evaluate the snapshot with highest validation performance on the test set. For our CSN variants, we use two masks over the embedding for the fonts dataset and four masks for the Zappos dataset, one mask per similarity notion. For models with pre-defined masks, we allocate 1/n c th of the embedding dimensions to one task. When learning masks, we initialize β m using a normal distribution with 0.9 mean and 0.7 variance. Following the ReLU, this results in initial mask values that induce random subspaces for each similarity measure. We observe that different random subspaces perform better than a setup where all subspaces start from the same values. Masks that are initialized as disjoint analogous to the predefined masks perform similar to random masks, but are not able to learn shared features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Visual Exploration of the Learned Subspaces</head><p>We visually explore the learned embeddings regarding their consistency according to respective similarity notions. We stress that all of these semantic representations are taking place within a shared space produced by the same network. The representations are disentangled so that each dimension encodes a feature for a specific notion of similarity. This allows us to use a simple masking operation to look into a specific semantic subspace. <ref type="figure" target="#fig_2">Figure 4</ref> shows embeddings of the two subspaces in the Fonts dataset, which we project down to two dimensions using t-SNE <ref type="bibr" target="#b21">[22]</ref>. The learned features are successfully disentangled such that the dimensions selected by the first mask describe the character type (left) and those selected by the second mask the font style (right). <ref type="figure" target="#fig_6">Figures 5 and 7</ref> show embeddings of the four subspaces learned with a CSN on the Zappos50k dataset. <ref type="figure">Figure 5(a)</ref> shows the subspace encoding features for the closure mechanism of the shoes. <ref type="figure">Figure 5(b)</ref> shows the subspace attending to the type of the shoes. The embedding clearly separates the different types of shoes into boots, slippers and so on. Highlighted areas reveal some interesting details. For example, the highlighted region on the upper right side shows nearby images of the same type ('shoes') that are completely different according to all other aspects. This means the selected feature dimensions successfully focus only on the type aspect and do not encode any of the other notions. <ref type="figure" target="#fig_6">Figure 7(a)</ref> shows the subspace for suggested gender for the shoes. The subspace separates shoes that are for female and male buyers as well as shoes for adult or youth buyers. The learned submanifold occupies a rotated square with axes defined by gender and age. Finally, <ref type="figure" target="#fig_6">Figure 7</ref>(b) shows a continuous embedding of heel heights, which is a subtle visual feature.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Qualitative Analysis Of Subspaces</head><p>The key feature of CSNs is the fact that they can learn separated semantic subspaces in the embeddings using the masking mechanism. We visualize the masks for our common model choices in <ref type="figure" target="#fig_7">Figure 8</ref>. We show the traditional triplet loss, where each dimension is equally taken into account for each triplet. Further, we show pre-defined masks that are used to factorize the embedding into fully disjoint  features. Lastly, we show a learned mask. Interestingly, the masks are very sparse in accordance with the 2D embeddings presented in the previous section, confirming that the concepts are low-dimensional. Further, although many additional dimensions are available, the model learned to share some of the features across concepts. This demonstrates that CSNs can learn to only use the required number of dimensions via relevance determination, reducing the need for picking the right embedding dimensionality.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Results on Triplet Prediction</head><p>To evaluate the quality of the learned embeddings by the different model variants, we test how well they generalize to unseen triplets. In particular, we perform triplet prediction on a testset of hold-out triplets from the Zappos50k dataset. We first train each model on a fixed set of triplets, where triplets are sourced from the four different notions of similarity. After convergence, we evaluate for each triplet with associated query {i, j, l, c} in the testset whether the distance between i and l is smaller than between i and j according to concept/query c. Since this is a binary task, random guessing would perform at an error rate of 50%.</p><p>The error rates for the different models are shown in Table 1. Standard Triplet Networks fail to capture fine-grained similarity and only reach an error rate of 23.72%. The set of task specific triplet networks greatly improves on that, achieving an error rate of 11.35%. This shows that simply learning a single space cannot capture multiple similarity notions. However, this comes at a the cost of n c times more model parameters. Conditional Similarity Networks with fixed disjoint masks achieve an error rate of 10.79%, clearly outperforming both the single triplet network as well as the set of specialist networks, which have a lot more parameters available for learning. This means by factorizing the embedding space into separate semantic subspaces, CSNs can successfully capture multiple similarity notions without requiring substantially more parameters. Moreover, CSNs benefit from learning all concepts jointly within one model, utilizing shared structure between the concepts while keeping the subspaces separated. CSNs with learned masks achieve an error rate of 10.73% improving performance even further. This indicates the benefits from allowing the model to determine the relevant dimensions and to share features across concepts. <ref type="table">Table 1</ref>. Triplet Prediction Results: We evaluate how many triplets of the test set are satisfied in the learned embeddings. Triplets come from four different similarity notions. The proposed Conditional Similarity Network clearly outperforms standard triplet networks that treat each triplet as if it came from the same similarity notion. Moreover, CSNs even outperform sets of specialist triplet networks where a lot more parameters are available during training and each network is specifically trained towards one similarity notion. CSNs with learned masks provide the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Method</head><p>Error Further, we evaluate the impact of the number of unique triplets available during training on performance. We compare models trained on 5, 12.5 25, 50 and 200 thousand triplets per concept. <ref type="figure">Figure 9</ref> shows that triplet networks generally improve with more available triplets. Further, CSNs with fixed masks consistently outperform set of specialized triplet networks. Lastly, CSNs with learned masks  <ref type="figure">Figure 9</ref>. Triplet prediction performance with respect to number of unique training triplets available. CSNs with fixed masks consistently outperform the set of specialized triplet networks. CSNs with learned masks generally require more triplets, since they need to learn the embedding as well as the masks. However, when enough triplets are available, they provide the best performance.</p><p>generally require more triplets, since they need to learn the embedding as well as the masks. However, when enough triplets are available, they provide the best performance.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Analysis of Convolutional Features Using OffTask Classification</head><p>We now evaluate how the different learning approaches affect the visual features of the networks. We compare standard triplet networks to CSNs. Both are initialized from the same ImageNet pre-trained residual network and fine-tuned using the same triplets and with their respective losses as described in Section 4.6. We evaluate the features learned by the two approaches, by subsequently performing brand classification on the Zappos dataset. In particular, we keep all convolutional filters fixed and replace the last embedding layer for both networks with one hidden and one softmax classification layer. We select the 30 brands in the Zappos dataset with the most examples and train with a standard multi-class classification approach using the 30 brands as classes. It is noteworthy that the triplets used for the finetuning do not contain brand information. The results are shown in <ref type="table" target="#tab_1">Table 2</ref>. The residual network trained on ImageNet leads to very good initial visual features for general classification tasks. Starting from the pretrained model, we observe that the standard triplet learning approach decreases the quality of the visual features, while CSNs retain most of the information. In the triplet prediction experiment in Section 4.6 standard triplet networks do not perform well, as they are naturally limited by the fact that contradicting notions cannot be satisfied in one single space. This classification result documents that the problem reaches even deeper. The contradicting gradients do not stop at the embedding layer, instead, they expose the entire network to inconsistent learning signals and hurt the underlying convolutional features.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Conclusion</head><p>In this work, we propose Conditional Similarity Networks to learn nonlinear embeddings which incorporate multiple aspect of similarity within a shared embedding. The learned embeddings are disentangled such that each embedding dimension encodes semantic features for a specific aspect of similarity. This allows to compare objects according to various notions by selecting an appropriate subspace using an element-wise mask. We demonstrate that CSNs clearly outperform single triplet networks, and even sets of specialist triplet networks where a lot more parameters are available and each network is trained towards one similarity notion.</p><p>Further, instead of being a black-box predictor, CSNs are qualitatively highly interpretable as evidenced by our exhibition of the semantic submanifolds they learn. Moreover, they provide a feature-exploration mechanism through the learned masks which surfaces the structure of the private and shared features between the different similarity aspects.</p><p>Lastly, we empirically find that naively training a triplet network with triplets generated through different similarity notions does not only limit the ability to correctly embed triplets, it also hurts the underlying convolutional features and thus generalization performance. The proposed CSNs are a simple to implement and easy to train end-to-end alternative to resolve these problems.</p><p>For future work, it would be interesting to consider learning from unlabeled triplets with a clustering mechanism to discover similarity substructures in an unsupervised way.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 4 .</head><label>4</label><figDesc>Figure 4. Visualization of 2D embeddings of two learned subspaces of the character feature space. The subspaces are obtained by attending to different subsets of the dimensions in the image representations. The subspace on the left groups images by character type, the one on the right according to font style. For clear visual representation we discretize the space into a grid and pick one image from each cell at random.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>(a ) Figure 5 .</head><label>)5</label><figDesc>Figure 5. Visualization of 2D embeddings of subspaces learned by the CSN. The spaces are clearly organized according to (a) closure mechanism of the shoes and (b) the category of the shoes. This shows that CSNs can successfully separate the subspaces.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. We show the four different model variants used in our experiments with the example of three objects being compared according to two contradictory notions of similarity, green and red. (a) A standard triplet network that treats all triplets equally (b) A set of nc-many triplet network experts specialized on green or red, respectively (c) A CSN with masks pre-set to be disjoint, so that in the embedding each dimension encodes a feature for a specific notion of similarity (d) A learned CSN, where the masks are learned to select features relevant to the respective notion of similarity.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head></head><label></label><figDesc>according to the height of the heels</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Visualization of the subspaces according to (a) suggested gender for the shoes and (b) height of the shoes' heel. The result shows that CSNs can learn categorical as well as continuous characteristics at the same time.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 .</head><label>8</label><figDesc>Figure 8. Visualization of the masks: Left: In standard triplet networks, each dimension is equally taken into account for each triplet. Center: The Conditional Similarity Network allows to focus on a subset of the embedding to answer a triplet question. Here, each mask focuses on one fourth. Right: For learned masks, it is evident that the model learns to switch off different dimensions per question. Further, a small subset is shared across tasks.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 2 .</head><label>2</label><figDesc>Using off-task classification, we evaluate how standard triplet networks and CSNs affect the convolutional features of the ImageNet-pretrained network they are based on. Naively training a standard triplet network with triplets from different similarity notions hurts the underlying convolutional features.</figDesc><table>Method 
Top 1 Accuracy 

ResNet trained on ImageNet 
54.00% 
Standard Triplet Network 
49.08% 
Conditional Similarity Network 53.67% 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Acknowledgements</head><p>We would like to thank Gunnar Rätsch and Baoguang Shi for insightful feedback. This work was supported in part by the AOL Connected Experiences Laboratory, a Google Focused Research Award, AWS Cloud Credits for Research and a Facebook equipment donation.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<analytic>
		<title level="a" type="main">Multiview triplet embedding: Learning attributes in multiple maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Amid</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Ukkonen</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1472" to="1480" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Similarity metrics for categorization: from monolithic to category specific</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Babenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;09)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="293" to="300" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Multiple kernel learning, conic duality, and the smo algorithm</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><forename type="middle">R</forename><surname>Bach</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">R</forename><surname>Lanckriet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">I</forename><surname>Jordan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 21st International Conference on Machine Learning</title>
		<meeting>the 21st International Conference on Machine Learning</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2004" />
			<biblScope unit="page">6</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Large scale online learning of image similarity through ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Chechik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Sharma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">U</forename><surname>Shalit</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">11</biblScope>
			<biblScope unit="page" from="1109" to="1135" />
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Infogan: Interpretable representation learning by information maximizing generative adversarial nets</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Duan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Houthooft</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Schulman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Abbeel</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03657</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;05)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">A unified architecture for natural language processing: Deep neural networks with multitask learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Collobert</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Weston</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 25st International Conference on Machine Learning (ICML-08)</title>
		<meeting>the 25st International Conference on Machine Learning (ICML-08)</meeting>
		<imprint>
			<publisher>ACM</publisher>
			<date type="published" when="2008" />
			<biblScope unit="page" from="160" to="167" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L.-J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Feifei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;09)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Draw: A recurrent neural network for image generation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gregor</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Danihelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Graves</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1502.04623</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Dimensionality reduction by learning an invariant mapping</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;06)</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2006" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page" from="1735" to="1742" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<monogr>
		<title level="m" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.03385</idno>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Bayesian representation learning with oracle constraints</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karaletsos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Learning Representations (ICLR)</title>
		<meeting><address><addrLine>San Juan, PR</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Bilinear cnn models for fine-grained visual recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T.-Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Roychowdhury</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1449" to="1457" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning multi-modal similarity</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Mcfee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Lanckriet</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">12</biblScope>
			<biblScope unit="page" from="491" to="523" />
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Learning to disentangle factors of variation with manifold interaction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 31st International Conference on Machine Learning (ICML-14)</title>
		<meeting>the 31st International Conference on Machine Learning (ICML-14)</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1431" to="1439" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
		<title level="m" type="main">Very deep convolutional networks for large-scale image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1409.1556</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Large scale multiple kernel learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Sonnenburg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Rätsch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Schäfer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schölkopf</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">The Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">7</biblScope>
			<biblScope unit="page" from="1531" to="1565" />
			<date type="published" when="2006" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Multiview convolutional neural networks for 3d shape recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Maji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Kalogerakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Learned-Miller</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="945" to="953" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Adaptively learning the crowd kernel</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Tamuz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Shamir</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kalai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">J</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 28th International Conference on Machine Learning (ICML-11)</title>
		<meeting>the 28th International Conference on Machine Learning (ICML-11)</meeting>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="673" to="680" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Visualizing data using t-sne</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">9</biblScope>
			<biblScope unit="page">85</biblScope>
			<date type="published" when="2008" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Visualizing non-metric similarities in multiple maps</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Machine learning</title>
		<imprint>
			<biblScope unit="volume">87</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="33" to="55" />
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Stochastic triplet embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Der Maaten</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Weinberger</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Machine Learning for Signal Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2012" />
			<biblScope unit="page" from="1" to="6" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Learning fine-grained image similarity with deep ranking</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Rosenberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;14)</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1386" to="1393" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">On deep multi-view representation learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Arora</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Livescu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Bilmes</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 32nd International Conference on Machine Learning (ICML-15)</title>
		<meeting>the 32nd International Conference on Machine Learning (ICML-15)</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1083" to="1092" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Learning concept embeddings with combined human-machine expertise</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Wilber</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><forename type="middle">S</forename><surname>Kwak</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kriegman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV &apos;15)</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="981" to="989" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<analytic>
		<title level="a" type="main">Fine-Grained Visual Comparisons with Local Learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition (CVPR &apos;14)</title>
		<imprint>
			<date type="published" when="2014-06" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
