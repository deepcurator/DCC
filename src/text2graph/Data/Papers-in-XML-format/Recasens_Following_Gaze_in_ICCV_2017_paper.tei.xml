<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:31+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Following Gaze in Video</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Adri√†</forename><surname>Recasens</surname></persName>
							<email>recasens@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Carl</forename><surname>Vondrick</surname></persName>
							<email>vondrick@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Aditya</forename><surname>Khosla</surname></persName>
							<email>khosla@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Antonio</forename><surname>Torralba</surname></persName>
							<email>torralba@csail.mit.edu</email>
							<affiliation key="aff0">
								<orgName type="institution">Massachusetts Institute of Technology</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Following Gaze in Video</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many times, the character will be looking at something that fall outside the frame, just like in (a), and detecting what object the character is looking at can not be addressed by previous saliency and gaze following models. Solving this problem requires analyzing gaze, making use of semantic knowledge about the typical 3D relationships between different views, and recognizing the objects that are the common targets of attention, just like we do when watching a movie. Here we study the problem of gaze following in video where the object attended by a character might appear only on a separate frame. Given a video (b) around the frame containing the character (t =0 ) our system selects the frames likely to contain the object attended by the selected character (c) and produces the output shown in (d). This figure shows an actual result from our system. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze in video by predicting where a person (in the video) is looking even when the object is in a different</head></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Many times, the character will be looking at something that fall outside the frame, just like in (a), and detecting what object the character is looking at can not be addressed by previous saliency and gaze following models. Solving this problem requires analyzing gaze, making use of semantic knowledge about the typical 3D relationships between different views, and recognizing the objects that are the common targets of attention, just like we do when watching a movie. Here we study the problem of gaze following in video where the object attended by a character might appear only on a separate frame. Given a video (b) around the frame containing the character (t =0 ) our system selects the frames likely to contain the object attended by the selected character (c) and produces the output shown in (d). This figure shows an actual result from our system.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Abstract</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Following the gaze of people inside videos is an important signal for understanding people and their actions. In this paper, we present an approach for following gaze in video by predicting where a person (in the video) is looking even when the object is in a different frame. We collect VideoGaze, a new dataset which we use as a benchmark to both train and evaluate models. Given one frame with a person in it, our model estimates a density for gaze location in every frame and the probability that the person is looking in that particular frame. A key aspect of our approach is an end-to-end model that jointly estimates: saliency, gaze pose, and geometric relationships between views while only using gaze as supervision. Visualizations suggest that the model learns to internally solve these intermediate tasks automatically without additional supervision. Experiments show that our approach follows gaze in video better than existing approaches, enabling a richer understanding of human activities in video.</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Can you tell where Tom Hanks (in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>) is looking? You might observe that there is not enough information in the frame to predict the location of his gaze. However, if we search the neighboring frames of the given video (shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>), we can identify he is looking at the woman (illustrated in <ref type="figure" target="#fig_0">Fig. 1(d)</ref>). In this paper, we introduce the problem of gaze following in video. Specifically, given a video frame with a person, and a set of neighboring frames from the same video, our goal is to identify which of the neighboring frames (if any) contain the object being looked at, and the location on that object that is being gazed upon.</p><p>Importantly, we observe that this task requires both a semantic and geometric understanding of the video. For example, semantic understanding is required to identify frames that are from the same scene (e.g., indoor and outdoor frames are unlikely to be from the same scene) while geometric understanding is required to localize exactly where the person is looking in a novel frame using the head pose and geometric relationship between the frames. Based on this observation, we propose a novel convolutional neural network based model that combines semantic and geometric understanding of frames to follow an individual's gaze in a video. Despite encapsulating the structure of the problem, our model requires minimal supervision and produces an interpretable representation of the problem.</p><p>In order to train and evaluate our model, we collect We present a novel large-scale dataset for gaze-following in video. Every person annotated in the dataset has its gaze annotated in five neighbor frames. We show some annotated examples from the dataset. In red, the frames without the gazed object on it. In green, we show the gaze annotations from the dataset.</p><p>a large scale dataset for gaze following in videos. Our dataset consists of around 50,000 people in short videos annotated with where they are looking throughout the video. We evaluate the performance of a variety of baseline approaches (e.g., saliency, gaze prediction in images, etc) on our dataset, and show that our model outperforms all existing approaches.</p><p>There are three main contributions of this paper. First, we introduce the problem of following gaze in videos. Second, we collect a large scale dataset for both training and evaluation on this task. Third, we present a novel network architecture that leverages the geometry of the scene to tackle this problem. The remainder of this paper details these contributions. In Section 2 we explore related work. In Section 3 we describe our dataset, VideoGaze. In Section 4, we describe the model in detail, and finally in Section 5 we evaluate the model and provide sample results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>We describe the related works in the areas of gazefollowing in both videos and images, deep learning for geometry prediction and saliency below.</p><p>Gaze-following in video: Previous works video gazefollowing deal with very restricted settings. Most notably <ref type="bibr" target="#b20">[21,</ref><ref type="bibr" target="#b19">20]</ref> tackles the problem of detecting people looking at each other in video, by using their head pose and location inside the frame. Although our model can be used with this goal, it is applicable to a wide variety of settings: it can predict gaze when it is located elsewhere in the image (not only on humans) or future/past frame of the video. Mukherjee and Robertson <ref type="bibr" target="#b21">[22]</ref> use RGB-D images to predict gaze in images and videos. They estimate the head-pose of the person using the multi-modal RGB-D data, and finally they regress the gaze location with a second system. Although the output of their system is gaze location, our model does not need multi-modal data and it is able to deal with gaze location in a different view. Extensive work has been done on human interaction and social prediction on both images and video involving gaze <ref type="bibr" target="#b32">[33,</ref><ref type="bibr" target="#b12">13,</ref><ref type="bibr" target="#b3">4]</ref>. Some of this work is focused on ego-centric camera data, such as in <ref type="bibr" target="#b8">[9,</ref><ref type="bibr" target="#b7">8]</ref>. Furthermore, <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b29">30]</ref> predicts social saliency, that is, the region that attracts attentions of a group of people in the image. Finally, <ref type="bibr" target="#b3">[4]</ref> estimates the 3D location and pose of the people, which is used to predict social interaction. Although their goal is completely different, we also model the scene with explicit 3D and use it to predict gaze.</p><p>Gaze-following in images: Our model is inspired by a previous gaze-following model for static images <ref type="bibr" target="#b25">[26]</ref>. However, the previous work focuses only on cases where a person, within the image, is looking at another object in the same image. In this work, we remove this restriction and extend gaze following to video. The model proposed in this paper deals with the situation where the person is looking at another frame in the video. Further, unlike <ref type="bibr" target="#b25">[26]</ref>, we use parametrized geometry transformations that help the model to deal with the underlying geometry of the world. There have also been recent works in applying deep learning to eye-tracking <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b34">35]</ref> that predict where an individual is looking on a device. Furthermore, <ref type="bibr" target="#b31">[32]</ref> introduces an eye-tracking technique which makes the calibration process avoidable. Finally, our work is also related to <ref type="bibr" target="#b4">[5]</ref>, which predicts the object of interaction in images.</p><p>Deep Learning with Geometry: Neural networks have previously been used to model geometric transforma- The output is the gaze location density and the probability of x t of containing the gazed object.</p><p>tions <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12]</ref>. Our work is also related to Spatial Transformers Networks <ref type="bibr" target="#b13">[14]</ref>, where a localization module generates the parameters of an affine transformation and warps the representation with bilinear interpolation. Our model generates parameters of a 3D affine transformation, but the transformation is applied analytically without warping, which is likely to be more stable. <ref type="bibr" target="#b27">[28,</ref><ref type="bibr" target="#b5">6]</ref> used 2D images to learn the underlying 3D structure. Similarly, we expect our model to learn the 3D structure of the frame composition only using 2D images. Finally, <ref type="bibr" target="#b9">[10]</ref> provide efficient implementations for adding geometric transformations to CNNs.</p><p>Saliency: Although related, gaze-following and freeviewing saliency refer to different problems. In gazefollowing, we predict the location of the gaze of an observer in the scene, while in saliency we predict the fixations of an external observer free-viewing the image. Some authors have used gaze to improve saliency prediction, such as in <ref type="bibr" target="#b24">[25]</ref>. Furthermore, <ref type="bibr" target="#b1">[2]</ref> showed how gaze prediction can improve state-of-the-art saliency models. Although our approach is not intended to solve video saliency, we believe it is worth mentioning some works learning saliency for videos such as <ref type="bibr" target="#b17">[18,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b18">19]</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">VideoGaze Dataset</head><p>We introduce VideoGaze, a large scale dataset containing the location where film characters are looking in movies. VideoGaze contains 166, 721 annotations from 140 movies. To build the dataset we used videos from the MovieQA dataset <ref type="bibr" target="#b30">[31]</ref>, which we consider a representative selection of movies. Each sample of the dataset consists of six frames. The first frame contains the character whose gaze is annotated. Eye location and a head bounding box for the character are provided. The other five frames contain the gaze location that the character is looking at the time, if present in the frame. To annotate the dataset, we used Amazon's Mechanical Turk (AMT). We annotated our dataset in two separate steps. In the first step, the workers were asked to first locate the head of the character and then scan through the video to find the location of the object the character is looking at. For cost efficiency reasons, we restricted the workers to only scan a 6 seconds temporal window around the frame with the character. In pilot experiments, we found this window to be sufficient. We also provided options to indicate that the gazed object never appears in the clip or that the head of the character was not visible in the scene. In the second step, we temporally sampled four additional frames nearby the first annotated frame and ask the Turkers to annotate the gazed object if present. Using this two-step process we ensure that if the gazed object appears in the video, it is annotated in our VideoGaze.</p><p>We split our data into training set and test set. We use all the annotations from 20 movies as the testing set and the rest of the annotations as training set. Note that we made the train/test split by source movie, not by clip, which prevents overfitting to particular movies. Additionally, we annotated five times one frame per each sample in the test set. We used this data to perform a robust evaluation of our methods and compute a human performance. Finally, for the same frames, we also annotated the similarity between the frame with the character and the frame with the object. In <ref type="figure" target="#fig_7">figure  8</ref> we use the similarity annotation to evaluate performance versus different levels of similarity.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Method</head><p>Suppose we have a video and a person inside the video. Our goal is to predict where the person is looking, which may possibly be in another frame of the video. Let x s be a source frame where the person is located, x h be an image crop containing only the person's head, and u e be the coordinates of the eyes of the person within the frame x s . Let x be a set of frames that we want to predict where a person is looking (if any). We wish to both select a target frame x t 2 x that the object of gaze appears in and then predict the coordinates of the person's gaze≈∑ in x t . We first explain how to predict≈∑ given x t . Then, we discuss how to learn to select x t .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Multi-Frame Gaze Network</head><p>Suppose we are given x t . We can design a convolutional neural network F (x s ,x h ,u e ,x t ) to predict the spatial location≈∑. While we could simply concatenate these inputs and train a network, the internal representation would be difficult to interpret and may require large amounts of training data to discover consistent patterns, which is inefficient. Instead, we seek to take advantage of the geometry of the scene to better predict people's gaze.</p><p>To follow gaze across frames, the network must be able to solve three sub-problems: (1) estimate the head pose of the person, (2) find the geometric relationship between the frame where the person is and the frame where the gaze location might be, and (3) find the potential locations in the target frame where the person might be looking (salient spots). We design a single model that internally solves each of these sub-problems even though we supervise the network only with the gaze annotations.</p><p>With this structure in mind, we design a convolutional network F to predictƒ• for a target frame x t :</p><formula xml:id="formula_0">F (x s ,x h ,u e ,x t )=S(x t ) G(u e ,x s ,x t )<label>(1)</label></formula><p>where S(¬∑) and G(¬∑) are decompositions of the original problem. Both S(¬∑) and G(¬∑) produce a positive matrix in R k√ók with k being the size of the spatial maps and is the element-wise product. Although we only supervise F (¬∑), our intention is that S(¬∑) will learn to detect salient objects and G(¬∑) will learn to estimate a mask of all the locations where the person could be looking in x t . We use the element-wise product as an "and operation" so that the network predicts people are looking at salient objects that are within their eyesight.</p><p>S is parametrized as a neural network. The structure of G is motivated to leverage the geometry of the scene. We write G as the intersection of the person's gaze cone with a plane representing the target frame x t transformed into the same coordinate frame as x s :</p><formula xml:id="formula_1">G(u e ,x s ,x t )=C(u e ,x h ) \ œÑ (T (x s ,x t ))<label>(2)</label></formula><p>where C(u e ,x s ) 2 R 7 estimates the parameters of a cone representing the person's gaze in the original image x s , T (x s ,x t ) 2 R 3√ó4 estimates the parameters of an affine transformation of the target frame, and œÑ applies the transformation. œÑ is expected to compute the coordinates of x t in the system of coordinates defined by x s . We illustrate this process in <ref type="figure" target="#fig_4">Figure 4</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Transformation œÑ</head><p>We use an affine transformation to geometrically relate the two frames x s and x t . Let Z be the set of coordinates inside the square with corners (¬±1, ¬±1, 0). Suppose the image x s is located in Z (x s is resized to have its corners in (¬±1, ¬±1, 0)) . Then:</p><formula xml:id="formula_2">œÑ (T )=Tz 8z 2 Z<label>(3)</label></formula><p>The affine transformation T is computing the geometric relation between both frames. To compute the parameters T we used a CNN. We use T to transform the coordinates of x t into the coordinate system defined by x s . In practice, we found it useful to output an additional scalar parameter Œ≥(x t ,x s ) and define œÑ (T )=Œ≥(x t ,x s )Tz. The parameter Œ≥ is expected to be used by the network to set G =0if no transformation can be found.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3.">Cone-Plane Intersection</head><p>Given a cone parametrization of the gaze direction C and a transformed frame plane œÑ (T ), we wish to find the intersection C \ œÑ (T ). The intersection is obtained by solving the following equation for Œ≤:</p><formula xml:id="formula_3">Œ≤ T Œ£Œ≤ =0 where Œ≤ =(Œ≤ 1 ,Œ≤ 2 , 1)<label>(4)</label></formula><p>where (Œ≤ 1 ,Œ≤ 2 ) are coordinates in the system of coordinates defined by x t , and Œ£ 2 R 3√ó3 is a matrix defining the cone-plane intersection as in <ref type="bibr" target="#b2">[3]</ref>. Solving Equation 4 for all Œ≤ gives us the cone-plane intersection, however it is not discrete, which would not provide a gradient for learning. Therefore, we use an approximation to make the intersection soft:</p><formula xml:id="formula_4">C(u e ,x h ) \ œÑ (T (x s ,x t )) = œÉ(Œ≤ T Œ£Œ≤)<label>(5)</label></formula><p>where œÉ is a sigmoid activation function. </p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4.">Frame Selection</head><p>We described an approach to predict the spatial location≈∑ where a person is looking inside a given frame x t . However, how should we pick the target frame x t ?T o do this, we can simultaneously estimate the probability the person of interest is looking inside a frame x t . Let E (S(x t ),G(u e ,x s ,x t )) be this probability where E is a neural network.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5.">Pathways</head><p>We estimate the parameters of the saliency map S, the cone C, and the transformation T using CNNs.</p><p>Saliency Pathway: The saliency pathway uses the target frame x t to generate a spatial map S(x t ). We used a 6-layer CNN to generate the spatial map from the input image. The five initial convolutional layers follow the structure of AlexNet introduced by <ref type="bibr" target="#b16">[17]</ref>. The last layer uses a 1 ‚á• 1 kernel to merge the 256 channels in a simple k ‚á• k map.</p><p>Cone Pathway: The cone pathway generates a cone parametrization from a close-up image of the head x h and the eyes u e . We set the origin of the cone at the head of the person u e and let a CNN generate v 2 R 3 , the direction of the cone and Œ± 2 R, its aperture. <ref type="figure" target="#fig_4">Figure 4</ref> shows an schematic example of the cone generation.</p><p>Transformation Pathway: The transformation pathway has two stages. We define T 1 ,a5-layer CNN following the structure defined in <ref type="bibr" target="#b16">[17]</ref>. T 1 is applied separately to both the source frame x s and the target frame x t . We define T 2 which is composed by one convolutional layer and three fully connected layers reducing the dimensionality of the representation. The output of the pathway is computed as: T (x s ,x t )=T 2 (T 1 (x s ),T 1 (x t )). We used <ref type="bibr" target="#b9">[10]</ref> to compute the transformation matrix from output parameters.</p><p>Discussion: We constrain each pathway to learn different aspects of the problem by providing each pathway only a subset of the inputs. The saliency pathway only has access to the target frame x t , which is insufficient to solve the full problem. Instead, we expect it to find salient objects in the target view x t . Likewise, the transformation pathway has access to both x s and x t , and the transformation will be later used to project the gaze cone. We expect it to compute a transformation that geometrically relates x s and x t . We expect each of the pathways to solve its particular subproblem to then get combined to generate the final output. Since every step is differentiable, it can be trained end-toend without intermediate supervision.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.6.">Learning</head><p>Since gaze-following is a multi-modal problem, we train F to estimate a spatial probability distribution q(x, y) instead of regressing a single gaze location. We use a generalization of the spatial loss used in <ref type="bibr" target="#b25">[26]</ref>. They use five different classification grids that are shifted and the predictions of each of them are combined. We generalize this loss by averaging over all the possible grids of different shifts and sizes:</p><formula xml:id="formula_5">L(p, q)= X w,h,‚àÜx,‚àÜy E w,h,‚àÜx,‚àÜy (p, q)<label>(6)</label></formula><p>where E w,h,‚àÜx,‚àÜy is a spatially smooth cross entropy with grid cells sized w ‚á• h and shifted (‚àÜ x , ‚àÜ y ) spaces over. Instead of using q to compute the loss, E uses a smoothed version of q where for each position (x, y) it sums up the probability in the rectangle around. For simplicity, we write this in one dimension:</p><formula xml:id="formula_6">E w,‚àÜx = ‚àí X x p(x) log Œ¥=w X Œ¥=0 q(x +‚àÜ x + Œ¥)<label>(7)</label></formula><p>which is similar to the cross-entropy loss function except the spatial bins are shifted by ‚àÜ x and scaled by w. This expression can be written as the output of a convolution, which is efficient to compute, and differentiable.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.7.">Inference</head><p>Our network F will produce a matrix A 2 R 20√ó20 ,a map that can be interpreted as a density where the person is looking. To infer the gaze location≈∑ in the target frame x t , we find the mode of this density≈∑ = arg max i,j A ij . To select the target frame x t , we pick the frame with the highest score from E.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.8.">Implementation Details</head><p>We implemented our model using PyTorch. In our experiments we use k = 13, the output of both the saliency pathway and the cone generator is a 13‚á•13 spatial map. We found useful to add a final fully connected layer to upscale the 13 ‚á• 13 spatial map to a 20 ‚á• 20 spatial map. We initialize the CNNs in the three pathways with ImageNet-CNN <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b28">29]</ref>. The cone pathway has three fully connected layers of sizes 500, 200 and 4 to generate the cone parametrization. The common part of the transformation pathway, T 2 , has one convolutional layer with a 1 ‚á• 1 kernel and 100 output channels, followed by one 2 ‚á• 2 max pooling layer and three fully connected layers of 200, 100 and the parameter size of the transformation. E is a Multilayer Perceptron with one hidden layer of 200 dimensions. For training, we augment data by flipping x t and x s and their annotations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Experiments</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.">Evaluation Procedure</head><p>To evaluate our model we conducted quantitative and qualitative analyses using our held out dataset. We use 4 ground truth annotations for evaluations and one to evaluate human performance. Similar to <ref type="bibr" target="#b6">[7]</ref>, for quantitative evaluation we provide bounding boxes for the heads of the persons. The bounding boxes are part of the dataset and have been collected using Amazon's Mechanical Turk. This makes the evaluation focused on the gaze following task. In <ref type="figure" target="#fig_6">Figure 7</ref> and 5 we provide some qualitative examples of our system working with head bounding boxes computed with an automatic head detector. For our quantitative evaluation, we report performances of the model in two tasks: predicting the gaze location given the frame with the object, and selecting the frame with the object.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head>Model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.1">Predicting gaze location</head><p>We use AUC, L 2 distances and KL divergence as our evaluation metrics for predicting gaze location. AUC refers to Area Under the Curve, a measure typically used to compare predicted distributions to samples. The predicted heat map is used as a confidence to build a ROC curve. We used <ref type="bibr" target="#b14">[15]</ref> to compute the AUC metric. We also used L 2 metric, which is computed as the euclidean error between the predicted point and the ground truth annotation. Additionally, we report minimum distance to human annotation, which is the L 2 distance for the closer ground truth point. For comparison purposes, we assume the images are normalized to having sides of length 1 unit. Finally, KL refers to the Kullback-Leibler divergence, a measure of the information lost when the output map is used as the gaze fixation map. KL is typically used to compare distributions <ref type="bibr" target="#b0">[1]</ref>. Previous work in gaze following in video cannot be evaluated in our benchmark because of its particular contains (only predicting social interaction or using multi-model data). We compare our method to several baselines described below. For methods producing a single location, we used a Gaussian distribution centered in the output location.</p><p>Random: The prediction is a random location in the image. Center: The prediction is always the center of the image. Fixed bias: The head location is quantized in a 13 ‚á• 13 grid and the training set is used to compute the average output location per each head location. Saliency: The output heatmap is the saliency prediction for x t . <ref type="bibr" target="#b22">[ 23]</ref> is used to compute the saliency map. The output point is computed as the mode of the saliency output distribution. Static Gaze: <ref type="bibr" target="#b25">[ 26]</ref> is used to compute the gaze prediction. Since it is a method for static images, the head image and the head location provided are from the source view but the image provided is the target view.</p><p>Additionally, we performed an analysis on the components of our model. With this analysis, we aim to understand the contribution of each of the parts to performance as well as suggest that all of them are needed.</p><p>Translation only: The affine transformation is a translation. Rotation only: The affine transformation is a 3-axis rotation. Identity: The affine transformation is the identity. Image only: The saliency pathway is used to generate the output. Cone only: The gaze pathway combined with the transformation pathway are used to generate the output. 3 axis rotation / translation: The affine transformation is a 3 axis rotation combined with a translation. Vertical axis rotation: The affine transformation is a rotation in the vertical axis combined with a translation.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.1.2">Frame selection</head><p>We use mean Average Precision as our evaluation metric for the frame selection. AP is defined as the area under the precision-recall curve and has been extensively used to evaluate detection problems. As for predicting the gaze location, previous work in gaze-following cannot be applicable to solve the frame selection task. We compare our method to the baselines described below.</p><p>Random: The score for the frame is randomly assigned. Closest: The score is inverse to the time difference between the source frame and the target frame. Saliency: The score assigned to the frame is inverse to the entropy of the saliency map <ref type="bibr" target="#b22">[23]</ref>. This value is higher if the saliency map is more concentrated, which could indicate the presence of a salient object. Additionally, we compare against some of the ablation model defined in the previous section. <ref type="table">Table 1</ref> summarizes our performance on both tasks.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.">Results</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.1">Predicting gaze location</head><p>Our model has a performance of 89.0 in AUC, 0.184 in L 2 , 0.123 in minimum L 2 distance and 7.76 in KL. Our performance is significantly better than all the baselines. Interestingly, the model with vertical rotation performs similarly (88.5/0.189/0.128/7.82), which we attribute to the fact that most of the camera rotations are on the vertical axis.</p><p>Our analysis show that our model outperforms all possible combinations of models and restricted transformations. We show that each component of the model is required to obtain good performance. Note that models generating one location perform worse in KL divergence because the metric is designed to evaluate distributions.</p><p>In <ref type="figure">Figure 6</ref> we show the output of the internal pathways of our model. This figure suggest that our network has internally learned to solve the sub-problems we intended it to solve. In addition to solving the overall gaze following problem, the network is able to estimate the geometrical relationship among frames along with estimating the gaze direction from the source view and predicting the salient regions in the target view. We follow a character through a movie and list which elements he has seen during the film. Here we present three examples of our predictions.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.2.2">Frame selection</head><p>The mean AP of our model is 87.5, over performing all the baselines and ablation models. Interestingly, the model using only the target frame performs significantly worse than the models using both source and target frames, showing the need of using the source frame to retrieve the frame of interest. In <ref type="figure">Figure 5</ref> we show two examples of the frame selection system. On the left, we show the source frame and, on the right, we show five frames. Below the frames we show the frame selector network score. In the first example, it clearly selects the right frame. In the second example, which is more ambiguous, it selects the right frame as well. <ref type="figure" target="#fig_6">Figure 7</ref> shows the output of our model using an automatic head detector (Face Recognition library in Python) and using the frame selector to select the frame. Furthermore, we used <ref type="bibr" target="#b26">[27]</ref> to detect and label the object the character is looking at. Using our model, we can list the objects that the character has seen during a movie. <ref type="figure">Figure 5</ref> presents two examples with out full pipeline. In <ref type="figure">Fig. 5</ref>.a) and c) we show the frame selection score over time. As expected, the frames containing the person who is going to be predicted have low score. Furthermore, frames likely to contain the gazed object have higher score. In <ref type="figure">Fig. 5.b</ref>) and d) we plot the final prediction.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.3.">Combined model</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.4.">Similarity analysis</head><p>How different is our method to a saliency model and to the gaze model on a single image? One could argue that when frames are different our system is simply doing saliency, and that when frames are similar you can use the static method. In <ref type="figure" target="#fig_7">Fig. 8</ref> we evaluate the performance of these models when varying the similarity between the source and the target frame. We used ground truth data annotated in AMT. We plot the performance of We plot performance versus similarity of the target and the source frame. Our model outperforms saliency and static gaze-following in all the similarity range for all the metrics.</p><p>our method, a static gaze-following method <ref type="bibr" target="#b25">[26]</ref>, a state-ofthe-art saliency method <ref type="bibr" target="#b22">[23]</ref> and humans. We outperform both static gaze-following and saliency in all the similarity ranges, showing that our model is doing more than just performing this two tasks combined. As mentioned in Sec. 5.2, humans perfom bad according to KL because the metric is designed to compare distributions and not locations.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Conclusions</head><p>We present a novel method for gaze following in video. Given one frame with a person on it, we are able to find the frame where the person is looking and predict the gaze location, even when the frames are quite different. We split our model in three pathways which automatically learn to solve the three sub problems involved in the task. We take advantage of the geometry of the scene to better predict people's gaze. We also introduce a new dataset where we benchmark our model and show that it over performs the baselines and produces meaningful outputs. We hope that our dataset will attract the community attention to the problem.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1</head><label>1</label><figDesc>Figure 1: a) What is Tom Hanks looking at? When we watch a movie, understanding what a character is paying attention to requires reasoning about multiple views. Many times, the character will be looking at something that fall outside the frame, just like in (a), and detecting what object the character is looking at can not be addressed by previous saliency and gaze following models. Solving this problem requires analyzing gaze, making use of semantic knowledge about the typical 3D relationships between different views, and recognizing the objects that are the common targets of attention, just like we do when watching a movie. Here we study the problem of gaze following in video where the object attended by a character might appear only on a separate frame. Given a video (b) around the frame containing the character (t =0 ) our system selects the frames likely to contain the object attended by the selected character (c) and produces the output shown in (d). This figure shows an actual result from our system.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 :</head><label>2</label><figDesc>Figure 2: VideoGaze Dataset: We present a novel large-scale dataset for gaze-following in video. Every person annotated in the dataset has its gaze annotated in five neighbor frames. We show some annotated examples from the dataset. In red, the frames without the gazed object on it. In green, we show the gaze annotations from the dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 :</head><label>3</label><figDesc>Figure 3: Network Architecture: Our model has three pathways. The saliency pathway (top left) finds salient spots on the target view. The gaze pathway (bottom left) computes the parameters of the cone coming out from the person's face. The transformation pathway (right) estimates the geometric relationship between views. The output is the gaze location density and the probability of x t of containing the gazed object.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head></head><label></label><figDesc>Figure 2 contains three samples from the dataset. On the left column we show the frame with the character on it. The other five frames are shown in the right with the gaze annotation if available (green).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 4 :</head><label>4</label><figDesc>Figure 4: Transformation and intersection: The cone pathway computes the cone parameters v and Œ±, and the transformation pathway estimates the geometric relation among the original view and the target view. The cone origin is u e and x h is indicated with the blue bounding box.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 5 :Figure 6 :</head><label>56</label><figDesc>Figure 5: Full Results: We show two detailed examples of how our model works. In a) and c), we show the probability distribution that our networks assigns to every frame in the video. Once the frame is selected, in b) and d) we show the final gaze prediction of our network. Original frame Target frame Cone projection Saliency map Final Output</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 :</head><label>7</label><figDesc>Figure 7: Following a character: We follow a character through a movie and list which elements he has seen during the film. Here we present three examples of our predictions.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_7"><head>Figure 8 :</head><label>8</label><figDesc>Figure 8: Similarity-performance representation: We plot performance versus similarity of the target and the source frame. Our model outperforms saliency and static gaze-following in all the similarity range for all the metrics.</figDesc></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgements. We thank Z. Bylinskii for proofreading. Funding for this research was partially supported by the Obra Social la Caixa Fellowship to AR and Samsung.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">What do different evaluation metrics tell us about saliency models?</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">*</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1604.03605</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Where should saliency models look next? In ECCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Bylinskii</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Oliva</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<publisher>Springer</publisher>
			<biblScope unit="page" from="809" to="824" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Teaching a humanoid robot to recognize and reproduce social cues</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Calinon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Billard</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proc. IEEE Intl Symposium on Robot and Human Interactive Communication</title>
		<meeting>IEEE Intl Symposium on Robot and Human Interactive Communication<address><addrLine>RoMan</addrLine></address></meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="346" to="351" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">3d visual proxemics: Recognizing human interactions in 3d from a single image</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Chakraborty</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Javed</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2013" />
			<biblScope unit="page" from="3406" to="3413" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
		<title level="m" type="main">Subjects and their objects: Localizing interactees for a person-centric view of importance. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Grauman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1" to="22" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Detone</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Malisiewicz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.03798</idno>
		<title level="m">Deep image homography estimation</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b6">
	<monogr>
		<title level="m" type="main">The PASCAL Visual Object Classes (VOC) Challenge. IJCV</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2010" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Social interactions: A first-person perspective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">K</forename><surname>Hodgins</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Learning to recognize daily actions using gaze</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Fathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">M</forename><surname>Rehg</surname></persName>
		</author>
		<idno>ECCV. 2012. 2</idno>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Handa</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bloesch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Patraucean</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Stent</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Mccormac</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Davison</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.07405</idno>
		<title level="m">gvnn: Neural network library for geometric computer vision</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="volume">3</biblScope>
			<biblScope unit="page">5</biblScope>
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Transforming auto-encoders</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">D</forename><surname>Wang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Artificial Neural Networks</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2011" />
			<biblScope unit="page" from="44" to="51" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">A parallel computation that assigns canonical object-based frames of reference</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">F</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 7th international joint conference on Artificial intelligence</title>
		<meeting>the 7th international joint conference on Artificial intelligence</meeting>
		<imprint>
			<publisher>Morgan Kaufmann Publishers Inc</publisher>
			<date type="published" when="1981" />
			<biblScope unit="volume">2</biblScope>
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<analytic>
		<title level="a" type="main">Talking heads: Detecting humans and recognizing their interactions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Hoai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="875" to="882" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Spatial transformer networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Simonyan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="2017" to="2025" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">Learning to predict where humans look</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Judd</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Ehinger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Durand</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Eye tracking for everyone</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Krafka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kellnhofer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Kannan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Bhandarkar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Matusik</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Imagenet classification with deep convolutional neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Krizhevsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Sutskever</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">A dataset and evaluation methodology for visual saliency in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Tian</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Gao</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2009 IEEE International Conference on Multimedia and Expo</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="442" to="445" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<analytic>
		<title level="a" type="main">Fast visual tracking using motion saliency in video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2007 IEEE International Conference on Acoustics, Speech and Signal Processing-ICASSP&apos;07</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2007" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page">1073</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<monogr>
		<title level="m" type="main">Detecting people looking at each other in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mar√≠n-Jim√©nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Eichner</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="volume">IJCV</biblScope>
			<biblScope unit="page" from="282" to="296" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Heres looking at you, kid. Detecting people looking at each other in videos</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">J</forename><surname>Mar√≠n-Jim√©nez</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Ferrari</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">BMVC</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="volume">5</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Deep head pose: Gaze-direction estimation in multimodal video</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">S</forename><surname>Mukherjee</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">M</forename><surname>Robertson</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">IEEE Transactions on Multimedia</title>
		<imprint>
			<biblScope unit="volume">17</biblScope>
			<biblScope unit="issue">11</biblScope>
			<biblScope unit="page" from="2094" to="2107" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Shallow and deep convolutional networks for saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Sayrol</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Giro-I Nieto</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Mcguinness</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>O&amp;apos;connor</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="volume">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">Predicting primary gaze behavior using social saliency fields</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Jain</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sheikh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICCV</title>
		<imprint>
			<date type="published" when="2013" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Augmented saliency model using automatic 3d head pose detection and learned gaze following in natural scenes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Parks</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Borji</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Itti</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Vision Research</title>
		<imprint>
			<biblScope unit="issue">3</biblScope>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<monogr>
		<title level="m" type="main">Where are they looking? In NIPS</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Recasens</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Vondrick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">6</biblScope>
			<biblScope unit="page" from="199" to="207" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Faster R-CNN: Towards real-time object detection with region proposal networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">NIPS</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">J</forename><surname>Rezende</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Eslami</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohamed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Battaglia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Jaderberg</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Heess</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.00662</idno>
		<title level="m">Unsupervised learning of 3d structure from images</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
	<note>Imagenet large scale visual recognition challenge. IJCV</note>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Social saliency prediction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Soo</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shi</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Tapaswi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Stiefelhagen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Torralba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1512.02902</idno>
		<title level="m">Movieqa: Understanding stories in movies through question-answering</title>
		<imprint>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b31">
	<analytic>
		<title level="a" type="main">A statistical approach to continuous self-calibrating eye gaze tracking for head-mounted virtual reality systems</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tripathi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Guenter</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">WACV, 2017 IEEE Winter Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="862" to="870" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">A game-theoretic probabilistic approach for detecting conversational groups</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Vascon</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">Z</forename><surname>Mequanint</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Cristani</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Pelillo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Murino</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Asian Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2014" />
			<biblScope unit="page" from="658" to="675" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">A novel method for generation of motion saliency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Su</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">2010 IEEE International Conference on Image Processing</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2010" />
			<biblScope unit="page">3</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<analytic>
		<title level="a" type="main">Appearancebased gaze estimation in the wild</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Sugano</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Fritz</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Bulling</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">CVPR</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="4511" to="4520" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
