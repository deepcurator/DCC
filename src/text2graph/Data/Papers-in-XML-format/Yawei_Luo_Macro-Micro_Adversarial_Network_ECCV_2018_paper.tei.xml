<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:21+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Macro-Micro Adversarial Network for Human Parsing</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yawei</forename><surname>Luo</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Zhedong</forename><surname>Zheng</surname></persName>
							<email>zdzheng12@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Liang</forename><surname>Zheng</surname></persName>
							<email>liangzheng06@gmail.com</email>
							<affiliation key="aff2">
								<orgName type="institution">Singapore University of Technology and Design</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Tao</forename><surname>Guan</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
								</address>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Junqing</forename><surname>Yu</surname></persName>
							<affiliation key="aff0">
								<orgName type="department">School of Computer Science and Technology</orgName>
								<orgName type="institution">Huazhong University of Science and Technology</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Yi</forename><surname>Yang</surname></persName>
							<email>yee.i.yang@gmail.com</email>
							<affiliation key="aff1">
								<orgName type="institution" key="instit1">CAI</orgName>
								<orgName type="institution" key="instit2">University of Technology</orgName>
								<address>
									<settlement>Sydney</settlement>
								</address>
							</affiliation>
						</author>
						<title level="a" type="main">Macro-Micro Adversarial Network for Human Parsing</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<textClass>
				<keywords>
					<term>Human parsing</term>
					<term>Adversarial network</term>
					<term>Inconsistency</term>
					<term>Macro- Micro</term>
				</keywords>
			</textClass>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Abstract. In human parsing, the pixel-wise classification loss has drawbacks in its low-level local inconsistency and high-level semantic inconsistency. The introduction of the adversarial network tackles the two problems using a single discriminator. However, the two types of parsing inconsistency are generated by distinct mechanisms, so it is difficult for a single discriminator to solve them both. To address the two kinds of inconsistencies, this paper proposes the Macro-Micro Adversarial Net (MMAN). It has two discriminators. One discriminator, Macro D, acts on the low-resolution label map and penalizes semantic inconsistency, e.g., misplaced body parts. The other discriminator, Micro D, focuses on multiple patches of the high-resolution label map to address the local inconsistency, e.g., blur and hole. Compared with traditional adversarial networks, MMAN not only enforces local and semantic consistency explicitly, but also avoids the poor convergence problem of adversarial networks when handling high resolution images. In our experiment, we validate that the two discriminators are complementary to each other in improving the human parsing accuracy. The proposed framework is capable of producing competitive parsing performance compared with the state-of-the-art methods, i.e., mIoU=46.81% and 59.91% on LIP and PASCAL-Person-Part, respectively. On a relatively small dataset PPSS, our pre-trained model demonstrates impressive generalization ability. The code is publicly available at https://github.com/RoyalVane/MMAN.</p></div>
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1">Introduction</head><p>Human parsing aims to segment a human image into multiple semantic parts. It is a pixel-level prediction task which requires to understand human images in both the global level and the local level. Human parsing can be widely applied to human behavior analysis <ref type="bibr" target="#b8">[9]</ref>, pose estimation <ref type="bibr" target="#b33">[34]</ref> and fashion synthesis <ref type="bibr" target="#b39">[40]</ref>. Recent advances in human parsing and semantic segmentation <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10,</ref><ref type="bibr" target="#b22">23,</ref><ref type="bibr" target="#b36">37,</ref><ref type="bibr" target="#b35">36]</ref> mostly explore the potential of the convolutional neural network (CNN). Based on CNN architecture, the pixel-wise classification loss is usually used <ref type="bibr" target="#b18">[19,</ref><ref type="bibr" target="#b33">34,</ref><ref type="bibr" target="#b9">10]</ref> which punishes the classification error for each pixel. Despite providing an effective baseline, the pixel-wise classification loss which is designed for perpixel category prediction, has two drawbacks. First, the pixel-wise classification loss may lead to local inconsistency, such as holes and blur. The reason is that it merely penalizes the false prediction on every pixel without explicitly considering the correlation among the adjacent pixels. For illustration, we train a baseline model (see Section 3.2) with the pixel-wise classification loss. As shown in <ref type="figure" target="#fig_0">Fig. 1(a)</ref>, some pixels which belongs to "arm" are incorrectly predicted as "upper-clothes" by the baseline. This is undesirable but is the consequence of local inconsistency of the baseline loss. Second, pixel-wise classification loss may lead to semantic inconsistency in the overall segmentation map, such as unreasonable human poses and incorrect spatial relationship of body parts. Compared to the local inconsistency, the semantic inconsistency is generated from deeper layers. When only looking at a local region, the learned model does not have an overall sense of the topology of body parts. As shown in <ref type="figure" target="#fig_0">Fig. 1(b)</ref>, the "arm" is merged with an adjacent "leg", indicating incorrect part topology (three legs). Therefore, the pixel-wise classification loss does not explicitly consider the semantic consistency, so that long-range dependency may not be well captured.</p><p>In the attempt to address the inconsistency problems, the conditional random fields (CRFs) <ref type="bibr" target="#b16">[17]</ref> can be employed as a post processing method. However, CRFs usually handle inconsistency in very limited scope (locally) due to the pairwise potentials, and may even generate worse label maps given poor initial segmentation result. As an alternative to CRFs, a recent work proposes the use of adversarial network <ref type="bibr" target="#b23">[24]</ref>. Since the adversarial loss assesses whether a label map is real or fake by joint configuration of many label variables, it can enforce higher-level consistency, which cannot be achieved with pairwise terms or the per-pixel classification loss. Now, an increasing number of works adopt the routine  and LossD(f ake) denote the adversarial losses of discriminator on real and fake image respectively, and LossG denotes the loss of generator. (a) Good convergence, where LossD(real) and LossD(f ake) converge to 0.5 and LossG converges to 0. It indicates a successful adversarial network training, where G is able to fool D. (b) Poor convergence, where LossD(real) and LossD(f ake) converge to 0 and LossG converges to 1. It stands for an unbalanced adversarial network training, where D can easily distinguish generated images from real images.</p><p>of combining the cross entropy loss with an adversarial loss to produce label maps closer to the ground truth <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27,</ref><ref type="bibr" target="#b11">12]</ref>. Nevertheless, the previous adversarial network also has its limitations. First, the single discriminator back propagates only one adversarial loss to the generator. However, the local inconsistency is generated from top layers and the semantic inconsistency is generated from deep layers. The two targeted layers can not be discretely trained with only one adversarial loss. Second, a single discriminator has to look at overall high-resolution image (or a large part of it) in order to supervise the global consistency. As mentioned by numbers of literatures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, it is very difficult for a generator to fool the discriminator on a high-resolution image. As a result, the single discriminator back propagates a maximum adversarial loss invariably, which makes the training unbalanced. We call it poor convergence problem, as shown in <ref type="figure" target="#fig_2">Fig. 2</ref>.</p><p>In this paper, the basic objective is to improve the local and semantic consistency of label maps in human parsing. We adopt the idea of adversarial training and at the same time aim to addresses its limitations, i.e., the inferior ability in improving parsing consistency with a single adversarial loss and the poor convergence problem. Specifically, we introduce the Macro-Micro Adversarial Nets (MMAN). MMAN consists of a dual-output generator (G) and two discriminators (D), named Macro D and Micro D. The three modules constitute two adversarial networks (Macro AN , Micro AN ), addressing the semantic consistency and the local consistency, respectively. Given an input human image, the CNN-based generator outputs two segmentation maps with different resolution levels, i.e., low resolution and high resolution. The input of Macro D is a low-resolution segmentation map, and the output is the confidence score of semantic consistency. The input of Micro D is the high-resolution segmentation result, and its outputs is the confidence score of local consistency. A brief pipeline of the proposed framework is shown in <ref type="figure">Fig. 3</ref>. It is in two critical aspects that MMAN departs from previous works. First, our method explicitly copes with the local inconsistency and semantic inconsistency problem using two task-specific adversarial networks individually. Second, our method does not use large-sized FOVs on high-resolution image, so we can avoid the poor convergence problem. More detailed description of the merits of the proposed network is provided in Section 3.5. Our contributions are summarized as follows:</p><formula xml:id="formula_0">Image CNN (Baseline) + Micro D + Macro D + Macro D + Micro D (MMAN) GT</formula><p>-We propose a new framework called Macro-Micro Adversarial Network (MMAN) for human parsing. The Macro AN and Micro AN focus on semantic and local inconsistency respectively, and work in complementary way to improve the parsing quality. -The two discriminators in our framework achieve local and global supervision on the label maps with small field of views (FOVs), which avoids the poor convergence problem caused by high-resolution images.</p><p>-The proposed adversarial net achieves very competitive mIoU on the LIP and PASCAL-Person-Part datasets, and can be well generalized on a relatively small dataset PPSS.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2">Related works</head><p>Our review focuses on three lines of literature most relevant to our work, i.e., CNNbased human parsing, the conditional random fields (CRFs) and the adversarial networks.</p><p>Human parsing. Recent progress in human parsing has been due to the two factors: 1) the available of the large-scale datasets <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b24">25,</ref><ref type="bibr" target="#b3">4]</ref>. Comparing to the small datasets, the large-scale datasets contain the common visual variance of people and provide a comprehensive evaluation. 2) the end-to-end learned model. Human parsing demands understanding the person on the pixel level. The recent works apply the convolutional neural network (CNN) to learn the segmentation result in an end-to-end manner. In <ref type="bibr" target="#b33">[34]</ref>, human poses are extracted in advance and utilized as strong structural cues to guide the parsing. In <ref type="bibr" target="#b20">[21]</ref>, four human-related contexts are integrated into a unified network. A novel human-related grammar is presented by <ref type="bibr" target="#b28">[29]</ref> which infers human body pose and human part segmentation jointly.</p><p>Conditional random fields Using the pixel-wise classification loss, CNN usually ignores the micro context between pixels and the macro context between semantic parts. Conditional random fields (CRFs) <ref type="bibr" target="#b16">[17,</ref><ref type="bibr" target="#b21">22,</ref><ref type="bibr" target="#b17">18]</ref> are one of the common methods to enforce spatial contiguity in the output label maps. Served as a post-process procedure for image segmentation, CRFs further fine-tune the output map. However, the most common used CRFs are with pair-wise potentials <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b25">26]</ref>, which has very limited parameters and handles low-level inconsistencies with a small scope. Higher-order potentials <ref type="bibr" target="#b15">[16,</ref><ref type="bibr" target="#b17">18]</ref> have also been observed to be effective in enforcing the semantic validity, but the corresponding energy pattern and the clique form are usually difficult to design. In summary, the utilization of context in CNN remains an open problem.</p><p>Adversarial networks. Adversarial networks have demonstrated the effectiveness in image synthesis <ref type="bibr" target="#b12">[13,</ref><ref type="bibr" target="#b27">28,</ref><ref type="bibr" target="#b29">30,</ref><ref type="bibr" target="#b38">39,</ref><ref type="bibr" target="#b37">38]</ref>. By minimizing the adversarial loss, the discriminator leads the generator to produce high-fidelity images. In <ref type="bibr" target="#b23">[24]</ref>, Luc et al. add the adversarial loss for training semantic segmentation and yield the competitive results. Similar idea then has been applied in street scene segmentation <ref type="bibr" target="#b11">[12]</ref> and medical image segmentation <ref type="bibr" target="#b4">[5,</ref><ref type="bibr" target="#b26">27]</ref>. Contemporarily, an increasing body of literature <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref> report the difficulty of training the adversarial networks on the high-resolution images. Discriminator can easily recognize the fake high-resolution image, which leads to the training unbalance. The generator and discriminator are prone to stuck in a local minimum.</p><p>The main difference between MMAN and the adversarial learning methods above is that the we explicitly endow adversarial training with the macro and micro subtasks. We observe that the two subtasks are complementary to each . Given an input image of size 3 × 256 × 256, the generator G first produces a low-resolution (8192 × 16 × 16) tensor, from which a low-resolution label map (C × 16 × 16) and a high-resolution label map (C × 256 × 256) are generated, where C is the number of classes. Finally, for the each label map (sized C ×16×16, for example), we concatenate it with an RGB image (sized 3×16×16) along the 1st axis (number of channels), which is fed into the corresponding discriminator.</p><p>other to achieve superior parsing accuracy to the baseline with a single adversarial loss and are able to reduce the risk of the training unbalance.</p><p>3 Macro-Micro Adversarial Network <ref type="figure" target="#fig_3">Figure 4</ref> illustrates the architecture of the proposed Macro-Micro Adversarial Network. The network consists of three components, i.e., a dual-output generator (G) and two task-specific discriminators (D M a and D M i ). Given an input image of size 3 × 256 × 256, G outputs two label maps of size C × 16 × 16 and C × 256 × 256, respectively. D M a supervises the entire label map of C × 16 × 16 and D M i focuses on patches of the label map of size C × 256 × 256, respectively, so that global and local inconsistencies are penalized. In Section 3.1, we illustrate the training objectives, followed by the structure illustration in Section 3.2, 3.3 and 3.4. The merits of the proposed network are discussed in Section 3.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1">Training Objectives</head><p>Given a human image x of shape 3 × H × W and a target label map y of shape C × H × W where C is the number of classes including the background, the traditional pixel-wise classification loss (multi-class cross-entropy loss) can be formulated as:</p><formula xml:id="formula_1">L mce (G) = H×W i=1 C c=1 −y ic logŷ ic ,<label>(1)</label></formula><p>whereŷ ic denotes the predicted probability of the class c on the i-th pixel. The y ic denotes the ground truth probability of the class c on the i-th pixel. If the i-th pixel belongs to class c, y ic = 1, else y ic = 0. To enforce the spatial consistency, we combine the pixel-wise classification loss with the adversarial loss. It can be formulated as:</p><formula xml:id="formula_2">L mix (G, D) = L mce (G) + λL adver (G, D),<label>(2)</label></formula><p>where λ controls the relative importance of the pixel-wise classification loss and the adversarial loss. Specifically, the adversarial loss</p><formula xml:id="formula_3">L adver (G, D) is: L adver (G, D) =E x,y [log D(x, y)]+ E x [log(1 − D(x, G(x))].<label>(3)</label></formula><p>As shown in <ref type="figure" target="#fig_3">Fig. 4</ref>, the proposed MMAN employs the "cross-entropy loss + adversarial loss" to supervise both the bottom and top output from the generator G:</p><formula xml:id="formula_4">L M M AN (G,D M a , D M i ) = L adver (G, D M a ) + λ 1 L mce l (G) + λ 2 L adver (G, D M i ) + λ 3 L mce h (G),<label>(4)</label></formula><p>where L mce l (G) donates the cross-entropy loss between the low-resolution output and the small-sized target label map, while the L mce h (G) refers to the crossentropy loss between the high-resolution output and the original ground-truth label map. Similarly, L adver (G, D M a ) is the adversarial loss focusing on the low-resolution map, and L adver (G, D M i ) is based on the high-resolution map. The hyper parameters λ 1 , λ 2 and λ 3 control the relative importance of the four losses. The training objective of MMAN is:</p><formula xml:id="formula_5">G * , D * M a , D * M i = arg min G max D M a ,D M a L M M AN (G, D M a , D M i ).<label>(5)</label></formula><p>We solve Eq. 5 by alternate between optimizing G,</p><formula xml:id="formula_6">D M a and D M i until L M M AN (G, D M a , D M i ) converges.</formula></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2">Dual-output Generator</head><p>For the generator (G), we utilize DeepLab-ASPP <ref type="bibr" target="#b1">[2]</ref> framework with ResNet-101 <ref type="bibr" target="#b10">[11]</ref> model pre-trained on the ImageNet dataset <ref type="bibr" target="#b5">[6]</ref> as our starting point due to its simplicity and effectiveness. We augment DeepLab-ASPP architecture with cascaded upsampling layers and skip connect them with early layers, which is similar with U-net <ref type="bibr" target="#b30">[31]</ref>. Furthermore, we add a bypass to output the deep feature tensor from the bottom layers and transfer it to a label map with a convolution layer. The small-sized label map severs as the second output in parallel with the original sized label map from the top layer. We refer to the augmented dual-output architecture as Do-DeepLab-ASPP and adopt it as our baseline. For the dual output, we supervise the cross-entropy loss from top layers with ground truth label maps of original size, since it can retain visual details. Besides, we supervise the cross-entropy loss of bottom layers with a resized label map, i.e., 1/16 times of the original size. The shrunken label map pays more attentions to the coarse-grained human structure. The same strategy is applied to adversarial loss. We concatenated the respect label map with RGB image of corresponding size along class channel as a strong condition to discriminators.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.3">Macro Discriminator</head><p>Macro discriminator (D M a ) aims to lead the generator to produce realistic label map that consist with high-level human characteristics, such as reasonable human poses and correct spatial relationship of body parts. D M a is attached to the bottom layer of G and focuses on an overall low-resolution label map. It consists of 4 convolution layers with kernel size of 4×4 and stride of 2. Each convolution layer follows by one instance-norm layer and one LeakyRelu function. Given a output label map from G, D M a downsamples it to 1 × 1 to achieve the global supervision on it. The output of D M a is the confidence score of semantic consistency.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.4">Micro Discriminator</head><p>Micro discriminator (D M i ) is designed to enforce the local consistency in label maps. We follow the idea of "PatchGAN" <ref type="bibr" target="#b12">[13]</ref> in designing the D M i . Different from D M a that has a global receptive field on the (shrunken) label map, D M i only penalizes local error at the scale of image patches. The kernel size of D M i is 4 × 4 and the stride is 2. Micro D has a shallow structure of 3 convolution layers, each convolution layer follows by one instance-norm layer and one LeakyRelu function. D M i aims to classify if each 22 × 22 patch in an high-resolution image is real or fake, which is suitable for enforcing the local consistency. After running D M i convolutationally across the label map, we will obtain multiple response from every receptive field. We finally averages all responses to provide the ultimate output of D M i .</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.5">Discussions</head><p>In CNN-based human parsing, convolution layers go deep to extract part-level features, and deconvolution layers bring the in-depth features back to pixel-level locations. It seems intuitive to arrange the Macro D to deeper layers to supervise high-level semantic features and Micro D to top layers, focusing on low-level visual features. Besides the intuitive motivation, however, we can benefit more from such arrangement. The merits of MMAN are summarized in four aspects.</p><p>Functional specialization of Macro D and Micro D. Compared with the single discriminator which attempts to solve two levels of inconsistency alone, Macro D and Micro D are specified in addressing one of the two consistency problems. Take Macro D as an example. First, Macro D is attached to the deep layer of G. Because the semantic inconsistency is originally generated from the deep layers, a such designed Macro D allows the loss to back propagated to G more directly. Second, Macro D acts on a low-resolution label map that retains the semantic-level human structure while filtering out the pixel-level details. It enforces Macro D to focus on the global inconsistency without disturbing by local errors. The same reasoning applies to Micro D. In section 4.5, we validate that MMAN consistently outperforms the adversarial networks with a single adversarial loss <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref>.</p><p>Functional complementarity of Macro D and Micro D. As mentioned in <ref type="bibr" target="#b34">[35]</ref>, supervising classification loss in early deep layers can offer a good coarsegrained initialization for later top layers. Correspondingly, decreasing the loss in top layers can remedy the coarse semantic feature with fine-grained visual details. We assume that the adversarial loss has the same characteristic to work in complementary pattern. We clarify our hypothesis in Section 4.4.</p><p>Small FOVs to avoid poor convergence problem. Reported by increasing literatures <ref type="bibr" target="#b6">[7,</ref><ref type="bibr" target="#b13">14]</ref>, the existing adversarial networks have drawbacks in coping with complex high-resolution images. In our framework, Macro D acts on a low-resolution label map and Micro D has multiple but small FOVs on a highresolution label map. As a result, both Macro D and Micro D avoid using large FOVs as the actual input, which effectively reduce the convergence risk caused by high resolution. We show this benefit in Section 4.5.</p><p>Efficiency. Comparing with the single adversarial network <ref type="bibr" target="#b23">[24,</ref><ref type="bibr" target="#b4">5]</ref>, MMAN achieves the supervision across the overall images with two shallower discriminators, which have fewer parameters. It also owning to the small FOVs of the discriminators. The efficiency of MMAN is showed in variant study in Section 4.5.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4">Experiment</head></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1">Dataset</head><p>LIP <ref type="bibr" target="#b9">[10]</ref> is a recently introduced large-scale dataset, challenging in the severe pose complexity, heavy occlusions and body truncation. It contains 50,462 images in total, including 30,362 for training, 10,000 for testing and 10,000 for validation. LIP defines 19 human part (clothes) labels, including hat, hair, sunglasses, upperclothes, dress, coat, socks, pants, gloves, scarf, skirt, jumpsuits, face, right arm, left arm, right leg, left leg, right shoe and left shoe, and a background class.</p><p>PASCAL-Person-Part <ref type="bibr" target="#b3">[4]</ref> annotates the human part segmentation labels and is a subset of PASCAL-VOC 2010 <ref type="bibr" target="#b7">[8]</ref>. PASCAL-Person-Part includes 1,716 images for training and 1,817 for testing. In this dataset, an image may contain multiple persons with unconstrained poses and environment. Six human body part classes and the background class are annotated.</p><p>PPSS <ref type="bibr" target="#b24">[25]</ref> includes 3,673 annotated samples, which are divided into a training set of 1,781 images and a testing set of 1,892 images. It defines seven human parts and a background class. Collected from 171 surveillance videos, the dataset can reflect the occlusion and illumination variation in real scene.</p><p>Evaluation metric. The human parsing accuracy of each class is measured in terms of pixel intersection-over-union (IoU). The mean intersection-over-union (mIoU) is computed by averaging the IoU across all classes. We use both IoU for each class and mIoU as evaluation metrics for each dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2">Implementation Details</head><p>In our implementation, input images are resized so that its shorter side is fixed to 288. A 256×256 crop is randomly sampled from the image or its horizontal flipped version. The per-pixel mean is subtracted from the cropped image. We adopt instance normalization <ref type="bibr" target="#b31">[32]</ref> after each convolution. For the hyperparameters in Eq.4, we set λ 1 = 25, λ 2 = 1 and λ 3 = 100. For the down-sampling network of the generator, we use the ImageNet <ref type="bibr" target="#b5">[6]</ref> pretrained network as initialization. The weights of the rest of the network are initialized from scratch using Gaussian distribution with standard deviation as 0.001. We use Adam optimizer <ref type="bibr" target="#b14">[15]</ref> with a mini-batch size of 1. We set β1 = 0.9, β2 = 0.999 and weightdecay = 0.0001. Learning rate starts from 0.0002. On the LIP dataset, learning rate is divided by 10 after 15 epochs, and the models are trained for 30 epochs. On the PascalPerson-Part dataset, learning rate is divided by 10 after 25 epochs, and the models are trained for 50 epochs. We use dropout in the deconvolution layers, following the practice in <ref type="bibr" target="#b12">[13]</ref>. We alternately optimize the D and G. During testing, we average the per-pixel classification scores at multiple scales, i.e., testing images are resized to {0.8, 1, 1.2} times of their original size.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.3">Comparison with the State-of-the-Art Methods</head><p>In this section, we compare our result with the state-of-the-art methods on the three datasets. First, on the LIP dataset, we compare MMAN with five state-of-the-art methods in <ref type="table" target="#tab_1">Table 1</ref>. The proposed MMAN yields an mIoU of 46.65%, while the mIoU of the five competing methods is 18.17% <ref type="bibr" target="#b0">[1]</ref>, 28.29% <ref type="bibr" target="#b22">[23]</ref>, 42.92% <ref type="bibr" target="#b2">[3]</ref>, 44.13% <ref type="bibr" target="#b1">[2]</ref> and 44.73% <ref type="bibr" target="#b9">[10]</ref>, respectively. For a fair comparison, we further implement ASN <ref type="bibr" target="#b23">[24]</ref> and SSL <ref type="bibr" target="#b9">[10]</ref> on our baseline, i.e, Do-Deeplab-ASPP. On the same baseline, MMAN outperforms ASN <ref type="bibr" target="#b23">[24]</ref> and SSL <ref type="bibr" target="#b9">[10]</ref> by +1.40% and +0.62% in terms of mIoU, respectively. It clearly indicates that our method outperforms the state of the art. The comparison of per-class IoU indicates that improvement is mainly from classes which are closely related to human pose, such as arms, legs and shoes. In particular, MMAN is capable of distinguishing between "left" and "right", which gives a huge boost in following human parts: more than +2.5% improvement in left/right arm, more than +10% improvement in left/right leg and more than +5% improvement in left/right shoe. The comparison implies that MMAN is capable of enforcing the consistency of semantic-level features, i.e., human pose. Method hat hair glov sung clot dress coat sock pant suit scarf skirt face l-arm r-arm l-leg r-leg l-sh r-sh bkg avg  Second, on PASCAL-Person-Part, the comparison is shown in <ref type="table" target="#tab_3">Table 2</ref>. We apply the same model structure used on the LIP dataset to train the PASCALPerson-Part dataset. Our model yields an mIoU of 58.45% on the test set. It is higher than most of the compared methods and is only slightly inferior to "Attention+SSL" <ref type="bibr" target="#b9">[10]</ref> by 0.91%. This is probably due to the human scale variance in this dataset, which can be addressed by the attention algorithm proposed in <ref type="bibr" target="#b2">[3]</ref> and applied in <ref type="bibr" target="#b9">[10]</ref>.</p><p>Therefore, we add a plug-and-play module to our model, i.e., attention network <ref type="bibr" target="#b2">[3]</ref>. In particular, we employ multi-scale input and use the attention network to merge the results. The final model "Attention+MMAN" improves mIoU to 59.91%, which is higher than the current state-of-the-art method <ref type="bibr" target="#b9">[10]</ref> by +0.55%. When we look into the per-class IoU scores, we have similar observations to the those on LIP. The largest improvement can be observed in arms and legs. The improvement over the state-of-the-art methods <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b19">20,</ref><ref type="bibr" target="#b2">3]</ref> is over +0.6% in upper arms, over +1.8% in lower arms, over +0.4% in upper legs and over +0.9% in lower legs, respectively. The comparisons indicate that our method is very competitive.</p><p>Third, we deploy the model trained on LIP to the testing set of the PPSS dataset without any fine-tuning. We aim to evaluate the generalization ability of the proposed model.</p><p>To make the labels in the LIP and PPSS datasets consistent, we merge the fine-grained labels of LIP into coarse-grained human part labels defined in PPSS.  The evaluation result is reported in <ref type="table" target="#tab_4">Table 3</ref>. MMAN yields an mIoU of 52.11%, which significantly outperforms DL <ref type="bibr" target="#b24">[25]</ref> DDN <ref type="bibr" target="#b24">[25]</ref> and ASN <ref type="bibr" target="#b23">[24]</ref> by +16.9% , +4.9% and +1.4%, respectively. Therefore, when directly tested on another dataset with different image styles, our model still yields good performance.</p><p>In <ref type="figure" target="#fig_4">Fig. 5</ref>, we provide some segmentation examples obtained by Baseline (Do-Deeplab-ASPP), Baseline+Macro D, Baseline+Micro D and full MMAN, respectively. The ground truth label maps are also shown. We observe that Baseline+Micro D reduces the blur and noise significantly and aids to generate sharp boundaries, and that Baseline+Macro D corrects the unreasonable human poses. The full MMAN method integrates the advantages of both Macro AN and Micro AN and achieves higher parsing accuracy. We also present qualitative results on the PPSS dataset in <ref type="figure" target="#fig_5">Fig. 6</ref>.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.4">Ablation Study</head><p>This section presents ablation studies of our method. Since two components are involved, i.e., Macro D and Micro D, we remove them one at a time to evaluate their contributions respectively. Results on LIP and PASCAL-PersonPart datasets are shown in <ref type="table" target="#tab_1">Table 1 and Table 2</ref>  To further evaluate the respective function of the two different discriminators, we add two external experiments: 1) For Macro D, we calculate another mIoU using the low-resolution segmentation maps, which filter out pixel-wise details and retain high-level human structures. So this new mIoU is more suitable for evaluating Macro D. 2) For Micro D, we count the "isolated pixels" in highresolution segmentation maps, which reflects local inconsistency such as "holes". The "isolated pixel rate" (IPR) can be viewed as a better indicator for evaluating Micro D. We see from <ref type="table" target="#tab_6">Table 4</ref> that Macro D is better than Micro D at improving "mIoU (low-reso.)", proving that Macro D specializes in preserving high-level human structures. We also see that Micro D is better than Macro D at decreasing IPR, suggesting that Micro D specializes in improving local consistency of the result.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.5">Variant Study</head><p>We further evaluate three different variants of MMAN, i.e., Single AN, Double AN, and Multiple AN, on the LIP dataset. <ref type="table" target="#tab_7">Table 5</ref>  Single AN refers to the traditional adversarial network with only one discriminator. The discriminator is attached to the top layer and has a global receptive field on a 256 × 256 label map. As the result shows, Single AN yields 45.23% in mean IoU, which is slightly higher than the baseline but lower than MMAN. This result suggests that employing Macro D and Micro D outperforms the single  Multiple AN is designed to evaluate the parsing accuracy when employing more than two discriminators. To this end, we attach an extra discriminator to the 3rd deconvolution layer of G. In particular, the discriminator has the same architecture with micro D and focuses on 22 × 22 patches on a 64 × 64 label map. As the result shows in <ref type="table" target="#tab_7">Table 5</ref>, employing three discriminators brings very slightly improvement (0.16%) in mean IoU, but results in more complex architecture and more parameters.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5">Conclusions</head><p>In this paper, we introduce a novel Macro-Micro adversarial network (MMAN) for human parsing, which significantly reduces the semantic inconsistency, e.g., misplaced human parts, and the local inconsistency, e.g., blur and holes, in the parsing results. Our model achieves comparative parsing accuracy with the state-of-the-art methods on two challenge human parsing datasets and has a good generalization ability on other datasets. The two adversarial losses are complementary and outperform previous methods that employ a single adversarial loss. Furthermore, MMAN achieves both global and local supervisions with small receptive fields, which effectively avoids the poor convergence problem of adversarial network in handling high-resolution images.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Fig. 1 :</head><label>1</label><figDesc>Fig. 1: Drawbacks of the pixel-wise classification loss. (a) Local inconsistency, which leads to a hole on the arm. (b) Semantic inconsistency, which causes unreasonable human poses. The inconsistencies are indicated by red arrows.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Fig. 2 :</head><label>2</label><figDesc>Fig. 2: Two types of convergence in adversarial network training. LossD(real) and LossD(f ake) denote the adversarial losses of discriminator on real and fake image respectively, and LossG denotes the loss of generator. (a) Good convergence, where LossD(real) and LossD(f ake) converge to 0.5 and LossG converges to 0. It indicates a successful adversarial network training, where G is able to fool D. (b) Poor convergence, where LossD(real) and LossD(f ake) converge to 0 and LossG converges to 1. It stands for an unbalanced adversarial network training, where D can easily distinguish generated images from real images.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Fig. 4 :</head><label>4</label><figDesc>Fig. 4: MMAN has three components: a dual-output generator (blue dashed box), a Macro discriminator (green dashed box) and a Micro discriminator (orange dashed box). Given an input image of size 3 × 256 × 256, the generator G first produces a low-resolution (8192 × 16 × 16) tensor, from which a low-resolution label map (C × 16 × 16) and a high-resolution label map (C × 256 × 256) are generated, where C is the number of classes. Finally, for the each label map (sized C ×16×16, for example), we concatenate it with an RGB image (sized 3×16×16) along the 1st axis (number of channels), which is fed into the corresponding discriminator.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Fig. 5 :</head><label>5</label><figDesc>Fig. 5: Qualitative parsing results on the Pascal-Person-Part dataset.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Fig. 6 :</head><label>6</label><figDesc>Fig. 6: Qualitative parsing results on the PPSS dataset. RGB image and the label map are showed in pairs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head></head><label></label><figDesc>details the numer of parameter, global FOV (g.FOV) and local FOV (l.FOV) sizes, as well as the architecture sketch of each variant. The result of original MMAN is also presented for a clear comparison.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_1" validated="false"><head>Table 1 :</head><label>1</label><figDesc>Method comparison of per-class IoU and mIoU on LIP validation set.</figDesc><table></table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_3" validated="false"><head>Table 2 :</head><label>2</label><figDesc></figDesc><table>Performance comparison in terms of per-class IoU with five state-of-the-
art methods on the PASCAL-Person-Part test set. 

Method 
head 
torso 
u-arms 
l-arms 
u-legs 
l-legs 
bkg 
avg 
Deeplab-ASPP [2] 
81.33 
60.06 
41.16 
40.95 
37.49 
32.56 
92.81 
55.19 
HAZN [33] 
80.79 
59.11 
43.05 
42.76 
38.99 
34.46 
93.59 
56.11 
Attention [3] 
81.47 
59.06 
44.15 
42.50 
38.28 
35.62 
93.65 
56.39 
LG-LSTM [20] 
82.72 
60.99 
45.40 
47.76 
42.33 
37.96 
88.63 
57.97 
Attention + SSL [10] 
83.26 
62.40 
47.80 
45.58 
42.32 
39.48 
94.68 
59.36 
Do-Deeplab-ASPP 
81.82 
59.53 
44.80 
42.79 
38.32 
36.38 
93.91 
56.79 
Macro AN 
82.01 
61.19 
45.24 
44.30 
39.73 
36.75 
93.89 
57.58 
Micro AN 
82.44 
61.35 
44.79 
43.68 
38.41 
36.05 
93.93 
57.23 
MMAN 
82.46 
61.41 
46.05 
45.17 
40.93 
38.83 
94.30 
58.45 
Attention + MMAN 
82.58 
62.83 
48.49 
47.37 
42.80 
40.40 
94.92 
59.91 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_4" validated="true"><head>Table 3 :</head><label>3</label><figDesc>Comparison of human parsing accuracy on the PPSS dataset [25]. Best performance is highlighted in blue.</figDesc><table>Method 
head 
face 
up-cloth 
arms 
lo-cloth 
legs 
bkg 
avg 
DL [25] 
22.0 
29.1 
57.3 
10.6 
46.1 
12.9 
68.6 
35.2 
DDN [25] 
35.5 
44.1 
68.4 
17.0 
61.7 
23.8 
80.0 
47.2 
ASN [24] 
51.7 
51.0 
65.9 
29.5 
52.8 
20.3 
83.8 
50.7 
MMAN 
53.1 
50.2 
69.0 
29.4 
55.9 
21.4 
85.7 
52.1 

Image 
Baseline 
Full MMAN 
GT 
+Macro D 
+Micro D 

head 
torso 
upper arms 
lower arms 
upper legs 
lower legs 
background 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_5" validated="false"><head></head><label></label><figDesc>, respectively.</figDesc><table>face 
hair 
upper-clothes 
arms 
lower-clothes 
shoes 
background 
legs 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_6" validated="true"><head>Table 4 :</head><label>4</label><figDesc>Comparison in IPR and mIOUs method IPR mIoU (low-reso.) mIoU (high-reso.)</figDesc><table>baseline 
5.62 
50.66 
44.72 

+macro D 4.23 
55.79 
45.60 

+micro D 2.81 
53.60 
45.52 

+CRF 
1.53 
52.77 
45.45 

MMAN 
2.47 
56.95 
46.81 

</table></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_7" validated="false"><head>Table 5 :</head><label>5</label><figDesc>Variant study of MMAN.discriminator, which proves the correctness of the analysis in Section 3.5. What is more, we observe the poor convergence (pc) problem when training the Single AN. It is due to the employment of large FOVs on the high-resolution label map. Double AN has the same number of discriminators with MMAN. The difference lies in that the Double AN attaches the Macro D to the top layer. Compared to Double AN, MMAN significantly improves the result by 0.82%. The result illustrates the complementary effects of Macro D and Micro D: Macro D acts on deep layers and offers a good coarse-grained initialization for later top layers and Micro D helps to remedies the coarse semantic feature with fine-grained visual details.</figDesc><table>variant arch. g.FOV 
l.FOV #par pc. mIoU 

sAN 
256 × 256 
-
3.2M 
√ 45.23 

dAN 
256 × 256 22 × 22 3.8M 
√ 46.15 

mAN 
16 × 16 22 × 22 1.8M -46.97 

MMAN 
16 × 16 22 × 22 1.2M -46.81 

</table></figure>
		</body>
		<back>

			<div type="acknowledgement">
<div xmlns="http://www.tei-c.org/ns/1.0"><p>Acknowledgment. This work is partially supported by the National Natural Science Foundation of China (No. 61572211). We acknowledge the Data to Decisions CRC (D2D CRC) and the Cooperative Research Centers Programme for funding this research.</p></div>
			</div>

			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title level="m" type="main">Segnet: A deep convolutional encoderdecoder architecture for image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Badrinarayanan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Kendall</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Cipolla</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="volume">39</biblScope>
			<biblScope unit="page" from="2481" to="2495" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b1">
	<monogr>
		<title level="m" type="main">Deeplab: Semantic image segmentation with deep convolutional nets, atrous convolution, and fully connected crfs</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Papandreou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">I</forename><surname>Kokkinos</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1606.00915</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Attention to scale: Scale-aware semantic image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3640" to="3649" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Detect what you can: Detecting and representing objects using holistic models and body parts</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Mottaghi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2014" />
			<biblScope unit="page" from="1971" to="1978" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Doyle</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">P</forename><surname>Xing</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.08770</idno>
		<title level="m">Scan: Structure correcting adversarial network for chest x-rays organ segmentation</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Imagenet: A large-scale hierarchical image database</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Socher</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">J</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Fei-Fei</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision and Pattern Recognition</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2009" />
			<biblScope unit="page" from="248" to="255" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Deep generative image models using a laplacian pyramid of adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">L</forename><surname>Denton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Fergus</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1486" to="1494" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Everingham</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Van Gool</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">K I</forename><surname>Williams</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Winn</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Zisserman</surname></persName>
		</author>
		<ptr target="http://www.pascal-network.org/challenges/VOC/voc2010/workshop/index.html" />
		<title level="m">The PAS-CAL Visual Object Classes Challenge 2010 (VOC2010) Results</title>
		<imprint/>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<analytic>
		<title level="a" type="main">Concepts not alone: Exploring pairwise relationships for zero-shot video activity recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Gan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>De Melo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">G</forename><surname>Hauptmann</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">AAAI. p</title>
		<imprint>
			<biblScope unit="page">3487</biblScope>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b9">
	<monogr>
		<title level="m" type="main">Look into person: Self-supervised structuresensitive learning and a new benchmark for human parsing</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Gong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1703.05446</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">Deep residual learning for image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>He</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ren</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Sun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="770" to="778" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<monogr>
		<title level="m" type="main">Adversarial learning for semi-supervised semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><forename type="middle">C</forename><surname>Hung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">H</forename><surname>Tsai</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">T</forename><surname>Liou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><forename type="middle">Y</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">H</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1802.07934</idno>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Image-to-image translation with conditional adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Isola</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">Y</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Zhou</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">A</forename><surname>Efros</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<monogr>
		<title level="m" type="main">Progressive growing of gans for improved quality, stability, and variation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Karras</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Aila</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Laine</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Lehtinen</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ICLR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<monogr>
		<title level="m" type="main">Adam: A method for stochastic optimization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><forename type="middle">P</forename><surname>Kingma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Ba</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1412.6980</idno>
		<imprint>
			<date type="published" when="2014" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Robust higher order potentials for enforcing label consistency</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Kohli</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">82</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page" from="302" to="324" />
			<date type="published" when="2009" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Efficient inference in fully connected crfs with gaussian edge potentials</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Krähenbühl</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Koltun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in neural information processing systems</title>
		<imprint>
			<date type="published" when="2011" />
			<biblScope unit="page" from="109" to="117" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Arnab</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">H</forename><surname>Torr</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1709.03612</idno>
		<title level="m">Holistic, instance-level human parsing</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Deep human parsing with active template regression</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Dong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">37</biblScope>
			<biblScope unit="page" from="2402" to="2414" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Semantic object parsing with local-global long short-term memory</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3185" to="3193" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Human parsing with contextualized convolutional neural network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Liang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Tang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Yan</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1386" to="1394" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Semantic image segmentation via deep parsing network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2015 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1377" to="1385" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<analytic>
		<title level="a" type="main">Fully convolutional networks for semantic segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><surname>Shelhamer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Darrell</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE conference on computer vision and pattern recognition</title>
		<meeting>the IEEE conference on computer vision and pattern recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="3431" to="3440" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<monogr>
		<title level="m" type="main">Semantic segmentation using adversarial networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luc</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Couprie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chintala</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Verbeek</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1611.08408</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Pedestrian parsing via deep decompositional network</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Tang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Computer Vision (ICCV), 2013 IEEE International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2013" />
			<biblScope unit="page" from="2648" to="2655" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b25">
	<analytic>
		<title level="a" type="main">Accurate localization for mobile device using a multi-planar city model</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Guan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Pan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Yu</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Pattern Recognition (ICPR), 2016 23rd International Conference on</title>
		<imprint>
			<publisher>IEEE</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3733" to="3738" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b26">
	<analytic>
		<title level="a" type="main">Adversarial training and dilated convolutions for brain mri segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Moeskops</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Veta</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><forename type="middle">W</forename><surname>Lafarge</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">A</forename><surname>Eppenhof</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">P</forename><surname>Pluim</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Deep Learning in Medical Image Analysis and Multimodal Learning for Clinical Decision Support</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2017" />
			<biblScope unit="page" from="56" to="64" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b27">
	<monogr>
				<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Odena</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Olah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Shlens</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1610.09585</idno>
		<title level="m">Conditional image synthesis with auxiliary classifier gans</title>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b28">
	<monogr>
		<title level="m" type="main">Attribute and-or grammar for joint parsing of human pose, parts and attributes</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Park</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Nie</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">C</forename><surname>Zhu</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b29">
	<analytic>
		<title level="a" type="main">Learning what and where to draw</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">E</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Akata</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Mohan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Tenka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Schiele</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Lee</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="217" to="225" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b30">
	<analytic>
		<title level="a" type="main">U-net: Convolutional networks for biomedical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Ronneberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Fischer</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Brox</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Medical image computing and computer-assisted intervention</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2015" />
			<biblScope unit="page" from="234" to="241" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b31">
	<monogr>
		<title level="m" type="main">Instance normalization: The missing ingredient for fast stylization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Ulyanov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Vedaldi</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><forename type="middle">S</forename><surname>Lempitsky</surname></persName>
		</author>
		<idno>CoRR abs/1607.08022</idno>
		<ptr target="http://arxiv.org/abs/1607.08022" />
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b32">
	<analytic>
		<title level="a" type="main">Zoom better to see clearer: Human and object parsing with hierarchical auto-zoom net</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">C</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2016" />
			<biblScope unit="page" from="648" to="663" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b33">
	<analytic>
		<title level="a" type="main">Pose-guided human parsing by an and/or graph using pose-context features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Xia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">AAAI. pp</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="3632" to="3640" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b34">
	<monogr>
		<title level="m" type="main">Segan: Adversarial network with multi-scale l 1 loss for medical image segmentation</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xue</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Xu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Long</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Huang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.01805</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b35">
	<analytic>
		<title level="a" type="main">Self-produced guidance for weakly-supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Kang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">European Conference on Computer Vision</title>
		<imprint>
			<publisher>Springer</publisher>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b36">
	<analytic>
		<title level="a" type="main">Adversarial complementary learning for weakly supervised object localization</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Wei</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Feng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Huang</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">IEEE CVPR</title>
		<imprint>
			<date type="published" when="2018" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b37">
	<monogr>
		<title level="m" type="main">Generalizing a person retrieval model heteroand homogeneously</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>ECCV</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b38">
	<monogr>
		<title level="m" type="main">Camera style adaptation for person re-identification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zhong</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Zheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Yang</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2018" />
			<publisher>CVPR</publisher>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b39">
	<analytic>
		<title level="a" type="main">Be your own prada: Fashion synthesis with structural coherence</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Zhu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Fidler</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Urtasun</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Lin</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><forename type="middle">C</forename><surname>Loy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Computer Vision (ICCV</title>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
