<?xml version="1.0" encoding="UTF-8"?>
<TEI xmlns="http://www.tei-c.org/ns/1.0" 
xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance" 
xsi:schemaLocation="http://www.tei-c.org/ns/1.0 C:\Grobid\grobid-0.5.5\grobid-home/schemas/xsd/Grobid.xsd"
 xmlns:xlink="http://www.w3.org/1999/xlink">
	<teiHeader xml:lang="en">
		<encodingDesc>
			<appInfo>
				<application version="0.5.5" ident="GROBID" when="2019-09-04T23:12+0000">
					<ref target="https://github.com/kermitt2/grobid">GROBID - A machine learning software for extracting information from scholarly documents</ref>
				</application>
			</appInfo>
		</encodingDesc>
		<fileDesc>
			<titleStmt>
				<title level="a" type="main">Low-Shot Learning with Imprinted Weights</title>
			</titleStmt>
			<publicationStmt>
				<publisher/>
				<availability status="unknown"><licence/></availability>
			</publicationStmt>
			<sourceDesc>
				<biblStruct>
					<analytic>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Hang</forename><surname>Qi</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Matthew</forename><surname>Brown Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<author>
							<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">David</forename><forename type="middle">G</forename><surname>Lowe Google</surname></persName>
							<affiliation key="aff0">
								<orgName type="institution">UCLA</orgName>
							</affiliation>
						</author>
						<title level="a" type="main">Low-Shot Learning with Imprinted Weights</title>
					</analytic>
					<monogr>
						<imprint>
							<date/>
						</imprint>
					</monogr>
				</biblStruct>
			</sourceDesc>
		</fileDesc>
		<profileDesc>
			<abstract>
<div xmlns="http://www.tei-c.org/ns/1.0" />
			</abstract>
		</profileDesc>
	</teiHeader>
	<text xml:lang="en">
		<body>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="1.">Introduction</head><p>Human vision can immediately recognize new categories after a person is shown just one or a few examples <ref type="bibr" target="#b9">[10,</ref><ref type="bibr" target="#b7">8]</ref>. For instance, humans can recognize a new face from a photo of an unknown person and new objects or fine-grained categories from a few examples by implicitly drawing connections from previously acquired knowledge. Although deep neural networks trained on millions of images have in some cases exceeded human performance in large-scale image recognition <ref type="bibr" target="#b14">[15]</ref>, under an open-world setting with emerging new categories it remains a challenging problem how to continuously expand the capability of an intelligent agent from limited new samples, also known as low-shot learning.</p><p>Embedding methods <ref type="bibr" target="#b24">[25]</ref> have a natural representation for low-shot learning, as new categories can be added simply by pushing data examples through the network and per- * The majority of the work was done while interning at Google.</p><p>forming a nearest neighbor algorithm on the result <ref type="bibr" target="#b15">[16]</ref>. It has long been realized in the semantic embedding literature that the activations of the penultimate layer of a ConvNet classifier can also be thought of as an embedding vector, which is a connection we further develop in this paper. ConvNets are the preferred solution for achieving the highest classification performance, and the softmax crossentropy loss is faster to train than the objectives typically used in embedding methods, such as triplet loss.</p><p>In this paper, we attempt to combine the best properties of ConvNet classifiers <ref type="bibr" target="#b0">1</ref> with embedding approaches for solving the low-shot learning problem. Inspired by the use of embeddings as proxies <ref type="bibr" target="#b10">[11]</ref> or agents <ref type="bibr" target="#b23">[24]</ref> for individual object classes, we argue that embedding vectors can be effectively compared to weights in the last linear layer of ConvNet classifiers. Our approach, called imprinting, is to compute these activations from a training image for a new object category and use an appropriately scaled version of these activation values as the final layer weights for the new category while leaving the weights of existing categories unchanged. This is extended to multiple training examples by incrementally averaging the activation vectors computed from the new training images, which our experiments find to outperform nearest-neighbor classification as used with embedding approaches.</p><p>We consider a low-shot learning scenario where a learner initially trained on base classes with abundant samples is then exposed to previously unseen novel classes with a limited amount of training data for each category <ref type="bibr" target="#b4">[5]</ref>. The goal is to have a learner that performs well on the combined set of classes. This setup aligns with human recognition which continuously learns new concepts during a lifetime.</p><p>Existing approaches exhibit characteristics that render them infeasible for resource-limited environments such as mobile devices and robots. For example, training a deep ConvNet classifier with stochastic gradient descent requires an extensive fine-tuning process that cycles through all prior training data together with examples from additional categories <ref type="bibr" target="#b4">[5]</ref>. Alternatively, semantic embedding methods such as <ref type="bibr" target="#b11">[12,</ref><ref type="bibr" target="#b15">16,</ref><ref type="bibr" target="#b17">18,</ref><ref type="bibr" target="#b19">20]</ref> can immediately remember new examples and use them for recognition without retraining. However, semantic embeddings are difficult to train due to the computationally expensive hard-negative mining step and these methods require storing all the embedding vectors of encountered examples at test time for nearest neighbor retrieval or classification.</p><p>We demonstrate that the imprinted weights enable instant learning in low-shot object recognition with a single new example. Moreover, since the resulting model after imprinting remains in the same parametric form as ConvNets, fine-tuning via backpropagation can be applied when more training samples are available and when iterative optimization is affordable. Experiments show that the imprinted weights provide a better starting point than the usual random initialization for fine-tuning all network weights and result in better final classification results for low-shot categories. Our imprinting method provides a potential model for immediate recognition in biological vision as well as a useful approach for on-line updates for novel training data, as in a mobile device or robot.</p><p>The remainder of the paper is organized as follows. In Section 2, we discuss related work. Section 3 discusses the connections between embedding training and classification. Section 4 describes our approach. Then we provide implementation details and evaluate our approach with experiments in Sections 5 and 6. Section 7 concludes the paper.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="2.">Related Work</head><p>Metric Learning. Metric learning has been successfully used to recognize faces of new identities <ref type="bibr" target="#b1">[2,</ref><ref type="bibr" target="#b15">16]</ref> and finegrained objects <ref type="bibr" target="#b10">[11,</ref><ref type="bibr" target="#b11">12,</ref><ref type="bibr" target="#b13">14,</ref><ref type="bibr" target="#b18">19,</ref><ref type="bibr" target="#b19">20]</ref>. The idea is to learn a mapping from inputs to vectors in an embedding space where the inputs of the same identity or category are closer than those of different identities or categories. Once the mapping is learned, at test time a nearest neighbors method can be used for retrieval and classification for new categories that are unseen during training.</p><p>Contrastive loss <ref type="bibr" target="#b1">[2]</ref> minimizes the distances between inputs with the same label while keeping the distances between inputs with different labels far apart. Rather than minimizing absolute distances, recent approaches formulate objectives focusing on relative distances. FaceNet <ref type="bibr" target="#b15">[16]</ref> optimizes a triplet loss and develops an online negative mining strategy to form triplets within a mini-batch. Instead of penalizing violating instance-based triplets independently, alternative loss functions regulate the global structure of the embedding space. Magnet loss <ref type="bibr" target="#b13">[14]</ref> optimizes the distribution of different classes by clustering the examples using kmeans and representing classes with centroids. Lifted structured loss <ref type="bibr" target="#b11">[12]</ref> incorporates all pair-wise relations within a mini-batch instead of forming triplets. The N -pair loss <ref type="bibr" target="#b17">[18]</ref> requires each batch to have examples from N categories for improved computational efficiency. All these methods require some online or offline batch generation step to form informative batches to speed up training. Structured clustering loss <ref type="bibr" target="#b18">[19]</ref> optimizes a clustering quality metric globally in the embeddings space.</p><p>The Proxy-NCA loss <ref type="bibr" target="#b10">[11]</ref> demonstrates faster convergence without requiring batch generation by assigning trainable proxies to each category, which we will describe in more detail in Section 3. NormFace <ref type="bibr" target="#b23">[24]</ref> explores a similar idea with all feature vectors normalized. The embedding can generalize to unseen categories, however the nearest neighbor model needs to store the embeddings of all reference points during testing. In our work, we retain the parametric form of ConvNet models and demonstrate that semantic embeddings can be used to imprint weights in the final layer. As a result, our approach has the same convergence advantages as <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b23">[24]</ref> during training, yet does not require storing embeddings for each training example or using nearest-neighbor search during inference.</p><p>One-shot and Low-shot Learning. One-shot or lowshot learning aims at training models with only one or a few training examples. The siamese network <ref type="bibr" target="#b6">[7]</ref> uses two network streams to extract features from a pair of images and regress the inputs to a similarity score between two feature vectors. Matching networks <ref type="bibr" target="#b21">[22]</ref> learn a neural network that maps a small support set of images from unseen categories and an unlabeled example to its label. Prototypical networks <ref type="bibr" target="#b16">[17]</ref> use the mean embeddings of new examples as prototypes, but the embedding space is local with respect to the support classes due to the episodic scheme. These works formulate the low-shot learning problem as classifying an image among a number of unseen classes characterized by the support images; a query image and a support set must be provided together every time at inference. However, this evaluation setup does not align with human vision and many real-world applications where a learner grows its capability as it encounters more categories and training samples. In contrast, we consider an alternative setup similar to <ref type="bibr" target="#b4">[5]</ref> which focuses on the overall performance of the learner on a combined set of categories including base classes represented by abundant examples together with novel low-shot classes. Hariharan and Girshick <ref type="bibr" target="#b4">[5]</ref> train a multi-layer perceptron to generate additional feature vectors from a single example by drawing an analogy with seen examples. Their method retrains the last linear classifier at the lowshot training stage, whereas our approach allows instant performance gain on novel classes without retraining. More similar to our work is <ref type="bibr" target="#b12">[13]</ref>, which trains parameter predictors for novel categories from activations. However, our method directly imprints weights from activations, which is made possible by architecture modifications that introduce a normalization layer.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.">Metric Learning and Softmax Classifiers</head><p>In this section, we discuss the connection between a proxy-based objective used in embedding training and softmax cross-entropy loss. Based on these observations, we then describe our method for extending ConvNet classifiers to new classes in the next section.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.1.">Proxy-based Embedding Training</head><p>Recent work has blurred the divide between triplet-based embedding training and softmax classification. For example, Neighborhood Components Analysis <ref type="bibr" target="#b3">[4]</ref> learns a distance metric with a softmax-like loss,</p><formula xml:id="formula_0">L NCA (x, y, Z) = − log exp(−d(x, y)) z∈Z exp(−d(x, z))<label>(1)</label></formula><p>which makes points x, y with the same label closer than examples z with different labels under the squared Euclidean distance d(x, y) = ||x − y|| 2 2 . Movshovitz-Attias et al. <ref type="bibr" target="#b10">[11]</ref> reformulated the loss by assigning proxies p(·) to training examples according to the class labels</p><formula xml:id="formula_1">L proxy (x) L NCA (x, p(x), p(Z)) = − log exp(−d(x, p(x))) p(z)∈p(Z) exp(−d(x, p(z))) ,<label>(2)</label></formula><p>where p(Z) is a set of all negative proxies. This formulation allows sampling anchor points x, rather than triplets, for each mini-batch and results in faster convergence than other objectives.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="3.2.">Connections to Softmax Classifiers</head><p>We will now discuss the connections between metric learning and softmax classifiers. We consider the case that each class has exactly one proxy and the proxy of a data point is determined statically according to its label. Concretely, let C be the set of category labels and P = {p 1 , p 2 , . . . , p |C| } be the set of trainable proxies, then the proxy of every point x is p(x) = p c(x) where c(x) ∈ C is the class label of x. We argue that the proxies p c are comparable to weights w c in softmax classifiers.</p><p>To see this, we assume point vectors and proxy vectors are normalized to the same length. It follows that minimizing the squared Euclidean distance between a point x and its proxy p(x) is equivalent to maximizing the inner-product, or equivalently cosine similarity, of the corresponding unit vectors</p><formula xml:id="formula_2">min d(x, p(x)) min ||x − p(x)|| 2 2 = max x ⊤ p(x),<label>(3)</label></formula><p>since ||u−v||</p><formula xml:id="formula_3">2 2 = 2−2u ⊤ v for unit vectors u, v ∈ R D .</formula><p>Substituting the squared Euclidean distance with inner product in Eq. 2, the resulting loss can be written as which is comparable to the softmax cross-entropy loss used for training classifiers</p><formula xml:id="formula_4">L(x, c(x)) = − log exp(x ⊤ p c(x) ) c∈C exp(x ⊤ p c ) ,<label>(4)</label></formula><formula xml:id="formula_5">L softmax (x, c(x)) = − log exp(x ⊤ w c(x) + b c(x) ) c∈C exp(x ⊤ w c + b c ) ,<label>(5)</label></formula><p>with bias terms b c = 0 for all c ∈ C.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.">Imprinting</head><p>Given the conceptual similarity of normalized embedding vectors and final layer weights as discussed above, it seems natural that we should be able to set network weights for a novel class immediately from a single exemplar. In the following, we outline our proposed method to do this, which we call imprinting. In essence, imprinting exploits the symmetry between normalized inputs and weights in a fully connected layer, copying the embedding activations for a novel exemplar into a new set of network weights.</p><p>To demonstrate this method, we focus on a two-stage low-shot classification problem where a learner is trained on a set of base classes with abundant training samples in the first stage and then grows its capability to additional novel classes for which only one or a few examples are available in the second stage.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.1.">Model Architecture</head><p>Our model consists of two parts. First, an embedding extractor φ : R N → R D , parameterized by a convolutional neural network, maps an input image x ∈ R N to a Ddimensional embedding vector φ(x). Different from standard ConvNet classifier architectures, we add an L 2 normalization layer at the end of the embedding extractor so that the output embedding has unit length, i.e. ||φ(x)|| 2 = 1. Second, a softmax classifier f (φ(x)) maps the embedding into unnormalized logit scores followed by a softmax activation that produces a probability distribution across all categories</p><formula xml:id="formula_6">f i (φ(x)) = exp(w ⊤ i φ(x)) c exp(w ⊤ c φ(x)) ,<label>(6)</label></formula><p>where w i is the i-th column of the weight matrix normalized to unit length. No bias term is used in this layer. We view each column of the weight matrix as a template of the corresponding category. Unlike in <ref type="bibr" target="#b10">[11]</ref> where only the embedding extractor part is used during test time with the auxiliary proxies thrown away, we keep the entirety of the network. In the forward pass, the last layer in our model computes the inner product between the embedding of the input image φ(x) and all the templates w i . With embeddings and templates normalized to unit lengths, the resulting prediction is equivalent to finding the nearest template in the embedding space in terms of squared Euclidean distanceŷ</p><formula xml:id="formula_7">= arg max c∈C w ⊤ c φ(x) = arg min c∈C d(φ(x), w c ). (7)</formula><p>Compared with non-parametric nearest neighbor models, however, our classifier only contains one template per class rather than storing a large set of reference data points.</p><p>Normalization. Normalizing embeddings and columns of the weight matrix in the last layer to unit lengths is an important architectural design in our model. Geometrically, normalized embeddings and weights lie on a highdimensional sphere. In contrast, existing deep neural networks normally encourage activations to have zero mean and unit variance within mini-batches <ref type="bibr" target="#b5">[6]</ref> or layers <ref type="bibr" target="#b0">[1]</ref> for optimization reasons while they do not address the scale differences between neuron activations and weights. In our model, as a result of normalizing embeddings and columns of the weight matrix, the magnitude differences do not affect the prediction as long as the angle between the normalized vectors remains the same, since the inner product w ⊤ i φ(x) ∈ [−1, 1] now measures cosine similarity. Recent work in cosine normalization <ref type="bibr" target="#b8">[9]</ref> discusses a similar idea of replacing the inner product with a cosine similarity for bounded activations and stable training, while we arrive at this design from a different direction. In particular, this establishes a symmetric relationship between normalized embeddings and weights, which enables us to treat them interchangeably.</p><p>Scale factor. The cosine similarity w ⊤ i φ(x) ∈ [−1, 1] can prevent the normalized probability of the correct class from reaching close to 1 when applying softmax activation. For example, consider for an input x the inner product producing 1 for the correct category and producing the minimum possible value −1 for the incorrect categories, the normalized probability is p(y i |x) = e 1 /[e 1 + (|C| − 1)e −1 ] = 0.069, assuming a total of |C| = 100 categories. In consequence, it fails to produce a distribution close to the one-hot encoding of the ground truth label and therefore imposes a lower bound on the cross-entropy loss. This effect becomes more severe as the number of categories increases. To alleviate this problem, we adapt a scaling factor in our model as discussed by Wang et al. <ref type="bibr" target="#b23">[24]</ref>. Concretely, we modify Eq. 6 by adding a trainable scalar s shared across all classes to scale the inner product</p><formula xml:id="formula_8">f i (φ(x)) = exp(sw ⊤ i φ(x)) c exp(sw ⊤ c φ(x)) .<label>(8)</label></formula><p>We also experimented with the option of using an adaptive scale factor per class, but we did not observe significant effects on classification accuracy compared to our use of a single global scale factor. In summary, our model architecture is similar to standard ConvNet classifiers except for two differences. The normalized embeddings and weights introduce a symmetric relationship that allows us to treat them interchangeably. The scaled inner product at the final layer enables training the entire model with the cross-entropy loss in the same way that standard ConvNet classifiers are trained. Next, we discuss how to extend such a classifier to novel categories by leveraging the symmetry between embeddings and weights.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="4.2.">Weight Imprinting</head><p>Inspired by the effectiveness of embeddings in retrieving and recognizing objects from unseen classes in metric learning, our proposed imprinting method is to directly set the final layer weights for new classes from the embeddings of training exemplars. Consider a single training sample x + from a novel class, our method computes the embedding φ(x + ) and uses it to set a new column in the weight matrix for the new class, i.e. w + = φ(x + ). <ref type="figure" target="#fig_0">Figure 1</ref> illustrates this idea of extending the final layer weight matrix of a trained classifier by imprinting additional columns for new categories.</p><p>Intuitively, one can think of the imprinting operation as remembering the semantic embeddings of low-shot examples as the templates for new classes. <ref type="figure" target="#fig_1">Figure 2</ref> illustrates the change of the decision boundaries after a new weight column is imprinted. The underlying assumption is that test examples from new classes are closer to the corresponding training examples, even if only one or a few are observed, than to instances of other classes in the embedding space. Notably, this desired property coincides with metric learning objectives such as triplet loss. The proxy-based loss, from which we have derived our method, upper bounds the instance-based triplet loss <ref type="bibr" target="#b10">[11]</ref>.</p><p>Average embedding. If n &gt; 1 examples {x</p><formula xml:id="formula_9">(i) + } n i=1</formula><p>are available for a new class, we compute new weights by averaging the normalized embeddingsw + = and re-normalizing the resulting vector to unit length w + = w + /||w + ||. In practice, the averaging operation can also be applied to the embeddings computed from the randomly augmented versions of the original low-shot training examples.</p><p>Fine-tuning. Since our model architecture has the same differentiable form as ordinary ConvNet classifiers, a finetuning step can be applied after new weights are imprinted. The average embedding strategy assumes that examples from each novel class have a unimodal distribution in the embedding space. This may not hold for every novel class since the learned embedding space could be biased towards features that are salient and discriminative among base classes. However, fine-tuning (using backpropagation to further optimize the network weights) should move the embedding space towards having unimodal distribution for the new class.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="5.">Implementation Details</head><p>The implementation details are comparable to <ref type="bibr" target="#b10">[11]</ref> and <ref type="bibr" target="#b11">[12]</ref>. For training, all the convolutional layers are initialized from ConvNet classifiers pre-trained on the ImageNet dataset <ref type="bibr" target="#b14">[15]</ref>. InceptionV1 <ref type="bibr" target="#b20">[21]</ref> is used in our experiments. The parameters of the fully-connected layers producing the embedding and unnormalized logit scores are initialized randomly. L 2 normalization is used for embedding vectors and weights in the last layer along the embedding dimension. Input images are resized to 256×256 and cropped to 224×224. Intensity is scaled to [−1, 1]. During training, we augment inputs with random cropping and random horizontal flipping. The learning rate is 0.0001 for pre-trained layers; a 10× multiplier is used for randomly initialized layers. We apply exponential decay every four epochs with decay rate 0.94. The RMSProp optimizer is used with momentum 0.9. During testing, input patches are cropped from the center.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.">Experiments</head><p>We empirically evaluate the classifiers containing imprinted weights. We first describe the overall protocols, then we present results on the CUB-200-2011 dataset.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.1.">Data Splits</head><p>The CUB-200-2011 dataset <ref type="bibr" target="#b22">[23]</ref> contains 200 finegrained categories of birds with 11,788 images. We use the train/test split provided by the dataset. In addition, we treat the first 100 classes as base classes where all the training examples (about 30 images per class on average) from these categories are used to train a base classifier. The remaining 100 classes are treated as novel classes where only n examples from the training split are used for low-shot learning. We experiment with a range of sizes n = 1, 2, 5, 10, 20 of novel exemplars for the low-shot training split. During testing, the original test split that includes both base and novel classes is used. We measure the top-1 classification accuracy of the final classifier on all categories. To show the effect of weight imprinting for low-shot categories, we also report the performance on the test examples from the novel classes only.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.2.">Models and Configuration Variants</head><p>Imprinting. To obtain imprinted models, we compute embeddings of novel examples and set novel weights in the final layer directly. When more than one novel example is available for a class, the mean of the normalized embeddings is used. The basic configuration (Imprinting) uses only the novel examples in their original forms. Alternatively, we experiment with random augmentation (Imprinting+Aug). Five augmented versions are generated for each novel example by random cropping and horizontal flipping, followed by averaging the embedded vectors. Both variants require only forward-pass computation of a trained embedding extractor without any iterative optimization. We compare these imprinting variants against a model initialization consisting of random novel weights without finetuning (Rand-noFT), which also involves zero backpropagation. Random weights are generated with a Xavier uniform initializer <ref type="bibr" target="#b2">[3]</ref>.</p><p>Fine-tuning. To demonstrate that imprinted weights can be used as better initializations than random values, we apply fine-tuning to the imprinting model (Imprinting+FT) and to the model with random novel weights (Rand+FT), respectively. In both cases, we fine-tune the entire network end-to-end. We use only low-shot examples from novel classes in addition to all training examples from base classes. When the distribution across all classes is imbalanced, we oversample the novel classes such that all the classes are sampled uniformly for each mini-batch. Random data augmentation is also applied.  <ref type="table">Table 2</ref>. 200-way top-1 accuracy measured across examples in all classes (100 base plus 100 novel classes) of CUB-200-2011. Imprinting retains similar advantages for rapid learning and initialization of fine-tuning as seen in <ref type="table">Table 1</ref>.</p><p>Jointly-trained ConvNet classifier. For comparison, we train a ConvNet classifier for base and novel classes jointly without a separate low-shot learning phase (AllClassJoint). The same data splits and preprocessing pipeline are used as in the fine-tuning cases. This model does not normalize embeddings or weights.</p><p>Other low-shot methods. We also apply the feature generator <ref type="bibr" target="#b4">[5]</ref> and matching networks <ref type="bibr" target="#b21">[22]</ref> to our normalized embeddings trained with the softmax loss for comparison. <ref type="table">Tables 1 and 2</ref> show the top-1 accuracy of 200-way classification for novel examples and all examples in CUB-200-2011, respectively. Without any backpropagation, the imprinted weights computed from one-shot examples instantly provide good classification performance: 21.26% on novel classes and 44.75% on all classes. Imprinting using the average of multiple augmented exemplars (Imprinting+Aug), using the same random flips and crops as for base class training, does not give a significant improvement in perfor-2 Rand-noFT is listed for easy comparison. Strictly, the header n = 1, . . . , 20 does not apply, since low-shot examples are not used.  . Accuracy of fine-tuned models measured over all classes (100 base plus 100 novel classes) for the first 40 epochs of training. <ref type="table">Table 2</ref> lists results after 112 epochs.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="6.3.">Results</head><p>mance. We conjecture this is because the embedding extractor has been trained on the base classes to be invariant to the applied transformations. When fine-tuning the network weights with backpropagation, models initialized with imprinted weights (Imprint-  ing+FT) take less time to converge and achieve better final accuracies than randomly initialized models, especially when limited low-shot examples are used. <ref type="figure" target="#fig_2">Figures 3 and 4</ref> plot evaluation accuracy of the fine-tunned models in the first 40 epochs on novel classes and all classes, respectively. Accuracies in <ref type="table">Tables 1 and 2</ref> are recorded after around 112 epochs. For cases n = 1, 2, the performance of imprinted weights is close to saturation and fine-tuning for more epochs can lead to degraded evaluation accuracies on novel classes, which we conjecture is due to overfitting on the 1 or 2 examples. The results show that the imprinted initialization can lead to better results for low-shot categories even when training from scratch on the entire dataset, as with AllClassJoint.</p><p>The classifier using generated features <ref type="bibr" target="#b4">[5]</ref> has a similar performance to imprinting for n = 1. While the matching network outperforms the feature generator as n increases, we observe a performance gap when compared with imprinting. For our tests we modified the matching network to perform 200-way classification instead of 5-way <ref type="bibr" target="#b21">[22]</ref>. <ref type="figure" target="#fig_4">Figure 5</ref> shows some sampled results following training of novel categories from the 1-shot imprinted model on CUB-200-2011. The top row shows randomly selected novel categories sorted by their classification accuracy as given below each exemplar. As might be expected, the highest-performing categories tend to exhibit more distinctive features of color, texture, and/or shape. In <ref type="figure" target="#fig_4">Figure 5</ref>(b) we show randomly selected success and failure cases predicted by the 1-shot imprinted model. The learned embeddings demonstrate an ability to generalize to different viewpoints, poses, and backgrounds from the single training example for each new category.</p><p>Transfer Learning with Imprinted Weights. We show that imprinting benefits transfer learning in general. To transfer a trained classifier to a new set of classes, we substitute the final layer parameters with the mean embeddings of examples from new classes. The only difference between our approach and standard transfer learning approaches is that we initialize the new weights by imprinting rather than with random values. Note that the imprinting process requires little cost in terms of computation. <ref type="table">Table 3</ref> shows the top-1 classification accuracy of the imprinted model on the new classes. Random initialization yields an accuracy of 0.85% while the models using imprinted weights have accuracies from 26.76% up to 52.25% as the number of training examples increases. Applying random augmentation (Imprinting+Aug) does not impact the performance significantly. Additional fine-tuning improves the performance. When novel training data is scarce (n = 1, 2, 5), starting from the imprinted weights (Imprinting+FT) outperforms fine-tuning from random weights (Rand+FT) by a large margin. With more training examples, fine-tuning from imprinted weights converges to similar accuracy as when starting from random weights.</p><p>Comparison with Nearest Neighbors. As discussed in Section 2, the usual approach used in metric learning has been to store all exemplar embeddings and use the nearest neighbor algorithm for classification. Therefore, we com-  pare our approach of using averaged embeddings with using a nearest-neighbor classifier where the embeddings of n low-shot training examples from each novel class form the population set. When there is only one training example per class, n = 1, the imprinted classifier is equivalent to the nearest neighbor classifier. When n &gt; 1, the size of the imprinted classifier remains constant, whereas the size of the nearest neighbor classifier grows linearly as n increases. Note that storing all embeddings trained with the softmax loss in a nearest-neighbor classifier is equivalent to a special case of Proxy-NCA <ref type="bibr" target="#b10">[11]</ref> using one proxy per class.</p><p>Perhaps surprisingly, the averaged embeddings perform better than storing all individual embeddings ( <ref type="figure" target="#fig_5">Figure 6</ref>). We conjecture that the averaging operation reduces potentially noisy dimensions in the embedding to focus on those that are more consistent for that category. Although the averaging may not seem to be the optimal choice in cases where the distribution of novel class examples has multiple modalities in the embedding space, we do not observe this in our experiments. When the embedding space is first trained on the base classes, lower layers of the network will have been trained to bring multiple modalities together for feature in- puts to the final linear layer. Moreover, keeping a single embedding for each class in the imprinted classifier has additional benefits since this standard form allows fine-tuning the embedding space with backpropagation and reduces test time computation and memory requirements.</p><p>Comparison with Lifted Structured Loss. Imprinted weights and Proxy-NCA are both trained with the softmax cross-entropy loss. Alternatively, we compare with embeddings trained with the lifted structured loss <ref type="bibr" target="#b11">[12]</ref>, which is a generalization of the widely used triplet loss. <ref type="figure" target="#fig_5">Figure 6</ref> shows that the softmax loss performs better in our experiments than the lifted structured loss. However, the lifted structured loss can also benefit from imprinting averaged embeddings rather than a nearest-neighbor classifier.</p><p>Embedding Dimensionality. We use 64-dimensional embeddings in all the experiments above. Empirically we experimented with various settings D = 64, 128, 256, 512 for the imprinting model and the jointly-trained ConvNet Classifier <ref type="figure" target="#fig_6">(Figure 7</ref>). Increasing the dimensionality does not appear to have significant effects on the results.</p></div>
<div xmlns="http://www.tei-c.org/ns/1.0"><head n="7.">Conclusions</head><p>This paper has presented a new method, weight imprinting, that directly sets the final layer weights of a ConvNet classifier for novel low-shot categories. This is a valuable complement to stochastic gradient descent, as it provides instant good classification performance on novel categories while allowing for further fine tuning when time permits. The key change that is made to the ConvNet architecture is a normalization layer with a scaling factor that allows activations computed for novel training examples to be directly copied (imprinted) as final layer weights. When multiple low-shot examples are presented, the computed activations for additional examples are averaged with the existing weights, which our experiments show to perform better than the nearest-neighbor approach typically used with embedding methods. An area for future research is whether the imprinting approach can also be used for more rapid training of other network layers, such as when encountering novel lower-level features.</p></div><figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_0"><head>Figure 1 .</head><label>1</label><figDesc>Figure 1. The overall architecture of imprinting. After a base classifier is trained, the embedding vectors of new low-shot examples are used to imprint weights for new classes in the extended classifier.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_1"><head>Figure 2 .</head><label>2</label><figDesc>Figure 2. Illustration of imprinting in the normalized embedding space. (a) Before imprinting, the decision boundaries are determined by the trained weights. (b) With imprinting, the embedding of an example (the yellow point) from a novel class defines a new region.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_2"><head>Figure 3 .</head><label>3</label><figDesc>Figure 3. Accuracy of fine-tuned models on novel classes for the first 40 epochs of training. Table 1 lists results after 112 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_3"><head>Figure 4</head><label>4</label><figDesc>Figure 4. Accuracy of fine-tuned models measured over all classes (100 base plus 100 novel classes) for the first 40 epochs of training. Table 2 lists results after 112 epochs.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_4"><head>Figure 5 .</head><label>5</label><figDesc>Figure 5. (a) A subset of exemplars used for 1-shot training of novel classes sorted by their recall@1 scores as shown below each exemplar. High-performing categories tend to exhibit more distinctive colors, shapes, and/or textures. (b) Randomly selected success and failure cases predicted by a 1-shot imprinted model on CUB-200-2011. Test images and the 1-shot exemplar whose embedding was used to imprint the predicted class are shown in separate rows. Correct and wrong predictions are marked with green and red borders, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_5"><head>Figure 6 .</head><label>6</label><figDesc>Figure 6. Top-1 accuracy of 100-way classification on novel classes of CUB-200-2011. Imprinting averaged embeddings with a softmax loss (blue bars) outperforms storing all individual embeddings with a nearest-neighbor classifier (green). By comparison, embeddings trained with the lifted structured loss do not perform as well as with the softmax loss (red and pink).</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" xml:id="fig_6"><head>Figure 7 .</head><label>7</label><figDesc>Figure 7. Classification accuracies for Imprinting and AllClassJoint with different embedding dimensionalities under 1-shot and 5-shot settings, respectively.</figDesc></figure>
<figure xmlns="http://www.tei-c.org/ns/1.0" type="table" xml:id="tab_2" validated="false"><head></head><label></label><figDesc>Imprinting + Aug 26.08 34.13 43.34 48.91 52.94Table 3. Top-1 accuracy for transfer learning on CUB-200-2011 using 1-20 examples for computing imprinted weights. The im- printed weights provide good immediate performance while also providing better final classification accuracy for 1 to 5 shot learn- ing following fine tuning.</figDesc><table>n = 

1 
2 
5 
10 
20 

w/o FT 

Rand-noFT 
0.85 
0.85 
0.85 
0.85 
0.85 
Imprinting 
26.76 33.11 43.00 48.74 52.25 
w/ FT 

Rand+FT 
15.90 28.84 46.21 61.37 71.57 
Imprinting + FT 
26.59 34.33 49.39 61.65 70.07 

</table></figure>
			<note xmlns="http://www.tei-c.org/ns/1.0" place="foot" n="1">In this paper we use the term "ConvNet classifiers" to refer to convolutional neural networks trained with the softmax cross-entropy loss for classification tasks.</note>
		</body>
		<back>
			<div type="references">

				<listBibl>

<biblStruct xml:id="b0">
	<monogr>
		<title/>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">L</forename><surname>Ba</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">R</forename><surname>Kiros</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1607.06450</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">Layer normalization. arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b1">
	<analytic>
		<title level="a" type="main">Learning a similarity metric discriminatively, with application to face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Chopra</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Hadsell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Lecun</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="539" to="546" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b2">
	<analytic>
		<title level="a" type="main">Understanding the difficulty of training deep feedforward neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Glorot</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Bengio</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics</title>
		<meeting>the Thirteenth International Conference on Artificial Intelligence and Statistics</meeting>
		<imprint>
			<date type="published" when="2010" />
			<biblScope unit="page" from="249" to="256" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b3">
	<analytic>
		<title level="a" type="main">Neighbourhood components analysis</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Goldberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><forename type="middle">E</forename><surname>Hinton</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><forename type="middle">T</forename><surname>Roweis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2005" />
			<biblScope unit="page" from="513" to="520" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b4">
	<analytic>
		<title level="a" type="main">Low-shot visual recognition by shrinking and hallucinating features</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Hariharan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Girshick</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE International Conference on Computer Vision</title>
		<meeting>the IEEE International Conference on Computer Vision</meeting>
		<imprint>
			<date type="published" when="2007" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b5">
	<analytic>
		<title level="a" type="main">Batch normalization: Accelerating deep network training by reducing internal covariate shift</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">International Conference on Machine Learning</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="448" to="456" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b6">
	<analytic>
		<title level="a" type="main">Siamese neural networks for one-shot image recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">G</forename><surname>Koch</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Zemel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">ICML Deep Learning Workshop</title>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">2</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b7">
	<analytic>
		<title level="a" type="main">Humanlevel concept learning through probabilistic program induction</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><forename type="middle">M</forename><surname>Lake</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><surname>Salakhutdinov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><forename type="middle">B</forename><surname>Tenenbaum</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Science</title>
		<imprint>
			<biblScope unit="volume">350</biblScope>
			<biblScope unit="issue">6266</biblScope>
			<biblScope unit="page" from="1332" to="1338" />
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b8">
	<monogr>
		<title level="m" type="main">Cosine normalization: Using cosine similarity instead of dot product in neural networks</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Luo</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Zhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Q</forename><surname>Yang</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1702.05870</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b9">
	<analytic>
		<title level="a" type="main">Learning from one example through shared densities on transforms</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">E</forename><forename type="middle">G</forename><surname>Miller</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">N</forename><forename type="middle">E</forename><surname>Matsakis</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><forename type="middle">A</forename><surname>Viola</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2000" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="464" to="471" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b10">
	<analytic>
		<title level="a" type="main">No fuss distance metric learning using proxies</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Movshovitz-Attias</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Toshev</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><forename type="middle">K</forename><surname>Leung</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ioffe</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Singh</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="360" to="368" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b11">
	<analytic>
		<title level="a" type="main">Deep metric learning via lifted structured feature embedding</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Savarese</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2008" />
			<biblScope unit="page" from="4004" to="4012" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b12">
	<monogr>
		<title level="m" type="main">Few-shot image recognition by predicting parameters from activations</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Qiao</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Shen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Yuille</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1706.03466</idno>
		<imprint>
			<date type="published" when="2017" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b13">
	<analytic>
		<title level="a" type="main">Metric learning with adaptive density discrimination</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Rippel</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Paluri</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Dollar</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><surname>Bourdev</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of International Conference on Learning Representations</title>
		<meeting>International Conference on Learning Representations</meeting>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b14">
	<analytic>
		<title level="a" type="main">ImageNet large scale visual recognition challenge</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Russakovsky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Deng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Su</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Krause</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Satheesh</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Ma</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Z</forename><surname>Huang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Karpathy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Khosla</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">M</forename><surname>Bernstein</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">International Journal of Computer Vision</title>
		<imprint>
			<biblScope unit="volume">115</biblScope>
			<biblScope unit="issue">3</biblScope>
			<biblScope unit="page">5</biblScope>
			<date type="published" when="2015" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b15">
	<analytic>
		<title level="a" type="main">Facenet: A unified embedding for face recognition and clustering</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Schroff</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Kalenichenko</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Philbin</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="volume">1</biblScope>
			<biblScope unit="page" from="815" to="823" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b16">
	<analytic>
		<title level="a" type="main">Prototypical networks for few-shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Snell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Swersky</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">R</forename><forename type="middle">S</forename><surname>Zemel</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2017" />
			<biblScope unit="page" from="4080" to="4090" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b17">
	<analytic>
		<title level="a" type="main">Improved deep metric learning with multi-class N-pair loss objective</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Sohn</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2016" />
			<biblScope unit="page" from="1857" to="1865" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b18">
	<monogr>
		<title level="m" type="main">Learnable structured clustering framework for deep metric learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><forename type="middle">O</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Jegelka</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Rathod</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><surname>Murphy</surname></persName>
		</author>
		<idno type="arXiv">arXiv:1612.01213</idno>
		<imprint>
			<date type="published" when="2016" />
		</imprint>
	</monogr>
<note type="report_type">arXiv preprint</note>
</biblStruct>

<biblStruct xml:id="b19">
	<analytic>
		<title level="a" type="main">Learning unified embedding for apparel recognition</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Song</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Li</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">B</forename><surname>Wu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C.-Y</forename><surname>Chen</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Zhang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">H</forename><surname>Adam</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">The IEEE International Conference on Computer Vision Workshops</title>
		<imprint>
			<date type="published" when="2002" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b20">
	<analytic>
		<title level="a" type="main">Going deeper with convolutions</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Szegedy</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">W</forename><surname>Liu</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">Y</forename><surname>Jia</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Sermanet</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Reed</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Anguelov</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Erhan</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">V</forename><surname>Vanhoucke</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><surname>Rabinovich</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition</title>
		<meeting>the IEEE Conference on Computer Vision and Pattern Recognition</meeting>
		<imprint>
			<date type="published" when="2015" />
			<biblScope unit="page" from="1" to="9" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b21">
	<analytic>
		<title level="a" type="main">Matching networks for one shot learning</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">O</forename><surname>Vinyals</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Blundell</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">T</forename><surname>Lillicrap</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">D</forename><surname>Wierstra</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Advances in Neural Information Processing Systems</title>
		<imprint>
			<date type="published" when="2006" />
			<biblScope unit="page">7</biblScope>
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b22">
	<monogr>
		<title level="m" type="main">The Caltech-UCSD birds-200-2011 dataset</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">C</forename><surname>Wah</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Branson</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Welinder</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">P</forename><surname>Perona</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">S</forename><surname>Belongie</surname></persName>
		</author>
		<imprint>
			<date type="published" when="2011" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b23">
	<analytic>
		<title level="a" type="main">NormFace: L2 hypersphere embedding for face verification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">F</forename><surname>Wang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">X</forename><surname>Xiang</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">J</forename><surname>Cheng</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">A</forename><forename type="middle">L</forename><surname>Yuille</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="m">Proceedings of the 2017 ACM on Multimedia Conference</title>
		<meeting>the 2017 ACM on Multimedia Conference</meeting>
		<imprint>
			<date type="published" when="2004" />
			<biblScope unit="page" from="1041" to="1049" />
		</imprint>
	</monogr>
</biblStruct>

<biblStruct xml:id="b24">
	<analytic>
		<title level="a" type="main">Distance metric learning for large margin nearest neighbor classification</title>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">K</forename><forename type="middle">Q</forename><surname>Weinberger</surname></persName>
		</author>
		<author>
			<persName xmlns="http://www.tei-c.org/ns/1.0"><forename type="first">L</forename><forename type="middle">K</forename><surname>Saul</surname></persName>
		</author>
	</analytic>
	<monogr>
		<title level="j">Journal of Machine Learning Research</title>
		<imprint>
			<biblScope unit="volume">10</biblScope>
			<biblScope unit="issue">1</biblScope>
			<biblScope unit="page" from="207" to="244" />
			<date type="published" when="2009-02" />
		</imprint>
	</monogr>
</biblStruct>

				</listBibl>
			</div>
		</back>
	</text>
</TEI>
